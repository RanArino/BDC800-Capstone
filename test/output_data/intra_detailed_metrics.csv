qa_id,query,ground_truth,llm_answer,rouge1_p,rouge1_r,rouge1_f,rouge2_p,rouge2_r,rouge2_f,rougeL_p,rougeL_r,rougeL_f,bleu,cosine_sim,self_checker
q0,What is the seed lexicon?,a vocabulary of positive and n,The seed lexicon consists of 1,0.2033898305084746,0.75,0.32,0.05172413793103448,0.2,0.0821917808219178,0.15254237288135594,0.5625,0.24000000000000002,0.031251907639724415,0.7026152014732361,Yes
q1,What are the results?,Using all data to train: AL --,The results are shown in Table,0.16901408450704225,0.23076923076923078,0.19512195121951217,0.0425531914893617,0.05825242718446602,0.04918032786885245,0.1267605633802817,0.17307692307692307,0.14634146341463414,0.07081467163036449,0.41043704748153687,No
q2,How are relations used to prop,based on the relation between ,Discourse relations (such as C,0.20588235294117646,0.3333333333333333,0.2545454545454545,0.030303030303030304,0.05,0.03773584905660378,0.17647058823529413,0.2857142857142857,0.21818181818181817,0.015167145487126126,0.505325436592102,Yes
q3,How big is the Japanese data?,7000000 pairs of events were e,"According to the context, the ",0.2222222222222222,0.19047619047619047,0.20512820512820512,0.058823529411764705,0.05,0.05405405405405405,0.16666666666666666,0.14285714285714285,0.15384615384615383,0.03956623635711305,0.3905796408653259,No
q4,How big are improvements of su,3%,"Unfortunately, the provided co",0.001,0.0,0.0,0.001,0.0,0.0,0.001,0.0,0.0,0.0,0.0,Undetermined
q5,How does their model learn usi,by exploiting discourse relati,The model learns using mostly ,0.11290322580645161,0.5,0.18421052631578946,0.01639344262295082,0.07692307692307693,0.02702702702702703,0.08064516129032258,0.35714285714285715,0.13157894736842105,0.013996036421018952,0.2745896577835083,Yes
q6,How big is seed lexicon used f,30 words,"According to the context, the ",0.001,0.0,0.0,0.001,0.0,0.0,0.001,0.0,0.0,0.0,0.0,Undetermined
