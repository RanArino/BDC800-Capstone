[
  {
    "query": "What is the seed lexicon?",
    "llm_answer": "The seed lexicon consists of 15 positive words and 15 negative words. It is used to assign polarity scores (+1 for positive events and -1 for negative events) to extracted events based on whether their predicates are in the seed lexicon and do not involve complex phenomena like negation.",
    "context": [
      {
        "id": "511fc609-03c5-4767-b7d9-b354506f96f3",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1909.00694",
          "start_idx": 8296,
          "end_idx": 8611
        },
        "page_content": "] made a serious mistake, [I] got fired. From this sentence, we extracted the event pair of \u201c\u91cd\u5927\u306a\u5931\u6557\u3092\u72af\u3059\u201d ([I] make a serious mistake) and \u201c\u4ed5\u4e8b\u3092\u30af\u30d3\u306b\u306a\u308b\u201d ([I] get fired), and tagged it with Cause. We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the",
        "type": "Document"
      },
      {
        "id": "2d50dda1-4bad-45e1-906f-e9ee938aa7f5",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1909.00694",
          "start_idx": 11244,
          "end_idx": 11751
        },
        "page_content": "or the reverse of it for negation) if the event's predicate is in the seed lexicon. We can see that the seed lexicon itself had practically no impact on prediction. The models in the top block performed considerably better than the random baselines. The performance gaps with their (semi-)supervised counterparts, shown in the middle block, were less than 7%. This demonstrates the effectiveness of discourse relation-based label propagation. Comparing the model variants, we obtained the highest score with",
        "type": "Document"
      },
      {
        "id": "5c228307-24e6-4a2d-8ba4-7e8ce494ef94",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1909.00694",
          "start_idx": 5168,
          "end_idx": 5658
        },
        "page_content": " As shown in Figure FIGREF1, we limit our scope to two discourse relations: Cause and Concession. The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the",
        "type": "Document"
      },
      {
        "id": "f102fc87-106b-4433-865b-b1bd3b940eb3",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1909.00694",
          "start_idx": 5454,
          "end_idx": 5949
        },
        "page_content": " to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.  The seed lexicon matches (1) the latter event but (2) not the former event, and (3) their discourse relation type is Cause or Concession. If the discourse relation type is Cause, the former event is given the same score as the latter. Likewise, if",
        "type": "Document"
      },
      {
        "id": "48b0afa8-0994-4154-b1f1-4d4b4e8c9da2",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1909.00694",
          "start_idx": 1452,
          "end_idx": 1786
        },
        "page_content": " If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the results?",
    "llm_answer": "The results are shown in Table TABREF23 (accuracy) and the text does not specify what these results are exactly, but based on the context, it appears to be accuracy values for different models and baselines.",
    "context": [
      {
        "id": "f5a340b1-f1a0-44a5-b07b-40956d4dd3f9",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1909.00694",
          "start_idx": 8898,
          "end_idx": 9330
        },
        "page_content": " CO such that it was five times larger than AL. The results are shown in Table TABREF16. We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively: . \u4f5c\u696d\u304c\u697d\u3060\u3002 The work",
        "type": "Document"
      },
      {
        "id": "3b25b28c-64f4-4a12-95bb-b71a300f4758",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1909.00694",
          "start_idx": 10558,
          "end_idx": 10948
        },
        "page_content": " output is the final hidden state corresponding to the special classification tag ([CLS]). For the details of ${\\rm Encoder}$, see Sections SECREF30. We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\mathcal {L}_{\\rm AL}$, $\\math",
        "type": "Document"
      },
      {
        "id": "ada9e075-1780-4d86-8cdf-b6a6453f8427",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1909.00694",
          "start_idx": 11000,
          "end_idx": 11367
        },
        "page_content": " + \\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$.   Table TABREF23 shows accuracy. As the Random baseline suggests, positive and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon and output the corresponding label (or the reverse of it for negation) if the event's predicate is in the seed lexicon",
        "type": "Document"
      },
      {
        "id": "a093c1bd-a405-47e7-9351-9de47ee71286",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1909.00694",
          "start_idx": 8611,
          "end_idx": 9077
        },
        "page_content": " consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF",
        "type": "Document"
      },
      {
        "id": "05fb6a80-99a8-4e60-8ab3-ecc77011ffa7",
        "metadata": {
          "chunk_id": 30,
          "document_id": "1909.00694",
          "start_idx": 9921,
          "end_idx": 10268
        },
        "page_content": " the ACP Corpus. To optimize the hyperparameters, we used the dev set of the ACP Corpus. For the evaluation, we used the test set of the ACP Corpus. The model output was classified as positive if $p(x) > 0$ and negative if $p(x) \\le 0$.  As for ${\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. GRU BIBREF16 is a recurrent",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are relations used to propagate polarity?",
    "llm_answer": "According to the context, discourse relations (specifically Cause and Concession) are used to efficiently propagate polarity from seed predicates. If the predicate of an extracted event is in the seed lexicon, its corresponding polarity score can be assigned to it. For events that do not have a matching predicate in the seed lexicon, their polarities are assumed to be reversed if they are connected by a Concession relation.",
    "context": [
      {
        "id": "4e5cf78f-44f5-4aa2-a8ef-7b6c02a00366",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1909.00694",
          "start_idx": 1786,
          "end_idx": 2082
        },
        "page_content": ".e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is",
        "type": "Document"
      },
      {
        "id": "45d130ab-62d2-4c54-943f-915bea9e608e",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1909.00694",
          "start_idx": 1955,
          "end_idx": 2465
        },
        "page_content": " (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event. We trained the models using a Japanese web corpus. Given the minimum amount of supervision, they performed well. In addition, the combination of annotated and unannotated data yielded a gain over a purely supervised baseline when labeled data were small. Learning affect",
        "type": "Document"
      },
      {
        "id": "5c228307-24e6-4a2d-8ba4-7e8ce494ef94",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1909.00694",
          "start_idx": 5168,
          "end_idx": 5658
        },
        "page_content": " As shown in Figure FIGREF1, we limit our scope to two discourse relations: Cause and Concession. The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the",
        "type": "Document"
      },
      {
        "id": "51940530-7cf4-48af-a60b-d08abf6f616f",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1909.00694",
          "start_idx": 6255,
          "end_idx": 6711
        },
        "page_content": " polarities.  The seed lexicon matches neither the former nor the latter event, and their discourse relation type is Concession. We assume the two events have the reversed polarities.  Using AL, CA, and CO data, we optimize the parameters of the polarity function $p(x)$. We define a loss function for each of the three types of event pairs and sum up the multiple loss functions. We use mean squared error to construct loss functions. For the AL data, the",
        "type": "Document"
      },
      {
        "id": "277579d4-93b0-40bd-bf33-93f5abb8a606",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1909.00694",
          "start_idx": 780,
          "end_idx": 1273
        },
        "page_content": " combinatorial nature of language, the non-compositionality of affective polarity entails the need for large amounts of world knowledge, which can hardly be learned from small annotated data. In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed pred",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is the Japanese data?",
    "llm_answer": "The context does not mention a specific \"Japanese data\" but rather mentions the use of Japanese BERT models pretrained with 100 million sentences from Japanese Wikipedia, and also uses Japanese websites for dataset extraction. The vocabulary size was 32,000 or 100,000 depending on the model being described.",
    "context": [
      {
        "id": "f5a340b1-f1a0-44a5-b07b-40956d4dd3f9",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1909.00694",
          "start_idx": 8898,
          "end_idx": 9330
        },
        "page_content": " CO such that it was five times larger than AL. The results are shown in Table TABREF16. We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively: . \u4f5c\u696d\u304c\u697d\u3060\u3002 The work",
        "type": "Document"
      },
      {
        "id": "1a3bd67c-5129-4d5b-9fd9-71d7e3a22b74",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1909.00694",
          "start_idx": 15131,
          "end_idx": 15574
        },
        "page_content": " vocabulary size was 100,000. The number of hidden layers was 2. The dimension of hidden units was 256. The optimizer was Momentum SGD BIBREF21. The mini-batch size was 1024. We ran 100 epochs and selected the snapshot that achieved the highest score for the dev set. We used a Japanese BERT model pretrained with Japanese Wikipedia. The input sentences were segmented into words by Juman++, and words were broken into subwords by applying BPE",
        "type": "Document"
      },
      {
        "id": "03dd14dd-51b6-440c-9380-05031c6bfdb7",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1909.00694",
          "start_idx": 12027,
          "end_idx": 12460
        },
        "page_content": " models (ACP) outperformed semi-supervised models (ACP+AL+CA+CO). This suggests that the training set of 0.6 million events is sufficiently large for training the models. For comparison, we trained the models with a subset (6,000 events) of the ACP dataset. As the results shown in Table TABREF24 demonstrate, our method is effective when labeled data are small. The result of hyperparameter optimization for the BiGRU encoder was as",
        "type": "Document"
      },
      {
        "id": "8a87b33c-a1b1-4227-8c6b-710619ad4380",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1909.00694",
          "start_idx": 15395,
          "end_idx": 15766
        },
        "page_content": " segmented into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam BIBREF19. The mini-batch size was 32. We ran 1 epoch.",
        "type": "Document"
      },
      {
        "id": "a093c1bd-a405-47e7-9351-9de47ee71286",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1909.00694",
          "start_idx": 8611,
          "end_idx": 9077
        },
        "page_content": " consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?",
    "llm_answer": "The question is a bit convoluted, but I'll try to provide an answer based on the context.\n\nAccording to the text, when the models were trained on a small dataset (6,000 events) of labeled data (ACP), and enhanced with additional unlabeled data (AL+CA+CO), the results show that the proposed approach (discourse relation-based label propagation) is effective. However, it does not explicitly mention the size of the improvements.\n\nBut we can infer from the text that the performance gaps between the supervised models (ACP) and their semi-supervised counterparts (ACP+AL+CA+CO) were less than 7%. This suggests that the proposed approach made a significant improvement on the results trained on small labeled data, but the exact magnitude of the improvements is not specified.",
    "context": [
      {
        "id": "1d058b4d-c856-4d08-988d-0567d148701e",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1909.00694",
          "start_idx": 11751,
          "end_idx": 12204
        },
        "page_content": " of discourse relation-based label propagation. Comparing the model variants, we obtained the highest score with the BiGRU encoder trained with the AL+CA+CO dataset. BERT was competitive but its performance went down if CA and CO were used in addition to AL. We conjecture that BERT was more sensitive to noises found more frequently in CA and CO. Contrary to our expectations, supervised models (ACP) outperformed semi-supervised models (ACP+AL+CA+CO).",
        "type": "Document"
      },
      {
        "id": "03dd14dd-51b6-440c-9380-05031c6bfdb7",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1909.00694",
          "start_idx": 12027,
          "end_idx": 12460
        },
        "page_content": " models (ACP) outperformed semi-supervised models (ACP+AL+CA+CO). This suggests that the training set of 0.6 million events is sufficiently large for training the models. For comparison, we trained the models with a subset (6,000 events) of the ACP dataset. As the results shown in Table TABREF24 demonstrate, our method is effective when labeled data are small. The result of hyperparameter optimization for the BiGRU encoder was as",
        "type": "Document"
      },
      {
        "id": "2d50dda1-4bad-45e1-906f-e9ee938aa7f5",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1909.00694",
          "start_idx": 11244,
          "end_idx": 11751
        },
        "page_content": "or the reverse of it for negation) if the event's predicate is in the seed lexicon. We can see that the seed lexicon itself had practically no impact on prediction. The models in the top block performed considerably better than the random baselines. The performance gaps with their (semi-)supervised counterparts, shown in the middle block, were less than 7%. This demonstrates the effectiveness of discourse relation-based label propagation. Comparing the model variants, we obtained the highest score with",
        "type": "Document"
      },
      {
        "id": "1a3bd67c-5129-4d5b-9fd9-71d7e3a22b74",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1909.00694",
          "start_idx": 15131,
          "end_idx": 15574
        },
        "page_content": " vocabulary size was 100,000. The number of hidden layers was 2. The dimension of hidden units was 256. The optimizer was Momentum SGD BIBREF21. The mini-batch size was 1024. We ran 100 epochs and selected the snapshot that achieved the highest score for the dev set. We used a Japanese BERT model pretrained with Japanese Wikipedia. The input sentences were segmented into words by Juman++, and words were broken into subwords by applying BPE",
        "type": "Document"
      },
      {
        "id": "8a87b33c-a1b1-4227-8c6b-710619ad4380",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1909.00694",
          "start_idx": 15395,
          "end_idx": 15766
        },
        "page_content": " segmented into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam BIBREF19. The mini-batch size was 32. We ran 1 epoch.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does their model learn using mostly raw data?",
    "llm_answer": "The model learns by using semi-supervised learning, where it uses a combination of labeled and unlabeled data (ACP Corpus for labeled data and AL+CA+CO datasets for unlabeled data) to automatically learn complex phenomena through label propagation. Additionally, the model is pre-trained with a seed lexicon that provides polarity scores for positive and negative events, allowing the model to start learning from the raw data and gradually refine its understanding of complex phenomena.",
    "context": [
      {
        "id": "3b25b28c-64f4-4a12-95bb-b71a300f4758",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1909.00694",
          "start_idx": 10558,
          "end_idx": 10948
        },
        "page_content": " output is the final hidden state corresponding to the special classification tag ([CLS]). For the details of ${\\rm Encoder}$, see Sections SECREF30. We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\mathcal {L}_{\\rm AL}$, $\\math",
        "type": "Document"
      },
      {
        "id": "f102fc87-106b-4433-865b-b1bd3b940eb3",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1909.00694",
          "start_idx": 5454,
          "end_idx": 5949
        },
        "page_content": " to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.  The seed lexicon matches (1) the latter event but (2) not the former event, and (3) their discourse relation type is Cause or Concession. If the discourse relation type is Cause, the former event is given the same score as the latter. Likewise, if",
        "type": "Document"
      },
      {
        "id": "25aba0e6-7d04-490d-b192-62a41cfd0c71",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1909.00694",
          "start_idx": 9585,
          "end_idx": 9921
        },
        "page_content": " last clause in it. The train/dev/test split of the data is shown in Table TABREF19. The objective function for supervised training is:  where $v_i$ is the $i$-th event, $R_i$ is the reference score of $v_i$, and $N_{\\rm ACP}$ is the number of the events of the ACP Corpus. To optimize the hyperparameters, we used the dev set of the AC",
        "type": "Document"
      },
      {
        "id": "05fb6a80-99a8-4e60-8ab3-ecc77011ffa7",
        "metadata": {
          "chunk_id": 30,
          "document_id": "1909.00694",
          "start_idx": 9921,
          "end_idx": 10268
        },
        "page_content": " the ACP Corpus. To optimize the hyperparameters, we used the dev set of the ACP Corpus. For the evaluation, we used the test set of the ACP Corpus. The model output was classified as positive if $p(x) > 0$ and negative if $p(x) \\le 0$.  As for ${\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. GRU BIBREF16 is a recurrent",
        "type": "Document"
      },
      {
        "id": "5c228307-24e6-4a2d-8ba4-7e8ce494ef94",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1909.00694",
          "start_idx": 5168,
          "end_idx": 5658
        },
        "page_content": " As shown in Figure FIGREF1, we limit our scope to two discourse relations: Cause and Concession. The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is seed lexicon used for training?",
    "llm_answer": "According to the context, the seed lexicon consisted of 15 positive words and 15 negative words.",
    "context": [
      {
        "id": "8a87b33c-a1b1-4227-8c6b-710619ad4380",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1909.00694",
          "start_idx": 15395,
          "end_idx": 15766
        },
        "page_content": " segmented into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam BIBREF19. The mini-batch size was 32. We ran 1 epoch.",
        "type": "Document"
      },
      {
        "id": "2d50dda1-4bad-45e1-906f-e9ee938aa7f5",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1909.00694",
          "start_idx": 11244,
          "end_idx": 11751
        },
        "page_content": "or the reverse of it for negation) if the event's predicate is in the seed lexicon. We can see that the seed lexicon itself had practically no impact on prediction. The models in the top block performed considerably better than the random baselines. The performance gaps with their (semi-)supervised counterparts, shown in the middle block, were less than 7%. This demonstrates the effectiveness of discourse relation-based label propagation. Comparing the model variants, we obtained the highest score with",
        "type": "Document"
      },
      {
        "id": "1a3bd67c-5129-4d5b-9fd9-71d7e3a22b74",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1909.00694",
          "start_idx": 15131,
          "end_idx": 15574
        },
        "page_content": " vocabulary size was 100,000. The number of hidden layers was 2. The dimension of hidden units was 256. The optimizer was Momentum SGD BIBREF21. The mini-batch size was 1024. We ran 100 epochs and selected the snapshot that achieved the highest score for the dev set. We used a Japanese BERT model pretrained with Japanese Wikipedia. The input sentences were segmented into words by Juman++, and words were broken into subwords by applying BPE",
        "type": "Document"
      },
      {
        "id": "511fc609-03c5-4767-b7d9-b354506f96f3",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1909.00694",
          "start_idx": 8296,
          "end_idx": 8611
        },
        "page_content": "] made a serious mistake, [I] got fired. From this sentence, we extracted the event pair of \u201c\u91cd\u5927\u306a\u5931\u6557\u3092\u72af\u3059\u201d ([I] make a serious mistake) and \u201c\u4ed5\u4e8b\u3092\u30af\u30d3\u306b\u306a\u308b\u201d ([I] get fired), and tagged it with Cause. We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the",
        "type": "Document"
      },
      {
        "id": "03dd14dd-51b6-440c-9380-05031c6bfdb7",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1909.00694",
          "start_idx": 12027,
          "end_idx": 12460
        },
        "page_content": " models (ACP) outperformed semi-supervised models (ACP+AL+CA+CO). This suggests that the training set of 0.6 million events is sufficiently large for training the models. For comparison, we trained the models with a subset (6,000 events) of the ACP dataset. As the results shown in Table TABREF24 demonstrate, our method is effective when labeled data are small. The result of hyperparameter optimization for the BiGRU encoder was as",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do the various social phenomena examined manifest in different types of communities?",
    "llm_answer": "The answer cannot be determined from the provided context as it does not mention anything about how social phenomena manifest in different types of communities. The text is actually discussing a model for learning affective events through discourse analysis and using a seed lexicon to propagate polarities, with no reference to community types or social phenomena beyond sentiment analysis.",
    "context": [
      {
        "id": "f102fc87-106b-4433-865b-b1bd3b940eb3",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1909.00694",
          "start_idx": 5454,
          "end_idx": 5949
        },
        "page_content": " to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.  The seed lexicon matches (1) the latter event but (2) not the former event, and (3) their discourse relation type is Cause or Concession. If the discourse relation type is Cause, the former event is given the same score as the latter. Likewise, if",
        "type": "Document"
      },
      {
        "id": "d9b035eb-7983-4d22-ab38-3acbc996fdcf",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1909.00694",
          "start_idx": 2465,
          "end_idx": 2938
        },
        "page_content": " unannotated data yielded a gain over a purely supervised baseline when labeled data were small. Learning affective events is closely related to sentiment analysis. Whereas sentiment analysis usually focuses on the polarity of what are described (e.g., movies), we work on how people are typically affected by events. In sentiment analysis, much attention has been paid to compositionality. Word-level polarity BIBREF5, BIBREF6, BIBREF7 and the roles of negation and intens",
        "type": "Document"
      },
      {
        "id": "ed35dcb3-ddd2-4a60-aea7-352d557c807f",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1909.00694",
          "start_idx": 13637,
          "end_idx": 14204
        },
        "page_content": " shoulders) is an idiom that expresses a disappointed feeling. The examples demonstrate that our model correctly learned non-compositional expressions.  In this paper, we proposed to use discourse relations to effectively propagate polarities of affective events from seeds. Experiments show that, even with a minimal amount of supervision, the proposed method performed well. Although event pairs linked by discourse analysis are shown to be useful, they nevertheless contain noises. Adding linguistically-motivated filtering rules would help improve the performance",
        "type": "Document"
      },
      {
        "id": "8b975852-7ea3-48e0-9b86-7d826e399f0e",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1909.00694",
          "start_idx": 4812,
          "end_idx": 5168
        },
        "page_content": " requires a very small seed lexicon and a large raw corpus. We assume that we can automatically extract discourse-tagged event pairs, $(x_{i1}, x_{i2})$ ($i=1, \\cdots $) from the raw corpus. We refer to $x_{i1}$ and $x_{i2}$ as former and latter events, respectively. As shown in Figure FIGREF1, we limit our scope to two discourse relations: Cause and Con",
        "type": "Document"
      },
      {
        "id": "bfe04002-5aea-43e8-a195-bf869277e786",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1909.00694",
          "start_idx": 3392,
          "end_idx": 3871
        },
        "page_content": " $B$\u201d and \u201c$A$ but $B$\u201d). We shift our scope to event pairs that are more complex than phrase pairs, and consequently exploit discourse connectives as event-level counterparts of phrase-level conjunctions. BIBREF0 constructed a network of events using word embedding-derived similarities. Compared with this method, our discourse relation-based linking of events is much simpler and more intuitive. Some previous studies made use of document structure to understand the sentiment",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How did the select the 300 Reddit communities for comparison?",
    "llm_answer": "The provided context does not mention selecting 300 Reddit communities for comparison at all. The text discusses the ACP Corpus, event pairs, hyperparameter optimization, and neural network models for classification tasks, but it does not mention Reddit communities. Therefore, there is no answer to the question as it pertains to information not included in the provided context.",
    "context": [
      {
        "id": "a093c1bd-a405-47e7-9351-9de47ee71286",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1909.00694",
          "start_idx": 8611,
          "end_idx": 9077
        },
        "page_content": " consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF",
        "type": "Document"
      },
      {
        "id": "f5a340b1-f1a0-44a5-b07b-40956d4dd3f9",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1909.00694",
          "start_idx": 8898,
          "end_idx": 9330
        },
        "page_content": " CO such that it was five times larger than AL. The results are shown in Table TABREF16. We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively: . \u4f5c\u696d\u304c\u697d\u3060\u3002 The work",
        "type": "Document"
      },
      {
        "id": "067fa0ac-914c-497c-bac9-7e21cceeb570",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1909.00694",
          "start_idx": 12460,
          "end_idx": 12825
        },
        "page_content": " data are small. The result of hyperparameter optimization for the BiGRU encoder was as follows: As the CA and CO pairs were equal in size (Table TABREF16), $\\lambda _{\\rm CA}$ and $\\lambda _{\\rm CO}$ were comparable values. $\\lambda _{\\rm CA}$ was about one-third of $\\lambda _{\\rm CO}$, and this indicated that the CA pairs were noisier than the CO pairs. A major",
        "type": "Document"
      },
      {
        "id": "03dd14dd-51b6-440c-9380-05031c6bfdb7",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1909.00694",
          "start_idx": 12027,
          "end_idx": 12460
        },
        "page_content": " models (ACP) outperformed semi-supervised models (ACP+AL+CA+CO). This suggests that the training set of 0.6 million events is sufficiently large for training the models. For comparison, we trained the models with a subset (6,000 events) of the ACP dataset. As the results shown in Table TABREF24 demonstrate, our method is effective when labeled data are small. The result of hyperparameter optimization for the BiGRU encoder was as",
        "type": "Document"
      },
      {
        "id": "05fb6a80-99a8-4e60-8ab3-ecc77011ffa7",
        "metadata": {
          "chunk_id": 30,
          "document_id": "1909.00694",
          "start_idx": 9921,
          "end_idx": 10268
        },
        "page_content": " the ACP Corpus. To optimize the hyperparameters, we used the dev set of the ACP Corpus. For the evaluation, we used the test set of the ACP Corpus. The model output was classified as positive if $p(x) > 0$ and negative if $p(x) \\le 0$.  As for ${\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. GRU BIBREF16 is a recurrent",
        "type": "Document"
      }
    ]
  }
]