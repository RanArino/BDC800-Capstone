[
  {
    "query": "What is the seed lexicon?",
    "llm_answer": "A seed lexicon consisting of 15 positive words and 15 negative words.\n",
    "context": [
      {
        "id": "ae6fb495-a6c0-4084-bfd9-8501e84ef267",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1909.00694",
          "start_idx": 8296,
          "end_idx": 8611
        },
        "page_content": "] made a serious mistake, [I] got fired. From this sentence, we extracted the event pair of \u201c\u91cd\u5927\u306a\u5931\u6557\u3092\u72af\u3059\u201d ([I] make a serious mistake) and \u201c\u4ed5\u4e8b\u3092\u30af\u30d3\u306b\u306a\u308b\u201d ([I] get fired), and tagged it with Cause. We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the",
        "type": "Document"
      },
      {
        "id": "a91598c8-9447-48fe-97fb-f0d1ef3c541f",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1909.00694",
          "start_idx": 11244,
          "end_idx": 11751
        },
        "page_content": "or the reverse of it for negation) if the event's predicate is in the seed lexicon. We can see that the seed lexicon itself had practically no impact on prediction. The models in the top block performed considerably better than the random baselines. The performance gaps with their (semi-)supervised counterparts, shown in the middle block, were less than 7%. This demonstrates the effectiveness of discourse relation-based label propagation. Comparing the model variants, we obtained the highest score with",
        "type": "Document"
      },
      {
        "id": "149c5bc3-0411-4eca-8082-d07c0b25a029",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1909.00694",
          "start_idx": 5168,
          "end_idx": 5658
        },
        "page_content": " As shown in Figure FIGREF1, we limit our scope to two discourse relations: Cause and Concession. The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the",
        "type": "Document"
      },
      {
        "id": "d1829256-a8cc-4411-a0c8-23c550a1b744",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1909.00694",
          "start_idx": 5454,
          "end_idx": 5949
        },
        "page_content": " to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.  The seed lexicon matches (1) the latter event but (2) not the former event, and (3) their discourse relation type is Cause or Concession. If the discourse relation type is Cause, the former event is given the same score as the latter. Likewise, if",
        "type": "Document"
      },
      {
        "id": "cf9de8ca-0110-468d-b0d3-751c84922b98",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1909.00694",
          "start_idx": 1452,
          "end_idx": 1786
        },
        "page_content": " If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the results?",
    "llm_answer": "Results are shown in Tables TABREF16 and TABREF23.\n",
    "context": [
      {
        "id": "cf348585-ca79-493f-be9d-ce678544cd42",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1909.00694",
          "start_idx": 8898,
          "end_idx": 9330
        },
        "page_content": " CO such that it was five times larger than AL. The results are shown in Table TABREF16. We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively: . \u4f5c\u696d\u304c\u697d\u3060\u3002 The work",
        "type": "Document"
      },
      {
        "id": "3f30cfc3-ee45-4616-bbb4-6d47b91f5a8d",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1909.00694",
          "start_idx": 10558,
          "end_idx": 10948
        },
        "page_content": " output is the final hidden state corresponding to the special classification tag ([CLS]). For the details of ${\\rm Encoder}$, see Sections SECREF30. We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\mathcal {L}_{\\rm AL}$, $\\math",
        "type": "Document"
      },
      {
        "id": "882407df-0fa7-40c5-bab3-f0797344836e",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1909.00694",
          "start_idx": 11000,
          "end_idx": 11367
        },
        "page_content": " + \\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$.   Table TABREF23 shows accuracy. As the Random baseline suggests, positive and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon and output the corresponding label (or the reverse of it for negation) if the event's predicate is in the seed lexicon",
        "type": "Document"
      },
      {
        "id": "7621bfc7-be78-4b69-9eaa-72f4cbc4cefe",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1909.00694",
          "start_idx": 8611,
          "end_idx": 9077
        },
        "page_content": " consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF",
        "type": "Document"
      },
      {
        "id": "3206227a-b07b-4d69-913b-317680146a4c",
        "metadata": {
          "chunk_id": 30,
          "document_id": "1909.00694",
          "start_idx": 9921,
          "end_idx": 10268
        },
        "page_content": " the ACP Corpus. To optimize the hyperparameters, we used the dev set of the ACP Corpus. For the evaluation, we used the test set of the ACP Corpus. The model output was classified as positive if $p(x) > 0$ and negative if $p(x) \\le 0$.  As for ${\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. GRU BIBREF16 is a recurrent",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are relations used to propagate polarity?",
    "llm_answer": "The reverse of one event's polarity can be propagated to another event based on the discourse relation (Cause or Concession) between them.  If the relation is Concession, the polarities are assumed to be reversed.\n",
    "context": [
      {
        "id": "2c0da366-549d-4a0a-96f2-89c01b90ec7c",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1909.00694",
          "start_idx": 1786,
          "end_idx": 2082
        },
        "page_content": ".e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is",
        "type": "Document"
      },
      {
        "id": "ebbe7200-f17f-4f26-9345-1a417875849a",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1909.00694",
          "start_idx": 1955,
          "end_idx": 2465
        },
        "page_content": " (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event. We trained the models using a Japanese web corpus. Given the minimum amount of supervision, they performed well. In addition, the combination of annotated and unannotated data yielded a gain over a purely supervised baseline when labeled data were small. Learning affect",
        "type": "Document"
      },
      {
        "id": "149c5bc3-0411-4eca-8082-d07c0b25a029",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1909.00694",
          "start_idx": 5168,
          "end_idx": 5658
        },
        "page_content": " As shown in Figure FIGREF1, we limit our scope to two discourse relations: Cause and Concession. The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the",
        "type": "Document"
      },
      {
        "id": "e0e1300f-3646-41db-ad56-069ec69c78a0",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1909.00694",
          "start_idx": 6255,
          "end_idx": 6711
        },
        "page_content": " polarities.  The seed lexicon matches neither the former nor the latter event, and their discourse relation type is Concession. We assume the two events have the reversed polarities.  Using AL, CA, and CO data, we optimize the parameters of the polarity function $p(x)$. We define a loss function for each of the three types of event pairs and sum up the multiple loss functions. We use mean squared error to construct loss functions. For the AL data, the",
        "type": "Document"
      },
      {
        "id": "542c0559-1273-4336-8221-c619c7c5ac8f",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1909.00694",
          "start_idx": 780,
          "end_idx": 1273
        },
        "page_content": " combinatorial nature of language, the non-compositionality of affective polarity entails the need for large amounts of world knowledge, which can hardly be learned from small annotated data. In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed pred",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is the Japanese data?",
    "llm_answer": "About 100 million sentences.\n",
    "context": [
      {
        "id": "cf348585-ca79-493f-be9d-ce678544cd42",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1909.00694",
          "start_idx": 8898,
          "end_idx": 9330
        },
        "page_content": " CO such that it was five times larger than AL. The results are shown in Table TABREF16. We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively: . \u4f5c\u696d\u304c\u697d\u3060\u3002 The work",
        "type": "Document"
      },
      {
        "id": "74f13364-433c-497c-b3bc-ac6efb7e2752",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1909.00694",
          "start_idx": 15131,
          "end_idx": 15574
        },
        "page_content": " vocabulary size was 100,000. The number of hidden layers was 2. The dimension of hidden units was 256. The optimizer was Momentum SGD BIBREF21. The mini-batch size was 1024. We ran 100 epochs and selected the snapshot that achieved the highest score for the dev set. We used a Japanese BERT model pretrained with Japanese Wikipedia. The input sentences were segmented into words by Juman++, and words were broken into subwords by applying BPE",
        "type": "Document"
      },
      {
        "id": "42ca48e4-a843-4062-b3e4-7c2bbbff0aef",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1909.00694",
          "start_idx": 12027,
          "end_idx": 12460
        },
        "page_content": " models (ACP) outperformed semi-supervised models (ACP+AL+CA+CO). This suggests that the training set of 0.6 million events is sufficiently large for training the models. For comparison, we trained the models with a subset (6,000 events) of the ACP dataset. As the results shown in Table TABREF24 demonstrate, our method is effective when labeled data are small. The result of hyperparameter optimization for the BiGRU encoder was as",
        "type": "Document"
      },
      {
        "id": "fb85c318-f6d9-48b7-982a-9106ee5d2aec",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1909.00694",
          "start_idx": 15395,
          "end_idx": 15766
        },
        "page_content": " segmented into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam BIBREF19. The mini-batch size was 32. We ran 1 epoch.",
        "type": "Document"
      },
      {
        "id": "7621bfc7-be78-4b69-9eaa-72f4cbc4cefe",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1909.00694",
          "start_idx": 8611,
          "end_idx": 9077
        },
        "page_content": " consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?",
    "llm_answer": "The context states that the proposed method was effective when labeled data was small, but doesn't provide a quantitative comparison of improvement sizes.\n",
    "context": [
      {
        "id": "a276b7d8-2a80-443e-8bd8-ae7df0c5cdae",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1909.00694",
          "start_idx": 11751,
          "end_idx": 12204
        },
        "page_content": " of discourse relation-based label propagation. Comparing the model variants, we obtained the highest score with the BiGRU encoder trained with the AL+CA+CO dataset. BERT was competitive but its performance went down if CA and CO were used in addition to AL. We conjecture that BERT was more sensitive to noises found more frequently in CA and CO. Contrary to our expectations, supervised models (ACP) outperformed semi-supervised models (ACP+AL+CA+CO).",
        "type": "Document"
      },
      {
        "id": "42ca48e4-a843-4062-b3e4-7c2bbbff0aef",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1909.00694",
          "start_idx": 12027,
          "end_idx": 12460
        },
        "page_content": " models (ACP) outperformed semi-supervised models (ACP+AL+CA+CO). This suggests that the training set of 0.6 million events is sufficiently large for training the models. For comparison, we trained the models with a subset (6,000 events) of the ACP dataset. As the results shown in Table TABREF24 demonstrate, our method is effective when labeled data are small. The result of hyperparameter optimization for the BiGRU encoder was as",
        "type": "Document"
      },
      {
        "id": "a91598c8-9447-48fe-97fb-f0d1ef3c541f",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1909.00694",
          "start_idx": 11244,
          "end_idx": 11751
        },
        "page_content": "or the reverse of it for negation) if the event's predicate is in the seed lexicon. We can see that the seed lexicon itself had practically no impact on prediction. The models in the top block performed considerably better than the random baselines. The performance gaps with their (semi-)supervised counterparts, shown in the middle block, were less than 7%. This demonstrates the effectiveness of discourse relation-based label propagation. Comparing the model variants, we obtained the highest score with",
        "type": "Document"
      },
      {
        "id": "74f13364-433c-497c-b3bc-ac6efb7e2752",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1909.00694",
          "start_idx": 15131,
          "end_idx": 15574
        },
        "page_content": " vocabulary size was 100,000. The number of hidden layers was 2. The dimension of hidden units was 256. The optimizer was Momentum SGD BIBREF21. The mini-batch size was 1024. We ran 100 epochs and selected the snapshot that achieved the highest score for the dev set. We used a Japanese BERT model pretrained with Japanese Wikipedia. The input sentences were segmented into words by Juman++, and words were broken into subwords by applying BPE",
        "type": "Document"
      },
      {
        "id": "fb85c318-f6d9-48b7-982a-9106ee5d2aec",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1909.00694",
          "start_idx": 15395,
          "end_idx": 15766
        },
        "page_content": " segmented into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam BIBREF19. The mini-batch size was 32. We ran 1 epoch.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does their model learn using mostly raw data?",
    "llm_answer": "The model automatically learns complex phenomena through label propagation.\n",
    "context": [
      {
        "id": "3f30cfc3-ee45-4616-bbb4-6d47b91f5a8d",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1909.00694",
          "start_idx": 10558,
          "end_idx": 10948
        },
        "page_content": " output is the final hidden state corresponding to the special classification tag ([CLS]). For the details of ${\\rm Encoder}$, see Sections SECREF30. We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\mathcal {L}_{\\rm AL}$, $\\math",
        "type": "Document"
      },
      {
        "id": "d1829256-a8cc-4411-a0c8-23c550a1b744",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1909.00694",
          "start_idx": 5454,
          "end_idx": 5949
        },
        "page_content": " to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.  The seed lexicon matches (1) the latter event but (2) not the former event, and (3) their discourse relation type is Cause or Concession. If the discourse relation type is Cause, the former event is given the same score as the latter. Likewise, if",
        "type": "Document"
      },
      {
        "id": "1e242fc7-0adb-4282-9273-1f665f612abe",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1909.00694",
          "start_idx": 9585,
          "end_idx": 9921
        },
        "page_content": " last clause in it. The train/dev/test split of the data is shown in Table TABREF19. The objective function for supervised training is:  where $v_i$ is the $i$-th event, $R_i$ is the reference score of $v_i$, and $N_{\\rm ACP}$ is the number of the events of the ACP Corpus. To optimize the hyperparameters, we used the dev set of the AC",
        "type": "Document"
      },
      {
        "id": "3206227a-b07b-4d69-913b-317680146a4c",
        "metadata": {
          "chunk_id": 30,
          "document_id": "1909.00694",
          "start_idx": 9921,
          "end_idx": 10268
        },
        "page_content": " the ACP Corpus. To optimize the hyperparameters, we used the dev set of the ACP Corpus. For the evaluation, we used the test set of the ACP Corpus. The model output was classified as positive if $p(x) > 0$ and negative if $p(x) \\le 0$.  As for ${\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. GRU BIBREF16 is a recurrent",
        "type": "Document"
      },
      {
        "id": "149c5bc3-0411-4eca-8082-d07c0b25a029",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1909.00694",
          "start_idx": 5168,
          "end_idx": 5658
        },
        "page_content": " As shown in Figure FIGREF1, we limit our scope to two discourse relations: Cause and Concession. The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is seed lexicon used for training?",
    "llm_answer": "15 positive words and 15 negative words\n",
    "context": [
      {
        "id": "fb85c318-f6d9-48b7-982a-9106ee5d2aec",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1909.00694",
          "start_idx": 15395,
          "end_idx": 15766
        },
        "page_content": " segmented into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam BIBREF19. The mini-batch size was 32. We ran 1 epoch.",
        "type": "Document"
      },
      {
        "id": "a91598c8-9447-48fe-97fb-f0d1ef3c541f",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1909.00694",
          "start_idx": 11244,
          "end_idx": 11751
        },
        "page_content": "or the reverse of it for negation) if the event's predicate is in the seed lexicon. We can see that the seed lexicon itself had practically no impact on prediction. The models in the top block performed considerably better than the random baselines. The performance gaps with their (semi-)supervised counterparts, shown in the middle block, were less than 7%. This demonstrates the effectiveness of discourse relation-based label propagation. Comparing the model variants, we obtained the highest score with",
        "type": "Document"
      },
      {
        "id": "74f13364-433c-497c-b3bc-ac6efb7e2752",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1909.00694",
          "start_idx": 15131,
          "end_idx": 15574
        },
        "page_content": " vocabulary size was 100,000. The number of hidden layers was 2. The dimension of hidden units was 256. The optimizer was Momentum SGD BIBREF21. The mini-batch size was 1024. We ran 100 epochs and selected the snapshot that achieved the highest score for the dev set. We used a Japanese BERT model pretrained with Japanese Wikipedia. The input sentences were segmented into words by Juman++, and words were broken into subwords by applying BPE",
        "type": "Document"
      },
      {
        "id": "ae6fb495-a6c0-4084-bfd9-8501e84ef267",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1909.00694",
          "start_idx": 8296,
          "end_idx": 8611
        },
        "page_content": "] made a serious mistake, [I] got fired. From this sentence, we extracted the event pair of \u201c\u91cd\u5927\u306a\u5931\u6557\u3092\u72af\u3059\u201d ([I] make a serious mistake) and \u201c\u4ed5\u4e8b\u3092\u30af\u30d3\u306b\u306a\u308b\u201d ([I] get fired), and tagged it with Cause. We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the",
        "type": "Document"
      },
      {
        "id": "42ca48e4-a843-4062-b3e4-7c2bbbff0aef",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1909.00694",
          "start_idx": 12027,
          "end_idx": 12460
        },
        "page_content": " models (ACP) outperformed semi-supervised models (ACP+AL+CA+CO). This suggests that the training set of 0.6 million events is sufficiently large for training the models. For comparison, we trained the models with a subset (6,000 events) of the ACP dataset. As the results shown in Table TABREF24 demonstrate, our method is effective when labeled data are small. The result of hyperparameter optimization for the BiGRU encoder was as",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do the various social phenomena examined manifest in different types of communities?",
    "llm_answer": "Radically contrasting engagement patterns emerge in different parts of the multi-community space,  with, for example, established members in niche communities more likely to engage with volatile content than outsiders, while the opposite is true in generic communities.\n",
    "context": [
      {
        "id": "f1025181-1ead-46f6-81d4-8292a4834194",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1705.09665",
          "start_idx": 4847,
          "end_idx": 5438
        },
        "page_content": " stay). Such insights into the relation between collective identity and user engagement can be informative to community maintainers seeking to better understand growth patterns within their online communities. More generally, our methodology stands as an example of how sociological questions can be addressed in a multi-community setting. In performing our analyses across a rich variety of communities, we reveal both the diversity of phenomena that can occur, as well as the systematic nature of this diversity. A community's identity derives from its members' common interests and shared",
        "type": "Document"
      },
      {
        "id": "fbb2f8e3-5192-467d-9acb-8983eeb494ff",
        "metadata": {
          "chunk_id": 83,
          "document_id": "1705.09665",
          "start_idx": 34021,
          "end_idx": 34670
        },
        "page_content": " systematically analyze how the psychological manifestations and implications of collective identity vary across diverse sets of communities. Our current understanding of engagement patterns in online communities is patched up from glimpses offered by several disparate studies focusing on a few individual communities. This work calls into attention the need for a method to systematically reason about similarities and differences across communities. By proposing a way to structure the multi-community space, we find not only that radically contrasting engagement patterns emerge in different parts of this space, but also that this variation can",
        "type": "Document"
      },
      {
        "id": "95069b50-0c8c-4b7f-9036-2c910b3e878a",
        "metadata": {
          "chunk_id": 85,
          "document_id": "1705.09665",
          "start_idx": 34950,
          "end_idx": 35527
        },
        "page_content": "\u2014retention, acculturation and content affinity\u2014and complements measures based on activity and size with additional interpretable information. For example, we find that in niche communities established members are more likely to engage with volatile content than outsiders, while the opposite is true in generic communities. Such insights can be useful for community maintainers seeking to understand engagement patterns in their own communities. One main area of future research is to examine the temporal dynamics in the multi-community landscape. By averaging our measures of",
        "type": "Document"
      },
      {
        "id": "6b346527-1153-40e4-a39d-88bf9adaa8b4",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1705.09665",
          "start_idx": 6067,
          "end_idx": 6609
        },
        "page_content": ", we first examine a few example communities and draw attention to some key social dynamics that occur within them. We consider four communities from Reddit: in Seahawks, fans of the Seahawks football team gather to discuss games and players; in BabyBumps, expecting mothers trade advice and updates on their pregnancy; Cooking consists of recipe ideas and general discussion about cooking; while in pics, users share various images of random things (like eels and hornets). We note that these communities are topically contrasting and foster",
        "type": "Document"
      },
      {
        "id": "42c2eae8-25f1-44aa-925d-13cfed3a64a1",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1705.09665",
          "start_idx": 5729,
          "end_idx": 6305
        },
        "page_content": " communities along these two dimensions. We start by providing an intuition through inspecting a few example communities. We then introduce a generalizable language-based methodology and use it to map a large set of Reddit communities onto the landscape defined by our typology of community identity. In order to illustrate the diversity within the multi-community space, and to provide an intuition for the underlying structure captured by the proposed typology, we first examine a few example communities and draw attention to some key social dynamics that occur within them",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How did the select the 300 Reddit communities for comparison?",
    "llm_answer": "The text states that almost 300 Reddit communities were mapped onto a typology, but does not describe the selection process.\n",
    "context": [
      {
        "id": "d2fba1ba-ddd8-45f8-8b85-e0ce99672509",
        "metadata": {
          "chunk_id": 30,
          "document_id": "1705.09665",
          "start_idx": 12307,
          "end_idx": 12917
        },
        "page_content": " comment sections. The website contains many highly active subreddits with thousands of active subscribers. These communities span an extremely rich variety of topical interests, as represented by the examples described earlier. They also vary along a rich multitude of structural dimensions, such as the number of users, the amount of conversation and social interaction, and the social norms determining which types of content become popular. The diversity and scope of Reddit's multicommunity ecosystem make it an ideal landscape in which to closely examine the relation between varying community identities",
        "type": "Document"
      },
      {
        "id": "31f18e26-8a09-4268-b5d5-89d118703f8f",
        "metadata": {
          "chunk_id": 76,
          "document_id": "1705.09665",
          "start_idx": 30890,
          "end_idx": 31419
        },
        "page_content": " follows prior literature which has examined differences in user and community dynamics across various online groups, such as email listservs. Such studies have primarily related variations in user behaviour to structural features such as group size and volume of content BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 . In focusing on the linguistic content of communities, we extend this research by providing a content-based framework through which user engagement can be examined. Reddit has been a particularly useful setting for",
        "type": "Document"
      },
      {
        "id": "0f9abe23-0536-42ac-a849-39df7a6d80f2",
        "metadata": {
          "chunk_id": 77,
          "document_id": "1705.09665",
          "start_idx": 31419,
          "end_idx": 31975
        },
        "page_content": " content-based framework through which user engagement can be examined. Reddit has been a particularly useful setting for studying multiple communities in prior work. Such studies have mostly focused on characterizing how individual users engage across a multi-community platform BIBREF34 , BIBREF35 , or on specific user engagement patterns such as loyalty to particular communities BIBREF22 . We complement these studies by seeking to understand how features of communities can mediate a broad array of user engagement patterns within them. Typologies of",
        "type": "Document"
      },
      {
        "id": "c3bdd638-0fb1-4c3f-9401-0f37bcbb3c93",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1705.09665",
          "start_idx": 3016,
          "end_idx": 3592
        },
        "page_content": " is also convenient since it renders our framework immediately applicable to a wide variety of online communities, where communication is primarily recorded in a textual format. Using our framework, we map almost 300 Reddit communities onto the landscape defined by the two axes of our typology (Section SECREF2 ). We find that this mapping induces conceptually sound categorizations that effectively capture key aspects of community-level social dynamics. In particular, we quantitatively validate the effectiveness of our mapping by showing that our two-dimensional typology",
        "type": "Document"
      },
      {
        "id": "61240946-73ce-49ae-9ca0-8b0bf3477ade",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1705.09665",
          "start_idx": 15656,
          "end_idx": 16151
        },
        "page_content": ", using only the top 5th percentile of words, by frequency within each INLINEFORM0 , to score comments and communities. Typology output on Reddit. The distribution of INLINEFORM0 and INLINEFORM1 across Reddit communities is shown in Figure FIGREF3 .B, along with examples of communities at the extremes of our typology. We find that interpretable groupings of communities emerge at various points within our axes. For instance, highly distinctive and dynamic communities tend to focus on rapidly",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the clinical text structuring task defined?",
    "llm_answer": "The QA-CTS task aims to discover the most related text from the input clinical text, unifying different specific tasks and making datasets shareable.\n",
    "context": [
      {
        "id": "c500de5c-853f-4342-b8da-b9e9582574ca",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1908.06606",
          "start_idx": 3526,
          "end_idx": 4037
        },
        "page_content": " work on clinical text structuring in Section SECREF2. Then, we present question answer based clinical text structuring task in Section SECREF3. In Section SECREF4, we present an effective model for this task. Section SECREF5 is devoted to computational studies and several investigations on the key issues of our proposed model. Finally, conclusions are given in Section SECREF6. Clinical text structuring is a final problem which is highly related to practical applications. Most of existing studies are case-",
        "type": "Document"
      },
      {
        "id": "a87029c8-b4fd-4303-8d01-5486dfd1198d",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1908.06606",
          "start_idx": 0,
          "end_idx": 498
        },
        "page_content": "Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained. It is important to extract structured data from clinical text because bio-medical systems or bio-medical researches greatly rely on",
        "type": "Document"
      },
      {
        "id": "e2030028-7042-419c-afea-1c94751ee4f8",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1908.06606",
          "start_idx": 3003,
          "end_idx": 3526
        },
        "page_content": " information into pre-trained language model. Experimental results show that QA-CTS task leads to significant improvement due to shared dataset. Our proposed model also achieves significantly better performance than the strong baseline methods. In addition, we also show that two-stage training mechanism has a great improvement on QA-CTS task. The rest of the paper is organized as follows. We briefly review the related work on clinical text structuring in Section SECREF2. Then, we present question answer based clinical",
        "type": "Document"
      },
      {
        "id": "79b8b8d4-9f72-4b2c-84ef-8a1a1647ccde",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1908.06606",
          "start_idx": 2693,
          "end_idx": 3201
        },
        "page_content": "ifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data. The main contributions of this work can be summarized as follows. We first present a question answering based clinical text structuring (QA-CTS) task, which unifies different specific tasks and make dataset shareable. We also propose an effective model to integrate clinical named entity information into pre-trained language model. Experimental results show that QA-CTS task leads to",
        "type": "Document"
      },
      {
        "id": "900e6464-dee9-43cc-9339-9125f9356fa3",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1908.06606",
          "start_idx": 1954,
          "end_idx": 2429
        },
        "page_content": " break down the entire process into several pieces which improves the performance and generality. However, when the pipeline depth grows, error propagation will have a greater impact on the performance. To reduce the pipeline depth and break the barrier of non-uniform output formats, we present a question answering based clinical text structuring (QA-CTS) task (see Fig. FIGREF1). Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is all text in this dataset a question, or are there unrelated sentences in between questions?",
    "llm_answer": "The context states that the dataset is a collection of affective events, but does not specify if all sentences are questions.  The answer is not found in the context.\n",
    "context": [
      {
        "id": "2df4c5da-dfcf-47cc-997e-1a8b2e8ea953",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1909.00694",
          "start_idx": 9330,
          "end_idx": 9716
        },
        "page_content": " positive and negative, respectively: . \u4f5c\u696d\u304c\u697d\u3060\u3002 The work is easy. . \u99d0\u8eca\u5834\u304c\u306a\u3044\u3002 There is no parking lot. Although the ACP corpus was originally constructed in the context of sentiment analysis, we found that it could roughly be regarded as a collection of affective events. We parsed each sentence and extracted the last clause in it. The train/dev/test split of the data is shown in Table T",
        "type": "Document"
      },
      {
        "id": "97728166-8797-409c-b889-6c8c4f3d0c13",
        "metadata": {
          "chunk_id": 30,
          "document_id": "1908.06606",
          "start_idx": 10621,
          "end_idx": 11075
        },
        "page_content": " both text and query. Afterwards, $V_s$ and $I_n$ are integrated together and fed into a feed forward network to calculate the start and end index of answer-related text. Here we define this calculation problem as a classification for each word to be the start or end word. For any clinical free-text paragraph $X$ and query $Q$, contextualized representation is to generate the encoded vector of both of them. Here we use pre-trained language model BERT",
        "type": "Document"
      },
      {
        "id": "76f81e16-451d-4de3-aeb7-c4518c71d023",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1908.06606",
          "start_idx": 11284,
          "end_idx": 11795
        },
        "page_content": "i$. To tell the model $Q$ and $X$ is two different sentence, a sentence type input is generated which is a binary label sequence to denote what sentence each character in the input belongs to. Positional encoding and mask matrix is also constructed automatically to bring in absolute position information and eliminate the impact of zero padding respectively. Then a hidden vector $V_s$ which contains both query and text information is generated through BERT-base model. Since BERT is trained on general corpus",
        "type": "Document"
      },
      {
        "id": "7ce42ed1-619f-4f52-80cb-ed9bdc45d15a",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1908.06606",
          "start_idx": 10348,
          "end_idx": 10728
        },
        "page_content": ", Single) tag scheme. $I_{nq}$ and $I_{nt}$ are then integrated together into $I_n$. Meanwhile, the paragraph text $X$ and query text $Q$ are organized and passed to contextualized representation model which is pre-trained language model BERT BIBREF26 here to obtain the contextualized representation vector $V_s$ of both text and query. Afterwards, $V_s$ and $I_n$ are integrated",
        "type": "Document"
      },
      {
        "id": "900e6464-dee9-43cc-9339-9125f9356fa3",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1908.06606",
          "start_idx": 1954,
          "end_idx": 2429
        },
        "page_content": " break down the entire process into several pieces which improves the performance and generality. However, when the pipeline depth grows, error propagation will have a greater impact on the performance. To reduce the pipeline depth and break the barrier of non-uniform output formats, we present a question answering based clinical text structuring (QA-CTS) task (see Fig. FIGREF1). Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many questions are in the dataset?",
    "llm_answer": "2,714\n",
    "context": [
      {
        "id": "7cff3727-f562-4a92-8a1b-ef14284a3066",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1908.06606",
          "start_idx": 15529,
          "end_idx": 15988
        },
        "page_content": " Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences.",
        "type": "Document"
      },
      {
        "id": "cf348585-ca79-493f-be9d-ce678544cd42",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1909.00694",
          "start_idx": 8898,
          "end_idx": 9330
        },
        "page_content": " CO such that it was five times larger than AL. The results are shown in Table TABREF16. We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively: . \u4f5c\u696d\u304c\u697d\u3060\u3002 The work",
        "type": "Document"
      },
      {
        "id": "d14d3b6d-7650-4c41-a57f-cb1b97f487ac",
        "metadata": {
          "chunk_id": 68,
          "document_id": "1908.06606",
          "start_idx": 23837,
          "end_idx": 24326
        },
        "page_content": " 60 or 70 percent. This proves that there is commonality between different tasks and our proposed QA-CTS task make this learnable. In conclusion, to achieve the best performance for a specific dataset, pre-training the model in multiple datasets and then fine tuning the model on the specific dataset is the best way. In this paper, we present a question answering based clinical text structuring (QA-CTS) task, which unifies different clinical text structuring tasks and utilize different",
        "type": "Document"
      },
      {
        "id": "045e9ffc-5f6d-4f44-b97a-9663e6b99e7d",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1705.09665",
          "start_idx": 12917,
          "end_idx": 13479
        },
        "page_content": " multicommunity ecosystem make it an ideal landscape in which to closely examine the relation between varying community identities and social dynamics. Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk",
        "type": "Document"
      },
      {
        "id": "e929c104-c4ce-40d5-aca7-762538286335",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1908.06606",
          "start_idx": 15988,
          "end_idx": 16383
        },
        "page_content": ") and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What aspects have been compared between various language models?",
    "llm_answer": "Model quality (e.g., lower perplexity) and performance (e.g., energy consumption, query latency).\n",
    "context": [
      {
        "id": "e8c072a1-f15c-41d6-ae92-606bb2ea0825",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1811.00942",
          "start_idx": 0,
          "end_idx": 514
        },
        "page_content": "Deep learning has unquestionably advanced the state of the art in many natural language processing tasks, from syntactic dependency parsing BIBREF0 to named-entity recognition BIBREF1 to machine translation BIBREF2 . The same certainly applies to language modeling, where recent advances in neural language models (NLMs) have led to dramatically better approaches as measured using standard metrics such as perplexity BIBREF3 , BIBREF4 . Specifically focused on language modeling, this paper examines an issue that",
        "type": "Document"
      },
      {
        "id": "1ed14e47-0282-475e-9d65-ac5d30b24d2f",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1908.06606",
          "start_idx": 6760,
          "end_idx": 7192
        },
        "page_content": " BIBREF26, BIBREF27 which makes language model a shared model to all natural language processing tasks. Radford et al. BIBREF24 proposed a framework for fine-tuning pre-trained language model. Peters et al. BIBREF25 proposed ELMo which concatenates forward and backward language models in a shallow manner. Devlin et al. BIBREF26 used bidirectional Transformers to model deep interactions between the two directions. Yang et al. BIB",
        "type": "Document"
      },
      {
        "id": "69bbf08f-3812-45ba-aa24-4ed688fa8b89",
        "metadata": {
          "chunk_id": 1,
          "document_id": "1811.00942",
          "start_idx": 514,
          "end_idx": 1077
        },
        "page_content": "REF3 , BIBREF4 . Specifically focused on language modeling, this paper examines an issue that to our knowledge has not been explored: advances in neural language models have come at a significant cost in terms of increased computational complexity. Computing the probability of a token sequence using non-neural techniques requires a number of phrase lookups and perhaps a few arithmetic operations, whereas model inference with NLMs require large matrix multiplications consuming perhaps millions of floating point operations (FLOPs). These performance tradeoffs",
        "type": "Document"
      },
      {
        "id": "7d2fc044-04ff-4c60-85ae-87afe6afe5f2",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1811.00942",
          "start_idx": 862,
          "end_idx": 1397
        },
        "page_content": " large matrix multiplications consuming perhaps millions of floating point operations (FLOPs). These performance tradeoffs are worth discussing. In truth, language models exist in a quality\u2013performance tradeoff space. As model quality increases (e.g., lower perplexity), performance as measured in terms of energy consumption, query latency, etc. tends to decrease. For applications primarily running in the cloud\u2014say, machine translation\u2014practitioners often solely optimize for the lowest perplexity. This is because such applications",
        "type": "Document"
      },
      {
        "id": "3107a076-5d8c-4b79-a367-2ef276fa07bd",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1705.09665",
          "start_idx": 2493,
          "end_idx": 3016
        },
        "page_content": " and at scale is that the language used within a community can simultaneously capture how distinctive and dynamic its interests are. This language-based approach draws on a wealth of literature characterizing linguistic variation in online communities and its relationship to community and user identity BIBREF16 , BIBREF5 , BIBREF17 , BIBREF18 , BIBREF19 . Basing our typology on language is also convenient since it renders our framework immediately applicable to a wide variety of online communities, where communication",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many attention layers are there in their model?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "58496f8f-e7cb-4e30-af08-0eacd94a4cd4",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1907.05664",
          "start_idx": 10451,
          "end_idx": 10985
        },
        "page_content": " low. What seems to happen in this application is that most of the information used is transmitted from the encoder to the decoder and the attention mechanism at each decoding step just changes marginally how it is used. Quantifying the difference between attention distribution and saliency map across multiple tasks is a possible future work. The second observation we can make is that the saliency map doesn't seem to highlight the right things in the input for the summary it generates. The saliency maps on Figure 3 correspond to",
        "type": "Document"
      },
      {
        "id": "27a7978c-b93c-4960-8b35-a38155a06e68",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1907.05664",
          "start_idx": 9953,
          "end_idx": 10451
        },
        "page_content": " in an experiment the relevance propagated through the attention mechanism to the encoder and didn't observe much changes in the saliency map. It can be seen as evidence that using the attention distribution as an \u201cexplanation\" of the prediction can be misleading. It is not the only information received by the decoder and the importance it \u201callocates\" to this attention state might be very low. What seems to happen in this application is that most of the information used is transmitted from the",
        "type": "Document"
      },
      {
        "id": "d7cdf7ff-77ae-4abe-9c9f-3eb00b7ca483",
        "metadata": {
          "chunk_id": 40,
          "document_id": "1907.05664",
          "start_idx": 14847,
          "end_idx": 15378
        },
        "page_content": " mechanism. We observed a peculiar behaviour of the saliency maps for the words in the output summary: they are almost all identical and seem uncorrelated with the attention distribution. We then proceeded to validate our attributions by averaging the absolute value of the relevance across the saliency maps. We obtain a ranking of the word from the most important to the least important and proceeded to delete one or another. We showed that in some cases the saliency maps are truthful to the network's computation, meaning that",
        "type": "Document"
      },
      {
        "id": "7325ad89-62b9-42cc-92b2-cdb715a7a9c4",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1907.05664",
          "start_idx": 8019,
          "end_idx": 8498
        },
        "page_content": " we propagate relevance through an element-wise multiplication operation, we give all the upper-layer's relevance to the \u201cinformation\" vector and none to the \u201cgate\" vector. We use the same method to transmit relevance through the attention mechanism back to the encoder because Bahdanau's attention BIBREF9 uses element-wise multiplications as well. We depict in Figure 2 the transmission end-to-end from the output layer to the input through the decoder, attention mechanism and",
        "type": "Document"
      },
      {
        "id": "858c4c70-af74-4b03-becd-ca1ea2c47b78",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1811.00942",
          "start_idx": 8253,
          "end_idx": 8645
        },
        "page_content": " specific hyperparameters are the same as those in the repository. Our model architecture consists of 400-dimensional tied embedding weights BIBREF18 and four QRNN layers, with 1550 hidden units per layer on PTB and 2500 per layer on WT103. Both QRNN models have window sizes of $r=2$ for the first layer and $r=1$ for the rest. For the KN-5 model, we trained an off-the-shelf five-gram model",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the three measures of bias which are reduced in experiments?",
    "llm_answer": "Geometric bias mitigation, WEAT statistic, and a probabilistic approach.\n",
    "context": [
      {
        "id": "c210a1cd-5bd6-4891-ba2e-70ed3fed21cc",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1910.14497",
          "start_idx": 805,
          "end_idx": 1304
        },
        "page_content": ") and subtracting out the difference between the resulting distances BIBREF0. On the other hand, the most popular metric for measuring bias is the WEAT statistic BIBREF1, which compares the cosine similarities between groups of words. However, WEAT has been recently shown to overestimate bias as a result of implicitly relying on similar frequencies for the target words BIBREF4, and BIBREF5 demonstrated that evidence of bias can still be recovered after geometric bias mitigation by examining the",
        "type": "Document"
      },
      {
        "id": "46775eea-f967-48bb-aea3-6e34fc9dbf5f",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1910.14497",
          "start_idx": 4762,
          "end_idx": 5257
        },
        "page_content": "\ufffds bias is measured as the ratio of its neighborhood of socially-biased male and socially-biased female words, so that a value of 0.5 in this metric would indicate a perfectly unbiased word, and values closer to 0 and 1 indicate stronger bias. Our objective here is to extend and complement the geometric notions of word embedding bias described in the previous section with an alternative, probabilistic, approach. Intuitively, we seek a notion of equality akin to that of demographic parity in",
        "type": "Document"
      },
      {
        "id": "aac96e8d-7c08-40ff-9afe-9c513f0765fe",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1910.14497",
          "start_idx": 15033,
          "end_idx": 15361
        },
        "page_content": "coon', 'violinist', 'vocalist', 'waiter', 'waitress', 'warden', 'warrior', 'worker', 'wrestler', 'writer' Words used for WEAT statistic, consisting of baseline bias tests and gender bias tests in the format X vs Y / A vs B Flowers vs Insects / Pleasant vs Unpleasant X: \"aster\", \"clover\", \"hyacinth\", \"marigold\", \"poppy\", \"azale",
        "type": "Document"
      },
      {
        "id": "673cd983-0eb1-455c-92ec-0499c155b596",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1910.14497",
          "start_idx": 2099,
          "end_idx": 2675
        },
        "page_content": " bias has also been approached (albeit in different contexts) in the fairness literature, where various intuitive notions of equity such as equalized odds have been formalized through probabilistic criteria. By considering analogous criteria for the word embedding setting, we seek to draw connections between these two bodies of work. We present experiments on various bias mitigation benchmarks and show that our framework is comparable to state-of-the-art alternatives according to measures of geometric bias mitigation and that it performs far better according to measures",
        "type": "Document"
      },
      {
        "id": "9b060bb7-35f3-4bfc-a58b-0390290bcb85",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1910.14497",
          "start_idx": 1623,
          "end_idx": 2099
        },
        "page_content": "abilistic at their core \u2013 i.e., they are trained (explicitly or implicitly BIBREF6) to minimize some form of word co-occurrence probabilities. Thus, we argue that a framework for measuring and treating bias in these embeddings should take into account, in addition to their geometric aspect, their probabilistic nature too. On the other hand, the issue of bias has also been approached (albeit in different contexts) in the fairness literature, where various intuitive notions",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is the dataset?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "cf348585-ca79-493f-be9d-ce678544cd42",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1909.00694",
          "start_idx": 8898,
          "end_idx": 9330
        },
        "page_content": " CO such that it was five times larger than AL. The results are shown in Table TABREF16. We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively: . \u4f5c\u696d\u304c\u697d\u3060\u3002 The work",
        "type": "Document"
      },
      {
        "id": "045e9ffc-5f6d-4f44-b97a-9663e6b99e7d",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1705.09665",
          "start_idx": 12917,
          "end_idx": 13479
        },
        "page_content": " multicommunity ecosystem make it an ideal landscape in which to closely examine the relation between varying community identities and social dynamics. Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk",
        "type": "Document"
      },
      {
        "id": "10a62407-61bd-4026-88e1-f9f860be4a2f",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1908.06606",
          "start_idx": 15209,
          "end_idx": 15723
        },
        "page_content": " to achieve a better contextualized representation performance. Then we deploy the proposed model and load the fine tuned BERT weights, attach named entity information layers and retrain the model. In this section, we devote to experimentally evaluating our proposed task and approach. The best results in tables are in bold. Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,",
        "type": "Document"
      },
      {
        "id": "fb85c318-f6d9-48b7-982a-9106ee5d2aec",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1909.00694",
          "start_idx": 15395,
          "end_idx": 15766
        },
        "page_content": " segmented into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam BIBREF19. The mini-batch size was 32. We ran 1 epoch.",
        "type": "Document"
      },
      {
        "id": "263505e0-cf0a-4942-bff5-9058099af378",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1907.05664",
          "start_idx": 3794,
          "end_idx": 4271
        },
        "page_content": " articles coupled with summaries of about three sentences. These summaries are in fact \u201chighlights\" of the articles provided by the media themselves. Articles have an average length of 780 words and the summaries of 50 words. We had 287 000 training pairs and 11 500 test pairs. Similarly to See et al. See2017, we limit during training and prediction the input text to 400 words and generate summaries of 200 words. We pad the shorter texts using an UNKNOWN token and truncate",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the intensity of the PTSD established?",
    "llm_answer": "LAXARY calculates possible scores for each survey question using a PTSD Linguistic dictionary created from twitter-based sentiments.  The dictionary includes words/aspects representing instantaneous perturbations.  The model provides extremely accurate measures of intensity estimation.\n",
    "context": [
      {
        "id": "62c29d9d-285a-4c74-84fe-36b385a9ed75",
        "metadata": {
          "chunk_id": 74,
          "document_id": "2003.07433",
          "start_idx": 29104,
          "end_idx": 29632
        },
        "page_content": " to detect Post Traumatic Stress Disorder (PTSD) sufferers in time before going out of control that may result catastrophic impacts on society, people around or even sufferers themselves. Although, psychiatrists invented several clinical diagnosis tools (i.e., surveys) by assessing symptoms, signs and impairment associated with PTSD, most of the times, the process of diagnosis happens at the severe stage of illness which may have already caused some irreversible damages of mental health of the sufferers. On the other hand,",
        "type": "Document"
      },
      {
        "id": "e89401f1-5a8b-48ed-a1a3-33c0d64cdf67",
        "metadata": {
          "chunk_id": 6,
          "document_id": "2003.07433",
          "start_idx": 2592,
          "end_idx": 3130
        },
        "page_content": " intensity estimation of PTSD? In this paper, we propose LAXARY, an explainable and trustworthy representation of PTSD classification and its intensity for clinicians. The key contributions of our work are summarized below, The novelty of LAXARY lies on the proposed clinical surveys-based PTSD Linguistic dictionary creation with words/aspects which represents the instantaneous perturbation of twitter-based sentiments as a specific pattern and help calculate the possible scores of each survey question. LAXARY includes a modified LIWC",
        "type": "Document"
      },
      {
        "id": "f0803a65-a9c2-4f49-b85d-64f66b755295",
        "metadata": {
          "chunk_id": 9,
          "document_id": "2003.07433",
          "start_idx": 3968,
          "end_idx": 4431
        },
        "page_content": " ($\\approx 96\\%$) and its intensity ($\\approx 1.2$ mean squared error). Fig. FIGREF7 shows a schematic representation of our proposed model. It consists of the following logical steps: (i) Develop PTSD Detection System using twitter posts of war-veterans(ii) design real surveys from the popular symptoms based mental disease assessment surveys; (iii) define single category and create PTSD Linguistic Dictionary for each survey question and multiple aspect/words",
        "type": "Document"
      },
      {
        "id": "0a08b929-929b-49d5-afff-1c0a4fe7df87",
        "metadata": {
          "chunk_id": 67,
          "document_id": "2003.07433",
          "start_idx": 26161,
          "end_idx": 26644
        },
        "page_content": " training samples (%), LAXARY models outperforms Coppersmith et. al. proposed model under any condition. In terms of intensity, Coppersmith et. al. totally fails to provide any idea however LAXARY provides extremely accurate measures of intensity estimation for PTSD sufferers (as shown in Fig FIGREF31) which can be explained simply providing LAXARY model filled out survey details. Table TABREF29 shows the details of accuracies of both PTSD detection and intensity estimation. Fig",
        "type": "Document"
      },
      {
        "id": "bb29ff85-3926-4b50-a0c9-1bf667eab507",
        "metadata": {
          "chunk_id": 64,
          "document_id": "2003.07433",
          "start_idx": 24931,
          "end_idx": 25419
        },
        "page_content": " rest of 50% dataset. The distribution of this training-test dataset segmentation followed a 50% distribution of PTSD and No PTSD from the original dataset. Our final survey based classification results showed an accuracy of 96% in detecting PTSD and mean squared error of 1.2 in estimating its intensity given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively. Table TABREF29 shows the classification details",
        "type": "Document"
      }
    ]
  },
  {
    "query": "how is quality measured?",
    "llm_answer": "EM-score (Exact Match) and (macro-averaged) F$_1$-score.  Reliability is also measured using Cronbach's alpha.\n",
    "context": [
      {
        "id": "e929c104-c4ce-40d5-aca7-762538286335",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1908.06606",
          "start_idx": 15988,
          "end_idx": 16383
        },
        "page_content": ") and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage",
        "type": "Document"
      },
      {
        "id": "6c10647f-e85b-46c8-830d-c455fa25babe",
        "metadata": {
          "chunk_id": 51,
          "document_id": "2003.07433",
          "start_idx": 20049,
          "end_idx": 20591
        },
        "page_content": " each of the category words within texts. After the PTSD Linguistic Dictionary has been created, we empirically evaluate its psychometric properties such as reliability and validity as per American Standards for educational and psychological testing guideline BIBREF24. In psychometrics, reliability is most commonly evaluated by Cronbach's alpha, which assesses internal consistency based on inter-correlations and the number of measured items. In the text analysis scenario, each word in our PTSD Linguistic dictionary is considered an item",
        "type": "Document"
      },
      {
        "id": "c7f90d9c-a5fe-449a-93d2-1f9bf8ddb178",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1907.05664",
          "start_idx": 12389,
          "end_idx": 12938
        },
        "page_content": " scaled down by a constant factor. The absolute value average seemed to deliver the best results. We delete incrementally the important words (words with the highest average) in the input and compared it to the control experiment that consists of deleting the least important word and compare the degradation of the resulting summaries. We obtain mitigated results: for some texts, we observe a quick degradation when deleting important words which are not observed when deleting unimportant words (see Figure 4 ), but for other test examples we don",
        "type": "Document"
      },
      {
        "id": "867c33b8-e8f2-45cb-9055-d7d8316c7b62",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1907.05664",
          "start_idx": 11265,
          "end_idx": 11791
        },
        "page_content": " the input features. We will call that truthfulness of the attribution in regard to the computation, meaning that an attribution is truthful in regard to the computation if it actually highlights the important input features that the network attended to during prediction. We proceed to measure the truthfulness of the attributions by validating them quantitatively. We propose to validate the saliency maps in a similar way as Arras et al. Arras2017 by incrementally deleting \u201cimportant\" words from the input text and observe",
        "type": "Document"
      },
      {
        "id": "646136aa-5187-4700-a298-f85bf4a02438",
        "metadata": {
          "chunk_id": 52,
          "document_id": "2003.07433",
          "start_idx": 20394,
          "end_idx": 20829
        },
        "page_content": ". In the text analysis scenario, each word in our PTSD Linguistic dictionary is considered an item, and reliability is calculated based on each text file's response to each word item, which forms an $N$(number of text files) $\\times $ $J$(number of words or stems in a dictionary) data matrix. There are two ways to quantify such responses: using percentage data (uncorrected method), or using \"present or not\" data (binary method) BIB",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the accuracy reported by state-of-the-art methods?",
    "llm_answer": "95.4%\n",
    "context": [
      {
        "id": "6cf0a06f-34c6-44b2-a329-2285cf3a8bc8",
        "metadata": {
          "chunk_id": 57,
          "document_id": "1910.04269",
          "start_idx": 21961,
          "end_idx": 22224
        },
        "page_content": " which is a recently developed technique to prevent over\ufb01tting on test data.This approach achieved an accuracy of 95.4%. We also analysed how attention mechanism and recurrent layers impact the performance of networks. This approach achieved an accuracy of 95.0%.",
        "type": "Document"
      },
      {
        "id": "02584c59-698c-44a3-97f8-0bc39c92b240",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1908.06606",
          "start_idx": 16204,
          "end_idx": 16661
        },
        "page_content": " are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage of predictions that match any one of the ground truth answers exactly. The F$_1$-score metric is a looser metric measures the average overlap between the prediction and ground truth answer. To implement deep neural network models, we utilize the Keras library BIBREF36 with TensorFlow BIBREF37 backend. Each model is run on a single NVIDIA GeForce GTX 1080 Ti GPU.",
        "type": "Document"
      },
      {
        "id": "38e1829d-9b8d-40ea-b7cc-76feafbb4188",
        "metadata": {
          "chunk_id": 8,
          "document_id": "2003.07433",
          "start_idx": 3481,
          "end_idx": 3968
        },
        "page_content": " surveys, but also obtain trusts of clinicians who are expected to see traditional survey results of the PTSD assessment. Finally, we evaluate the accuracy of LAXARY model performance and reliability-validity of generated PTSD Linguistic Dictionary using real twitter users' posts. Our results show that, given normal weekly messages posted in twitter, LAXARY can provide very high accuracy in filling up surveys towards identifying PTSD ($\\approx 96\\%$) and its intensity ($\\approx 1.2$",
        "type": "Document"
      },
      {
        "id": "e929c104-c4ce-40d5-aca7-762538286335",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1908.06606",
          "start_idx": 15988,
          "end_idx": 16383
        },
        "page_content": ") and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage",
        "type": "Document"
      },
      {
        "id": "867c33b8-e8f2-45cb-9055-d7d8316c7b62",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1907.05664",
          "start_idx": 11265,
          "end_idx": 11791
        },
        "page_content": " the input features. We will call that truthfulness of the attribution in regard to the computation, meaning that an attribution is truthful in regard to the computation if it actually highlights the important input features that the network attended to during prediction. We proceed to measure the truthfulness of the attributions by validating them quantitatively. We propose to validate the saliency maps in a similar way as Arras et al. Arras2017 by incrementally deleting \u201cimportant\" words from the input text and observe",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do the authors define or exemplify 'incorrect words'?",
    "llm_answer": "The context exemplifies \"incorrect words\" as missing or incorrectly spelled words in sentences, and sentences with poor grammar.  It also mentions tweets written in informal language that ignore sentence correctness.\n",
    "context": [
      {
        "id": "2efa6298-95b9-49a6-9fcd-14e4a86bc096",
        "metadata": {
          "chunk_id": 1,
          "document_id": "2001.00137",
          "start_idx": 549,
          "end_idx": 1083
        },
        "page_content": " grammatically correct sentences. Panko BIBREF0 goes as far as claiming that human accuracy regarding research paper writing is none when considering the entire document. This has been aggravated with the advent of internet and social networks, which allowed language and modern communication to be been rapidly transformed BIBREF1, BIBREF2. Take Twitter for instance, where information is expected to be readily communicated in short and concise sentences with little to no regard to correct sentence grammar or word spelling BIBREF3",
        "type": "Document"
      },
      {
        "id": "1e7c9317-2fed-4a38-a6d6-003b9edad312",
        "metadata": {
          "chunk_id": 0,
          "document_id": "2001.00137",
          "start_idx": 0,
          "end_idx": 549
        },
        "page_content": "Understanding a user's intent and sentiment is of utmost importance for current intelligent chatbots to respond appropriately to human requests. However, current systems are not able to perform to their best capacity when presented with incomplete data, meaning sentences with missing or incorrect words. This scenario is likely to happen when one considers human error done in writing. In fact, it is rather naive to assume that users will always type fully grammatically correct sentences. Panko BIBREF0 goes as far as claiming that human accuracy",
        "type": "Document"
      },
      {
        "id": "1efae2e2-37d5-4ef3-a1a3-9ea2b87b2471",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1910.14497",
          "start_idx": 0,
          "end_idx": 500
        },
        "page_content": "Word embeddings, or vector representations of words, are an important component of Natural Language Processing (NLP) models and necessary for many downstream tasks. However, word embeddings, including embeddings commonly deployed for public use, have been shown to exhibit unwanted societal stereotypes and biases, raising concerns about disparate impact on axes of gender, race, ethnicity, and religion BIBREF0, BIBREF1. The impact of this bias has manifested in a range of downstream tasks, ranging",
        "type": "Document"
      },
      {
        "id": "6723e97f-3ffd-462d-9146-e397f3350caf",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1705.09665",
          "start_idx": 14064,
          "end_idx": 14593
        },
        "page_content": ", ignoring successive uses of the same word by the same user. This ensures that our word-level characterizations are not skewed by a small subset of highly active contributors. In our subsequent analyses, we will only look at these measures computed over the nouns used in comments. In principle, our framework can be applied to any choice of vocabulary. However, in the case of Reddit using nouns provides a convenient degree of interpretability. We can easily understand the implication of a community preferentially mentioning",
        "type": "Document"
      },
      {
        "id": "ae2e803a-39d9-49d5-90b0-00859a7195a6",
        "metadata": {
          "chunk_id": 5,
          "document_id": "2001.00137",
          "start_idx": 2222,
          "end_idx": 2741
        },
        "page_content": "REF8. Surprisingly, however, there is currently no research on incomplete text classification in the NLP community. Realizing the need of research in that area, we make it the focus of this paper. In this novel task, the model aims to identify the user's intent or sentiment by analyzing a sentence with missing and/or incorrect words. In the sentiment classification task, the model aims to identify the user's sentiment given a tweet, written in informal language and without regards for sentence correctness. Current",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much do they outperform other models in the sentiment in intent classification tasks?",
    "llm_answer": "6% to 8% in the Twitter Sentiment Classification task.\n",
    "context": [
      {
        "id": "2fb117e9-d662-42e2-a3ec-08b2da6afef7",
        "metadata": {
          "chunk_id": 49,
          "document_id": "2001.00137",
          "start_idx": 18686,
          "end_idx": 19161
        },
        "page_content": " Chatbot Intent Classification Corpus). Experimental results for the Twitter Sentiment Classification task on Kaggle's Sentiment140 Corpus dataset, displayed in Table TABREF37, show that our model has better F1-micros scores, outperforming the baseline models by 6$\\%$ to 8$\\%$. We evaluate our model and baseline models on three versions of the dataset. The first one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80",
        "type": "Document"
      },
      {
        "id": "95364f13-e836-49cd-9aa4-10882f45ef3e",
        "metadata": {
          "chunk_id": 54,
          "document_id": "2001.00137",
          "start_idx": 20406,
          "end_idx": 20871
        },
        "page_content": " BERT performs with approximately 50%. However, Stacked DeBERT is able to improve that to 72%, although to a cost of a small decrease in performance of class 0. A similar situation happens in the remaining two datasets, with improved accuracy in class 0 from 64% to 84% and 60% to 76% respectively. Experimental results for the Intent Classification task on the Chatbot NLU Corpus with STT error can be seen in Table TABREF40. When presented with data containing ST",
        "type": "Document"
      },
      {
        "id": "1f8fbf85-5c54-4475-b348-f41a5cc05ad7",
        "metadata": {
          "chunk_id": 48,
          "document_id": "2001.00137",
          "start_idx": 18254,
          "end_idx": 18686
        },
        "page_content": " time depending on the size of the dataset and train batch size. The stack of multilayer perceptrons are trained for 100 and 1,000 epochs with Adam Optimizer, learning rate of $10^{-3}$, weight decay of $10^{-5}$, MSE loss criterion and batch size the same as BERT (4 for the Twitter Sentiment Corpus and 8 for the Chatbot Intent Classification Corpus). Experimental results for the Twitter Sentiment Classification task on Kaggle's",
        "type": "Document"
      },
      {
        "id": "38871ba1-5834-47e1-bb8d-fd8f6d8a097d",
        "metadata": {
          "chunk_id": 42,
          "document_id": "2001.00137",
          "start_idx": 16295,
          "end_idx": 16781
        },
        "page_content": " complete data and one for each of the TTS-STT combinations (gtts-witai and macsay-witai). Regarding the sentiment classification from incorrect sentences task, the baseline and proposed models are each trained 3 times: original text, corrected text and incorrect with correct texts. The reported F1 scores are the best accuracies obtained from 10 runs. No settable training configurations available in the online platforms. Trained on 3-gram, feature vector size of 768 as to match the",
        "type": "Document"
      },
      {
        "id": "d7c2957d-42c7-4db1-8dd0-65c3931c0eeb",
        "metadata": {
          "chunk_id": 47,
          "document_id": "2001.00137",
          "start_idx": 17991,
          "end_idx": 18416
        },
        "page_content": " GPUs for 3 epochs with Adam Optimizer, learning rate of $2*10^{-5}$, maximum sequence length of 128, and warm up proportion of $0.1$. The train batch size is 4 for the Twitter Sentiment Corpus and 8 for the Chatbot Intent Classification Corpus. Our proposed model is trained in end-to-end manner on 2 Titan X GPUs, with training time depending on the size of the dataset and train batch size. The stack of multilayer percept",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which experiments are perfomed?",
    "llm_answer": "Experiments were performed on the WNC dataset using variations of $RoBERTa_{large}$, $ALBERT_{xxlarge.v2}$, $DistilRoBERTa$ and $BERT$, and also using weighted-average ensembling technique with these models.  Further, experiments are suggested regarding feature extraction and neural network architectures.\n",
    "context": [
      {
        "id": "e929c104-c4ce-40d5-aca7-762538286335",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1908.06606",
          "start_idx": 15988,
          "end_idx": 16383
        },
        "page_content": ") and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage",
        "type": "Document"
      },
      {
        "id": "c160faef-78d8-4ec5-908a-9ff0a05f16de",
        "metadata": {
          "chunk_id": 13,
          "document_id": "2002.06644",
          "start_idx": 5099,
          "end_idx": 5495
        },
        "page_content": " variations of $RoBERTa_{large}$, $ALBERT_{xxlarge.v2}$, $DistilRoBERTa$ and $BERT$ and outline selected combinations in tab:experimental-results. We perform our experiments on the WNC dataset open-sourced by the authors of BIBREF2. It consists of aligned pre and post neutralized sentences made by Wikipedia editors under the neutral point of view. It contains $180k$ biased sentences, and their",
        "type": "Document"
      },
      {
        "id": "577f6feb-1a55-497e-a9ae-b43551ab6a28",
        "metadata": {
          "chunk_id": 54,
          "document_id": "1910.04269",
          "start_idx": 20617,
          "end_idx": 21192
        },
        "page_content": " robust to variations which might be present in real world scenarios. There can be further exploration of various feature extraction techniques like Constant-Q transform and Fast Fourier Transform and assessment of their impact on Language Identification. There can be further improvements in neural network architectures like concatenating the high level features obtained from 1D-ConvNet and 2D-ConvNet, before performing classification. There can be experiments using deeper networks with skip connections and Inception modules. These are known to have positively impacted",
        "type": "Document"
      },
      {
        "id": "e3e45aef-9ffe-4d22-aabb-ccc4b72d9951",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1908.06606",
          "start_idx": 3825,
          "end_idx": 4300
        },
        "page_content": "uring is a final problem which is highly related to practical applications. Most of existing studies are case-by-case. Few of them are developed for the general purpose structuring task. These studies can be roughly divided into three categories: rule and dictionary based methods, task-specific end-to-end methods and pipeline methods. Rule and dictionary based methods BIBREF0, BIBREF1, BIBREF2 rely extremely on heuristics and handcrafted extraction rules which is more of",
        "type": "Document"
      },
      {
        "id": "f9c80977-8f36-4f53-b9ea-e4f910b452b6",
        "metadata": {
          "chunk_id": 12,
          "document_id": "2002.06644",
          "start_idx": 4609,
          "end_idx": 5099
        },
        "page_content": "rained distilled models on the training corpus to efficiently detect subjectivity. BERT-based ensemble models: Lastly, we use the weighted-average ensembling technique to exploit the predictions made by different variations of the above models. Ensembling methodology entails engendering a predictive model by utilizing predictions from multiple models in order to improve Accuracy and F1, decrease variance, and bias. We experiment with variations of $RoBERTa_{large}$, $ALBERT_{xxlarge.v2",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is ROUGE their only baseline?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "a5d060c0-c300-47e2-9f8c-0b28733ad015",
        "metadata": {
          "chunk_id": 54,
          "document_id": "1809.08731",
          "start_idx": 17965,
          "end_idx": 18451
        },
        "page_content": " correlated. In fact, sometimes it is difficult to distinguish which one is problematic: to illustrate this, we show some extreme examples\u2014compressions which got the highest fluency rating and the lowest possible content rating by at least one rater, but the lowest fluency score and the highest content score by another\u2014in Table 6 . We, thus, hypothesize that ROUGE should contain information about fluency which is complementary to SLOR, and want to make use of references for fluency",
        "type": "Document"
      },
      {
        "id": "dd3b7bd7-2b98-4425-8c3d-cb65a2597ecb",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1809.08731",
          "start_idx": 14576,
          "end_idx": 14995
        },
        "page_content": " references and no annotated examples are available. In contrast to SVR (line 1), ROUGE-LM (line 2) has only the same requirements as conventional word-overlap metrics (besides a large corpus for training the LM, which is easy to obtain for most languages). Thus, it can be used in the same settings as other word-overlap metrics. Since ROUGE-LM\u2014an uninformed combination\u2014performs significantly better than both ROUGE-L",
        "type": "Document"
      },
      {
        "id": "a84d1e58-a64f-46e4-bb74-a6256abb3a67",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1809.08731",
          "start_idx": 14319,
          "end_idx": 14742
        },
        "page_content": ", respectively. Results are shown in Table 7 . First, we can see that using SVR (line 1) to combine ROUGE-L-mult and WPSLOR outperforms both individual scores (lines 3-4) by a large margin. This serves as a proof of concept: the information contained in the two approaches is indeed complementary. Next, we consider the setting where only references and no annotated examples are available. In contrast to SVR (line 1), ROU",
        "type": "Document"
      },
      {
        "id": "03a0323f-a519-499e-9edf-b4b26c06d7d8",
        "metadata": {
          "chunk_id": 53,
          "document_id": "1809.08731",
          "start_idx": 17672,
          "end_idx": 18191
        },
        "page_content": " performance for fluency evaluation. However, a domain-specific LM might additionally improve the metrics' correlation with human judgments. We leave a more detailed analysis of the importance of the training data's domain for future work. ROUGE was shown to correlate well with ratings of a generated text's content or meaning at the sentence level BIBREF2 . We further expect content and fluency ratings to be correlated. In fact, sometimes it is difficult to distinguish which one is problematic: to illustrate this,",
        "type": "Document"
      },
      {
        "id": "1d812102-98f4-45bc-b355-c1653ccfaf7e",
        "metadata": {
          "chunk_id": 72,
          "document_id": "1809.08731",
          "start_idx": 24617,
          "end_idx": 25067
        },
        "page_content": " metrics on a benchmark dataset. Additionally, they can be applied even in settings where no references are available, or would be costly to obtain. Finally, for given references, we proposed the reference-based metric ROUGE-LM, which consists of a combination of WPSLOR and ROUGE. Thus, we were able to obtain an even more accurate fluency evaluation. We would like to thank Sebastian Ebert and Samuel Bowman for their detailed and helpful feedback.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does their system outperform the lexicon-based models?",
    "llm_answer": "The context states that the lexicon-based models had practically no impact on prediction, while the BERT-based models performed considerably better.  However, it does not provide a specific numerical comparison.\n",
    "context": [
      {
        "id": "a91598c8-9447-48fe-97fb-f0d1ef3c541f",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1909.00694",
          "start_idx": 11244,
          "end_idx": 11751
        },
        "page_content": "or the reverse of it for negation) if the event's predicate is in the seed lexicon. We can see that the seed lexicon itself had practically no impact on prediction. The models in the top block performed considerably better than the random baselines. The performance gaps with their (semi-)supervised counterparts, shown in the middle block, were less than 7%. This demonstrates the effectiveness of discourse relation-based label propagation. Comparing the model variants, we obtained the highest score with",
        "type": "Document"
      },
      {
        "id": "1903f30e-e87c-4559-8a22-7e065025b5dd",
        "metadata": {
          "chunk_id": 19,
          "document_id": "2002.06644",
          "start_idx": 7139,
          "end_idx": 7509
        },
        "page_content": "BERTa_{large}$, $ALBERT_{xxlarge.v2}$, $DistilRoBERTa$ and $BERT$ outperforms all the proposed models obtaining $0.704$ F1 score, $0.733$ precision, and $71.61\\%$ Accuracy. In this paper, we investigated BERT-based architectures for sentence level subjective bias detection. We perform our experiments on a general Wikipedia corpus consisting of more than $360k$ pre and",
        "type": "Document"
      },
      {
        "id": "a2d9d185-aa9e-429c-a8ae-d7bb6c5fd8dd",
        "metadata": {
          "chunk_id": 16,
          "document_id": "2002.06644",
          "start_idx": 6017,
          "end_idx": 6467
        },
        "page_content": " along with a recurrent dropout of $0.2$ in two 64 unit sized stacked BiLSTMs, using softmax activation layer as the final dense layer. tab:experimental-results shows the performance of different models on the WNC corpus evaluated on the following four metrics: Precision, Recall, F1, and Accuracy. Our proposed methodology, the use of finetuned optimized BERT based models, and BERT-based ensemble models outperform the baselines for all the metrics",
        "type": "Document"
      },
      {
        "id": "a276b7d8-2a80-443e-8bd8-ae7df0c5cdae",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1909.00694",
          "start_idx": 11751,
          "end_idx": 12204
        },
        "page_content": " of discourse relation-based label propagation. Comparing the model variants, we obtained the highest score with the BiGRU encoder trained with the AL+CA+CO dataset. BERT was competitive but its performance went down if CA and CO were used in addition to AL. We conjecture that BERT was more sensitive to noises found more frequently in CA and CO. Contrary to our expectations, supervised models (ACP) outperformed semi-supervised models (ACP+AL+CA+CO).",
        "type": "Document"
      },
      {
        "id": "e8c072a1-f15c-41d6-ae92-606bb2ea0825",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1811.00942",
          "start_idx": 0,
          "end_idx": 514
        },
        "page_content": "Deep learning has unquestionably advanced the state of the art in many natural language processing tasks, from syntactic dependency parsing BIBREF0 to named-entity recognition BIBREF1 to machine translation BIBREF2 . The same certainly applies to language modeling, where recent advances in neural language models (NLMs) have led to dramatically better approaches as measured using standard metrics such as perplexity BIBREF3 , BIBREF4 . Specifically focused on language modeling, this paper examines an issue that",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are the main international development topics that states raise identified?",
    "llm_answer": "By modeling topic prevalence in the context of structural covariates, controlling for region fixed effects and time trend, and examining the key words linked to the topics.\n",
    "context": [
      {
        "id": "605890f9-fe1a-4c4c-8778-21786409de1a",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1708.05873",
          "start_idx": 6744,
          "end_idx": 7219
        },
        "page_content": "', `growth', `financ-', and etc. suggests that Topic 2 represent a more traditional view of international development in that the emphasis is specifically on economic processes and relations. Topic 3 - Nuclear disarmament. This topic picks up the issue of nuclear weapons, which has been a major issue in the UN since its founding. Topic 4 - Post-conflict development. This topic relates to post-conflict development. The countries that feature in the key words (e.g. Rwanda,",
        "type": "Document"
      },
      {
        "id": "25df5d09-a693-4812-abe4-d8606be0b3b1",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1708.05873",
          "start_idx": 18219,
          "end_idx": 18780
        },
        "page_content": " most. States in South Asia and East Asia and the Pacific discuss Topic 7 the most. The figure shows that countries in North America are likely to speak about Topic 7 least. The analysis of discussion of international development in annual UN General Debate statements therefore uncovers two principle development topics: economic development and sustainable development. We find that discussion of Topic 2 is not significantly impacted by country-specific factors, such as wealth, population, democracy, levels of ODA, and conflict (although there are regional",
        "type": "Document"
      },
      {
        "id": "d028132f-ab83-4f88-bba3-e300d1692ba1",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1708.05873",
          "start_idx": 9933,
          "end_idx": 10425
        },
        "page_content": ", and the Helsinki Accords. Based on these topics, we examine Topic 2 and Topic 7 as the principal \u201cinternational development\u201d topics. While a number of other topics \u2013 for example post-conflict development, Africa, Latin America, etc. \u2013 are related to development issues, Topic 2 and Topic 7 most directly capture aspects of international development. We consider these two topics more closely by contrasting the main words linked to these two topics. In Figure FIGREF6 , the word clouds show",
        "type": "Document"
      },
      {
        "id": "33adf41a-5c35-436f-9573-46e9f29812f2",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1708.05873",
          "start_idx": 946,
          "end_idx": 1515
        },
        "page_content": " of development BIBREF1 . More generally, we know little about the types of development issues that different countries prioritise, or whether country-specific factors such as wealth or democracy make countries more likely to push for specific development issues to be put on the global political agenda. The lack of knowledge about the agenda setting process in the global governance of development is in large part due to the absence of obvious data sources on states' preferences about international development issues. To address this gap we employ a novel approach",
        "type": "Document"
      },
      {
        "id": "6c8bdce1-82e1-454f-9496-48b03741c9d2",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1708.05873",
          "start_idx": 4026,
          "end_idx": 4560
        },
        "page_content": " the key international development topics discussed in the GD. We model topic prevalence in the context of the structural covariates. In addition, we control for region fixed effects and time trend. The aim is to allow the observed metadata to affect the frequency with which a topic is discussed in General Debate speeches. This allows us to test the degree of association between covariates (and region/time effects) and the average proportion of a document discussing a topic. We assess the optimal number of topics that need to be",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is the semantic hierarchy representation used for any task?",
    "llm_answer": "Yes.\n",
    "context": [
      {
        "id": "e32e427c-d33e-40b3-81b5-0db1e7a2e4f7",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1909.12140",
          "start_idx": 6708,
          "end_idx": 7227
        },
        "page_content": " of Open IE systems can be easily enriched with contextual information that allows to restore the semantic relationship between a set of propositions and, hence, preserve their interpretability in downstream tasks. We developed and implemented a discourse-aware syntactic TS approach that recursively splits and rephrases complex English or German sentences into a semantic hierarchy of simplified sentences. The resulting lightweight semantic representation can be used to facilitate and improve a variety of AI tasks.",
        "type": "Document"
      },
      {
        "id": "0fe38026-928b-4b9c-a20b-06dc4c53517c",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1909.12140",
          "start_idx": 1335,
          "end_idx": 1894
        },
        "page_content": " approach based on Rhetorical Structure Theory (RST) BIBREF3. It establishes a contextual hierarchy between the split components, and identifies and classifies the semantic relationship that holds between them. In that way, a complex source sentence is turned into a so-called discourse tree, consisting of a set of hierarchically ordered and semantically interconnected sentences that present a simplified syntax which is easier to process for downstream semantic applications and may support a faster generalization in machine learning tasks. We present Dis",
        "type": "Document"
      },
      {
        "id": "5026906d-b0a3-4cf0-a0cc-5f82cc871d1d",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1909.12140",
          "start_idx": 3417,
          "end_idx": 3981
        },
        "page_content": " sentences (satellites) that disclose additional information about it. To differentiate between those two types of constituents, the transformation patterns encode a simple syntax-based approach where subordinate clauses/phrases are classified as context sentences, while superordinate as well as coordinate clauses/phrases are labelled as core. Second, we aim to restore the semantic relationship between the disembedded components. For this purpose, we identify and classify the rhetorical relations that hold between the simplified sentences, making use of both",
        "type": "Document"
      },
      {
        "id": "21a42ff1-5f16-427b-baea-4d2377ca9323",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1909.12140",
          "start_idx": 2277,
          "end_idx": 2783
        },
        "page_content": " These patterns were heuristically determined in a comprehensive linguistic analysis and encode syntactic and lexical features that can be derived from a sentence's parse tree. Each rule specifies (1) how to split up and rephrase the input into structurally simplified sentences and (2) how to set up a semantic hierarchy between them. They are recursively applied on a given source sentence in a top-down fashion. When no more rule matches, the algorithm stops and returns the generated discourse tree. In",
        "type": "Document"
      },
      {
        "id": "dca9c1f3-665e-4f94-a704-90b8cf4710db",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1909.12140",
          "start_idx": 3098,
          "end_idx": 3645
        },
        "page_content": " more sentences with a simplified syntax. To establish a semantic hierarchy between them, two subtasks are carried out: First, we set up a contextual hierarchy between the split sentences by connecting them with information about their hierarchical level, similar to the concept of nuclearity in RST. For this purpose, we distinguish core sentences (nuclei), which carry the key information of the input, from accompanying contextual sentences (satellites) that disclose additional information about it. To differentiate between those two types of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is the model evaluated?",
    "llm_answer": "Yes.\n",
    "context": [
      {
        "id": "72561058-d008-4fff-a632-c8a68443199c",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1809.04960",
          "start_idx": 24606,
          "end_idx": 25124
        },
        "page_content": " the analysis of the error types of our model, we still need to focus on avoiding predicting the plausible comments. There are few studies regarding machine commenting. Qin et al. QinEA2018 is the first to propose the article commenting task and a dataset, which is used to evaluate our model in this work. More studies about the comments aim to automatically evaluate the quality of the comments. Park et al. ParkSDE16 propose a system called CommentIQ, which assist the comment moderators in identifying high quality",
        "type": "Document"
      },
      {
        "id": "d900a4c6-a914-4046-90c3-c644cb9e5bbc",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1809.08731",
          "start_idx": 11851,
          "end_idx": 12232
        },
        "page_content": " extreme case of rating outputs of identical quality, the correlation is either not defined or 0, caused by noise of the evaluation model. Thus, we additionally evaluate using mean squared error (MSE), which is defined as the squares of residuals after a linear transformation, divided by the sample size:  $$\\text{MSE}_{X,Y} = \\underset{f}{\\min }\\frac{1}{|X|}\\sum \\limits _{i = 1}",
        "type": "Document"
      },
      {
        "id": "268d0437-5785-4965-b0aa-9d98ae01a6ce",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1809.04960",
          "start_idx": 16805,
          "end_idx": 17328
        },
        "page_content": " various, so it is intractable to find out all the possible references to be compared with the model outputs. Inspired by the evaluation methods of dialogue models, we formulate the evaluation as a ranking problem. Given a piece of news and a set of candidate comments, the comment model should return the rank of the candidate comments. The candidate comment set consists of the following parts: Correct: The ground-truth comments of the corresponding news provided by the human. Plausible: The 50 most similar comments to",
        "type": "Document"
      },
      {
        "id": "3d5a7fc0-83c3-46db-b973-cea8a957cd50",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1907.05664",
          "start_idx": 12938,
          "end_idx": 13449
        },
        "page_content": " are not observed when deleting unimportant words (see Figure 4 ), but for other test examples we don't observe a significant difference between the two settings (see Figure 5 ). One might argue that the second summary in Figure 5 is better than the first one as it makes better sentences but as the model generates inaccurate summaries, we do not wish to make such a statement. This however allows us to say that the attribution generated for the text at the origin of the summaries in Figure 4 are truthful in",
        "type": "Document"
      },
      {
        "id": "1a847ffb-f552-49ff-b241-9d03fe67759c",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1809.08731",
          "start_idx": 1267,
          "end_idx": 1699
        },
        "page_content": " subject of a decade-long debate in cognitive science. In particular, the question has been raised whether the grammatical knowledge that underlies this ability is probabilistic or categorical in nature BIBREF3 , BIBREF4 , BIBREF5 . Within this context, lau2017grammaticality have recently shown that neural language models (LMs) can be used for modeling human ratings of acceptability. Namely, they found SLOR BIBREF6 \u2014sentence log",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How better is accuracy of new model compared to previously reported models?",
    "llm_answer": "The new model outperforms previous models (TF-IDF, S2S, IR, BERT-Base, QANet) in EM-score and F$_1$-score in several experiments.  Specific improvements vary, from 0.13% to 6.39% in EM-score and 3.69% to 93.75% in F$_1$-score.\n",
    "context": [
      {
        "id": "3991767e-4489-4f2f-a25b-7d16b4e0d6dd",
        "metadata": {
          "chunk_id": 57,
          "document_id": "1809.04960",
          "start_idx": 22226,
          "end_idx": 22753
        },
        "page_content": " further improve the performance of the supervised model. Although our proposed model can achieve better performance than previous models, there are still remaining two questions: why our model can outperform them, and how to further improve the performance. To address these queries, we perform error analysis to analyze the error types of our model and the baseline models. We select TF-IDF, S2S, and IR as the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types",
        "type": "Document"
      },
      {
        "id": "11d5de76-9aa8-482d-97f8-e7f8104a711c",
        "metadata": {
          "chunk_id": 52,
          "document_id": "1908.06606",
          "start_idx": 18241,
          "end_idx": 18595
        },
        "page_content": "formed it with 0.75% score in EM-score. Compared with BERT-Base, our model led to a 5.64% performance improvement in EM-score and 3.69% in F$_1$-score. Although our model didn't outperform much with QANet in F$_1$-score (only 0.13%), our model significantly outperformed it with 6.39% score in EM-score. To further investigate the effects of named entity",
        "type": "Document"
      },
      {
        "id": "3817b437-4a8b-4bbe-970f-1be7126f027c",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1908.06606",
          "start_idx": 18088,
          "end_idx": 18399
        },
        "page_content": " TABREF23. Table TABREF23 indicates that our proposed model achieved the best performance both in EM-score and F$_1$-score with EM-score of 91.84% and F$_1$-score of 93.75%. QANet outperformed BERT-Base with 3.56% score in F$_1$-score but underperformed it with 0.75% score in EM-score. Compared with BERT-Base,",
        "type": "Document"
      },
      {
        "id": "dafa5052-d9fa-47cf-bcf5-a0c22a1efe79",
        "metadata": {
          "chunk_id": 63,
          "document_id": "2001.00137",
          "start_idx": 23847,
          "end_idx": 24394
        },
        "page_content": " their accuracy while the lower achieving class saw improvement. Further evaluation on the F1-scores decay in the presence of noise demonstrated that our model is more robust than the baseline models when considering noisy data, be that in the form of incorrect sentences or sentences with STT error. Not only that, experiments on the Twitter dataset also showed improved accuracy in clean data, with complete sentences. We infer that this is due to our model being able to extract richer data representations from the input data regardless of the",
        "type": "Document"
      },
      {
        "id": "e8c072a1-f15c-41d6-ae92-606bb2ea0825",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1811.00942",
          "start_idx": 0,
          "end_idx": 514
        },
        "page_content": "Deep learning has unquestionably advanced the state of the art in many natural language processing tasks, from syntactic dependency parsing BIBREF0 to named-entity recognition BIBREF1 to machine translation BIBREF2 . The same certainly applies to language modeling, where recent advances in neural language models (NLMs) have led to dramatically better approaches as measured using standard metrics such as perplexity BIBREF3 , BIBREF4 . Specifically focused on language modeling, this paper examines an issue that",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does the active learning model work?",
    "llm_answer": "Active learning uses a learning engine (a classifier) and a selection engine (sampling strategy) to choose samples from unlabeled data for annotators to relabel.  These relabeled samples are then added to the training set, improving the classifier's accuracy in an iterative process.\n",
    "context": [
      {
        "id": "0b19ecf9-91d8-43aa-8945-369ce8cf345f",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1908.08419",
          "start_idx": 2078,
          "end_idx": 2590
        },
        "page_content": " in general domains, causing a major problem that a considerable amount of manually annotated data is required, when introducing the models into specific domains. One of the solutions for this obstacle is to use active learning, where only a small scale of samples are selected and labeled in an active manner. Active learning methods are favored by the researchers in many natural language processing (NLP) tasks, such as text classification BIBREF9 and named entity recognition (NER) BIBREF10 . However, only a",
        "type": "Document"
      },
      {
        "id": "4c77c7da-e7a5-439e-9f8b-ee904ea4e4bf",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1908.08419",
          "start_idx": 8307,
          "end_idx": 8763
        },
        "page_content": " which chooses samples that need to be relabeled by annotators from unlabeled data. Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, a CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively. Fig. FIGREF7 and Algorithm SECREF3 demonstrate the procedure of CWS based on active learning. First, we train",
        "type": "Document"
      },
      {
        "id": "2be2d618-316d-4679-b60e-0baf024f9920",
        "metadata": {
          "chunk_id": 21,
          "document_id": "1908.08419",
          "start_idx": 7803,
          "end_idx": 8307
        },
        "page_content": ". However, in some complicated tasks, such as CWS and NER, only considering the uncertainty of classifier is obviously not enough. Active learning methods can generally be described into two parts: a learning engine and a selection engine BIBREF28 . The learning engine is essentially a classifier, which is mainly used for training of classification problems. The selection engine is based on the sampling strategy, which chooses samples that need to be relabeled by annotators from unlabeled data. Then",
        "type": "Document"
      },
      {
        "id": "d78d671c-32c7-4bfe-a8dc-77b0094c905a",
        "metadata": {
          "chunk_id": 64,
          "document_id": "1908.08419",
          "start_idx": 23290,
          "end_idx": 23799
        },
        "page_content": " to annotate the unlabeled set. Then, sampling strategy in active learning selects a part of samples for annotators to relabel. Finally, the relabeled samples are added to train set for segmenter to re-train. Our proposed scoring strategy selects samples according to the sequence scores of the segmented sentences, while uncertainty sampling suggests relabeling samples that are closest to the segmenter\u2019s decision boundary. Generally, two main parameters in active learning are the numbers of iterations and",
        "type": "Document"
      },
      {
        "id": "f9b4986d-9114-4363-b490-fed261a32a8c",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1908.08419",
          "start_idx": 6703,
          "end_idx": 7256
        },
        "page_content": " approach for CWS in medical text, which combines information entropy with neural network to effectively reduce annotation cost. Active learning BIBREF22 mainly aims to ease the data collection process by automatically deciding which instances should be labeled by annotators to train a model as quickly and effectively as possible BIBREF23 . The sampling strategy plays a key role in active learning. In the past decade, the rapid development of active learning has resulted in various sampling strategies, such as uncertainty sampling BIBREF24 , query",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Did the annotators agreed and how much?",
    "llm_answer": "Inter-annotator agreement was found to be sufficiently high.  A specific Kappa value (90.5%) is mentioned as good agreement.\n",
    "context": [
      {
        "id": "7ad84a74-dfae-4c36-99a3-0c3aa9c1ab59",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1703.05260",
          "start_idx": 17551,
          "end_idx": 18087
        },
        "page_content": " results were discussed with the authors. Throughout the whole annotation phase, annotators could discuss any emerging issues with the authors. All annotations were done by undergraduate students of computational linguistics. The annotation was rather time-consuming due to the complexity of the task, and thus we decided for single annotation mode. To assess annotation quality, a small sample of texts was annotated by all four annotators and their inter-annotator agreement was measured (see Section \"Inter-Annotator Agreement\" ). It",
        "type": "Document"
      },
      {
        "id": "d57b48a2-0597-49c7-ba89-594a0726bee9",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1703.05260",
          "start_idx": 17876,
          "end_idx": 18350
        },
        "page_content": " inter-annotator agreement was measured (see Section \"Inter-Annotator Agreement\" ). It was found to be sufficiently high. Annotation of the corpus together with some pre- and post-processing of the data required about 500 hours of work. All stories were annotated with event and participant types (a total of 12,188 and 43,946 instances, respectively). On average there were 7 coreference chains per story with an average length of 6 tokens. After the first annotation round",
        "type": "Document"
      },
      {
        "id": "db06e4fb-8b42-4227-85db-7ab7688ea935",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1703.05260",
          "start_idx": 23142,
          "end_idx": 23652
        },
        "page_content": " by all 4 annotators after the first annotation phase. We checked the agreement on these data using Fleiss' Kappa BIBREF4 . The results are shown in Figure 4 and indicate moderate to substantial agreement BIBREF5 . Interestingly, if we calculated the Kappa only on the subset of cases that were annotated with script-specific event and participant labels by all annotators, results were better than those of the evaluation on all labeled instances (including also unrelated and related non-script events). This",
        "type": "Document"
      },
      {
        "id": "507bc063-cda2-47a3-9621-3b02661b8656",
        "metadata": {
          "chunk_id": 64,
          "document_id": "1703.05260",
          "start_idx": 22746,
          "end_idx": 23142
        },
        "page_content": "dresser}}}$ is cutting my $_{\\textsc {\\scriptsize {ScrPart\\_customer}}}$ hair we $_{\\textsc {\\scriptsize Scr\\_Part\\_hairdresser\\_customer}}$ usually talk a bit. In order to calculate inter-annotator agreement, a total of 30 stories from 6 scenarios were randomly chosen for parallel annotation by all 4 annotators after the first annotation phase. We checked the agreement on these data using Fle",
        "type": "Document"
      },
      {
        "id": "51967a51-6d78-44f6-802a-a0611befeb24",
        "metadata": {
          "chunk_id": 67,
          "document_id": "1703.05260",
          "start_idx": 23974,
          "end_idx": 24460
        },
        "page_content": " 3 annotators (qualified majority vote) compared to the set of those pairs annotated by at least one person (see Figure 4 ). We take the result of 90.5% between annotators to be a good agreement. Figure 5 gives an overview of the number of event and participant types provided in the templates. Taking a flight and getting a haircut stand out with a large number of both event and participant types, which is due to the inherent complexity of the scenarios. In contrast, planting a tree",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What datasets are used to evaluate this approach?",
    "llm_answer": "815 test instances (5,421 sentences).\n",
    "context": [
      {
        "id": "e929c104-c4ce-40d5-aca7-762538286335",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1908.06606",
          "start_idx": 15988,
          "end_idx": 16383
        },
        "page_content": ") and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage",
        "type": "Document"
      },
      {
        "id": "61243fd2-2c6a-43aa-84f8-76e156e5bfc9",
        "metadata": {
          "chunk_id": 49,
          "document_id": "2003.07433",
          "start_idx": 19311,
          "end_idx": 19770
        },
        "page_content": " well as dimensions' text analysis scores. We chose three categories based on the three different surveys: 1) DOSPERT scale; 2) BSSS scale; and 3) VIAS scale. Dimension selection: We have to define the word sets (also called dimension) for each category. We chose one dimension for each of the questions under each category to reflect real survey system evaluation. Our chosen categories are state in Fig FIGREF20. Score calculation $\\alpha $-score: $\\alpha $",
        "type": "Document"
      },
      {
        "id": "7d4754c7-4ea8-4d20-9fb6-3d1e50bf0cf6",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1809.04960",
          "start_idx": 22497,
          "end_idx": 22960
        },
        "page_content": " the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types of comments as described in the above retrieval evaluation setting: Correct, Plausible, Popular, and Random. We rank the candidate comment set with four models (TF-IDF, S2S, IR, and Proposed+IR), and record the types of top-1 comments. Figure FIGREF40 shows the percentage of different types of top-1 comments generated by each model. It shows",
        "type": "Document"
      },
      {
        "id": "d14d3b6d-7650-4c41-a57f-cb1b97f487ac",
        "metadata": {
          "chunk_id": 68,
          "document_id": "1908.06606",
          "start_idx": 23837,
          "end_idx": 24326
        },
        "page_content": " 60 or 70 percent. This proves that there is commonality between different tasks and our proposed QA-CTS task make this learnable. In conclusion, to achieve the best performance for a specific dataset, pre-training the model in multiple datasets and then fine tuning the model on the specific dataset is the best way. In this paper, we present a question answering based clinical text structuring (QA-CTS) task, which unifies different clinical text structuring tasks and utilize different",
        "type": "Document"
      },
      {
        "id": "1d812102-98f4-45bc-b355-c1653ccfaf7e",
        "metadata": {
          "chunk_id": 72,
          "document_id": "1809.08731",
          "start_idx": 24617,
          "end_idx": 25067
        },
        "page_content": " metrics on a benchmark dataset. Additionally, they can be applied even in settings where no references are available, or would be costly to obtain. Finally, for given references, we proposed the reference-based metric ROUGE-LM, which consists of a combination of WPSLOR and ROUGE. Thus, we were able to obtain an even more accurate fluency evaluation. We would like to thank Sebastian Ebert and Samuel Bowman for their detailed and helpful feedback.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How was the dataset collected?",
    "llm_answer": "The Chinese dataset was collected from Tencent News, a popular Chinese website for news and opinion articles.  It contains 198,112 news articles with titles, content, and user comments.  A Japanese dataset was extracted from Japanese websites using HTML layouts and linguistic patterns.  A dataset of twitter posts was used to develop PTSD Linguistic Dictionary.  A dataset of train transport connections was obtained with two intents.  An incomplete dataset was obtained through manipulation of original corpora.\n",
    "context": [
      {
        "id": "9bbb5981-d2da-4825-be7f-156f2ab59ca7",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1809.04960",
          "start_idx": 13507,
          "end_idx": 13972
        },
        "page_content": " and paired data INLINEFORM4 . We select a large-scale Chinese dataset BIBREF0 with millions of real comments and a human-annotated test set to evaluate our model. The dataset is collected from Tencent News, which is one of the most popular Chinese websites for news and opinion articles. The dataset consists of 198,112 news articles. Each piece of news contains a title, the content of the article, and a list of the users' comments. Following the previous work B",
        "type": "Document"
      },
      {
        "id": "4c338ca2-d72d-4ab4-a4d9-1d14883ea058",
        "metadata": {
          "chunk_id": 63,
          "document_id": "2003.07433",
          "start_idx": 24649,
          "end_idx": 25131
        },
        "page_content": "' twitter posts into training and test dataset. Then, we first developed PTSD Linguistic Dictionary from the twitter posts from training dataset and apply LAXARY framework on test dataset. To provide an initial results, we take 50% of users' last week's (the week they responded of having PTSD) data to develop PTSD Linguistic dictionary and apply LAXARY framework to fill up surveys on rest of 50% dataset. The distribution of this training-test dataset segmentation followed a 50%",
        "type": "Document"
      },
      {
        "id": "cf348585-ca79-493f-be9d-ce678544cd42",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1909.00694",
          "start_idx": 8898,
          "end_idx": 9330
        },
        "page_content": " CO such that it was five times larger than AL. The results are shown in Table TABREF16. We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively: . \u4f5c\u696d\u304c\u697d\u3060\u3002 The work",
        "type": "Document"
      },
      {
        "id": "a6d359ca-9edf-4f47-b659-020f371dc375",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1809.04960",
          "start_idx": 17625,
          "end_idx": 18070
        },
        "page_content": " dataset. We count the frequency of each comments in the training set, and select the 50 most frequent comments to form the popular comment set. The popular comments are the general and meaningless comments, such as \u201cYes\u201d, \u201cGreat\u201d, \u201cThat's right', and \u201cMake Sense\u201d. These comments are dull and do not carry any information, so they are regarded as incorrect comments. Random: After selecting the correct, plausible, and popular comments, we fill",
        "type": "Document"
      },
      {
        "id": "ba3faa5f-7fd7-4bbf-81ef-ce947eab4cd1",
        "metadata": {
          "chunk_id": 35,
          "document_id": "2001.00137",
          "start_idx": 13822,
          "end_idx": 14323
        },
        "page_content": " transport connections. The dataset has two intents, namely Departure Time and Find Connection with 100 train and 106 test samples, shown in Table TABREF18. Even though English is the main language of the benchmark, this dataset contains a few German station and street names. The incomplete dataset used for training is composed of lower-cased incomplete data obtained by manipulating the original corpora. The incomplete sentences with STT error are obtained in a 2-step process shown in Fig. FIGREF",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What models other than standalone BERT is new model compared to?",
    "llm_answer": "BERT, RoBERTa, ALBERT, and their base and large specifications.  Also, distilled BERT-based models.\n",
    "context": [
      {
        "id": "f09f867a-bb11-4305-a53b-02b63aafd4c1",
        "metadata": {
          "chunk_id": 6,
          "document_id": "2002.06644",
          "start_idx": 2489,
          "end_idx": 2911
        },
        "page_content": " various BERT-based models, including BERT, RoBERTa, ALBERT, with their base and large specifications along with their native classifiers. We propose an ensemble model exploiting predictions from these models using multiple ensembling techniques. We show that our model outperforms the baselines by a margin of $5.6$ of F1 score and $5.95\\%$ of Accuracy. In this section, we outline baseline models like $BERT_{large}$. We",
        "type": "Document"
      },
      {
        "id": "f1459c54-8658-4eab-9888-63760d4ad82c",
        "metadata": {
          "chunk_id": 46,
          "document_id": "2001.00137",
          "start_idx": 17632,
          "end_idx": 17991
        },
        "page_content": "$ and regularization term of $1.0$. Most often, the best performing classifier was MLP. Conventional BERT is a BERT-base-uncased model, meaning that it has 12 transformer blocks $L$, hidden size $H$ of 768, and 12 self-attention heads $A$. The model is fine-tuned with our dataset on 2 Titan X GPUs for 3 epochs with Adam Optimizer, learning rate of $2*10^{-5",
        "type": "Document"
      },
      {
        "id": "2164b4e1-d758-4994-92ac-17bbce5a8959",
        "metadata": {
          "chunk_id": 11,
          "document_id": "2002.06644",
          "start_idx": 4295,
          "end_idx": 4797
        },
        "page_content": "QuAD for Question Answering. Distilled BERT-based models: Secondly, we propose to use distilled BERT-based models, as introduced in BIBREF8. They are smaller general-purpose language representation model, pre-trained by leveraging distillation knowledge. This results in significantly smaller and faster models with performance comparable to their undistilled versions. We finetune these pretrained distilled models on the training corpus to efficiently detect subjectivity. BERT-based ensemble models:",
        "type": "Document"
      },
      {
        "id": "43bb823d-5182-48a7-8830-51b536b4ab02",
        "metadata": {
          "chunk_id": 78,
          "document_id": "1910.07181",
          "start_idx": 25645,
          "end_idx": 26144
        },
        "page_content": " standalone BERT model and one combined with Attentive Mimicking across all tasks. While keeping the original BERT embeddings in addition to Bertram's representation brings no benefit, adding in-domain data clearly helps for two out of three datasets. This makes sense as for rare words, every single additional context can be crucial for gaining a deeper understanding. To further understand for which words using Bertram is helpful, in Figure FIGREF39 we look at the accuracy of BERT both with and",
        "type": "Document"
      },
      {
        "id": "76b8b551-5acb-4396-ad68-bd7052c2758b",
        "metadata": {
          "chunk_id": 17,
          "document_id": "2002.06644",
          "start_idx": 6467,
          "end_idx": 6878
        },
        "page_content": "ERT based models, and BERT-based ensemble models outperform the baselines for all the metrics. Among the optimized BERT based models, $RoBERTa_{large}$ outperforms all other non-ensemble models and the baselines for all metrics. It further achieves a maximum recall of $0.681$ for all the proposed models. We note that DistillRoBERTa, a distilled model, performs competitively, achieving $69.69\\%$ accuracy, and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is the performance difference between this method and the baseline?",
    "llm_answer": "The context states that the two SLOR approaches outperform the best word-overlap metric baseline by 0.244 and 0.097 Pearson correlation, and 0.012 and 0.012 MSE, respectively, for SEQ2SEQ and T3.\n",
    "context": [
      {
        "id": "cc99aa6c-159a-48e7-8bb5-ed6951398266",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1811.00942",
          "start_idx": 14041,
          "end_idx": 14133
        },
        "page_content": " $\\times $ rise in latency and 32 $\\times $ increase in energy usage, when compared to KN-5.",
        "type": "Document"
      },
      {
        "id": "b8e609ae-3701-454d-85e0-0983396c3cd5",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1809.08731",
          "start_idx": 15656,
          "end_idx": 15966
        },
        "page_content": "35$ vs. $\\text{Var}(Y) = 0.17$ for SEQ2SEQ). Comparing the metrics, the two SLOR approaches perform best for SEQ2SEQ and T3. In particular, they outperform the best word-overlap metric baseline by $0.244$ and $0.097$ Pearson correlation as well as $0.012$ and $0.012$ MSE, respectively. Since T3 is an abstract",
        "type": "Document"
      },
      {
        "id": "586f9914-737b-4c1c-a186-96eb5f527202",
        "metadata": {
          "chunk_id": 50,
          "document_id": "2001.00137",
          "start_idx": 18932,
          "end_idx": 19320
        },
        "page_content": " one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80$\\%$ against BERT's 72$\\%$. The second version (Corr) considers the corrected tweets, and shows higher accuracy given that it is less noisy. In that version, Stacked DeBERT achieves 82$\\%$ accuracy against BERT's 76$\\%$, an improvement of 6$\\%$. In the last case (Inc+Corr), we",
        "type": "Document"
      },
      {
        "id": "89402f7f-66e9-450c-8d42-c30fc9bf8d45",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1811.00942",
          "start_idx": 12674,
          "end_idx": 13079
        },
        "page_content": " over 1.2 seconds per next-word prediction. Neural models achieve perplexity drops of 60\u201380% and R@3 increases of 22\u201334%, but these improvements come at a much higher cost in latency and energy usage. In Table 2 (last two columns), the desktop yields very different results: the neural models on PTB (rows 2\u20133) are 9 $\\times $ slower than KN-5, but the absolute latency is only 8 ms/q, which is still much",
        "type": "Document"
      },
      {
        "id": "4f21d9f4-3aef-4f6c-8ec7-e4c742b14161",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1908.06606",
          "start_idx": 22920,
          "end_idx": 23336
        },
        "page_content": " performance on distal resection margin improved by 5.56% in EM-score and 6.32% in F$_1$-score. Other performances also usually improved a lot. This proves the usefulness of two-stage training and named entity information as well. Lastly, we fine tune the model for each task with a pre-trained parameter. Table TABREF32 summarizes the result. (Add some explanations for the Table TABREF32). Comparing Table TABREF32",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What approaches without reinforcement learning have been tried?",
    "llm_answer": "Classification-based and regression-based approaches.\n",
    "context": [
      {
        "id": "413bd7c3-9f2b-4bd5-8c48-fea8f8176948",
        "metadata": {
          "chunk_id": 101,
          "document_id": "1902.00330",
          "start_idx": 34293,
          "end_idx": 34753
        },
        "page_content": " state to help disambiguation. In the last few years, reinforcement learning has emerged as a powerful tool for solving complex sequential decision-making problems. It is well known for its great success in the game field, such as Go BIBREF37 and Atari games BIBREF38 . Recently, reinforcement learning has also been successfully applied to many natural language processing tasks and achieved good performance BIBREF12 , BIBREF39 , BIBREF5 . Feng et al. BIBREF",
        "type": "Document"
      },
      {
        "id": "bd9e9b93-3c0e-4e65-b722-95063c9d5e85",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1909.00542",
          "start_idx": 16697,
          "end_idx": 17106
        },
        "page_content": " for the reward function used by a reinforcement learning system. Reinforcement learning gives promising results, especially in human evaluations made on the runs submitted to BioASQ 6b. This year we introduced very small changes to the runs using reinforcement learning, and will aim to explore more complex reinforcement learning strategies and more complex neural models in the policy and value estimators.",
        "type": "Document"
      },
      {
        "id": "2d9e100c-8cb0-48b7-81d1-63d646c42530",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1909.00542",
          "start_idx": 8896,
          "end_idx": 9426
        },
        "page_content": " the results are compatible with the top part of the table and prior work suggesting that classification-based approaches improve over regression-based approaches. We also experiment with the use of reinforcement learning techniques. Again these experiments are based on BIBREF2, who uses REINFORCE to train a global policy. The policy predictor uses a simple feedforward network with a hidden layer. The results reported by BIBREF2 used ROUGE Recall and indicated no improvement with respect to deep learning architectures. Human",
        "type": "Document"
      },
      {
        "id": "ea332dca-00d6-42fe-b44a-9b8f20bfc7d5",
        "metadata": {
          "chunk_id": 102,
          "document_id": "1902.00330",
          "start_idx": 34613,
          "end_idx": 35109
        },
        "page_content": "REF12 , BIBREF39 , BIBREF5 . Feng et al. BIBREF5 used reinforcement learning for relation classification task by filtering out the noisy data from the sentence bag and they achieved huge improvements compared with traditional classifiers. Zhang et al. BIBREF40 applied the reinforcement learning on sentence representation by automatically discovering task-relevant structures. To automatic taxonomy induction from a set of terms, Han et al. BIBREF41 designed an end-to-end reinforcement learning",
        "type": "Document"
      },
      {
        "id": "8202e7b7-083e-4462-b7d8-b7a03cae2a56",
        "metadata": {
          "chunk_id": 100,
          "document_id": "1902.00330",
          "start_idx": 33757,
          "end_idx": 34293
        },
        "page_content": " of previously referred entities for the subsequent entity disambiguation. Nguyen et al. BIBREF2 use the sequence model, but they simply encode the results of the greedy choice, and measure the similarities between the global encoding and the candidate entity representations. Their model does not consider the long-term impact of current decisions on subsequent choices, nor does they add the selected target entity information to the current state to help disambiguation. In the last few years, reinforcement learning has emerged as a",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which languages do they validate on?",
    "llm_answer": "Spanish (Es), French (Fr), Italian (It), English (En), German (De), and Russian (Ru).\n",
    "context": [
      {
        "id": "b4048d90-aa2a-4fd2-8f5e-b7848534cb17",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1910.04269",
          "start_idx": 17760,
          "end_idx": 18217
        },
        "page_content": " of 1,500 samples for each of six languages. Out of 1,500 samples for each language, 1,200 were randomly selected as training dataset for that language and rest 300 as validation dataset using k-fold cross-validation. To sum up, we trained our model on 7,200 samples and validated it on 1800 samples comprising six languages. The results are discussed in next section. This paper discusses two end-to-end approaches which achieve state-of-the-art results in",
        "type": "Document"
      },
      {
        "id": "e8fb7d0e-86e1-4332-bb4a-d8109e4146e4",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1910.04269",
          "start_idx": 19553,
          "end_idx": 20023
        },
        "page_content": ", the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Kat",
        "type": "Document"
      },
      {
        "id": "85779acb-8ae5-4981-9d9e-b56749f83aec",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1910.04269",
          "start_idx": 19085,
          "end_idx": 19553
        },
        "page_content": "fold cross validation for six languages when mixup was applied. In Fig FIGREF27 (b) we present the confusion matrix for the 2D-ConvNet model. 2D attention models focused on the important features extracted by convolutional layers and bi-directional GRU captured the temporal features. Several of the spoken languages in Europe belong to the Indo-European family. Within this family, the languages are divided into three phyla which are Romance, Germanic and Slavic. Of",
        "type": "Document"
      },
      {
        "id": "7da029d3-0e6a-48d3-9dd2-3cfd37690294",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1910.04269",
          "start_idx": 6853,
          "end_idx": 7342
        },
        "page_content": " comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (",
        "type": "Document"
      },
      {
        "id": "e12f8be6-5e2b-4af6-bff4-3297f5e8d2b7",
        "metadata": {
          "chunk_id": 55,
          "document_id": "2003.07433",
          "start_idx": 21602,
          "end_idx": 22174
        },
        "page_content": " and .96 based on the binary method, which confirm the high reliability of our PTSD Dictionary created PTSD survey based categories. After assessing the reliability of the PTSD Linguistic dictionary, we focus on the two most common forms of construct validity: convergent validity and discriminant validity BIBREF25. Convergent validity provides evidence that two measures designed to assess the same construct are indeed related; discriminate validity involves evidence that two measures designed to assess different constructs are not too strongly related. In theory, we",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the baseline method for the task?",
    "llm_answer": "TF-IDF (Lexical, Non-Neural)\n",
    "context": [
      {
        "id": "ac42627b-633e-4513-90d4-3dc7214959c7",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1910.07181",
          "start_idx": 15902,
          "end_idx": 16251
        },
        "page_content": "ii) training the baseline model on the train set and (iii) modifying a subset of the test candidates to generate the final test set. We partition $\\mathcal {D}$ into a train set $\\mathcal {D}_\\text{train}$ and a set of test candidates, $\\mathcal {D}_\\text{cand}$, with the latter containing all instances $(\\mathbf {x},y) \\in \\mathcal {D}$ such that",
        "type": "Document"
      },
      {
        "id": "7d4754c7-4ea8-4d20-9fb6-3d1e50bf0cf6",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1809.04960",
          "start_idx": 22497,
          "end_idx": 22960
        },
        "page_content": " the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types of comments as described in the above retrieval evaluation setting: Correct, Plausible, Popular, and Random. We rank the candidate comment set with four models (TF-IDF, S2S, IR, and Proposed+IR), and record the types of top-1 comments. Figure FIGREF40 shows the percentage of different types of top-1 comments generated by each model. It shows",
        "type": "Document"
      },
      {
        "id": "8937964f-11b5-487b-9960-db106025360c",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1909.02764",
          "start_idx": 17421,
          "end_idx": 17831
        },
        "page_content": " layer. This setup is inspired by BIBREF38. We use a dropout rate of 0.3 in all layers and optimize with Adam BIBREF39 with a learning rate of $10^{-5}$ (These parameters are the same for all further experiments). We build on top of the Keras library with the TensorFlow backend. We consider this setup our baseline model. We train models on a variety of corpora, namely the common format published by BIBREF27",
        "type": "Document"
      },
      {
        "id": "b70e6e5c-d4a2-4bbf-ac54-07de71fbedc5",
        "metadata": {
          "chunk_id": 21,
          "document_id": "2002.11893",
          "start_idx": 8062,
          "end_idx": 8606
        },
        "page_content": "domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context. To make workers understand the task more easily, we crafted templates to generate natural language descriptions for each structured goal. Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and",
        "type": "Document"
      },
      {
        "id": "8a126462-c5fa-46d2-8d8b-c57956c37c65",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1809.04960",
          "start_idx": 14887,
          "end_idx": 15333
        },
        "page_content": " and use batch normalization BIBREF9 in each layer. We also gradually increase the KL term from 0 to 1 after each epoch. We compare our model with several unsupervised models and supervised models. Unsupervised baseline models are as follows: TF-IDF (Lexical, Non-Neural) is an important unsupervised baseline. We use the concatenation of the title and the body as the query to retrieve the candidate comment set by means of the similarity of the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what amounts of size were used on german-english?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "a9ca64bb-aa91-4680-affb-f77aa02298b9",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1905.11901",
          "start_idx": 11977,
          "end_idx": 12375
        },
        "page_content": "word vocabulary size (see Table 1). Table TABREF24 shows some sample translations that represent typical errors of our PBSMT and NMT systems, trained with ultra-low (100k words) and low (3.2M words) amounts of data. For unknown words such as blutbefleckten (`bloodstained') or Spaniern (`Spaniards', `Spanish'), PBSMT systems default to copying, while NMT systems produce translations on a subword-",
        "type": "Document"
      },
      {
        "id": "364a21c4-42f1-432a-b1ca-928daf3d9bde",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1905.11901",
          "start_idx": 3269,
          "end_idx": 3706
        },
        "page_content": " dropout BIBREF27 , word dropout BIBREF28 , layer normalization BIBREF29 and tied embeddings BIBREF30 . Subword representations such as BPE BIBREF31 have become a popular choice to achieve open-vocabulary translation. BPE has one hyperparameter, the number of merge operations, which determines the size of the final vocabulary. For high-resource settings, the effect of vocabulary size on translation quality is relatively small; BIBREF",
        "type": "Document"
      },
      {
        "id": "b17df396-dc07-41d4-98b5-5b9bf4067902",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1905.11901",
          "start_idx": 3534,
          "end_idx": 4016
        },
        "page_content": " high-resource settings, the effect of vocabulary size on translation quality is relatively small; BIBREF32 report mixed results when comparing vocabularies of 30k and 90k subwords. In low-resource settings, large vocabularies result in low-frequency (sub)words being represented as atomic units at training time, and the ability to learn good high-dimensional representations of these is doubtful. BIBREF33 propose a minimum frequency threshold for subword units, and splitting any",
        "type": "Document"
      },
      {
        "id": "a17fb120-a0e4-4822-be10-0767efd76e1b",
        "metadata": {
          "chunk_id": 72,
          "document_id": "1902.00330",
          "start_idx": 22937,
          "end_idx": 23326
        },
        "page_content": " We set the dimensions of word embedding and entity embedding to 300, where the word embedding and entity embedding are released by BIBREF20 and BIBREF0 respectively. For parameters of the local LSTM network, the number of LSTM cell units is set to 512, the batch size is 64, and the rank margin $\\gamma $ is 0.1. Similarly, in global LSTM network, the number of LSTM cell units is 700 and",
        "type": "Document"
      },
      {
        "id": "7da029d3-0e6a-48d3-9dd2-3cfd37690294",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1910.04269",
          "start_idx": 6853,
          "end_idx": 7342
        },
        "page_content": " comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is the dataset?",
    "llm_answer": "198,112 news articles\n",
    "context": [
      {
        "id": "9ccb7945-9a96-417a-b52e-68855c4910b1",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "cf348585-ca79-493f-be9d-ce678544cd42",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1909.00694",
          "start_idx": 8898,
          "end_idx": 9330
        },
        "page_content": " CO such that it was five times larger than AL. The results are shown in Table TABREF16. We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively: . \u4f5c\u696d\u304c\u697d\u3060\u3002 The work",
        "type": "Document"
      },
      {
        "id": "8df4e3d7-a42b-46ef-9f32-e1c798bdcc70",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1809.04960",
          "start_idx": 13972,
          "end_idx": 14405
        },
        "page_content": " the content of the article, and a list of the users' comments. Following the previous work BIBREF0 , we tokenize all text with the popular python package Jieba, and filter out short articles with less than 30 words in content and those with less than 20 comments. The dataset is split into training/validation/test sets, and they contain 191,502/5,000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1",
        "type": "Document"
      },
      {
        "id": "f8ec0442-a15a-41b1-b2a7-4614f86ba6e5",
        "metadata": {
          "chunk_id": 53,
          "document_id": "1908.08419",
          "start_idx": 19170,
          "end_idx": 19642
        },
        "page_content": " from 3868 records as unlabeled set. Then, we manually annotate remaining 668 records as labeled set, which contains 1170 sentences. Finally, we divide labeled set into train set and test set with the ratio of 7:3 randomly. Statistics of datasets are listed in Table TABREF33 . To determine suitable parameters, we divide training set into two sets, the first 80% sentences as training set and the rest 20% sentences as validation set. Character embedding dimensions and K",
        "type": "Document"
      },
      {
        "id": "9bbb5981-d2da-4825-be7f-156f2ab59ca7",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1809.04960",
          "start_idx": 13507,
          "end_idx": 13972
        },
        "page_content": " and paired data INLINEFORM4 . We select a large-scale Chinese dataset BIBREF0 with millions of real comments and a human-annotated test set to evaluate our model. The dataset is collected from Tencent News, which is one of the most popular Chinese websites for news and opinion articles. The dataset consists of 198,112 news articles. Each piece of news contains a title, the content of the article, and a list of the users' comments. Following the previous work B",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What MC abbreviate for?",
    "llm_answer": "The provided context does not contain an abbreviation for MC.\n",
    "context": [
      {
        "id": "c60fcc2c-c26e-46c1-b058-1fe76c5191fd",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1910.14497",
          "start_idx": 17746,
          "end_idx": 18046
        },
        "page_content": "salary\", \"office\", \"business\", \"career\", \"industry\", \"company\", \"promotion\", \"profession\", \"CEO\", \"manager\", \"coworker\", \"entrepreneur\" B: \"home\", \"parents\", \"children\", \"family\", \"cousins\", \"marriage\", \"wedding\", \"relatives\", \"grandparents\", \"grandchildren\", \"nurture\", \"child\", \"toddler\", \"infant\",",
        "type": "Document"
      },
      {
        "id": "696cba26-6ae8-4137-9c94-8634af01f0b2",
        "metadata": {
          "chunk_id": 57,
          "document_id": "1910.14497",
          "start_idx": 17282,
          "end_idx": 17564
        },
        "page_content": "tten\", \"vomit\", \"agony\", \"prison\" Male vs Female / Career vs Family: X: \"brother\", \"father\", \"uncle\", \"grandfather\", \"son\", \"he\", \"his\", \"him\", \"man\", \"himself\", \"men\", \"husband\", \"boy\", \"uncle\", \"nephew\", \"boyfriend\", \"king\", \"actor\" Y: \"sister\", \"mother\", \"aunt\", \"grandmother\", \"",
        "type": "Document"
      },
      {
        "id": "1fc74f07-214e-49bf-a7e3-ee60ccfc2eb8",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1910.14497",
          "start_idx": 12004,
          "end_idx": 12213
        },
        "page_content": "\ufffd, \u2019pregnancy\u2019, \u2019eva\u2019, \u2019pageant\u2019, \u2019distress\u2019, \u2019cristina\u2019, \u2019ida\u2019, \u2019beauty\u2019, \u2019sexuality\u2019,\u2019fertility\u2019 'accountant', 'acquaintance', 'actor', 'actress', 'administrator', 'adventurer', 'advocate', 'aide', 'alderman",
        "type": "Document"
      },
      {
        "id": "1520d14d-bfb8-4192-9cd9-8f2ebddd21b0",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1910.14497",
          "start_idx": 19137,
          "end_idx": 19387
        },
        "page_content": "\", \"son\", \"he\", \"his\", \"him\", \"man\", \"himself\", \"men\", \"husband\", \"boy\", \"uncle\", \"nephew\", \"boyfriend\" B: \"sister\", \"mother\", \"aunt\", \"grandmother\", \"daughter\", \"she\", \"hers\", \"her\", \"woman\", \"herself\", \"women\", \"wife\", \"aunt\", \"niece\", \"girlfriend\"",
        "type": "Document"
      },
      {
        "id": "e2f9d1b0-e0b1-43ac-bc86-8c2d84932673",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1910.14497",
          "start_idx": 17564,
          "end_idx": 17865
        },
        "page_content": " \"actor\" Y: \"sister\", \"mother\", \"aunt\", \"grandmother\", \"daughter\", \"she\", \"hers\", \"her\", \"woman\", \"herself\", \"women\", \"wife\", \"aunt\", \"niece\", \"girlfriend\", \"queen\", \"actress\" A: \"executive\", \"management\", \"professional\", \"corporation\", \"salary\", \"office\", \"business\", \"career\", \"industry\", \"company\",",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are their correlation results?",
    "llm_answer": "ROUGE-2 and ROUGE-SU4 are similar; ROUGE-SU4 is marginally better.  Precision and F1 are similar, but precision correlates better. Recall correlates poorly, and virtually not at all with the revised Kendall measure.  Pearson correlation for OR increases to 0.048.  WordSLOR and WPSLOR have the lowest MSE, with WordSLOR slightly better.  GE-LM performs better than ROUGE-L-mult and WPSLOR.  Pearson and MSE are both lower for results per compression system.\n",
    "context": [
      {
        "id": "6a7109e3-7475-449a-8a22-d7dd6533ba57",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1809.08731",
          "start_idx": 16548,
          "end_idx": 16964
        },
        "page_content": " correlations for all models but different domains (cf. Table 5 ), we first observe that the results across domains are similar, i.e., we do not observe the same effect as in Subsection \"Analysis I: Fluency Evaluation per Compression System\" . This is due to the distributions of scores being uniform ( $\\text{Var}(Y) \\in [0.28, 0.36]$ ). Next, we focus on an important question: How much does the performance of our",
        "type": "Document"
      },
      {
        "id": "1389fee1-3bec-4d90-ac24-94416fb2ed04",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1909.00542",
          "start_idx": 13913,
          "end_idx": 14353
        },
        "page_content": "PLAY_FORM37, where $S$ is the list of different sets. Table TABREF38 shows the results of all correlation metrics. Overall, ROUGE-2 and ROUGE-SU4 give similar correlation values but ROUGE-SU4 is marginally better. Among precision, recall and F1, both precision and F1 are similar, but precision gives a better correlation. Recall shows poor correlation, and virtually no correlation when using the revised Kendall measure. For reporting the",
        "type": "Document"
      },
      {
        "id": "34370da7-fcd6-41ec-a358-a7ff6fe29d91",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1809.08731",
          "start_idx": 13017,
          "end_idx": 13402
        },
        "page_content": "OR increases to $0.048$ for Pearson correlation. Note that, since having a single reference is very common, this result is highly relevant for practical applications. Considering MSE, the top two metrics are still WordSLOR and WPSLOR, with a $0.008$ and, respectively, $0.002$ lower error than the third best metric, ROUGE-L-mult.  Comparing WordSLOR and WPSLOR, we find no significant",
        "type": "Document"
      },
      {
        "id": "067c7284-a678-413f-ae31-26ee3ce7b5e7",
        "metadata": {
          "chunk_id": 33,
          "document_id": "1909.00542",
          "start_idx": 12727,
          "end_idx": 13230
        },
        "page_content": ", F1 for ROUGE-2 and ROUGE-SU4) against the average of the human scores. The correlation metrics are Pearson, Kendall, and a revised Kendall correlation explained below. The Pearson correlation between two variables is computed as the covariance of the two variables divided by the product of their standard deviations. This correlation is a good indication of a linear relation between the two variables, but may not be very effective when there is non-linear correlation. The Spearman rank correlation",
        "type": "Document"
      },
      {
        "id": "ce2d0eb1-b2ba-4232-a1dc-f5ed6911fde3",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1809.08731",
          "start_idx": 14995,
          "end_idx": 15436
        },
        "page_content": "GE-LM\u2014an uninformed combination\u2014performs significantly better than both ROUGE-L-mult and WPSLOR on their own, it should be the metric of choice for evaluating fluency with given references. The results per compression system (cf. Table 4 ) look different from the correlations in Table 3 : Pearson and MSE are both lower. This is due to the outputs of each given system being of comparable quality. Therefore, the datapoints are similar and,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What simpler models do they look at?",
    "llm_answer": "Distilled BERT-based models, smaller general-purpose language representation models pre-trained by leveraging distillation knowledge.\n",
    "context": [
      {
        "id": "338b0462-0627-4a30-a6bb-da4380439d78",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1703.04617",
          "start_idx": 3555,
          "end_idx": 4085
        },
        "page_content": " has the potential to capture long-distance interactions over the structures. Different types of questions are often used to seek for different types of information. For example, a \"what\" question could have very different property from that of a \"why\" question, while they may share information and need to be trained together instead of separately. We view this as a \"adaptation\" problem to let different types of questions share a basic model but still discriminate them when needed. Specifically, we are motivated by the ideas",
        "type": "Document"
      },
      {
        "id": "a32765f3-dc59-4597-be5b-7711061bf9d7",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1811.00942",
          "start_idx": 3335,
          "end_idx": 3833
        },
        "page_content": " however, their focus is not on the computational footprint of each model, but rather the perplexity. To further reduce perplexity, many neural language model extensions exist, such as continuous cache pointer BIBREF5 and mixture of softmaxes BIBREF6 . Since our focus is on comparing \u201ccore\u201d neural and non-neural approaches, we disregard these extra optimizations techniques in all of our models. Other work focus on designing lightweight models for resource-efficient inference on mobile devices.",
        "type": "Document"
      },
      {
        "id": "ba680f73-b8e3-418b-8a18-14a8c7646d58",
        "metadata": {
          "chunk_id": 57,
          "document_id": "1905.00563",
          "start_idx": 17255,
          "end_idx": 17740
        },
        "page_content": "-10 models are more robust than those for WN18. Looking at sample attacks (provided in Appendix \"Sample Adversarial Attacks\" ), mostly tries to change the type of the target object by associating it with a subject and a relation for a different entity type. Uncertain-Test To better understand the effect of attacks, we consider a subset of test triples that 1) the model predicts correctly, 2) difference between their scores and the negative sample with the highest score is minimum.",
        "type": "Document"
      },
      {
        "id": "2164b4e1-d758-4994-92ac-17bbce5a8959",
        "metadata": {
          "chunk_id": 11,
          "document_id": "2002.06644",
          "start_idx": 4295,
          "end_idx": 4797
        },
        "page_content": "QuAD for Question Answering. Distilled BERT-based models: Secondly, we propose to use distilled BERT-based models, as introduced in BIBREF8. They are smaller general-purpose language representation model, pre-trained by leveraging distillation knowledge. This results in significantly smaller and faster models with performance comparable to their undistilled versions. We finetune these pretrained distilled models on the training corpus to efficiently detect subjectivity. BERT-based ensemble models:",
        "type": "Document"
      },
      {
        "id": "54830273-3c90-417d-8898-e4c92c7fa94a",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1703.04617",
          "start_idx": 14584,
          "end_idx": 15140
        },
        "page_content": "ity. The previous models are often trained for all questions without explicitly discriminating different question types; however, for a target question, both the common features shared by all questions and the specific features for a specific type of question are further considered in this paper, as they could potentially obey different distributions. In this paper we further explicitly model different types of questions in the end-to-end training. We start from a simple way to first analyze the word frequency of all questions, and obtain top-10 most",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What linguistic quality aspects are addressed?",
    "llm_answer": "Fluency, and five criteria given in Figure FIGREF2.\n",
    "context": [
      {
        "id": "b3faad5f-9632-4a4f-a843-367d204e29af",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1909.00578",
          "start_idx": 710,
          "end_idx": 1242
        },
        "page_content": " use a pre-trained BERT model adding just a task-specific layer, and fine-tune the entire model on the task of predicting linguistic quality scores manually assigned to summaries. The five criteria addressed are given in Figure FIGREF2. We provide a thorough evaluation on three publicly available summarization datasets from NIST shared tasks, and compare the performance of our model to a wide variety of baseline methods capturing different aspects of linguistic quality. Sum-QE achieves very high correlations with human ratings",
        "type": "Document"
      },
      {
        "id": "f2bbf933-ddc1-46e7-9f9d-629baf2bcc68",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1809.08731",
          "start_idx": 812,
          "end_idx": 1267
        },
        "page_content": " evaluation, especially of fluency, causing word-overlap metrics like ROUGE BIBREF1 to correlate only weakly with human judgments BIBREF2 . As a result, fluency evaluation of NLG is often done manually, which is costly and time-consuming. Evaluating sentences on their fluency, on the other hand, is a linguistic ability of humans which has been the subject of a decade-long debate in cognitive science. In particular, the question has been raised whether",
        "type": "Document"
      },
      {
        "id": "ead559c6-47da-4fc5-9538-fb77b0805dbe",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1809.08731",
          "start_idx": 0,
          "end_idx": 505
        },
        "page_content": "Producing sentences which are perceived as natural by a human addressee\u2014a property which we will denote as fluency throughout this paper \u2014is a crucial goal of all natural language generation (NLG) systems: it makes interactions more natural, avoids misunderstandings and, overall, leads to higher user satisfaction and user trust BIBREF0 . Thus, fluency evaluation is important, e.g., during system development, or for filtering unacceptable generations at application time. However, fluency evaluation of",
        "type": "Document"
      },
      {
        "id": "92b8ff76-32ba-42a1-9fa9-25f66bf2e37f",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1809.08731",
          "start_idx": 5000,
          "end_idx": 5512
        },
        "page_content": " data they had created by translating and back-translating sentences with neural models. While they tried different types of LMs, best results were obtained for neural models, namely recurrent neural networks (RNNs). In this work, we investigate if approaches which have proven successful for modeling acceptability can be applied to the NLP problem of automatic fluency evaluation. In this section, we first describe SLOR and the intuition behind this score. Then, we introduce WordPieces, before explaining how",
        "type": "Document"
      },
      {
        "id": "652d4c94-eb07-4fa0-892e-4efa5677655e",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1809.08731",
          "start_idx": 20536,
          "end_idx": 21023
        },
        "page_content": " fluency evaluation in NLP has been rare. heilman2014predicting predicted the fluency (which they called grammaticality) of sentences written by English language learners. In contrast to ours, their approach is supervised. stent2005evaluating and cahill2009correlating found only low correlation between automatic metrics and fluency ratings for system-generated English paraphrases and the output of a German surface realiser, respectively. Explicit fluency evaluation of NLG, including",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What dataset do they use?",
    "llm_answer": "AIDA-Train, Wikipedia, FigureEight, ISEAR, Twitter Emotion Corpus, MCTest, CNN/Daily Mail, CBT, PubMed.\n",
    "context": [
      {
        "id": "241163c0-7b81-4ad7-a9ce-9a027f5fde6b",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1902.00330",
          "start_idx": 20691,
          "end_idx": 21180
        },
        "page_content": " AIDA-Train and Wikipedia data in the training set. Furthermore, we compare the RLEL with some baseline methods, where our model achieves the state-of-the-art results. We implement our models in Tensorflow and run experiments on 4 Tesla V100 GPU. We conduct experiments on several different types of public datasets including news and encyclopedia corpus. The training set is AIDA-Train and Wikipedia datasets, where AIDA-Train contains 18448 mentions and Wikipedia contains 25995 mentions",
        "type": "Document"
      },
      {
        "id": "6d0d2432-27ef-4993-bacc-25b807a31eb5",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1909.02764",
          "start_idx": 17673,
          "end_idx": 18041
        },
        "page_content": " We train models on a variety of corpora, namely the common format published by BIBREF27 of the FigureEight (formally known as Crowdflower) data set of social media, the ISEAR data BIBREF40 (self-reported emotional events), and, the Twitter Emotion Corpus (TEC, weakly annotated Tweets with #anger, #disgust, #fear, #happy, #sadness, and #surprise, Mohammad2012). From",
        "type": "Document"
      },
      {
        "id": "2222ff1c-48c2-4593-8cbe-f96acd7fec63",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1703.04617",
          "start_idx": 1343,
          "end_idx": 1868
        },
        "page_content": " can help attain better results on our competitive baselines. Recent advance on reading comprehension and question answering has been closely associated with the availability of various datasets. BIBREF0 released the MCTest data consisting of 500 short, fictional open-domain stories and 2000 questions. The CNN/Daily Mail dataset BIBREF1 contains news articles for close style machine comprehension, in which only entities are removed and tested for comprehension. Children's Book Test (CBT) BIBREF2 leverages named entities",
        "type": "Document"
      },
      {
        "id": "a4a4c58b-d1f1-46a1-9912-24f7a070ad68",
        "metadata": {
          "chunk_id": 43,
          "document_id": "2001.00137",
          "start_idx": 16781,
          "end_idx": 17197
        },
        "page_content": " the online platforms. Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector",
        "type": "Document"
      },
      {
        "id": "05d37bee-f941-488b-81c9-7aa089b2d5b4",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1909.00542",
          "start_idx": 2427,
          "end_idx": 2993
        },
        "page_content": " relevant concepts from designated terminologies and ontologies, relevant articles from PubMed, relevant snippets from the relevant articles, and relevant RDF triples from designated ontologies. Phase B provides a biomedical question and a list of relevant articles and snippets, and participant systems are expected to return the exact answers and the ideal answers. The training data is composed of the test data from all previous years, and amounts to 2,747 samples. There has been considerable research on the use of machine learning approaches for tasks related",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is typical GAN architecture for each text-to-image synhesis group?",
    "llm_answer": "The provided text describes *types* of GAN-based text-to-image synthesis frameworks, but does not detail *typical architectures* for each group.  The text mentions *designs and architectures* such as using multiple discriminators, progressively trained discriminators, and hierarchical discriminators, but does not specify architectures for each framework type.\n",
    "context": [
      {
        "id": "bf327ff6-f3db-4510-8a0c-f220417aef5d",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1910.09399",
          "start_idx": 5638,
          "end_idx": 6156
        },
        "page_content": ", as they are the engines that make text-to-image synthesis possible and are essential building blocks to achieve photo-realistic images from text descriptions. Section 4 proposes a taxonomy to summarize GAN based text-to-image synthesis, discusses models and architectures of novel works focused solely on text-to-image synthesis. This section will also draw key contributions from these works in relation to their applications. Section 5 reviews GAN based text-to-image synthesis benchmarks, performance metrics, and",
        "type": "Document"
      },
      {
        "id": "455fbc41-f086-4ec1-aa94-14c68cde91ec",
        "metadata": {
          "chunk_id": 62,
          "document_id": "1910.09399",
          "start_idx": 24164,
          "end_idx": 24615
        },
        "page_content": ". The fourth user group adds a new dimension in image synthesis, and aims to generate sequences of images which are coherent in temporal order, i.e. capture the motion information. black Based on the above descriptions, we categorize GAN based Text-to-Image Synthesis into a taxonomy with four major categories, as shown in Fig. FIGREF24. Semantic Enhancement GANs: Semantic enhancement GANs represent pioneer works of GAN frameworks for text-to-image",
        "type": "Document"
      },
      {
        "id": "68446b15-b861-47e8-a06d-0ed5f733d2d6",
        "metadata": {
          "chunk_id": 56,
          "document_id": "1910.09399",
          "start_idx": 21820,
          "end_idx": 22241
        },
        "page_content": ", but in this paper we focus specifically on models for text-to-image synthesis, with the review emphasizing on the \u201cmodel\u201d and \u201ccontributions\u201d for text-to-image synthesis. At the end of this section, we also briefly review methods using GANs for other image-synthesis applications. black In this section, we propose a taxonomy to summarize advanced GAN based text-to-image synthesis frameworks, as shown in Figure FIGREF",
        "type": "Document"
      },
      {
        "id": "d7cfb7a5-2332-468a-b4be-76f9c506c16f",
        "metadata": {
          "chunk_id": 52,
          "document_id": "1910.09399",
          "start_idx": 20138,
          "end_idx": 20664
        },
        "page_content": " to convert input text as numeric features, which are further concatenated with other features to train GANs. black Motivated by the GAN and conditional GAN (cGAN) design, many GAN based frameworks have been proposed to generate images, with different designs and architectures, such as using multiple discriminators, using progressively trained discriminators, or using hierarchical discriminators. Figure FIGREF17 outlines several advanced GAN frameworks in the literature. In addition to these frameworks, many news designs",
        "type": "Document"
      },
      {
        "id": "99629fd8-0402-44e0-8d52-bd1f3eb07449",
        "metadata": {
          "chunk_id": 165,
          "document_id": "1910.09399",
          "start_idx": 64023,
          "end_idx": 64502
        },
        "page_content": " paper, we reviewed novel methods that generate, in our opinion, the most visually-rich and photo-realistic images, from text-based natural language. These generated images often rely on generative adversarial networks (GANs), deep convolutional decoder networks, and multimodal learning methods. blackIn the paper, we first proposed a taxonomy to organize GAN based text-to-image synthesis frameworks into four major groups: semantic enhancement GANs, resolution enhancement GAN",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What language do the agents talk in?",
    "llm_answer": "Natural language.\n",
    "context": [
      {
        "id": "8c213ab0-1b58-4bc4-b1d4-8293f925ebda",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1909.02764",
          "start_idx": 3194,
          "end_idx": 3734
        },
        "page_content": " speech interactions, the study was conducted in a driving simulator. Based on the collected data, we provide baseline predictions with off-the-shelf tools for face and speech emotion recognition and compare them to a neural network-based approach for emotion recognition from text. Our second contribution is the introduction of transfer learning to adapt models trained on established out-of-domain corpora to our use case. We work on German language, therefore the transfer consists of a domain and a language transfer. A common approach",
        "type": "Document"
      },
      {
        "id": "e9a51339-8874-4210-8a0c-8383aa942af2",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1807.03367",
          "start_idx": 1550,
          "end_idx": 2025
        },
        "page_content": " humans. Hence, we argue that we should incorporate all three fundamental aspects of human language acquisition\u2014perception, action and interactive communication\u2014and develop a task and dataset to that effect. We introduce the Talk the Walk dataset, where the aim is for two agents, a \u201cguide\u201d and a \u201ctourist\u201d, to interact with each other via natural language in order to achieve a common goal: having the tourist navigate towards the correct location. The guide has access to a",
        "type": "Document"
      },
      {
        "id": "e8fb7d0e-86e1-4332-bb4a-d8109e4146e4",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1910.04269",
          "start_idx": 19553,
          "end_idx": 20023
        },
        "page_content": ", the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Kat",
        "type": "Document"
      },
      {
        "id": "7da029d3-0e6a-48d3-9dd2-3cfd37690294",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1910.04269",
          "start_idx": 6853,
          "end_idx": 7342
        },
        "page_content": " comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (",
        "type": "Document"
      },
      {
        "id": "e5865879-f7ff-4b02-af83-5a27fb275cd8",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1910.04269",
          "start_idx": 0,
          "end_idx": 512
        },
        "page_content": "Language Identification (LI) is a problem which involves classifying the language being spoken by a speaker. LI systems can be used in call centers to route international calls to an operator who is fluent in that identified language BIBREF0. In speech-based assistants, LI acts as the first step which chooses the corresponding grammar from a list of available languages for its further semantic analysis BIBREF1. It can also be used in multi-lingual voice-controlled information retrieval systems, for example,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much better is performance of proposed method than state-of-the-art methods in experiments?",
    "llm_answer": "The context shows the proposed method outperforming Coppersmith et al.'s method in some experiments, and the three fine-tuned BERT versions in others.  It also states that multi-task versions of the proposed method often perform better than single-task versions.  However, a precise comparison to *all* state-of-the-art methods is not provided.\n",
    "context": [
      {
        "id": "5c46d1e3-00dd-432e-b58a-1aaa94f3928f",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1909.00542",
          "start_idx": 10682,
          "end_idx": 11217
        },
        "page_content": " experiments. We ran the version trained on Python ROUGE once, and the version trained on Perl twice. The two Perl runs have different results, and one of them clearly outperforms the Python run. However, given the differences of results between the two Perl runs we advice to re-run the experiments multiple times and obtain the mean and standard deviation of the runs before concluding whether there is any statistical difference between the results. But it seems that there may be an improvement of the final evaluation results when",
        "type": "Document"
      },
      {
        "id": "309bcaa2-ef9d-45b3-9b52-81e934a26747",
        "metadata": {
          "chunk_id": 66,
          "document_id": "2003.07433",
          "start_idx": 25731,
          "end_idx": 26161
        },
        "page_content": "REF28 illustrates the comparisons between LAXARY and Coppersmith et. al. proposed method. Here we can see, the outperformance of our proposed method as well as the importance of $s-score$ estimation. We also illustrates the importance of $\\alpha -score$ and $S-score$ in Fig FIGREF30. Fig FIGREF30 illustrates that if we change the number of training samples (%), LAXARY models outperforms Coppersmith et. al. proposed model under",
        "type": "Document"
      },
      {
        "id": "6cf0a06f-34c6-44b2-a329-2285cf3a8bc8",
        "metadata": {
          "chunk_id": 57,
          "document_id": "1910.04269",
          "start_idx": 21961,
          "end_idx": 22224
        },
        "page_content": " which is a recently developed technique to prevent over\ufb01tting on test data.This approach achieved an accuracy of 95.4%. We also analysed how attention mechanism and recurrent layers impact the performance of networks. This approach achieved an accuracy of 95.0%.",
        "type": "Document"
      },
      {
        "id": "1d6367cf-46af-402b-80b1-8d986b737935",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1811.00942",
          "start_idx": 13604,
          "end_idx": 14041
        },
        "page_content": " for the task of language modeling. Specifically, we explore the quality\u2013performance tradeoffs between KN-5, a non-neural approach, and AWD-LSTM and QRNN, two neural language models. We find that with decreased perplexity comes vastly increased computational requirements: In one of the NLMs, a perplexity reduction by 2.5 $\\times $ results in a 49 $\\times $ rise in latency and 32 $\\times $ increase in energy usage, when compared to KN",
        "type": "Document"
      },
      {
        "id": "97194657-ec5f-4398-9cc9-11be4fc5df47",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1909.00578",
          "start_idx": 8968,
          "end_idx": 9354
        },
        "page_content": " the training set of each fold. Table TABREF23 shows Spearman's $\\rho $, Kendall's $\\tau $ and Pearson's $r$ for all datasets and models. The three fine-tuned BERT versions clearly outperform all other methods. Multi-task versions seem to perform better than single-task ones in most cases. Especially for $\\mathcal {Q}4$ and $\\mathcal {Q}5$, which are highly correlated, the multi-task",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What baseline model is used?",
    "llm_answer": "TF-IDF\n",
    "context": [
      {
        "id": "7d4754c7-4ea8-4d20-9fb6-3d1e50bf0cf6",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1809.04960",
          "start_idx": 22497,
          "end_idx": 22960
        },
        "page_content": " the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types of comments as described in the above retrieval evaluation setting: Correct, Plausible, Popular, and Random. We rank the candidate comment set with four models (TF-IDF, S2S, IR, and Proposed+IR), and record the types of top-1 comments. Figure FIGREF40 shows the percentage of different types of top-1 comments generated by each model. It shows",
        "type": "Document"
      },
      {
        "id": "8937964f-11b5-487b-9960-db106025360c",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1909.02764",
          "start_idx": 17421,
          "end_idx": 17831
        },
        "page_content": " layer. This setup is inspired by BIBREF38. We use a dropout rate of 0.3 in all layers and optimize with Adam BIBREF39 with a learning rate of $10^{-5}$ (These parameters are the same for all further experiments). We build on top of the Keras library with the TensorFlow backend. We consider this setup our baseline model. We train models on a variety of corpora, namely the common format published by BIBREF27",
        "type": "Document"
      },
      {
        "id": "7a4bec79-566e-4543-bc20-7faebdd813e6",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1703.04617",
          "start_idx": 3849,
          "end_idx": 4374
        },
        "page_content": " questions share a basic model but still discriminate them when needed. Specifically, we are motivated by the ideas \"i-vector\" BIBREF18 in speech recognition, where neural network based adaptation is performed among different (groups) of speakers and we focused instead on different types of questions here. Our baseline model is composed of the following typical components: word embedding, input encoder, alignment, aggregation, and prediction. Below we discuss these components in more details. We concatenate embedding at",
        "type": "Document"
      },
      {
        "id": "8a126462-c5fa-46d2-8d8b-c57956c37c65",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1809.04960",
          "start_idx": 14887,
          "end_idx": 15333
        },
        "page_content": " and use batch normalization BIBREF9 in each layer. We also gradually increase the KL term from 0 to 1 after each epoch. We compare our model with several unsupervised models and supervised models. Unsupervised baseline models are as follows: TF-IDF (Lexical, Non-Neural) is an important unsupervised baseline. We use the concatenation of the title and the body as the query to retrieve the candidate comment set by means of the similarity of the",
        "type": "Document"
      },
      {
        "id": "26fdc38f-842b-4ef6-9e26-d6482bc535be",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1910.04269",
          "start_idx": 12971,
          "end_idx": 13505
        },
        "page_content": " its hyperparameter. We took some specific design choices for this model, which are as follows: We added residual connections with each convolutional layer. Residual connections in a way makes the model selective of the contributing layers, determines the optimal number of layers required for training and solves the problem of vanishing gradients. Residual connections or skip connections skip training of those layers that do not contribute much in the overall outcome of model. We added spatial attention BIBREF27 networks to help",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What stylistic features are used to detect drunk texts?",
    "llm_answer": "Capitalization, spelling errors, etc.\n",
    "context": [
      {
        "id": "caca8783-6f55-4f5d-bf46-dcf3aaffc14a",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1610.00879",
          "start_idx": 7309,
          "end_idx": 7708
        },
        "page_content": "ifier is 78.1% for all features, and 75.6% for stylistic features. This shows that text-based clues can indeed be used for drunk-texting prediction. Using held-out dataset H, we evaluate how our system performs in comparison to humans. Three annotators, A1-A3, mark each tweet in the Dataset H as drunk or sober. Table TABREF19 shows a moderate agreement between our annotators (for example, it is 0",
        "type": "Document"
      },
      {
        "id": "a8df2ab8-0463-4626-a31b-618b91a7658c",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1610.00879",
          "start_idx": 1216,
          "end_idx": 1654
        },
        "page_content": " (c) most discriminative stylistic features, and (d) an error analysis that points to future work. To the best of our knowledge, this is a first study that shows the feasibility of text-based analysis for drunk-texting prediction. Past studies show the relation between alcohol abuse and unsociable behaviour such as aggression BIBREF0 , crime BIBREF1 , suicide attempts BIBREF2 , drunk driving BIBREF3 , and risky sexual behaviour BIBREF",
        "type": "Document"
      },
      {
        "id": "86d56e0f-4f11-4c7e-8266-24a4cd77c6bb",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1610.00879",
          "start_idx": 2308,
          "end_idx": 2814
        },
        "page_content": " of social media text and not explicitly seeking user input. Thus, automatic drunk-texting prediction will improve systems aimed to avoid regrettable drunk-texting. To the best of our knowledge, ours is the first study that does a quantitative analysis, in terms of prediction of the drunk state by using textual clues. Several studies have studied linguistic traits associated with emotion expression and mental health issues, suicidal nature, criminal status, etc. BIBREF5 , BIBREF6 . NLP techniques have",
        "type": "Document"
      },
      {
        "id": "a882d0eb-da89-45ea-b5e8-a66b187ba99a",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1610.00879",
          "start_idx": 9169,
          "end_idx": 9660
        },
        "page_content": " for drunk-texting prediction as means of identifying risky social behavior arising out of alcohol abuse, and the need to build tools that avoid privacy leaks due to drunk-texting. We then highlight the challenges of drunk-texting prediction: one of the challenges is selection of negative examples (sober tweets). Using hashtag-based supervision, we create three datasets annotated with drunk or sober labels. We then present SVM-based classifiers which use two sets of features: N-gram and",
        "type": "Document"
      },
      {
        "id": "f87b80a2-b9d1-4606-a85b-ee7e3a4d8cc4",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1610.00879",
          "start_idx": 758,
          "end_idx": 1216
        },
        "page_content": " datasets by using different strategies that are related to the use of hashtags. We then present SVM-based classifiers that use N-gram and stylistic features such as capitalisation, spelling errors, etc. Through our experiments, we make subtle points related to: (a) the performance of our features, (b) how our approach compares against human ability to detect drunk-texting, (c) most discriminative stylistic features, and (d) an error analysis that points",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the accuracy of the proposed technique?",
    "llm_answer": "95.4%\n",
    "context": [
      {
        "id": "6cf0a06f-34c6-44b2-a329-2285cf3a8bc8",
        "metadata": {
          "chunk_id": 57,
          "document_id": "1910.04269",
          "start_idx": 21961,
          "end_idx": 22224
        },
        "page_content": " which is a recently developed technique to prevent over\ufb01tting on test data.This approach achieved an accuracy of 95.4%. We also analysed how attention mechanism and recurrent layers impact the performance of networks. This approach achieved an accuracy of 95.0%.",
        "type": "Document"
      },
      {
        "id": "586f9914-737b-4c1c-a186-96eb5f527202",
        "metadata": {
          "chunk_id": 50,
          "document_id": "2001.00137",
          "start_idx": 18932,
          "end_idx": 19320
        },
        "page_content": " one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80$\\%$ against BERT's 72$\\%$. The second version (Corr) considers the corrected tweets, and shows higher accuracy given that it is less noisy. In that version, Stacked DeBERT achieves 82$\\%$ accuracy against BERT's 76$\\%$, an improvement of 6$\\%$. In the last case (Inc+Corr), we",
        "type": "Document"
      },
      {
        "id": "c10b4ffc-1960-489e-866a-5f1c84ec57ff",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1905.00563",
          "start_idx": 13739,
          "end_idx": 14244
        },
        "page_content": " evaluate the quality of our approximations and compare with influence function (IF), we conduct leave one out experiments. In this setup, we take all the neighbors of a random target triple as candidate modifications, remove them one at a time, retrain the model each time, and compute the exact change in the score of the target triple. We can use the magnitude of this change in score to rank the candidate triples, and compare this exact ranking with ranking as predicted by: , influence function with",
        "type": "Document"
      },
      {
        "id": "f642a979-b7f0-421c-ac6a-b71015c46b0d",
        "metadata": {
          "chunk_id": 78,
          "document_id": "1902.00330",
          "start_idx": 24702,
          "end_idx": 25205
        },
        "page_content": "1 to evaluate our method. We first test the model on the AIDA-B dataset. From Table 2, we can observe that our model achieves the best result. Previous best results on this dataset are generated by BIBREF0 , BIBREF1 which both built CRF models. They calculate the pairwise scores between all candidate entities. Differently, our model only considers the consistency of the target entities and ignores the relationship between incorrect candidates. The experimental results show that our model can reduce",
        "type": "Document"
      },
      {
        "id": "5fffe60f-0f11-465b-a9af-8f7c895ff3c1",
        "metadata": {
          "chunk_id": 65,
          "document_id": "2003.07433",
          "start_idx": 25419,
          "end_idx": 25905
        },
        "page_content": " score of 0, 1, 2 and 3 respectively. Table TABREF29 shows the classification details of our experiment which provide the very good accuracy of our classification. To compare the outperformance of our method, we also implemented Coppersmith et. al. proposed method and achieved an 86% overall accuracy of detecting PTSD users BIBREF11 following the same training-test dataset distribution. Fig FIGREF28 illustrates the comparisons between LAXARY and Coppersmith et. al. proposed method.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which retrieval system was used for baselines?",
    "llm_answer": "TF-IDF, S2S, IR, and Proposed+IR\n",
    "context": [
      {
        "id": "26654c16-c460-4900-b78d-5ad48b826e37",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1909.00542",
          "start_idx": 15059,
          "end_idx": 15516
        },
        "page_content": " the results and details of the runs submitted to BioASQ. The table uses ROUGE-SU4 Recall since this is the metric available at the time of writing this paper. However, note that, as explained in Section SECREF6, these results might differ from the final human evaluation results. Therefore we do not comment on the results, other than observing that the \u201cfirst $n$\u201d baseline produces the same results as the neural regressor. As mentioned in Section SECREF",
        "type": "Document"
      },
      {
        "id": "3f956a4e-f9f6-4339-b75c-d529dbc23a27",
        "metadata": {
          "chunk_id": 53,
          "document_id": "1707.03904",
          "start_idx": 20086,
          "end_idx": 20524
        },
        "page_content": " \u2013 finding the few most relevant ones will allow the reader to work best. In Tables 2 and 3 we compare all baselines when the context size is tuned to maximize the overall accuracy on the validation set. For Quasar-S the best performing baseline is the BiRNN language model, which achieves $33.6\\%$ accuracy. The GA model achieves $48.3\\%$ accuracy on the set of instances for which the answer is in context, however, a search accuracy of",
        "type": "Document"
      },
      {
        "id": "7d4754c7-4ea8-4d20-9fb6-3d1e50bf0cf6",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1809.04960",
          "start_idx": 22497,
          "end_idx": 22960
        },
        "page_content": " the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types of comments as described in the above retrieval evaluation setting: Correct, Plausible, Popular, and Random. We rank the candidate comment set with four models (TF-IDF, S2S, IR, and Proposed+IR), and record the types of top-1 comments. Figure FIGREF40 shows the percentage of different types of top-1 comments generated by each model. It shows",
        "type": "Document"
      },
      {
        "id": "86a8a3a2-fdfa-4e01-b118-bdbd4e810445",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1704.05572",
          "start_idx": 10856,
          "end_idx": 11303
        },
        "page_content": " powerful information-retrieval baseline BIBREF6 that selects the answer option with the best matching sentence in a corpus. TableILP is the state-of-the-art structured inference baseline BIBREF9 developed for science questions. Table 2 shows that TupleInf, with no curated knowledge, outperforms TableILP on both question sets by more than 11%. The lower half of the table shows that even when both solvers are given the same knowledge (C+T), the",
        "type": "Document"
      },
      {
        "id": "0f21bb8b-5779-4444-a6aa-dcdf33cd0ed7",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1704.05572",
          "start_idx": 10656,
          "end_idx": 11029
        },
        "page_content": " the IR solver and also used to create the tuple KB T and on-the-fly tuples $T^{\\prime }_{qa}$ . Additionally, TableILP uses $\\sim $ 70 Curated tables (C) designed for 4th grade NY Regents exams. We compare TupleInf with two state-of-the-art baselines. IR is a simple yet powerful information-retrieval baseline BIBREF6 that selects the answer option with the best matching",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much better was the BLSTM-CNN-CRF than the BLSTM-CRF?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "88d70074-2e19-4109-8c84-f28320fec049",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1911.07228",
          "start_idx": 7484,
          "end_idx": 7865
        },
        "page_content": " about 200k words Based on state-of-the-art methods for NER, BLSTM-CNN-CRF is the end-to-end deep neural network model that achieves the best result on F-score BIBREF9. Therefore, we decide to conduct the experiment on this model and analyze the errors. We run experiment with the Ma and Hovy (2016) model BIBREF8, source code provided by (Motoki Sato) and analysis the errors from",
        "type": "Document"
      },
      {
        "id": "f904e2e0-cddb-4c77-b068-a17ebc2fc287",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1911.07228",
          "start_idx": 8371,
          "end_idx": 8784
        },
        "page_content": " 3 shows perfomance of the BLSTM-CNN-CRF model. In our experiments, we use three evaluation parameters (precision, recall, and F1 score) to access our experimental result. They will be described as follow in Table 3. The \"correctNE\", the number of correct label for entity that the model can found. The \"goldNE\", number of the real label annotated by annotator in the gold data. The \"foundNE\", number of the label",
        "type": "Document"
      },
      {
        "id": "d4a8b56d-8390-4cdf-90d5-ec47c44546e9",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1911.07228",
          "start_idx": 2956,
          "end_idx": 3283
        },
        "page_content": "ard Hovy with BLSTM-CNN-CRF in BIBREF8 achieved F1 score of 91.21, Thai-Hoang Pham and Phuong Le-Hong with BLSTM-CNN-CRF in BIBREF9 got 88.59% F1 score. These DNN models are also the state-of-the-art models. The results of our analysis experiments are reported in precision and recall over all labels (name of person, location,",
        "type": "Document"
      },
      {
        "id": "4ecb3e96-1eab-40cf-9214-a565589e62b7",
        "metadata": {
          "chunk_id": 30,
          "document_id": "1911.07228",
          "start_idx": 8165,
          "end_idx": 8527
        },
        "page_content": " models with and without different pre-trained word embedding \u2013 KP means the Kyubyong Park\u2019s pre-trained word embeddings and EG means Edouard Grave\u2019s pre-trained word embeddings. We compare the outputs of BLSTM-CNN-CRF model (predicted) to the annotated data (gold) and analyzed the errors. Table 3 shows perfomance of the BLSTM-CNN-CRF model. In our experiments",
        "type": "Document"
      },
      {
        "id": "a2d9d185-aa9e-429c-a8ae-d7bb6c5fd8dd",
        "metadata": {
          "chunk_id": 16,
          "document_id": "2002.06644",
          "start_idx": 6017,
          "end_idx": 6467
        },
        "page_content": " along with a recurrent dropout of $0.2$ in two 64 unit sized stacked BiLSTMs, using softmax activation layer as the final dense layer. tab:experimental-results shows the performance of different models on the WNC corpus evaluated on the following four metrics: Precision, Recall, F1, and Accuracy. Our proposed methodology, the use of finetuned optimized BERT based models, and BERT-based ensemble models outperform the baselines for all the metrics",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What supplemental tasks are used for multitask learning?",
    "llm_answer": "Task A, task B, and task C.\n",
    "context": [
      {
        "id": "eb63dfd0-f0ba-4a84-9655-6ef91f544f87",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1603.07044",
          "start_idx": 8235,
          "end_idx": 8649
        },
        "page_content": " (e.g., the ranking score from the IR system) to enhance the classifier. So the final input to the classifier will be $h_N$ , $h^{\\prime }$ , as well as augmented features. For task C, in addition to an original question (oriQ) and an external comment (relC), the question which relC commented on is also given (relQ). To incorporate this extra information, we consider a multitask learning framework which jointly",
        "type": "Document"
      },
      {
        "id": "548f933a-c98a-4258-8c9d-95d0d285426b",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1603.07044",
          "start_idx": 12962,
          "end_idx": 13403
        },
        "page_content": "-External Comments\" , we also explored a multitask learning framework that jointly learns to predict the relationships of all three tasks. We set $0.8$ for the main task (task C) and $0.1$ for the other auxiliary tasks. The MAP score did not improve, but F1 increases to $0.1617$ . We believe this is because other tasks have more balanced labels, which improves the shared parameters for task C. There are many sources of external question-",
        "type": "Document"
      },
      {
        "id": "3becfb0c-a598-4ab9-890b-5f1dbbf6f959",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1909.08859",
          "start_idx": 17614,
          "end_idx": 18113
        },
        "page_content": " We used Adam optimizer with a learning rate of 1e-4 with an early stopping criteria with the patience set to 10 indicating that the training procedure ends after 10 iterations if the performance would not improve. We considered a batch size of 32 due to our hardware constraints. In the multi-task setting, batches are sampled round-robin from all tasks, where each batch is solely composed of examples from one task. We performed our experiments on a system containing four NVIDIA GTX-1080Ti GPUs,",
        "type": "Document"
      },
      {
        "id": "0fd8b447-bb11-4afd-8aa0-b8f09ddd6d71",
        "metadata": {
          "chunk_id": 57,
          "document_id": "1909.08859",
          "start_idx": 19980,
          "end_idx": 20459
        },
        "page_content": " a single model is trained to solve all the tasks at once, PRN and BIDAF w/ static memory perform comparably and give much better results than BIDAF. Note that the model performances in the multi-task training setting are worse than single-task performances. We believe that this is due to the nature of the tasks that some are more difficult than the others. We think that the performance could be improved by employing a carefully selected curriculum strategy BIBREF20. In Fig.",
        "type": "Document"
      },
      {
        "id": "cd8c40ea-2e1c-4b78-9aa2-6471751bd592",
        "metadata": {
          "chunk_id": 60,
          "document_id": "2002.11893",
          "start_idx": 22325,
          "end_idx": 22851
        },
        "page_content": " high quality of the collected dialogues. Explicit annotations are provided at not only the system side but also the user side. This feature allows us to model user behaviors or develop user simulators more easily. CrossWOZ can be used in different tasks or settings of a task-oriented dialogue system. To facilitate further research, we provided benchmark models for different components of a pipelined task-oriented dialogue system (Figure FIGREF32), including natural language understanding (NLU), dialogue state tracking (",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much performance gap between their approach and the strong handcrafted method?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "97194657-ec5f-4398-9cc9-11be4fc5df47",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1909.00578",
          "start_idx": 8968,
          "end_idx": 9354
        },
        "page_content": " the training set of each fold. Table TABREF23 shows Spearman's $\\rho $, Kendall's $\\tau $ and Pearson's $r$ for all datasets and models. The three fine-tuned BERT versions clearly outperform all other methods. Multi-task versions seem to perform better than single-task ones in most cases. Especially for $\\mathcal {Q}4$ and $\\mathcal {Q}5$, which are highly correlated, the multi-task",
        "type": "Document"
      },
      {
        "id": "309bcaa2-ef9d-45b3-9b52-81e934a26747",
        "metadata": {
          "chunk_id": 66,
          "document_id": "2003.07433",
          "start_idx": 25731,
          "end_idx": 26161
        },
        "page_content": "REF28 illustrates the comparisons between LAXARY and Coppersmith et. al. proposed method. Here we can see, the outperformance of our proposed method as well as the importance of $s-score$ estimation. We also illustrates the importance of $\\alpha -score$ and $S-score$ in Fig FIGREF30. Fig FIGREF30 illustrates that if we change the number of training samples (%), LAXARY models outperforms Coppersmith et. al. proposed model under",
        "type": "Document"
      },
      {
        "id": "5c46d1e3-00dd-432e-b58a-1aaa94f3928f",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1909.00542",
          "start_idx": 10682,
          "end_idx": 11217
        },
        "page_content": " experiments. We ran the version trained on Python ROUGE once, and the version trained on Perl twice. The two Perl runs have different results, and one of them clearly outperforms the Python run. However, given the differences of results between the two Perl runs we advice to re-run the experiments multiple times and obtain the mean and standard deviation of the runs before concluding whether there is any statistical difference between the results. But it seems that there may be an improvement of the final evaluation results when",
        "type": "Document"
      },
      {
        "id": "b35502d9-d2c1-413d-bbfd-58034f1724ed",
        "metadata": {
          "chunk_id": 18,
          "document_id": "2002.06644",
          "start_idx": 6707,
          "end_idx": 7139
        },
        "page_content": ", a distilled model, performs competitively, achieving $69.69\\%$ accuracy, and $0.672$ F1 score. This observation shows that distilled pretrained models can replace their undistilled counterparts in a low-computing environment. We further observe that ensemble models perform better than optimized BERT-based models and distilled pretrained models. Our proposed ensemble comprising of $RoBERTa_{large}$, $ALBERT_{xxlarge.v2}$, $Dist",
        "type": "Document"
      },
      {
        "id": "4f21d9f4-3aef-4f6c-8ec7-e4c742b14161",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1908.06606",
          "start_idx": 22920,
          "end_idx": 23336
        },
        "page_content": " performance on distal resection margin improved by 5.56% in EM-score and 6.32% in F$_1$-score. Other performances also usually improved a lot. This proves the usefulness of two-stage training and named entity information as well. Lastly, we fine tune the model for each task with a pre-trained parameter. Table TABREF32 summarizes the result. (Add some explanations for the Table TABREF32). Comparing Table TABREF32",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is their model?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "36dc5e74-3ea0-4b03-9b0b-f67f735f5ffe",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1902.09314",
          "start_idx": 16362,
          "end_idx": 16777
        },
        "page_content": " is a lightweight alternative of recurrent models, we study the model size of each model on the Restaurant dataset. Statistical results are reported in Table TABREF37 . We implement all the compared models base on the same source code infrastructure, use the same hyperparameters, and run them on the same GPU . RNN-based and BERT-based models indeed have larger model size. ATAE-LSTM, IAN, RAM, and AEN-GloVe-BiLST",
        "type": "Document"
      },
      {
        "id": "9ccb7945-9a96-417a-b52e-68855c4910b1",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "93b69f2a-888f-460d-aad3-b4c7a2125c64",
        "metadata": {
          "chunk_id": 96,
          "document_id": "1807.03367",
          "start_idx": 36310,
          "end_idx": 36688
        },
        "page_content": ") in order to solve a task, with respect to their shared environment BIBREF35 , BIBREF21 , BIBREF41 , BIBREF24 , BIBREF36 , BIBREF47 , BIBREF34 . For the emergent communication models, we use an embedding size INLINEFORM0 . The natural language experiments use 128-dimensional word embeddings and a bidirectional RNN with 256 units. In all experiments, we train the guide with a",
        "type": "Document"
      },
      {
        "id": "fb85c318-f6d9-48b7-982a-9106ee5d2aec",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1909.00694",
          "start_idx": 15395,
          "end_idx": 15766
        },
        "page_content": " segmented into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam BIBREF19. The mini-batch size was 32. We ran 1 epoch.",
        "type": "Document"
      },
      {
        "id": "858c4c70-af74-4b03-becd-ca1ea2c47b78",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1811.00942",
          "start_idx": 8253,
          "end_idx": 8645
        },
        "page_content": " specific hyperparameters are the same as those in the repository. Our model architecture consists of 400-dimensional tied embedding weights BIBREF18 and four QRNN layers, with 1550 hidden units per layer on PTB and 2500 per layer on WT103. Both QRNN models have window sizes of $r=2$ for the first layer and $r=1$ for the rest. For the KN-5 model, we trained an off-the-shelf five-gram model",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many emotions do they look at?",
    "llm_answer": "24\n",
    "context": [
      {
        "id": "2f8ed056-a4f8-4c11-9900-32b955f68847",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1909.02764",
          "start_idx": 15881,
          "end_idx": 16327
        },
        "page_content": " seconds, with the minimum at 2.2s and the maximum at 54.7s. We apply an off-the-shelf tool for emotion recognition (the manufacturer cannot be disclosed due to licensing restrictions). It delivers frame-by-frame scores ($\\in [0;100]$) for discrete emotional states of joy, anger and fear. While joy corresponds directly to our annotation, we map anger to our label annoyance and fear to our label insecurity. The maximal average score across all",
        "type": "Document"
      },
      {
        "id": "2e0bc3f3-be71-4353-8f89-2d3c57752c00",
        "metadata": {
          "chunk_id": 41,
          "document_id": "1909.02764",
          "start_idx": 16704,
          "end_idx": 17205
        },
        "page_content": " set of 24 discrete emotions for the entire utterance. We consider the outputs for the states of joy, anger, and fear, mapping analogously to our classes as for facial expressions. Low-confidence predictions are interpreted as \u201cno emotion\u201d. We accept the emotion with the highest score as the discrete prediction otherwise. For the emotion recognition from text, we manually transcribe all utterances of our AMMER study. To exploit existing and available data sets which are larger than the AMMER data",
        "type": "Document"
      },
      {
        "id": "5181f499-955c-4c08-afb2-f0fc8dd907d1",
        "metadata": {
          "chunk_id": 40,
          "document_id": "1909.02764",
          "start_idx": 16157,
          "end_idx": 16704
        },
        "page_content": " we map anger to our label annoyance and fear to our label insecurity. The maximal average score across all frames constitutes the overall classification for the video sequence. Frames where the software is not able to detect the face are ignored. We extract the audio signal for the same sequence as described for facial expressions and apply an off-the-shelf tool for emotion recognition. The software delivers single classification scores for a set of 24 discrete emotions for the entire utterance. We consider the outputs for the states of joy",
        "type": "Document"
      },
      {
        "id": "705d6faf-f2bc-4044-90ff-8ed3dd6564ec",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1909.02764",
          "start_idx": 4894,
          "end_idx": 5437
        },
        "page_content": " at detecting frustration in a simulator environment. They induce the emotion with specific scenarios and a demanding secondary task and are able to associate specific face movements according to FACS. Paschero2012 use OpenCV (https://opencv.org/) to detect the eyes and the mouth region and track facial movements. They simulate different lightning conditions and apply a multilayer perceptron for the classification task of Ekman's set of fundamental emotions. Overall, we found that studies using facial features usually focus on continuous",
        "type": "Document"
      },
      {
        "id": "389ea6cd-b8d1-4349-a0d6-fc9984e56757",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1909.02764",
          "start_idx": 15115,
          "end_idx": 15596
        },
        "page_content": " to 288 interactions, 180 between driver and the agent and 108 between driver and co-driver. The emotion self-ratings from the participants yielded 90 utterances labeled with joy, 26 with annoyance, 49 with insecurity, 9 with boredom, 111 with relaxation and 3 with no emotion. One example interaction per interaction type and emotion is shown in Table TABREF7. For further experiments, we only use joy, annoyance/anger, and insecurity/fear due to the small sample size for boredom",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What approach did previous models use for multi-span questions?",
    "llm_answer": "Only one model, MTMSN, directly targeted multi-span questions.  Their approach consisted of training a dedicated categorical variable to predict the number of spans and generalizing the single-span head method using non-maximum suppression.\n",
    "context": [
      {
        "id": "8f9d5917-c90e-478e-b480-9e5080670592",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1909.13375",
          "start_idx": 22213,
          "end_idx": 22660
        },
        "page_content": " and 0.6 in F1 compared to MTMSN. Therefore, it could prove beneficial to try and analyze the reasons behind each model's (ours and MTMSN) relative advantages, and perhaps try to combine them into a more holistic approach of tackling span questions. Table TABREF25 shows the results on DROP's test set, with our model being the best overall as of the time of writing, and not just on multi-span questions. In order to analyze the effect of each of",
        "type": "Document"
      },
      {
        "id": "41464b78-216e-4dcc-8f3a-446d35bf9f68",
        "metadata": {
          "chunk_id": 72,
          "document_id": "1909.13375",
          "start_idx": 24838,
          "end_idx": 25324
        },
        "page_content": " has a clear advantage in handling multi-span questions, with a 29.7 absolute improvement in EM, and a 15.1 absolute improvement in F1. Furthermore, we show that our model slightly eclipses the current state-of-the-art results on the entire DROP dataeset. Finally, we present some ablation studies, analyzing the benefit gained from individual components of our model. We believe that combining our tag-based approach for handling multi-span questions with current successful techniques",
        "type": "Document"
      },
      {
        "id": "7269ddec-1a35-4361-96b8-3862d4dd7c88",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1909.13375",
          "start_idx": 808,
          "end_idx": 1322
        },
        "page_content": " of questions. Among these new types were questions that require simple numerical reasoning, i.e questions whose answer is the result of a simple arithmetic expression containing numbers from the passage, and questions whose answers consist of several spans taken from the paragraph or the question itself, what we will denote as \"multi-span questions\". Of all the existing models that tried to tackle DROP, only one model BIBREF4 directly targeted multi-span questions in a manner that wasn't just a by-product of",
        "type": "Document"
      },
      {
        "id": "11eb0dbc-f319-4225-8170-46edb38123a8",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1909.13375",
          "start_idx": 1322,
          "end_idx": 1782
        },
        "page_content": "REF4 directly targeted multi-span questions in a manner that wasn't just a by-product of the model's overall performance. In this paper, we propose a new method for tackling multi-span questions. Our method takes a different path from that of the aforementioned model. It does not try to generalize the existing approach for tackling single-span questions, but instead attempts to attack this issue with a new, tag-based, approach. Numerically-aware QANet (NAQ",
        "type": "Document"
      },
      {
        "id": "a460ea96-b4ab-40ff-b959-4231aefda8ac",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1909.13375",
          "start_idx": 3497,
          "end_idx": 3987
        },
        "page_content": " capabilities, by adding the ability of doing simple multiplications and divisions between up to three numbers. MTMSN BIBREF4 is the first, and only model so far, that specifically tried to tackle the multi-span questions of DROP. Their approach consisted of two parts. The first was to train a dedicated categorical variable to predict the number of spans to extract. The second was to generalize the single-span head method of extracting a span, by utilizing the non-maximum suppression (",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is difference in peformance between proposed model and state-of-the art on other question types?",
    "llm_answer": "The proposed model achieves the best performance and better accuracy in discriminating plausible comments from correct comments.\n",
    "context": [
      {
        "id": "ec0fa9ed-f42a-4bf7-a415-81be525166e6",
        "metadata": {
          "chunk_id": 54,
          "document_id": "1703.04617",
          "start_idx": 14070,
          "end_idx": 14584
        },
        "page_content": "Q}^{TL}$ for M times to fit with $\\mathbf {I}$ . Questions by nature are often composed to fulfill different types of information needs. For example, a \"when\" question seeks for different types of information (i.e., temporal information) than those for a \"why\" question. Different types of questions and the corresponding answers could potentially have different distributional regularity. The previous models are often trained for all questions without explicitly discriminating different question types; however,",
        "type": "Document"
      },
      {
        "id": "179efbe8-bdbd-4ceb-918e-fd05c7c528cb",
        "metadata": {
          "chunk_id": 62,
          "document_id": "1809.04960",
          "start_idx": 24031,
          "end_idx": 24606
        },
        "page_content": " get lower scores than the plausible comments which are more literally relevant with the articles. With the help of our proposed model, proposed+IR achieves the best performance, and achieves a better accuracy to discriminate the plausible comments and the correct comments. Our proposed model incorporates the topic information, so the correct comments which are more similar to the articles in topic obtain higher scores than the other types of comments. According to the analysis of the error types of our model, we still need to focus on avoiding predicting the plausible",
        "type": "Document"
      },
      {
        "id": "338b0462-0627-4a30-a6bb-da4380439d78",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1703.04617",
          "start_idx": 3555,
          "end_idx": 4085
        },
        "page_content": " has the potential to capture long-distance interactions over the structures. Different types of questions are often used to seek for different types of information. For example, a \"what\" question could have very different property from that of a \"why\" question, while they may share information and need to be trained together instead of separately. We view this as a \"adaptation\" problem to let different types of questions share a basic model but still discriminate them when needed. Specifically, we are motivated by the ideas",
        "type": "Document"
      },
      {
        "id": "54830273-3c90-417d-8898-e4c92c7fa94a",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1703.04617",
          "start_idx": 14584,
          "end_idx": 15140
        },
        "page_content": "ity. The previous models are often trained for all questions without explicitly discriminating different question types; however, for a target question, both the common features shared by all questions and the specific features for a specific type of question are further considered in this paper, as they could potentially obey different distributions. In this paper we further explicitly model different types of questions in the end-to-end training. We start from a simple way to first analyze the word frequency of all questions, and obtain top-10 most",
        "type": "Document"
      },
      {
        "id": "17e839cc-213e-41c5-b048-c6f0c9c6dfdc",
        "metadata": {
          "chunk_id": 1,
          "document_id": "1703.04617",
          "start_idx": 493,
          "end_idx": 995
        },
        "page_content": " of relatively large training datasets (see Section \"Related Work\" for more details) has made it more feasible to train and estimate rather complex models in an end-to-end fashion for these problems, in which a whole model is fit directly with given question-answer tuples and the resulting model has shown to be rather effective. In this paper, we take a closer look at modeling questions in such an end-to-end neural network framework, since we regard question understanding is of importance for such",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance of proposed model on entire DROP dataset?",
    "llm_answer": "The proposed model slightly eclipses the current state-of-the-art results on the entire DROP dataset.\n",
    "context": [
      {
        "id": "5e0f3225-070e-4250-8f70-93c083311ec4",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "1cd7150b-9ae6-4cd6-bbf9-b2e3f17549b4",
        "metadata": {
          "chunk_id": 83,
          "document_id": "1703.04617",
          "start_idx": 22381,
          "end_idx": 22802
        },
        "page_content": ". We apply dropout to the Encoder layer and aggregation layer with a dropout rate of 0.5. Table 1 shows the official leaderboard on SQuAD test set when we submitted our system. Our model achieves a 68.73% EM score and 77.39% F1 score, which is ranked among the state of the art single models (without model ensembling). Table 2 shows the ablation performances of various Q-code on the development set. Note that since the",
        "type": "Document"
      },
      {
        "id": "dc948dec-3a23-4196-b1ee-c386b0464b4d",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1909.13375",
          "start_idx": 20052,
          "end_idx": 20444
        },
        "page_content": "s. For the large models we used a batch size of 3 with a learning rate of $5e^{-6}$ and trained for 5 epochs, except for the model without the single-span heads that was trained with a batch size of 2 for 7 epochs. F1 was used as our validation metric. All models were trained on a single GPU with 12-16GB of memory. Table TABREF24 shows the results on DROP's development set. Compared to our",
        "type": "Document"
      },
      {
        "id": "41464b78-216e-4dcc-8f3a-446d35bf9f68",
        "metadata": {
          "chunk_id": 72,
          "document_id": "1909.13375",
          "start_idx": 24838,
          "end_idx": 25324
        },
        "page_content": " has a clear advantage in handling multi-span questions, with a 29.7 absolute improvement in EM, and a 15.1 absolute improvement in F1. Furthermore, we show that our model slightly eclipses the current state-of-the-art results on the entire DROP dataeset. Finally, we present some ablation studies, analyzing the benefit gained from individual components of our model. We believe that combining our tag-based approach for handling multi-span questions with current successful techniques",
        "type": "Document"
      },
      {
        "id": "8937964f-11b5-487b-9960-db106025360c",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1909.02764",
          "start_idx": 17421,
          "end_idx": 17831
        },
        "page_content": " layer. This setup is inspired by BIBREF38. We use a dropout rate of 0.3 in all layers and optimize with Adam BIBREF39 with a learning rate of $10^{-5}$ (These parameters are the same for all further experiments). We build on top of the Keras library with the TensorFlow backend. We consider this setup our baseline model. We train models on a variety of corpora, namely the common format published by BIBREF27",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How accurate is the aspect based sentiment classifier trained only using the XR loss?",
    "llm_answer": "The context states that training a BiLSTM with XR surpasses pre-trained BERT models, and using XR training on top of pre-trained BERT models further increases results.  However, it doesn't state the *absolute* accuracy of an aspect-based classifier trained only using XR.\n",
    "context": [
      {
        "id": "aee770f2-f1bd-4913-8651-1abebf815db2",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1909.00430",
          "start_idx": 2288,
          "end_idx": 2765
        },
        "page_content": " not be fully accurate, allowing us to make use of noisy predictions which are assigned by an automatic classifier (Sections SECREF12 and SECREF4 ). In other words, given a medium-sized sentiment corpus with sentence-level labels, and a large collection of un-annotated text from the same distribution, we can train an accurate aspect-level sentiment classifier. The XR loss allows us to use task A labels for training task B predictors. This ability seamlessly integrates into",
        "type": "Document"
      },
      {
        "id": "b443846f-8234-49d4-bc7f-5780209cb34b",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1909.00430",
          "start_idx": 4830,
          "end_idx": 5281
        },
        "page_content": " of NLP, XR was used by BIBREF20 to train twitter-user attribute prediction using hundreds of noisy distributional expectations based on census demographics. Here, we suggest using XR to train a target task (aspect-level sentiment) based on the output of a related source-task classifier (sentence-level sentiment). The main idea of XR is moving from a fully supervised situation in which each data-point INLINEFORM0 has an associated label INLINEFORM",
        "type": "Document"
      },
      {
        "id": "e37d0d85-22c3-40ad-a203-a478b3d52976",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1909.00430",
          "start_idx": 12355,
          "end_idx": 12894
        },
        "page_content": " the complete set. We demonstrate the procedure given above by training Aspect-based Sentiment Classifier (ABSC) using sentence-level sentiment signals. We observe that while the sentence-level sentiment does not determine the sentiment of individual aspects (a positive sentence may contain negative remarks about some aspects), it is very predictive of the proportion of sentiment labels of the fragments within a sentence. Positively labeled sentences are likely to have more positive aspects and fewer negative ones, and vice-versa for",
        "type": "Document"
      },
      {
        "id": "4adfe3ff-2801-46c6-8943-247972c6b99c",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1909.00430",
          "start_idx": 1532,
          "end_idx": 2003
        },
        "page_content": "ed aspects within this set. For example, in a random set of positive sentences, we expect to find 90% positive aspects, while in a set of negative sentences, we expect to find 70% negative aspects. These proportions can be easily either guessed or estimated from a small set. We propose a novel application of the XR framework for transfer learning in this setup. We present an algorithm (Sec SECREF12 ) that, given a corpus labeled for task A (sentence-level sentiment),",
        "type": "Document"
      },
      {
        "id": "11b4085b-91af-4192-8151-f0400b227c43",
        "metadata": {
          "chunk_id": 62,
          "document_id": "1909.00430",
          "start_idx": 21984,
          "end_idx": 22439
        },
        "page_content": " The results are presented in Table TABREF55 . As before, aspect-based fine-tuning is beneficial for both SemEval-16 and SemEval-15. Training a BiLSTM with XR surpasses pre-trained bert models and using XR training on top of the pre-trained Bert models substantially increases the results even further. We presented a transfer learning method based on expectation regularization (XR), and demonstrated its effectiveness for training aspect-based sentiment",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What were the non-neural baselines used for the task?",
    "llm_answer": "\"first $n$\" baseline\n",
    "context": [
      {
        "id": "26654c16-c460-4900-b78d-5ad48b826e37",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1909.00542",
          "start_idx": 15059,
          "end_idx": 15516
        },
        "page_content": " the results and details of the runs submitted to BioASQ. The table uses ROUGE-SU4 Recall since this is the metric available at the time of writing this paper. However, note that, as explained in Section SECREF6, these results might differ from the final human evaluation results. Therefore we do not comment on the results, other than observing that the \u201cfirst $n$\u201d baseline produces the same results as the neural regressor. As mentioned in Section SECREF",
        "type": "Document"
      },
      {
        "id": "cc19f0cb-3472-43b2-b27a-65551596a757",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1909.00542",
          "start_idx": 7743,
          "end_idx": 8155
        },
        "page_content": " layer, and use cross-entropy as the loss function. The complete architecture is shown in Fig. FIGREF28. The bottom section of Table TABREF26 shows the results of several variants of the neural architecture. The table includes a neural regressor (NNR) and a neural classifier (NNC). The neural classifier is trained in two set ups: \u201cNNC top 5\u201d uses classification labels as described in Section SECREF3, and \u201cNNC",
        "type": "Document"
      },
      {
        "id": "36775c40-92f5-4fb6-82d8-ea96852b0d2b",
        "metadata": {
          "chunk_id": 40,
          "document_id": "1909.00542",
          "start_idx": 15344,
          "end_idx": 15845
        },
        "page_content": "$\u201d baseline produces the same results as the neural regressor. As mentioned in Section SECREF3, the labels used for the classification experiments are the 5 sentences with highest ROUGE-SU4 F1 score. Macquarie University's participation in BioASQ 7 focused on the task of generating the ideal answers. The runs use query-based extractive techniques and we experiment with classification, regression, and reinforcement learning approaches. At the time of writing there were no human evaluation results,",
        "type": "Document"
      },
      {
        "id": "8937964f-11b5-487b-9960-db106025360c",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1909.02764",
          "start_idx": 17421,
          "end_idx": 17831
        },
        "page_content": " layer. This setup is inspired by BIBREF38. We use a dropout rate of 0.3 in all layers and optimize with Adam BIBREF39 with a learning rate of $10^{-5}$ (These parameters are the same for all further experiments). We build on top of the Keras library with the TensorFlow backend. We consider this setup our baseline model. We train models on a variety of corpora, namely the common format published by BIBREF27",
        "type": "Document"
      },
      {
        "id": "65333987-2358-483b-95cd-3ff028ff6a96",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the models evaluated on?",
    "llm_answer": "200 unique comments, ranked by TF-IDF, S2S, IR, and Proposed+IR models.\n",
    "context": [
      {
        "id": "7d4754c7-4ea8-4d20-9fb6-3d1e50bf0cf6",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1809.04960",
          "start_idx": 22497,
          "end_idx": 22960
        },
        "page_content": " the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types of comments as described in the above retrieval evaluation setting: Correct, Plausible, Popular, and Random. We rank the candidate comment set with four models (TF-IDF, S2S, IR, and Proposed+IR), and record the types of top-1 comments. Figure FIGREF40 shows the percentage of different types of top-1 comments generated by each model. It shows",
        "type": "Document"
      },
      {
        "id": "0b64308f-c420-4f0a-b1a3-2e2d3aee5598",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1905.00563",
          "start_idx": 13447,
          "end_idx": 13944
        },
        "page_content": " evaluate by ( \"Influence Function vs \" ) comparing estimate with the actual effect of the attacks, ( \"Robustness of Link Prediction Models\" ) studying the effect of adversarial attacks on evaluation metrics, ( \"Interpretability of Models\" ) exploring its application to the interpretability of KG representations, and ( \"Finding Errors in Knowledge Graphs\" ) detecting incorrect triples. To evaluate the quality of our approximations and compare with influence function (IF), we conduct leave one",
        "type": "Document"
      },
      {
        "id": "268d0437-5785-4965-b0aa-9d98ae01a6ce",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1809.04960",
          "start_idx": 16805,
          "end_idx": 17328
        },
        "page_content": " various, so it is intractable to find out all the possible references to be compared with the model outputs. Inspired by the evaluation methods of dialogue models, we formulate the evaluation as a ranking problem. Given a piece of news and a set of candidate comments, the comment model should return the rank of the candidate comments. The candidate comment set consists of the following parts: Correct: The ground-truth comments of the corresponding news provided by the human. Plausible: The 50 most similar comments to",
        "type": "Document"
      },
      {
        "id": "72561058-d008-4fff-a632-c8a68443199c",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1809.04960",
          "start_idx": 24606,
          "end_idx": 25124
        },
        "page_content": " the analysis of the error types of our model, we still need to focus on avoiding predicting the plausible comments. There are few studies regarding machine commenting. Qin et al. QinEA2018 is the first to propose the article commenting task and a dataset, which is used to evaluate our model in this work. More studies about the comments aim to automatically evaluate the quality of the comments. Park et al. ParkSDE16 propose a system called CommentIQ, which assist the comment moderators in identifying high quality",
        "type": "Document"
      },
      {
        "id": "607428e1-f046-45ed-8ec5-5c2e5bf9ac82",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1910.03891",
          "start_idx": 9736,
          "end_idx": 10179
        },
        "page_content": "REF17, textual descriptions BIBREF11, BIBREF12 and logical rules BIBREF23; (ii) those which tries to design more complicated strategies, e.g., deep neural network models BIBREF24. Except for TransE and its extensions, some efforts measure plausibility by matching latent semantics of entities and relations. The basic idea behind these models is that the plausible triples of a KG is assigned low energies. For examples, Distant Model BIBREF25",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the results of multimodal compared to unimodal models?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "3f575905-09b6-4ea2-aac2-6af230db202a",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1909.02764",
          "start_idx": 759,
          "end_idx": 1210
        },
        "page_content": "0. Note that the term \u201cmultimodal\u201d is also used beyond the combination of vision, audio, and text. For example, Soleymani2012 use it to refer to the combination of electroencephalogram, pupillary response and gaze distance. In this paper, we deal with the specific situation of car environments as a testbed for multimodal emotion recognition. This is an interesting environment since it is, to some degree, a controlled environment: Dialogue partners",
        "type": "Document"
      },
      {
        "id": "3606af56-990e-448a-9335-ca1554ab62d8",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1910.09399",
          "start_idx": 4538,
          "end_idx": 4998
        },
        "page_content": "IBREF13. Researchers often describe multimodal learning as a method that incorporates characteristics from several methods, algorithms, and ideas. This can include ideas from two or more learning approaches in order to create a robust implementation to solve an uncommon problem or improve a solution BIBREF8, BIBREF14, BIBREF15, BIBREF16, BIBREF17. black In this survey, we focus primarily on reviewing recent works that aim to solve the challenge of text-to-",
        "type": "Document"
      },
      {
        "id": "e08b425d-fdf8-477d-acb8-9b5413f312ff",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1909.13375",
          "start_idx": 21507,
          "end_idx": 21934
        },
        "page_content": " Our best model, large-squad, exhibits a huge improvement of 29.7 in EM and 15.1 in F1 compared to MTMSNlarge. When comparing single-span performance, our best model exhibits slightly better results, but it should be noted that it retains the single-span heads from NABERT+, while in MTMSN they have one head to predict both single-span and multi-span answers. For a fairer comparison, we trained our model with the single-span",
        "type": "Document"
      },
      {
        "id": "e60e0d3d-b592-4a3f-9ebd-b12edba8a33b",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1909.13375",
          "start_idx": 20788,
          "end_idx": 21299
        },
        "page_content": " For the other types, most of the improvement came from better preprocessing. A more detailed discussion could be found in Section (SECREF36). Notice that different BERTlarge models were used, so the comparison is less direct. Overall, our large models exhibits similar results to those of MTMSNlarge. For multi-span questions we achieve a significantly better performance. While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still",
        "type": "Document"
      },
      {
        "id": "8f9d5917-c90e-478e-b480-9e5080670592",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1909.13375",
          "start_idx": 22213,
          "end_idx": 22660
        },
        "page_content": " and 0.6 in F1 compared to MTMSN. Therefore, it could prove beneficial to try and analyze the reasons behind each model's (ours and MTMSN) relative advantages, and perhaps try to combine them into a more holistic approach of tackling span questions. Table TABREF25 shows the results on DROP's test set, with our model being the best overall as of the time of writing, and not just on multi-span questions. In order to analyze the effect of each of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What were their performance results?",
    "llm_answer": "Recall rate, F1 score, and Overall accuracy were used as metrics.  A model trained on the AMMER corpus achieved an F1 score of 57%.  Results on the DROP's development set showed substantial improvement across all metrics compared to base models.  A base model surpassed the NABERT+ baseline in every metric.\n",
    "context": [
      {
        "id": "96ecbdb9-17ee-46e0-9f1c-9eef560fb526",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1912.13109",
          "start_idx": 10203,
          "end_idx": 10681
        },
        "page_content": " metrics - Recall rate, F1 score and Overall accuracy. Choice of model parameters were in the above models were inspired from previous work done but then were tuned to the best performance of the Test dataset. Following parameters were considered for tuning. Learning rate: Based on grid search the best performance was achieved when learning rate was set to 0.01. This value was arrived by a grid search on lr parameter. Number of Bidirectional LSTM units: A set of 32, 64, 128",
        "type": "Document"
      },
      {
        "id": "0ab3fdf3-8ea1-480c-8cac-92715fc9d565",
        "metadata": {
          "chunk_id": 52,
          "document_id": "1909.02764",
          "start_idx": 20420,
          "end_idx": 20841
        },
        "page_content": "=73 % on TEC. The model trained on our AMMER corpus achieves an F1 score of 57%. This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set. These results are mostly in line with Bostan2018. Now we analyze how well the models trained in Experiment 1 perform when applied to our data set. The results are shown in column \u201cSimple\u201d in Table TAB",
        "type": "Document"
      },
      {
        "id": "1a6adc72-1767-4e4a-be77-138554d2b622",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1909.00542",
          "start_idx": 734,
          "end_idx": 1323
        },
        "page_content": " the training procedure was based on a regression set up, this year we experiment with various classification set ups. The main contributions of this paper are: We compare classification and regression approaches and show that classification produces better results than regression but the quality of the results depends on the approach followed to annotate the data labels. We conduct correlation analysis between various ROUGE evaluation metrics and the human evaluations conducted at BioASQ and show that Precision and F1 correlate better than Recall. Section SECREF2 briefly introduces",
        "type": "Document"
      },
      {
        "id": "1e2f925a-9a0d-4318-af38-ee9ae9d2a426",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1912.13109",
          "start_idx": 7921,
          "end_idx": 8471
        },
        "page_content": " on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we",
        "type": "Document"
      },
      {
        "id": "5e0f3225-070e-4250-8f70-93c083311ec4",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much did they outperform the other methods?",
    "llm_answer": "The context states that the fine-tuned BERT versions \"clearly outperform all other methods,\" but does not quantify the difference.\n",
    "context": [
      {
        "id": "97194657-ec5f-4398-9cc9-11be4fc5df47",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1909.00578",
          "start_idx": 8968,
          "end_idx": 9354
        },
        "page_content": " the training set of each fold. Table TABREF23 shows Spearman's $\\rho $, Kendall's $\\tau $ and Pearson's $r$ for all datasets and models. The three fine-tuned BERT versions clearly outperform all other methods. Multi-task versions seem to perform better than single-task ones in most cases. Especially for $\\mathcal {Q}4$ and $\\mathcal {Q}5$, which are highly correlated, the multi-task",
        "type": "Document"
      },
      {
        "id": "309bcaa2-ef9d-45b3-9b52-81e934a26747",
        "metadata": {
          "chunk_id": 66,
          "document_id": "2003.07433",
          "start_idx": 25731,
          "end_idx": 26161
        },
        "page_content": "REF28 illustrates the comparisons between LAXARY and Coppersmith et. al. proposed method. Here we can see, the outperformance of our proposed method as well as the importance of $s-score$ estimation. We also illustrates the importance of $\\alpha -score$ and $S-score$ in Fig FIGREF30. Fig FIGREF30 illustrates that if we change the number of training samples (%), LAXARY models outperforms Coppersmith et. al. proposed model under",
        "type": "Document"
      },
      {
        "id": "5c6209f3-cef8-4a17-88a8-a660a1a73e1e",
        "metadata": {
          "chunk_id": 76,
          "document_id": "1701.00185",
          "start_idx": 27376,
          "end_idx": 27736
        },
        "page_content": " and NMI performance of our proposed approaches and four baseline methods, K-means, SkipVec, RecNN and Para2vec based clustering methods. Intuitively, we get a general observation that (1) BoW based approaches, including K-means (TF) and K-means (TF-IDF), and SkipVec based approaches perform not well; (2) RecNN based approaches, both RecNN (Ave.) and RecNN (",
        "type": "Document"
      },
      {
        "id": "2979bc16-b544-48d5-a7fe-c8186994e1c9",
        "metadata": {
          "chunk_id": 168,
          "document_id": "1910.09399",
          "start_idx": 65080,
          "end_idx": 65581
        },
        "page_content": ". In other words, beyond the work of BIBREF8 in which images were generated from text in 64$\\times $64 tiny swatches. Lastly, all methods were evaluated on datasets that included birds, flowers, humans, and other miscellaneous elements. We were also able to allocate some important papers that were as impressive as the papers we finally surveyed. Though, these notable papers have yet to contribute directly or indirectly to the expansion of the vast computer vision AI field. Looking into the future",
        "type": "Document"
      },
      {
        "id": "f7f7d683-68b0-4b82-bc24-6152efcc20e1",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1910.11769",
          "start_idx": 7771,
          "end_idx": 8258
        },
        "page_content": "-fold cross validation for each technique. We provide a brief overview of each benchmark experiment below. Among all of the benchmarks, Bidirectional Encoder Representations from Transformers (BERT) BIBREF11 achieved the best performance with a 0.604 micro-F1 score. Overall, we observed that deep-learning based techniques performed better than lexical based methods. This suggests that a method which attends to context and themes could do well on the dataset. We computed bag-of-words",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the state of the art?",
    "llm_answer": "State-of-the-art systems for many other languages have been significantly boosted by large-scale reading comprehension datasets like CNN/DailyMail and Squad.  DNN methods are also achieving great results in NER tasks due to increased data.\n",
    "context": [
      {
        "id": "a4a4c58b-d1f1-46a1-9912-24f7a070ad68",
        "metadata": {
          "chunk_id": 43,
          "document_id": "2001.00137",
          "start_idx": 16781,
          "end_idx": 17197
        },
        "page_content": " the online platforms. Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector",
        "type": "Document"
      },
      {
        "id": "99aa59ef-b5ee-40f0-a832-bcb982d5ab1f",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1909.02764",
          "start_idx": 5772,
          "end_idx": 6281
        },
        "page_content": "2004automatic compare local and global features in support vector machines. Next to such discriminative approaches, hidden Markov models are well-studied, however, there is no agreement on which feature-based classifier is most suitable BIBREF13. Similar to the facial expression modality, recent efforts on applying deep learning have been increased for acoustic speech processing. For instance, lee2015high use a recurrent neural network and palaz2015analysis apply a convolutional neural network to the raw",
        "type": "Document"
      },
      {
        "id": "f1c37aec-36b2-4d7f-af12-77950d3a7638",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1707.03904",
          "start_idx": 1253,
          "end_idx": 1756
        },
        "page_content": " text which best answers the question BIBREF4 , BIBREF5 . Like most other language technologies, the current research focus for both these steps is firmly on machine learning based approaches for which performance improves with the amount of data available. Machine reading performance, in particular, has been significantly boosted in the last few years with the introduction of large-scale reading comprehension datasets such as CNN / DailyMail BIBREF6 and Squad BIBREF7 . State-of-the-art systems for",
        "type": "Document"
      },
      {
        "id": "627dbfa3-5493-4f17-b5e4-e8a90355b5fd",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1911.03894",
          "start_idx": 22277,
          "end_idx": 22771
        },
        "page_content": " many other languages. We will publish an updated version in the near future where we will explore and release models trained for longer, with additional downstream tasks, baselines (e.g. XLM) and analysis, we will also train additional models with potentially cleaner corpora such as CCNet BIBREF56 for more accurate performance evaluation and more complete ablation. This work was partly funded by three French National grants from the Agence Nationale de la Recherche, namely projects PARSIT",
        "type": "Document"
      },
      {
        "id": "4794b743-1694-4c35-9149-86a3a13dcc8d",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1911.07228",
          "start_idx": 2445,
          "end_idx": 2848
        },
        "page_content": " used traditional machine learning methods such as Maximum Entropy Markov Model (MEMM), Support Vector Machine (SVM) and Conditional Random Field (CRF). In particular, most of the toolkits for NER task attempted to use MEMM BIBREF4, and CRF BIBREF5 to solve this problem. Nowadays, because of the increase in data, DNN methods are used a lot. They have archived great results when it comes to NER tasks,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What difficulties does sentiment analysis on Twitter have, compared to sentiment analysis in other domains?",
    "llm_answer": "Initially, a lack of suitable datasets and lexicons for system training, development, and testing hindered research.  Some Twitter-specific resources were small and proprietary.\n",
    "context": [
      {
        "id": "de60cfae-245e-4b31-80ee-4a8adf6707b9",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1710.01492",
          "start_idx": 3007,
          "end_idx": 3411
        },
        "page_content": "e., a sentence or a headline rather than a full document. How to handle such challenges has only recently been the subject of thorough research BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 . Sentiment analysis has a wide number of applications in areas such as market research, political and social sciences, and for studying public opinion in general, and Twitter is one",
        "type": "Document"
      },
      {
        "id": "33f6c645-f026-4721-8790-aa5dfd13a136",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1710.01492",
          "start_idx": 3204,
          "end_idx": 3704
        },
        "page_content": " market research, political and social sciences, and for studying public opinion in general, and Twitter is one of the most commonly-used platforms for this. This is due to its streaming nature, which allows for real-time analysis, to its social aspect, which encourages people to share opinions, and to the short size of the tweets, which simplifies linguistic analysis. There are several formulations of the task of Sentiment Analysis on Twitter that look at different sizes of the target (e.g., at",
        "type": "Document"
      },
      {
        "id": "e2416301-cc50-45b4-a596-a7a4a3919853",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1710.01492",
          "start_idx": 0,
          "end_idx": 547
        },
        "page_content": "Microblog sentiment analysis; Twitter opinion mining Sentiment Analysis: This is text analysis aiming to determine the attitude of a speaker or a writer with respect to some topic or the overall contextual polarity of a piece of text. Sentiment analysis on Twitter is the use of natural language processing techniques to identify and categorize opinions expressed in a tweet, in order to determine the author's attitude toward a particular topic or in general. Typically, discrete labels such as positive, negative, neutral, and objective are used",
        "type": "Document"
      },
      {
        "id": "bc2da18d-4dd9-43e4-a669-b1d780f8f724",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1710.01492",
          "start_idx": 6828,
          "end_idx": 7363
        },
        "page_content": ", which have proven to be very valuable for their respective domains and types of texts, but less useful for short social media messages. Later, with the emergence of social media, sentiment analysis in Twitter became a hot research topic. Unfortunately, research in that direction was hindered by the unavailability of suitable datasets and lexicons for system training, development, and testing. While some Twitter-specific resources were developed, initially they were either small and proprietary, such as the i-sieve corpus BIBREF",
        "type": "Document"
      },
      {
        "id": "a3e39600-f5e4-48a0-bacf-4cd5d548337d",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1710.01492",
          "start_idx": 3704,
          "end_idx": 4099
        },
        "page_content": " of Sentiment Analysis on Twitter that look at different sizes of the target (e.g., at the level of words vs. phrases vs. tweets vs. sets of tweets), at different types of semantic targets (e.g., aspect vs. topic vs. overall tweet), at the explicitness of the target (e.g., sentiment vs. stance detection), at the scale of the expected label (2-point vs. 3-point vs. ordinal), etc. All these are",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many sentence transformations on average are available per unique sentence in dataset?",
    "llm_answer": "15\n",
    "context": [
      {
        "id": "f9303325-cd7c-4a49-b5e3-ff88f5a2c359",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1912.01673",
          "start_idx": 11172,
          "end_idx": 11741
        },
        "page_content": " most of the transformations are likely to lead to a large number possible outcomes. As documented in scratching2013, Czech sentence might have hundreds of thousand of paraphrases. To support some minimal exploration of this possible diversity, most of sentences were assigned to several annotators. The annotation is a challenging task and the annotators naturally make mistakes. Unfortunately, a single typo can significantly influence the resulting embedding BIBREF26. After collecting all the sentence variations, we applied the statistical spellchecker and grammar",
        "type": "Document"
      },
      {
        "id": "732444dc-2d5f-4be1-bf18-3fec97240e01",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1912.01673",
          "start_idx": 12286,
          "end_idx": 12785
        },
        "page_content": ". Out of the 4262 distinct sentences, only 188 was recorded more than once. In other words, the chance of two annotators producing the same output string is quite low. The most repeated transformations are by far past, future and ban. The least repeated is paraphrase with only single one repeated. multiple-annots documents this in another way. The 293 annotations are split into groups depending on how many annotators saw the same input sentence: 30 annotations were annotated by one person only,",
        "type": "Document"
      },
      {
        "id": "0093df87-abcd-4759-863f-1dba19bfbc0a",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1908.08419",
          "start_idx": 24134,
          "end_idx": 24599
        },
        "page_content": " iterations, which is a good trade-off between speed and performance. As for the number of samples selected per iteration, there are 6078 sentences in unlabeled set, considering the high cost of relabeling, we set four sizes of samples selected per iteration, which are 2%, 5%, 8% and 11%. The experimental results of two sampling strategies with 30 iterations on four different proportions of relabeled data are shown in Fig. FIGREF45 , where x-axis represents the",
        "type": "Document"
      },
      {
        "id": "64642fa4-7c9c-473b-98c9-3b3e8c86b091",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1912.01673",
          "start_idx": 12000,
          "end_idx": 12463
        },
        "page_content": "ktor, there are 4262 unique sentences (including 150 seed sentences) that form the COSTRA 1.0 dataset. Statistics of individual annotators are available in tab:statistics. The time needed to carry out one piece of annotation (i.e. to provide one seed sentence with all 15 transformations) was on average almost 20 minutes but some annotators easily needed even half an hour. Out of the 4262 distinct sentences, only 188 was recorded more than once. In other words",
        "type": "Document"
      },
      {
        "id": "14aed39c-b53e-4a6d-b613-b1bf0fc430f4",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1912.01673",
          "start_idx": 1987,
          "end_idx": 2499
        },
        "page_content": " in sentences. In this work, we present COSTRA, a new dataset of COmplex Sentence TRAnsformations. In its first version, the dataset is limited to sample sentences in Czech. The goal is to support studies of semantic and syntactic relations between sentences in the continuous space. Our dataset is the prerequisite for one of possible ways of exploring sentence meaning relatability: we envision that the continuous space of sentences induced by an ideal embedding method would exhibit topological similarity to",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What annotations are available in the dataset?",
    "llm_answer": "Participant labels, BIO format (Inside, Outside, Beginning) labels for NP heads, user states, system states, and dialogue acts.\n",
    "context": [
      {
        "id": "6d2fb9a2-9305-4bd8-864d-143c83b5e90c",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1909.00430",
          "start_idx": 0,
          "end_idx": 440
        },
        "page_content": "Data annotation is a key bottleneck in many data driven algorithms. Specifically, deep learning models, which became a prominent tool in many data driven tasks in recent years, require large datasets to work well. However, many tasks require manual annotations which are relatively hard to obtain at scale. An attractive alternative is lightly supervised learning BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF",
        "type": "Document"
      },
      {
        "id": "9c29b30f-983c-4da5-9469-bc961c44c746",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1703.05260",
          "start_idx": 7578,
          "end_idx": 8108
        },
        "page_content": ". This section deals with the annotation of the data. We first describe the final annotation schema. Then, we describe the iterative process of corpus annotation and the refinement of the schema. This refinement was necessary due to the complexity of the annotation. For each of the scenarios, we designed a specific annotation template. A script template consists of scenario-specific event and participant labels. An example of a template is shown in Table 1 . All NP heads in the corpus were annotated with a participant label;",
        "type": "Document"
      },
      {
        "id": "6cbe4da1-641e-4851-b898-32218f4b87d1",
        "metadata": {
          "chunk_id": 54,
          "document_id": "1911.07228",
          "start_idx": 14818,
          "end_idx": 15154
        },
        "page_content": " BIO format (Inside, Outside, Beginning) BIBREF6 at the top layer (CRF). If we can unify the labeling of the annotators between the train, dev and test sets. We will improve data quality and classifier. It is better if there is a pre-trained word embeddings that overlays the data, and segmentation algorithm need to be more accurately.",
        "type": "Document"
      },
      {
        "id": "28eaec66-d29a-442f-80e4-0404e8d15452",
        "metadata": {
          "chunk_id": 22,
          "document_id": "2002.11893",
          "start_idx": 8390,
          "end_idx": 8922
        },
        "page_content": " before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states. Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three",
        "type": "Document"
      },
      {
        "id": "45540b48-0892-47ef-9a7d-86f9efe45ec0",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1910.11769",
          "start_idx": 4963,
          "end_idx": 5497
        },
        "page_content": " using in-house data annotators who are experts in narrative content. A passage is accepted as valid if the in-house annotator's label matched any one of the MTurk annotators' labels. The remaining passages are discarded. We provide the fraction of annotator agreement for each label in the dataset. Though passages may lose some emotional context when read independently of the complete narrative, we believe annotator agreement on our dataset supports the assertion that small excerpts can still convey coherent emotions. During the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are possible sentence transformations represented in dataset, as new sentences?",
    "llm_answer": "The dataset contains multiple variations of each sentence, assigned to several annotators.  These variations represent possible transformations.\n",
    "context": [
      {
        "id": "14aed39c-b53e-4a6d-b613-b1bf0fc430f4",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1912.01673",
          "start_idx": 1987,
          "end_idx": 2499
        },
        "page_content": " in sentences. In this work, we present COSTRA, a new dataset of COmplex Sentence TRAnsformations. In its first version, the dataset is limited to sample sentences in Czech. The goal is to support studies of semantic and syntactic relations between sentences in the continuous space. Our dataset is the prerequisite for one of possible ways of exploring sentence meaning relatability: we envision that the continuous space of sentences induced by an ideal embedding method would exhibit topological similarity to",
        "type": "Document"
      },
      {
        "id": "f9303325-cd7c-4a49-b5e3-ff88f5a2c359",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1912.01673",
          "start_idx": 11172,
          "end_idx": 11741
        },
        "page_content": " most of the transformations are likely to lead to a large number possible outcomes. As documented in scratching2013, Czech sentence might have hundreds of thousand of paraphrases. To support some minimal exploration of this possible diversity, most of sentences were assigned to several annotators. The annotation is a challenging task and the annotators naturally make mistakes. Unfortunately, a single typo can significantly influence the resulting embedding BIBREF26. After collecting all the sentence variations, we applied the statistical spellchecker and grammar",
        "type": "Document"
      },
      {
        "id": "2f596a0c-4f49-49ec-a9fd-1088b3f92b6a",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1912.01673",
          "start_idx": 3172,
          "end_idx": 3674
        },
        "page_content": " behaviour to emerge, as it happened for word vector operations, but regardless if the behaviour is emergent or trained, we need a dataset of sentences illustrating these patterns. If large enough, such a dataset could serve for training. If it will be smaller, it will provide a test set. In either case, these sentences could provide a \u201cskeleton\u201d to the continuous space of sentence embeddings. The paper is structured as follows: related summarizes existing methods of sentence embeddings evaluation",
        "type": "Document"
      },
      {
        "id": "76f81e16-451d-4de3-aeb7-c4518c71d023",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1908.06606",
          "start_idx": 11284,
          "end_idx": 11795
        },
        "page_content": "i$. To tell the model $Q$ and $X$ is two different sentence, a sentence type input is generated which is a binary label sequence to denote what sentence each character in the input belongs to. Positional encoding and mask matrix is also constructed automatically to bring in absolute position information and eliminate the impact of zero padding respectively. Then a hidden vector $V_s$ which contains both query and text information is generated through BERT-base model. Since BERT is trained on general corpus",
        "type": "Document"
      },
      {
        "id": "736740af-2556-49ed-a4cd-01b9d559015a",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1912.01673",
          "start_idx": 2877,
          "end_idx": 3401
        },
        "page_content": " the previous ones in multiple sentences could be partially or linearly ordered according to the strength of the speakers confidence in the claim. Our long term goal is to search for an embedding method which exhibits this behaviour, i.e. that the topological map of the embedding space corresponds to meaningful operations or changes in the set of sentences of a language (or more languages at once). We prefer this behaviour to emerge, as it happened for word vector operations, but regardless if the behaviour is emergent",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are all 15 types of modifications ilustrated in the dataset?",
    "llm_answer": "The answer is not found in the context.  While the text discusses *types* of modifications, it does not list 15 specific types.\n",
    "context": [
      {
        "id": "bc4a0f60-c159-4667-a613-4d43b77dc4ee",
        "metadata": {
          "chunk_id": 21,
          "document_id": "1912.01673",
          "start_idx": 8152,
          "end_idx": 8670
        },
        "page_content": " several of its possible alternations, see tab:firstroundexamples. Unfortunately, these examples turned out to be highly influential on the annotators' decisions and they correspond to almost two thirds of all of modifications gathered in the first round. Other very common transformations include change of a word order or transformation into a interrogative/imperative sentence. Other interesting modification were also proposed such as change into a fairy-tale style, excessive use of diminutives/vulgarisms or dada",
        "type": "Document"
      },
      {
        "id": "9e5b0bbb-7fff-40fc-a541-c395ca9850cc",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1909.13375",
          "start_idx": 22494,
          "end_idx": 22947
        },
        "page_content": " writing, and not just on multi-span questions. In order to analyze the effect of each of our changes, we conduct ablation studies on the development set, depicted in Table TABREF26. Not using the simple preprocessing from Section SECREF17 resulted in a 2.5 point decrease in both EM and F1. The numeric questions were the most affected, with their performance dropping by 3.5 points. Given that number questions make up about 61% of the dataset, we can",
        "type": "Document"
      },
      {
        "id": "92530700-5d96-4b09-9292-1d42a9e0b200",
        "metadata": {
          "chunk_id": 41,
          "document_id": "1810.06743",
          "start_idx": 15559,
          "end_idx": 16023
        },
        "page_content": " UniMorph data and the existing type-level UniMorph data. We transform all UD data to the UniMorph. We compare the simple lookup-based transformation to the one with linguistically informed post-edits on all languages with both UD and UniMorph data. We then evaluate the recall of MSDs without partial credit. Because the UniMorph tables only possess annotations for verbs, nouns, adjectives, or some combination, we can only examine performance for these parts of",
        "type": "Document"
      },
      {
        "id": "f13d701c-40e3-4e0e-a7c5-cc09dba5d512",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1912.01673",
          "start_idx": 8962,
          "end_idx": 9489
        },
        "page_content": " with 269 described unique changes. We use them as an inspiration for second round of annotation. We selected 15 modifications types to collect COSTRA 1.0. They are presented in annotationinstructions. We asked for two distinct paraphrases of each sentence because we believe that a good sentence embedding should put paraphrases close together in vector space. Several modification types were specifically selected to constitute a thorough test of embeddings. In different meaning, the annotators should create a sentence with",
        "type": "Document"
      },
      {
        "id": "be5a42e8-fd42-4263-8b74-c0b5d2c7a1eb",
        "metadata": {
          "chunk_id": 112,
          "document_id": "1910.09399",
          "start_idx": 43485,
          "end_idx": 43906
        },
        "page_content": "REF8, BIBREF36, BIBREF39, BIBREF40, BIBREF41, BIBREF42, BIBREF22, BIBREF26. These datasets were first utilized on earlier works BIBREF8, and most of them sport modified features such image annotations, labels, or descriptions. The qualitative and quantitative results reported by researchers in this study were far superior of earlier works in this same field of computer vision AI. black In this subsection, we introduce",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What were their results on the three datasets?",
    "llm_answer": "The context provides results for various models on different datasets, but not a single set of results for three specific datasets.  It describes results on TEC, AMMER, Figure8, ISEAR, and DROP.\n",
    "context": [
      {
        "id": "bbc66420-7a93-4f6e-af63-76b70af3decb",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1707.03904",
          "start_idx": 13913,
          "end_idx": 14370
        },
        "page_content": " human testers were penalized for entering \u201c0\u201d as answer instead of \u201czero\u201d. However, a comparison between systems may still be meaningful. To put the difficulty of the introduced datasets into perspective, we evaluated human performance on answering the questions. For each dataset, we recruited one domain expert (a developer with several years of programming experience for Quasar-S, and an avid trivia enthusiast for Quasar-T) and $1-3$ non-experts. Each",
        "type": "Document"
      },
      {
        "id": "7da029d3-0e6a-48d3-9dd2-3cfd37690294",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1910.04269",
          "start_idx": 6853,
          "end_idx": 7342
        },
        "page_content": " comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (",
        "type": "Document"
      },
      {
        "id": "0ab3fdf3-8ea1-480c-8cac-92715fc9d565",
        "metadata": {
          "chunk_id": 52,
          "document_id": "1909.02764",
          "start_idx": 20420,
          "end_idx": 20841
        },
        "page_content": "=73 % on TEC. The model trained on our AMMER corpus achieves an F1 score of 57%. This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set. These results are mostly in line with Bostan2018. Now we analyze how well the models trained in Experiment 1 perform when applied to our data set. The results are shown in column \u201cSimple\u201d in Table TAB",
        "type": "Document"
      },
      {
        "id": "e3060c9f-09bf-4735-86e2-63d0d62382e7",
        "metadata": {
          "chunk_id": 53,
          "document_id": "1909.02764",
          "start_idx": 20698,
          "end_idx": 21118
        },
        "page_content": " to our data set. The results are shown in column \u201cSimple\u201d in Table TABREF19. We observe a clear drop in performance, with an average of F$_1$=48 %. The best performing model is again the one trained on TEC, en par with the one trained on the Figure8 data. The model trained on ISEAR performs second best in Experiment 1, it performs worst in Experiment 2. To adapt models trained on previously existing data sets to our",
        "type": "Document"
      },
      {
        "id": "5e0f3225-070e-4250-8f70-93c083311ec4",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What semantic rules are proposed?",
    "llm_answer": "Ten semantic rules are proposed, based on the presentation of BIBREF16.  Five are used in the proposed method; the other five are used to compute polarity.\n",
    "context": [
      {
        "id": "317fbbcf-f2c0-44ed-87e8-3fd9e485a775",
        "metadata": {
          "chunk_id": 21,
          "document_id": "1706.08032",
          "start_idx": 8281,
          "end_idx": 8786
        },
        "page_content": " and devised ten semantic rules in their hybrid approach based on the presentation of BIBREF16 . We use five rules in the semantic rules set because other five rules are only used to compute polarity of words after POS tagging or Parsing steps. We follow the same naming convention for rules utilized by BIBREF11 to represent the rules utilized in our proposed method. The rules utilized in the proposed method are displayed in Table TABREF15 in which is included examples from STS Corpus and output after",
        "type": "Document"
      },
      {
        "id": "21a42ff1-5f16-427b-baea-4d2377ca9323",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1909.12140",
          "start_idx": 2277,
          "end_idx": 2783
        },
        "page_content": " These patterns were heuristically determined in a comprehensive linguistic analysis and encode syntactic and lexical features that can be derived from a sentence's parse tree. Each rule specifies (1) how to split up and rephrase the input into structurally simplified sentences and (2) how to set up a semantic hierarchy between them. They are recursively applied on a given source sentence in a top-down fashion. When no more rule matches, the algorithm stops and returns the generated discourse tree. In",
        "type": "Document"
      },
      {
        "id": "9aadcff9-96f8-47e5-8187-e2925810555f",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1706.08032",
          "start_idx": 7773,
          "end_idx": 8281
        },
        "page_content": " the main sentiment is only in the sub-sentences following but and however. This inspires a processing step to remove unessential parts in a tweet. Rule-based approach can assists these problems in handling negation and dealing with specific PoS particles led to effectively affect the final output of classification BIBREF11 BIBREF16 . BIBREF11 summarized a full presentation of their semantic rules approach and devised ten semantic rules in their hybrid approach based on the presentation of BIBREF16 . We",
        "type": "Document"
      },
      {
        "id": "d298f1ed-0bff-400f-8f60-9b3f9e383c0f",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1909.12140",
          "start_idx": 2552,
          "end_idx": 3098
        },
        "page_content": "down fashion. When no more rule matches, the algorithm stops and returns the generated discourse tree. In a first step, source sentences that present a complex linguistic form are turned into clean, compact structures by decomposing clausal and phrasal components. For this purpose, the transformation rules encode both the splitting points and rephrasing procedure for reconstructing proper sentences. Each split will create two or more sentences with a simplified syntax. To establish a semantic hierarchy between them, two subtasks are carried",
        "type": "Document"
      },
      {
        "id": "ec321cf6-5a3c-4121-8845-f4fe7dbb76c2",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1710.01492",
          "start_idx": 3953,
          "end_idx": 4458
        },
        "page_content": " label (2-point vs. 3-point vs. ordinal), etc. All these are explored at SemEval, the International Workshop on Semantic Evaluation, which has created a number of benchmark datasets and has enabled direct comparison between different systems and approaches, both as part of the competition and beyond. Traditionally, the task has been addressed using supervised and semi-supervised methods, as well as using distant supervision, with the most important resource being sentiment polarity lexicons, and with",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance of the model?",
    "llm_answer": "The model trained on the AMMER corpus achieves an F1 score of 57%.  Other models show substantial improvement on DROP's development set.\n",
    "context": [
      {
        "id": "0ab3fdf3-8ea1-480c-8cac-92715fc9d565",
        "metadata": {
          "chunk_id": 52,
          "document_id": "1909.02764",
          "start_idx": 20420,
          "end_idx": 20841
        },
        "page_content": "=73 % on TEC. The model trained on our AMMER corpus achieves an F1 score of 57%. This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set. These results are mostly in line with Bostan2018. Now we analyze how well the models trained in Experiment 1 perform when applied to our data set. The results are shown in column \u201cSimple\u201d in Table TAB",
        "type": "Document"
      },
      {
        "id": "5e0f3225-070e-4250-8f70-93c083311ec4",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "dc948dec-3a23-4196-b1ee-c386b0464b4d",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1909.13375",
          "start_idx": 20052,
          "end_idx": 20444
        },
        "page_content": "s. For the large models we used a batch size of 3 with a learning rate of $5e^{-6}$ and trained for 5 epochs, except for the model without the single-span heads that was trained with a batch size of 2 for 7 epochs. F1 was used as our validation metric. All models were trained on a single GPU with 12-16GB of memory. Table TABREF24 shows the results on DROP's development set. Compared to our",
        "type": "Document"
      },
      {
        "id": "96ecbdb9-17ee-46e0-9f1c-9eef560fb526",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1912.13109",
          "start_idx": 10203,
          "end_idx": 10681
        },
        "page_content": " metrics - Recall rate, F1 score and Overall accuracy. Choice of model parameters were in the above models were inspired from previous work done but then were tuned to the best performance of the Test dataset. Following parameters were considered for tuning. Learning rate: Based on grid search the best performance was achieved when learning rate was set to 0.01. This value was arrived by a grid search on lr parameter. Number of Bidirectional LSTM units: A set of 32, 64, 128",
        "type": "Document"
      },
      {
        "id": "7f324f7a-757d-477c-a688-6562382fe543",
        "metadata": {
          "chunk_id": 86,
          "document_id": "1703.04617",
          "start_idx": 23354,
          "end_idx": 23758
        },
        "page_content": " be 20, the performance improves to 68.73%/77.74% on EM and F1, respectively, which corresponds to the results of our model reported in Table 1 . Furthermore, after submitted our result, we have experimented with a large value of $K$ and found that when $K=100$ , we can achieve a better performance of 69.10%/78.38% on the development set. Figure UID61 shows the EM/F1 scores of different question types",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was the performance of both approaches on their dataset?",
    "llm_answer": "The context states that a single system performed better than a feature-rich system on Task A with sufficient training data.  On Task B, the performance is described as \"limited\" without specifics.  The performance of the other approaches mentioned (TRADE, etc.) is also described in terms of specific metrics (e.g., joint state accuracy), but not as overall performance on their datasets.\n",
    "context": [
      {
        "id": "5c46d1e3-00dd-432e-b58a-1aaa94f3928f",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1909.00542",
          "start_idx": 10682,
          "end_idx": 11217
        },
        "page_content": " experiments. We ran the version trained on Python ROUGE once, and the version trained on Perl twice. The two Perl runs have different results, and one of them clearly outperforms the Python run. However, given the differences of results between the two Perl runs we advice to re-run the experiments multiple times and obtain the mean and standard deviation of the runs before concluding whether there is any statistical difference between the results. But it seems that there may be an improvement of the final evaluation results when",
        "type": "Document"
      },
      {
        "id": "95c62085-b478-4b5e-a4fa-049a6c8fc5d0",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1908.10449",
          "start_idx": 22880,
          "end_idx": 23381
        },
        "page_content": " hard time learning in this setting. As shown in Table TABREF16, both datasets have a vocabulary size of more than 100k. This is much larger than in the other two settings, where on average the length of questions is around 10. This suggests that the methods with better sample efficiency are needed to act in more realistic problem settings with huge action spaces. Experiments also show that a larger memory slot size always helps. Intuitively, with a memory mechanism (either implicit or explicit),",
        "type": "Document"
      },
      {
        "id": "a34ba259-71d4-4abe-aba1-4c61f5e677b8",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1909.13375",
          "start_idx": 19611,
          "end_idx": 20052
        },
        "page_content": " to gain further insights on the effect of the components that we ourselves introduced. For ease of performance comparison, we followed NABERT+'s training settings: we used the BERT Adam optimizer from huggingface with default settings and a learning rate of $1e^{-5}$. The only difference was that we used a batch size of 12. We trained our base model for 20 epochs. For the large models we used a batch size of 3 with a learning rate of $5",
        "type": "Document"
      },
      {
        "id": "abc51b19-3db2-4cbf-ac46-ff97aa2fa1d1",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1603.07044",
          "start_idx": 14997,
          "end_idx": 15515
        },
        "page_content": " task A already had a substantial amount of training data. Table 4 gives the final comparison between different models (we only list the MAP score because it is the official score for the challenge). Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited",
        "type": "Document"
      },
      {
        "id": "bc03fd03-c242-49e0-bafb-22e9b13f22d8",
        "metadata": {
          "chunk_id": 72,
          "document_id": "2002.11893",
          "start_idx": 26889,
          "end_idx": 27376
        },
        "page_content": " system often records full user constraints, while the last one records relaxed constraints for recommendation. Thus the last one involves system policy, which is out of the scope of state tracking. We used the first query for these models and left state tracking with recommendation for future work. Result Analysis: We evaluated the joint state accuracy (percentage of exact matching) of these two models (Table TABREF31). TRADE, the state-of-the-art model on MultiWOZ, performs poorly",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What genres are covered?",
    "llm_answer": "mechanics, thermodynamics, poetry, art, Shakespeare, dance, literature, novel, symphony, drama, orchestra, music, ballet, arts, creative, sculpture, coming-of-age, strong-female-lead, LGBTQ+.\n",
    "context": [
      {
        "id": "6ecbf71a-a840-41c6-a574-6a8545d5194a",
        "metadata": {
          "chunk_id": 64,
          "document_id": "1910.14497",
          "start_idx": 18963,
          "end_idx": 19236
        },
        "page_content": "mechanics\", \"thermodynamics\" Y: \"poetry\", \"art\", \"Shakespeare\", \"dance\", \"literature\", \"novel\", \"symphony\", \"drama\", \"orchestra\", \"music\", \"ballet\", \"arts\", \"creative\", \"sculpture\" A: \"brother\", \"father\", \"uncle\", \"grandfather\", \"son\", \"he\", \"his\", \"him\", \"man\", \"himself\",",
        "type": "Document"
      },
      {
        "id": "a2c164fc-bd29-46ed-928e-f8614477c500",
        "metadata": {
          "chunk_id": 52,
          "document_id": "1910.14497",
          "start_idx": 17148,
          "end_idx": 17415
        },
        "page_content": "agony\", \"prison\" Instruments vs Weapons / Pleasant vs Unpleasant: X: \"bagpipe\", \"cello\", \"guitar\", \"lute\", \"trombone\", \"banjo\", \"clarinet\", \"harmonica\", \"mandolin\", \"trumpet\", \"bassoon\", \"drum\", \"harp\", \"oboe\", \"tuba\", \"bell\", \"fiddle\", \"harpsichord\", \"piano\", \"viola",
        "type": "Document"
      },
      {
        "id": "30637857-ef7d-4fd1-bfe8-6f5216e83f4b",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1911.01799",
          "start_idx": 3245,
          "end_idx": 3755
        },
        "page_content": " advertisement. The speech of a particular speaker may be in more than 5 genres. As a comparison, most of the utterances in VoxCeleb were extracted from interview videos. The diversity in genres makes our database more representative for the true scenarios in unconstrained conditions, but also more challenging. CN-Celeb is not fully automated, but involves human check. We found that more complex the genre is, more errors the automated pipeline tends to produce. Ironically, the error-pron segments could be",
        "type": "Document"
      },
      {
        "id": "8b3c3cb3-5797-4daa-bf16-2561d190d69c",
        "metadata": {
          "chunk_id": 53,
          "document_id": "1910.14497",
          "start_idx": 17148,
          "end_idx": 17391
        },
        "page_content": " \"bell\", \"fiddle\", \"harpsichord\", \"piano\", \"viola\", \"bongo\", \"flute\", \"horn\", \"saxophone\", \"violin\" Y: \"arrow\", \"club\", \"gun\", \"missile\", \"spear\", \"ax\", \"dagger\", \"harpoon\", \"pistol\", \"sword\", \"blade\", \"dynamite\", \"hatchet\", \"rifle\", \"tank\", \"",
        "type": "Document"
      },
      {
        "id": "10b0b69a-a822-437a-ae6a-58657aeee9b2",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1910.11769",
          "start_idx": 6259,
          "end_idx": 6704
        },
        "page_content": " 1600 unique titles across multiple categories, including 88 titles (1520 passages) from Project Gutenberg. All of the modern narratives were written after the year 2000, with notable amount of themes in coming-of-age, strong-female-lead, and LGBTQ+. The genre distribution is listed in Table TABREF8. In the final dataset, 21.0% of the data has consensus between all annotators, 73.5% has majority agreement, and 5.48% has labels assigned after",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which of the two speech recognition models works better overall on CN-Celeb?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "1aa4e319-6dad-4c8b-8acb-4b4caf2d64dd",
        "metadata": {
          "chunk_id": 40,
          "document_id": "1911.01799",
          "start_idx": 14641,
          "end_idx": 15051
        },
        "page_content": " with re-training the back-end model with VoxCeleb(L) (4th row), the performance on SITW becomes better than the same-source result on CN-Celeb(E) (11.34% vs 14.24%). All these results reconfirmed the significant difference between the two datasets, and indicates that CN-Celeb is more challenging than VoxCeleb. We introduced a free dataset CN-Celeb for speaker recognition research. The dataset contains more",
        "type": "Document"
      },
      {
        "id": "22e658c1-15c1-4fc9-bc91-91d4380347a4",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1911.01799",
          "start_idx": 15300,
          "end_idx": 15674
        },
        "page_content": "art speaker recognition models. Experimental results demonstrated that CN-Celeb is significantly different from VoxCeleb, and it is more challenging for speaker recognition research. The EER performance we obtained in this paper suggests that in unconstrained conditions, the performance of the current speaker recognition techniques might be much worse than it was thought.",
        "type": "Document"
      },
      {
        "id": "5542893a-e7f1-46d4-a813-7ebd7ed02591",
        "metadata": {
          "chunk_id": 41,
          "document_id": "1911.01799",
          "start_idx": 15051,
          "end_idx": 15520
        },
        "page_content": "b. We introduced a free dataset CN-Celeb for speaker recognition research. The dataset contains more than $130k$ utterances from $1,000$ Chinese celebrities, and covers 11 different genres in real world. We compared CN-Celeb and VoxCeleb, a widely used dataset in speaker recognition, by setting up a series of experiments based on two state-of-the-art speaker recognition models. Experimental results demonstrated that CN-Celeb is significantly different from VoxCeleb",
        "type": "Document"
      },
      {
        "id": "1e55633c-1949-4d91-b8d6-f62cae2ee4fc",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1911.01799",
          "start_idx": 13776,
          "end_idx": 14166
        },
        "page_content": " source (SITW). To further compare CN-Celeb and VoxCeleb in a quantitative way, we built systems based on CN-Celeb and VoxCeleb, respectively. For a fair comparison, we randomly sampled 800 speakers from VoxCeleb and built a new dataset VoxCeleb(L) whose size is comparable to CN-Celeb(T). This data set was used for back-end (LDA-PLDA) training. The experimental results are shown in Table",
        "type": "Document"
      },
      {
        "id": "763c90a3-6cde-431d-a022-b94b42097f91",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1911.01799",
          "start_idx": 5298,
          "end_idx": 5793
        },
        "page_content": " between CN-Celeb and VoxCeleb. Compared to VoxCeleb, CN-Celeb is a more complex dataset and more challenging for speaker recognition research. More details of these challenges are as follows. Most of the utterances involve real-world noise, including ambient noise, background babbling, music, cheers and laugh. A certain amount of utterances involve strong and overlapped background speakers, especially in the dram and movie genres. Most of speakers have different genres of utterances, which",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much is performance on CN-Celeb inferior to performance on VoxCeleb?",
    "llm_answer": "14.24% (CN-Celeb) vs. 11.34% (VoxCeleb(L) on SITW)\n",
    "context": [
      {
        "id": "6ee07f15-f7f0-4ab8-b1a9-266125f5db7c",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1911.01799",
          "start_idx": 13328,
          "end_idx": 13776
        },
        "page_content": "-vector system and the x-vector system, the performance on CN-Celeb(E) is much worse than the performance on SITW and SITW(S). This indicates that there is big difference between these two datasets. From another perspective, it demonstrates that the model trained with VoxCeleb does not generalize well, although it has achieved reasonable performance on data from a similar source (SITW). To further compare CN-Celeb and VoxCeleb in a quantitative",
        "type": "Document"
      },
      {
        "id": "1e55633c-1949-4d91-b8d6-f62cae2ee4fc",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1911.01799",
          "start_idx": 13776,
          "end_idx": 14166
        },
        "page_content": " source (SITW). To further compare CN-Celeb and VoxCeleb in a quantitative way, we built systems based on CN-Celeb and VoxCeleb, respectively. For a fair comparison, we randomly sampled 800 speakers from VoxCeleb and built a new dataset VoxCeleb(L) whose size is comparable to CN-Celeb(T). This data set was used for back-end (LDA-PLDA) training. The experimental results are shown in Table",
        "type": "Document"
      },
      {
        "id": "1aa4e319-6dad-4c8b-8acb-4b4caf2d64dd",
        "metadata": {
          "chunk_id": 40,
          "document_id": "1911.01799",
          "start_idx": 14641,
          "end_idx": 15051
        },
        "page_content": " with re-training the back-end model with VoxCeleb(L) (4th row), the performance on SITW becomes better than the same-source result on CN-Celeb(E) (11.34% vs 14.24%). All these results reconfirmed the significant difference between the two datasets, and indicates that CN-Celeb is more challenging than VoxCeleb. We introduced a free dataset CN-Celeb for speaker recognition research. The dataset contains more",
        "type": "Document"
      },
      {
        "id": "799bbb02-345a-46fb-94c4-607f3de734a4",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1911.01799",
          "start_idx": 14437,
          "end_idx": 14763
        },
        "page_content": " SITW(S) (1st row). This is understandable as VoxCeleb and SITW(S) were collected from the same source. For the pure CN-Celeb system (2nd row), although CN-Celeb(T) and CN-Celeb(E) are from the same source, the performance is still poor (14.24%). More importantly, with re-training the back-end model with VoxCeleb(L) (4th row",
        "type": "Document"
      },
      {
        "id": "26c18c8b-5b6e-41b9-a106-6842e623c97c",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1911.01799",
          "start_idx": 4886,
          "end_idx": 5298
        },
        "page_content": " In summary, CN-Celeb contains over $130,000$ utterances from $1,000$ Chinese celebrities. It covers 11 genres and the total amount of speech waveforms is 274 hours. Table TABREF5 gives the data distribution over the genres, and Table TABREF6 presents the data distribution over the length of utterances. Table TABREF13 summarizes the main difference between CN-Celeb and VoxCeleb. Compared to VoxCeleb, CN-Celeb",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do the authors measure performance?",
    "llm_answer": "The authors use EM-score, F\u2081-score, F\u2081 score (for question answering), Spearman's \u03c1, Kendall's \u03c4, and Pearson's r.  They also use metrics for search and reading accuracy, WPSLOR and ROUGE.\n",
    "context": [
      {
        "id": "e929c104-c4ce-40d5-aca7-762538286335",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1908.06606",
          "start_idx": 15988,
          "end_idx": 16383
        },
        "page_content": ") and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage",
        "type": "Document"
      },
      {
        "id": "e901bd2a-cc88-4572-a952-318c9e30a71a",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1908.10449",
          "start_idx": 10837,
          "end_idx": 11300
        },
        "page_content": "RC involves both MRC and RL, we adopt evaluation metrics from both settings. First, as a question answering task, we use $\\text{F}_1$ score to compare predicted answers against ground-truth, as in previous works. When there exist multiple ground-truth answers, we report the max $\\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we evaluate an agent's performance during both its training and testing phases.",
        "type": "Document"
      },
      {
        "id": "9612e20e-4bb2-43b7-ac27-1494213608ba",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1707.03904",
          "start_idx": 4820,
          "end_idx": 5304
        },
        "page_content": "5\\%$ on Quasar-S and Quasar-T, while human performance is $50\\%$ and $60.6\\%$ respectively. For the automatic systems, we see an interesting tension between searching and reading accuracies \u2013 retrieving more documents in the search phase leads to a higher coverage of answers, but makes the comprehension task more difficult. We also collect annotations on a subset of the development set questions to allow researchers to analyze the categories in which their system performs well or",
        "type": "Document"
      },
      {
        "id": "1d812102-98f4-45bc-b355-c1653ccfaf7e",
        "metadata": {
          "chunk_id": 72,
          "document_id": "1809.08731",
          "start_idx": 24617,
          "end_idx": 25067
        },
        "page_content": " metrics on a benchmark dataset. Additionally, they can be applied even in settings where no references are available, or would be costly to obtain. Finally, for given references, we proposed the reference-based metric ROUGE-LM, which consists of a combination of WPSLOR and ROUGE. Thus, we were able to obtain an even more accurate fluency evaluation. We would like to thank Sebastian Ebert and Samuel Bowman for their detailed and helpful feedback.",
        "type": "Document"
      },
      {
        "id": "b6548093-0afc-44f1-b61d-963a3b6b2fbf",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1909.00578",
          "start_idx": 8378,
          "end_idx": 8820
        },
        "page_content": " calculate the average of the predicted scores for the summaries of each particular contestant, and the average of the corresponding manual scores assigned to the contestant's summaries. We measure the correlation between the two (predicted vs. manual) across all contestants using Spearman's $\\rho $, Kendall's $\\tau $ and Pearson's $r$. We train and test the Sum-QE and BiGRU-ATT versions using a 3-fold procedure. In each fold, we train on",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the latest paper covered by this survey?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "eea3c109-6b88-4981-b78a-e0dfbd4ae610",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1912.13109",
          "start_idx": 14747,
          "end_idx": 15180
        },
        "page_content": " sentiment analysis: A survey Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery [8] Caruana, Rich and Lawrence, Steve and Giles, C Lee Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping Advances in neural information processing systems [9] Beale, Mark Hudson and Hagan, Martin T and Demuth, Howard B Neural network toolbox user\u2019s guide The MathWorks Incs [10] Chollet, Fran\u00e7ois and",
        "type": "Document"
      },
      {
        "id": "a7601ac1-81cf-4f31-8dfe-c5e1b0c654cb",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1912.13109",
          "start_idx": 14243,
          "end_idx": 14747
        },
        "page_content": "-CNN model for Vietnamese sentiment analysis 2017 9th international conference on knowledge and systems engineering (KSE) [6] Pennington, Jeffrey and Socher, Richard and Manning, Christopher Glove: Global vectors for word representation Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) [7] Zhang, Lei and Wang, Shuai and Liu, Bing Deep learning for sentiment analysis: A survey Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery [8] Caruana",
        "type": "Document"
      },
      {
        "id": "e21a0cfd-c2ea-44f1-87e5-62836fa66a6b",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1905.08949",
          "start_idx": 1553,
          "end_idx": 1945
        },
        "page_content": "2012 BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 . Recently, driven by advances in deep learning, QG research has also begun to utilize \u201cneural\u201d techniques, to develop end-to-end neural models to generate deeper questions BIBREF8 and to pursue broader applications BIBREF9 , BIBREF10 . While there have been considerable advances made in NQG, the area lacks a comprehensive survey. This paper fills",
        "type": "Document"
      },
      {
        "id": "08f284a9-cff7-4835-bdcb-09f1df2a3076",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1910.09399",
          "start_idx": 5309,
          "end_idx": 5847
        },
        "page_content": " further improvement. The visual abstract of the survey and the list of reviewed GAN frameworks is shown in Figure FIGREF8. black The remainder of the survey is organized as follows. Section 2 presents a brief summary of existing works on subjects similar to that of this paper and highlights the key distinctions making ours unique. Section 3 gives a short introduction to GANs and some preliminary concepts related to image generation, as they are the engines that make text-to-image synthesis possible and are essential building blocks",
        "type": "Document"
      },
      {
        "id": "d451c276-9ade-4d64-92b5-2fd1ba146e8f",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1910.09399",
          "start_idx": 7020,
          "end_idx": 7508
        },
        "page_content": " Recently, many new GAN architectures and designs have been proposed to use GANs for different applications, e.g. using GANs to generate sentimental texts BIBREF18, or using GANs to transform natural images into cartoons BIBREF19. Although GANs are becoming increasingly popular, very few survey papers currently exist to summarize and outline contemporaneous technical innovations and contributions of different GAN architectures BIBREF20, BIBREF21. Survey papers specifically attuned to",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What learning paradigms do they cover in this survey?",
    "llm_answer": "(1) in-domain, (2) out-of-domain, and (3) transfer learning.\n",
    "context": [
      {
        "id": "65333987-2358-483b-95cd-3ff028ff6a96",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      },
      {
        "id": "00c65782-38c7-4229-a350-95fda62b65a6",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1703.04617",
          "start_idx": 789,
          "end_idx": 1343
        },
        "page_content": " an end-to-end neural network framework, since we regard question understanding is of importance for such problems. We first introduced syntactic information to help encode questions. We then viewed and modelled different types of questions and the information shared among them as an adaptation problem and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results on our competitive baselines. Recent advance on reading comprehension and question answering has",
        "type": "Document"
      },
      {
        "id": "338b0462-0627-4a30-a6bb-da4380439d78",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1703.04617",
          "start_idx": 3555,
          "end_idx": 4085
        },
        "page_content": " has the potential to capture long-distance interactions over the structures. Different types of questions are often used to seek for different types of information. For example, a \"what\" question could have very different property from that of a \"why\" question, while they may share information and need to be trained together instead of separately. We view this as a \"adaptation\" problem to let different types of questions share a basic model but still discriminate them when needed. Specifically, we are motivated by the ideas",
        "type": "Document"
      },
      {
        "id": "54830273-3c90-417d-8898-e4c92c7fa94a",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1703.04617",
          "start_idx": 14584,
          "end_idx": 15140
        },
        "page_content": "ity. The previous models are often trained for all questions without explicitly discriminating different question types; however, for a target question, both the common features shared by all questions and the specific features for a specific type of question are further considered in this paper, as they could potentially obey different distributions. In this paper we further explicitly model different types of questions in the end-to-end training. We start from a simple way to first analyze the word frequency of all questions, and obtain top-10 most",
        "type": "Document"
      },
      {
        "id": "1f515e22-fa8c-42c9-8c88-3d8828ca4201",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1909.00430",
          "start_idx": 23326,
          "end_idx": 23878
        },
        "page_content": " (knowing the geographical region gives a prior on languages spoken), training predictors for renal failure from textual medical records given classifier for diabetes (there is a strong correlation between the two conditions), training a political affiliation classifier from social media tweets based on age-group classifiers, zip-code information, or social-status classifiers (there are known correlations between all of these to political affiliation), training hate-speech detection based on emotion detection, and so on. The work was supported in",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are all the input modalities considered in prior work in question generation?",
    "llm_answer": "Sentence or paragraph, image, and table.\n",
    "context": [
      {
        "id": "057b7c17-92ad-4851-8b55-b1862b09568b",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1905.08949",
          "start_idx": 6831,
          "end_idx": 7344
        },
        "page_content": " . We discuss adapting NQG models to other input modalities in Section \"Wider Input Modalities\" . Finally, we consider the required cognitive process behind question asking, a distinguishing factor for questions BIBREF32 . A typical framework that attempts to categorize the cognitive levels involved in question asking comes from Bloom's taxonomy BIBREF33 , which has undergone several revisions and currently has six cognitive levels: Remembering, Understanding, Applying, Analyzing, Evaluating and Creating BIB",
        "type": "Document"
      },
      {
        "id": "3090caa2-641a-44fc-adbf-df390d6cafa5",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1905.08949",
          "start_idx": 5794,
          "end_idx": 6247
        },
        "page_content": " be further explored. The change of learning paradigm in NQG era is also represented by multi-task learning with other NLP tasks, for which we discuss in Section \"Multi-task Learning\" . Question generation is an NLG task for which the input has a wealth of possibilities depending on applications. While a host of input modalities have been considered in other NLG tasks, such as text summarization BIBREF24 , image captioning BIBREF25 and table-to-text",
        "type": "Document"
      },
      {
        "id": "83c3f771-bbe4-47f7-8267-cb499dcad923",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1905.08949",
          "start_idx": 3484,
          "end_idx": 3942
        },
        "page_content": " Given a sentence or a paragraph as input, content selection selects a particular salient topic worthwhile to ask about and determines the question type (What, When, Who, etc.). Approaches either take a syntactic BIBREF11 , BIBREF12 , BIBREF13 or semantic BIBREF14 , BIBREF3 , BIBREF15 , BIBREF16 tack, both starting by applying syntactic or semantic parsing, respectively, to obtain intermediate symbolic representations. Question construction then converts",
        "type": "Document"
      },
      {
        "id": "82e780a3-c865-4d55-951b-4d2d6f7f0706",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1905.08949",
          "start_idx": 17216,
          "end_idx": 17745
        },
        "page_content": " the question intention, as different intents may yield different questions, even when presented with the same (passage, answer) input pair. This points to the direction of exploring question pragmatics, where external contextual information (such as intent) can inform and influence how questions should optimally be generated. Leveraging rich paragraph-level contexts around the input text is another natural consideration to produce better questions. According to BIBREF47 , around 20% of questions in SQuAD require paragraph-",
        "type": "Document"
      },
      {
        "id": "8a7b8a56-bb48-415c-afba-38c02e4059a8",
        "metadata": {
          "chunk_id": 71,
          "document_id": "1905.08949",
          "start_idx": 26238,
          "end_idx": 26651
        },
        "page_content": "generated questions also overgeneralize \u2014 a model asks \u201cWho was born in New York?\" when given the triple (Donald_Trump, Place_of_birth, New_York). To solve this, BIBREF29 enrich the input with a sequence of keywords collected from its related triples. Visual Question Generation (VQG) is another emerging topic which aims to ask questions given an image. We categorize VQG into grounded- and open-ended VQG by the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do this framework facilitate demographic inference from social media?",
    "llm_answer": "The framework leverages weighted lexicons of terms (e.g., BIBREF73), profile descriptions (with rules and patterns), homophily interaction information, and content to predict age and gender.  It also uses visual features, emotional features, and domain-specific features.\n",
    "context": [
      {
        "id": "d83baca4-cb6e-4b49-9411-a7dfb2cc28c6",
        "metadata": {
          "chunk_id": 69,
          "document_id": "1902.06843",
          "start_idx": 27195,
          "end_idx": 27674
        },
        "page_content": " leverage both the visual and textual content for predicting age and gender. Prediction with Textual Content: We employ BIBREF73 's weighted lexicon of terms that uses the dataset of 75,394 Facebook users who shared their status, age and gender. The predictive power of this lexica was evaluated on Twitter, blog, and Facebook, showing promising results BIBREF73 . Utilizing these two weighted lexicon of terms, we are predicting the demographic information (age or gender) of IN",
        "type": "Document"
      },
      {
        "id": "7ef67d5f-3098-42eb-8048-19cd1014a664",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1902.06843",
          "start_idx": 11357,
          "end_idx": 11906
        },
        "page_content": " Twitter users. Similarly, relying on profile descriptions while devising a set of rules and patterns, a novel model introduced for extracting age for Twitter users BIBREF41 . They also parse description for occupation by consulting the SOC2010 list of occupations and validating it through social surveys. A novel age inference model was developed while relying on homophily interaction information and content for predicting age of Twitter users BIBREF42 . The limitations of textual content for predicting age and gender was highlighted by BIBREF",
        "type": "Document"
      },
      {
        "id": "eb047d03-2922-48a3-8b4d-3526a3853aa6",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1902.06843",
          "start_idx": 10268,
          "end_idx": 10791
        },
        "page_content": " visual features, emotional features BIBREF34 , topic-level features, and domain-specific features. Particularly, our choice of multi-model prediction framework is intended to improve upon the prior works involving use of images in multimodal depression analysis BIBREF33 and prior works on studying Instagram photos BIBREF6 , BIBREF35 . Demographic information inference on Social Media:  There is a growing interest in understanding online user's demographic information due to its numerous applications in healthcare BIB",
        "type": "Document"
      },
      {
        "id": "f1025181-1ead-46f6-81d4-8292a4834194",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1705.09665",
          "start_idx": 4847,
          "end_idx": 5438
        },
        "page_content": " stay). Such insights into the relation between collective identity and user engagement can be informative to community maintainers seeking to better understand growth patterns within their online communities. More generally, our methodology stands as an example of how sociological questions can be addressed in a multi-community setting. In performing our analyses across a rich variety of communities, we reveal both the diversity of phenomena that can occur, as well as the systematic nature of this diversity. A community's identity derives from its members' common interests and shared",
        "type": "Document"
      },
      {
        "id": "549e396f-afac-4ccf-b992-6ee8c59b81d5",
        "metadata": {
          "chunk_id": 88,
          "document_id": "1705.09665",
          "start_idx": 36224,
          "end_idx": 36805
        },
        "page_content": " newcomers at certain points in their lifecycle? Another important avenue of future work is to explore other ways of mapping the landscape of online communities. For example, combining structural properties of communities BIBREF40 with topical information BIBREF35 and with our identity-based measures could further characterize and explain variations in user engagement patterns. Furthermore, extending the present analyses to even more diverse communities supported by different platforms (e.g., GitHub, StackExchange, Wikipedia) could enable the characterization of more complex",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the data annotated?",
    "llm_answer": "The data is annotated using a final annotation schema, an iterative process, and scenario-specific annotation templates.  NP heads are labeled with participant labels.  Dialogue acts and states are annotated using rules and by human annotators.  In-house experts annotate narrative content and passages are accepted if the label matches any MTurk annotator.  The BIO format (Inside, Outside, Beginning) is used at a high level (CRF).\n",
    "context": [
      {
        "id": "9c29b30f-983c-4da5-9469-bc961c44c746",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1703.05260",
          "start_idx": 7578,
          "end_idx": 8108
        },
        "page_content": ". This section deals with the annotation of the data. We first describe the final annotation schema. Then, we describe the iterative process of corpus annotation and the refinement of the schema. This refinement was necessary due to the complexity of the annotation. For each of the scenarios, we designed a specific annotation template. A script template consists of scenario-specific event and participant labels. An example of a template is shown in Table 1 . All NP heads in the corpus were annotated with a participant label;",
        "type": "Document"
      },
      {
        "id": "28eaec66-d29a-442f-80e4-0404e8d15452",
        "metadata": {
          "chunk_id": 22,
          "document_id": "2002.11893",
          "start_idx": 8390,
          "end_idx": 8922
        },
        "page_content": " before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states. Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three",
        "type": "Document"
      },
      {
        "id": "6d2fb9a2-9305-4bd8-864d-143c83b5e90c",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1909.00430",
          "start_idx": 0,
          "end_idx": 440
        },
        "page_content": "Data annotation is a key bottleneck in many data driven algorithms. Specifically, deep learning models, which became a prominent tool in many data driven tasks in recent years, require large datasets to work well. However, many tasks require manual annotations which are relatively hard to obtain at scale. An attractive alternative is lightly supervised learning BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF",
        "type": "Document"
      },
      {
        "id": "45540b48-0892-47ef-9a7d-86f9efe45ec0",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1910.11769",
          "start_idx": 4963,
          "end_idx": 5497
        },
        "page_content": " using in-house data annotators who are experts in narrative content. A passage is accepted as valid if the in-house annotator's label matched any one of the MTurk annotators' labels. The remaining passages are discarded. We provide the fraction of annotator agreement for each label in the dataset. Though passages may lose some emotional context when read independently of the complete narrative, we believe annotator agreement on our dataset supports the assertion that small excerpts can still convey coherent emotions. During the",
        "type": "Document"
      },
      {
        "id": "6cbe4da1-641e-4851-b898-32218f4b87d1",
        "metadata": {
          "chunk_id": 54,
          "document_id": "1911.07228",
          "start_idx": 14818,
          "end_idx": 15154
        },
        "page_content": " BIO format (Inside, Outside, Beginning) BIBREF6 at the top layer (CRF). If we can unify the labeling of the annotators between the train, dev and test sets. We will improve data quality and classifier. It is better if there is a pre-trained word embeddings that overlays the data, and segmentation algorithm need to be more accurately.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Where does the information on individual-level demographics come from?",
    "llm_answer": "Electronic health records, posted images (aesthetic features), profile pictures, screen names, language features from textual content, and external resources (personal blogs).\n",
    "context": [
      {
        "id": "f1025181-1ead-46f6-81d4-8292a4834194",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1705.09665",
          "start_idx": 4847,
          "end_idx": 5438
        },
        "page_content": " stay). Such insights into the relation between collective identity and user engagement can be informative to community maintainers seeking to better understand growth patterns within their online communities. More generally, our methodology stands as an example of how sociological questions can be addressed in a multi-community setting. In performing our analyses across a rich variety of communities, we reveal both the diversity of phenomena that can occur, as well as the systematic nature of this diversity. A community's identity derives from its members' common interests and shared",
        "type": "Document"
      },
      {
        "id": "2c625191-b4e8-41a8-96b3-6b70f07c64be",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1902.06843",
          "start_idx": 4454,
          "end_idx": 5006
        },
        "page_content": "REF9 . Inferring demographic information like gender and age can be crucial for stratifying our understanding of population-level epidemiology of mental health disorders. Relying on electronic health records data, previous studies explored gender differences in depressive behavior from different angles including prevalence, age at onset, comorbidities, as well as biological and psychosocial factors. For instance, women have been diagnosed with depression twice as often as men BIBREF10 and national psychiatric morbidity survey in Britain has shown",
        "type": "Document"
      },
      {
        "id": "b8b55ba4-3d9a-4646-b662-50e7c1221a7f",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1902.06843",
          "start_idx": 6173,
          "end_idx": 6709
        },
        "page_content": " of depression. The recent advancements in deep neural networks, specifically for image analysis task, can lead to determining demographic features such as age and gender BIBREF13 . We show that by determining and integrating heterogeneous set of features from different modalities \u2013 aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness), choice of profile picture (for gender, age, and facial expression), the screen name, the language features from both textual content and",
        "type": "Document"
      },
      {
        "id": "d83baca4-cb6e-4b49-9411-a7dfb2cc28c6",
        "metadata": {
          "chunk_id": 69,
          "document_id": "1902.06843",
          "start_idx": 27195,
          "end_idx": 27674
        },
        "page_content": " leverage both the visual and textual content for predicting age and gender. Prediction with Textual Content: We employ BIBREF73 's weighted lexicon of terms that uses the dataset of 75,394 Facebook users who shared their status, age and gender. The predictive power of this lexica was evaluated on Twitter, blog, and Facebook, showing promising results BIBREF73 . Utilizing these two weighted lexicon of terms, we are predicting the demographic information (age or gender) of IN",
        "type": "Document"
      },
      {
        "id": "ee594dde-b8c4-4fdf-8fa0-11218e9ecbfd",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1902.06843",
          "start_idx": 10565,
          "end_idx": 11055
        },
        "page_content": " is a growing interest in understanding online user's demographic information due to its numerous applications in healthcare BIBREF36 , BIBREF37 . A supervised model developed by BIBREF38 for determining users' gender by employing features such as screen-name, full-name, profile description and content on external resources (e.g., personal blog). Employing features including emoticons, acronyms, slangs, punctuations, capitalization, sentence length and included links/images, along with",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the source of the user interaction data? ",
    "llm_answer": "Reddit subreddits from January 2013 to December 2014.\n",
    "context": [
      {
        "id": "045e9ffc-5f6d-4f44-b97a-9663e6b99e7d",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1705.09665",
          "start_idx": 12917,
          "end_idx": 13479
        },
        "page_content": " multicommunity ecosystem make it an ideal landscape in which to closely examine the relation between varying community identities and social dynamics. Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk",
        "type": "Document"
      },
      {
        "id": "99d93598-f200-475c-8d83-56620f16064e",
        "metadata": {
          "chunk_id": 75,
          "document_id": "1705.09665",
          "start_idx": 30548,
          "end_idx": 31135
        },
        "page_content": " these communities indicates that temporally ephemeral content might serve as an entry point into such a community, without necessarily engaging users in the long term. Our language-based typology and analysis of user engagement draws on and contributes to several distinct research threads, in addition to the many foundational studies cited in the previous sections. Multicommunity studies. Our investigation of user engagement in multicommunity settings follows prior literature which has examined differences in user and community dynamics across various online groups, such as email",
        "type": "Document"
      },
      {
        "id": "0f9abe23-0536-42ac-a849-39df7a6d80f2",
        "metadata": {
          "chunk_id": 77,
          "document_id": "1705.09665",
          "start_idx": 31419,
          "end_idx": 31975
        },
        "page_content": " content-based framework through which user engagement can be examined. Reddit has been a particularly useful setting for studying multiple communities in prior work. Such studies have mostly focused on characterizing how individual users engage across a multi-community platform BIBREF34 , BIBREF35 , or on specific user engagement patterns such as loyalty to particular communities BIBREF22 . We complement these studies by seeking to understand how features of communities can mediate a broad array of user engagement patterns within them. Typologies of",
        "type": "Document"
      },
      {
        "id": "12f4a650-55ec-4a10-b7ad-be50e9794cc0",
        "metadata": {
          "chunk_id": 18,
          "document_id": "2002.11893",
          "start_idx": 6697,
          "end_idx": 7228
        },
        "page_content": " user dialogue policy, we provide annotation of user states in addition to system states and dialogue acts. While the system state tracks the dialogue history, the user state is maintained by the user and indicates whether the sub-goals have been completed, which can be used to predict user actions. This information will facilitate the construction of the user simulator. To the best of our knowledge, CrossWOZ is the first large-scale Chinese dataset for task-oriented dialogue systems, which will largely alleviate the shortage",
        "type": "Document"
      },
      {
        "id": "389ea6cd-b8d1-4349-a0d6-fc9984e56757",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1909.02764",
          "start_idx": 15115,
          "end_idx": 15596
        },
        "page_content": " to 288 interactions, 180 between driver and the agent and 108 between driver and co-driver. The emotion self-ratings from the participants yielded 90 utterances labeled with joy, 26 with annoyance, 49 with insecurity, 9 with boredom, 111 with relaxation and 3 with no emotion. One example interaction per interaction type and emotion is shown in Table TABREF7. For further experiments, we only use joy, annoyance/anger, and insecurity/fear due to the small sample size for boredom",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the source of the textual data? ",
    "llm_answer": "Biomedical dataset and InScript corpus.\n",
    "context": [
      {
        "id": "64a25737-469b-4e82-875c-ba81cb2e5018",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1701.00185",
          "start_idx": 3790,
          "end_idx": 4304
        },
        "page_content": " this paper we use a new short text dataset, Biomedical, in the experiment to verify the effectiveness of our approach. Third, we put much effort on studying the influence of various different semantic features integrated in our self-taught CNN framework, which is not involved in the conference paper. For the purpose of reproducibility, we make the datasets and software used in our experiments publicly available at the website. The remainder of this paper is organized as follows: In Section SECREF2 , we first",
        "type": "Document"
      },
      {
        "id": "f420687c-1cd6-4839-be87-8508949281e2",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1707.03904",
          "start_idx": 14370,
          "end_idx": 14849
        },
        "page_content": "\ufffdopen-book\u201d setting, where they had access to a search engine over the short pseudo-documents extracted for each dataset (as described in Section \"Context Retrieval\" ). We decided to use short pseudo-documents for this exercise to reduce the burden of reading on the volunteers, though we note that the long pseudo-documents have greater coverage of answers. We also asked the volunteers to provide annotations to categorize the type of each question they were asked, and a label",
        "type": "Document"
      },
      {
        "id": "45540b48-0892-47ef-9a7d-86f9efe45ec0",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1910.11769",
          "start_idx": 4963,
          "end_idx": 5497
        },
        "page_content": " using in-house data annotators who are experts in narrative content. A passage is accepted as valid if the in-house annotator's label matched any one of the MTurk annotators' labels. The remaining passages are discarded. We provide the fraction of annotator agreement for each label in the dataset. Though passages may lose some emotional context when read independently of the complete narrative, we believe annotator agreement on our dataset supports the assertion that small excerpts can still convey coherent emotions. During the",
        "type": "Document"
      },
      {
        "id": "866385d4-c1fc-4a80-a560-9192ba5f31c6",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1912.01673",
          "start_idx": 3674,
          "end_idx": 4204
        },
        "page_content": "dings. The paper is structured as follows: related summarizes existing methods of sentence embeddings evaluation and related work. annotation describes our methodology for constructing our dataset. data details the obtained dataset and some first observations. We conclude and provide the link to the dataset in conclusion As hinted above, there are many methods of converting a sequence of words into a vector in a highly dimensional space. To name a few: BiLSTM with the max-pooling trained for natural language inference BIBREF",
        "type": "Document"
      },
      {
        "id": "78c8f6f4-eeb8-4ed1-a661-151bd760112c",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1703.05260",
          "start_idx": 2773,
          "end_idx": 3335
        },
        "page_content": " words. Relevant verbs and noun phrases in all stories are annotated with event types and participant types respectively. Additionally, the texts have been annotated with coreference information in order to facilitate the study of the interdependence between script structure and coreference. The InScript corpus is a unique resource that provides a basis for studying various aspects of the role of script knowledge in language processing by humans. The acquisition of this corpus is part of a larger research effort that aims at using script knowledge to model",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the source of the visual data? ",
    "llm_answer": "Image data captured from photo-ready devices (like smartphones) and online social media services.\n",
    "context": [
      {
        "id": "d058ef9d-41ed-4ef7-84a9-c29dd6d70435",
        "metadata": {
          "chunk_id": 128,
          "document_id": "1910.09399",
          "start_idx": 49534,
          "end_idx": 50036
        },
        "page_content": " of the objects, allowing users to precisely specific the relationships between objects in the images. In order to convert the visual scene graph as input for GAN to generate images, this method uses graph convolution to process input graphs. It computes a scene layout by predicting bounding boxes and segmentation masks for objects. After that, it converts the computed layout to an image with a cascaded re\ufb01nement network. black Instead of focusing on generating static images, another line of text-",
        "type": "Document"
      },
      {
        "id": "b6c38ea7-7f52-4279-a808-71a4d826b6a5",
        "metadata": {
          "chunk_id": 127,
          "document_id": "1910.09399",
          "start_idx": 49259,
          "end_idx": 49739
        },
        "page_content": " semantic text regeneration and alignment module (STREAM). The back to back Text-to-Image (T2I) and Image-to-Text (I2T) are combined to progressively enhance the diversity and semantic consistency of the generated images. black In order to enhance the diversity of the output image, Scene Graph GAN BIBREF61 proposes to use visual scene graphs to describe the layout of the objects, allowing users to precisely specific the relationships between objects in the images. In order to",
        "type": "Document"
      },
      {
        "id": "6d0d2432-27ef-4993-bacc-25b807a31eb5",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1909.02764",
          "start_idx": 17673,
          "end_idx": 18041
        },
        "page_content": " We train models on a variety of corpora, namely the common format published by BIBREF27 of the FigureEight (formally known as Crowdflower) data set of social media, the ISEAR data BIBREF40 (self-reported emotional events), and, the Twitter Emotion Corpus (TEC, weakly annotated Tweets with #anger, #disgust, #fear, #happy, #sadness, and #surprise, Mohammad2012). From",
        "type": "Document"
      },
      {
        "id": "6fc5d828-a1e2-40c6-a512-667bff84727a",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1909.08859",
          "start_idx": 6453,
          "end_idx": 6882
        },
        "page_content": " single image or a series of $L$ images depending on the reasoning task. In particular, for the visual cloze and the visual coherence type questions, the answer contains a single image ($L=1$) and for the visual ordering task, it includes a sequence. We encode the input recipe $\\mathbf {R}$ at character, word, and step levels. Character-level embedding layer uses a convolutional neural network, namely CharCNN model by BIBREF7",
        "type": "Document"
      },
      {
        "id": "a629b090-5d89-4ffd-915f-a3333a5fff96",
        "metadata": {
          "chunk_id": 144,
          "document_id": "1910.09399",
          "start_idx": 55753,
          "end_idx": 56314
        },
        "page_content": " for computer vision AI and deep learning researchers and enthusiasts. The consistent advancement in hardware, software, and contemporaneous development of computer vision AI research disrupts multiple industries. These advances in technology allow for the extraction of several data types from a variety of sources. For example, image data captured from a variety of photo-ready devices, such as smart-phones, and online social media services opened the door to the analysis of large amounts of media datasets BIBREF70. The availability of large media datasets",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What result from experiments suggest that natural language based agents are more robust?",
    "llm_answer": "The context states that experiments were conducted under similar conditions for all representation methods, but does not state that natural language representations were more robust.\n",
    "context": [
      {
        "id": "a4ad3590-07fe-489f-a54e-b3e7b2ffe956",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1910.02789",
          "start_idx": 14681,
          "end_idx": 15255
        },
        "page_content": " from all of the previous scenarios. Our agent was implemented using a Convolutional Neural Network as described in Section SECREF4. We converted the parsed state into embedded representations of fixed length. We tested both a DQN and a PPO based agent, and compared the natural language representation to the other representation techniques, namely the raw image, feature vector, and semantic segmentation representations. In order to effectively compare the performance of the different representation methods, we conducted our experiments under similar conditions for all",
        "type": "Document"
      },
      {
        "id": "577f6feb-1a55-497e-a9ae-b43551ab6a28",
        "metadata": {
          "chunk_id": 54,
          "document_id": "1910.04269",
          "start_idx": 20617,
          "end_idx": 21192
        },
        "page_content": " robust to variations which might be present in real world scenarios. There can be further exploration of various feature extraction techniques like Constant-Q transform and Fast Fourier Transform and assessment of their impact on Language Identification. There can be further improvements in neural network architectures like concatenating the high level features obtained from 1D-ConvNet and 2D-ConvNet, before performing classification. There can be experiments using deeper networks with skip connections and Inception modules. These are known to have positively impacted",
        "type": "Document"
      },
      {
        "id": "187b6c4f-6755-48ee-b0ee-c9537909ef58",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1910.02789",
          "start_idx": 2954,
          "end_idx": 3540
        },
        "page_content": "15. A basic observation of our work is that natural language representations are also beneficial for solving problems in which natural language is not the underlying source of input. Moreover, our results indicate that natural language is a strong alternative to current complementary methods for semantic representations of a state. In this work we assume a state can be described using natural language sentences. We use distributional embedding methods in order to represent sentences, processed with a standard Convolutional Neural Network for feature extraction. In Section SECREF2",
        "type": "Document"
      },
      {
        "id": "f3b145dd-d778-4cba-8d9a-68eacfb0d7c8",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1910.02789",
          "start_idx": 20294,
          "end_idx": 20840
        },
        "page_content": "-level plans and lower-level actions. Their suggested framework leverages the structure inherent to natural language, allowing for transfer to unfamiliar tasks and situations. This use of semantic structure has also been leveraged by BIBREF42, where abstract actions (not necessarily words) were recognized as symbols of a natural and expressive language, improving performance and transfer of RL agents. Outside the context of RL, previous work has also shown that high-quality linguistic representations can assist in cross-modal transfer, such",
        "type": "Document"
      },
      {
        "id": "92b8ff76-32ba-42a1-9fa9-25f66bf2e37f",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1809.08731",
          "start_idx": 5000,
          "end_idx": 5512
        },
        "page_content": " data they had created by translating and back-translating sentences with neural models. While they tried different types of LMs, best results were obtained for neural models, namely recurrent neural networks (RNNs). In this work, we investigate if approaches which have proven successful for modeling acceptability can be applied to the NLP problem of automatic fluency evaluation. In this section, we first describe SLOR and the intuition behind this score. Then, we introduce WordPieces, before explaining how",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which datasets are used in the paper?",
    "llm_answer": "Biomedical, StackOverflow, CNN/Daily Mail\n",
    "context": [
      {
        "id": "64a25737-469b-4e82-875c-ba81cb2e5018",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1701.00185",
          "start_idx": 3790,
          "end_idx": 4304
        },
        "page_content": " this paper we use a new short text dataset, Biomedical, in the experiment to verify the effectiveness of our approach. Third, we put much effort on studying the influence of various different semantic features integrated in our self-taught CNN framework, which is not involved in the conference paper. For the purpose of reproducibility, we make the datasets and software used in our experiments publicly available at the website. The remainder of this paper is organized as follows: In Section SECREF2 , we first",
        "type": "Document"
      },
      {
        "id": "5aae2354-e125-4179-9260-e8eac191d87c",
        "metadata": {
          "chunk_id": 99,
          "document_id": "1701.00185",
          "start_idx": 35418,
          "end_idx": 35678
        },
        "page_content": "ASQ for making the datasets available. This work is supported by the National Natural Science Foundation of China (No. 61602479, No. 61303172, No. 61403385) and the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDB02070005).",
        "type": "Document"
      },
      {
        "id": "c962083a-5691-4043-a261-dc3813120b23",
        "metadata": {
          "chunk_id": 44,
          "document_id": "2002.02224",
          "start_idx": 18011,
          "end_idx": 18184
        },
        "page_content": " the institutional support of the Masaryk University. This paper was presented at CEILI Workshop on Legal Data Analysis held in conjunction with Jurix 2019 in Madrid, Spain.",
        "type": "Document"
      },
      {
        "id": "af27ee8c-701c-4fe4-b55b-63a0e04b788a",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1701.00185",
          "start_idx": 18784,
          "end_idx": 19212
        },
        "page_content": " BIBREF41 . StackOverflow. We use the challenge data published in Kaggle.com. The raw dataset consists 3,370,528 samples through July 31st, 2012 to August 14, 2012. In our experiments, we randomly select 20,000 question titles from 20 different tags as in Table TABREF25 . Biomedical. We use the challenge data published in BioASQ's official website. In our experiments, we randomly select 20, 000 paper titles from 20 different",
        "type": "Document"
      },
      {
        "id": "261f5a88-ab73-4695-a32b-5ad8caaed6ac",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1907.05664",
          "start_idx": 3532,
          "end_idx": 3981
        },
        "page_content": " in this section the baseline model from See et al. See2017 trained on the CNN/Daily Mail dataset. We reproduce the results from See et al. See2017 to then apply LRP on it. The CNN/Daily mail dataset BIBREF12 is a text summarization dataset adapted from the Deepmind question-answering dataset BIBREF13 . It contains around three hundred thousand news articles coupled with summaries of about three sentences. These summaries are in fact \u201chighlights",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they quantify moral relevance?",
    "llm_answer": "They compute diachronic moral relevance scores as $p(c_1\\,|\\,\\mathbf {q})$ for each word $\\mathbf{q}$ across 20 decades.  A linear regression of these scores on time ($T$) is used to measure change.  Words with average relevance scores below 0.5 are removed.\n",
    "context": [
      {
        "id": "ef7cfd26-0ee6-4119-8711-d4e30c4ef04a",
        "metadata": {
          "chunk_id": 38,
          "document_id": "2001.07209",
          "start_idx": 14939,
          "end_idx": 15306
        },
        "page_content": " \u201cnot a moral issue\". We compared human ratings with model predictions at two tiers: for moral relevance, we paired the proportion of \u201cnot a moral issue\u201d human responses with irrelevance predictions $p(c_0\\,|\\,\\mathbf {q})$ for each topic, and for moral acceptability, we paired the proportion of \u201cacceptable\u201d responses with positive predictions $p(c_+\\,|\\,\\mathbf {q",
        "type": "Document"
      },
      {
        "id": "82bb4852-d147-41a6-9db7-0e60f7b7be1e",
        "metadata": {
          "chunk_id": 42,
          "document_id": "2001.07209",
          "start_idx": 16301,
          "end_idx": 16747
        },
        "page_content": " performed a linear regression of $R$ on $T = 1,\\ldots ,n$ and took the fitted slope as a measure of moral relevance change. We repeated the same procedure for moral polarity. Finally, we removed words with average relevance score below $0.5$ to focus on morally relevant retrievals. Table TABREF17 shows the words with steepest predicted change toward moral relevance, along with their predicted fine-grained moral categories in modern times (i.",
        "type": "Document"
      },
      {
        "id": "183a2599-2c66-4927-b583-c2a9ce84c3ae",
        "metadata": {
          "chunk_id": 13,
          "document_id": "2001.07209",
          "start_idx": 5311,
          "end_idx": 5884
        },
        "page_content": " methodology to discover principles of moral sentiment change in human society. Our framework treats the moral sentiment toward a concept at three incremental levels, as illustrated in Figure FIGREF3. First, we consider moral relevance, distinguishing between morally irrelevant and morally relevant concepts. At the second tier, moral polarity, we further split morally relevant concepts into those that are positively or negatively perceived in the moral domain. Finally, a third tier classifies these concepts into fine-grained categories of human morality. We draw from",
        "type": "Document"
      },
      {
        "id": "d46c0a21-ed0a-4373-a825-d884a166689a",
        "metadata": {
          "chunk_id": 41,
          "document_id": "2001.07209",
          "start_idx": 16142,
          "end_idx": 16447
        },
        "page_content": " restricted to words labelled as nouns in WordNet BIBREF34 for validation. For each such word $\\mathbf {q}$, we computed diachronic moral relevance scores $R_i = p(c_1\\,|\\,\\mathbf {q}), i=1,\\ldots ,20$ for the 20 decades in our time span. Then, we performed a linear regression of $R$ on $T = 1,\\ldots ,n$",
        "type": "Document"
      },
      {
        "id": "e4ce4f59-4b4e-4e1a-91d3-96e6012482e7",
        "metadata": {
          "chunk_id": 37,
          "document_id": "2001.07209",
          "start_idx": 14631,
          "end_idx": 15157
        },
        "page_content": " into the public sentiment toward these concepts as evidenced by text. We explored the predictive potential of our framework by comparing model predictions with human judgments of moral relevance and acceptability. We used data from the Pew Research Center's 2013 Global Attitudes survey BIBREF33, in which participants from 40 countries judged 8 topics such as abortion and homosexuality as one of \u201cacceptable\", \u201cunacceptable\", and \u201cnot a moral issue\". We compared human ratings with model predictions at two tiers: for moral",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much is proposed model better in perplexity and BLEU score than typical UMT models?",
    "llm_answer": "The context shows improvements in perplexity and BLEU score for the proposed model compared to other models (e.g., SC-LSTM), but does not quantify the difference against \"typical UMT models.\"  The answer is not found in the context.\n",
    "context": [
      {
        "id": "5e190f2a-e33c-4c41-8bda-872ae53d7042",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1811.00942",
          "start_idx": 2196,
          "end_idx": 2670
        },
        "page_content": ", widely accepted as the state of the art prior to NLMs, to the best NLMs today. The decrease in perplexity on standard datasets has been well documented BIBREF3 , but to our knowledge no one has examined the performances tradeoffs. With deployment on a mobile device in mind, we evaluate energy usage and inference latency on a Raspberry Pi (which shares the same ARM architecture as nearly all smartphones today). We find that a 2.5 $\\times $ reduction in perplexity on PT",
        "type": "Document"
      },
      {
        "id": "2b57faad-3707-4825-98b5-2c4e3e67efb8",
        "metadata": {
          "chunk_id": 81,
          "document_id": "2002.11893",
          "start_idx": 30351,
          "end_idx": 30691
        },
        "page_content": " as references (100 references on average), which results in high BLEU score. For user-side utterances, the BLEU score for TemplateNLG is 0.5780, while the BLEU score for SC-LSTM is 0.7858. For system-side, the two scores are 0.6828 and 0.8595. As exemplified in Table TABREF39, the gap between the two models can be attributed to that SC-L",
        "type": "Document"
      },
      {
        "id": "3817b437-4a8b-4bbe-970f-1be7126f027c",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1908.06606",
          "start_idx": 18088,
          "end_idx": 18399
        },
        "page_content": " TABREF23. Table TABREF23 indicates that our proposed model achieved the best performance both in EM-score and F$_1$-score with EM-score of 91.84% and F$_1$-score of 93.75%. QANet outperformed BERT-Base with 3.56% score in F$_1$-score but underperformed it with 0.75% score in EM-score. Compared with BERT-Base,",
        "type": "Document"
      },
      {
        "id": "11d5de76-9aa8-482d-97f8-e7f8104a711c",
        "metadata": {
          "chunk_id": 52,
          "document_id": "1908.06606",
          "start_idx": 18241,
          "end_idx": 18595
        },
        "page_content": "formed it with 0.75% score in EM-score. Compared with BERT-Base, our model led to a 5.64% performance improvement in EM-score and 3.69% in F$_1$-score. Although our model didn't outperform much with QANet in F$_1$-score (only 0.13%), our model significantly outperformed it with 6.39% score in EM-score. To further investigate the effects of named entity",
        "type": "Document"
      },
      {
        "id": "a974374c-7daa-43b2-b8fb-012e8cfcec0b",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1905.11901",
          "start_idx": 8768,
          "end_idx": 9134
        },
        "page_content": " hyperparameters (+0.7 BLEU) has a stronger effect than the lexical model, and adding the lexical model (9) on top of the optimized configuration (8) does not improve performance. Together, the adaptations to the ultra-low data setting yield 9.4 BLEU (7.2 INLINEFORM2 16.6). The model trained on full IWSLT data is less sensitive to our changes (31.9 INLINEFORM3 32.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Do they train a different training method except from scheduled sampling?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "d78d671c-32c7-4bfe-a8dc-77b0094c905a",
        "metadata": {
          "chunk_id": 64,
          "document_id": "1908.08419",
          "start_idx": 23290,
          "end_idx": 23799
        },
        "page_content": " to annotate the unlabeled set. Then, sampling strategy in active learning selects a part of samples for annotators to relabel. Finally, the relabeled samples are added to train set for segmenter to re-train. Our proposed scoring strategy selects samples according to the sequence scores of the segmented sentences, while uncertainty sampling suggests relabeling samples that are closest to the segmenter\u2019s decision boundary. Generally, two main parameters in active learning are the numbers of iterations and",
        "type": "Document"
      },
      {
        "id": "3becfb0c-a598-4ab9-890b-5f1dbbf6f959",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1909.08859",
          "start_idx": 17614,
          "end_idx": 18113
        },
        "page_content": " We used Adam optimizer with a learning rate of 1e-4 with an early stopping criteria with the patience set to 10 indicating that the training procedure ends after 10 iterations if the performance would not improve. We considered a batch size of 32 due to our hardware constraints. In the multi-task setting, batches are sampled round-robin from all tasks, where each batch is solely composed of examples from one task. We performed our experiments on a system containing four NVIDIA GTX-1080Ti GPUs,",
        "type": "Document"
      },
      {
        "id": "4c77c7da-e7a5-439e-9f8b-ee904ea4e4bf",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1908.08419",
          "start_idx": 8307,
          "end_idx": 8763
        },
        "page_content": " which chooses samples that need to be relabeled by annotators from unlabeled data. Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, a CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively. Fig. FIGREF7 and Algorithm SECREF3 demonstrate the procedure of CWS based on active learning. First, we train",
        "type": "Document"
      },
      {
        "id": "7ea2c42c-a091-46b1-a2f5-ba97e64ac31c",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1911.03894",
          "start_idx": 18085,
          "end_idx": 18526
        },
        "page_content": " learning rates and batch sizes. We select the best model on the validation set out of the 30 first epochs. Although this might push the performances even further, for all tasks except NLI, we don't apply any regularisation techniques such as weight decay, learning rate warm-up or discriminative fine-tuning. We show that fine-tuning CamemBERT in a straight-forward manner leads to state-of-the-art results on most tasks and outperforms the",
        "type": "Document"
      },
      {
        "id": "90e64144-d86d-4b35-93da-10c65dbc3126",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1908.08419",
          "start_idx": 7019,
          "end_idx": 7526
        },
        "page_content": " of active learning has resulted in various sampling strategies, such as uncertainty sampling BIBREF24 , query-by-committee BIBREF25 and information gain BIBREF26 . Currently, the most mainstream sampling strategy is uncertainty sampling. It focuses its selection on samples closest to the decision boundary of the classifier and then chooses these samples for annotators to relabel BIBREF27 . The formal definition of uncertainty sampling is to select a sample INLINEFORM0 that maximizes the entropy INLINE",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they define upward and downward reasoning?",
    "llm_answer": "The text describes upward and downward inferences based on the polarity of expressions (e.g., \"no,\" \"a few\") in premises and conclusions.  It does not explicitly define them.\n",
    "context": [
      {
        "id": "16a1824d-3adf-4a18-9746-ea8f4b7c9f43",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1906.06448",
          "start_idx": 18127,
          "end_idx": 18651
        },
        "page_content": " having rich lexical knowledge from a knowledge base (KIM) or pretraining (BERT). This indicates that downward inferences are difficult to perform even with the expansion of lexical knowledge. In addition, it is interesting to see that if a model performed better on upward inferences, it performed worse on downward inferences. We will investigate these results in detail below. To explore whether the performance of models on monotonicity reasoning depends on the training set or the model themselves, we conducted further",
        "type": "Document"
      },
      {
        "id": "0042659e-e768-4f0b-9aff-d5d2411cd11e",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1906.06448",
          "start_idx": 6603,
          "end_idx": 7096
        },
        "page_content": "c) replace the phrase in the argument position with a more general or specific phrase in natural and various ways (e.g., by using lexical knowledge or logical connectives). For (a) and (b), we first conduct polarity computation on a syntactic structure for each sentence, and then select premises involving upward/downward expressions. For (c), we use crowdsourcing to narrow or broaden the arguments. The motivation for using crowdsourcing is to collect naturally alike monotonicity inference",
        "type": "Document"
      },
      {
        "id": "435476c7-8505-4fa2-827d-d281df645312",
        "metadata": {
          "chunk_id": 67,
          "document_id": "1906.06448",
          "start_idx": 25183,
          "end_idx": 25612
        },
        "page_content": "P$ : No racin' on the Range $H$ : No horse racing is allowed on the Range  One possible reason why there are few downward inferences is that certain pragmatic factors can block people to draw a downward inference. For instance, in the case of the inference problem in ( \"Discussion\" ), unless the added disjunct in $H$ , i.e., a small cat with green eyes, is salient in the context, it would be difficult to draw the conclusion $",
        "type": "Document"
      },
      {
        "id": "b0faea9f-15fc-45ef-91c4-c9a4293c92ba",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1906.06448",
          "start_idx": 4926,
          "end_idx": 5370
        },
        "page_content": "subseteq $ bought a show ticket), or adding disjunction (bought a movie ticket $\\sqsubseteq $ bought or sold a movie ticket). Table 1 shows determiners modeled as binary operators and their polarities with respect to the first and second arguments. There are various types of downward operators, not limited to determiners (see Table 2 ). As shown in ( \"Monotonicity\" ), if a propositional object is embedded in a downward monotonic context (e.",
        "type": "Document"
      },
      {
        "id": "5c83906c-55b8-4556-9b97-72f1cc389068",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1906.06448",
          "start_idx": 24738,
          "end_idx": 25183
        },
        "page_content": " few and the upward operator a few. The results of crowdsourcing tasks in Section 3.1.3 showed that some downward inferences can naturally be performed in human reasoning. However, we also found that the MultiNLI training set BIBREF10 , which is one of the dataset created from naturally-occurring texts, contains only 77 downward inference problems, including the following one.  $P$ : No racin' on the Range $H$ : No horse racing is allowed on",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Do they annotate their own dataset or use an existing one?",
    "llm_answer": "The context describes annotating a dataset.\n",
    "context": [
      {
        "id": "45540b48-0892-47ef-9a7d-86f9efe45ec0",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1910.11769",
          "start_idx": 4963,
          "end_idx": 5497
        },
        "page_content": " using in-house data annotators who are experts in narrative content. A passage is accepted as valid if the in-house annotator's label matched any one of the MTurk annotators' labels. The remaining passages are discarded. We provide the fraction of annotator agreement for each label in the dataset. Though passages may lose some emotional context when read independently of the complete narrative, we believe annotator agreement on our dataset supports the assertion that small excerpts can still convey coherent emotions. During the",
        "type": "Document"
      },
      {
        "id": "6d2fb9a2-9305-4bd8-864d-143c83b5e90c",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1909.00430",
          "start_idx": 0,
          "end_idx": 440
        },
        "page_content": "Data annotation is a key bottleneck in many data driven algorithms. Specifically, deep learning models, which became a prominent tool in many data driven tasks in recent years, require large datasets to work well. However, many tasks require manual annotations which are relatively hard to obtain at scale. An attractive alternative is lightly supervised learning BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF",
        "type": "Document"
      },
      {
        "id": "f84b0b36-4d69-46b7-ab40-db1d1c37787d",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1810.06743",
          "start_idx": 1487,
          "end_idx": 2017
        },
        "page_content": " each other. Further, we show similar downstream performance for each annotation scheme in the task of morphological tagging. This tool enables a synergistic use of UniMorph and Universal Dependencies, as well as teasing out the annotation discrepancies within and across projects. When one dataset disobeys its schema or disagrees with a related language, the flaws may not be noticed except by such a methodological dive into the resources. When the maintainers of the resources ameliorate these flaws, the resources move closer",
        "type": "Document"
      },
      {
        "id": "6cbe4da1-641e-4851-b898-32218f4b87d1",
        "metadata": {
          "chunk_id": 54,
          "document_id": "1911.07228",
          "start_idx": 14818,
          "end_idx": 15154
        },
        "page_content": " BIO format (Inside, Outside, Beginning) BIBREF6 at the top layer (CRF). If we can unify the labeling of the annotators between the train, dev and test sets. We will improve data quality and classifier. It is better if there is a pre-trained word embeddings that overlays the data, and segmentation algorithm need to be more accurately.",
        "type": "Document"
      },
      {
        "id": "9c29b30f-983c-4da5-9469-bc961c44c746",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1703.05260",
          "start_idx": 7578,
          "end_idx": 8108
        },
        "page_content": ". This section deals with the annotation of the data. We first describe the final annotation schema. Then, we describe the iterative process of corpus annotation and the refinement of the schema. This refinement was necessary due to the complexity of the annotation. For each of the scenarios, we designed a specific annotation template. A script template consists of scenario-specific event and participant labels. An example of a template is shown in Table 1 . All NP heads in the corpus were annotated with a participant label;",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What kind of neural network architecture do they use?",
    "llm_answer": "Neural regressor (NNR) and neural classifier (NNC) with an explicit reasoning module.  A two-layer BiLSTM architecture is also mentioned.\n",
    "context": [
      {
        "id": "cc19f0cb-3472-43b2-b27a-65551596a757",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1909.00542",
          "start_idx": 7743,
          "end_idx": 8155
        },
        "page_content": " layer, and use cross-entropy as the loss function. The complete architecture is shown in Fig. FIGREF28. The bottom section of Table TABREF26 shows the results of several variants of the neural architecture. The table includes a neural regressor (NNR) and a neural classifier (NNC). The neural classifier is trained in two set ups: \u201cNNC top 5\u201d uses classification labels as described in Section SECREF3, and \u201cNNC",
        "type": "Document"
      },
      {
        "id": "768e8e84-3e84-4d1a-a7d5-fe2929dc50c3",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1909.08859",
          "start_idx": 4603,
          "end_idx": 5140
        },
        "page_content": " with an explicit reasoning module that acts on entity-specific relational memory units. Fig. FIGREF4 shows an overview of the network architecture. It consists of five main modules: An input module, an attention module, a reasoning module, a modeling module, and an output module. Note that the question answering tasks we consider here are multimodal in that while the context is a procedural text, the question and the multiple choice answers are composed of images. Input Module extracts vector representations of inputs at different",
        "type": "Document"
      },
      {
        "id": "837057af-d05e-4a07-95aa-f852f4f14aea",
        "metadata": {
          "chunk_id": 8,
          "document_id": "2002.06644",
          "start_idx": 3205,
          "end_idx": 3642
        },
        "page_content": " the local word order efficiently. BiLSTM: Unlike feedforward neural networks, recurrent neural networks like BiLSTMs use memory based on history information to learn long-distance features and then predict the output. We use a two-layer BiLSTM architecture with GloVe word embeddings as a strong RNN baseline. BERT BIBREF5: It is a contextualized word representation model that uses bidirectional transformers, pretrained on a large $3.",
        "type": "Document"
      },
      {
        "id": "ed385b18-6171-42af-9672-281f1df69e11",
        "metadata": {
          "chunk_id": 53,
          "document_id": "1710.01492",
          "start_idx": 19472,
          "end_idx": 19941
        },
        "page_content": " particular, the best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks BIBREF53 , BIBREF54 . Conversely, kernel machines seem to be less frequently used than in the past, and the use of learning methods other than the ones mentioned above is at this point scarce. All these models are examples of supervised learning as they need labeled training data. Semi-supervised learning. We should note two things about the use of deep neural",
        "type": "Document"
      },
      {
        "id": "a4a4c58b-d1f1-46a1-9912-24f7a070ad68",
        "metadata": {
          "chunk_id": 43,
          "document_id": "2001.00137",
          "start_idx": 16781,
          "end_idx": 17197
        },
        "page_content": " the online platforms. Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What web and user-generated NER datasets are used for the analysis?",
    "llm_answer": "UMBC corpus for Twitter NER, and a further Twitter NER corpus created by BIBREF21.  WEB and TWI are also mentioned as datasets.\n",
    "context": [
      {
        "id": "637d4fe3-c7c1-4e7c-a855-261351957759",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1701.02877",
          "start_idx": 4315,
          "end_idx": 4755
        },
        "page_content": " NER approaches, namely Stanford NER BIBREF34 , SENNA BIBREF35 and CRFSuite BIBREF36 (see Section \"NER Models and Features\" for details). Since the goal of this study is to compare NER performance on corpora from diverse domains and genres, seven benchmark NER corpora are included, spanning newswire, broadcast conversation, Web content, and social media (see Table 1 for details). These datasets were chosen such that they have been annot",
        "type": "Document"
      },
      {
        "id": "6e88f193-653f-4ba8-8d1d-4192af66023b",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1701.02877",
          "start_idx": 3620,
          "end_idx": 4066
        },
        "page_content": " primarily by F1 score), and also to study the relationship between OOV NEs and features and F1. See Section \"Experiments\" for details. To ensure representativeness and comprehensiveness, our experimental findings are based on key benchmark NER corpora spanning multiple genres, time periods, and corpus annotation methodologies and guidelines. As detailed in Section \"Datasets\" , the corpora studied are OntoNotes BIBREF29 , ACE BIBREF30 , MUC 7",
        "type": "Document"
      },
      {
        "id": "68c9fa82-7b71-4d3b-8d4e-70ded7bf95e4",
        "metadata": {
          "chunk_id": 21,
          "document_id": "1701.02877",
          "start_idx": 7791,
          "end_idx": 8297
        },
        "page_content": " studying NER performance on this genre gained momentum. So far, there have been no big evaluation efforts, such as ACE and OntoNotes, resulting in substantial amounts of gold standard data. Instead, benchmark corpora were created as part of smaller challenges or individual projects. The first such corpus is the UMBC corpus for Twitter NER BIBREF33 , where researchers used crowdsourcing to obtain annotations for persons, locations and organisations. A further Twitter NER corpus was created by BIBREF21",
        "type": "Document"
      },
      {
        "id": "a5bbc687-2bb1-4b13-80ba-9811f5e6bf39",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1701.02877",
          "start_idx": 7291,
          "end_idx": 7791
        },
        "page_content": " NER, but also for other tasks such as coreference resolution, relation and event extraction and word sense disambiguation. In this paper, however, we restrict ourselves purely to the English NER annotations, for consistency across datasets. The ACE corpus contains HEAD as well as EXTENT annotations for NE spans. For our experiments we use the EXTENT tags. With the emergence of social media, studying NER performance on this genre gained momentum. So far, there have been no big evaluation efforts",
        "type": "Document"
      },
      {
        "id": "bc04a345-bdd9-4c6b-839b-2b8077504285",
        "metadata": {
          "chunk_id": 88,
          "document_id": "1701.02877",
          "start_idx": 31506,
          "end_idx": 31962
        },
        "page_content": " genres. On average, newswire corpora and OntoNotes MZ are the easiest corpora and ACE UN, WEB and TWI are the hardest corpora for NER methods to reach good performance on. Normalising corpora by size results in more noisy data such as TWI and WEB data achieving similar results to NW corpora. Increasing the amount of available in-domain training data will likely result in improved NERC performance. There is a strong positive correlation between high F1",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which unlabeled data do they pretrain with?",
    "llm_answer": "The context does not specify the unlabeled data used for pre-training.\n",
    "context": [
      {
        "id": "3eff4214-8e3b-4659-93c2-ab190040adc5",
        "metadata": {
          "chunk_id": 56,
          "document_id": "1710.01492",
          "start_idx": 20569,
          "end_idx": 20967
        },
        "page_content": " the network, e.g., with convolution or with LSTMs BIBREF55 , BIBREF54 , BIBREF56 , need to learn something like a language model, i.e., which word is likely to follow which one. Training these parts needs no labels. While these parts can be also pre-trained, it is easier, and often better, to use self training. Distantly-supervised learning. Another way to make use of large unannotated datasets",
        "type": "Document"
      },
      {
        "id": "e27a50fe-424e-4576-b62f-2a0e605c4676",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1904.05862",
          "start_idx": 0,
          "end_idx": 568
        },
        "page_content": "Current state of the art models for speech recognition require large amounts of transcribed audio data to attain good performance BIBREF1 . Recently, pre-training of neural networks has emerged as an effective technique for settings where labeled data is scarce. The key idea is to learn general representations in a setup where substantial amounts of labeled or unlabeled data is available and to leverage the learned representations to improve performance on a downstream task for which the amount of data is limited. This is particularly interesting for tasks where",
        "type": "Document"
      },
      {
        "id": "d0a14e43-0927-422e-b252-788be56d2957",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1909.00124",
          "start_idx": 11950,
          "end_idx": 12394
        },
        "page_content": ", the sentiment of each aspect gives the ground-truth as the sentiments of all aspects are the same. For each clean-labeled dataset, the sentences are randomly partitioned into training set and test set with $80\\%$ and $20\\%$, respectively. Following BIBREF25, We also randomly select $10\\%$ of the test data for validation to check the model during training. Summary statistics of the training, validation, and test data are shown in Table TAB",
        "type": "Document"
      },
      {
        "id": "ae59178e-0a04-4b80-980b-d4e4559d0966",
        "metadata": {
          "chunk_id": 40,
          "document_id": "1909.00124",
          "start_idx": 12671,
          "end_idx": 13117
        },
        "page_content": " we treat 4 or 5 stars as positive and 1 or 2 stars as negative. The data is noisy because a positive (negative) review can contain negative (positive) sentences, and there are also neutral sentences. This gives us three noisy-labeled training datasets. We still use the same test sets as those for the clean-labeled datasets. Summary statistics of all the datasets are shown in Table TABREF9. Experiment 1: Here we use the clean-labeled data (i.",
        "type": "Document"
      },
      {
        "id": "65333987-2358-483b-95cd-3ff028ff6a96",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many convolutional layers does their model have?",
    "llm_answer": "Two\n",
    "context": [
      {
        "id": "415aa967-c234-4a69-b6c2-9dbc77f6c838",
        "metadata": {
          "chunk_id": 71,
          "document_id": "1910.02789",
          "start_idx": 28472,
          "end_idx": 28930
        },
        "page_content": " has 8 6X6 filters with stride 3 and ReLU activation. The second convolutional layer has 16 3X3 filters with stride 2 and ReLU activation. The fully connected layers has 32 and 16 units, both of them are followed by ReLU activation. The output layer's size is the amount of actions the agent has available in the trained scenario. Used in the feature vector based agent. Naturally, some discretization is needed in order to build a feature vector, so some of",
        "type": "Document"
      },
      {
        "id": "0c0f2ff1-f774-4ba8-b373-4b1cfed4eefb",
        "metadata": {
          "chunk_id": 72,
          "document_id": "1701.00185",
          "start_idx": 26026,
          "end_idx": 26417
        },
        "page_content": " ) is fixed to 15 when constructing the graph structures for LE and LPI. For CNN model, the networks has two convolutional layers. The widths of the convolutional filters are both 3. The value of INLINEFORM0 for the top INLINEFORM1 -max pooling in Eqn. ( EQREF10 ) is 5. The number of feature maps at the first convolutional layer is 12, and 8 feature maps at the second convolutional layer.",
        "type": "Document"
      },
      {
        "id": "05369310-3efb-49de-93a3-9ac0e501a37c",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1811.00942",
          "start_idx": 4741,
          "end_idx": 5020
        },
        "page_content": " comprises two separate parts: a convolution layer with three weights, and a recurrent pooling layer. Given an input $\\mathbf {X} \\in \\mathbb {R}^{k \\times n}$ , the convolution layer is $\n\\mathbf {Z} = \\tanh (\\mathbf {W}_z \\cdot \\mathbf {X})\\\\\n\\mathbf {F} = \\sigma (\\mathbf {W}_",
        "type": "Document"
      },
      {
        "id": "a1061a92-7bff-4acc-a12c-46948eeae8da",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1908.10449",
          "start_idx": 13757,
          "end_idx": 14232
        },
        "page_content": " the encoding transformer blocks. Each encoding transformer block consists of four convolutional layers (with shared weights), a self-attention layer, and an MLP. Each convolutional layer has 96 filters, each kernel's size is 7. In the self-attention layer, we use a block hidden size of 96 and a single head attention mechanism. Layer normalization and dropout are applied after each component inside the block. We add positional encoding into each block's input. We use one",
        "type": "Document"
      },
      {
        "id": "ada36b7b-c413-4def-b9e5-e5aa955e1f46",
        "metadata": {
          "chunk_id": 73,
          "document_id": "1701.00185",
          "start_idx": 26417,
          "end_idx": 26850
        },
        "page_content": " first convolutional layer is 12, and 8 feature maps at the second convolutional layer. Both those two convolutional layers are followed by a folding layer. We further set the dimension of word embeddings INLINEFORM2 as 48. Finally, the dimension of the deep feature representation INLINEFORM3 is fixed to 480. Moreover, we set the learning rate INLINEFORM4 as 0.01 and the mini-batch training size as 200. The output size INLINEFORM",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big are the datasets?",
    "llm_answer": "000/1,610 pieces of news, respectively.  Vocabulary size of 1,858,452.\n",
    "context": [
      {
        "id": "9ccb7945-9a96-417a-b52e-68855c4910b1",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "cc9faf95-ff74-4532-ad40-1937aafe18a1",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1707.03904",
          "start_idx": 12758,
          "end_idx": 13269
        },
        "page_content": " to evaluate the performance of the reading system independently from the search system, while the full set allows us to evaluate the performance of Quasar as a whole. We also split the full set into training, validation and test sets. The final size of each data subset after all discards is listed in Table 1 . Evaluation is straightforward on Quasar-S since each answer comes from a fixed output vocabulary of entities, and we report the average accuracy of predictions as the evaluation metric. For Quasar-T",
        "type": "Document"
      },
      {
        "id": "95c62085-b478-4b5e-a4fa-049a6c8fc5d0",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1908.10449",
          "start_idx": 22880,
          "end_idx": 23381
        },
        "page_content": " hard time learning in this setting. As shown in Table TABREF16, both datasets have a vocabulary size of more than 100k. This is much larger than in the other two settings, where on average the length of questions is around 10. This suggests that the methods with better sample efficiency are needed to act in more realistic problem settings with huge action spaces. Experiments also show that a larger memory slot size always helps. Intuitively, with a memory mechanism (either implicit or explicit),",
        "type": "Document"
      },
      {
        "id": "95bf2b84-1a56-4eed-b193-4a34d2c76c4c",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1706.08032",
          "start_idx": 14264,
          "end_idx": 14661
        },
        "page_content": " datasets in Table III. for all datasets, the filter window size ( INLINEFORM0 ) is 7 with 6 feature maps each for the first wide convolution layer, the second wide convolution layer has a filter window size of 5 with 14 feature maps each. Dropout rate ( INLINEFORM1 ) is 0.5, INLINEFORM2 constraint, learning rate is 0.1 and momentum of 0.9. Mini-batch size for STS Corpus is 100 and others are 4",
        "type": "Document"
      },
      {
        "id": "b99f9b51-787d-423c-8c49-bf72943f2462",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1701.02877",
          "start_idx": 9957,
          "end_idx": 10406
        },
        "page_content": " training and testing data sizes measured in number of NEs are listed in Table 2 . Separate models are then trained on the training parts of each corpus and evaluated on the development (if available) and test parts of the same corpus. If development parts are available, as they are for CoNLL (CoNLL Test A) and MUC (MUC 7 Dev), they are not merged with the training corpora for testing, as it was permitted to do in the context of those evaluation",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What baselines did they compare their model with?",
    "llm_answer": "Baseline, Ablation models, content, content-network, and image-based models, NABERT+ baseline\n",
    "context": [
      {
        "id": "d7a0b7aa-da14-4112-80c0-cc26c62fe83f",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "fc23cf7f-21ff-43cd-8d00-ef5914322400",
        "metadata": {
          "chunk_id": 68,
          "document_id": "1810.00663",
          "start_idx": 26280,
          "end_idx": 26763
        },
        "page_content": " room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM. Note that the average number of actions in the ground truth output sequences is 7.07 for the Test-New set. Our model's predictions are just INLINEFORM1 edits off on average from the correct navigation plans. This",
        "type": "Document"
      },
      {
        "id": "c9d62915-21d1-42b6-a16c-8d9d28de6e0c",
        "metadata": {
          "chunk_id": 91,
          "document_id": "1902.06843",
          "start_idx": 35139,
          "end_idx": 35416
        },
        "page_content": "s by -0.27. Baselines: To test the efficacy of our multi-modal framework for detecting depressed users, we compare it against existing content, content-network, and image-based models (based on the aforementioned general image feature, facial presence, and facial expressions.)",
        "type": "Document"
      },
      {
        "id": "7d4754c7-4ea8-4d20-9fb6-3d1e50bf0cf6",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1809.04960",
          "start_idx": 22497,
          "end_idx": 22960
        },
        "page_content": " the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types of comments as described in the above retrieval evaluation setting: Correct, Plausible, Popular, and Random. We rank the candidate comment set with four models (TF-IDF, S2S, IR, and Proposed+IR), and record the types of top-1 comments. Figure FIGREF40 shows the percentage of different types of top-1 comments generated by each model. It shows",
        "type": "Document"
      },
      {
        "id": "5e0f3225-070e-4250-8f70-93c083311ec4",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was the performance of their model?",
    "llm_answer": "The model trained on the AMMER corpus achieved an F1 score of 57%.\n",
    "context": [
      {
        "id": "1e2f925a-9a0d-4318-af38-ee9ae9d2a426",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1912.13109",
          "start_idx": 7921,
          "end_idx": 8471
        },
        "page_content": " on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we",
        "type": "Document"
      },
      {
        "id": "0ab3fdf3-8ea1-480c-8cac-92715fc9d565",
        "metadata": {
          "chunk_id": 52,
          "document_id": "1909.02764",
          "start_idx": 20420,
          "end_idx": 20841
        },
        "page_content": "=73 % on TEC. The model trained on our AMMER corpus achieves an F1 score of 57%. This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set. These results are mostly in line with Bostan2018. Now we analyze how well the models trained in Experiment 1 perform when applied to our data set. The results are shown in column \u201cSimple\u201d in Table TAB",
        "type": "Document"
      },
      {
        "id": "5e0f3225-070e-4250-8f70-93c083311ec4",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "5e5c6963-88aa-4a86-96ff-323aeb271bfb",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1810.00663",
          "start_idx": 25415,
          "end_idx": 25983
        },
        "page_content": " In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models. The previous section evaluated model performance on new instructions (and corresponding navigation plans) for environments that were previously seen at training time. Here, we examine whether the trained models succeed on environments that are completely new. The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as",
        "type": "Document"
      },
      {
        "id": "d7a0b7aa-da14-4112-80c0-cc26c62fe83f",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What evaluation metrics are used?",
    "llm_answer": "BLEU, BIBREF4, METEOR, ROUGE, ROUGE-LM, WPSLOR, F<sub>1</sub> score.\n",
    "context": [
      {
        "id": "d88a1149-0827-4f42-b9b1-620cfd250358",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1905.08949",
          "start_idx": 10602,
          "end_idx": 11043
        },
        "page_content": " they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks. As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE B",
        "type": "Document"
      },
      {
        "id": "1d812102-98f4-45bc-b355-c1653ccfaf7e",
        "metadata": {
          "chunk_id": 72,
          "document_id": "1809.08731",
          "start_idx": 24617,
          "end_idx": 25067
        },
        "page_content": " metrics on a benchmark dataset. Additionally, they can be applied even in settings where no references are available, or would be costly to obtain. Finally, for given references, we proposed the reference-based metric ROUGE-LM, which consists of a combination of WPSLOR and ROUGE. Thus, we were able to obtain an even more accurate fluency evaluation. We would like to thank Sebastian Ebert and Samuel Bowman for their detailed and helpful feedback.",
        "type": "Document"
      },
      {
        "id": "7de8d13e-a44b-4918-a827-2e4a363b6e00",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1909.00578",
          "start_idx": 2625,
          "end_idx": 3083
        },
        "page_content": " MT evaluation metrics BIBREF4, BIBREF18. QE models for MT make use of large post-edited datasets, and apply machine learning methods to predict post-editing effort scores and quality (good/bad) labels. We apply QE to summarization, focusing on linguistic qualities that reflect the readability and fluency of the generated texts. Since no post-edited datasets \u2013 like the ones used in MT \u2013 are available for summarization, we use instead the ratings assigned",
        "type": "Document"
      },
      {
        "id": "fba7109c-978a-4f4d-a628-9538e8d3cf9c",
        "metadata": {
          "chunk_id": 30,
          "document_id": "1905.08949",
          "start_idx": 11311,
          "end_idx": 11823
        },
        "page_content": ". To overcome this, BIBREF46 proposed a new metric to evaluate the \u201canswerability\u201d of a question by calculating the scores for several question-specific factors, including question type, content words, function words, and named entities. However, as it is newly proposed, it has not been applied to evaluate any NQG system yet. To accurately measure what makes a good question, especially deep questions, improved evaluation schemes are required to specifically investigate the mechanism of question asking. Many",
        "type": "Document"
      },
      {
        "id": "e901bd2a-cc88-4572-a952-318c9e30a71a",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1908.10449",
          "start_idx": 10837,
          "end_idx": 11300
        },
        "page_content": "RC involves both MRC and RL, we adopt evaluation metrics from both settings. First, as a question answering task, we use $\\text{F}_1$ score to compare predicted answers against ground-truth, as in previous works. When there exist multiple ground-truth answers, we report the max $\\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we evaluate an agent's performance during both its training and testing phases.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How were the navigation instructions collected?",
    "llm_answer": "Collected through Mechanical Turk using 100 simulated environments.\n",
    "context": [
      {
        "id": "db85195f-94ed-470e-879b-a300fade8304",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1810.00663",
          "start_idx": 3631,
          "end_idx": 4203
        },
        "page_content": " of behaviors to reach the desired destination according to the instructions and the map (Fig. FIGREF4 (c)). Our main insight is that using attention mechanisms to correlate navigation instructions with the topological map of the environment can facilitate predicting correct navigation plans. This work also contributes a new dataset of INLINEFORM0 pairs of free-form natural language instructions and high-level navigation plans. This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of",
        "type": "Document"
      },
      {
        "id": "2949e9be-db9e-4e1e-880a-43eb552afa7b",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1810.00663",
          "start_idx": 18696,
          "end_idx": 19261
        },
        "page_content": " instructions is not always the same. For instance, a person said \u201cturn right and advance\u201d to describe part of a route, while another person said \u201cgo straight after turning right\u201d in a similar situation. The high variability present in the natural language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort. This section describes our evaluation of the proposed approach for interpreting navigation commands in natural",
        "type": "Document"
      },
      {
        "id": "eaa30ec0-abca-4895-8546-5cd72c58cc71",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1810.00663",
          "start_idx": 4584,
          "end_idx": 5197
        },
        "page_content": "ization to new instructions in known and in new environments. We conclude this paper by discussing the benefits of the proposed approach as well as opportunities for future research based on our findings. This section reviews relevant prior work on following navigation instructions. Readers interested in an in-depth review of methods to interpret spatial natural language for robotics are encouraged to refer to BIBREF11 . Typical approaches to follow navigation commands deal with the complexity of natural language by manually parsing commands, constraining language descriptions, or using statistical machine",
        "type": "Document"
      },
      {
        "id": "43062274-1ab4-4211-9848-083f40ebf0af",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1807.03367",
          "start_idx": 11628,
          "end_idx": 12165
        },
        "page_content": " guide predicts that the tourist is at target, we evaluate its location. If successful, the task ends, otherwise we continue until there have been three wrong evaluations. The protocol is given as pseudo-code in Appendix SECREF12 . The designed navigation protocol relies on a trained localization model that predicts the tourist's location from a communicated message. Before we formalize this localization sub-task in Section UID21 , we further introduce two simplifying assumptions\u2014perfect perception and orientation-agnosticism\u2014so as",
        "type": "Document"
      },
      {
        "id": "fc1afff4-f2df-47e0-af8f-19c02ecfc366",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1810.00663",
          "start_idx": 18361,
          "end_idx": 18910
        },
        "page_content": " was collected from 88 unique simulated environments, totaling 6064 distinct navigation plans (2002 plans have two different navigation instructions each; the rest has one). The dataset contains two test set variants: While the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said \u201cturn right and advance\ufffd",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What language is the experiment done in?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "78dd82c6-c6af-4015-b4e0-f73037eb863b",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1904.05862",
          "start_idx": 15343,
          "end_idx": 15454
        },
        "page_content": "letter++ experiments, and Tatiana Likhomanenko for providing convolutional language models for our experiments.",
        "type": "Document"
      },
      {
        "id": "e8fb7d0e-86e1-4332-bb4a-d8109e4146e4",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1910.04269",
          "start_idx": 19553,
          "end_idx": 20023
        },
        "page_content": ", the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Kat",
        "type": "Document"
      },
      {
        "id": "7da029d3-0e6a-48d3-9dd2-3cfd37690294",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1910.04269",
          "start_idx": 6853,
          "end_idx": 7342
        },
        "page_content": " comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (",
        "type": "Document"
      },
      {
        "id": "086a5c4b-797a-4ff7-b066-d89ca3ac3e4a",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1911.00069",
          "start_idx": 19862,
          "end_idx": 20285
        },
        "page_content": " in the previous works was motivated by domain adaptation, the focus of this paper is on cross-lingual model transfer, and hence we apply a random data split as follows. For the source language English and each target language, we randomly select $80\\%$ of the data as the training set, $10\\%$ as the development set, and keep the remaining $10\\%$ as the test set. The sizes of the sets are summarized in Table TABREF41. We",
        "type": "Document"
      },
      {
        "id": "09464d15-9b8c-4f9c-a9fa-0f3ccdf067eb",
        "metadata": {
          "chunk_id": 41,
          "document_id": "1807.03367",
          "start_idx": 16248,
          "end_idx": 16711
        },
        "page_content": "4 0.1ex plus0.1ex minus.1ex-1em Natural language In contrast to our emergent communication experiments, we do not take random actions but instead extract actions, observations, and messages from the dataset. Specifically, we consider each tourist utterance (i.e. at any point in the dialogue), obtain the current tourist location as target location INLINEFORM0 , the utterance itself as message INLINEFORM1 , and the sequence of observations and actions that took",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What additional features are proposed for future work?",
    "llm_answer": "A deeper analysis of clinical narratives in EHRs, structural properties of online communities, topical information, identity-based measures, and analyses of more diverse communities supported by different platforms (e.g., GitHub, StackExchange, Wikipedia).  Also, additional features beyond paragraph domain scores, bag-of-words, length of stay, and number of previous admissions.\n",
    "context": [
      {
        "id": "93181738-225e-45ea-8678-bf56a3b9b442",
        "metadata": {
          "chunk_id": 57,
          "document_id": "1809.05752",
          "start_idx": 23059,
          "end_idx": 23621
        },
        "page_content": " future versions of our risk factor domain classification model we will explore increasing robustness through sequence modeling that considers more contextual information. Our current feature set for training a machine learning classifier is relatively small, consisting of paragraph domain scores, bag-of-words, length of stay, and number of previous admissions, but we intend to factor in many additional features that extend beyond the scope of the present study. These include a deeper analysis of clinical narratives in EHRs: our next task will be to extend",
        "type": "Document"
      },
      {
        "id": "549e396f-afac-4ccf-b992-6ee8c59b81d5",
        "metadata": {
          "chunk_id": 88,
          "document_id": "1705.09665",
          "start_idx": 36224,
          "end_idx": 36805
        },
        "page_content": " newcomers at certain points in their lifecycle? Another important avenue of future work is to explore other ways of mapping the landscape of online communities. For example, combining structural properties of communities BIBREF40 with topical information BIBREF35 and with our identity-based measures could further characterize and explain variations in user engagement patterns. Furthermore, extending the present analyses to even more diverse communities supported by different platforms (e.g., GitHub, StackExchange, Wikipedia) could enable the characterization of more complex",
        "type": "Document"
      },
      {
        "id": "e499fa8f-9b01-4bfc-b855-53cf304b8ec4",
        "metadata": {
          "chunk_id": 78,
          "document_id": "1710.01492",
          "start_idx": 28162,
          "end_idx": 28625
        },
        "page_content": " etc. We expect the quest for more interesting formulations of the general sentiment analysis task to continue. We see competitions such as those at SemEval as the engine of this innovation, as they not only perform head-to-head comparisons, but also create databases and tools that enable follow-up research for many years afterward. In terms of methods, we believe that deep learning BIBREF55 , BIBREF54 , BIBREF56 , together with semi-supervised and distantly-",
        "type": "Document"
      },
      {
        "id": "577f6feb-1a55-497e-a9ae-b43551ab6a28",
        "metadata": {
          "chunk_id": 54,
          "document_id": "1910.04269",
          "start_idx": 20617,
          "end_idx": 21192
        },
        "page_content": " robust to variations which might be present in real world scenarios. There can be further exploration of various feature extraction techniques like Constant-Q transform and Fast Fourier Transform and assessment of their impact on Language Identification. There can be further improvements in neural network architectures like concatenating the high level features obtained from 1D-ConvNet and 2D-ConvNet, before performing classification. There can be experiments using deeper networks with skip connections and Inception modules. These are known to have positively impacted",
        "type": "Document"
      },
      {
        "id": "ce891315-7798-441c-97cf-a5176d99fc42",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1905.08949",
          "start_idx": 1945,
          "end_idx": 2409
        },
        "page_content": " have been considerable advances made in NQG, the area lacks a comprehensive survey. This paper fills this gap by presenting a systematic survey on recent development of NQG, focusing on three emergent trends that deep learning has brought in QG: (1) the change of learning paradigm, (2) the broadening of the input spectrum, and (3) the generation of deep questions. For the sake of clean exposition, we first provide a broad overview of QG by conceptualizing the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are their initial results on this task?",
    "llm_answer": "The initial results using the RNN encoder are shown in Table 2.\n",
    "context": [
      {
        "id": "ec846c61-b035-431a-88d4-858e8c47d3be",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1603.07044",
          "start_idx": 9966,
          "end_idx": 10403
        },
        "page_content": " for task A, there are a total number of 26,700 question-comment pairs. For task B, there are 2,670 question-question pairs. For task C, there are 26,700 question-comment pairs. The test dataset includes 50 questions, 500 related questions and 5,000 comments which do not overlap with the training set. To evaluate the performance, we use mean average precision (MAP) and F1 score. Table 2 shows the initial results using the RNN encoder",
        "type": "Document"
      },
      {
        "id": "ad8f57e6-9103-48d1-bcd3-77aa60670c8e",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1906.06448",
          "start_idx": 11534,
          "end_idx": 12014
        },
        "page_content": " the premise and/or the hypothesis is ungrammatical or does not make sense. Workers were paid US$0.04 for each question, and each question was assigned to three workers. To collect high-quality annotation results, we imposed ten test questions on each worker, and removed workers who gave more than three wrong answers. We also set the minimum time it should take to complete each question to 200 seconds. 1,237 workers contributed to this task, and we annotated gold labels of 15",
        "type": "Document"
      },
      {
        "id": "1b2a774a-b45c-41e9-91e6-599deb244649",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1807.03367",
          "start_idx": 9457,
          "end_idx": 9982
        },
        "page_content": " given to our workers before they started their task, are shown in Appendix SECREF15 . We paired Turkers at random and let them alternate between the tourist and guide role across different HITs. The Talk The Walk dataset consists of over 10k successful dialogues\u2014see Table FIGREF66 in the appendix for the dataset statistics split by neighborhood. Turkers successfully completed INLINEFORM0 of all finished tasks (we use this statistic as the human success rate). More than six hundred participants successfully completed at",
        "type": "Document"
      },
      {
        "id": "d7a0b7aa-da14-4112-80c0-cc26c62fe83f",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "f6fc3632-85ae-4401-b21d-36d939a2b100",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1906.06448",
          "start_idx": 9932,
          "end_idx": 10429
        },
        "page_content": " each set of substitutions, and each set was assigned to three workers. To remove low-quality examples, we set the minimum time it should take to complete each set to 200 seconds. The entry in our task was restricted to workers from native speaking English countries. 128 workers contributed to the task, and we created 15,339 hypotheses (7,179 upward examples and 8,160 downward examples). The gold label of each premise-hypothesis pair created in the previous task is automatically determined by",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is morphology knowledge implemented in the method?",
    "llm_answer": "A morphological word segmentation method is proposed, generating a vocabulary with effective morpheme units and moderate subword units, improving generalization over morphologically-rich words.  The method combines morpheme segmentation and BPE.  Modified Morfessor is also used to incorporate morphology knowledge into word segmentation.\n",
    "context": [
      {
        "id": "a3c219dc-693f-445a-9709-6e61b2baed83",
        "metadata": {
          "chunk_id": 4,
          "document_id": "2001.01589",
          "start_idx": 1465,
          "end_idx": 2002
        },
        "page_content": ". The complex morpheme structure and relatively free constituent order can produce very large vocabulary because of the derivational morphology, so when translating from the agglutinative languages, many words are unseen at training time. Moreover, due to the semantic context, the same word generally has different segmentation forms in the training corpus. For the purpose of incorporating morphology knowledge of agglutinative languages into word segmentation for NMT, we propose a morphological word segmentation method on the source",
        "type": "Document"
      },
      {
        "id": "1fac8028-f560-437b-a4c4-178107df3be2",
        "metadata": {
          "chunk_id": 34,
          "document_id": "2001.01589",
          "start_idx": 12428,
          "end_idx": 13004
        },
        "page_content": " generate a more appropriate vocabulary that containing effective morpheme units and moderate subword units, which makes better generalization over the morphologically-rich words. The NMT system is typically trained with a limited vocabulary, which creates bottleneck on translation accuracy and generalization capability. Many word segmentation methods have been proposed to cope with the above problems, which consider the morphological properties of different languages. Bradbury and Socher BIBREF16 employed the modified Morfessor to provide morphology knowledge into word",
        "type": "Document"
      },
      {
        "id": "5f1d4e5c-c073-4dce-b5de-a547cc34c877",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1910.11493",
          "start_idx": 21090,
          "end_idx": 21632
        },
        "page_content": " (lemmatization and morphological feature analysis), 16 systems were submitted. All used neural network models, completing a trend in past years' shared tasks and other recent work on morphology. In task 1, gains from cross-lingual training were generally modest, with gains positively correlating with the linguistic similarity of the two languages. In the second task, several methods were implemented by multiple groups, with the most successful systems implementing variations of multi-headed attention, multi-level encoding, multiple dec",
        "type": "Document"
      },
      {
        "id": "359882ad-27f0-4761-84f9-0311a8a0cbd6",
        "metadata": {
          "chunk_id": 16,
          "document_id": "2001.01589",
          "start_idx": 5973,
          "end_idx": 6532
        },
        "page_content": " information. Hence, on the analyses of the above popular word segmentation methods, we propose the morphologically motivated segmentation strategy that combines the morpheme segmentation and BPE for further improving the translation performance of NMT. Compared with the sentence of word surface forms, the corresponding sentence of stem units only contains the structure information without considering morphological information, which can make better generalization over inflectional variants of the same word and reduce data sparseness BIBREF8. Therefore,",
        "type": "Document"
      },
      {
        "id": "53b781a9-41b4-489a-96d6-340875c16317",
        "metadata": {
          "chunk_id": 35,
          "document_id": "2001.01589",
          "start_idx": 13004,
          "end_idx": 13508
        },
        "page_content": "bury and Socher BIBREF16 employed the modified Morfessor to provide morphology knowledge into word segmentation, but they neglected the morphological varieties between subword units, which might result in ambiguous translation results. Sanchez-Cartagena and Toral BIBREF17 proposed a rule-based morphological word segmentation for Finnish, which applies BPE on all the morpheme units uniformly without distinguishing their inner morphological roles. Huck BIBREF18 explored target-side segmentation method",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the performance on the task evaluated?",
    "llm_answer": "Mean average precision (MAP) and F1 score; F1 score for question answering; evaluation during training and testing phases; per-turn evaluation metrics (BLEU, METEOR, ROUGE).\n",
    "context": [
      {
        "id": "405e6db8-0ef4-406a-b26f-b659c4cebac1",
        "metadata": {
          "chunk_id": 90,
          "document_id": "2002.11893",
          "start_idx": 33632,
          "end_idx": 34069
        },
        "page_content": " are filled by real values, the simulator terminates. This is regarded as \"task finish\". It's worth noting that \"task finish\" does not mean the task is success, because the system may provide wrong information. We calculated \"task finish rate\" on 1000 times simulations for each goal type (See Table TABREF31). Findings are summarized below: Cross multi-domain tasks (CM and CM+T) are much harder to finish. Comparing M and M+T, although",
        "type": "Document"
      },
      {
        "id": "ec846c61-b035-431a-88d4-858e8c47d3be",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1603.07044",
          "start_idx": 9966,
          "end_idx": 10403
        },
        "page_content": " for task A, there are a total number of 26,700 question-comment pairs. For task B, there are 2,670 question-question pairs. For task C, there are 26,700 question-comment pairs. The test dataset includes 50 questions, 500 related questions and 5,000 comments which do not overlap with the training set. To evaluate the performance, we use mean average precision (MAP) and F1 score. Table 2 shows the initial results using the RNN encoder",
        "type": "Document"
      },
      {
        "id": "e901bd2a-cc88-4572-a952-318c9e30a71a",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1908.10449",
          "start_idx": 10837,
          "end_idx": 11300
        },
        "page_content": "RC involves both MRC and RL, we adopt evaluation metrics from both settings. First, as a question answering task, we use $\\text{F}_1$ score to compare predicted answers against ground-truth, as in previous works. When there exist multiple ground-truth answers, we report the max $\\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we evaluate an agent's performance during both its training and testing phases.",
        "type": "Document"
      },
      {
        "id": "d5651b41-0bd5-449f-8b8c-696e3b149605",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1812.07023",
          "start_idx": 8700,
          "end_idx": 9153
        },
        "page_content": " of the highly relevant video. Automated evaluation of both task-oriented and non-task-oriented dialogue systems has been a challenge BIBREF26 , BIBREF27 too. Most such dialogue systems are evaluated using per-turn evaluation metrics since there is no suitable per-dialogue metric as conversations do not need to happen in a deterministic ordering of turns. These per-turn evaluation metrics are mostly word-overlap-based metrics such as BLEU, METEOR, R",
        "type": "Document"
      },
      {
        "id": "606c7438-617d-4245-acfc-80a4526dedd6",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1810.00663",
          "start_idx": 19261,
          "end_idx": 19754
        },
        "page_content": " our data collection effort. This section describes our evaluation of the proposed approach for interpreting navigation commands in natural language. We provide both quantitative and qualitative results. While computing evaluation metrics, we only consider the behaviors present in the route because they are sufficient to recover the high-level navigation plan from the graph. Our metrics treat each behavior as a single token. For example, the sample plan \u201cR-1 oor C-1 cf C-1 lt C-0 cf C-0 i",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what datasets were used?",
    "llm_answer": "Amazon Mechanical Turk dataset (100 simulated indoor environments, 8066 instruction/plan pairs), MCTest, CNN/Daily Mail, Children's Book Test (CBT), UltraSuite (UXTD, UXSSD, UPX), MSCOCO, Oxford-102, CUB, Oxford, COCO, and CIFAR-10.\n",
    "context": [
      {
        "id": "750cf7d4-eaf2-41cf-a5fe-3fdf7b919809",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1810.00663",
          "start_idx": 17810,
          "end_idx": 18361
        },
        "page_content": " . This dataset was created using Amazon Mechanical Turk and 100 maps of simulated indoor environments, each with 6 to 65 rooms. To the best of our knowledge, this is the first benchmark for comparing translation models in the context of behavioral robot navigation. As shown in Table TABREF16 , the dataset consists of 8066 pairs of free-form natural language instructions and navigation plans for training. This training data was collected from 88 unique simulated environments, totaling 6064 distinct navigation plans (2002 plans have two different",
        "type": "Document"
      },
      {
        "id": "2222ff1c-48c2-4593-8cbe-f96acd7fec63",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1703.04617",
          "start_idx": 1343,
          "end_idx": 1868
        },
        "page_content": " can help attain better results on our competitive baselines. Recent advance on reading comprehension and question answering has been closely associated with the availability of various datasets. BIBREF0 released the MCTest data consisting of 500 short, fictional open-domain stories and 2000 questions. The CNN/Daily Mail dataset BIBREF1 contains news articles for close style machine comprehension, in which only entities are removed and tested for comprehension. Children's Book Test (CBT) BIBREF2 leverages named entities",
        "type": "Document"
      },
      {
        "id": "75c8f911-af20-49c1-8b12-e4aecd8370c1",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1707.03904",
          "start_idx": 5304,
          "end_idx": 5850
        },
        "page_content": " subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short. We plan to release these annotations along with the datasets, and our retrieved documents for each question. Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated",
        "type": "Document"
      },
      {
        "id": "16a2312f-db4b-49e5-bd1a-b62820b498b9",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1907.00758",
          "start_idx": 9356,
          "end_idx": 9842
        },
        "page_content": ". We use UltraSuite: a repository of ultrasound and acoustic data gathered from child speech therapy sessions BIBREF15 . We used all three datasets from the repository: UXTD (recorded with typically developing children), and UXSSD and UPX (recorded with children with speech sound disorders). In total, the dataset contains 13,815 spoken utterances from 86 speakers, corresponding to 35.9 hours of recordings. The utterances have been categorised by the type of task the child was given",
        "type": "Document"
      },
      {
        "id": "7d4af53c-0878-4ce8-8945-c54db6f33d53",
        "metadata": {
          "chunk_id": 147,
          "document_id": "1910.09399",
          "start_idx": 57115,
          "end_idx": 57512
        },
        "page_content": " MSCOCO, and Oxford-102 datasets, which facilitated the work on text-to-image synthesis for several papers released more recently. While most deep learning algorithms use MNIST BIBREF74 dataset as the benchmark, there are three main datasets that are commonly used for evaluation of proposed GAN models for text-to-image synthesis: CUB BIBREF75, Oxford BIBREF76, COCO BIBREF77, and CIFAR-10 BIBREF",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they obtain human judgements?",
    "llm_answer": "The provided text describes using human judgments for evaluating selected concepts in historical time courses and predicting human judgments, but does not specify how those judgments are obtained.\n",
    "context": [
      {
        "id": "381b29a6-07c9-463b-b4b6-467a5d7ebb0a",
        "metadata": {
          "chunk_id": 9,
          "document_id": "2001.07209",
          "start_idx": 3691,
          "end_idx": 4290
        },
        "page_content": " framework that we propose consists of a parameter-free approach that facilitates the prediction of public moral sentiment toward individual concepts, automated retrieval of morally changing concepts, and broad-scale psycholinguistic analyses of historical rates of moral sentiment change. We provide a description of the probabilistic models and data used, followed by comprehensive evaluations of our methodology. An emerging body of work in natural language processing and computational social science has investigated how NLP systems can detect moral sentiment in online text. For example, moral",
        "type": "Document"
      },
      {
        "id": "7d55d6bb-ebcd-421b-8cf9-dd96b0cdfff4",
        "metadata": {
          "chunk_id": 34,
          "document_id": "2001.07209",
          "start_idx": 13176,
          "end_idx": 13731
        },
        "page_content": "s corpus, which enabled superior model performance throughout our evaluations. We applied our framework in three ways: 1) evaluation of selected concepts in historical time courses and prediction of human judgments; 2) automatic detection of moral sentiment change; and 3) broad-scale study of the relations between psycholinguistic variables and historical change of moral sentiment toward concepts. We applied our models diachronically to predict time courses of moral relevance, moral polarity, and fine-grained moral categories toward two historically",
        "type": "Document"
      },
      {
        "id": "dfa4bf53-23d7-469f-8314-f746bee8ac8f",
        "metadata": {
          "chunk_id": 69,
          "document_id": "1809.08731",
          "start_idx": 23501,
          "end_idx": 23955
        },
        "page_content": ":EMNLP20163 obtained close to no correlation with human judgements for BLEU, ROUGE and METEOR. They contributed this in a large part to the unrestrictedness of dialogue answers, which makes it hard to match given references. They emphasized that the community should move away from these metrics for dialogue generation tasks, and develop metrics that correlate more strongly with human judgments. elliott-keller:2014:P14-2 reported the same for BLEU and",
        "type": "Document"
      },
      {
        "id": "fbae4867-62b9-4352-b55c-910524c043c3",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1809.04960",
          "start_idx": 17128,
          "end_idx": 17625
        },
        "page_content": "truth comments of the corresponding news provided by the human. Plausible: The 50 most similar comments to the news. We use the news as the query to retrieve the comments that appear in the training set based on the cosine similarity of their tf-idf values. We select the top 50 comments that are not the correct comments as the plausible comments. Popular: The 50 most popular comments from the dataset. We count the frequency of each comments in the training set, and select the 50 most frequent",
        "type": "Document"
      },
      {
        "id": "459768c7-dec5-41b1-9d89-fd634c10cb90",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1809.08731",
          "start_idx": 4451,
          "end_idx": 5000
        },
        "page_content": " this research question, lau2017grammaticality aimed at modeling human acceptability judgments automatically, with the goal to gain insight into the nature of human perception of acceptability. In particular, they tried to answer the question: Do humans judge acceptability on a gradient scale? Their experiments showed a strong correlation between human judgments and normalized sentence log-probabilities under a variety of LMs for artificial data they had created by translating and back-translating sentences with neural models. While they tried",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Does this approach perform better in the multi-domain or single-domain setting?",
    "llm_answer": "Better performance in multi-span questions.\n",
    "context": [
      {
        "id": "a18a5eec-5472-445c-9ec8-12161fcd6c57",
        "metadata": {
          "chunk_id": 15,
          "document_id": "2002.11893",
          "start_idx": 5632,
          "end_idx": 6105
        },
        "page_content": " turns for 16 domains in the training set. However, the multi-domain dependency in these two datasets is only embodied in imposing the same pre-specified constraints on different domains, such as requiring a restaurant and an attraction to locate in the same area, or the city of a hotel and the destination of a flight to be the same (Table TABREF6). Table TABREF5 presents a comparison between our dataset with other task-oriented datasets. In comparison to MultiWOZ, our",
        "type": "Document"
      },
      {
        "id": "97f73259-88fa-4f4c-83aa-cc19dd71daac",
        "metadata": {
          "chunk_id": 68,
          "document_id": "1909.13375",
          "start_idx": 23233,
          "end_idx": 23710
        },
        "page_content": ".4 EM in multi-span questions and 1.5 EM in single-span questions. The single-span improvement is probably due to the combination of better multi-span head learning as a result of fixing multi-span questions and the fact that the multi-span head can answer single-span questions as well. Not using the single-span heads results in a slight drop in multi-span performance, and a noticeable drop in single-span performance. However when performing the same comparison between our",
        "type": "Document"
      },
      {
        "id": "3272c144-e0dd-4e5f-a7de-6da32fa8c008",
        "metadata": {
          "chunk_id": 68,
          "document_id": "2002.11893",
          "start_idx": 25301,
          "end_idx": 25772
        },
        "page_content": " well with context information. The performance on cross multi-domain dialogues (CM and CM+T) drops slightly, which may be due to the decrease of \"General\" intent and the increase of \"NoOffer\" as well as \"Select\" intent in the dialogue data. We also noted that the F1 score of \"Select\" intent is remarkably lower than those of other types, but context information can improve the performance significantly. Since recognizing domain transition is a key factor for a cross-",
        "type": "Document"
      },
      {
        "id": "41464b78-216e-4dcc-8f3a-446d35bf9f68",
        "metadata": {
          "chunk_id": 72,
          "document_id": "1909.13375",
          "start_idx": 24838,
          "end_idx": 25324
        },
        "page_content": " has a clear advantage in handling multi-span questions, with a 29.7 absolute improvement in EM, and a 15.1 absolute improvement in F1. Furthermore, we show that our model slightly eclipses the current state-of-the-art results on the entire DROP dataeset. Finally, we present some ablation studies, analyzing the benefit gained from individual components of our model. We believe that combining our tag-based approach for handling multi-span questions with current successful techniques",
        "type": "Document"
      },
      {
        "id": "0c63f370-2cb5-4a7d-bc41-ee4d70392878",
        "metadata": {
          "chunk_id": 56,
          "document_id": "2002.11893",
          "start_idx": 20810,
          "end_idx": 21322
        },
        "page_content": " challenging than other tasks because additional cross-domain constraints in HAR domains are strict and will result in more \"NoOffer\" situations (i.e., the wizard finds no result that satisfies the current constraints). In this situation, the wizard will try to relax some constraints and issue multiple queries to find some results for a recommendation while the user will compromise and change the original goal. The negotiation process is captured by \"NoOffer rate\", \"Multi-query rate\", and \"Goal change rate\"",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many samples did they generate for the artificial language?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "b4048d90-aa2a-4fd2-8f5e-b7848534cb17",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1910.04269",
          "start_idx": 17760,
          "end_idx": 18217
        },
        "page_content": " of 1,500 samples for each of six languages. Out of 1,500 samples for each language, 1,200 were randomly selected as training dataset for that language and rest 300 as validation dataset using k-fold cross-validation. To sum up, we trained our model on 7,200 samples and validated it on 1800 samples comprising six languages. The results are discussed in next section. This paper discusses two end-to-end approaches which achieve state-of-the-art results in",
        "type": "Document"
      },
      {
        "id": "fb85c318-f6d9-48b7-982a-9106ee5d2aec",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1909.00694",
          "start_idx": 15395,
          "end_idx": 15766
        },
        "page_content": " segmented into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam BIBREF19. The mini-batch size was 32. We ran 1 epoch.",
        "type": "Document"
      },
      {
        "id": "0093df87-abcd-4759-863f-1dba19bfbc0a",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1908.08419",
          "start_idx": 24134,
          "end_idx": 24599
        },
        "page_content": " iterations, which is a good trade-off between speed and performance. As for the number of samples selected per iteration, there are 6078 sentences in unlabeled set, considering the high cost of relabeling, we set four sizes of samples selected per iteration, which are 2%, 5%, 8% and 11%. The experimental results of two sampling strategies with 30 iterations on four different proportions of relabeled data are shown in Fig. FIGREF45 , where x-axis represents the",
        "type": "Document"
      },
      {
        "id": "3c82b205-5d09-49eb-b2f7-0d3d9bb50ffb",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1911.01799",
          "start_idx": 10361,
          "end_idx": 10700
        },
        "page_content": " two parts: VoxCeleb1 and VoxCeleb2. We used SITW BIBREF21, a subset of VoxCeleb1 as the evaluation set. The rest of VoxCeleb1 was merged with VoxCeleb2 to form the training set (simply denoted by VoxCeleb). The training set involves $1,236,567$ utterances from $7,185$ speakers, and the evaluation set involves $6,445$ utterances from 299",
        "type": "Document"
      },
      {
        "id": "78dd82c6-c6af-4015-b4e0-f73037eb863b",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1904.05862",
          "start_idx": 15343,
          "end_idx": 15454
        },
        "page_content": "letter++ experiments, and Tatiana Likhomanenko for providing convolutional language models for our experiments.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Why does not the approach from English work on other languages?",
    "llm_answer": "The approach, while successful for some languages, is less robust to new speakers in the test dataset, and struggles when source and target languages have differing morphological structures (e.g., suffixing vs. prefixing).  Also, annotation discrepancies and low overlap in forms between languages pose challenges.\n",
    "context": [
      {
        "id": "910e2fe4-f2a4-42fb-afd4-880f58ce8885",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1910.05456",
          "start_idx": 9884,
          "end_idx": 10340
        },
        "page_content": " additionally makes use of internal stem changes (e.g., o $\\rightarrow $ ue). Since English and Spanish are both Indo-European languages, and, thus, relatively similar, we further add a third, unrelated target language. We choose Zulu (ZUL), a Bantoid language. In contrast to the first two, it is strongly prefixing. For pretraining, we choose languages with different degrees of relatedness and varying morphological similarity to English, Spanish, and Z",
        "type": "Document"
      },
      {
        "id": "bc88a120-0f38-469e-ae34-2ef2537334f3",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1910.05456",
          "start_idx": 2256,
          "end_idx": 2734
        },
        "page_content": " on each of the source languages and then fine-tune the resulting models on small datasets in each of the target languages. Analyzing the errors made by the systems, we find that (i) source and target language being closely related simplifies the successful learning of inflection in the target language, (ii) the task is harder to learn in a prefixing language if the source language is suffixing \u2013 as well as the other way around, and (iii) a source language which exhibits an",
        "type": "Document"
      },
      {
        "id": "05489258-3cbf-4bf5-86c1-af65b5a78974",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1910.04269",
          "start_idx": 6552,
          "end_idx": 7071
        },
        "page_content": " for feature extraction. An elaborate analysis of how deep architectures outperform their shallow counterparts is presented in this reseacrch. The difficulties in classifying perceptually similar languages like German and English were also put forward in this work. It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize",
        "type": "Document"
      },
      {
        "id": "214dd06f-dd3d-4cff-9476-05e6acd67a75",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1810.06743",
          "start_idx": 18569,
          "end_idx": 19061
        },
        "page_content": " from irremediable annotation discrepancies. Largely, we are hamstrung by differences in choice of attributes to annotate. When one resource marks gender and the other marks case, we can't infer the gender of the word purely from its surface form. The resources themselves would need updating to encode the relevant morphosyntactic information. Some languages had a very low number of overlapping forms, and no tag matches or near-matches between them: Arabic, Hindi, Lithuanian, Persian, and",
        "type": "Document"
      },
      {
        "id": "e72a717a-7cc7-4ecf-a6e2-86a27126da6c",
        "metadata": {
          "chunk_id": 76,
          "document_id": "1911.00069",
          "start_idx": 24530,
          "end_idx": 24998
        },
        "page_content": ".9 $F_1$ points, except for Arabic. Since our approach projects the target-language word embeddings to the source-language embedding space preserving the word order, it is expected to work better for a target language that has more similar word order as the source language. This has been verified by our experiments. The source language, English, belongs to the SVO (Subject, Verb, Object) language family where in a sentence the subject comes first, the verb second,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they measure grammaticality?",
    "llm_answer": "Speakers' intuitions about sentence well-formedness, often using \"acceptability\" or \"grammaticality\" interchangeably, are used to draw conclusions about language rules.  A log ratio of grammatical phrases to ungrammatical phrases is also considered.\n",
    "context": [
      {
        "id": "f2bbf933-ddc1-46e7-9f9d-629baf2bcc68",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1809.08731",
          "start_idx": 812,
          "end_idx": 1267
        },
        "page_content": " evaluation, especially of fluency, causing word-overlap metrics like ROUGE BIBREF1 to correlate only weakly with human judgments BIBREF2 . As a result, fluency evaluation of NLG is often done manually, which is costly and time-consuming. Evaluating sentences on their fluency, on the other hand, is a linguistic ability of humans which has been the subject of a decade-long debate in cognitive science. In particular, the question has been raised whether",
        "type": "Document"
      },
      {
        "id": "652d4c94-eb07-4fa0-892e-4efa5677655e",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1809.08731",
          "start_idx": 20536,
          "end_idx": 21023
        },
        "page_content": " fluency evaluation in NLP has been rare. heilman2014predicting predicted the fluency (which they called grammaticality) of sentences written by English language learners. In contrast to ours, their approach is supervised. stent2005evaluating and cahill2009correlating found only low correlation between automatic metrics and fluency ratings for system-generated English paraphrases and the output of a German surface realiser, respectively. Explicit fluency evaluation of NLG, including",
        "type": "Document"
      },
      {
        "id": "678531b1-f787-4325-85b9-e2b62d4eac02",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1809.08731",
          "start_idx": 3495,
          "end_idx": 3943
        },
        "page_content": "' judgments of the well-formedness of sentences, have been the basis of much linguistics research BIBREF10 , BIBREF11 : a speakers intuition about a sentence is used to draw conclusions about a language's rules. Commonly, \u201cacceptability\u201d is used synonymously with \u201cgrammaticality\u201d, and speakers are in practice asked for grammaticality judgments or acceptability judgments interchangeably. Strictly speaking, however, a sentence can be unacceptable",
        "type": "Document"
      },
      {
        "id": "328e8d41-c8e5-498c-84b0-ed80e0155465",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1906.04571",
          "start_idx": 15701,
          "end_idx": 16142
        },
        "page_content": " that are grammatical, we additionally consider the following log ratio (i.e., the grammatical phrase over the ungrammatical phrase): DISPLAYFORM0  We trained the linear parameterization using UD treebanks for Spanish, Hebrew, French, and Italian (see tab:data). For each of the four languages, we parsed one million sentences from Wikipedia (May 2018 dump) using BIBREF24 's parser and extracted taggings and lemmata using the method of BIB",
        "type": "Document"
      },
      {
        "id": "6c10647f-e85b-46c8-830d-c455fa25babe",
        "metadata": {
          "chunk_id": 51,
          "document_id": "2003.07433",
          "start_idx": 20049,
          "end_idx": 20591
        },
        "page_content": " each of the category words within texts. After the PTSD Linguistic Dictionary has been created, we empirically evaluate its psychometric properties such as reliability and validity as per American Standards for educational and psychological testing guideline BIBREF24. In psychometrics, reliability is most commonly evaluated by Cronbach's alpha, which assesses internal consistency based on inter-correlations and the number of measured items. In the text analysis scenario, each word in our PTSD Linguistic dictionary is considered an item",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the difference in recall score between the systems?",
    "llm_answer": "The context states that recall is consistently significantly lower than precision, and that Stanford NER and CRFSuite have almost 50% lower recall on unseen NEs compared to seen NEs.  However, a direct difference in recall scores between *all* systems is not provided.\n",
    "context": [
      {
        "id": "0b9c40d2-cf4a-4254-9e98-5f7c70ba79be",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1910.03814",
          "start_idx": 19558,
          "end_idx": 20033
        },
        "page_content": " results for the LSTM, for the Davison method proposed in BIBREF7 trained with MMHS150K, and for random scores. Fig. FIGREF32 shows the Precision vs Recall plot and the ROC curve (which plots the True Positive Rate vs the False Positive Rate) of the different models. First, notice that given the subjectivity of the task and the discrepancies between annotators, getting optimal scores in the evaluation metrics is virtually impossible. However, a system with relatively low",
        "type": "Document"
      },
      {
        "id": "0e1d4379-0ace-4e60-854e-27d00cf8fd0e",
        "metadata": {
          "chunk_id": 94,
          "document_id": "1701.02877",
          "start_idx": 33635,
          "end_idx": 34079
        },
        "page_content": " performance result. What becomes clear from the macro averages is that F1 on unseen NEs is significantly lower than F1 on seen NEs for all three NERC approaches. This is mostly due to recall on unseen NEs being lower than that on seen NEs, and suggests some memorisation and poor generalisation in existing systems. In particular, Stanford NER and CRFSuite have almost 50% lower recall on unseen NEs compared to seen NEs. One outlier is ACE UN",
        "type": "Document"
      },
      {
        "id": "f9a054f1-4f5c-40a9-ba52-d557f3fb00ed",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1906.04571",
          "start_idx": 12581,
          "end_idx": 13071
        },
        "page_content": " intervened. We present the results in tab:intrinsic. Recall is consistently significantly lower than precision. As expected, the baselines have the highest precision (though not by much). This is because they reflect well-known rules for each language. That said, they have lower recall than our approach because they fail to capture more subtle relationships. For both languages, our approach struggles with conjunctions. For example, consider the phrase \u00e9l es un ingeniero y escritor (he",
        "type": "Document"
      },
      {
        "id": "2cbac908-057e-4ae3-83db-8958c941765a",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1809.04960",
          "start_idx": 21435,
          "end_idx": 21923
        },
        "page_content": ". With the help of our proposed model, both IR and S2S achieve an improvement under the semi-supervised scenarios. We analyze the performance of the proposed method under the semi-supervised setting. We train the supervised IR model with different numbers of paired data. Figure FIGREF39 shows the curve (blue) of the recall1 score. As expected, the performance grows as the paired dataset becomes larger. We further combine the supervised IR with our unsupervised model, which is trained",
        "type": "Document"
      },
      {
        "id": "ce9a5da7-b644-459d-89dc-6f6485bbc871",
        "metadata": {
          "chunk_id": 62,
          "document_id": "1701.02877",
          "start_idx": 22362,
          "end_idx": 22802
        },
        "page_content": " data gives little to no improvement, whereas adding positive examples drastically improves performance. Table UID9 shows results with size normalised precision (P), recall (R), and F1-Score (F1). The five lowest P, R and F1 values per method (CRFSuite, Stanford NER, SENNA) are in bold to highlight underperformers. Results for all corpora are summed with macro average. Comparing the different methods, the highest F1 results are achieved",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is their f1 score and recall?",
    "llm_answer": "The provided text gives an F1 score of 78.38% on the development set, but does not give a recall score.\n",
    "context": [
      {
        "id": "96ecbdb9-17ee-46e0-9f1c-9eef560fb526",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1912.13109",
          "start_idx": 10203,
          "end_idx": 10681
        },
        "page_content": " metrics - Recall rate, F1 score and Overall accuracy. Choice of model parameters were in the above models were inspired from previous work done but then were tuned to the best performance of the Test dataset. Following parameters were considered for tuning. Learning rate: Based on grid search the best performance was achieved when learning rate was set to 0.01. This value was arrived by a grid search on lr parameter. Number of Bidirectional LSTM units: A set of 32, 64, 128",
        "type": "Document"
      },
      {
        "id": "afb1b755-5b9b-4004-b49d-6c5e2eb9114b",
        "metadata": {
          "chunk_id": 88,
          "document_id": "1703.04617",
          "start_idx": 24013,
          "end_idx": 24428
        },
        "page_content": " question is the major class. Figure 5 shows the composition of F1 score. Take our best model as an example, we observed a 78.38% F1 score on the whole development set, which can be separated into two parts: one is where F1 score equals to 100%, which means an exact match. This part accounts for 69.10% of the entire development set. And the other part accounts for 30.90%, of which the average F1 score is 30.03%.",
        "type": "Document"
      },
      {
        "id": "ce9a5da7-b644-459d-89dc-6f6485bbc871",
        "metadata": {
          "chunk_id": 62,
          "document_id": "1701.02877",
          "start_idx": 22362,
          "end_idx": 22802
        },
        "page_content": " data gives little to no improvement, whereas adding positive examples drastically improves performance. Table UID9 shows results with size normalised precision (P), recall (R), and F1-Score (F1). The five lowest P, R and F1 values per method (CRFSuite, Stanford NER, SENNA) are in bold to highlight underperformers. Results for all corpora are summed with macro average. Comparing the different methods, the highest F1 results are achieved",
        "type": "Document"
      },
      {
        "id": "b09849dc-7c1a-4d5b-b18a-ed9e0b2c65af",
        "metadata": {
          "chunk_id": 100,
          "document_id": "1701.02877",
          "start_idx": 35714,
          "end_idx": 36084
        },
        "page_content": " higher at 76.42; the less diverse data is, on average, easier to tag. Interestingly, average F1 on seen NEs in the Twitter corpora (MSM and Ritter) is around 80, whereas average F1 on the ACE corpora, which are of similar size, is lower, at around 70. To summarise, our findings are: [noitemsep] F1 on unseen NEs is significantly lower than F1 on seen NEs for all three",
        "type": "Document"
      },
      {
        "id": "78e433ea-6d35-4d13-8ff8-4d8704e6615f",
        "metadata": {
          "chunk_id": 69,
          "document_id": "1701.02877",
          "start_idx": 24783,
          "end_idx": 25180
        },
        "page_content": " this we study again Precision (P), Recall (R) and F1 metrics on size-normalised corpora (Table UID9 ), on original corpora (Tables \"RQ1: NER performance with Different Approaches\" and \"RQ1: NER performance with Different Approaches\" ), and we further test performance per genre in a separate table (Table 3 ). F1 scores over size-normalised corpora vary widely (Table UID9 ). For example, the SEN",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many layers does their system have?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "cc19f0cb-3472-43b2-b27a-65551596a757",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1909.00542",
          "start_idx": 7743,
          "end_idx": 8155
        },
        "page_content": " layer, and use cross-entropy as the loss function. The complete architecture is shown in Fig. FIGREF28. The bottom section of Table TABREF26 shows the results of several variants of the neural architecture. The table includes a neural regressor (NNR) and a neural classifier (NNC). The neural classifier is trained in two set ups: \u201cNNC top 5\u201d uses classification labels as described in Section SECREF3, and \u201cNNC",
        "type": "Document"
      },
      {
        "id": "415aa967-c234-4a69-b6c2-9dbc77f6c838",
        "metadata": {
          "chunk_id": 71,
          "document_id": "1910.02789",
          "start_idx": 28472,
          "end_idx": 28930
        },
        "page_content": " has 8 6X6 filters with stride 3 and ReLU activation. The second convolutional layer has 16 3X3 filters with stride 2 and ReLU activation. The fully connected layers has 32 and 16 units, both of them are followed by ReLU activation. The output layer's size is the amount of actions the agent has available in the trained scenario. Used in the feature vector based agent. Naturally, some discretization is needed in order to build a feature vector, so some of",
        "type": "Document"
      },
      {
        "id": "e9eaaa3e-8387-46f5-908f-2956e516ab75",
        "metadata": {
          "chunk_id": 22,
          "document_id": "2001.00137",
          "start_idx": 8966,
          "end_idx": 9386
        },
        "page_content": "ERT embedding size for a single token, and 128 is the maximum sequence length in a sentence. The stacks of multilayer perceptrons are structured as two sets of three layers with two hidden layers each. The first set is responsible for compressing the $h_{inc}$ into a latent-space representation, extracting more abstract features into lower dimension vectors $z_1$, $z_2$ and $\\mathbf {z}$ with shape $(N_{bs}, 128, 128",
        "type": "Document"
      },
      {
        "id": "90598282-ee61-46a3-b320-5d4da1e9e7ee",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1806.00722",
          "start_idx": 14702,
          "end_idx": 15086
        },
        "page_content": " upper layer structure of the model. We also introduce three additional models we use for ablation study, all using 4-layer structure. Based on the residual connected BASE-4L model, (1) DenseENC-4L only makes encoder side dense, (2) DenseDEC-4L only makes decoder side dense, and (3) DenseAtt-4L only makes the attention dense using DenseAtt-2. There is no summary layer in the models",
        "type": "Document"
      },
      {
        "id": "a1061a92-7bff-4acc-a12c-46948eeae8da",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1908.10449",
          "start_idx": 13757,
          "end_idx": 14232
        },
        "page_content": " the encoding transformer blocks. Each encoding transformer block consists of four convolutional layers (with shared weights), a self-attention layer, and an MLP. Each convolutional layer has 96 filters, each kernel's size is 7. In the self-attention layer, we use a block hidden size of 96 and a single head attention mechanism. Layer normalization and dropout are applied after each component inside the block. We add positional encoding into each block's input. We use one",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What context modelling methods are evaluated?",
    "llm_answer": "Different context modeling methods are evaluated.  The text mentions some representative models but doesn't explicitly list all methods.\n",
    "context": [
      {
        "id": "f3d0c66e-fc52-4734-a87b-52f79c8141bc",
        "metadata": {
          "chunk_id": 63,
          "document_id": "2002.00652",
          "start_idx": 20343,
          "end_idx": 20834
        },
        "page_content": " locate the substitution target, while implicit does not. On most cases, the target is schema or operator. In order to study the effect of context modeling methods on various phenomena, as shown in Table TABREF39, we take the development set of SParC as an example to perform our analysis. The analysis begins by presenting Ques.Match of three representative models on above fine-grained types in Figure FIGREF42. As shown, though different methods have different strengths, they all perform",
        "type": "Document"
      },
      {
        "id": "b95b5f06-43bf-4e7b-beb5-99dfc997f83a",
        "metadata": {
          "chunk_id": 58,
          "document_id": "2002.00652",
          "start_idx": 18202,
          "end_idx": 18702
        },
        "page_content": " modeling methods, they do not significantly improve the performance as we expected. As mentioned in Section SECREF1, intuitively, methods which only use the precedent SQL enjoys better generalizability. To validate it, we further conduct an out-of-distribution experiment to assess the generalizability of different context modeling methods. Concretely, we select three representative methods and train them on questions at turn 1 and 2, whereas test them at turn 3, 4 and beyond. As shown in Figure",
        "type": "Document"
      },
      {
        "id": "1e627a2c-64b1-4fa5-ba0b-aec3dfbb96b6",
        "metadata": {
          "chunk_id": 60,
          "document_id": "2002.00652",
          "start_idx": 19012,
          "end_idx": 19544
        },
        "page_content": " context modeling methods in the task of SPC are not as effective as expected, since they do not show a significant advantage over the simple concatenation method. By a careful investigation on contextual phenomena, we summarize them in multiple hierarchies. Roughly, there are three kinds of contextual phenomena in questions: semantically complete, coreference and ellipsis. Semantically complete means a question can reflect all the meaning of its corresponding SQL. Coreference means a question contains pronouns, while ellipsis",
        "type": "Document"
      },
      {
        "id": "bb6c8e4a-7e69-4bf8-a788-0022a1b37b92",
        "metadata": {
          "chunk_id": 73,
          "document_id": "2002.00652",
          "start_idx": 24143,
          "end_idx": 24692
        },
        "page_content": " methods. This work conducts an exploratory study on semantic parsing in context, to realize how far we are from effective context modeling. Through a thorough comparison, we find that existing context modeling methods are not as effective as expected. A simple concatenation method can be much competitive. Furthermore, by performing a fine-grained analysis, we summarize two potential directions as our future work: incorporating common sense for better pronouns inference, and modeling contextual clues in a more explicit manner. By open-sourcing",
        "type": "Document"
      },
      {
        "id": "53d9bc9b-6a3e-44dc-84cd-41feba0c46e0",
        "metadata": {
          "chunk_id": 9,
          "document_id": "2002.00652",
          "start_idx": 3418,
          "end_idx": 3964
        },
        "page_content": " of different context modeling methods. Our best model achieves state-of-the-art (SOTA) performances on both datasets with significant improvements. Furthermore, we summarize and generalize the most frequent contextual phenomena, with a fine-grained analysis on representative models. Through the analysis, we obtain some interesting findings, which may benefit the community on the potential research directions. We will open-source our code and materials to facilitate future work upon acceptance. In the task of semantic parsing in context, we",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is the baseline a non-heirarchical model like BERT?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "f09f867a-bb11-4305-a53b-02b63aafd4c1",
        "metadata": {
          "chunk_id": 6,
          "document_id": "2002.06644",
          "start_idx": 2489,
          "end_idx": 2911
        },
        "page_content": " various BERT-based models, including BERT, RoBERTa, ALBERT, with their base and large specifications along with their native classifiers. We propose an ensemble model exploiting predictions from these models using multiple ensembling techniques. We show that our model outperforms the baselines by a margin of $5.6$ of F1 score and $5.95\\%$ of Accuracy. In this section, we outline baseline models like $BERT_{large}$. We",
        "type": "Document"
      },
      {
        "id": "76b8b551-5acb-4396-ad68-bd7052c2758b",
        "metadata": {
          "chunk_id": 17,
          "document_id": "2002.06644",
          "start_idx": 6467,
          "end_idx": 6878
        },
        "page_content": "ERT based models, and BERT-based ensemble models outperform the baselines for all the metrics. Among the optimized BERT based models, $RoBERTa_{large}$ outperforms all other non-ensemble models and the baselines for all metrics. It further achieves a maximum recall of $0.681$ for all the proposed models. We note that DistillRoBERTa, a distilled model, performs competitively, achieving $69.69\\%$ accuracy, and",
        "type": "Document"
      },
      {
        "id": "3ac3a0e8-69f1-43ec-aaf0-7e7c3efcd5f4",
        "metadata": {
          "chunk_id": 33,
          "document_id": "1902.09314",
          "start_idx": 12164,
          "end_idx": 12554
        },
        "page_content": " of AEN-GloVe. We also design a basic BERT-based model to evaluate the performance of AEN-BERT.   Non-RNN based baselines:  INLINEFORM0 Feature-based SVM BIBREF18 is a traditional support vector machine based model with extensive feature engineering.  INLINEFORM0 Rec-NN BIBREF0 firstly uses rules to transform the dependency tree and put the opinion target at the root, and then learns the",
        "type": "Document"
      },
      {
        "id": "4ee1b439-88f0-4cb0-bd49-10dc06151b3b",
        "metadata": {
          "chunk_id": 62,
          "document_id": "1906.06448",
          "start_idx": 23037,
          "end_idx": 23574
        },
        "page_content": " was better on upward inferences than that on downward inferences. This indicates that BERT might fail to capture the monotonicity property that conditionals create a downward entailing context in their scope while they create an upward entailing context out of their scope. Regarding lexical knowledge, the data augmentation technique improved the performance much better on downward inferences which do not require lexical knowledge. However, among the 394 problems for which all models provided wrong answers, 244 problems are non-lex",
        "type": "Document"
      },
      {
        "id": "16a1824d-3adf-4a18-9746-ea8f4b7c9f43",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1906.06448",
          "start_idx": 18127,
          "end_idx": 18651
        },
        "page_content": " having rich lexical knowledge from a knowledge base (KIM) or pretraining (BERT). This indicates that downward inferences are difficult to perform even with the expansion of lexical knowledge. In addition, it is interesting to see that if a model performed better on upward inferences, it performed worse on downward inferences. We will investigate these results in detail below. To explore whether the performance of models on monotonicity reasoning depends on the training set or the model themselves, we conducted further",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How better are results compared to baseline models?",
    "llm_answer": "The model outperforms baselines by a clear margin in new environments, with improvements in Exact Match and Goal Match metrics by 35% and 25%, respectively, in previously seen environments.  In task A, the model performs better than a strong feature-rich system with sufficient training data.  In task B, the model exhibits substantial improvement across all metrics compared to base models.\n",
    "context": [
      {
        "id": "fc23cf7f-21ff-43cd-8d00-ef5914322400",
        "metadata": {
          "chunk_id": 68,
          "document_id": "1810.00663",
          "start_idx": 26280,
          "end_idx": 26763
        },
        "page_content": " room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM. Note that the average number of actions in the ground truth output sequences is 7.07 for the Test-New set. Our model's predictions are just INLINEFORM1 edits off on average from the correct navigation plans. This",
        "type": "Document"
      },
      {
        "id": "d7a0b7aa-da14-4112-80c0-cc26c62fe83f",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "abc51b19-3db2-4cbf-ac46-ff97aa2fa1d1",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1603.07044",
          "start_idx": 14997,
          "end_idx": 15515
        },
        "page_content": " task A already had a substantial amount of training data. Table 4 gives the final comparison between different models (we only list the MAP score because it is the official score for the challenge). Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited",
        "type": "Document"
      },
      {
        "id": "5e0f3225-070e-4250-8f70-93c083311ec4",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "5e5c6963-88aa-4a86-96ff-323aeb271bfb",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1810.00663",
          "start_idx": 25415,
          "end_idx": 25983
        },
        "page_content": " In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models. The previous section evaluated model performance on new instructions (and corresponding navigation plans) for environments that were previously seen at training time. Here, we examine whether the trained models succeed on environments that are completely new. The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is dataset used for training/testing?",
    "llm_answer": "The context describes various datasets and their sizes for training and testing, but does not provide a single overall answer to the question of the size of the datasets used for training/testing.\n",
    "context": [
      {
        "id": "001abd8b-7ba1-407b-8110-49e732f239a9",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1910.12618",
          "start_idx": 19089,
          "end_idx": 19565
        },
        "page_content": " maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014",
        "type": "Document"
      },
      {
        "id": "b99f9b51-787d-423c-8c49-bf72943f2462",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1701.02877",
          "start_idx": 9957,
          "end_idx": 10406
        },
        "page_content": " training and testing data sizes measured in number of NEs are listed in Table 2 . Separate models are then trained on the training parts of each corpus and evaluated on the development (if available) and test parts of the same corpus. If development parts are available, as they are for CoNLL (CoNLL Test A) and MUC (MUC 7 Dev), they are not merged with the training corpora for testing, as it was permitted to do in the context of those evaluation",
        "type": "Document"
      },
      {
        "id": "2d3bdfec-4ed5-456d-bd82-023fad798858",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "1e2f925a-9a0d-4318-af38-ee9ae9d2a426",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1912.13109",
          "start_idx": 7921,
          "end_idx": 8471
        },
        "page_content": " on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we",
        "type": "Document"
      },
      {
        "id": "9d6e4c30-d037-4a07-a951-0e7d1761e751",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1912.13109",
          "start_idx": 7657,
          "end_idx": 8136
        },
        "page_content": " above few data augmentation techniques were applied to boost the learning of the network. Before applying augmentation, a train-test split of 78%-22% was done from the original, cleansed data set. Thus, 700 tweets/messages were held out for testing. All model evaluation were done in on the test set that got generated by this process. The results presented in this report are based on the performance of the model on the test set. The training set of 2489 messages were however",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What geometric properties do embeddings display?",
    "llm_answer": "Distances between word vectors, specifically cosine distance.\n",
    "context": [
      {
        "id": "e43dfd8b-27df-4b96-855a-a9974f2914af",
        "metadata": {
          "chunk_id": 81,
          "document_id": "1910.12618",
          "start_idx": 32047,
          "end_idx": 32551
        },
        "page_content": "ial properties translating linguistic ones. However considering the objective function of our problem, there was no obvious reason for such attributes to appear in our own. Nevertheless for both languages we conducted an analysis of the geometric properties of our embedding matrix. We investigated the distances between word vectors, the relevant metric being the cosine distance given by: where $\\overrightarrow{w_1}$ and $\\overrightarrow{w_2}$ are given word vectors. Thus a cosine distance lower than",
        "type": "Document"
      },
      {
        "id": "39a7d1e1-c508-4f9f-a6c6-5df3492db85f",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1910.12618",
          "start_idx": 14094,
          "end_idx": 14564
        },
        "page_content": ". In any case, the very nature of the objective function allows the embedding models to learn to translate linguistic similarities into geometric properties in the vector space. For instance the vector $\\overrightarrow{king} - \\overrightarrow{man} + \\overrightarrow{woman}$ is expected to be very close to the vector $\\overrightarrow{queen}$. However in our case we want a vector encoding which is tailored for the technical vocabulary of our weather reports and for the",
        "type": "Document"
      },
      {
        "id": "17902d4c-26a5-4f6d-8c8c-344c7f658d89",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1910.07181",
          "start_idx": 13094,
          "end_idx": 13495
        },
        "page_content": "formative ones. The final embedding $v_{(w, \\mathcal {C})}$ is then a linear combination of the embeddings obtained from each context, where the weight of each embedding is determined based on the self-attention layer. For further details on this mechanism, we refer to BIBREF19. Like previous work, we use mimicking BIBREF8 as a training objective. That is, given a frequent word $w$ with known embed",
        "type": "Document"
      },
      {
        "id": "54ac8b48-80ec-4213-a247-050b88ae1cea",
        "metadata": {
          "chunk_id": 73,
          "document_id": "1905.00563",
          "start_idx": 22995,
          "end_idx": 23443
        },
        "page_content": " of the Change For TransE\" . Furthermore, there is a growing body of literature that incorporates an extra types of evidence for more informed embeddings such as numerical values BIBREF26 , images BIBREF27 , text BIBREF28 , BIBREF29 , BIBREF30 , and their combinations BIBREF31 . Using , we can gain a deeper understanding of these methods, especially those that build their embeddings wit hmultiplicative scoring functions. Interpretability and Ad",
        "type": "Document"
      },
      {
        "id": "1efae2e2-37d5-4ef3-a1a3-9ea2b87b2471",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1910.14497",
          "start_idx": 0,
          "end_idx": 500
        },
        "page_content": "Word embeddings, or vector representations of words, are an important component of Natural Language Processing (NLP) models and necessary for many downstream tasks. However, word embeddings, including embeddings commonly deployed for public use, have been shown to exhibit unwanted societal stereotypes and biases, raising concerns about disparate impact on axes of gender, race, ethnicity, and religion BIBREF0, BIBREF1. The impact of this bias has manifested in a range of downstream tasks, ranging",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How accurate is model trained on text exclusively?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "b54e5b94-84e8-4314-9e0d-1e3d9daa7055",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1907.00758",
          "start_idx": 17344,
          "end_idx": 17855
        },
        "page_content": " across different conditions. Table shows the model accuracy broken down by utterance type. The model achieves 91.2% accuracy on utterances containing words, sentences, and conversations, all of which exhibit natural variation in speech. The model is less successful with Articulatory utterances, which contain isolated phones occurring once or repeated (e.g., \u201csh sh sh\"). Such utterances contain subtle tongue movement, making it more challenging to correlate the visual signal with the audio. And indeed, the",
        "type": "Document"
      },
      {
        "id": "9cbb0694-fdd6-4d5c-b0b6-6d4c9d087138",
        "metadata": {
          "chunk_id": 51,
          "document_id": "2001.00137",
          "start_idx": 19320,
          "end_idx": 19801
        },
        "page_content": "$, an improvement of 6$\\%$. In the last case (Inc+Corr), we consider both incorrect and correct tweets as input to the models in hopes of improving performance. However, the accuracy was similar to the first aforementioned version, 80$\\%$ for our model and 74$\\%$ for the second highest performing model. Since the first and last corpus gave similar performances with our model, we conclude that the Twitter dataset does not require complete sentences to be given as training input",
        "type": "Document"
      },
      {
        "id": "ccd81d25-b4ef-4d16-98ac-f9898ca20a7d",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1610.00879",
          "start_idx": 9948,
          "end_idx": 10164
        },
        "page_content": ".8%, our system reaches reasonably close and performs with a best accuracy of 64%. Our analysis of the task and experimental findings make a case for drunk-texting prediction as a useful and feasible NLP application.",
        "type": "Document"
      },
      {
        "id": "dafa5052-d9fa-47cf-bcf5-a0c22a1efe79",
        "metadata": {
          "chunk_id": 63,
          "document_id": "2001.00137",
          "start_idx": 23847,
          "end_idx": 24394
        },
        "page_content": " their accuracy while the lower achieving class saw improvement. Further evaluation on the F1-scores decay in the presence of noise demonstrated that our model is more robust than the baseline models when considering noisy data, be that in the form of incorrect sentences or sentences with STT error. Not only that, experiments on the Twitter dataset also showed improved accuracy in clean data, with complete sentences. We infer that this is due to our model being able to extract richer data representations from the input data regardless of the",
        "type": "Document"
      },
      {
        "id": "1a05445c-9a17-4749-8eed-17235b271571",
        "metadata": {
          "chunk_id": 53,
          "document_id": "1909.00430",
          "start_idx": 19021,
          "end_idx": 19456
        },
        "page_content": " from this signal, it learns mostly to predict the majority class, and hence reaches very low F1 scores of 35.28. The more data given to the sentence level classifier, the better the potential results will be when training with our method using the classifier labels, with a classifiers trained on 100,1000,5000 and 10000 labeled sentences, we get a F1 scores of 53.81, 58.84, 61.81, 65.58 respectively. Improvements in the source task",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was their result on Stance Sentiment Emotion Corpus?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "7cd500f0-a47b-4913-b9f9-0e8a0c26187f",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1911.12569",
          "start_idx": 14068,
          "end_idx": 14516
        },
        "page_content": " word. In this section we present the details of the datasets used for the experiments, results that we obtain and the necessary analysis. We evaluate our proposed approach for joint sentiment and emotion analysis on the benchmark dataset of SemEval 2016 Task 6 BIBREF7 and Stance Sentiment Emotion Corpus (SSEC) BIBREF15. The SSEC corpus is an annotation of the SemEval 2016 Task 6 corpus with emotion labels. The re-annotation of the SemEval 2016",
        "type": "Document"
      },
      {
        "id": "300a63df-b6fa-4825-8248-0f25d1ca9218",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1910.11769",
          "start_idx": 4371,
          "end_idx": 4765
        },
        "page_content": " sentiment template and instructed the crowd annotators to `pick the best/major emotion embodied in the passage'. We further provided instructions to clarify the intensity of an emotion, such as: \u201cRage/Annoyance is a form of Anger\u201d, \u201cSerenity/Ecstasy is a form of Joy\u201d, and \u201cLove includes Romantic/Family/Friendship\u201d, along with sample passages. We required all annotators have a `master' MTurk",
        "type": "Document"
      },
      {
        "id": "2719dfbe-4fdf-406c-89e4-4223bcb8903e",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1710.01492",
          "start_idx": 13584,
          "end_idx": 14049
        },
        "page_content": " respect from sentiment analysis, is that of stance detection. The goal here is to determine whether the author of a piece of text is in favor of, against, or neutral toward a proposition or a target BIBREF36 . For example, in (8) the author has a negative stance toward the proposition w\u200bomen have the right to abortion, even though the target is not mentioned at all. Similarly, in (9\u00a7) the author expresses a negative sentiment toward Mitt Romney, from which one",
        "type": "Document"
      },
      {
        "id": "a6c974c0-1b78-486f-89bf-2406cebcc145",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1909.00694",
          "start_idx": 3871,
          "end_idx": 4400
        },
        "page_content": " events is much simpler and more intuitive. Some previous studies made use of document structure to understand the sentiment. BIBREF11 proposed a sentiment-specific pre-training strategy using unlabeled dialog data (tweet-reply pairs). BIBREF12 proposed a method of building a polarity-tagged corpus (ACP Corpus). They automatically gathered sentences that had positive or negative opinions utilizing HTML layout structures in addition to linguistic patterns. Our method depends only on raw texts and thus has wider applicability",
        "type": "Document"
      },
      {
        "id": "4eaa6a4a-7c51-4cbe-a43e-48fef5fd5473",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1911.12569",
          "start_idx": 923,
          "end_idx": 1418
        },
        "page_content": " perception, predict stock prices and model public opinions BIBREF1. Sentiment analysis BIBREF2 is an important area of research in natural language processing (NLP) where we automatically determine the sentiments (positive, negative, neutral). Emotion analysis focuses on the extraction of predefined emotion from documents. Discrete emotions BIBREF3, BIBREF4 are often classified into anger, anticipation, disgust, fear, joy, sadness, surprise and trust. Sentiments and emotions are subjective",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What performance did they obtain on the SemEval dataset?",
    "llm_answer": "Improved the existing best SemEval 2016 task 6 system by 3.2 F-score points for sentiment analysis.\n",
    "context": [
      {
        "id": "fceed6b1-b7ea-45e1-aa4f-55f1e50b1354",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1911.12569",
          "start_idx": 19389,
          "end_idx": 19799
        },
        "page_content": " features. Our system comfortably surpasses the existing best system at SemEval. Our system manages to improve the existing best system of SemEval 2016 task 6 by 3.2 F-score points for sentiment analysis. We also compare our system with the state-of-the-art systems proposed by BIBREF15 on the emotion dataset. The comparison is demonstrated in Table TABREF22. Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN were",
        "type": "Document"
      },
      {
        "id": "4da00c19-a6a0-4f7d-b63a-545b96be130e",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1911.12569",
          "start_idx": 18757,
          "end_idx": 19135
        },
        "page_content": " anticipation, fear, disgust, joy, sadness, surprise and trust are 0.000002, 0.000143, 0.00403, 0.000015, 0.004607, 0.069, 0.000001 and 0.000001, respectively. These results provide a good indication of statistical significance. Table TABREF19 shows the comparison of our proposed system with the existing state-of-the-art system of SemEval 2016 Task 6 for the sentiment dataset",
        "type": "Document"
      },
      {
        "id": "1e2f925a-9a0d-4318-af38-ee9ae9d2a426",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1912.13109",
          "start_idx": 7921,
          "end_idx": 8471
        },
        "page_content": " on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we",
        "type": "Document"
      },
      {
        "id": "5e0f3225-070e-4250-8f70-93c083311ec4",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "ae6816a3-03eb-4eb7-b090-5c0db38cbefa",
        "metadata": {
          "chunk_id": 91,
          "document_id": "1905.06566",
          "start_idx": 25555,
          "end_idx": 25950
        },
        "page_content": " comparison ( $p < 0.05$ ), which indicates our model still lags behind Human, but is better than all other systems. As mentioned earlier, our pre-training includes two stages. The first stage is the open-domain pre-training stage on the GIGA-CM dataset and the following stage is the in-domain pre-training on the CNNDM (or NYT50) dataset. As shown in Table 3 , we pretrained $\\text{\\sc Hibert}",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the state-of-the-art systems?",
    "llm_answer": "Deep convolutional networks, maximum entropy, SVM, LSTM, Bi-LSTM, and CNN.\n",
    "context": [
      {
        "id": "a4a4c58b-d1f1-46a1-9912-24f7a070ad68",
        "metadata": {
          "chunk_id": 43,
          "document_id": "2001.00137",
          "start_idx": 16781,
          "end_idx": 17197
        },
        "page_content": " the online platforms. Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector",
        "type": "Document"
      },
      {
        "id": "ed385b18-6171-42af-9672-281f1df69e11",
        "metadata": {
          "chunk_id": 53,
          "document_id": "1710.01492",
          "start_idx": 19472,
          "end_idx": 19941
        },
        "page_content": " particular, the best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks BIBREF53 , BIBREF54 . Conversely, kernel machines seem to be less frequently used than in the past, and the use of learning methods other than the ones mentioned above is at this point scarce. All these models are examples of supervised learning as they need labeled training data. Semi-supervised learning. We should note two things about the use of deep neural",
        "type": "Document"
      },
      {
        "id": "99db0986-6552-4425-8d58-52b78652feef",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1912.13109",
          "start_idx": 9357,
          "end_idx": 9891
        },
        "page_content": " Vo et al and Mathur et al. This was also used as a baseline model but we did not get appreciable performance on such architecture due to FC networks not being able to capture local and long term dependencies. Convolution based architectures: Architecture and hyperparameter choices were chosen from the past study Deon on the subject. We were able to boost the performance as compared to only FC based network but we noticed better performance from architectures that are suitable to sequences such as text messages or any timeseries",
        "type": "Document"
      },
      {
        "id": "fceed6b1-b7ea-45e1-aa4f-55f1e50b1354",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1911.12569",
          "start_idx": 19389,
          "end_idx": 19799
        },
        "page_content": " features. Our system comfortably surpasses the existing best system at SemEval. Our system manages to improve the existing best system of SemEval 2016 task 6 by 3.2 F-score points for sentiment analysis. We also compare our system with the state-of-the-art systems proposed by BIBREF15 on the emotion dataset. The comparison is demonstrated in Table TABREF22. Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN were",
        "type": "Document"
      },
      {
        "id": "be480e49-ccbc-40a3-8b15-f1c653ea37d2",
        "metadata": {
          "chunk_id": 11,
          "document_id": "2002.11893",
          "start_idx": 4186,
          "end_idx": 4668
        },
        "page_content": "ues, which need a ready dialogue system to converse with humans. The famous Dialogue State Tracking Challenges provided a set of human-to-machine dialogue data BIBREF17, BIBREF7. The performance of the dialogue system will largely influence the quality of dialogue data. The third one is machine-to-machine dialogues. It needs to build both user and system simulators to generate dialogue outlines, then use templates BIBREF3 to generate dialogues or further employ people to paraph",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the size of their collected dataset?",
    "llm_answer": "287,226 documents for training, 13,368 for validation, and 11,490 for test.\n",
    "context": [
      {
        "id": "2d3bdfec-4ed5-456d-bd82-023fad798858",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "9ccb7945-9a96-417a-b52e-68855c4910b1",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "95bf2b84-1a56-4eed-b193-4a34d2c76c4c",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1706.08032",
          "start_idx": 14264,
          "end_idx": 14661
        },
        "page_content": " datasets in Table III. for all datasets, the filter window size ( INLINEFORM0 ) is 7 with 6 feature maps each for the first wide convolution layer, the second wide convolution layer has a filter window size of 5 with 14 feature maps each. Dropout rate ( INLINEFORM1 ) is 0.5, INLINEFORM2 constraint, learning rate is 0.1 and momentum of 0.9. Mini-batch size for STS Corpus is 100 and others are 4",
        "type": "Document"
      },
      {
        "id": "001abd8b-7ba1-407b-8110-49e732f239a9",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1910.12618",
          "start_idx": 19089,
          "end_idx": 19565
        },
        "page_content": " maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014",
        "type": "Document"
      },
      {
        "id": "cc9faf95-ff74-4532-ad40-1937aafe18a1",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1707.03904",
          "start_idx": 12758,
          "end_idx": 13269
        },
        "page_content": " to evaluate the performance of the reading system independently from the search system, while the full set allows us to evaluate the performance of Quasar as a whole. We also split the full set into training, validation and test sets. The final size of each data subset after all discards is listed in Table 1 . Evaluation is straightforward on Quasar-S since each answer comes from a fixed output vocabulary of entities, and we report the average accuracy of predictions as the evaluation metric. For Quasar-T",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What kind of features are used by the HMM models, and how interpretable are those?",
    "llm_answer": "HMM models use spaces, indentation, and special characters (like comment symbols in Linux data).  The interpretability is described as \"some examples where the HMM and LSTM complement each other, such as learning different things about spaces and comments on Linux data, or punctuation on the Shakespeare data.\"\n",
    "context": [
      {
        "id": "038ff157-8a55-4ec7-89d4-5ae917e67d57",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1606.05320",
          "start_idx": 2248,
          "end_idx": 2783
        },
        "page_content": " quotes, brackets and other clearly identifiable aspects of the text. Building on these results, we take a somewhat more systematic approach to looking for interpretable hidden state dimensions, by using decision trees to predict individual hidden state dimensions (Figure 2 ). We visualize the overall dynamics of the hidden states by coloring the training data with the k-means clusters on the state vectors (Figures 3 , 3 ). We explore several methods for building interpretable models by combining LSTMs and HMMs. The existing body",
        "type": "Document"
      },
      {
        "id": "f47dc703-74e2-4e60-9216-a6bf6e68965c",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1606.05320",
          "start_idx": 8305,
          "end_idx": 8784
        },
        "page_content": "such as comment symbols in Linux data). We see some examples where the HMM and LSTM complement each other, such as learning different things about spaces and comments on Linux data, or punctuation on the Shakespeare data. In Figure 2 , we see that some individual LSTM hidden state dimensions identify similar features, such as comment symbols in the Linux data. Hybrid HMM-RNN approaches combine the interpretability of HMMs with the predictive power of RNNs. Sometimes, a small",
        "type": "Document"
      },
      {
        "id": "7532cb71-8027-493d-8e44-73b7dff410f9",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1909.08859",
          "start_idx": 4906,
          "end_idx": 5447
        },
        "page_content": " question and the multiple choice answers are composed of images. Input Module extracts vector representations of inputs at different levels of granularity by using several different encoders. Reasoning Module scans the procedural text and tracks the states of the entities and their relations through a recurrent relational memory core unit BIBREF5. Attention Module computes context-aware query vectors and query-aware context vectors as well as query-aware memory vectors. Modeling Module employs two multi-layered RNNs to encode previous",
        "type": "Document"
      },
      {
        "id": "e97db9b2-3790-4266-8648-bdbd03e2029b",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1606.05320",
          "start_idx": 8072,
          "end_idx": 8468
        },
        "page_content": "STM components. In Figures 3 and 3 , we color-code the training data with the 10 HMM states. In Figures 3 and 3 , we apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters. The HMM and LSTM states pick up on spaces, indentation, and special characters in the data (such as comment symbols in Linux data). We see some examples where the HMM and LSTM",
        "type": "Document"
      },
      {
        "id": "de327b15-172a-4668-a8be-8217c25727ec",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1606.05320",
          "start_idx": 5068,
          "end_idx": 5452
        },
        "page_content": "0, \\Sigma _i) \\text{IW}(\\Sigma _i) $  (Discrete HMM) Sample the emission parameters from a Multinomial-Dirichlet posterior. Evaluation: We evaluate the methods on how well they predict the next observation in the validation set. For the HMM models, we do a forward pass on the validation set (no backward pass unlike the full FFBS), and compute the HMM state distribution vector $p_t$",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What kind of information do the HMMs learn that the LSTMs don't?",
    "llm_answer": "HMMs learn information about spaces, comments, and punctuation.\n",
    "context": [
      {
        "id": "f47dc703-74e2-4e60-9216-a6bf6e68965c",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1606.05320",
          "start_idx": 8305,
          "end_idx": 8784
        },
        "page_content": "such as comment symbols in Linux data). We see some examples where the HMM and LSTM complement each other, such as learning different things about spaces and comments on Linux data, or punctuation on the Shakespeare data. In Figure 2 , we see that some individual LSTM hidden state dimensions identify similar features, such as comment symbols in the Linux data. Hybrid HMM-RNN approaches combine the interpretability of HMMs with the predictive power of RNNs. Sometimes, a small",
        "type": "Document"
      },
      {
        "id": "20ad0b2e-4636-4748-a22b-ccfba08f2b23",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1606.05320",
          "start_idx": 8784,
          "end_idx": 9098
        },
        "page_content": " combine the interpretability of HMMs with the predictive power of RNNs. Sometimes, a small hybrid model can perform better than a standalone LSTM of the same size. We use visualizations to show how the LSTM and HMM components of the hybrid algorithm complement each other in terms of features learned in the data.",
        "type": "Document"
      },
      {
        "id": "9a5f3f9a-f4c3-4d6e-be5a-658e07108f47",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1606.05320",
          "start_idx": 6884,
          "end_idx": 7353
        },
        "page_content": " HMM based on a PCA analysis of the LSTM states, as the first 20 components captured almost all the variance. Table 1 shows the predictive log likelihood of the next text character for each method. On all text data sets, the hybrid algorithm performs a bit better than the standalone LSTM with the same LSTM state dimension. This effect gets smaller as we increase the LSTM size and the HMM makes less difference to the prediction (though it can still make a difference",
        "type": "Document"
      },
      {
        "id": "cb3f758e-51c0-4eca-b8ae-dcd55a3191ea",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1606.05320",
          "start_idx": 3337,
          "end_idx": 3764
        },
        "page_content": " then make use of the information from the HMM, and fill in the gaps when the HMM is not performing well, resulting in an LSTM with a smaller number of hidden state dimensions that could be interpreted individually (Figures 3 , 3 ). We compare a hybrid HMM-LSTM approach with a continuous emission HMM (trained on the hidden states of a 2-layer LSTM), and a discrete emission HMM (trained directly on data). We use a character-",
        "type": "Document"
      },
      {
        "id": "2d77f86f-eedf-413b-b91a-183fe02613d5",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1606.05320",
          "start_idx": 5921,
          "end_idx": 6363
        },
        "page_content": " HMM on the data, outputting the hidden state distributions obtained by the HMM's forward pass, and then add this information to the architecture in parallel with a 1-layer LSTM. The linear layer between the LSTM and the prediction layer is augmented with an extra column for each HMM state. The LSTM component of this architecture can be smaller than a standalone LSTM, since it only needs to fill in the gaps in the HMM's predictions. The H",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How large is the gap in performance between the HMMs and the LSTMs?",
    "llm_answer": "The hybrid algorithm (combining HMM and LSTM) performs a bit better than a standalone LSTM with the same LSTM state dimension, but this difference diminishes as the LSTM size increases.  The HMM can still make a difference.\n",
    "context": [
      {
        "id": "9a5f3f9a-f4c3-4d6e-be5a-658e07108f47",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1606.05320",
          "start_idx": 6884,
          "end_idx": 7353
        },
        "page_content": " HMM based on a PCA analysis of the LSTM states, as the first 20 components captured almost all the variance. Table 1 shows the predictive log likelihood of the next text character for each method. On all text data sets, the hybrid algorithm performs a bit better than the standalone LSTM with the same LSTM state dimension. This effect gets smaller as we increase the LSTM size and the HMM makes less difference to the prediction (though it can still make a difference",
        "type": "Document"
      },
      {
        "id": "20ad0b2e-4636-4748-a22b-ccfba08f2b23",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1606.05320",
          "start_idx": 8784,
          "end_idx": 9098
        },
        "page_content": " combine the interpretability of HMMs with the predictive power of RNNs. Sometimes, a small hybrid model can perform better than a standalone LSTM of the same size. We use visualizations to show how the LSTM and HMM components of the hybrid algorithm complement each other in terms of features learned in the data.",
        "type": "Document"
      },
      {
        "id": "008c54bf-1dfa-4377-9cb3-e7ce711cb7d5",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1606.05320",
          "start_idx": 6195,
          "end_idx": 6640
        },
        "page_content": "M, since it only needs to fill in the gaps in the HMM's predictions. The HMM is written in Python, and the rest of the architecture is in Torch. We also build a joint hybrid model, where the LSTM and HMM are simultaneously trained in Torch. We implemented an HMM Torch module, optimized using stochastic gradient descent rather than FFBS. Similarly to the sequential hybrid model, we concatenate the LSTM outputs with the HMM state probabilities",
        "type": "Document"
      },
      {
        "id": "64e2c1fb-ce65-4bb0-8d93-750be17c175b",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1606.05320",
          "start_idx": 7659,
          "end_idx": 8072
        },
        "page_content": " sets. The joint hybrid is an order of magnitude slower than the sequential hybrid, as the SGD-based HMM is slower to train than the FFBS-based HMM. We interpret the HMM and LSTM states in the hybrid algorithm with 10 LSTM state dimensions and 10 HMM states in Figures 3 and 3 , showing which features are identified by the HMM and LSTM components. In Figures 3 and 3 , we color-code the training data with the 10",
        "type": "Document"
      },
      {
        "id": "8951670c-a290-4246-8bea-c038e66a5e74",
        "metadata": {
          "chunk_id": 62,
          "document_id": "1909.13375",
          "start_idx": 21089,
          "end_idx": 21507
        },
        "page_content": " available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1. All that, while keeping in mind we compare a base model to a large model (for reference, note the 8 point improvement between MTMSNbase and MTMSNlarge in both EM and F1). Our best model, large-squad, exhibits a huge improvement of 29.7 in EM and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what was their system's f1 performance?",
    "llm_answer": "78.38% on the whole development set.\n",
    "context": [
      {
        "id": "96ecbdb9-17ee-46e0-9f1c-9eef560fb526",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1912.13109",
          "start_idx": 10203,
          "end_idx": 10681
        },
        "page_content": " metrics - Recall rate, F1 score and Overall accuracy. Choice of model parameters were in the above models were inspired from previous work done but then were tuned to the best performance of the Test dataset. Following parameters were considered for tuning. Learning rate: Based on grid search the best performance was achieved when learning rate was set to 0.01. This value was arrived by a grid search on lr parameter. Number of Bidirectional LSTM units: A set of 32, 64, 128",
        "type": "Document"
      },
      {
        "id": "0ab3fdf3-8ea1-480c-8cac-92715fc9d565",
        "metadata": {
          "chunk_id": 52,
          "document_id": "1909.02764",
          "start_idx": 20420,
          "end_idx": 20841
        },
        "page_content": "=73 % on TEC. The model trained on our AMMER corpus achieves an F1 score of 57%. This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set. These results are mostly in line with Bostan2018. Now we analyze how well the models trained in Experiment 1 perform when applied to our data set. The results are shown in column \u201cSimple\u201d in Table TAB",
        "type": "Document"
      },
      {
        "id": "b09849dc-7c1a-4d5b-b18a-ed9e0b2c65af",
        "metadata": {
          "chunk_id": 100,
          "document_id": "1701.02877",
          "start_idx": 35714,
          "end_idx": 36084
        },
        "page_content": " higher at 76.42; the less diverse data is, on average, easier to tag. Interestingly, average F1 on seen NEs in the Twitter corpora (MSM and Ritter) is around 80, whereas average F1 on the ACE corpora, which are of similar size, is lower, at around 70. To summarise, our findings are: [noitemsep] F1 on unseen NEs is significantly lower than F1 on seen NEs for all three",
        "type": "Document"
      },
      {
        "id": "afb1b755-5b9b-4004-b49d-6c5e2eb9114b",
        "metadata": {
          "chunk_id": 88,
          "document_id": "1703.04617",
          "start_idx": 24013,
          "end_idx": 24428
        },
        "page_content": " question is the major class. Figure 5 shows the composition of F1 score. Take our best model as an example, we observed a 78.38% F1 score on the whole development set, which can be separated into two parts: one is where F1 score equals to 100%, which means an exact match. This part accounts for 69.10% of the entire development set. And the other part accounts for 30.90%, of which the average F1 score is 30.03%.",
        "type": "Document"
      },
      {
        "id": "1e057f3e-cf5a-49bb-9baf-a36b869560d8",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1603.07044",
          "start_idx": 14511,
          "end_idx": 14997
        },
        "page_content": " $0.2064$ . To further enhance the system, we incorporate a one hot vector of the original IR ranking as an additional feature into the FNN classifier. Table 3 shows the results. In comparing the models with and without augmented features, we can see large improvement for task B and C. The F1 score for task A degrades slightly but MAP improves. This might be because task A already had a substantial amount of training data. Table 4 gives the final comparison between different models",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much more coverage is in the new dataset?",
    "llm_answer": "Answer not found in the context.\n",
    "context": [
      {
        "id": "5e5c6963-88aa-4a86-96ff-323aeb271bfb",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1810.00663",
          "start_idx": 25415,
          "end_idx": 25983
        },
        "page_content": " In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models. The previous section evaluated model performance on new instructions (and corresponding navigation plans) for environments that were previously seen at training time. Here, we examine whether the trained models succeed on environments that are completely new. The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as",
        "type": "Document"
      },
      {
        "id": "b744e65c-657e-4027-a1ce-6f063f2f2d46",
        "metadata": {
          "chunk_id": 67,
          "document_id": "1909.13375",
          "start_idx": 22947,
          "end_idx": 23384
        },
        "page_content": " 3.5 points. Given that number questions make up about 61% of the dataset, we can deduce that our improved number handling is responsible for about a 2.1 point gain, while the rest could be be attributed to the improved Wikipedia parsing. Although NER span cleaning (Section SECREF23) affected only 3% of the multi-span questions, it provided a solid improvement of 5.4 EM in multi-span questions and 1.5 EM in single-span questions. The",
        "type": "Document"
      },
      {
        "id": "9d6e4c30-d037-4a07-a951-0e7d1761e751",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1912.13109",
          "start_idx": 7657,
          "end_idx": 8136
        },
        "page_content": " above few data augmentation techniques were applied to boost the learning of the network. Before applying augmentation, a train-test split of 78%-22% was done from the original, cleansed data set. Thus, 700 tweets/messages were held out for testing. All model evaluation were done in on the test set that got generated by this process. The results presented in this report are based on the performance of the model on the test set. The training set of 2489 messages were however",
        "type": "Document"
      },
      {
        "id": "8d8c69e7-bb1a-4f05-96ab-2b33b61d748d",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1610.00879",
          "start_idx": 7038,
          "end_idx": 7487
        },
        "page_content": "2%. This degradation implies that our features capture a subset of drunk tweets and that there are properties of drunk tweets that may be more subtle. When both N-gram and stylistic features are used, there is negligible improvement. The accuracy for Dataset 2 increases from 77.9% to 78.1%. Precision/Recall metrics do not change significantly either. The best accuracy of our classifier is 78.1% for all features, and 75.6% for stylistic features.",
        "type": "Document"
      },
      {
        "id": "c3706a8a-b105-48a8-befd-ab5d3cefe8cb",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1904.05862",
          "start_idx": 13422,
          "end_idx": 13926
        },
        "page_content": "training and the best accuracy is achieved when we use the largest amount of data for pre-training. In this section we analyze some of the design choices we made for . We pre-train on the 80 hour subset of clean Librispeech and evaluate on TIMIT. Table shows that increasing the number of negative samples only helps up to ten samples. Thereafter, performance plateaus while training time increases. We suspect that this is because the training signal from the positive samples decreases as the number of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How was quality measured?",
    "llm_answer": "Human evaluation, often by randomly sampling generated questions and asking annotators to rate them on a 5-point Likert scale, averaging ranks or percentages of best-ranked questions.  Expert and amateur human evaluators graded generated poems on fluency and semantic coherence.  Macro-averaging of precision, recall, and F1 scores was used for multilabel classification.\n",
    "context": [
      {
        "id": "d88a1149-0827-4f42-b9b1-620cfd250358",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1905.08949",
          "start_idx": 10602,
          "end_idx": 11043
        },
        "page_content": " they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks. As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE B",
        "type": "Document"
      },
      {
        "id": "7bf10c00-7441-406d-ba02-f7684f2cfab0",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1909.00578",
          "start_idx": 2193,
          "end_idx": 2625
        },
        "page_content": " without references. The scores of their model correlate well with Pyramid and Responsiveness, but text quality is only addressed indirectly. Quality Estimation is well established in MT BIBREF15, BIBREF0, BIBREF1, BIBREF16, BIBREF17. QE methods provide a quality indicator for translation output at run-time without relying on human references, typically needed by MT evaluation metrics BIBREF4, BIBREF18. QE models for MT make use",
        "type": "Document"
      },
      {
        "id": "e91734d8-8ad5-42af-b9bb-f6b51ee290df",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1809.05752",
          "start_idx": 17771,
          "end_idx": 18259
        },
        "page_content": " the paragraphs in our gold standard. To assess relative performance of feature representations, we also include performance metrics of our models without MWEs. Because this is a multilabel classification task we use macro-averaging to compute precision, recall, and F1 scores for each paragraph in the testing set. In identifying domains individually, our models achieved the highest per-domain scores on Substance (F1 INLINEFORM0 0.8) and the lowest scores on Interpersonal and Mood (F1",
        "type": "Document"
      },
      {
        "id": "c85b69e6-0bcc-433b-b0ae-ba235722e16f",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1909.00124",
          "start_idx": 12394,
          "end_idx": 12855
        },
        "page_content": " during training. Summary statistics of the training, validation, and test data are shown in Table TABREF9. Noisy-labeled Training Datasets. For the above three domains (movie, laptop, and restaurant), we collected 2,000 reviews for each domain from the same review source. We extracted sentences from each review and assigned review's label to its sentences. Like previous work, we treat 4 or 5 stars as positive and 1 or 2 stars as negative. The data is noisy",
        "type": "Document"
      },
      {
        "id": "efc3bcef-ff25-486a-ad34-fc2f8998a97d",
        "metadata": {
          "chunk_id": 41,
          "document_id": "1909.00279",
          "start_idx": 15265,
          "end_idx": 15755
        },
        "page_content": "ators to join our human evaluation. The human evaluators were divided into two groups. The expert group contains 15 people who hold a bachelor degree in Chinese literature, and the amateur group contains 15 people who holds a bachelor degree in other fields. All 30 human evaluators are native Chinese speakers. We ask evaluators to grade each generated poem from four perspectives: 1) Fluency: Is the generated poem grammatically and rhythmically well formed, 2) Semantic coherence: Is the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is different in the improved annotation protocol?",
    "llm_answer": "The improved annotation protocol includes a refined annotation schema, scenario-specific annotation templates, and single annotation mode for annotators.  Iterative refinement was necessary due to the complexity of the task.\n",
    "context": [
      {
        "id": "f84b0b36-4d69-46b7-ab40-db1d1c37787d",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1810.06743",
          "start_idx": 1487,
          "end_idx": 2017
        },
        "page_content": " each other. Further, we show similar downstream performance for each annotation scheme in the task of morphological tagging. This tool enables a synergistic use of UniMorph and Universal Dependencies, as well as teasing out the annotation discrepancies within and across projects. When one dataset disobeys its schema or disagrees with a related language, the flaws may not be noticed except by such a methodological dive into the resources. When the maintainers of the resources ameliorate these flaws, the resources move closer",
        "type": "Document"
      },
      {
        "id": "7ad84a74-dfae-4c36-99a3-0c3aa9c1ab59",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1703.05260",
          "start_idx": 17551,
          "end_idx": 18087
        },
        "page_content": " results were discussed with the authors. Throughout the whole annotation phase, annotators could discuss any emerging issues with the authors. All annotations were done by undergraduate students of computational linguistics. The annotation was rather time-consuming due to the complexity of the task, and thus we decided for single annotation mode. To assess annotation quality, a small sample of texts was annotated by all four annotators and their inter-annotator agreement was measured (see Section \"Inter-Annotator Agreement\" ). It",
        "type": "Document"
      },
      {
        "id": "9c29b30f-983c-4da5-9469-bc961c44c746",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1703.05260",
          "start_idx": 7578,
          "end_idx": 8108
        },
        "page_content": ". This section deals with the annotation of the data. We first describe the final annotation schema. Then, we describe the iterative process of corpus annotation and the refinement of the schema. This refinement was necessary due to the complexity of the annotation. For each of the scenarios, we designed a specific annotation template. A script template consists of scenario-specific event and participant labels. An example of a template is shown in Table 1 . All NP heads in the corpus were annotated with a participant label;",
        "type": "Document"
      },
      {
        "id": "397bdc32-660c-48d9-acea-aa11761c3cb2",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1911.03243",
          "start_idx": 12706,
          "end_idx": 13226
        },
        "page_content": "A-SRL. It enabled us to release a new gold standard for evaluations, notably of much higher coverage of core and implied roles than the previous Dense evaluation dataset. We believe that our annotation methodology and dataset would facilitate future research on natural semantic annotations and QA-SRL parsing. For completeness, we include several examples with some questions restructured into its 7 template slots in Table TABREF26 As described in section 3 The consolidator receives two sets of QA annotations and mer",
        "type": "Document"
      },
      {
        "id": "4a6e38ab-4a60-4f88-9b6b-8473ad5354e2",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1810.06743",
          "start_idx": 11922,
          "end_idx": 12457
        },
        "page_content": " door open to allow for such imperfections. With no strict guidance, it's natural that annotators would take different paths. Nevertheless, modulo annotator disagreement, we assume that within a particular corpus, one word form will always be consistently annotated. Three categories of annotation difficulty are missing values, language-specific attributes, and multiword expressions. In our work, the goal is not simply to translate one schema into the other, but to translate one resource (the imperfect manifestation of the schema)",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What data were they used to train the multilingual encoder?",
    "llm_answer": "The context states the multilingual encoder was trained on data from language pair BIBREF30.\n",
    "context": [
      {
        "id": "fc37bb94-aca9-4764-9da4-2827cc039bbe",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1809.04686",
          "start_idx": 13809,
          "end_idx": 14312
        },
        "page_content": " Finally, we perform an experiment where we freeze the encoder after initialization and only update the classifier component of the system. Table 1 summarizes the accuracy of our proposed system for these three different approaches and the state-of-the-art results on all the tasks. The first row in the table shows the baseline accuracy of our system for all four datasets. The second row shows the result from initializing with a pre-trained multilingual NMT encoder. It can be seen that this provides",
        "type": "Document"
      },
      {
        "id": "7f66ad33-3a97-4924-bc67-5f8f11eb0906",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1809.04686",
          "start_idx": 11259,
          "end_idx": 11672
        },
        "page_content": "rightarrow $ Fr and Fr $\\rightarrow $ En examples are interleaved. We picked the best model based on the best average development set BLEU score on both of the language pairs. The Encoder-Classifier model here uses the encoder defined previously. With regards to the classifier, the pre- and post-pooling networks ( $f_{pre}$ , $f_{post}$ ) are both one-layer feed forward networks to cast the dimension size from",
        "type": "Document"
      },
      {
        "id": "9dbb20fb-cc3c-4c3e-955d-d4232680e257",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1809.04686",
          "start_idx": 13320,
          "end_idx": 13809
        },
        "page_content": " tasks - Amazon Reviews (English and French), SST, and SNLI. For each task, we first build a baseline system using the proposed Encoder-Classifier architecture described in Section \"Proposed Method\" where the encoder is initialized randomly. Next, we experiment with using the pre-trained multilingual NMT encoder to initialize the system as described in Section \"Analyses\" . Finally, we perform an experiment where we freeze the encoder after initialization and only update the classifier",
        "type": "Document"
      },
      {
        "id": "77b711f8-b927-448f-aed1-ce6f5f84281f",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1809.04686",
          "start_idx": 3867,
          "end_idx": 4351
        },
        "page_content": " language BIBREF30 . After training such a multilingual NMT model, the decoder and the corresponding attention mechanisms (which are target-language specific) are discarded, while the multilingual encoder is used to initialize the encoder of our proposed Encoder-Classifier model. In order to leverage pre-trained multilingual representations introduced in Section \"Analyses\" , our encoder strictly follows the structure of a regular Recurrent Neural Network (RNN) based NMT encoder B",
        "type": "Document"
      },
      {
        "id": "789ad165-0f7a-4387-b8ae-21b5ad0ad4cc",
        "metadata": {
          "chunk_id": 40,
          "document_id": "1809.04686",
          "start_idx": 14114,
          "end_idx": 14616
        },
        "page_content": "izing with a pre-trained multilingual NMT encoder. It can be seen that this provides a significant improvement in accuracy, an average of 4.63%, across all the tasks. This illustrates that the multilingual NMT encoder has successfully learned transferable contextualized representations that are leveraged by the classifier component of our proposed system. These results are in line with the results in BIBREF5 where the authors used the representations from the top NMT encoder layer as an additional",
        "type": "Document"
      }
    ]
  },
  {
    "query": "From when are many VQA datasets collected?",
    "llm_answer": "2014\u20132016\n",
    "context": [
      {
        "id": "aaa6f920-b25e-42ae-9e34-b02ba711e173",
        "metadata": {
          "chunk_id": 1,
          "document_id": "1703.09684",
          "start_idx": 479,
          "end_idx": 906
        },
        "page_content": "A datasets must test a wide range of abilities for progress to be adequately measured. VQA research began in earnest in late 2014 when the DAQUAR dataset was released BIBREF0 . Including DAQUAR, six major VQA datasets have been released, and algorithms have rapidly improved. On the most popular dataset, `The VQA Dataset' BIBREF1 , the best algorithms are now approaching 70% accuracy BIBREF2 (human performance is 83%). While",
        "type": "Document"
      },
      {
        "id": "1ffa0a05-13fd-482b-95b8-a9e4d3b1f42f",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1703.09684",
          "start_idx": 2425,
          "end_idx": 2802
        },
        "page_content": "?', `Does the use of spatial attention help answer specific question-types?', `How successful are the VQA algorithms in answering less-common questions?', and 'Can the VQA algorithms differentiate between real and absurd questions?' Six datasets for VQA with natural images have been released between 2014\u20132016: DAQUAR BIBREF0 , COCO-QA BIBREF3 , FM-IQA BIBREF4 , The VQA Datas",
        "type": "Document"
      },
      {
        "id": "62317ffc-1e47-48f4-97a3-e6ebdf462308",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1703.09684",
          "start_idx": 8700,
          "end_idx": 9279
        },
        "page_content": " contains natural images and questions. To improve algorithm analysis and comparison, our dataset has more (12) explicitly defined question-types and new evaluation metrics. In the past two years, multiple publicly released datasets have spurred the VQA research. However, due to the biases and issues with evaluation metrics, interpreting and comparing the performance of VQA systems can be opaque. We propose a new benchmark dataset that explicitly assigns questions into 12 distinct categories. This enables measuring performance within each category and understand which kind",
        "type": "Document"
      },
      {
        "id": "a8a27d37-446d-4291-9a51-ef583b02d668",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1703.09684",
          "start_idx": 3024,
          "end_idx": 3354
        },
        "page_content": " , BIBREF8 , BIBREF9 , we refer to the portion of The VQA Dataset containing natural images as COCO-VQA. Detailed dataset reviews can be found in BIBREF10 and BIBREF11 . All of the aforementioned VQA datasets are biased. DAQUAR and COCO-QA are small and have a limited variety of question-types. Visual Genome, Visual7W, and COCO-",
        "type": "Document"
      },
      {
        "id": "740873a8-714a-4ee6-aa6f-7e1bf752c34b",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1703.09684",
          "start_idx": 19286,
          "end_idx": 19586
        },
        "page_content": " majority of VQA systems formulate it as a classification problem in which the system is given an image and a question, with the answers as categories. BIBREF1 , BIBREF3 , BIBREF2 , BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26 , BIBREF9 , BIBREF27 , BIBREF28 , BIBREF8 ,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is task success rate achieved? ",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "405e6db8-0ef4-406a-b26f-b659c4cebac1",
        "metadata": {
          "chunk_id": 90,
          "document_id": "2002.11893",
          "start_idx": 33632,
          "end_idx": 34069
        },
        "page_content": " are filled by real values, the simulator terminates. This is regarded as \"task finish\". It's worth noting that \"task finish\" does not mean the task is success, because the system may provide wrong information. We calculated \"task finish rate\" on 1000 times simulations for each goal type (See Table TABREF31). Findings are summarized below: Cross multi-domain tasks (CM and CM+T) are much harder to finish. Comparing M and M+T, although",
        "type": "Document"
      },
      {
        "id": "ef07453b-7ad9-4584-b0f6-8942514e2d2a",
        "metadata": {
          "chunk_id": 54,
          "document_id": "2002.11893",
          "start_idx": 20211,
          "end_idx": 20604
        },
        "page_content": " we generate independent multi-domain, cross multi-domain, and traffic domain sub-goals one by one. Thus in terms of the task complexity, we have S<M<CM and M<M+T<CM+T, which is supported by the average number of sub-goals, semantic tuples, and turns per dialogue in Table TABREF26. The average number of tokens also becomes larger when the goal becomes more complex. About 60% of dialogues (M",
        "type": "Document"
      },
      {
        "id": "3e1f56fc-4c1e-441a-b8f8-037d17734bfd",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1909.00754",
          "start_idx": 16549,
          "end_idx": 16980
        },
        "page_content": ".15x, while for an ITC of $O(n)$ , it will be a multiplier of 25.1, and 1,143 for $O(mn)$ . As a convention, the metric of joint goal accuracy is used to compare our model to previous work. The joint goal accuracy only regards the model making a successful belief state prediction if all of the slots and values predicted are exactly matched with the labels provided. This metric gives a strict measurement that tells how often the",
        "type": "Document"
      },
      {
        "id": "ec846c61-b035-431a-88d4-858e8c47d3be",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1603.07044",
          "start_idx": 9966,
          "end_idx": 10403
        },
        "page_content": " for task A, there are a total number of 26,700 question-comment pairs. For task B, there are 2,670 question-question pairs. For task C, there are 26,700 question-comment pairs. The test dataset includes 50 questions, 500 related questions and 5,000 comments which do not overlap with the training set. To evaluate the performance, we use mean average precision (MAP) and F1 score. Table 2 shows the initial results using the RNN encoder",
        "type": "Document"
      },
      {
        "id": "1b2a774a-b45c-41e9-91e6-599deb244649",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1807.03367",
          "start_idx": 9457,
          "end_idx": 9982
        },
        "page_content": " given to our workers before they started their task, are shown in Appendix SECREF15 . We paired Turkers at random and let them alternate between the tourist and guide role across different HITs. The Talk The Walk dataset consists of over 10k successful dialogues\u2014see Table FIGREF66 in the appendix for the dataset statistics split by neighborhood. Turkers successfully completed INLINEFORM0 of all finished tasks (we use this statistic as the human success rate). More than six hundred participants successfully completed at",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Does proposed end-to-end approach learn in reinforcement or supervised learning manner?",
    "llm_answer": "Reinforcement learning.\n",
    "context": [
      {
        "id": "e05520e1-5e3b-42a2-91f1-8af9becbc64b",
        "metadata": {
          "chunk_id": 103,
          "document_id": "1902.00330",
          "start_idx": 35109,
          "end_idx": 35645
        },
        "page_content": " of terms, Han et al. BIBREF41 designed an end-to-end reinforcement learning model to determine which term to select and where to place it on the taxonomy, which effectively reduced the error propagation between two phases. Inspired by the above works, we also add reinforcement learning to our framework. In this paper we consider entity linking as a sequence decision problem and present a reinforcement learning based model. Our model learns the policy on selecting target entities in a sequential manner and makes decisions based on",
        "type": "Document"
      },
      {
        "id": "ba819c5d-6b1f-43ac-a872-c1f8943ff687",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1911.11744",
          "start_idx": 1380,
          "end_idx": 1908
        },
        "page_content": "REF4, and human-robot interaction BIBREF5 an important question is how to incorporate language and vision into a differentiable end-to-end system for complex robot control. In this paper, we present an imitation learning approach that combines language, vision, and motion in order to synthesize natural language-conditioned control policies that have strong generalization capabilities while also capturing the semantics of the task. We argue that such a multi-modal teaching approach enables robots to acquire complex policies",
        "type": "Document"
      },
      {
        "id": "2fb47fd4-1060-4958-9c5c-455c954df04f",
        "metadata": {
          "chunk_id": 76,
          "document_id": "1810.00663",
          "start_idx": 29565,
          "end_idx": 30130
        },
        "page_content": " applications, including information retrieval. We proposed an end-to-end system to translate user instructions to a high-level navigation plan. Our model utilized an attention mechanism to merge relevant information from the navigation instructions with a behavioral graph of the environment. The model then used a decoder to predict a sequence of navigation behaviors that matched the input commands. As part of this effort, we contributed a new dataset of 11,051 pairs of user instructions and navigation plans from 100 different environments. Our model achieved",
        "type": "Document"
      },
      {
        "id": "1b6ac9fc-809d-487e-8c2f-2e4fdf748d00",
        "metadata": {
          "chunk_id": 75,
          "document_id": "1810.00663",
          "start_idx": 28993,
          "end_idx": 29565
        },
        "page_content": " the model is indeed using the input instructions and is not just approximating shortest paths in the behavioral graph. Other examples on the prediction of sub-obtimal paths are described in the Appendix. This work introduced behavioral navigation through free-form natural language instructions as a challenging and a novel task that falls at the intersection of natural language processing and robotics. This problem has a range of interesting cross-domain applications, including information retrieval. We proposed an end-to-end system to translate user instructions to",
        "type": "Document"
      },
      {
        "id": "413bd7c3-9f2b-4bd5-8c48-fea8f8176948",
        "metadata": {
          "chunk_id": 101,
          "document_id": "1902.00330",
          "start_idx": 34293,
          "end_idx": 34753
        },
        "page_content": " state to help disambiguation. In the last few years, reinforcement learning has emerged as a powerful tool for solving complex sequential decision-making problems. It is well known for its great success in the game field, such as Go BIBREF37 and Atari games BIBREF38 . Recently, reinforcement learning has also been successfully applied to many natural language processing tasks and achieved good performance BIBREF12 , BIBREF39 , BIBREF5 . Feng et al. BIBREF",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is performance of this system measured?",
    "llm_answer": "Performance is measured using EM-score, macro-averaged F$_1$-score, normalized accuracy, and human evaluation (average rank or percentage of best-ranked questions).  Recall, F1 score, and overall accuracy are also used.\n",
    "context": [
      {
        "id": "4f74c00a-d3e2-4fd1-995d-9267d8b8483c",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1703.09684",
          "start_idx": 18525,
          "end_idx": 19045
        },
        "page_content": " all question-types and is skewed towards lowest performing categories. We also use normalized metrics that compensate for bias in the form of imbalance in the distribution of answers within each question-type, e.g., the most repeated answer `two' covers over 35% of all the counting-type questions. To do this, we compute the accuracy for each unique answer separately within a question-type and then average them together for the question-type. To compute overall performance, we compute the arithmetic normalized mean",
        "type": "Document"
      },
      {
        "id": "e929c104-c4ce-40d5-aca7-762538286335",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1908.06606",
          "start_idx": 15988,
          "end_idx": 16383
        },
        "page_content": ") and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage",
        "type": "Document"
      },
      {
        "id": "6fe40bf4-6eaa-45b0-9c82-e51d07a9d182",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1811.00942",
          "start_idx": 9060,
          "end_idx": 9476
        },
        "page_content": " same frameworks for evaluation. We plugged the RPi into a Watts Up Pro meter, a power meter that can be read programatically over USB at a frequency of 1 Hz. For the QRNNs, we used the first 350 words of the test set, and averaged the ms/query and mJ/query. For KN-5, we used the entire test set for evaluation, since the latency was much lower. To adjust for the base power load, we subtracted idle power draw from",
        "type": "Document"
      },
      {
        "id": "d88a1149-0827-4f42-b9b1-620cfd250358",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1905.08949",
          "start_idx": 10602,
          "end_idx": 11043
        },
        "page_content": " they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks. As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE B",
        "type": "Document"
      },
      {
        "id": "96ecbdb9-17ee-46e0-9f1c-9eef560fb526",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1912.13109",
          "start_idx": 10203,
          "end_idx": 10681
        },
        "page_content": " metrics - Recall rate, F1 score and Overall accuracy. Choice of model parameters were in the above models were inspired from previous work done but then were tuned to the best performance of the Test dataset. Following parameters were considered for tuning. Learning rate: Based on grid search the best performance was achieved when learning rate was set to 0.01. This value was arrived by a grid search on lr parameter. Number of Bidirectional LSTM units: A set of 32, 64, 128",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big dataset is used for training this system?",
    "llm_answer": "The context mentions various datasets with different sizes.  No single \"training dataset\" size is given for the system as a whole.\n",
    "context": [
      {
        "id": "dc948dec-3a23-4196-b1ee-c386b0464b4d",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1909.13375",
          "start_idx": 20052,
          "end_idx": 20444
        },
        "page_content": "s. For the large models we used a batch size of 3 with a learning rate of $5e^{-6}$ and trained for 5 epochs, except for the model without the single-span heads that was trained with a batch size of 2 for 7 epochs. F1 was used as our validation metric. All models were trained on a single GPU with 12-16GB of memory. Table TABREF24 shows the results on DROP's development set. Compared to our",
        "type": "Document"
      },
      {
        "id": "001abd8b-7ba1-407b-8110-49e732f239a9",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1910.12618",
          "start_idx": 19089,
          "end_idx": 19565
        },
        "page_content": " maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014",
        "type": "Document"
      },
      {
        "id": "1e2f925a-9a0d-4318-af38-ee9ae9d2a426",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1912.13109",
          "start_idx": 7921,
          "end_idx": 8471
        },
        "page_content": " on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we",
        "type": "Document"
      },
      {
        "id": "fab192c8-d4f5-41fd-8b79-fb5fc40590b4",
        "metadata": {
          "chunk_id": 67,
          "document_id": "1905.06566",
          "start_idx": 18293,
          "end_idx": 18730
        },
        "page_content": "DM, NYT50 and GIGA-CM are all segmented and tokenized using Stanford CoreNLP toolkit BIBREF40 . To reduce the vocabulary size, we applied byte pair encoding (BPE; BIBREF41 ) to all of our datasets. To limit the memory consumption during training, we limit the length of each sentence to be 50 words (51th word and onwards are removed) and split documents with more than 30 sentences into smaller documents with each containing at most 30",
        "type": "Document"
      },
      {
        "id": "2d3bdfec-4ed5-456d-bd82-023fad798858",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they obtain word lattices from words?",
    "llm_answer": "For a sentence in Chinese, all possible substrings that can be considered as words are treated as vertices.  Neighboring words are connected by directed edges.  Out-of-vocabulary characters are replaced with labels to maintain graph connectivity.\n",
    "context": [
      {
        "id": "cc806c7f-cf17-4b62-8a5c-05ad635c4c8f",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1902.09087",
          "start_idx": 8933,
          "end_idx": 9429
        },
        "page_content": ". The inherent graph structure of a word lattice allows all possible words represented explicitly, no matter the overlapping and nesting cases, and all of them can contribute directly to the sentence representations. As we mentioned in previous section, we can not directly apply standard CNNs to take word lattice as input, since there could be multiple feature vectors produced for a given word. Inspired by previous lattice LSTM models BIBREF10 , BIBREF11 , here we propose a lattice based CNN",
        "type": "Document"
      },
      {
        "id": "12645b65-30cd-45e1-b82f-0ca6882dea3f",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1902.09087",
          "start_idx": 7413,
          "end_idx": 7832
        },
        "page_content": "\u201d. As shown in Figure FIGREF4 , a word lattice is a directed graph INLINEFORM0 , where INLINEFORM1 represents a node set and INLINEFORM2 represents a edge set. For a sentence in Chinese, which is a sequence of Chinese characters INLINEFORM3 , all of its possible substrings that can be considered as words are treated as vertexes, i.e. INLINEFORM4 . Then, all neighbor words are connected by directed edges according to",
        "type": "Document"
      },
      {
        "id": "977db7d9-ce0c-489b-9c0d-dcc00b831e4b",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1902.09087",
          "start_idx": 8403,
          "end_idx": 8933
        },
        "page_content": " be disconnected because of a few out-of-vocabulary characters. Thus, we append INLINEFORM0 labels to replace those characters to connect the graph. Obviously, word lattices are collections of characters and all possible words. Therefore, it is not necessary to make explicit decisions regarding specific word segmentations, but just embed all possible information into the lattice and take them to the next CNN layers. The inherent graph structure of a word lattice allows all possible words represented explicitly, no matter the",
        "type": "Document"
      },
      {
        "id": "4b9ff3f6-ed2d-4b5f-bbb9-91da749434c7",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1902.09087",
          "start_idx": 6782,
          "end_idx": 7248
        },
        "page_content": " each word may have different context words in different granularities in a lattice and may be treated as the center in various kernel spans with same length. Therefore, different from the original CNNs, there could be several feature vectors produced for a given word, which is the key challenge to apply the standard CNNs directly to a lattice input. For the example shown in Figure FIGREF6 , the word \u201ccitizen\u201d is the center word of four text spans with length 3:",
        "type": "Document"
      },
      {
        "id": "b674ccbf-dcdf-445d-a229-0056217a385a",
        "metadata": {
          "chunk_id": 68,
          "document_id": "1902.09087",
          "start_idx": 25087,
          "end_idx": 25615
        },
        "page_content": " construct from various word segmentations with different strategies to investigate the balance between the noisy words and additional information introduced by word lattice. We only use the DBQA dataset because word lattices here are more complex, so the construction strategies have more influence. Pilot experiments show that word lattices constructed based on character sequence perform better, so the strategies in Table TABREF33 are based on CNN-char. From Table TABREF33 , it is shown that all kinds of lattice are better",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How better is proposed method than baselines perpexity wise?",
    "llm_answer": "Answer not found in the context.\n",
    "context": [
      {
        "id": "d4db216e-684b-499d-9a56-c04add953940",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1603.07044",
          "start_idx": 12277,
          "end_idx": 12632
        },
        "page_content": ": for example, we could only use the parameter for embedding layers etc. For task B, the SNLI yields a slight improvement on MAP ( $0.2\\%$ ), and Task A could give ( $1.2\\%$ ) on top of that. No improvement was observed on F1. For task C, pretraining by task A is also better than using SNLI (task A is $1\\%$ better than the baseline, while SNLI is almost",
        "type": "Document"
      },
      {
        "id": "487566c1-6b22-466e-8847-fa293a6f80e8",
        "metadata": {
          "chunk_id": 83,
          "document_id": "1905.06566",
          "start_idx": 23289,
          "end_idx": 23633
        },
        "page_content": " by 0.5 ROUGE. We also implemented two baselines. One is the hierarchical transformer summarization model (HeriTransfomer; described in \"Extractive Summarization\" ) without pre-training. Note the setting for HeriTransfomer is ( $L=4$ , $H=300$ and $A=4$ ) . We can see that the pre-training (details in Section \"Pre-training\" ) leads to a +1.25",
        "type": "Document"
      },
      {
        "id": "abc51b19-3db2-4cbf-ac46-ff97aa2fa1d1",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1603.07044",
          "start_idx": 14997,
          "end_idx": 15515
        },
        "page_content": " task A already had a substantial amount of training data. Table 4 gives the final comparison between different models (we only list the MAP score because it is the official score for the challenge). Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited",
        "type": "Document"
      },
      {
        "id": "8a361ea8-4617-48e6-b345-07e7d0d1ca4c",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1806.00722",
          "start_idx": 18032,
          "end_idx": 18359
        },
        "page_content": " marked in boldface. In almost all genres, DenseNMT models are significantly better than the baselines. With embedding size 256, where all models achieve their best scores, DenseNMT outperforms baselines by 0.7-1.0 BLEU on De-En, 0.5-1.3 BLEU on Tr-En, 0.8-1.5 BLEU on Tr-En-morph. We observe significant gain using other embed",
        "type": "Document"
      },
      {
        "id": "c9d62915-21d1-42b6-a16c-8d9d28de6e0c",
        "metadata": {
          "chunk_id": 91,
          "document_id": "1902.06843",
          "start_idx": 35139,
          "end_idx": 35416
        },
        "page_content": "s by -0.27. Baselines: To test the efficacy of our multi-modal framework for detecting depressed users, we compare it against existing content, content-network, and image-based models (based on the aforementioned general image feature, facial presence, and facial expressions.)",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Who manually annotated the semantic roles for the set of learner texts?",
    "llm_answer": "Two senior students.\n",
    "context": [
      {
        "id": "2c8abbda-0126-491c-8e76-4597252bf66d",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1808.09409",
          "start_idx": 24257,
          "end_idx": 24791
        },
        "page_content": ", we use additional 156,520 sentences to enhance the Berkeley parser. Statistical models of annotating learner texts are making rapid progress. Although there have been some initial studies on defining annotation specification as well as corpora for syntactic analysis, there is almost no work on semantic parsing for interlanguages. This paper discusses this topic, taking Semantic Role Labeling as a case task and learner Chinese as a case language. We reveal three unknown facts that are important towards a deeper analysis of lear",
        "type": "Document"
      },
      {
        "id": "dbbd99a7-327c-4995-9b8e-260be73043e2",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1808.09409",
          "start_idx": 8985,
          "end_idx": 9407
        },
        "page_content": ". As for role labels, we mainly apply representations defined by CPB BIBREF1 . The predicate in a sentence was labeled as rel, the core semantic roles were labeled as AN and the adjuncts were labeled as AM. For inter-annotator agreement, we evaluate the precision (P), recall (R), and F1-score (F) of the semantic labels given by the two annotators. Table TABREF5 shows that our inter-annotator agreement is promising. All",
        "type": "Document"
      },
      {
        "id": "85706cfe-2a4d-4d02-b087-8edf87b7dd35",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1808.09409",
          "start_idx": 6851,
          "end_idx": 7401
        },
        "page_content": " sentence pairs. We take the mother languages of the learners into consideration, which have a great impact on grammatical errors and hence automatic semantic analysis. We hope that four selected mother tongues guarantee a good coverage of typologies. The annotated corpus can be used both for linguistic investigation and as test data for NLP systems. Semantic role labeling (SRL) is the process of assigning semantic roles to constituents or their head words in a sentence according to their relationship to the predicates expressed in the sentence",
        "type": "Document"
      },
      {
        "id": "bcd298e5-e82c-43e8-9e40-120e4edf1703",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1808.09409",
          "start_idx": 7401,
          "end_idx": 7887
        },
        "page_content": " constituents or their head words in a sentence according to their relationship to the predicates expressed in the sentence. Typical semantic roles can be divided into core arguments and adjuncts. The core arguments include Agent, Patient, Source, Goal, etc, while the adjuncts include Location, Time, Manner, Cause, etc. To create a standard semantic-role-labeled corpus for learner Chinese, we first annotate a 50-sentence trial set for each native language. Two senior students major",
        "type": "Document"
      },
      {
        "id": "1dcb227c-73bf-4259-b38f-60c35e162e1c",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1808.09409",
          "start_idx": 8494,
          "end_idx": 8985
        },
        "page_content": " already fully guided by the existing annotation guidelines, we worked to articulate an extension to the guidelines that would support the decision. During the annotation, the annotators apply both position labels and semantic role labels. Position labels include S, B, I and E, which are used to mark whether the word is an argument by itself, or at the beginning or in the middle or at the end of a argument. As for role labels, we mainly apply representations defined by CPB BIBREF1 . The",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they obtain region descriptions and object annotations?",
    "llm_answer": "The context states they use the Visual Genome dataset.\n",
    "context": [
      {
        "id": "11b48343-06ef-4844-bcf6-b0766edf7fcd",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1808.00265",
          "start_idx": 14833,
          "end_idx": 15411
        },
        "page_content": " of visual grounding, these region and object annotations provide complementary information. As an example, as shown in Figure FIGREF3 , for questions related to interaction between objects, region annotations result highly relevant. In contrast, for questions related to properties of specific objects, object annotations result more valuable. Consequently, in this section we present a method to automatically select region and object annotations from VG that can be used as labels to implement visual grounding as an auxiliary task for VQA. For region annotations, we propose",
        "type": "Document"
      },
      {
        "id": "9c29b30f-983c-4da5-9469-bc961c44c746",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1703.05260",
          "start_idx": 7578,
          "end_idx": 8108
        },
        "page_content": ". This section deals with the annotation of the data. We first describe the final annotation schema. Then, we describe the iterative process of corpus annotation and the refinement of the schema. This refinement was necessary due to the complexity of the annotation. For each of the scenarios, we designed a specific annotation template. A script template consists of scenario-specific event and participant labels. An example of a template is shown in Table 1 . All NP heads in the corpus were annotated with a participant label;",
        "type": "Document"
      },
      {
        "id": "018579d7-bb1e-4dcc-bac7-40926c3fec5a",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1808.00265",
          "start_idx": 15190,
          "end_idx": 15648
        },
        "page_content": " labels to implement visual grounding as an auxiliary task for VQA. For region annotations, we propose a simple heuristic to mine visual groundings: for each INLINEFORM0 we enumerate all the region descriptions of INLINEFORM1 and pick the description INLINEFORM2 that has the most (at least two) overlapped informative words with INLINEFORM3 and INLINEFORM4 . Informative words are all nouns and verbs, where two informative words are matched if at least one",
        "type": "Document"
      },
      {
        "id": "673c2b53-bbf5-4261-9163-0087dee6cb8d",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1808.00265",
          "start_idx": 4054,
          "end_idx": 4642
        },
        "page_content": " Genome dataset, and using these to automatically construct attention maps that can be used for attention supervision, instead of requiring human annotators to manually provide grounding labels. Our framework achieves competitive state-of-the-art VQA performance, while generating visual groundings that outperform other algorithms that use human annotated attention during training. The contributions of this paper are: (1) we introduce a mechanism to automatically obtain meaningful attention supervision from both region descriptions and object annotations in the Visual Genome dataset",
        "type": "Document"
      },
      {
        "id": "06d540e1-958c-4f24-91a1-577a4ec09c1d",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1808.00265",
          "start_idx": 4375,
          "end_idx": 4918
        },
        "page_content": " mechanism to automatically obtain meaningful attention supervision from both region descriptions and object annotations in the Visual Genome dataset; (2) we show that by using the prediction of region and object label attention maps as auxiliary tasks in a VQA application, it is possible to obtain more interpretable intermediate representations. (3) we experimentally demonstrate state-of-the-art performances in VQA benchmarks as well as visual grounding that closely matches human attention annotations. Since its introduction BIBREF0 , B",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which training dataset allowed for the best generalization to benchmark sets?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "44c61d15-ebf2-4135-898b-8349401e4a05",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1812.06705",
          "start_idx": 15717,
          "end_idx": 16178
        },
        "page_content": "-trained one. Six benchmark classification datasets are listed in table 1 . Following Kim BIBREF24 , for a dataset without validation data, we use 10% of its training set for the validation set. Summary statistics of six classification datasets are shown in table 1. SST BIBREF25 SST (Stanford Sentiment Treebank) is a dataset for sentiment classification on movie reviews, which are annotated with five labels (SST5: very positive, positive, neutral, negative,",
        "type": "Document"
      },
      {
        "id": "7d4af53c-0878-4ce8-8945-c54db6f33d53",
        "metadata": {
          "chunk_id": 147,
          "document_id": "1910.09399",
          "start_idx": 57115,
          "end_idx": 57512
        },
        "page_content": " MSCOCO, and Oxford-102 datasets, which facilitated the work on text-to-image synthesis for several papers released more recently. While most deep learning algorithms use MNIST BIBREF74 dataset as the benchmark, there are three main datasets that are commonly used for evaluation of proposed GAN models for text-to-image synthesis: CUB BIBREF75, Oxford BIBREF76, COCO BIBREF77, and CIFAR-10 BIBREF",
        "type": "Document"
      },
      {
        "id": "3daf8d99-7f70-48bf-aa35-4433dbc002ea",
        "metadata": {
          "chunk_id": 71,
          "document_id": "1908.06606",
          "start_idx": 25208,
          "end_idx": 25664
        },
        "page_content": " for improving the performance on most of the task-specific datasets. In conclusion, the best way to achieve the best performance for a specific dataset is to pre-train the model in multiple datasets and then fine tune it on the specific dataset. We would like to thank Ting Li and Xizhou Hong (Ruijin Hospital) who have helped us very much in data fetching and data cleansing. This work is supported by the National Key R&D Program of China for \u201cPrecision",
        "type": "Document"
      },
      {
        "id": "637d4fe3-c7c1-4e7c-a855-261351957759",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1701.02877",
          "start_idx": 4315,
          "end_idx": 4755
        },
        "page_content": " NER approaches, namely Stanford NER BIBREF34 , SENNA BIBREF35 and CRFSuite BIBREF36 (see Section \"NER Models and Features\" for details). Since the goal of this study is to compare NER performance on corpora from diverse domains and genres, seven benchmark NER corpora are included, spanning newswire, broadcast conversation, Web content, and social media (see Table 1 for details). These datasets were chosen such that they have been annot",
        "type": "Document"
      },
      {
        "id": "750cf7d4-eaf2-41cf-a5fe-3fdf7b919809",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1810.00663",
          "start_idx": 17810,
          "end_idx": 18361
        },
        "page_content": " . This dataset was created using Amazon Mechanical Turk and 100 maps of simulated indoor environments, each with 6 to 65 rooms. To the best of our knowledge, this is the first benchmark for comparing translation models in the context of behavioral robot navigation. As shown in Table TABREF16 , the dataset consists of 8066 pairs of free-form natural language instructions and navigation plans for training. This training data was collected from 88 unique simulated environments, totaling 6064 distinct navigation plans (2002 plans have two different",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which models were compared?",
    "llm_answer": "\"Ours with Mask and Ordered Triplets,\" \"Baseline,\" and \"Ablation\" models.  Also, \"MEED,\" and other unspecified models are mentioned in relation to perplexity scores.  Finally, various RF models, an aggregated forecaster (Agg), and a strong feature-rich system are mentioned.\n",
    "context": [
      {
        "id": "d7a0b7aa-da14-4112-80c0-cc26c62fe83f",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "5e5c6963-88aa-4a86-96ff-323aeb271bfb",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1810.00663",
          "start_idx": 25415,
          "end_idx": 25983
        },
        "page_content": " In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models. The previous section evaluated model performance on new instructions (and corresponding navigation plans) for environments that were previously seen at training time. Here, we examine whether the trained models succeed on environments that are completely new. The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as",
        "type": "Document"
      },
      {
        "id": "f4e21ae9-bea0-4fc9-84f7-861bf90ec947",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1908.07816",
          "start_idx": 20270,
          "end_idx": 20712
        },
        "page_content": " the perplexity scores obtained by the three models on the two validation sets and the test set. As shown in the table, MEED achieves the lowest perplexity score on all three sets. We also conducted t-test on the perplexity obtained, and results show significant improvements (with $p$-value $<0.05$). Table TABREF34, TABREF35 and TABREF35 summarize the human evaluation results on the responses' grammatical correctness, contextual coherence",
        "type": "Document"
      },
      {
        "id": "abc51b19-3db2-4cbf-ac46-ff97aa2fa1d1",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1603.07044",
          "start_idx": 14997,
          "end_idx": 15515
        },
        "page_content": " task A already had a substantial amount of training data. Table 4 gives the final comparison between different models (we only list the MAP score because it is the official score for the challenge). Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited",
        "type": "Document"
      },
      {
        "id": "47401a98-3774-4cb1-816d-255b6b9652b2",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1910.12618",
          "start_idx": 24043,
          "end_idx": 24512
        },
        "page_content": " the gradient descent in the NNs for instance. In order to tone it down and to increase the consistency of our results, the different models are run $B=10$ times. The results presented hereafter correspond to the average and standard-deviation on those runs. The RF model denoted as \"sel\" is the one with the reduced number of features, whereas the other RF uses the full vocabulary. We also considered an aggregated forecaster (abridged Agg), consisting of the average",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is private dashboard?",
    "llm_answer": "Answer not found in the context.\n",
    "context": [
      {
        "id": "0ad8ad6b-fb77-48bc-87bb-39a10cea1a8e",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1909.02764",
          "start_idx": 10937,
          "end_idx": 11442
        },
        "page_content": " in this paper) and questionnaires. Two RGB cameras are fixed in the vehicle to capture the drivers face, one at the sun shield above the drivers seat and one in the middle of the dashboard. A microphone is placed on the center console. One experimenter sits next to the driver, the other behind the simulator. The virtual agent accompanying the drive is realized as Wizard-of-Oz prototype which enables the experimenter to manually trigger prerecorded voice samples playing trough the in-car speakers and",
        "type": "Document"
      },
      {
        "id": "e4aeeeae-a2cb-47dd-96a0-9a153d683582",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1909.02764",
          "start_idx": 12958,
          "end_idx": 13486
        },
        "page_content": " drive, where participants are asked to describe their mood and thoughts regarding the (upcoming) drive. This results in five interactions between the driver and the virtual agent. Furthermore, the co-driver asks three different questions during sessions with light traffic and low cognitive demand (Driver-Co-Driver Interactions, D\u2013Co). These questions are more general and non-traffic-related and aim at triggering the participants' memory and fantasy. Participants are asked to describe their last vacation, their dream house",
        "type": "Document"
      },
      {
        "id": "a0f38a6f-ce06-4183-a582-3b2cc572b2f8",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1901.04899",
          "start_idx": 1293,
          "end_idx": 1671
        },
        "page_content": " identified and annotated as: Set/Change Destination, Set/Change Route (including turn-by-turn instructions), Go Faster, Go Slower, Stop, Park, Pull Over, Drop Off, Open Door, and Other (turn music/radio on/off, open/close window/trunk, change AC/temp, show map, etc.). Relevant slots are identified and annotated as: Location, Position/Direction, Object, Time-Guidance, Person,",
        "type": "Document"
      },
      {
        "id": "f527b28a-1125-497f-9778-c49271016bd3",
        "metadata": {
          "chunk_id": 37,
          "document_id": "2002.11893",
          "start_idx": 13977,
          "end_idx": 14497
        },
        "page_content": " online. On the website, workers are free to choose one of the two roles: tourist (user) or system (wizard). Then, two paired workers are sent to a chatroom. The user needs to accomplish the allocated goal through conversation while the wizard searches the database to provide the necessary information and gives responses. Before the formal data collection, we trained the workers to complete a small number of dialogues by giving them feedback. Finally, 90 well-trained workers are participating in the data collection",
        "type": "Document"
      },
      {
        "id": "0d85e30c-f372-4817-a25a-c0324e724fa5",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1901.04899",
          "start_idx": 0,
          "end_idx": 573
        },
        "page_content": "Understanding passenger intents and extracting relevant slots are important building blocks towards developing a contextual dialogue system responsible for handling certain vehicle-passenger interactions in autonomous vehicles (AV). When the passengers give instructions to AMIE (Automated-vehicle Multimodal In-cabin Experience), the agent should parse such commands properly and trigger the appropriate functionality of the AV system. In our AMIE scenarios, we describe usages and support various natural commands for interacting with the vehicle. We collected a multimod",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is public dashboard?",
    "llm_answer": "Answer not found in the context.\n",
    "context": [
      {
        "id": "f527b28a-1125-497f-9778-c49271016bd3",
        "metadata": {
          "chunk_id": 37,
          "document_id": "2002.11893",
          "start_idx": 13977,
          "end_idx": 14497
        },
        "page_content": " online. On the website, workers are free to choose one of the two roles: tourist (user) or system (wizard). Then, two paired workers are sent to a chatroom. The user needs to accomplish the allocated goal through conversation while the wizard searches the database to provide the necessary information and gives responses. Before the formal data collection, we trained the workers to complete a small number of dialogues by giving them feedback. Finally, 90 well-trained workers are participating in the data collection",
        "type": "Document"
      },
      {
        "id": "0ad8ad6b-fb77-48bc-87bb-39a10cea1a8e",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1909.02764",
          "start_idx": 10937,
          "end_idx": 11442
        },
        "page_content": " in this paper) and questionnaires. Two RGB cameras are fixed in the vehicle to capture the drivers face, one at the sun shield above the drivers seat and one in the middle of the dashboard. A microphone is placed on the center console. One experimenter sits next to the driver, the other behind the simulator. The virtual agent accompanying the drive is realized as Wizard-of-Oz prototype which enables the experimenter to manually trigger prerecorded voice samples playing trough the in-car speakers and",
        "type": "Document"
      },
      {
        "id": "a0f38a6f-ce06-4183-a582-3b2cc572b2f8",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1901.04899",
          "start_idx": 1293,
          "end_idx": 1671
        },
        "page_content": " identified and annotated as: Set/Change Destination, Set/Change Route (including turn-by-turn instructions), Go Faster, Go Slower, Stop, Park, Pull Over, Drop Off, Open Door, and Other (turn music/radio on/off, open/close window/trunk, change AC/temp, show map, etc.). Relevant slots are identified and annotated as: Location, Position/Direction, Object, Time-Guidance, Person,",
        "type": "Document"
      },
      {
        "id": "5139df10-2ee3-4f4e-b1ee-e46963eed339",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1910.02789",
          "start_idx": 24164,
          "end_idx": 24628
        },
        "page_content": "\u0144 University of Technology. It is based on \"ZDoom\" game executable, and includes a Python based API. The API offers the user the ability to run game instances, query the game state, and execute actions. The original purpose of VizDoom is to provide a research platform for vision based reinforcement learning. Thus, a natural language representation for the game was needed to be implemented. ViZDoom emulates the \"Doom\" game and enables us to access data within a",
        "type": "Document"
      },
      {
        "id": "d6205032-bd1d-4319-84cd-6d91c96ecd2f",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1807.03367",
          "start_idx": 17057,
          "end_idx": 17590
        },
        "page_content": "FORM6 taken from the dataset and with INLINEFORM7 generated from the extracted observations INLINEFORM8 and actions INLINEFORM9 . This section outlines the tourist and guide architectures. We first describe how the tourist produces messages for the various communication channels across which the messages are sent. We subsequently describe how these messages are processed by the guide, and introduce the novel Masked Attention for Spatial Convolutions (MASC) mechanism that allows for grounding into the 2D overhead map in order to",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What dataset do they use?",
    "llm_answer": "BIBREF3, FM-IQA BIBREF4, The VQA Dataset BIBREF1, Visual7W BIBREF5, Visual Genome BIBREF6, MSCOCO, Oxford-102, CUB, Oxford, COCO, CIFAR-10, StackOverflow, and Biomedical.\n",
    "context": [
      {
        "id": "75c8f911-af20-49c1-8b12-e4aecd8370c1",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1707.03904",
          "start_idx": 5304,
          "end_idx": 5850
        },
        "page_content": " subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short. We plan to release these annotations along with the datasets, and our retrieved documents for each question. Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated",
        "type": "Document"
      },
      {
        "id": "91672d1f-40de-4361-a563-5175325a46aa",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1703.09684",
          "start_idx": 2802,
          "end_idx": 3121
        },
        "page_content": " BIBREF3 , FM-IQA BIBREF4 , The VQA Dataset BIBREF1 , Visual7W BIBREF5 , and Visual Genome BIBREF6 . FM-IQA needs human judges and has not been widely used, so we do not discuss it further. Table 1 shows statistics for the other datasets. Following others BIBREF7 , BIBREF8 , BIBREF9 , we refer to the portion of The VQ",
        "type": "Document"
      },
      {
        "id": "7d4af53c-0878-4ce8-8945-c54db6f33d53",
        "metadata": {
          "chunk_id": 147,
          "document_id": "1910.09399",
          "start_idx": 57115,
          "end_idx": 57512
        },
        "page_content": " MSCOCO, and Oxford-102 datasets, which facilitated the work on text-to-image synthesis for several papers released more recently. While most deep learning algorithms use MNIST BIBREF74 dataset as the benchmark, there are three main datasets that are commonly used for evaluation of proposed GAN models for text-to-image synthesis: CUB BIBREF75, Oxford BIBREF76, COCO BIBREF77, and CIFAR-10 BIBREF",
        "type": "Document"
      },
      {
        "id": "e925a6ff-8825-4f0a-bef0-ce1426b042e5",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1910.12618",
          "start_idx": 9057,
          "end_idx": 9544
        },
        "page_content": " the French meteorological office, M\u00e9t\u00e9o France, while the British ones are scrapped from stations of the National Oceanic and Atmospheric Administration (NOAA). Available on the same time span as the consumption, they usually have a 3 hours temporal resolution but are averaged to a daily one as well. Finally the time series were scaled to the range $[0,1]$ before the training phase, and re-scaled during prediction time. Our work aims at predicting time series using exclusively text",
        "type": "Document"
      },
      {
        "id": "af27ee8c-701c-4fe4-b55b-63a0e04b788a",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1701.00185",
          "start_idx": 18784,
          "end_idx": 19212
        },
        "page_content": " BIBREF41 . StackOverflow. We use the challenge data published in Kaggle.com. The raw dataset consists 3,370,528 samples through July 31st, 2012 to August 14, 2012. In our experiments, we randomly select 20,000 question titles from 20 different tags as in Table TABREF25 . Biomedical. We use the challenge data published in BioASQ's official website. In our experiments, we randomly select 20, 000 paper titles from 20 different",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Do the use word embeddings alone or they replace some previous features of the model with word embeddings?",
    "llm_answer": "Word embeddings are used *in addition* to other features.\n",
    "context": [
      {
        "id": "5c1f382b-823e-4c71-82a8-d8d3a9bdf20e",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1909.08859",
          "start_idx": 6882,
          "end_idx": 7285
        },
        "page_content": " embedding layer uses a convolutional neural network, namely CharCNN model by BIBREF7, which outputs character level embeddings for each word and alleviates the issue of out-of-vocabulary (OOV) words. In word embedding layer, we use a pretrained GloVe model BIBREF8 and extract word-level embeddings. The concatenation of the character and the word embeddings are then fed to a two-layer highway network",
        "type": "Document"
      },
      {
        "id": "821fe873-cb0d-418d-a7b3-edc43aed3bb9",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1910.12618",
          "start_idx": 13663,
          "end_idx": 14094
        },
        "page_content": " articles for example). The two most popular embeddings are probably Google's Word2Vec BIBREF19 and Standford's GloVe BIBREF20. In the former, a neural network is trained to predict a word given its context (continuous bag of word model), whereas in the latter a matrix factorization scheme on the log co-occurences of words is applied. In any case, the very nature of the objective function allows the embedding models to learn to",
        "type": "Document"
      },
      {
        "id": "e512a07e-09ee-42b7-83ba-de9345a89798",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1910.14497",
          "start_idx": 9536,
          "end_idx": 10043
        },
        "page_content": "000 words are used in all embeddings, though preliminary results indicate the findings extend to the full corpus. For our novel methods of mitigating bias, a shallow neural network is used to adjust the embedding. The single layer of the model is an embedding layer with weights initialized to those of the original embedding. For the composite method, these weights are initialized to those of the embedding after probabilistic bias mitigation. A batch of word indices is fed into the model, which are then",
        "type": "Document"
      },
      {
        "id": "91cc1124-5bec-4207-a8cf-bdfd345e52d1",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1809.10644",
          "start_idx": 13665,
          "end_idx": 14179
        },
        "page_content": " information about the user. For example, saying something in all CAPS is a useful signal that the model can take advantage of. Since our method was a simple word embedding based model, we explored the learned embedding space to analyze results. For this analysis, we only use the max pooling part of our architecture to help analyze the learned embedding space because it encourages salient words to increase their values to be selected. We projected the original pre-trained embeddings to the learned space using",
        "type": "Document"
      },
      {
        "id": "1eb77a3b-e072-4dd0-ac3d-d9ee1fc51fc0",
        "metadata": {
          "chunk_id": 18,
          "document_id": "2003.06279",
          "start_idx": 7372,
          "end_idx": 7852
        },
        "page_content": " via embeddings information modify the underlying structure of networks and, most importantly, how it can improve the model to provide improved classification performance in the stylometry task. To represent texts as networks, we used the so-called word adjacency network representation BIBREF35, BIBREF28, BIBREF32. Typically, before creating the networks, the text is pre-processed. An optional pre-processing step is the removal of stopwords. This step is optional because such",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many natural language explanations are human-written?",
    "llm_answer": "100\n",
    "context": [
      {
        "id": "05eaad36-c78e-4f37-97ac-f6a9891bfae7",
        "metadata": {
          "chunk_id": 54,
          "document_id": "2004.03744",
          "start_idx": 19461,
          "end_idx": 20038
        },
        "page_content": " explanations obtains 72.52%. This is encouraging, since it shows that one can obtain additional natural language explanations without sacrificing performance (and eventually even improving the label performance, however, future work is needed to conclude whether the difference $0.48\\%$ improvement in performance is statistically significant). Camburu BIBREF6 mentioned that the BLEU score was not an appropriate measure for the quality of explanations and suggested human evaluation instead. We therefore manually scored the relevance of 100 explanations that were generated",
        "type": "Document"
      },
      {
        "id": "1f126715-854e-4e25-bf88-c872a57117c4",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1807.03367",
          "start_idx": 1227,
          "end_idx": 1800
        },
        "page_content": " for natural language understanding, such as in machine translation and reading comprehension, have progressed at an unprecedented rate in recent years, but still rely on static, large-scale, text-only datasets that lack crucial aspects of how humans understand and produce natural language. Namely, humans develop language capabilities by being embodied in an environment which they can perceive, manipulate and move around in; and by interacting with other humans. Hence, we argue that we should incorporate all three fundamental aspects of human language acquisition\u2014per",
        "type": "Document"
      },
      {
        "id": "3ace560c-0a12-4a96-973b-cec1d33a7583",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1808.09409",
          "start_idx": 5300,
          "end_idx": 5801
        },
        "page_content": "1) ruling out redundant content, (2) excluding sentences containing foreign words or Chinese phonetic alphabet by checking the Unicode values, (3) dropping overly simple sentences which may not be informative, and (4) utilizing a rule-based classifier to determine whether to include the sentence into the corpus. The final corpus consists of 717,241 learner sentences from writers of 61 different native languages, in which English and Japanese constitute the majority. As for completeness, 82.78% of",
        "type": "Document"
      },
      {
        "id": "a3312889-165d-46e1-8054-ed216ee1627a",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1809.04686",
          "start_idx": 8648,
          "end_idx": 9144
        },
        "page_content": " largest datasets for a natural language inference task in English and contains multiple sentence pairs with a sentence-level entailment label. Each pair of sentences can have one of three labels - entailment, contradiction, and neutral, which are annotated by multiple humans. The dataset contains 550K training, 10K validation, and 10K testing examples. To enable research on multilingual SNLI, BIBREF40 chose a subset of the SNLI test set (1332 sentences) and professionally translated it into",
        "type": "Document"
      },
      {
        "id": "f93ff554-a440-4f23-8982-0217cf8636c5",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1910.02789",
          "start_idx": 1610,
          "end_idx": 2211
        },
        "page_content": " summarize and transfer information efficiently from one to the other BIBREF5. The ability to associate states with natural language sentences that describe them is a hallmark of understanding representations for reinforcement learning. Humans use rich natural language to describe and communicate their visual perceptions, feelings, beliefs, strategies, and more. The semantics inherent to natural language carry knowledge and cues of complex types of content, including: events, spatial relations, temporal relations, semantic roles, logical structures, support for inference and entailment, as well",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the dataset used as input to the Word2Vec algorithm?",
    "llm_answer": "100 billion words from Google and Twitter.  Wikipedia dumps for SearchSnippets; the whole StackOverflow dataset (including question titles and post contents) for StackOverflow; all titles and abstracts for Biomedical.\n",
    "context": [
      {
        "id": "47bc3adf-6781-479a-9291-9a1bc4974658",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1910.11471",
          "start_idx": 7565,
          "end_idx": 8068
        },
        "page_content": " data, the code is written in Python programming language. To train the neural model, the texts should be converted to a computational entity. To do that, two separate vocabulary files are created - one for the source texts and another for the code. Vocabulary generation is done by tokenization of words. Afterwards, the words are put into their contextual vector space using the popular word2vec BIBREF10 method to make the words computational. In order to train the translation model between text-to-",
        "type": "Document"
      },
      {
        "id": "b7162a44-cd7b-4cf5-9166-9f2fead4413f",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1706.08032",
          "start_idx": 14661,
          "end_idx": 15094
        },
        "page_content": " momentum of 0.9. Mini-batch size for STS Corpus is 100 and others are 4. In addition, training is done through stochastic gradient descent over shuffled mini-batches with Adadelta update rule BIBREF19 . we use the publicly available Word2Vec trained from 100 billion words from Google and TwitterGlove of Stanford is performed on aggregated global word-word co-occurrence statistics from a corpus. Word2Vec has dimensionality of 300",
        "type": "Document"
      },
      {
        "id": "841df81e-37d6-4b34-b435-a749c015fb71",
        "metadata": {
          "chunk_id": 28,
          "document_id": "2003.06279",
          "start_idx": 11324,
          "end_idx": 11766
        },
        "page_content": " each word follows the reference word are obtained. Once the neural network is trained, the weights of the hidden layer are used as vectors of each corpus word. FastText: this method is another extension of the Word2Vec model BIBREF41. Unlike Word2Vec, FastText represents each word as a bag of character n-grams. Therefore, the neural network not only trains individual words, but also several n-grams of such words. The vector for a word is",
        "type": "Document"
      },
      {
        "id": "cda0c66f-b6e9-4514-bc4e-e91773d42646",
        "metadata": {
          "chunk_id": 59,
          "document_id": "2003.06279",
          "start_idx": 22723,
          "end_idx": 23190
        },
        "page_content": "=10,000$ words, SVM could not be improved. However, even without improvement it yielded the maximum accuracy rate. The Naive Bayes algorithm, in average, could be improved by a margin of about 10%. The results obtained for Word2vec are summarized in Table TABREF29 of the Supplementary Information. Considering short documents ($w=1,000$ words), here the best results occurs only with the decision tree method combined with enriched networks. Differently from the Glo",
        "type": "Document"
      },
      {
        "id": "af7c958d-24a4-489a-8f48-fe7aa6cf0ce7",
        "metadata": {
          "chunk_id": 54,
          "document_id": "1701.00185",
          "start_idx": 19748,
          "end_idx": 20221
        },
        "page_content": " and the most parameters are set as same as Mikolov et al. BIBREF23 to train word vectors on Google News setting, except of vector dimensionality using 48 and minimize count using 5. For SearchSnippets, we train word vectors on Wikipedia dumps. For StackOverflow, we train word vectors on the whole corpus of the StackOverflow dataset described above which includes the question titles and post contents. For Biomedical, we train word vectors on all titles and abstracts of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What methodology is used to compensate for limited labelled data?",
    "llm_answer": "Lightly supervised learning, self training, and distantly-supervised learning.\n",
    "context": [
      {
        "id": "f10a63d5-bb11-4ae3-946b-28e59ccec26b",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1908.06606",
          "start_idx": 806,
          "end_idx": 1288
        },
        "page_content": " often have non-uniform output formats, such as specific-class classifications (e.g. tumor stage), strings in the original text (e.g. result for a laboratory test) and inferred values from part of the original text (e.g. calculated tumor size). Researchers have to construct different models for it, which is already costly, and hence it calls for a lot of labeled data for each model. Moreover, labeling necessary amount of data for training neural network requires expensive labor",
        "type": "Document"
      },
      {
        "id": "34b84b10-1ac1-4128-9337-71a8682b7d8d",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1909.00124",
          "start_idx": 4084,
          "end_idx": 4496
        },
        "page_content": " or users' information (we only use single-labeled data, and we do not use any additional information), they have different settings than ours. Our work is closely related to DNNs based approaches to learning with noisy labels. DNNs based approaches explored three main directions: (1) training DNNs on selected samples BIBREF33, BIBREF34, BIBREF35, BIBREF17, (2) modifying the loss function of DNNs with regular",
        "type": "Document"
      },
      {
        "id": "69c8fd11-666b-45d8-8e2b-92d81ef6b093",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1909.00430",
          "start_idx": 3451,
          "end_idx": 3911
        },
        "page_content": " SECREF19 ). Source code is available at https://github.com/MatanBN/XRTransfer. An effective way to supplement small annotated datasets is to use lightly supervised learning, in which the objective function is supplemented by a set of domain-specific soft-constraints over the model's predictions on unlabeled data. Previous work in lightly-supervised learning focused on training classifiers by using prior knowledge of label proportions BIBREF2 , BIBREF3 , B",
        "type": "Document"
      },
      {
        "id": "3eff4214-8e3b-4659-93c2-ab190040adc5",
        "metadata": {
          "chunk_id": 56,
          "document_id": "1710.01492",
          "start_idx": 20569,
          "end_idx": 20967
        },
        "page_content": " the network, e.g., with convolution or with LSTMs BIBREF55 , BIBREF54 , BIBREF56 , need to learn something like a language model, i.e., which word is likely to follow which one. Training these parts needs no labels. While these parts can be also pre-trained, it is easier, and often better, to use self training. Distantly-supervised learning. Another way to make use of large unannotated datasets",
        "type": "Document"
      },
      {
        "id": "03d45d95-94c6-48f5-9a92-d4ee3a02fefc",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1909.00430",
          "start_idx": 5107,
          "end_idx": 5512
        },
        "page_content": " fully supervised situation in which each data-point INLINEFORM0 has an associated label INLINEFORM1 , to a setup in which sets of data points INLINEFORM2 are associated with corresponding label proportions INLINEFORM3 over that set. Formally, let INLINEFORM0 be a set of data points, INLINEFORM1 be a set of INLINEFORM2 class labels, INLINEFORM3 be a set of sets where INLINEFORM4 for every INLINEFORM5 ,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the baseline state of the art models?",
    "llm_answer": "BiMPM, ESIM, Decomposable Attention Model, KIM, and BERT.\n",
    "context": [
      {
        "id": "a4a4c58b-d1f1-46a1-9912-24f7a070ad68",
        "metadata": {
          "chunk_id": 43,
          "document_id": "2001.00137",
          "start_idx": 16781,
          "end_idx": 17197
        },
        "page_content": " the online platforms. Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector",
        "type": "Document"
      },
      {
        "id": "e218c2da-6400-4484-b946-c9f95cc23931",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1906.06448",
          "start_idx": 17189,
          "end_idx": 17518
        },
        "page_content": " five state-of-the-art NLI models adopting different approaches: BiMPM (Bilateral Multi-Perspective Matching Model; BIBREF31 , BIBREF31 ), ESIM (Enhanced Sequential Inference Model; BIBREF32 , BIBREF32 ), Decomposable Attention Model BIBREF33 , KIM (Knowledge-based Inference Model; BIBREF34 , BIBREF34 ), and BERT (Bidirectional",
        "type": "Document"
      },
      {
        "id": "7d4754c7-4ea8-4d20-9fb6-3d1e50bf0cf6",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1809.04960",
          "start_idx": 22497,
          "end_idx": 22960
        },
        "page_content": " the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types of comments as described in the above retrieval evaluation setting: Correct, Plausible, Popular, and Random. We rank the candidate comment set with four models (TF-IDF, S2S, IR, and Proposed+IR), and record the types of top-1 comments. Figure FIGREF40 shows the percentage of different types of top-1 comments generated by each model. It shows",
        "type": "Document"
      },
      {
        "id": "8937964f-11b5-487b-9960-db106025360c",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1909.02764",
          "start_idx": 17421,
          "end_idx": 17831
        },
        "page_content": " layer. This setup is inspired by BIBREF38. We use a dropout rate of 0.3 in all layers and optimize with Adam BIBREF39 with a learning rate of $10^{-5}$ (These parameters are the same for all further experiments). We build on top of the Keras library with the TensorFlow backend. We consider this setup our baseline model. We train models on a variety of corpora, namely the common format published by BIBREF27",
        "type": "Document"
      },
      {
        "id": "60373253-fa83-4f44-bcbd-a972e3cb2654",
        "metadata": {
          "chunk_id": 24,
          "document_id": "2002.11402",
          "start_idx": 8819,
          "end_idx": 9288
        },
        "page_content": "es other models in all scores. Spacy results are good despite not being trained for case-less data. In terms of F1 and overall stability Spacy did better than Stanford NER, on our News Validation set. Similarly, Stanford did well in Precision but could not catch up with Spacy and our model in terms of Recall. Flair overall performed poorly, but as said before these open-source models are not trained for our particular use-case. Lets check some examples for detailed",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they extract causality from text?",
    "llm_answer": "Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'.  The word `cause' was not included.\n",
    "context": [
      {
        "id": "e73eaff6-bb5b-449c-86d1-09190d1610c1",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1604.05781",
          "start_idx": 5120,
          "end_idx": 5547
        },
        "page_content": " not performed. Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'. The word `cause' was not included due to its use as a popular contraction for `because'. One `cause-word' per document restricted the analysis to single relationships between two relata. Documents that contain bidirectional words (`associate', `relate', `connect', `correlate', and any of their",
        "type": "Document"
      },
      {
        "id": "28ff51bf-cb69-43db-9ae3-fae5245257a3",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1604.05781",
          "start_idx": 2254,
          "end_idx": 2805
        },
        "page_content": "REF27 . Despite these long lines of research, causal attributions made via social media or online social networks have not been well studied. The goal of this paper is to explore the language and topics of causal statements in a large corpus of social media taken from Twitter. We hypothesize that language and sentiment biases play a significant role in these statements, and that tools from natural language processing and computational linguistics can be used to study them. We do not attempt to study the factual correctness of these statements or",
        "type": "Document"
      },
      {
        "id": "454be66f-e519-4102-9aa3-ba81a91e4df6",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1604.05781",
          "start_idx": 1939,
          "end_idx": 2460
        },
        "page_content": " by the cause) depending on the types of verbs used to describe the cause and effect. Researchers have hypothesized BIBREF28 that this is because of the innate weighting property of the verbs in the English language that humans use to attribute causes and effects. Another finding is the role of a valence bias: the volume and intensity of causal reasoning may increase due to negative feedback or negative events BIBREF27 . Despite these long lines of research, causal attributions made via social media or online social",
        "type": "Document"
      },
      {
        "id": "560e4e71-20c7-4d4b-9e88-babc6910d1db",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1604.05781",
          "start_idx": 2805,
          "end_idx": 3363
        },
        "page_content": " can be used to study them. We do not attempt to study the factual correctness of these statements or offer any degree of verification, nor do we exhaustively identify and extract all causal statements from these data. Instead, here we focus on statements that are with high certainty causal statements, with the goal to better understand key characteristics about causal statements that differ from everyday online communication. The rest of this paper is organized as follows: In Sec. \"Materials and Methods\" we discuss our materials and methods, including",
        "type": "Document"
      },
      {
        "id": "bdedab9a-6868-49eb-9166-aacd7a1e83c8",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1604.05781",
          "start_idx": 1169,
          "end_idx": 1643
        },
        "page_content": "IBREF16 , and text mining procedures BIBREF17 , BIBREF18 , BIBREF19 to better infer causal influence from data. In the cognitive sciences, the famous perception experiments of Michotte et al. led to a long line of research exploring the cognitive biases that humans possess when attempting to link cause and effect BIBREF20 , BIBREF21 , BIBREF22 . How humans understand and communicate cause and effect relationships is complicated, and is influenced by language structure B",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the source of the \"control\" corpus?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "788ac258-5d9d-44a4-b679-525540030f79",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1604.05781",
          "start_idx": 6146,
          "end_idx": 6498
        },
        "page_content": "560 control documents. Each of the three \u201ccause-words\u201d, `caused', `causes', and `causing' appeared in 38.2%, 35.0%, and 26.8% of causal documents, respectively. Documents were further studied by annotating their unigrams with Parts-of-Speech (POS) and Named Entities (NE) tags. POS tagging was done using NLTK v3.1 BIBREF29 which implements an averaged",
        "type": "Document"
      },
      {
        "id": "53a2e4af-aa7a-4329-add2-704c3b432a5c",
        "metadata": {
          "chunk_id": 21,
          "document_id": "1604.05781",
          "start_idx": 7823,
          "end_idx": 8087
        },
        "page_content": "))}{p_N(x) / (1-p_N(x))},$$   (Eq. 1)   where $p_C(x)$ and $p_N(x)$ are the probabilities that a unigram, POS, or NE $x$ occurs in the causal and control corpus, respectively. These probabilities were computed for each corpus separately as $p(x) = f(x) / \\sum _{x^",
        "type": "Document"
      },
      {
        "id": "ffcdee05-7820-4632-b45a-84ec7ddff98e",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1604.05781",
          "start_idx": 3160,
          "end_idx": 3653
        },
        "page_content": " organized as follows: In Sec. \"Materials and Methods\" we discuss our materials and methods, including the dataset we studied, how we preprocessed that data and extracted a `causal' corpus and a corresponding `control' corpus, and the details of the statistical and language analysis tools we studied these corpora with. In Sec. \"Results\" we present results using these tools to compare the causal statements to control statements. We conclude with a discussion in Sec. \"Discussion\" . Data was",
        "type": "Document"
      },
      {
        "id": "b495585b-0fa7-47cf-8952-6efb7d7701f9",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1910.11949",
          "start_idx": 2984,
          "end_idx": 3467
        },
        "page_content": "REF8 by Joseph Weizenbaum at MIT. Its implementation consisted in pattern matching and substitution methodology. Recently, data driven approaches have drawn significant attention. Existing work along this line includes retrieval-based methods BIBREF9BIBREF10 and generation-based methodsBIBREF11BIBREF12. In this work we focus on generative models, where sequence-to-sequence algorithm that uses RNNs to encode and decode inputs into responses is a current best practice. Our convers",
        "type": "Document"
      },
      {
        "id": "9c7f218a-80a4-486c-815b-422ad44b40e9",
        "metadata": {
          "chunk_id": 121,
          "document_id": "1701.02877",
          "start_idx": 43692,
          "end_idx": 44145
        },
        "page_content": " the CoNLL 2003 corpus. The seed domain for the experiments here is newswire, where we use the classifier trained on the biggest NW corpus investigated in this study, i.e. OntoNotes NW. That classifier is then applied to all other corpora. The rationale is to test how suitable such a big corpus would be for improving Twitter NER, for which only small training corpora are available. Results for out-of-domain performance are reported in Table 11 . The",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the selection criteria for \"causal statements\"?",
    "llm_answer": "Statements with high certainty of being causal statements.\n",
    "context": [
      {
        "id": "6b2cf8d9-5deb-42eb-b7e9-b4783477ee4f",
        "metadata": {
          "chunk_id": 71,
          "document_id": "1604.05781",
          "start_idx": 25272,
          "end_idx": 25869
        },
        "page_content": " causal statements are made? Our work here focused on a limited subset of causal statements, but more generally, these results may inform new methods for automatically detecting causal statements from unstructured, natural language text BIBREF17 . Better computational tools focused on causal statements are an important step towards further understanding misinformation campaigns and other online activities. Lastly, an important but deeply challenging open question is how, if it is even possible, to validate the accuracy of causal statements. Can causal statements be ranked by some confidence",
        "type": "Document"
      },
      {
        "id": "22abf00e-e5de-4152-8498-2ba1c0b84d93",
        "metadata": {
          "chunk_id": 70,
          "document_id": "1604.05781",
          "start_idx": 24966,
          "end_idx": 25505
        },
        "page_content": " causal statements from other users? The topics of causal statements also bring forth important questions to be addressed: how timely are causal statements? Are certain topics always being discussed in causal statements? Are there causal topics that are very popular for only brief periods and then forgotten? Temporal dynamics of causal statements are also interesting: do time-of-day or time-of-year factors play a role in how causal statements are made? Our work here focused on a limited subset of causal statements, but more generally",
        "type": "Document"
      },
      {
        "id": "55c8736d-3b06-4858-9e9c-578632166b3b",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1604.05781",
          "start_idx": 21180,
          "end_idx": 21680
        },
        "page_content": "Figs. 1 and 2 ) and sentiment (Fig. 3 ) differences between causal statements made on social media compared with other social media statements. But what is being discussed? What are the topical foci of causal statements? To study this, for our last analysis we applied topic models to the causal statements. Topic modeling finds groups of related terms (unigrams) by considering similarities between how those terms co-occur across a set of documents. We used the popular topic modeling method Latent",
        "type": "Document"
      },
      {
        "id": "b64d0359-d433-4dae-a45b-35999e5e34b7",
        "metadata": {
          "chunk_id": 69,
          "document_id": "1604.05781",
          "start_idx": 24428,
          "end_idx": 24966
        },
        "page_content": ". Given the central role of causal statements, further studies of the interplay of information propagation and online causal attributions are crucial. Are causal statements more likely to spread online and, if so, in which ways? What types of social media users are more or less likely to make causal statements? Will a user be more likely to make a causal statement if they have recently been exposed to one or more causal statements from other users? The topics of causal statements also bring forth important questions to be addressed:",
        "type": "Document"
      },
      {
        "id": "560e4e71-20c7-4d4b-9e88-babc6910d1db",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1604.05781",
          "start_idx": 2805,
          "end_idx": 3363
        },
        "page_content": " can be used to study them. We do not attempt to study the factual correctness of these statements or offer any degree of verification, nor do we exhaustively identify and extract all causal statements from these data. Instead, here we focus on statements that are with high certainty causal statements, with the goal to better understand key characteristics about causal statements that differ from everyday online communication. The rest of this paper is organized as follows: In Sec. \"Materials and Methods\" we discuss our materials and methods, including",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Do they use expert annotations, crowdsourcing, or only automatic methods to analyze the corpora?",
    "llm_answer": "Expert annotations and crowdsourcing.\n",
    "context": [
      {
        "id": "2cb50ca4-54a4-4f7e-80ee-bb02c2a50d5d",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1703.09684",
          "start_idx": 9860,
          "end_idx": 10324
        },
        "page_content": "QA and Visual Genome. Second, we created algorithms that generated questions from COCO's semantic segmentation annotations BIBREF17 , and Visual Genome's objects and attributes annotations BIBREF6 . Third, we used human annotators for certain question-types. In the following sections, we briefly describe each of these methods. We imported questions from COCO-VQA and Visual Genome belonging to all question-types except `object utilities and affordances'. We did",
        "type": "Document"
      },
      {
        "id": "808d625a-6c17-4f58-a61e-f6bdb7ce2194",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1703.09684",
          "start_idx": 16154,
          "end_idx": 16695
        },
        "page_content": "-exhaustive and duplicated, which makes using them to automatically make QA pairs difficult. We only use Visual Genome to make color and positional reasoning questions. The methods we used are similar to those used with COCO, but additional precautions were needed due to quirks in their annotations. Additional details are provided in the Appendix. Creating sentiment understanding and object utility/affordance questions cannot be readily done using templates, so we used manual annotation to create these. Twelve volunteer annotators were",
        "type": "Document"
      },
      {
        "id": "68c9fa82-7b71-4d3b-8d4e-70ded7bf95e4",
        "metadata": {
          "chunk_id": 21,
          "document_id": "1701.02877",
          "start_idx": 7791,
          "end_idx": 8297
        },
        "page_content": " studying NER performance on this genre gained momentum. So far, there have been no big evaluation efforts, such as ACE and OntoNotes, resulting in substantial amounts of gold standard data. Instead, benchmark corpora were created as part of smaller challenges or individual projects. The first such corpus is the UMBC corpus for Twitter NER BIBREF33 , where researchers used crowdsourcing to obtain annotations for persons, locations and organisations. A further Twitter NER corpus was created by BIBREF21",
        "type": "Document"
      },
      {
        "id": "9612e20e-4bb2-43b7-ac27-1494213608ba",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1707.03904",
          "start_idx": 4820,
          "end_idx": 5304
        },
        "page_content": "5\\%$ on Quasar-S and Quasar-T, while human performance is $50\\%$ and $60.6\\%$ respectively. For the automatic systems, we see an interesting tension between searching and reading accuracies \u2013 retrieving more documents in the search phase leads to a higher coverage of answers, but makes the comprehension task more difficult. We also collect annotations on a subset of the development set questions to allow researchers to analyze the categories in which their system performs well or",
        "type": "Document"
      },
      {
        "id": "6e88f193-653f-4ba8-8d1d-4192af66023b",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1701.02877",
          "start_idx": 3620,
          "end_idx": 4066
        },
        "page_content": " primarily by F1 score), and also to study the relationship between OOV NEs and features and F1. See Section \"Experiments\" for details. To ensure representativeness and comprehensiveness, our experimental findings are based on key benchmark NER corpora spanning multiple genres, time periods, and corpus annotation methodologies and guidelines. As detailed in Section \"Datasets\" , the corpora studied are OntoNotes BIBREF29 , ACE BIBREF30 , MUC 7",
        "type": "Document"
      }
    ]
  },
  {
    "query": "how do they collect the comparable corpus?",
    "llm_answer": "A corpus of poems and a corpus of vernacular literature were collected from online resources.  The poem corpus contains 163K quatrain poems, and the vernacular literature corpus contains 337K short paragraphs from 281 famous books.\n",
    "context": [
      {
        "id": "bc04100d-78b1-4864-8b56-9fe2b8a04d0f",
        "metadata": {
          "chunk_id": 33,
          "document_id": "1604.05781",
          "start_idx": 11498,
          "end_idx": 11828
        },
        "page_content": " unigrams for each corpus was used to compare the corpora. In addition, a single sentiment score for each corpus was computed as the average sentiment score over all unigrams in that corpus, weighed by unigram frequency: $\\sum _{w \\in \\tilde{V}} {f(w) s(w)} \\Big / \\sum _{w^{\\prime } \\in \\tilde{V}} f(w^{\\prime })$ . To supplement",
        "type": "Document"
      },
      {
        "id": "63e8453a-a82c-4c94-840b-5a9dea7d035f",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1604.05781",
          "start_idx": 8014,
          "end_idx": 8330
        },
        "page_content": " corpus separately as $p(x) = f(x) / \\sum _{x^{\\prime } \\in V} f(x^{\\prime })$ , where $f(x)$ is the total number of occurrences of $x$ in the corpus and $V$ is the relevant set of unigrams, POS, or NEs. Confidence intervals for the ORs were computed using Wald's methodology BIBREF36 . As there are many unique unig",
        "type": "Document"
      },
      {
        "id": "116d63ff-ea12-4da6-a510-80a348767ca0",
        "metadata": {
          "chunk_id": 99,
          "document_id": "1902.00330",
          "start_idx": 33470,
          "end_idx": 33958
        },
        "page_content": " the pairs of mentions at most once, they assume each mention only needs to be consistent with one another mention in the document. The limitation of their method is that the consistency information is too sparse, resulting in low confidence. Similar to us, Guo et al. BIBREF18 also sort mentions according to the difficulty of disambiguation, but they did not make full use of the information of previously referred entities for the subsequent entity disambiguation. Nguyen et al. BIBREF",
        "type": "Document"
      },
      {
        "id": "5254b27e-1ebe-4049-8699-d0767f294567",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1909.00279",
          "start_idx": 13718,
          "end_idx": 14228
        },
        "page_content": " models, please refer to supplementary materials. Training and Validation Sets We collected a corpus of poems and a corpus of vernacular literature from online resources. The poem corpus contains 163K quatrain poems from Tang Poems and Song Poems, the vernacular literature corpus contains 337K short paragraphs from 281 famous books, the corpus covers various literary forms including prose, fiction and essay. Note that our poem corpus and a vernacular corpus are not aligned. We further split the two corpor",
        "type": "Document"
      },
      {
        "id": "9c7f218a-80a4-486c-815b-422ad44b40e9",
        "metadata": {
          "chunk_id": 121,
          "document_id": "1701.02877",
          "start_idx": 43692,
          "end_idx": 44145
        },
        "page_content": " the CoNLL 2003 corpus. The seed domain for the experiments here is newswire, where we use the classifier trained on the biggest NW corpus investigated in this study, i.e. OntoNotes NW. That classifier is then applied to all other corpora. The rationale is to test how suitable such a big corpus would be for improving Twitter NER, for which only small training corpora are available. Results for out-of-domain performance are reported in Table 11 . The",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they collect the control corpus?",
    "llm_answer": "The provided text does not describe how the control corpus is collected.\n",
    "context": [
      {
        "id": "53a2e4af-aa7a-4329-add2-704c3b432a5c",
        "metadata": {
          "chunk_id": 21,
          "document_id": "1604.05781",
          "start_idx": 7823,
          "end_idx": 8087
        },
        "page_content": "))}{p_N(x) / (1-p_N(x))},$$   (Eq. 1)   where $p_C(x)$ and $p_N(x)$ are the probabilities that a unigram, POS, or NE $x$ occurs in the causal and control corpus, respectively. These probabilities were computed for each corpus separately as $p(x) = f(x) / \\sum _{x^",
        "type": "Document"
      },
      {
        "id": "9c7f218a-80a4-486c-815b-422ad44b40e9",
        "metadata": {
          "chunk_id": 121,
          "document_id": "1701.02877",
          "start_idx": 43692,
          "end_idx": 44145
        },
        "page_content": " the CoNLL 2003 corpus. The seed domain for the experiments here is newswire, where we use the classifier trained on the biggest NW corpus investigated in this study, i.e. OntoNotes NW. That classifier is then applied to all other corpora. The rationale is to test how suitable such a big corpus would be for improving Twitter NER, for which only small training corpora are available. Results for out-of-domain performance are reported in Table 11 . The",
        "type": "Document"
      },
      {
        "id": "cc05587e-3a5e-4024-b6cc-b6d7eb503148",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1809.05752",
          "start_idx": 9188,
          "end_idx": 9738
        },
        "page_content": " set. The corpus consists of discharge summaries, encounter notes, and visit notes from approximately 30,000 patients admitted to the system's hospitals with psychiatric diagnoses and symptoms. This breadth of data captures a wide range of clinical narratives, creating a comprehensive foundation for topic extraction. After using the RPDR query tool to extract EHR paragraphs from the RPDR database, we created a training corpus by categorizing the extracted paragraphs according to their risk factor domain using a lexicon of 120 keywords that were",
        "type": "Document"
      },
      {
        "id": "ff43bc7c-b607-4c0d-9912-3694b0550880",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1701.02877",
          "start_idx": 12460,
          "end_idx": 12904
        },
        "page_content": ", a number of tokens equivalent to those in the corpus, e.g. 7037 for UMBC (Table 5 ) or, respectively, a number of NEs equivalent to those in the corpus (506 for UMBC) are selected (Table 4 ). An easy choice of sampling method would be to sample tokens and NEs randomly. However, this would not reflect the composition of corpora appropriately. Corpora consist of several documents, tweets or blog entries, which are likely to repeat the words",
        "type": "Document"
      },
      {
        "id": "6da9606f-82ac-4107-84e8-1b7f0d0f9701",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1911.01799",
          "start_idx": 6152,
          "end_idx": 6612
        },
        "page_content": " CN-Celeb was collected following a two-stage strategy: firstly we used an automated pipeline to extract potential segments of the Person of Interest (POI), and then applied a human check to remove incorrect segments. This process is much faster than purely human-based segmentation, and reduces errors caused by a purely automated process. Briefly, the automated pipeline we used is similar to the one used to collect VoxCeleb1 BIBREF14 and VoxCeleb2 BIBREF15",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are resolution model variables?",
    "llm_answer": "Resolution mode variables $\\Pi = \\lbrace \\pi _1, \\ldots , \\pi _n\\rbrace $, where for each mention $j$ the variable $\\pi _j \\in \\lbrace str, prec, attr\\rbrace $ indicates in which mode the mention should be resolved.\n",
    "context": [
      {
        "id": "f2996a1c-6451-43bc-92fc-108678fab5f0",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1603.04553",
          "start_idx": 4984,
          "end_idx": 5354
        },
        "page_content": " as apposition or acronym. Motivated by this, we introduce resolution mode variables $\\Pi = \\lbrace \\pi _1, \\ldots , \\pi _n\\rbrace $ , where for each mention $j$ the variable $\\pi _j \\in \\lbrace str, prec, attr\\rbrace $ indicates in which mode the mention should be resolved. In our model, we define three resolution modes \u2014 string-matching (str), precise-construct (pre",
        "type": "Document"
      },
      {
        "id": "e12a6d09-5f23-4600-9e19-2ea78f6c7a94",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1603.04553",
          "start_idx": 12601,
          "end_idx": 13098
        },
        "page_content": " differentiate more resolution modes. Another one is to add more precise or even event-based features to improve the model's performance. This research was supported in part by DARPA grant FA8750-12-2-0342 funded under the DEFT program. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA. Appendix A. Derivation of Model Learning Formally, we iteratively estimate the model parameters $\\",
        "type": "Document"
      },
      {
        "id": "c9e8640d-fb7e-4978-a25d-d688db36cad3",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1603.04553",
          "start_idx": 15461,
          "end_idx": 15606
        },
        "page_content": "_{j-1}, c_{j+1}, \\ldots , c_{n}\\rbrace $ . The above derivations correspond to the learning algorithm in Algorithm \"Resolution Mode Variables\" . ",
        "type": "Document"
      },
      {
        "id": "96626800-9f9e-4ed6-8553-18e8b02aee94",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1603.04553",
          "start_idx": 5173,
          "end_idx": 5502
        },
        "page_content": " model, we define three resolution modes \u2014 string-matching (str), precise-construct (prec), and attribute-matching (attr) \u2014 and $\\Pi $ is deterministic when $D$ is given (i.e. $P(\\Pi |D)$ is a point distribution). We determine $\\pi _j$ for each mention $m_j$ in the following way:  $\\pi _j = str$ , if there exists a mention $m_i",
        "type": "Document"
      },
      {
        "id": "e5c361e5-7be4-4bdb-bc6c-4f05814e6992",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1603.04553",
          "start_idx": 4512,
          "end_idx": 4984
        },
        "page_content": " to be resolved. According to previous work BIBREF17 , BIBREF18 , BIBREF1 , antecedents are resolved by different categories of information for different mentions. For example, the Stanford system BIBREF1 uses string-matching sieves to link two mentions with similar text and precise-construct sieve to link two mentions which satisfy special syntactic or semantic relations such as apposition or acronym. Motivated by this, we introduce resolution mode variables $\\Pi = \\",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is the model presented in the paper state of the art?",
    "llm_answer": "Yes.\n",
    "context": [
      {
        "id": "fc23cf7f-21ff-43cd-8d00-ef5914322400",
        "metadata": {
          "chunk_id": 68,
          "document_id": "1810.00663",
          "start_idx": 26280,
          "end_idx": 26763
        },
        "page_content": " room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM. Note that the average number of actions in the ground truth output sequences is 7.07 for the Test-New set. Our model's predictions are just INLINEFORM1 edits off on average from the correct navigation plans. This",
        "type": "Document"
      },
      {
        "id": "627dbfa3-5493-4f17-b5e4-e8a90355b5fd",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1911.03894",
          "start_idx": 22277,
          "end_idx": 22771
        },
        "page_content": " many other languages. We will publish an updated version in the near future where we will explore and release models trained for longer, with additional downstream tasks, baselines (e.g. XLM) and analysis, we will also train additional models with potentially cleaner corpora such as CCNet BIBREF56 for more accurate performance evaluation and more complete ablation. This work was partly funded by three French National grants from the Agence Nationale de la Recherche, namely projects PARSIT",
        "type": "Document"
      },
      {
        "id": "3b61fd1a-0d50-4af7-bc74-5de4e141f81c",
        "metadata": {
          "chunk_id": 89,
          "document_id": "1905.06566",
          "start_idx": 24955,
          "end_idx": 25353
        },
        "page_content": " We also conducted human experiment with 20 randomly sampled documents from the CNNDM test set. We compared our model $\\text{\\sc Hibert}_M$ against Lead3, DCA, Latent, BERT and the human reference (Human). We asked the subjects to rank the outputs of these systems from best to worst. As shown in Table 4 , the output of $\\text{\\sc Hibert}_M$ is selected as the best in 30% of cases and we obtained",
        "type": "Document"
      },
      {
        "id": "1ee84850-4be0-4ba1-b60f-c47efa963feb",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1905.00563",
          "start_idx": 659,
          "end_idx": 1149
        },
        "page_content": " However, there are only a few studies BIBREF12 , BIBREF13 that investigate the quality of the different KG models. There is a need to go beyond just the accuracy on link prediction, and instead focus on whether these representations are robust and stable, and what facts they make use of for their predictions. In this paper, our goal is to design approaches that minimally change the graph structure such that the prediction of a target fact changes the most after the embeddings are rele",
        "type": "Document"
      },
      {
        "id": "5e5c6963-88aa-4a86-96ff-323aeb271bfb",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1810.00663",
          "start_idx": 25415,
          "end_idx": 25983
        },
        "page_content": " In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models. The previous section evaluated model performance on new instructions (and corresponding navigation plans) for environments that were previously seen at training time. Here, we examine whether the trained models succeed on environments that are completely new. The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was the result of the highest performing system?",
    "llm_answer": "Human.\n",
    "context": [
      {
        "id": "9c895dc0-f4fa-41bb-8502-b392e778c1ee",
        "metadata": {
          "chunk_id": 90,
          "document_id": "1905.06566",
          "start_idx": 25175,
          "end_idx": 25555
        },
        "page_content": " Hibert}_M$ is selected as the best in 30% of cases and we obtained lower mean rank than all systems except for Human. We also converted the rank numbers into ratings (rank $i$ to $7-i$ ) and applied student $t$ -test on the ratings. $\\text{\\sc Hibert}_M$ is significantly different from all systems in comparison ( $p < 0.05$ ), which indicates our model still lags behind Human,",
        "type": "Document"
      },
      {
        "id": "fceed6b1-b7ea-45e1-aa4f-55f1e50b1354",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1911.12569",
          "start_idx": 19389,
          "end_idx": 19799
        },
        "page_content": " features. Our system comfortably surpasses the existing best system at SemEval. Our system manages to improve the existing best system of SemEval 2016 task 6 by 3.2 F-score points for sentiment analysis. We also compare our system with the state-of-the-art systems proposed by BIBREF15 on the emotion dataset. The comparison is demonstrated in Table TABREF22. Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN were",
        "type": "Document"
      },
      {
        "id": "c686ef15-dda8-4ff8-a15a-946f0174a62e",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1603.04553",
          "start_idx": 10414,
          "end_idx": 10861
        },
        "page_content": " baseline with two deterministic systems, namely Stanford: the Stanford system BIBREF10 and Multigraph: the unsupervised multigraph system BIBREF26 , and one unsupervised system, namely MIR: the unsupervised system using most informative relations BIBREF27 . Our model outperforms the three baseline systems on all the evaluation metrics. Specifically, our model achieves improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford system,",
        "type": "Document"
      },
      {
        "id": "8005ecd6-6854-40ee-8c5b-c296aeb8fee3",
        "metadata": {
          "chunk_id": 141,
          "document_id": "1701.02877",
          "start_idx": 50827,
          "end_idx": 51373
        },
        "page_content": " The other genre that was particularly difficult for the systems was ACE Usenet. This is a form of user-generated content, not intended for publication but rather discussion among communities. In this sense, it is social media, and so it is not surprising that system performance on ACE UN resembles performance on social media more than other genres. Crucially, the computationally-cheap memorisation method actually acts as a reasonable predictor of the performance of other methods. This suggests that high entity diversity predicts difficulty",
        "type": "Document"
      },
      {
        "id": "edbea1f2-3b12-4de3-bdc4-8777605295b1",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1603.07044",
          "start_idx": 15296,
          "end_idx": 15790
        },
        "page_content": " already performs better than a very strong feature-rich based system. For task B, since only limited training data is given, both feature-rich based system and our system are worse than the IR system. For task C, our system also got comparable results with the feature-rich based system. If we do a simple system combination (average the rank score) between our system and the IR system, the combined system will give large gains on tasks B and C. This implies that our system is complimentary",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What do they mean by answer styles?",
    "llm_answer": "Answer styles are related to the context of the question in the answer sentence, and can also control other styles such as length and speaking styles.  The system uses an artificial token corresponding to the target style at the beginning of the answer sentence.\n",
    "context": [
      {
        "id": "1224529c-e428-495a-9ab7-da98dce11214",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1901.02262",
          "start_idx": 2098,
          "end_idx": 2374
        },
        "page_content": " x^{p_k}_{L}\\rbrace $ , and an answer style $s$ , an RC system outputs an answer $y = \\lbrace y_1, \\ldots , y_T \\rbrace $ conditioned on the style. In short, for inference, given a set of 3-tuples $(x^q, \\lbrace x^{p_k}\\rbrace , s)$ , the system predicts $P(y)$ . The training",
        "type": "Document"
      },
      {
        "id": "5906a004-7795-4512-a21a-cf20cea662dd",
        "metadata": {
          "chunk_id": 83,
          "document_id": "1901.02262",
          "start_idx": 22902,
          "end_idx": 23374
        },
        "page_content": " The key strength of our model is its high accuracy of generating abstractive summaries from the question and passages; our model achieved state-of-the-art performance in terms of Rouge-L on the Q&A and NLG tasks of MS MARCO 2.1 that have different answer styles BIBREF5 . The styles considered in this paper are only related to the context of the question in the answer sentence; our model will be promising for controlling other styles such as length and speaking styles",
        "type": "Document"
      },
      {
        "id": "233f3d98-fa10-4685-b2e6-baa1d2edabf6",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1901.02262",
          "start_idx": 9817,
          "end_idx": 10299
        },
        "page_content": " $t$ depend only on the known outputs at positions less than $t$ . Moreover, to be able to make use of multiple answer styles within a single system, our model introduces an artificial token corresponding to the target style at the beginning of the answer sentence ( $y_1$ ), like BIBREF14 . At test time, the user can specify the first token to control the answer styles. This modification does not require any changes to the model architecture. Note that introducing the tokens on",
        "type": "Document"
      },
      {
        "id": "a74ce882-db16-4511-82f1-ccf7036a73d9",
        "metadata": {
          "chunk_id": 80,
          "document_id": "1901.02262",
          "start_idx": 21638,
          "end_idx": 22164
        },
        "page_content": " 5 shows the lengths of the answers generated by our model, which are broken down by answer style and query type. The generated answers were relatively shorter than the reference answers but well controlled with the target style in every query type. Also, we should note that our model does not guarantee the consistency in terms of meaning across the answer styles. We randomly selected 100 questions and compared the answers our model generated with the NLG and Q&A styles. The consistency ratio was 0.81, where major errors",
        "type": "Document"
      },
      {
        "id": "b87e923b-05c6-464a-abd4-466cc7f9d727",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1901.02262",
          "start_idx": 1960,
          "end_idx": 2184
        },
        "page_content": " with $J$ words $x^q = \\lbrace x^q_1, \\ldots , x^q_J\\rbrace $ , a set of $K$ passages, where each $k$ -th passage is composed of $L$ words $x^{p_k} = \\lbrace x^{p_k}_1, \\ldots , x^{p_k}_{L}\\rbrace $ , and an answer style $s$",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the baselines that Masque is compared against?",
    "llm_answer": "Baseline and Ablation models.\n",
    "context": [
      {
        "id": "a32be793-bd84-416f-b322-e49036c1e5ba",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1901.02262",
          "start_idx": 1583,
          "end_idx": 1960
        },
        "page_content": ", for multi-passage RC. On the MS MARCO 2.1 dataset, Masque achieves state-of-the-art performance on the dataset's two tasks, Q&A and NLG, with different answer styles. The main contributions of this study are that our model enables the following two abilities. The task considered in this paper, is defined as: Problem 1 Given a question with $J$ words $x^q = \\lbrace x^q_1, \\",
        "type": "Document"
      },
      {
        "id": "d7a0b7aa-da14-4112-80c0-cc26c62fe83f",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "ac689a53-2fbb-4cbb-8161-69de443732ab",
        "metadata": {
          "chunk_id": 81,
          "document_id": "1901.02262",
          "start_idx": 22164,
          "end_idx": 22610
        },
        "page_content": " the NLG and Q&A styles. The consistency ratio was 0.81, where major errors were due to copying words from different parts of the passages and generating different words, especially yes/no, from a fixed vocabulary. Appendix \"Reading Comprehension Examples generated by Masque from MS MARCO 2.1\" shows examples of generated answers. We found (d) style errors; (e) yes/no classification errors; (f) copy errors with respect to numerical values; and",
        "type": "Document"
      },
      {
        "id": "42837a06-4572-4044-8126-3100084f04f6",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1703.09684",
          "start_idx": 24411,
          "end_idx": 24828
        },
        "page_content": " simple baseline methods routinely exceed more complex methods using simple accuracy BIBREF7 , BIBREF8 , BIBREF19 . On TDIUC, MLP surpasses MCB and NMN in terms of simple accuracy, but a closer inspection reveals that MLP's score is highly determined by performance on categories with a large number of examples, such as `absurd' and `object presence.' Using MPT, we find that both NMN and MCB outperform MLP. Inspect",
        "type": "Document"
      },
      {
        "id": "487566c1-6b22-466e-8847-fa293a6f80e8",
        "metadata": {
          "chunk_id": 83,
          "document_id": "1905.06566",
          "start_idx": 23289,
          "end_idx": 23633
        },
        "page_content": " by 0.5 ROUGE. We also implemented two baselines. One is the hierarchical transformer summarization model (HeriTransfomer; described in \"Extractive Summarization\" ) without pre-training. Note the setting for HeriTransfomer is ( $L=4$ , $H=300$ and $A=4$ ) . We can see that the pre-training (details in Section \"Pre-training\" ) leads to a +1.25",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance achieved on NarrativeQA?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "cb3ea81d-94bc-49c2-8249-6b43f558696c",
        "metadata": {
          "chunk_id": 67,
          "document_id": "1909.08859",
          "start_idx": 24277,
          "end_idx": 24845
        },
        "page_content": " to keep track of the changes in the states of the entities over the course of the procedure. Our experimental analysis on visual reasoning tasks in the RecipeQA dataset shows that the model significantly improves the results of the previous models, indicating that it better understands the procedural text and the accompanying images. Additionally, we carefully analyze our results and find that our approach learns meaningful dynamic representations of entities without any entity-level supervision. Although we achieve state-of-the-art results on RecipeQA, clearly",
        "type": "Document"
      },
      {
        "id": "c4a1badf-c232-489a-9699-692fcf4d8eea",
        "metadata": {
          "chunk_id": 68,
          "document_id": "1908.10449",
          "start_idx": 23672,
          "end_idx": 24120
        },
        "page_content": " the purpose of our study of partially observable text environments. Not surprisingly, QA-DQN performs worse in general on iNewsQA, in all experiments. As shown in Table TABREF16, the average number of sentences per document in iNewsQA is about 6 times more than in iSQuAD. This is analogous to games with larger maps in the RL literature, where the environment is partially observable. A better exploration (in our case, jumping) strategy may help",
        "type": "Document"
      },
      {
        "id": "4d9d04eb-d7d8-43cd-8acf-dcebd7dbe48e",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1812.07023",
          "start_idx": 17962,
          "end_idx": 18472
        },
        "page_content": ": not concatenating the question, answer pairs before encoding them, the auxiliary decoder module, and using the Time-Extended FiLM module for feature extraction. These, combined with using scheduled sampling and running hyperparameter optimization over the validation set to select hyperparameters, give us the observed performance boost. We observe that our models generate fairly relevant responses to questions in the dialogues, and models with audio-visual inputs respond to audio-visual questions (e.g. \u201c",
        "type": "Document"
      },
      {
        "id": "2c45f58e-ecd6-4e13-9ca6-7360b4fdf0ae",
        "metadata": {
          "chunk_id": 73,
          "document_id": "1901.02262",
          "start_idx": 19166,
          "end_idx": 19597
        },
        "page_content": " positive labels to 0.9. Table 2 shows that our ensemble model, controlled with the NLG and Q&A styles, achieved state-of-the-art performance on the NLG and Q&A tasks in terms of Rouge-L. In particular, for the NLG task, our single model outperformed competing models in terms of both Rouge-L and Bleu-1. The capability of creating abstractive summaries from the question and passages contributed to its improvements over the state",
        "type": "Document"
      },
      {
        "id": "5f5a488a-b689-46d4-b32b-50ea0883fe6f",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1703.09684",
          "start_idx": 2106,
          "end_idx": 2596
        },
        "page_content": " evaluate both baseline and state-of-the-art VQA algorithms. We found that our proposed approach enables more nuanced comparisons of VQA algorithms, and helps us understand the benefits of specific techniques better. In addition, it also allowed us to answer several key questions about VQA algorithms, such as, `Is the generalization capacity of the algorithms hindered by the bias in the dataset?', `Does the use of spatial attention help answer specific question-types?', `How successful",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is an \"answer style\"?",
    "llm_answer": "An answer style is a style, like BIBREF14, corresponding to the target style at the beginning of the answer sentence (y1).  It is related to the context of the question in the answer sentence.  Other styles, like length and speaking styles, can also be controlled.\n",
    "context": [
      {
        "id": "1224529c-e428-495a-9ab7-da98dce11214",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1901.02262",
          "start_idx": 2098,
          "end_idx": 2374
        },
        "page_content": " x^{p_k}_{L}\\rbrace $ , and an answer style $s$ , an RC system outputs an answer $y = \\lbrace y_1, \\ldots , y_T \\rbrace $ conditioned on the style. In short, for inference, given a set of 3-tuples $(x^q, \\lbrace x^{p_k}\\rbrace , s)$ , the system predicts $P(y)$ . The training",
        "type": "Document"
      },
      {
        "id": "233f3d98-fa10-4685-b2e6-baa1d2edabf6",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1901.02262",
          "start_idx": 9817,
          "end_idx": 10299
        },
        "page_content": " $t$ depend only on the known outputs at positions less than $t$ . Moreover, to be able to make use of multiple answer styles within a single system, our model introduces an artificial token corresponding to the target style at the beginning of the answer sentence ( $y_1$ ), like BIBREF14 . At test time, the user can specify the first token to control the answer styles. This modification does not require any changes to the model architecture. Note that introducing the tokens on",
        "type": "Document"
      },
      {
        "id": "5906a004-7795-4512-a21a-cf20cea662dd",
        "metadata": {
          "chunk_id": 83,
          "document_id": "1901.02262",
          "start_idx": 22902,
          "end_idx": 23374
        },
        "page_content": " The key strength of our model is its high accuracy of generating abstractive summaries from the question and passages; our model achieved state-of-the-art performance in terms of Rouge-L on the Q&A and NLG tasks of MS MARCO 2.1 that have different answer styles BIBREF5 . The styles considered in this paper are only related to the context of the question in the answer sentence; our model will be promising for controlling other styles such as length and speaking styles",
        "type": "Document"
      },
      {
        "id": "c7a6c7ff-b7ec-42e4-b372-0dd2ddf1b638",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1905.08949",
          "start_idx": 9103,
          "end_idx": 9529
        },
        "page_content": " on deep question datasets, such as LearningQ BIBREF8 , shown in Section \"Generation of Deep Questions\" . The second factor is the answer type, i.e., the expected form of the answer, typically having four settings: (1) the answer is a text span in the passage, which is usually the case for factoid questions, (2) human-generated, abstractive answer that may not appear in the passage, usually the case for deep questions, (3)",
        "type": "Document"
      },
      {
        "id": "b87e923b-05c6-464a-abd4-466cc7f9d727",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1901.02262",
          "start_idx": 1960,
          "end_idx": 2184
        },
        "page_content": " with $J$ words $x^q = \\lbrace x^q_1, \\ldots , x^q_J\\rbrace $ , a set of $K$ passages, where each $k$ -th passage is composed of $L$ words $x^{p_k} = \\lbrace x^{p_k}_1, \\ldots , x^{p_k}_{L}\\rbrace $ , and an answer style $s$",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are the EAU text spans annotated?",
    "llm_answer": "Workers choose a label, highlight words in the hypothesis, and use at least half of the highlighted words to write an explanation for their decision.\n",
    "context": [
      {
        "id": "d084da0e-21bf-4cc2-a6a4-10e5c2d405b7",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1906.03338",
          "start_idx": 9744,
          "end_idx": 10174
        },
        "page_content": " the EAU span ( INLINEFORM0 ) and the words of the EAU-surrounding context ( INLINEFORM1 ). Additionally, we compute the element-wise subtraction of the source EAU vector from the target EAU vector, with the aim of modelling directions in distributional space, similarly to BIBREF25 . Words with no corresponding pre-trained word vector and empty sequences (e.g., no preceding context available) are treated as a zero-vector. Tree",
        "type": "Document"
      },
      {
        "id": "b9e7d003-f997-43da-b7f7-5b2c1047c1f7",
        "metadata": {
          "chunk_id": 41,
          "document_id": "2004.03744",
          "start_idx": 14905,
          "end_idx": 15352
        },
        "page_content": " explanations collected at the same time as the labels for these splits (see Figure FIGREF21). Statistics of e-SNLI-VE-2.0 are shown in Appendix SECREF39, Table TABREF40. As mentioned before, in order to submit the annotation of an image-sentence pair, three steps must be completed: workers must choose a label, highlight words in the hypothesis, and use at least half of the highlighted words to write an explanation for their decision. The last",
        "type": "Document"
      },
      {
        "id": "5655470c-4c2a-440d-b39a-7fd3330a3abc",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1906.03338",
          "start_idx": 19480,
          "end_idx": 19921
        },
        "page_content": " annotated (e.g., \u201cHowever, INLINEFORM0 A INLINEFORM1 \u201d). If annotations extend over the EAUs (e.g., only full sentences are annotated, \u201c INLINEFORM2 However, A INLINEFORM3 \u201d), such transformations could be performed automatically after a discourse parsing step. When inspecting the student essays corpus, we further observed that an EAU mining step should involve coreference resolution to better capture relations between EAUs that involve",
        "type": "Document"
      },
      {
        "id": "913286eb-889a-4de2-9e10-9b33c2c91c2d",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1906.03338",
          "start_idx": 9472,
          "end_idx": 9933
        },
        "page_content": " content-based representation we retrieve only discourse relations that are confined within the span of the argumentative unit. In the very frequent case that discourse features cross the boundaries of embedding context and EAU span, we only take them into account for INLINEFORM0 . We use the element-wise sum of 300-dimensional pre-trained GloVe vectors BIBREF24 corresponding to the words within the EAU span ( INLINEFORM0 ) and the words of the EAU-surround",
        "type": "Document"
      },
      {
        "id": "757f9acf-def0-47aa-9b67-c706227138ba",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1906.03338",
          "start_idx": 10472,
          "end_idx": 10892
        },
        "page_content": " node of the context which does not contain the EAU (ADVP in Figure FIGREF26 ). The sentiment for an EAU span ( INLINEFORM1 ) is assigned to the highest possible node covering the EAU span which does not contain the context sub-tree (S in Figure FIGREF26 ). The full-access ( INLINEFORM2 ) score is assigned to the lowest possible node which covers both the EAU span and its surrounding context (S' in Figure FIGREF26 ).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which Twitter corpus was used to train the word vectors?",
    "llm_answer": "1, 7, and 15 days of Twitter data.\n",
    "context": [
      {
        "id": "e43efa29-e6fa-40d8-a07c-7bfa50cc83c9",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1602.08741",
          "start_idx": 18542,
          "end_idx": 19007
        },
        "page_content": " We achieve results comparable with results obtained while training Word2Vec on traditional corpora, like Wikipedia and Web pages BIBREF3 , BIBREF11 . This is especially important because Twitter data is highly dynamic, and traditional sources are mostly static (rarely change over time). Thus verbal data acquired from Twitter may be used to estimate word vectors for neologisms, or determine other changes in word semantic, as soon as they appear in human speech.",
        "type": "Document"
      },
      {
        "id": "b4234314-4066-45a9-a57c-f1b829c8c031",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1602.08741",
          "start_idx": 16975,
          "end_idx": 17486
        },
        "page_content": " with algebraic operations over these vectors. Authors of BIBREF2 propose to assess quality of these vectors on task of exact prediction of these word relations. However, word vectors learned from Twitter seem to perform poorly on this task. We don\u2019t make systematic research on this subject here because it goes outside of the scope of the current paper, though it is an important direction of future studies. Twitter post often contains three special types of words: user mentions, hashtags and hyperlinks. It",
        "type": "Document"
      },
      {
        "id": "af7c958d-24a4-489a-8f48-fe7aa6cf0ce7",
        "metadata": {
          "chunk_id": 54,
          "document_id": "1701.00185",
          "start_idx": 19748,
          "end_idx": 20221
        },
        "page_content": " and the most parameters are set as same as Mikolov et al. BIBREF23 to train word vectors on Google News setting, except of vector dimensionality using 48 and minimize count using 5. For SearchSnippets, we train word vectors on Wikipedia dumps. For StackOverflow, we train word vectors on the whole corpus of the StackOverflow dataset described above which includes the question titles and post contents. For Biomedical, we train word vectors on all titles and abstracts of",
        "type": "Document"
      },
      {
        "id": "2fb77748-8ad3-49b9-8ee0-a6448a4d6ccd",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1602.08741",
          "start_idx": 13648,
          "end_idx": 14083
        },
        "page_content": "REF2 . It was mentioned that accuracy of estimated word vectors improves with size of corpus. Twitter provides an enormous amount of data, thus it is a perfect job for Word2Vec. We fix parameters for the model with following values: vector size of 300, min-freq of 40, context size of 5 and downsampling of 1e-3. We train our model subsequently with 1, 7 and 15 days of Twitter data (each starting with 07/21 and followed by subsequent",
        "type": "Document"
      },
      {
        "id": "16838a09-6199-4829-9521-b0d96f558096",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1604.05781",
          "start_idx": 4670,
          "end_idx": 5120
        },
        "page_content": " either not natural language text, or it is Twitter-specific, or both. Documents were broken into individual words (unigrams) on whitespace. Casing information was retained, as we will use it for our Named Entity analysis, but otherwise all words were considered lowercase only (see also the Appendix). Stemming BIBREF30 and lemmatization BIBREF31 were not performed. Causal documents were chosen to contain one occurrence only of the exact unigrams:",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does proposed word embeddings compare to Sindhi fastText word representations?",
    "llm_answer": "The proposed Sindhi word embeddings have surpassed SdfastText in intrinsic evaluation matrices.\n",
    "context": [
      {
        "id": "2aa9b408-55b3-464a-aa20-95b53c1bc6f0",
        "metadata": {
          "chunk_id": 125,
          "document_id": "1911.12579",
          "start_idx": 46126,
          "end_idx": 46618
        },
        "page_content": " same vocabulary because of character-level learning of word representations and sub-sampling approaches in SG and CBoW. Our proposed Sindhi word embeddings have surpassed SdfastText in the intrinsic evaluation matrix. Also, the vocabulary of SdfastText is limited because they are trained on a small Wikipedia corpus of Sindhi Persian-Arabic. We will further investigate the extrinsic performance of proposed word embeddings on the Sindhi text classification task in the future. The proposed",
        "type": "Document"
      },
      {
        "id": "a5b5d165-aa10-4f08-8a42-4374855cbf5c",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1911.12579",
          "start_idx": 6379,
          "end_idx": 6878
        },
        "page_content": " corpus. Generate word embeddings using GloVe, CBoW, and SG Word2Vec algorithms also evaluate and compare them using the intrinsic evaluation approaches of cosine similarity matrix and WordSim353. We are the first to evaluate SdfastText word representations and compare them with our proposed Sindhi word embeddings. The remaining sections of the paper are organized as; Section SECREF2 presents the literature survey regarding computational resources, Sindhi corpus construction, and word embedding",
        "type": "Document"
      },
      {
        "id": "a77c97d7-ff9d-4160-ba7c-11cd15af8854",
        "metadata": {
          "chunk_id": 126,
          "document_id": "1911.12579",
          "start_idx": 46397,
          "end_idx": 46655
        },
        "page_content": " performance of proposed word embeddings on the Sindhi text classification task in the future. The proposed resources along with systematic evaluation will be a sophisticated addition to the computational resources for statistical Sindhi language processing.",
        "type": "Document"
      },
      {
        "id": "965f94a8-cb24-4e26-b5b7-e1242b0cd85d",
        "metadata": {
          "chunk_id": 117,
          "document_id": "1911.12579",
          "start_idx": 42864,
          "end_idx": 43393
        },
        "page_content": ". The large corpus obtained from multiple web resources is utilized for the training of word embeddings using SG, CBoW and Glove models. The intrinsic evaluation along with comparative results demonstrates that the proposed Sindhi word embeddings have accurately captured the semantic information as compare to recently revealed SdfastText word vectors. The SG yield best results in nearest neighbors, word pair relationship and semantic similarity. The performance of CBoW is also close to SG in all the evaluation matrices. The",
        "type": "Document"
      },
      {
        "id": "f0e54486-05ba-4289-8c8c-86943cfec9fa",
        "metadata": {
          "chunk_id": 121,
          "document_id": "1911.12579",
          "start_idx": 44514,
          "end_idx": 45073
        },
        "page_content": " refined further by creating custom benchmarks and the extrinsic evaluation approach will be employed for the performance analysis of proposed word embeddings. Moreover, we will also utilize the corpus using Bi-directional Encoder Representation Transformer BIBREF13 for learning deep contextualized Sindhi word representations. Furthermore, the generated word embeddings will be utilized for the automatic construction of Sindhi WordNet. In this paper, we mainly present three novel contributions of large corpus development contains large vocabulary of more",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many uniue words are in the dataset?",
    "llm_answer": "618,224\n",
    "context": [
      {
        "id": "9478230e-3422-4766-b535-cc55374230bb",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1912.13109",
          "start_idx": 2336,
          "end_idx": 2800
        },
        "page_content": " large variation. Spelling variation: There is no agreement on the spellings of the words which are mixed with English. For example to express love, a code mixed spelling, specially when used social platforms might be pyaar, pyar or pyr. Dataset: Based on some earlier work, only available labelled dataset had 3189 rows of text messages of average length of 116 words and with a range of 1, 1295. Prior work addresses this concern by using Transfer Learning on an",
        "type": "Document"
      },
      {
        "id": "beedd766-c9b4-4a6f-b65f-05e64e1a4624",
        "metadata": {
          "chunk_id": 21,
          "document_id": "2001.09332",
          "start_idx": 7980,
          "end_idx": 8493
        },
        "page_content": " it was also decided to replace every number present in the text with the particular $\\langle NUM \\rangle $ token; which probably also assumes a better representation in the embedding space (not separating into the various possible values). All the words were then transformed to lowercase (to avoid a double presence) finally producing a vocabulary of $618\\,224$ words. Note that among the special characters are also included punctuation marks, which therefore do not appear within the vocabulary. However, some",
        "type": "Document"
      },
      {
        "id": "625d564f-a387-4b58-8c66-f092e8901d86",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1910.12618",
          "start_idx": 12926,
          "end_idx": 13347
        },
        "page_content": " and $\\#\\lbrace d: w \\in d \\rbrace $ is the number of documents in which the word $w$ appears. In our work we considered only individual words, also commonly referred as 1-grams in the field of natural language processing (NLP). The methodology can be easily extended to $n$-grams (groups of $n$ consecutive words), but initial experiments showed that it did not bring any significant improvement over 1-grams. The second",
        "type": "Document"
      },
      {
        "id": "d756222b-9b91-49e5-9f7a-e2196c79c298",
        "metadata": {
          "chunk_id": 41,
          "document_id": "1602.08741",
          "start_idx": 14742,
          "end_idx": 15167
        },
        "page_content": "and pair) is \"russian\u0439\u0435\u043b\u044c\", Yale, the name of university in the USA. There are no \"missing\" words in 15-days corpus. Training the model on 15-days corpus took 8 hours on our machine with 2 cores and 4Gb of RAM. We have an intuition that further improvements are possible with larger corpus. Comparing our results to ones reported by RUSSE participants, we conclude that our best result of 0.598 is comparable to other results",
        "type": "Document"
      },
      {
        "id": "decc396a-e61e-463a-93ed-ff0db0865ae0",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1809.04686",
          "start_idx": 6349,
          "end_idx": 6815
        },
        "page_content": " sub-word vocabulary BIBREF37 , BIBREF38 of 32K units from all source and target training data. We use this sub-word vocabulary for all of our experiments below. The Amazon reviews dataset BIBREF39 is a multilingual sentiment classification dataset, providing data for four languages - English (En), French (Fr), German (De), and Japanese. We use the English and French datasets in our experiments. The dataset contains 6,000 documents in the train and test portions",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which baseline methods are used?",
    "llm_answer": "K-means (TF), K-means (TF-IDF), SkipVec, RecNN (Ave.), RecNN (SNLI + MultiNLI), and Paragraph Vector based clustering methods.  TF-IDF, S2S, IR, and Proposed+IR are also used as baseline models for ranking comments.\n",
    "context": [
      {
        "id": "c9d62915-21d1-42b6-a16c-8d9d28de6e0c",
        "metadata": {
          "chunk_id": 91,
          "document_id": "1902.06843",
          "start_idx": 35139,
          "end_idx": 35416
        },
        "page_content": "s by -0.27. Baselines: To test the efficacy of our multi-modal framework for detecting depressed users, we compare it against existing content, content-network, and image-based models (based on the aforementioned general image feature, facial presence, and facial expressions.)",
        "type": "Document"
      },
      {
        "id": "5aa6c306-2f30-44e9-91e3-5e2c343bc7c3",
        "metadata": {
          "chunk_id": 56,
          "document_id": "1701.00185",
          "start_idx": 20521,
          "end_idx": 21012
        },
        "page_content": "ans, Skip-thought Vectors, Recursive Neural Network and Paragraph Vector based clustering methods, four baseline clustering methods are directly based on the popular unsupervised dimensionality reduction methods as described in Section SECREF11 . We further compare our approach with some other non-biased neural networks, such as bidirectional RNN. More details are listed as follows: K-means K-means BIBREF42 on original keyword features which are respectively weighted with term frequency",
        "type": "Document"
      },
      {
        "id": "7d4754c7-4ea8-4d20-9fb6-3d1e50bf0cf6",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1809.04960",
          "start_idx": 22497,
          "end_idx": 22960
        },
        "page_content": " the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types of comments as described in the above retrieval evaluation setting: Correct, Plausible, Popular, and Random. We rank the candidate comment set with four models (TF-IDF, S2S, IR, and Proposed+IR), and record the types of top-1 comments. Figure FIGREF40 shows the percentage of different types of top-1 comments generated by each model. It shows",
        "type": "Document"
      },
      {
        "id": "5c6209f3-cef8-4a17-88a8-a660a1a73e1e",
        "metadata": {
          "chunk_id": 76,
          "document_id": "1701.00185",
          "start_idx": 27376,
          "end_idx": 27736
        },
        "page_content": " and NMI performance of our proposed approaches and four baseline methods, K-means, SkipVec, RecNN and Para2vec based clustering methods. Intuitively, we get a general observation that (1) BoW based approaches, including K-means (TF) and K-means (TF-IDF), and SkipVec based approaches perform not well; (2) RecNN based approaches, both RecNN (Ave.) and RecNN (",
        "type": "Document"
      },
      {
        "id": "b2775552-5ca7-4d50-97ee-2a3b6b097611",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1810.09774",
          "start_idx": 6998,
          "end_idx": 7473
        },
        "page_content": " SNLI + MultiNLI we report the baseline using the SNLI test data. All the experimental combinations are listed in Table 1 . Examples from the selected datasets are provided in Table 2 . To be more precise, we vary three things: training dataset, model and testing dataset. We should qualify this though, since the three datasets we look at, can also be grouped by text domain/genre and type of data collection, with MultiNLI and SNLI using the same data collection style, and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much is the BLEU score?",
    "llm_answer": "The context provides many BLEU scores, but no single \"the\" BLEU score.\n",
    "context": [
      {
        "id": "d057750d-74f4-4533-8d28-f8d0324733fc",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1707.00110",
          "start_idx": 10538,
          "end_idx": 10965
        },
        "page_content": " size 128. We train for at most 200,000 steps (see Figure 3 for sample learning curves). BLEU scores are calculated on tokenized data using the multi-bleu.perl script in Moses. We decode using beam search with a beam size of 10 BIBREF18 . Table 1 shows the BLEU scores of our model on different sequence lengths while varying $K$ . This is a study of the trade-off between computational time and representational power. A large",
        "type": "Document"
      },
      {
        "id": "d29f233d-49ab-4bb0-9376-5a0d850c267c",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1910.11949",
          "start_idx": 12034,
          "end_idx": 12516
        },
        "page_content": "U BIBREF30 metric on the validation set for the VQG model training. BLEU is a measure of similitude between generated and target sequences of words, widely used in natural language processing. It assumes that valid generated responses have significant word overlap with the ground truth responses. We use it because in this case we have five different references for each of the generated questions. We obtain a BLEU score of 2.07. Our chatbot model instead, only have one reference",
        "type": "Document"
      },
      {
        "id": "ca00c04e-c52f-45f1-92c2-b5393472b8eb",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1809.08731",
          "start_idx": 22358,
          "end_idx": 22769
        },
        "page_content": " expressed, e.g., by callison2006re, who doubted the meaningfulness of BLEU, and claimed that a higher BLEU score was neither a necessary precondition nor a proof of improved translation quality. Similarly, song2013bleu discussed BLEU being unreliable at the sentence or sub-sentence level (in contrast to the system-level), or for only one single reference. This was supported by isabelle-cherry-foster:2017:EM",
        "type": "Document"
      },
      {
        "id": "2b57faad-3707-4825-98b5-2c4e3e67efb8",
        "metadata": {
          "chunk_id": 81,
          "document_id": "2002.11893",
          "start_idx": 30351,
          "end_idx": 30691
        },
        "page_content": " as references (100 references on average), which results in high BLEU score. For user-side utterances, the BLEU score for TemplateNLG is 0.5780, while the BLEU score for SC-LSTM is 0.7858. For system-side, the two scores are 0.6828 and 0.8595. As exemplified in Table TABREF39, the gap between the two models can be attributed to that SC-L",
        "type": "Document"
      },
      {
        "id": "ab0400a3-2f73-41ae-bc81-a64631b41288",
        "metadata": {
          "chunk_id": 38,
          "document_id": "2001.00137",
          "start_idx": 14872,
          "end_idx": 15265
        },
        "page_content": "-STT combinations, thus varying rates of missing and incorrect words. The level of noise in the STT imbued sentences is denoted by a inverted BLEU (iBLEU) score ranging from 0 to 1. The inverted BLEU score is denoted in Eq. (DISPLAY_FORM23): where BLEU is a common metric usually used in machine translation tasks BIBREF21. We decide to showcase that instead of regular BLEU because it is more",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which datasets are used in experiments?",
    "llm_answer": "MCTest, CNN/Daily Mail, CBT, MSCOCO, Oxford-102, MNIST, CUB, Oxford, COCO, CIFAR-10, Biomedical.\n",
    "context": [
      {
        "id": "2222ff1c-48c2-4593-8cbe-f96acd7fec63",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1703.04617",
          "start_idx": 1343,
          "end_idx": 1868
        },
        "page_content": " can help attain better results on our competitive baselines. Recent advance on reading comprehension and question answering has been closely associated with the availability of various datasets. BIBREF0 released the MCTest data consisting of 500 short, fictional open-domain stories and 2000 questions. The CNN/Daily Mail dataset BIBREF1 contains news articles for close style machine comprehension, in which only entities are removed and tested for comprehension. Children's Book Test (CBT) BIBREF2 leverages named entities",
        "type": "Document"
      },
      {
        "id": "7d4af53c-0878-4ce8-8945-c54db6f33d53",
        "metadata": {
          "chunk_id": 147,
          "document_id": "1910.09399",
          "start_idx": 57115,
          "end_idx": 57512
        },
        "page_content": " MSCOCO, and Oxford-102 datasets, which facilitated the work on text-to-image synthesis for several papers released more recently. While most deep learning algorithms use MNIST BIBREF74 dataset as the benchmark, there are three main datasets that are commonly used for evaluation of proposed GAN models for text-to-image synthesis: CUB BIBREF75, Oxford BIBREF76, COCO BIBREF77, and CIFAR-10 BIBREF",
        "type": "Document"
      },
      {
        "id": "65333987-2358-483b-95cd-3ff028ff6a96",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      },
      {
        "id": "91672d1f-40de-4361-a563-5175325a46aa",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1703.09684",
          "start_idx": 2802,
          "end_idx": 3121
        },
        "page_content": " BIBREF3 , FM-IQA BIBREF4 , The VQA Dataset BIBREF1 , Visual7W BIBREF5 , and Visual Genome BIBREF6 . FM-IQA needs human judges and has not been widely used, so we do not discuss it further. Table 1 shows statistics for the other datasets. Following others BIBREF7 , BIBREF8 , BIBREF9 , we refer to the portion of The VQ",
        "type": "Document"
      },
      {
        "id": "64a25737-469b-4e82-875c-ba81cb2e5018",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1701.00185",
          "start_idx": 3790,
          "end_idx": 4304
        },
        "page_content": " this paper we use a new short text dataset, Biomedical, in the experiment to verify the effectiveness of our approach. Third, we put much effort on studying the influence of various different semantic features integrated in our self-taught CNN framework, which is not involved in the conference paper. For the purpose of reproducibility, we make the datasets and software used in our experiments publicly available at the website. The remainder of this paper is organized as follows: In Section SECREF2 , we first",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are new best results on standard benchmark?",
    "llm_answer": "The final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments.\n",
    "context": [
      {
        "id": "637d4fe3-c7c1-4e7c-a855-261351957759",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1701.02877",
          "start_idx": 4315,
          "end_idx": 4755
        },
        "page_content": " NER approaches, namely Stanford NER BIBREF34 , SENNA BIBREF35 and CRFSuite BIBREF36 (see Section \"NER Models and Features\" for details). Since the goal of this study is to compare NER performance on corpora from diverse domains and genres, seven benchmark NER corpora are included, spanning newswire, broadcast conversation, Web content, and social media (see Table 1 for details). These datasets were chosen such that they have been annot",
        "type": "Document"
      },
      {
        "id": "5e0f3225-070e-4250-8f70-93c083311ec4",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "d7a0b7aa-da14-4112-80c0-cc26c62fe83f",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "68c9fa82-7b71-4d3b-8d4e-70ded7bf95e4",
        "metadata": {
          "chunk_id": 21,
          "document_id": "1701.02877",
          "start_idx": 7791,
          "end_idx": 8297
        },
        "page_content": " studying NER performance on this genre gained momentum. So far, there have been no big evaluation efforts, such as ACE and OntoNotes, resulting in substantial amounts of gold standard data. Instead, benchmark corpora were created as part of smaller challenges or individual projects. The first such corpus is the UMBC corpus for Twitter NER BIBREF33 , where researchers used crowdsourcing to obtain annotations for persons, locations and organisations. A further Twitter NER corpus was created by BIBREF21",
        "type": "Document"
      },
      {
        "id": "ce9a5da7-b644-459d-89dc-6f6485bbc871",
        "metadata": {
          "chunk_id": 62,
          "document_id": "1701.02877",
          "start_idx": 22362,
          "end_idx": 22802
        },
        "page_content": " data gives little to no improvement, whereas adding positive examples drastically improves performance. Table UID9 shows results with size normalised precision (P), recall (R), and F1-Score (F1). The five lowest P, R and F1 values per method (CRFSuite, Stanford NER, SENNA) are in bold to highlight underperformers. Results for all corpora are summed with macro average. Comparing the different methods, the highest F1 results are achieved",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How better is performance compared to competitive baselines?",
    "llm_answer": "The final model \"Ours with Mask and Ordered Triplets\" outperforms baselines on all metrics in previously seen environments, increasing accuracy by 35% and 25% for Exact Match and Goal Match, respectively.  Other models outperform heuristic models, and the described model outperforms baselines by a clear margin in new environments.\n",
    "context": [
      {
        "id": "a98b3d51-106c-4190-b27f-eed96e43fcbb",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1707.03904",
          "start_idx": 20754,
          "end_idx": 21226
        },
        "page_content": " baselines, however, lag behind human performance by $16.4\\%$ and $32.1\\%$ for Quasar-S and Quasar-T respectively, indicating the strong potential for improvement. Interestingly, for human performance we observe that non-experts are able to match or beat the performance of experts when given access to the background corpus for searching the answers. We also emphasize that the human performance is limited by either the knowledge of the experts, or the usefulness of the",
        "type": "Document"
      },
      {
        "id": "d7a0b7aa-da14-4112-80c0-cc26c62fe83f",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "abc51b19-3db2-4cbf-ac46-ff97aa2fa1d1",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1603.07044",
          "start_idx": 14997,
          "end_idx": 15515
        },
        "page_content": " task A already had a substantial amount of training data. Table 4 gives the final comparison between different models (we only list the MAP score because it is the official score for the challenge). Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited",
        "type": "Document"
      },
      {
        "id": "933da131-cb85-477b-93eb-7374189de506",
        "metadata": {
          "chunk_id": 54,
          "document_id": "1707.03904",
          "start_idx": 20334,
          "end_idx": 20754
        },
        "page_content": " accuracy on the set of instances for which the answer is in context, however, a search accuracy of only $65\\%$ means its overall performance is lower. This can improve with improved retrieval. For Quasar-T, both the neural models significantly outperform the heuristic models, with BiDAF getting the highest F1 score of $28.5\\%$ . The best performing baselines, however, lag behind human performance by $16.4\\%$ and $32",
        "type": "Document"
      },
      {
        "id": "fc23cf7f-21ff-43cd-8d00-ef5914322400",
        "metadata": {
          "chunk_id": 68,
          "document_id": "1810.00663",
          "start_idx": 26280,
          "end_idx": 26763
        },
        "page_content": " room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM. Note that the average number of actions in the ground truth output sequences is 7.07 for the Test-New set. Our model's predictions are just INLINEFORM1 edits off on average from the correct navigation plans. This",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What 6 language pairs is experimented on?",
    "llm_answer": "English, Spanish, French, German, Russian, Italian.\n",
    "context": [
      {
        "id": "e8fb7d0e-86e1-4332-bb4a-d8109e4146e4",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1910.04269",
          "start_idx": 19553,
          "end_idx": 20023
        },
        "page_content": ", the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Kat",
        "type": "Document"
      },
      {
        "id": "e8ffc8fd-73a2-4096-87aa-87f5967c213f",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1910.11493",
          "start_idx": 5173,
          "end_idx": 5630
        },
        "page_content": " languages. Data for all but four languages (Basque, Kurmanji, Murrinhpatha, and Sorani) are extracted from English Wiktionary, a large multi-lingual crowd-sourced dictionary with morphological paradigms for many lemmata. 20 of the 100 language pairs are either distantly related or unrelated; this allows speculation into the relative importance of data quantity and linguistic relatedness. For each language, the basic data consists of triples of the form",
        "type": "Document"
      },
      {
        "id": "7da029d3-0e6a-48d3-9dd2-3cfd37690294",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1910.04269",
          "start_idx": 6853,
          "end_idx": 7342
        },
        "page_content": " comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (",
        "type": "Document"
      },
      {
        "id": "78dd82c6-c6af-4015-b4e0-f73037eb863b",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1904.05862",
          "start_idx": 15343,
          "end_idx": 15454
        },
        "page_content": "letter++ experiments, and Tatiana Likhomanenko for providing convolutional language models for our experiments.",
        "type": "Document"
      },
      {
        "id": "f62571fd-1f0a-42cd-9ad8-3a37e2e9d943",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1910.05456",
          "start_idx": 10144,
          "end_idx": 10648
        },
        "page_content": " choose languages with different degrees of relatedness and varying morphological similarity to English, Spanish, and Zulu. We limit our experiments to languages which are written in Latin script. As an estimate for morphological similarity we look at the features from the Morphology category mentioned in The World Atlas of Language Structures (WALS). An overview of the available features as well as the respective values for our set of languages is shown in Table TABREF13. We decide on Basque (EUS),",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they enrich the positional embedding with length information",
    "llm_answer": "The context describes two methods:  one sums positional vectors with the difference between sentence length and position, and the other with the proportion of sentence covered.\n",
    "context": [
      {
        "id": "2caeadb0-1283-4763-8df1-b27f40773b69",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1910.10408",
          "start_idx": 4404,
          "end_idx": 4859
        },
        "page_content": " input. Thus, in order to enrich input embeddings in source and target with positional information, they are summed with positional vectors of the same dimension $d$, which are computed with the following trigonometric encoding ($\\text{PE}$): for $i=1,\\ldots ,d/2$. Recently, an extension of the positional encoding BIBREF11 was proposed to model the output length for text summarization. The goal is achieved by computing the distance from every position",
        "type": "Document"
      },
      {
        "id": "5aa5ddd7-ebd4-4933-b086-89baf948459a",
        "metadata": {
          "chunk_id": 18,
          "document_id": "2001.00137",
          "start_idx": 7419,
          "end_idx": 7883
        },
        "page_content": " our case, since our data are formed of single sentences, the segment is 1 until the first `[SEP]' character appears (indicating segment A) and then it becomes 0 (segment B). The position embedding layer, as the name indicates, adds information related to the token's position in the sentence. This prepares the data to be considered by the layers of vanilla bidirectional transformers, which outputs a hidden embedding that can be used by our novel layers of deno",
        "type": "Document"
      },
      {
        "id": "96efd096-9759-4d74-93a7-9071f68206fc",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1910.10408",
          "start_idx": 9446,
          "end_idx": 9953
        },
        "page_content": "ing strategy, we specifically aim; i) to decouple the performance of the baseline NMT model from that of the additional length information, ii) control the level of aggressiveness that can come from the data (length token) and the model (length encoding), and iii) make the approaches versatile to any pre-trained model. More importantly, it will allow to transform any NMT model to an output length aware version, while getting better improvements on the quality of the generated sequences. Our experiments",
        "type": "Document"
      },
      {
        "id": "43af6182-0b66-4657-842d-597d1cd346e5",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1910.10408",
          "start_idx": 4645,
          "end_idx": 5163
        },
        "page_content": " model the output length for text summarization. The goal is achieved by computing the distance from every position to the end of the sentence. The new length encoding is present only in the decoder network as an additional vector summed to the input embedding. The authors proposed two different variants. The first variant replaces the variable pos in equations (1-2) with the difference $len - pos$, where len is the sentence length. The second variant attempts to model the proportion of the sentence that has been",
        "type": "Document"
      },
      {
        "id": "d999f24b-4c79-4c30-b2c8-cc354ba6834a",
        "metadata": {
          "chunk_id": 40,
          "document_id": "1910.12618",
          "start_idx": 16003,
          "end_idx": 16518
        },
        "page_content": " concatenate all the vector representations of a sentence together beforehand and result in a network with too many parameters to be trained correctly with our number of available documents. Recall that we decided to train our own vector representation of words instead of using an already available one. In order to obtain the embedding, the texts are first converted into a sequence of integers: each word is given a number ranging from 1 to $V$, where $V$ is the vocabulary size (0 is used for padding or unknown",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they condition the output to a given target-source class?",
    "llm_answer": "The condition vector, which is the class label (text string), is fed to both the generator and discriminator.  The class label must be related to the real data.\n",
    "context": [
      {
        "id": "837e2e34-2172-4269-99ba-ee0850c1e108",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1910.09399",
          "start_idx": 18297,
          "end_idx": 18765
        },
        "page_content": " tailored outputs. In Figure FIGREF14, the condition vector is the class label (text string) \"Red bird\", which is fed to both the generator and discriminator. It is important, however, that the condition vector is related to the real data. If the model in Figure FIGREF14 was trained with the same set of real data (red birds) but the condition text was \"Yellow fish\", the generator would learn to create images of red birds when conditioned with the text \"Yellow fish",
        "type": "Document"
      },
      {
        "id": "3f30cfc3-ee45-4616-bbb4-6d47b91f5a8d",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1909.00694",
          "start_idx": 10558,
          "end_idx": 10948
        },
        "page_content": " output is the final hidden state corresponding to the special classification tag ([CLS]). For the details of ${\\rm Encoder}$, see Sections SECREF30. We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\mathcal {L}_{\\rm AL}$, $\\math",
        "type": "Document"
      },
      {
        "id": "233f3d98-fa10-4685-b2e6-baa1d2edabf6",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1901.02262",
          "start_idx": 9817,
          "end_idx": 10299
        },
        "page_content": " $t$ depend only on the known outputs at positions less than $t$ . Moreover, to be able to make use of multiple answer styles within a single system, our model introduces an artificial token corresponding to the target style at the beginning of the answer sentence ( $y_1$ ), like BIBREF14 . At test time, the user can specify the first token to control the answer styles. This modification does not require any changes to the model architecture. Note that introducing the tokens on",
        "type": "Document"
      },
      {
        "id": "4ed1ff59-03b9-415a-841f-8e5f8ad6a294",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1911.00069",
          "start_idx": 9323,
          "end_idx": 9589
        },
        "page_content": "}^{\\prime }= \\frac{\\mathbf {y}}{||\\mathbf {y}||}$ for each target-language word embedding $\\mathbf {y}$. Next, we add an orthogonality constraint to (DISPLAY_FORM14) such that $\\mathbf {M}$ is an orthogonal matrix, i.e., $\\mathbf {M}^\\mathrm {T}\\mathbf {M} = \\mathbf",
        "type": "Document"
      },
      {
        "id": "e4b5656f-ad5d-4ea9-ac3c-e6aa6c263a66",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1902.00330",
          "start_idx": 12947,
          "end_idx": 13482
        },
        "page_content": " indicates the output of our model. After pre-training the global encoder, we start using the entity selector to choose the target entity for each mention and encode these selections. In the entity selector module, we choose the target entity from candidate set based on the results of local and global encoder. In the process of sequence disambiguation, each selection result will have an impact on subsequent decisions. Therefore, we transform the choice of the target entity into a reinforcement learning problem and view the entity",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is this library implemented into Torch or is framework agnostic?",
    "llm_answer": "PyTorch.\n",
    "context": [
      {
        "id": "fc5ebb81-c489-42d5-94dd-ceb341f4ab49",
        "metadata": {
          "chunk_id": 4,
          "document_id": "2002.00876",
          "start_idx": 1722,
          "end_idx": 2323
        },
        "page_content": " with the complexity of research in deep structured prediction. With this challenge in mind, we introduce Torch-Struct with three specific contributions: Modularity: models are represented as distributions with a standard flexible API integrated into a deep learning framework. Completeness: a broad array of classical algorithms are implemented and new models can easily be added in Python. Efficiency: implementations target computational/memory efficiency for GPUs and the backend includes extensions for optimization. In this system description, we first motivate the approach taken by the library",
        "type": "Document"
      },
      {
        "id": "d6676cdd-0528-4d56-b843-746c5bdaeaff",
        "metadata": {
          "chunk_id": 36,
          "document_id": "2002.00876",
          "start_idx": 12589,
          "end_idx": 13120
        },
        "page_content": " naively, dynamic programming algorithms in Python are prohibitively slow. As such Torch-Struct provides key primitives to help batch and vectorize these algorithms to take advantage of GPU computation and to minimize the overhead of backpropagating through chart-based dynamic programmming. Figure FIGREF17 shows the impact of these optimizations on the core algorithms. The commutative properties of semiring algorithms allow flexibility in the order in which we compute $A(\\ell )$. Typical implementations of dynamic programming",
        "type": "Document"
      },
      {
        "id": "e1462db7-41b2-49b7-80b5-8f5a457688f4",
        "metadata": {
          "chunk_id": 15,
          "document_id": "2002.00876",
          "start_idx": 5684,
          "end_idx": 6163
        },
        "page_content": " amount of user code, the implementation achieves near perfect accuracy on the ListOps dataset. The library design of Torch-Struct follows the distributions API used by both TensorFlow and PyTorch BIBREF29. For each structured model in the library, we define a conditional random field (CRF) distribution object. From a user's standpoint, this object provides all necessary distributional properties. Given log-potentials (scores) output from a deep network $\\ell $, the user can",
        "type": "Document"
      },
      {
        "id": "01240890-c4fc-4d2a-aeb6-986458363884",
        "metadata": {
          "chunk_id": 7,
          "document_id": "2002.00876",
          "start_idx": 3079,
          "end_idx": 3655
        },
        "page_content": " integrate with deep learning such as Pyro BIBREF23, allow for specification and inference over some discrete domains. Most ambitiously, inference libraries such as Dyna BIBREF24 allow for declarative specifications of dynamic programming algorithms to support inference for generic algorithms. Torch-Struct takes a different approach and integrates a library of optimized structured distributions into a vectorized deep learning system. We begin by motivating this approach with a case study. While structured prediction is traditionally presented at the output layer, recent",
        "type": "Document"
      },
      {
        "id": "544e1004-b9f9-4133-bf31-5be1540f10da",
        "metadata": {
          "chunk_id": 46,
          "document_id": "2002.00876",
          "start_idx": 15991,
          "end_idx": 16529
        },
        "page_content": " vectorize important dynamic programming steps. In addition to the problems discussed so far, Torch-Struct also includes several other example implementations including supervised dependency parsing with BERT, unsupervised tagging, structured attention, and connectionist temporal classification (CTC) for speech. The full library is available at https://github.com/harvardnlp/pytorch-struct. In the future, we hope to support research and production applications employing structured models. We also believe the library provides a strong",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does this compare to traditional calibration methods like Platt Scaling?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "6c1950c3-d4e6-4e73-8afa-d9cd95555c88",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1909.01013",
          "start_idx": 11517,
          "end_idx": 11713
        },
        "page_content": ". Results show that strengthening bi-directional mapping consistency significantly improves the effectiveness over the state-of-the-art method, leading to the best results on a standard benchmark.",
        "type": "Document"
      },
      {
        "id": "a7b65e59-9859-41ef-ab1f-e20163a10b09",
        "metadata": {
          "chunk_id": 56,
          "document_id": "1910.04269",
          "start_idx": 21494,
          "end_idx": 21961
        },
        "page_content": "ConvNet and evaluated their effect on validation accuracy. This method is able to bypass the computational overhead of conventional approaches which depend on generation of spectrograms as a necessary pre-procesing step. We were able to achieve an accauracy of 93.7% using this technique. Next, we discussed the enhancement in performance of 2D-ConvNet using mixup augmentation, which is a recently developed technique to prevent over\ufb01tting on test data.This approach",
        "type": "Document"
      },
      {
        "id": "4d93321c-7baf-447f-867e-3d97d046990c",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1703.09684",
          "start_idx": 5561,
          "end_idx": 6066
        },
        "page_content": " that work poorly when deployed in the real-world. For related reasons, major benchmarks released in the last decade do not use simple accuracy for evaluating image recognition and related computer vision tasks, but instead use metrics such as mean-per-class accuracy that compensates for unbalanced categories. For example, on Caltech-101 BIBREF13 , even with balanced training data, simple accuracy fails to address the fact that some categories were much easier to classify than others (e.g., faces and",
        "type": "Document"
      },
      {
        "id": "7df5e2a9-f532-4240-aaba-69fb05986e82",
        "metadata": {
          "chunk_id": 67,
          "document_id": "1908.08419",
          "start_idx": 24426,
          "end_idx": 24948
        },
        "page_content": " of relabeled data are shown in Fig. FIGREF45 , where x-axis represents the number of iterations and y-axis denotes the INLINEFORM0 -score of the segmenter. Scoring strategy shows consistent improvements over uncertainty sampling in the early iterations, indicating that scoring strategy is more capable of selecting representative samples. Furthermore, we also investigate the relations between the best INLINEFORM0 -score and corresponding number of iteration on two sampling strategies, which is depicted in Fig. FIGREF",
        "type": "Document"
      },
      {
        "id": "d32fa265-9222-412d-be38-fcfb98100e2d",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1909.00430",
          "start_idx": 17515,
          "end_idx": 18046
        },
        "page_content": " accuracy, and matching it on F1. We report significance tests for the robustness of the method under random parameter initialization. Our reported numbers are averaged over five random initialization. Since the datasets are unbalanced w.r.t the label distribution, we report both accuracy and macro-F1. The XR training is also more stable than the other semi-supervised baselines, achieving substantially lower standard deviations across different runs. In each experiment in this section we estimate the proportions using the Sem",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What's the input representation of OpenIE tuples into the model?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "6812fba9-cfc0-4b70-b159-395216a1aefd",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1905.13413",
          "start_idx": 0,
          "end_idx": 383
        },
        "page_content": "Open information extraction (IE, sekine2006demand, Banko:2007:OIE) aims to extract open-domain assertions represented in the form of $n$ -tuples (e.g., was born in; Barack Obama; Hawaii) from natural language sentences (e.g., Barack Obama was born in Hawaii). Open IE started from rule-based BIBREF0 and syntax-driven systems BIBREF1 , BIBREF2 , and recently has used neural networks",
        "type": "Document"
      },
      {
        "id": "6bfbf5e4-1e12-4f38-980f-0c0d775116f1",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1704.05572",
          "start_idx": 9274,
          "end_idx": 9770
        },
        "page_content": " ordering constraint (third group in Table 1 ). Its worth mentioning that TupleInf only combines parallel evidence i.e. each tuple must connect words in the question to the answer choice. For reliable multi-hop reasoning using OpenIE tuples, we can add inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules for the Science domain is an open problem and potential avenue of future work. Comparing our method with",
        "type": "Document"
      },
      {
        "id": "deef9e54-533f-4ad3-971e-812f51c1256a",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1810.00663",
          "start_idx": 12567,
          "end_idx": 13009
        },
        "page_content": "hot encoded into vectors of dimensionality INLINEFORM7 , where INLINEFORM8 and INLINEFORM9 are the number of nodes and edges in INLINEFORM10 , respectively. Encoder layer: The model then uses two bidirectional Gated Recurrent Units (GRUs) BIBREF25 to independently process the information from INLINEFORM0 and INLINEFORM1 , and incorporate contextual cues from the surrounding embeddings in each sequence. The outputs of the encoder layer are",
        "type": "Document"
      },
      {
        "id": "02788853-e41f-4a1d-8b43-6c0ab3511b3a",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1905.00563",
          "start_idx": 12060,
          "end_idx": 12389
        },
        "page_content": " $(s^{\\prime },r^{\\prime })$ . In particular, we train an auto-encoder where the encoder is fixed to receive the $s$ and $\\langle s^{\\prime }, r^{\\prime }, o \\rangle $0 as one-hot inputs, and calculates $\\langle s^{\\prime }, r^{\\prime }, o \\rangle $1 in the same way as the DistMult and ConvE encoders respectively (using trained",
        "type": "Document"
      },
      {
        "id": "f7b3b479-cccf-4159-a5b1-9d7c987dc82f",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1909.08859",
          "start_idx": 15768,
          "end_idx": 16158
        },
        "page_content": " \\mathbb {R}^{2d \\times 1}$ where $\\varphi _o$ is a multilayer perceptron with $\\operatorname{tanh}$ activation function. The output module takes the output of the modeling module, encoding vector embeddings of the question-aware recipe and the entities $\\mathbf {Y}$, and the embedding of the answer $\\mathbf {A}$, and returns a similarity score which is used while determining the correct",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is CRWIZ already used for data collection, what are the results?",
    "llm_answer": "Yes, CRWIZ is used for data collection, and a dataset was collected and compared to a dataset from a lab setting with a single Wizard.  The results are not explicitly stated.\n",
    "context": [
      {
        "id": "37757042-777d-492e-ac9c-82c328c3f649",
        "metadata": {
          "chunk_id": 9,
          "document_id": "2003.05995",
          "start_idx": 3904,
          "end_idx": 4417
        },
        "page_content": "REF4 and discuss the advantages/disadvantages of both approaches. Finally, we present future work. Our contributions are as follows: The release of a platform for the CRWIZ Intelligent Wizard Interface to allow for the collection of dialogue data for longer complex tasks, by providing a dynamic selection of relevant dialogue acts. A survey of existing datasets and data collection platforms, with a comparison to the CRWIZ data collection for Wizarded crowdsourced data in task-based interactions. Table TABREF3",
        "type": "Document"
      },
      {
        "id": "7bcb5514-0a74-4878-8c4f-fcd948d06d8d",
        "metadata": {
          "chunk_id": 10,
          "document_id": "2003.05995",
          "start_idx": 4229,
          "end_idx": 4714
        },
        "page_content": "IZ data collection for Wizarded crowdsourced data in task-based interactions. Table TABREF3 gives an overview of prior work and datasets. We report various factors to compare to the CRWIZ dataset corresponding to columns in Table TABREF3: whether or not the person was aware they were talking to a bot; whether each dialogue had a single or multiple participants per role; whether the data collection was crowdsourced; and the modality of the interaction and the domain. As we see from",
        "type": "Document"
      },
      {
        "id": "c4ecfd93-f2d9-4efd-95b0-8d46cd391660",
        "metadata": {
          "chunk_id": 3,
          "document_id": "2003.05995",
          "start_idx": 1353,
          "end_idx": 1907
        },
        "page_content": " have traditionally not been scalable because of the scarcity of Wizard experts or the expense to train up workers. This was the situation with an initial study reported in BIBREF4, which was conducted in a traditional lab setting and where the Wizard (an academic researcher) had to learn, through training and reading manuals, how best to perform operations in our domain of emergency response. We present the CRWIZ Intelligent Wizard Interface that enables a crowdsourced Wizard to make intelligent, relevant choices without such intensive training by",
        "type": "Document"
      },
      {
        "id": "62e7022c-7216-4572-9ffb-90a0f0b3bf6b",
        "metadata": {
          "chunk_id": 7,
          "document_id": "2003.05995",
          "start_idx": 3074,
          "end_idx": 3590
        },
        "page_content": " to automate a conversational Intelligent Assistant (Fred), who acts as an intermediary between the operator and the multiple robotic systems BIBREF6, BIBREF7. Emergency response is clearly a high-stakes situation, which is difficult to emulate in a lab or crowdsourced data collection environment. Therefore, in order to foster engagement and collaboration, the scenario was gamified with a monetary reward given for task success. In this paper, we provide a brief survey of existing datasets and describe the CRWIZ",
        "type": "Document"
      },
      {
        "id": "34df580c-7305-49f9-ae44-9f449cada4e7",
        "metadata": {
          "chunk_id": 8,
          "document_id": "2003.05995",
          "start_idx": 3380,
          "end_idx": 3904
        },
        "page_content": " success. In this paper, we provide a brief survey of existing datasets and describe the CRWIZ framework for pairing crowdworkers and having half of them acting as Wizards by limiting their dialogue options only to relevant and plausible ones, at any one point in the interaction. We then perform a data collection and compare our dataset to a similar dataset collected in a more controlled lab setting with a single Wizard BIBREF4 and discuss the advantages/disadvantages of both approaches. Finally, we present future work",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What contextual features are used?",
    "llm_answer": "Contextual keywords, ratio of quotation marks, exclamation marks, modal verbs, stop words, type-token ratio, hedging, named entity types, POS n-grams, sentiment, subjectivity scores, spell-checking, readability features (Coleman-Liau, Flesch), argument lexicon features, and surface features (word lengths, sentence lengths).\n",
    "context": [
      {
        "id": "82d16133-b2ed-41ff-bdda-2f22997513c0",
        "metadata": {
          "chunk_id": 56,
          "document_id": "1907.02636",
          "start_idx": 20055,
          "end_idx": 20644
        },
        "page_content": "14 , we initialize the contextual features of each given token using the automatically extracted contextual keywords and jointly learn them during the process of training with the whole ANN model. To prove the effectiveness of the contextual features, we visualize the learned weights martix of each contextual keyword of contextual feature and show several examples in Fig. FIGREF28 . Each row of the matrix in each plot indicates the weights of contextual keywords for the given tokens. From this we see which contextual keyword were considered more important to represent the contextual",
        "type": "Document"
      },
      {
        "id": "1e627a2c-64b1-4fa5-ba0b-aec3dfbb96b6",
        "metadata": {
          "chunk_id": 60,
          "document_id": "2002.00652",
          "start_idx": 19012,
          "end_idx": 19544
        },
        "page_content": " context modeling methods in the task of SPC are not as effective as expected, since they do not show a significant advantage over the simple concatenation method. By a careful investigation on contextual phenomena, we summarize them in multiple hierarchies. Roughly, there are three kinds of contextual phenomena in questions: semantically complete, coreference and ellipsis. Semantically complete means a question can reflect all the meaning of its corresponding SQL. Coreference means a question contains pronouns, while ellipsis",
        "type": "Document"
      },
      {
        "id": "a6b29bb9-376b-44f0-98a0-79c305b4f0ce",
        "metadata": {
          "chunk_id": 39,
          "document_id": "2004.03034",
          "start_idx": 15301,
          "end_idx": 15680
        },
        "page_content": ", ratio of quotation marks, exclamation marks, modal verbs, stop words, type-token ratio, hedging BIBREF29, named entity types, POS n-grams, sentiment BIBREF30 and subjectivity scores BIBREF31, spell-checking, readibility features such as Coleman-Liau BIBREF32, Flesch BIBREF33, argument lexicon features BIBREF34 and surface features such as word lengths, sentence lengths, word",
        "type": "Document"
      },
      {
        "id": "a9ff3510-1417-42ef-9c4f-470ebb75414b",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1710.01492",
          "start_idx": 17786,
          "end_idx": 18151
        },
        "page_content": "52 . Features. Traditionally, systems for Sentiment Analysis on Twitter have relied on handcrafted features derived from word-level (e.g., great, freshly roasted coffee, becoming president) and character-level INLINEFORM0 -grams (e.g., bec, beco, comin, oming), stems (e.g., becom), lemmata (e.g., become, roast), punctuation (e.g., exclamation and question marks),",
        "type": "Document"
      },
      {
        "id": "f54ce504-5c6f-450d-8f87-6cf459c47cc0",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1910.07181",
          "start_idx": 12787,
          "end_idx": 13094
        },
        "page_content": " contexts $\\mathcal {C} = \\lbrace C_1, \\ldots , C_m\\rbrace $ and the corresponding embeddings $v_{(w,C_1)}, \\ldots , v_{(w,C_m)}$, we apply a self-attention mechanism to all embeddings, allowing the model to distinguish informative contexts from uninformative ones. The final embedding $v_{(w, \\mathcal {C})",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the data in RAFAEL labelled?",
    "llm_answer": "The authors projected word-sense annotations of publicly available corpora to supersenses and applied perceptron-trained Hidden Markov Model for sequence classification.  The system also analyses labels such as plant, animal or shape.\n",
    "context": [
      {
        "id": "91abd502-0ceb-4e9d-9c62-6946327241cc",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1605.08675",
          "start_idx": 5272,
          "end_idx": 5770
        },
        "page_content": " a separate test set, previously unused in development, and is checked manually. The results are shown in section SECREF93 and discussed in chapter SECREF6 . Finally, chapter SECREF7 concludes the paper. As stated in previous chapter, RAFAEL is a computer system solving a task of Polish text-based, open-domain, factoid question answering. It means that provided questions, knowledge base and returned answers are expressed in Polish and may belong to any domain. The system analyses the knowledge",
        "type": "Document"
      },
      {
        "id": "988d6569-ddc5-4302-8aa1-5e7121199602",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1605.08675",
          "start_idx": 14177,
          "end_idx": 14672
        },
        "page_content": ", but also other labels, such as plant, animal or shape. The authors projected word-sense annotations of publicly available corpora to supersenses and applied perceptron-trained Hidden Markov Model for sequence classification, obtaining precision and recall around 77 per cent. A general architectural scheme of RAFAEL (figure FIGREF11 ) has been inspired by similar systems developed for English; for examples see works by BIBREF22 and BIBREF23 . Two of the steps in the diagram concern offline",
        "type": "Document"
      },
      {
        "id": "bea234e4-2d49-4fc5-ba3d-9b3419ca0450",
        "metadata": {
          "chunk_id": 64,
          "document_id": "1605.08675",
          "start_idx": 24641,
          "end_idx": 25068
        },
        "page_content": " named entity recognition in RAFAEL, including a description of Quant and a mapping between question types and named entity types available in NERF and Liner2. An alternative being in focus of this work, i.e. DeepER approach, is thorougly discussed in chapter SECREF3 . RAFAEL may use any of the two approaches to entity recognition: NER (via NERF, Liner2 and Quant) or novel DeepER; this choice affects its overall performance",
        "type": "Document"
      },
      {
        "id": "98844a2c-c76c-4b40-a8b3-9999d93046fb",
        "metadata": {
          "chunk_id": 113,
          "document_id": "1605.08675",
          "start_idx": 41672,
          "end_idx": 42162
        },
        "page_content": "per-synset recall 88.15 per cent), which instead includes 14 false synsets. Evaluation of RAFAEL is typical for factoid QA systems: given a knowledge base and and questions, its responses are compared to the expected ones, prepared in advance. Section SECREF80 describes data used in this procedure, whereas section SECREF87 explains how an automatic evaluation is possible without human labour. The Polish Wikipedia serves as a knowledge base. It has been downloaded from a project site as",
        "type": "Document"
      },
      {
        "id": "599b2453-e1a3-4d89-b755-b8406c2b1780",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1810.06743",
          "start_idx": 6580,
          "end_idx": 7020
        },
        "page_content": "ational in some languages. For example, the Czech UD guidance uses a Coll value for the Number feature to denote mass nouns (for example, \"lidstvo\" \"humankind\" from the root \"lid\" \"people\"). UD represents a confederation of datasets BIBREF8 annotated with dependency relationships (which are not the focus of this work) and morphosyntactic descriptions. Each dataset is an annotated treebank, making it a resource of token-level annotations",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the fluctuation in the sense of the word and its neighbors measured?",
    "llm_answer": "Surrounding uniformity, which roughly corresponds to statistical fluctuation in the vectors of neighbor words.\n",
    "context": [
      {
        "id": "21dbad95-c873-4203-a5b2-15f5ad670698",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1709.08858",
          "start_idx": 759,
          "end_idx": 1292
        },
        "page_content": " the major sense, which is most frequently used. The information about a word's minor sense is subtle, and the effect of a minor sense is difficult to distinguish from statistical fluctuation. To measure the effect of a minor sense, this paper proposes to use the concept of surrounding uniformity. The surrounding uniformity roughly corresponds to statistical fluctuation in the vectors that correspond to the words in the neighbor. We have found that there is a difference in the surrounding uniformity between a monosemic word and",
        "type": "Document"
      },
      {
        "id": "e0ccd194-a011-4710-8496-37cf0900fca1",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1705.09665",
          "start_idx": 8370,
          "end_idx": 8945
        },
        "page_content": " use volatile language that rapidly changes across successive windows of time. To capture this intuition automatically, we start by defining word-level measures of specificity and volatility. We then extend these word-level primitives to characterize entire comments, and the community itself. Our characterizations of words in a community are motivated by methodology from prior literature that compares the frequency of a word in a particular setting to its frequency in some background distribution, in order to identify instances of linguistic variation BIBREF21 , BIBREF",
        "type": "Document"
      },
      {
        "id": "7b8debad-341b-4fdf-9616-16dff63e8ef9",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1709.08858",
          "start_idx": 15039,
          "end_idx": 15552
        },
        "page_content": " the accuracy of the test. To make the statistical test more accurate, we need more samples from the neighbors. At the same time, since we assume that we can measure the statistical fluctuation from the neighbors, we need to exclude words of a different nature from the neighbors. It is natural that the right number for a neighbor may be different according to the word. The number that we choose is the minimum value for the statistical test, and has room to adjust for improvement. We computed the neighbor and",
        "type": "Document"
      },
      {
        "id": "d7a760fb-a683-46af-89a4-537bd3407784",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1709.08858",
          "start_idx": 0,
          "end_idx": 460
        },
        "page_content": "Distributed representation of word sense provides us with the ability to perform several operations on the word. One of the most important operations on a word is to obtain the set of words whose meaning is similar to the word, or whose usage in text is similar to the word. We call this set the neighbor of the word. When a word has several senses, it is called a polysemic word. When a word has only one sense, it is called a monosemic word. We have observed",
        "type": "Document"
      },
      {
        "id": "5d261eaa-0463-4a63-9d19-b6ab5245fde4",
        "metadata": {
          "chunk_id": 84,
          "document_id": "1910.12618",
          "start_idx": 33113,
          "end_idx": 33548
        },
        "page_content": " sense) and on all the available data (i.e. everything is used as training) to compensate for the increased size of the vocabulary. We then calculated the distance of a few prominent words to the others. The analysis of the average cosine distance over $B=10$ runs for three major words is given by tables TABREF38 and TABREF39, and three other examples are given in the appendix tables TABREF57 and TABREF58. The first row corresponds",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does SPNet outperforms state-of-the-art abstractive summarization methods on evaluation metrics?",
    "llm_answer": "SPNet outperforms Pointer-Generator, Transformer, and other baseline methods by a large margin.\n",
    "context": [
      {
        "id": "877d0653-9813-4efd-988e-ce7e8b6b65cb",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1910.00825",
          "start_idx": 16549,
          "end_idx": 17119
        },
        "page_content": " over Pointer-Generator but is having a higher cost on training time and computing resources. We observe that SPNet outperforms other methods in all the automatic evaluation metrics with a big margin, as it incorporates all the three semantic scaffolds. Semantic slot contributes the most to SPNet's increased performance, bringing the largest increase on all automatic evaluation metrics. We also perform human evaluation to verify if our method's increased performance on automatic evaluation metrics entails better human perceived quality. We randomly select 100 test",
        "type": "Document"
      },
      {
        "id": "694219db-ba27-499b-be04-644d2c0d81ba",
        "metadata": {
          "chunk_id": 56,
          "document_id": "1910.00825",
          "start_idx": 20655,
          "end_idx": 21208
        },
        "page_content": " as the semantic scaffolds to improve abstractive summary quality. We also propose an automatic evaluation metric CIC that considers semantic slot relevance to serve as a complementary metric to ROUGE. SPNet outperforms baseline methods in both automatic and human evaluation metrics. It suggests that involving semantic scaffolds efficiently improves abstractive summarization quality in the dialog scene. Moreover, we can easily extend SPNet to other summarization tasks. We plan to apply semantic slot scaffold to news summarization. Specifically, we",
        "type": "Document"
      },
      {
        "id": "d6d67917-6d58-4cca-a00b-e61f69f49921",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1910.00825",
          "start_idx": 18207,
          "end_idx": 18733
        },
        "page_content": " This motivates us to collect a dialog summarization dataset with high-quality human-written summaries in the future. Results in the ranking evaluation show more differences between different summaries. SPNet outperforms Pointer-Generator with a large margin. Its performance is relatively close to the ground truth summary. Table TABREF25 shows an example summary from all models along with ground truth summary. We observe that Pointer-Generator ignores some essential fragments, such as the restaurant booking information (",
        "type": "Document"
      },
      {
        "id": "03989c13-67dd-4fed-8dce-94a38416c2fe",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1910.00825",
          "start_idx": 2432,
          "end_idx": 2913
        },
        "page_content": " delexicalized utterances for producing delexicalized summary, and fills in slot values to generate complete summary. Finally, we incorporate dialog domain scaffold by jointly optimizing dialog domain classification task along with the summarization task. We evaluate SPNet with both automatic and human evaluation metrics on MultiWOZ. SPNet outperforms Pointer-Generator BIBREF5 and Transformer BIBREF6 on all the metrics. BIBREF7 first applied modern neural models to abstractive",
        "type": "Document"
      },
      {
        "id": "9d93773a-007e-4f2d-8eaf-80a32f11d980",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1910.00825",
          "start_idx": 17413,
          "end_idx": 17884
        },
        "page_content": " summary on three indicators: relevance, conciseness and readability on a 1 to 5 scale, and rank the summary pair (tie allowed). We present human evaluation results in Table TABREF27. In the scoring part, our model outperforms Pointer-Generator in all three evaluation metrics. SPNet scored better than Pointer-Generator on relevance and readability. All generated summaries are relatively concise; therefore, they score very similar in conciseness. Ground truth is still",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is it expected to have speaker role, semantic slot and dialog domain annotations in real world datasets?",
    "llm_answer": "Yes.\n",
    "context": [
      {
        "id": "bdd48b70-abcc-4f33-a1cd-f7bd8b5a2a56",
        "metadata": {
          "chunk_id": 54,
          "document_id": "1910.00825",
          "start_idx": 19824,
          "end_idx": 20350
        },
        "page_content": " of SPNet still needs extra annotations for semantic scaffolds. For a dialog dataset, speaker role scaffold is a natural pattern for modeling. Most multi-domain dialog corpus has the domain annotation. While for texts, for example news, its topic categorization such as sports or entertainment can be used as domain annotation. We find that semantic slot scaffold brings the most significant improvement, but it is seldom explicitly annotated. However, the semantic slot scaffold can be relaxed to any critical entities in the",
        "type": "Document"
      },
      {
        "id": "10b2a8ca-9bdf-4203-b008-9c8943606f60",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1910.00825",
          "start_idx": 20141,
          "end_idx": 20655
        },
        "page_content": " explicitly annotated. However, the semantic slot scaffold can be relaxed to any critical entities in the corpus, such as team name in sports news or professional terminology in a technical meeting. We adapt a dialog generation dataset, MultiWOZ to an abstractive dialog summarization dataset. We propose SPNet, an end-to-end model that incorporates the speaker role, semantic slot and dialog domain as the semantic scaffolds to improve abstractive summary quality. We also propose an automatic evaluation metric C",
        "type": "Document"
      },
      {
        "id": "e4b3cd13-7c62-4785-a373-31f8e9492efb",
        "metadata": {
          "chunk_id": 47,
          "document_id": "2002.11893",
          "start_idx": 17952,
          "end_idx": 18486
        },
        "page_content": " We also obtained a binary label for each semantic tuple in the user state, which indicates whether this semantic tuple has been selected to be expressed by the user. This annotation directly illustrates the progress of the conversation. To evaluate the quality of the annotation of dialogue acts and states (both user and system states), three experts were employed to manually annotate dialogue acts and states for the same 50 dialogues (806 utterances), 10 for each goal type (see Section SECREF4). Since dialogue act annotation is",
        "type": "Document"
      },
      {
        "id": "28eaec66-d29a-442f-80e4-0404e8d15452",
        "metadata": {
          "chunk_id": 22,
          "document_id": "2002.11893",
          "start_idx": 8390,
          "end_idx": 8922
        },
        "page_content": " before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states. Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three",
        "type": "Document"
      },
      {
        "id": "340c7c6d-7e27-4d4e-85d6-0cbe28375f14",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1910.00825",
          "start_idx": 5168,
          "end_idx": 5645
        },
        "page_content": " incorporating dialog acts to perform abstractive summarization. Moreover, BIBREF28 first built a model to summarize audio-visual meeting data with an abstractive method. However, previous work has not investigated the utilization of semantic patterns in dialog, so we explore it in-depth in our work. As discussed above, state-of-the-art document summarizers are not applicable in conversation settings. We propose Scaffold Pointer Network (SPNet) based on Pointer-Generator B",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does new evaluation metric considers critical informative entities?",
    "llm_answer": "The new metric, Critical Information Completeness (CIC), is a recall of semantic slot information between a candidate summary and a reference summary, considering a set of delexicalized values in the reference summary.\n",
    "context": [
      {
        "id": "4976cf8c-0af1-45e6-bfaa-38fca59bbe06",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1910.00825",
          "start_idx": 13254,
          "end_idx": 13678
        },
        "page_content": " However, ROUGE did not take this into consideration. To address this drawback in ROUGE, we propose a new evaluation metric: Critical Information Completeness (CIC). Formally, CIC is a recall of semantic slot information between a candidate summary and a reference summary. CIC is defined as follows: where $V$ stands for a set of delexicalized values in the reference summary, $Count_{match}(v)$ is the number of values co-",
        "type": "Document"
      },
      {
        "id": "fba7109c-978a-4f4d-a628-9538e8d3cf9c",
        "metadata": {
          "chunk_id": 30,
          "document_id": "1905.08949",
          "start_idx": 11311,
          "end_idx": 11823
        },
        "page_content": ". To overcome this, BIBREF46 proposed a new metric to evaluate the \u201canswerability\u201d of a question by calculating the scores for several question-specific factors, including question type, content words, function words, and named entities. However, as it is newly proposed, it has not been applied to evaluate any NQG system yet. To accurately measure what makes a good question, especially deep questions, improved evaluation schemes are required to specifically investigate the mechanism of question asking. Many",
        "type": "Document"
      },
      {
        "id": "7de8d13e-a44b-4918-a827-2e4a363b6e00",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1909.00578",
          "start_idx": 2625,
          "end_idx": 3083
        },
        "page_content": " MT evaluation metrics BIBREF4, BIBREF18. QE models for MT make use of large post-edited datasets, and apply machine learning methods to predict post-editing effort scores and quality (good/bad) labels. We apply QE to summarization, focusing on linguistic qualities that reflect the readability and fluency of the generated texts. Since no post-edited datasets \u2013 like the ones used in MT \u2013 are available for summarization, we use instead the ratings assigned",
        "type": "Document"
      },
      {
        "id": "a1a0d143-2f56-41d7-8054-1ba6a473f2ca",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1911.03894",
          "start_idx": 13285,
          "end_idx": 13679
        },
        "page_content": "\ufffd, 3357 of \u201cCompany\u201d, 67 of \u201cProduct\u201d, 15 of \u201cPOI\u201d (Point of Interest) and 29 of \u201cFictional Character\u201d. A large proportion of the entity mentions in the treebank are multi-word entities. For NER we therefore report the 3 metrics that are commonly used to evaluate models: precision, recall, and F1 score. Here precision measures the percentage of entities found by the system that are correctly",
        "type": "Document"
      },
      {
        "id": "b3036530-5228-47f4-b1f0-9a1687f209da",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1709.10217",
          "start_idx": 4141,
          "end_idx": 4657
        },
        "page_content": ", in which only the released data can be used for training and developing. The other is an open evaluation that allow to collect external data for training and developing. For task 1, we use F1-score as evaluation metric. For the task-oriented dialogue systems, the best way for evaluation is to use the online human-computer dialogue. After finishing an online human-computer dialogue with a dialogue system, the human then manually evaluate the system by using the metrics of user satisfaction degree, dialogue flu",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are state of the art methods MMM is compared to?",
    "llm_answer": "FCNN classifier, MTMSNlarge, MAN classifier.  Also, simple baseline methods, MLP, MCB, and NMN.\n",
    "context": [
      {
        "id": "3cfa366a-046d-4ba0-9a92-cb3f45dde15d",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1910.00458",
          "start_idx": 22388,
          "end_idx": 22877
        },
        "page_content": " control experiments for investigation. The results in Table TABREF27, show that casting the fine-tuning process on three datasets into separate stages performs better, indicating that multi-stage training is also necessary. This verifies our MMM framework with coarse-tuning on out-of-domain datasets and fine-tuning on in-domain datesets. Previous results show that the MAN classifier shows improvement compared with the FCNN classifier, but we are also interested in how the performance",
        "type": "Document"
      },
      {
        "id": "d2f9d4dd-1e75-499a-a321-5a8647a5a263",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1910.00458",
          "start_idx": 2607,
          "end_idx": 3053
        },
        "page_content": " the former is about three times smaller. To tackle this issue, we investigate how to improve the generalization of BERT-based MCQA models with the constraint of limited training data using four representative MCQA datasets: DREAM, MCTest, TOEFL, and SemEval-2018 Task 11. We proposed MMM, a Multi-stage Multi-task learning framework for Multi-choice question answering. Our framework involves two sequential stages: coarse-tuning stage using out",
        "type": "Document"
      },
      {
        "id": "e60e0d3d-b592-4a3f-9ebd-b12edba8a33b",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1909.13375",
          "start_idx": 20788,
          "end_idx": 21299
        },
        "page_content": " For the other types, most of the improvement came from better preprocessing. A more detailed discussion could be found in Section (SECREF36). Notice that different BERTlarge models were used, so the comparison is less direct. Overall, our large models exhibits similar results to those of MTMSNlarge. For multi-span questions we achieve a significantly better performance. While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still",
        "type": "Document"
      },
      {
        "id": "42837a06-4572-4044-8126-3100084f04f6",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1703.09684",
          "start_idx": 24411,
          "end_idx": 24828
        },
        "page_content": " simple baseline methods routinely exceed more complex methods using simple accuracy BIBREF7 , BIBREF8 , BIBREF19 . On TDIUC, MLP surpasses MCB and NMN in terms of simple accuracy, but a closer inspection reveals that MLP's score is highly determined by performance on categories with a large number of examples, such as `absurd' and `object presence.' Using MPT, we find that both NMN and MCB outperform MLP. Inspect",
        "type": "Document"
      },
      {
        "id": "fabc9411-a5dc-46a6-8641-6373466f5ac1",
        "metadata": {
          "chunk_id": 68,
          "document_id": "1910.00458",
          "start_idx": 23438,
          "end_idx": 23891
        },
        "page_content": " itself. Table TABREF31 summarizes the results of BERT-Base model on the RACE dataset obtained by adding the coarse-tuning stage, adding the multi-task training together with DREAM, and adding the MAN module. From this table, we see that all three techniques can bring in improvements over the baseline model for the source dataset RACE, among which NLI coarse-tuning stage can help elevate the scores most. Since we found all parts of MMM can work well",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the problems related to ambiguity in PICO sentence prediction tasks?",
    "llm_answer": "The assignment of a single label to a sentence limits the quality of predictions for real-world reviewing tasks, because sentences can contain additional information related to other PICO classes.\n",
    "context": [
      {
        "id": "bbd8d312-d1c6-4718-a6ef-eb9153c2d95d",
        "metadata": {
          "chunk_id": 26,
          "document_id": "2001.11268",
          "start_idx": 10281,
          "end_idx": 10811
        },
        "page_content": " noise in the evaluation because as the network learns, it can assign correct tags to wrongly labelled data. We also show that sentence labels are often ambiguous, and that the assignment of a single label limits the quality of the predictions for their use in real-world reviewing tasks. We acknowledge that the assignment of classes such as `Results' or `Conclusions' to sentences is potentially valuable for many use-cases. However, those sentences can contain additional information related to the PICO classes of interest. In",
        "type": "Document"
      },
      {
        "id": "92198ea1-f581-4860-b70d-76574508910b",
        "metadata": {
          "chunk_id": 66,
          "document_id": "2001.11268",
          "start_idx": 26100,
          "end_idx": 26630
        },
        "page_content": " included an entity, whereas its sub-classes age, gender, condition and size averaged 10% each. In the remaining classes, these percentages were higher. F1 scores for correctly detecting that a sentence includes no PICO element exceeded 0.92 in all classes. This indicates that the addition of impossible answer elements was successful, and that the model learned a representation of how to discriminate PICO contexts. The scores for correctly predicting PICOs in positive scenarios are lower. These results are presented in Table",
        "type": "Document"
      },
      {
        "id": "21e12686-e7ae-4e7d-b6b3-5a0429bde32f",
        "metadata": {
          "chunk_id": 28,
          "document_id": "2001.11268",
          "start_idx": 11105,
          "end_idx": 11632
        },
        "page_content": " weakness of predicting labels for whole sentences is the practical usability of the predictions. We will show sentence highlighting as a potential use-case for focusing reader's attention to passages of interest. However, the data obtained through this method are not fine-grained enough for usage in data extraction, or for the use in pipelines for automated evidence synthesis. Therefore, we expand our experiments to include QA-BERT, a question-answering model that predicts the locations of PICO entities within sentences.",
        "type": "Document"
      },
      {
        "id": "b8511081-37df-4d7c-aed1-402f17edcfb2",
        "metadata": {
          "chunk_id": 67,
          "document_id": "2001.11268",
          "start_idx": 26630,
          "end_idx": 27130
        },
        "page_content": " The scores for correctly predicting PICOs in positive scenarios are lower. These results are presented in Table TABREF30. Here, two factors could influence this score in a negative way. First, labelled spans can be noisy. Training spans were annotated by crowd workers and the authors of the original dataset noted inter-annotator disagreement. Often, these spans include full stops, other punctuation or different levels of detail describing a PICO. The F1 score decreases if the model predicts a P",
        "type": "Document"
      },
      {
        "id": "fcb762db-2036-44f6-8c14-5c9542112826",
        "metadata": {
          "chunk_id": 32,
          "document_id": "2001.11268",
          "start_idx": 12852,
          "end_idx": 13416
        },
        "page_content": "-crafted rules. Our fine-tuning approach shows that an abstract as context, together with a combination of annotated PICO entities and SQuAD data can result in a system that outperforms contemporary entity recognition systems, while retaining general reading comprehension capabilities. A language processing model's performance is limited by its capability of representing linguistic concepts numerically. In this preliminary experiment, we used the PubMed corpus for sentence classification to show the quality of PICO sentence embeddings retrieved from BERT. We",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is knowledge stored in the memory?",
    "llm_answer": "Entities and relations in the text are modeled as memory slots.  Existing knowledge is not explicitly mentioned as a method of storage.  Information from curated knowledge bases is considered for question-answering, but the text focuses on the simpler case where all information is contained within the text.\n",
    "context": [
      {
        "id": "2c25009d-5913-45b2-959c-bcdb472d224d",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1909.00754",
          "start_idx": 13218,
          "end_idx": 13687
        },
        "page_content": " the belief, is decided empirically. We can interpret this hierarchical structure as the internal order for the memory processing, since from the daily life experience, people tend to attend to the most contemporary memories (system/user utterance) first and then attend to the older history (belief states). All of the parameters are shared between the attention modules. The concatenated working memory, $\\mathbf {r}_0$ , is then fed into a Multi-Layer Perceptron (ML",
        "type": "Document"
      },
      {
        "id": "bb8178dc-7d1a-49e7-8b7e-b48d6db7501a",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1706.07179",
          "start_idx": 1224,
          "end_idx": 1706
        },
        "page_content": " question-answering, we instead make an attempt at an end-to-end approach which directly models the entities and relations in the text as memory slots. While incorporating existing knowledge (from curated knowledge bases) for the purpose of question-answering BIBREF11 , BIBREF8 , BIBREF15 is an important area of research, we consider the simpler setting where all the information is contained within the text itself \u2013 which is the approach taken by many recent memory based neural",
        "type": "Document"
      },
      {
        "id": "162ae093-965d-41f5-a57d-6c8578e753b6",
        "metadata": {
          "chunk_id": 24,
          "document_id": "2001.06888",
          "start_idx": 9500,
          "end_idx": 9914
        },
        "page_content": " on the most recent tokens. Instead of using RNN, authors used LSTM. The long and short term memory capability of these networks helps them to keep in memory what is important and forget what is not necessary to remember. Equation DISPLAY_FORM6 formulates forget-gate of an LSTM neural network, eq. shows input-gate, eq. notes output-gate and eq. presents memory-cell. Finally, eq. shows the hidden part of an LSTM",
        "type": "Document"
      },
      {
        "id": "a8f75cd5-eac4-4a1d-8126-f5cc8abeda2e",
        "metadata": {
          "chunk_id": 21,
          "document_id": "1902.00330",
          "start_idx": 8140,
          "end_idx": 8466
        },
        "page_content": "IBREF11 . Then, we use Long Short-Term Memory (LSTM) networks to encode the contextual word sequence $\\lbrace w_c^1, w_c^2,..., w_c^n\\rbrace $ as a fixed-size vector $V_{m_t}$ . The description of entity is encoded as $D_{e_t^i}$ in the same way. Apart from the description of entity, there are many other valuable information",
        "type": "Document"
      },
      {
        "id": "4a0042d1-d3ce-4a02-8dfd-d4690478eb16",
        "metadata": {
          "chunk_id": 41,
          "document_id": "1703.04617",
          "start_idx": 11824,
          "end_idx": 12097
        },
        "page_content": "L$ for the left child input and $\\mathbf {f}_t^R$ for the right. The memory cell $\\mathbf {c}_t$ considers each child's cell vector, $\\mathbf {c}_{t-1}^L$ and $\\mathbf {c}_{t-1}^R$ , which are gated by the left forget gate $\\mathbf {f}_t^L$ and right forget gate $\\mathbf {",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they measure the diversity of inferences?",
    "llm_answer": "The context describes measuring NE and token/type diversity, but does not discuss measuring the diversity of inferences.  The answer is not found in the context.\n",
    "context": [
      {
        "id": "d41b639a-933b-4970-aa2c-b151a6e43a12",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1701.02877",
          "start_idx": 11428,
          "end_idx": 11888
        },
        "page_content": " training data size for subsequent experiments. In order to compare corpus diversity across genres, we measure NE and token/type diversity (following e.g. BIBREF2 ). Note that types are the unique tokens, so the ratio can be understood as ratio of total tokens to unique ones. Table 4 shows the ratios between the number of NEs and the number of unique NEs per corpus, while Table 5 reports the token/type ratios. The lower those ratios are, the more diverse a",
        "type": "Document"
      },
      {
        "id": "b4f878c5-1850-4344-a310-9dbe252199f7",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1911.03243",
          "start_idx": 7494,
          "end_idx": 8068
        },
        "page_content": " and modality extracted from the question. Final reported numbers on labelled argument detection rates are based on bipartite aligned arguments passing Strict-Match. We later manually estimate the rate of correct equivalences missed by this conservative method. As we will see, our evaluation heuristics, adapted from those in BIBREF5, significantly underestimate agreement between annotations, hence reflecting performance lower bounds. Devising more tight evaluation measures remains a challenge for future research. We extend our metric for evaluating manual or automatic",
        "type": "Document"
      },
      {
        "id": "752bb6ca-d37b-4051-b7f7-02572160bda9",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1701.02877",
          "start_idx": 11679,
          "end_idx": 12145
        },
        "page_content": " while Table 5 reports the token/type ratios. The lower those ratios are, the more diverse a corpus is. While token/type ratios also include tokens which are NEs, they are a good measure of broader linguistic diversity. Aside from these metrics, there are other factors which contribute to corpus diversity, including how big a corpus is and how well sampled it is, e.g. if a corpus is only about one story, it should not be surprising to see a high token/type ratio",
        "type": "Document"
      },
      {
        "id": "9da14bdb-e5a9-41b5-b4dd-74e3d9bf28d5",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1911.03243",
          "start_idx": 7821,
          "end_idx": 8395
        },
        "page_content": " more tight evaluation measures remains a challenge for future research. We extend our metric for evaluating manual or automatic redundant annotations, like the Dense dataset or the parser in BIBREF5, which predicts argument spans independently of each other. To that end, we ignore predicted arguments that match ground-truth but are not selected by the bipartite matching due to redundancy. After connecting unmatched predicted arguments that overlap, we count one false positive for every connected component to avoid penalizing precision too harshly when predictions are",
        "type": "Document"
      },
      {
        "id": "1a2b1e44-6d5f-4f47-92b7-c851a3e559d2",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1708.05873",
          "start_idx": 4331,
          "end_idx": 4846
        },
        "page_content": " average proportion of a document discussing a topic. We assess the optimal number of topics that need to be specified for the STM analysis. We follow the recommendations of the original STM paper and focus on exclusivity and semantic coherence measures. BIBREF5 propose semantic coherence measure, which is closely related to point-wise mutual information measure posited by BIBREF6 to evaluate topic quality. BIBREF5 show that semantic coherence corresponds to expert judgments and more general human judgments in",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much do they improve the accuracy of inferences over state-of-the-art methods?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "e845a923-aa4c-475b-9700-e7b767dd4632",
        "metadata": {
          "chunk_id": 64,
          "document_id": "1703.09684",
          "start_idx": 23901,
          "end_idx": 24411
        },
        "page_content": " lower amounts of training data, bias, or limitations of the models. We next investigate how much of the good performance is due to bias in the answer distribution, which N-MPT compensates for. One of our major aims was to compensate for the fact that algorithms can achieve high scores by simply learning to answer more populated and easier question-types. For existing datasets, earlier work has shown that simple baseline methods routinely exceed more complex methods using simple accuracy BIBREF7 , BIBREF8",
        "type": "Document"
      },
      {
        "id": "577f6feb-1a55-497e-a9ae-b43551ab6a28",
        "metadata": {
          "chunk_id": 54,
          "document_id": "1910.04269",
          "start_idx": 20617,
          "end_idx": 21192
        },
        "page_content": " robust to variations which might be present in real world scenarios. There can be further exploration of various feature extraction techniques like Constant-Q transform and Fast Fourier Transform and assessment of their impact on Language Identification. There can be further improvements in neural network architectures like concatenating the high level features obtained from 1D-ConvNet and 2D-ConvNet, before performing classification. There can be experiments using deeper networks with skip connections and Inception modules. These are known to have positively impacted",
        "type": "Document"
      },
      {
        "id": "8163e84f-0be9-4a39-b8f9-dc60456a1cfa",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1703.09684",
          "start_idx": 5082,
          "end_idx": 5561
        },
        "page_content": "OCO-VQA, improving accuracy on `Is/Are' questions by 15% will increase overall accuracy by over 5%, but answering all `Why/Where' questions correctly will increase accuracy by only 4.1% BIBREF10 . Due to the inability of the existing evaluation metrics to properly address these biases, algorithms trained on these datasets learn to exploit these biases, resulting in systems that work poorly when deployed in the real-world. For related reasons, major benchmarks released in the",
        "type": "Document"
      },
      {
        "id": "ad406d05-a5bd-438b-89a3-2a90c7f67569",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1910.00458",
          "start_idx": 1913,
          "end_idx": 2356
        },
        "page_content": " non-trivial reasoning techniques BIBREF7. As a result, the performance of machine readers on these tasks can more accurately gauge comprehension ability of a model. Recently large and powerful pre-trained language models such as BERT BIBREF8 have been achieving the state-of-the-art (SOTA) results on various tasks, however, its potency on MCQA datasets has been severely limited by the data insufficiency. For example, the MCTest dataset has",
        "type": "Document"
      },
      {
        "id": "3c4118f4-49c1-4857-b3f6-a80bb8c553e6",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1906.06448",
          "start_idx": 23574,
          "end_idx": 24095
        },
        "page_content": ", among the 394 problems for which all models provided wrong answers, 244 problems are non-lexical inference problems. This indicates that some non-lexical inference problems are more difficult than lexical inference problems, though accuracy on non-lexical inference problems was better than that on lexical inference problems. One of our findings is that there is a type of downward inferences to which every model fails to provide correct answers. One such example is concerned with the contrast between few and a few.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does the context-aware variational autoencoder learn event background information?",
    "llm_answer": "In the pretrain stage, CWVAE is trained on an auxiliary dataset with rich event context information.  The context-acquiring latent variable $z_c$ is directly conditioned on the context, allowing it to acquire background knowledge.  Then, a context-aware latent variable $z_{c'}$ learns background knowledge from $z_c$.\n",
    "context": [
      {
        "id": "31ed6623-2973-4f37-b470-0ae663a5eb63",
        "metadata": {
          "chunk_id": 21,
          "document_id": "1909.08824",
          "start_idx": 6888,
          "end_idx": 7354
        },
        "page_content": " we model the If-Then reasoning as a [(background), event]-target process. It means that in addition to the observed event, we also want to involve the event background knowledge (which can be learned from event contexts) to generate the reasonable targets. To this end, we propose a context-aware variational autoencoder (CWVAE), with two additional latent variables: a context-acquiring latent variable $z_c$ to directly acquire context information, and a context-",
        "type": "Document"
      },
      {
        "id": "88f626e0-292b-4812-a5db-9ec54e55e99f",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1909.08824",
          "start_idx": 7178,
          "end_idx": 7635
        },
        "page_content": "-acquiring latent variable $z_c$ to directly acquire context information, and a context-aware latent variable $z_{c^{\\prime }}$ to learn background knowledge from $z_c$, as shown in Figure FIGREF6 (a). However, the event context information is absent in the Event2Mind and Atomic dataset. To learn from the external event context information, we design the following two-stage training procedure for CWVAE. Pretrain: Learning Event Background Knowledge from",
        "type": "Document"
      },
      {
        "id": "19d4ad0d-0f34-4d45-a2ba-05bbbef29f39",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1909.08824",
          "start_idx": 2049,
          "end_idx": 2530
        },
        "page_content": " problem and generate diversified inferences BIBREF8, BIBREF9. In addition to the traditional VAE structure, we introduces an extra context-aware latent variable in CWVAE to learn the event background knowledge. In the pretrain stage, CWVAE is trained on an auxiliary dataset (consists of three narrative story corpora and contains rich event background knowledge), to learn the event background information by using the context-aware latent variable. Subsequently, in the finetune",
        "type": "Document"
      },
      {
        "id": "e09169cc-64ba-4353-b265-3c007a6c7984",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1909.08824",
          "start_idx": 7635,
          "end_idx": 8089
        },
        "page_content": " the following two-stage training procedure for CWVAE. Pretrain: Learning Event Background Knowledge from Auxiliary Dataset In the pretrain stage, CWVAE is trained on three narrative story corpora with rich event context information. As shown in Figure FIGREF6 (a), context-acquiring latent variable $z_c$ is directly conditioned on the context $c$. Hence, $z_c$ could be employed for acquiring background knowledge from event contexts. Then, we minimize",
        "type": "Document"
      },
      {
        "id": "b6670116-b2b7-4c5a-8052-9d36bc39a9f4",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1909.08824",
          "start_idx": 2530,
          "end_idx": 3016
        },
        "page_content": " event background information by using the context-aware latent variable. Subsequently, in the finetune stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target (e.g., intents, reactions, etc.). Experiments on the Event2Mind and Atomic dataset show that our proposed approach outperforms baseline methods in both the accuracy and diversity of inferences. The code is released at https://github.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much improvement does their method get over the fine tuning baseline?",
    "llm_answer": "The context states that fine-tuning improves performance in several NMT scenarios, but does not specify the magnitude of improvement over a fine-tuning baseline.\n",
    "context": [
      {
        "id": "860baa46-071d-4950-80e5-bec2253ba96a",
        "metadata": {
          "chunk_id": 33,
          "document_id": "1905.13413",
          "start_idx": 10297,
          "end_idx": 10787
        },
        "page_content": " of using the confidence (E.q. 7 ) of the fine-tuned model to rerank the extractions of the base model (Rerank Only), and the end-to-end performance of the fine-tuned model in assertion generation (Generate). We found both settings lead to improved performance compared to the base model, which demonstrates that calibrating confidence using binary classification loss can improve the performance of both reranking and assertion generation. Finally, our proposed iterative learning approach",
        "type": "Document"
      },
      {
        "id": "d8cd352f-0eaa-406b-b512-43a9a4998966",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1910.10408",
          "start_idx": 9045,
          "end_idx": 9446
        },
        "page_content": ", fine-tuning a pre-trained network shows to improve performance in several NMT scenarios BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF24. For our length control approaches, we further propose to use fine-tuning an NMT model with length information, instead of training it from scratch. By adopting a fine-tuning strategy, we specifically aim; i) to decouple the performance of the baseline NMT model",
        "type": "Document"
      },
      {
        "id": "1edddcca-818b-4167-9c9c-a0494ebeedfd",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1701.03214",
          "start_idx": 3413,
          "end_idx": 3829
        },
        "page_content": " fine tuning method is a combination of the above methods (shown in Figure 2 ). The training procedure is as follows: Train an NMT model on out-of-domain data till convergence. Resume training the NMT model from step 1 on a mix of in-domain and out-of-domain data (by oversampling the in-domain data) till convergence. By default, we utilize domain tags, but we also consider settings where we do not use them (i.e.,",
        "type": "Document"
      },
      {
        "id": "7ea2c42c-a091-46b1-a2f5-ba97e64ac31c",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1911.03894",
          "start_idx": 18085,
          "end_idx": 18526
        },
        "page_content": " learning rates and batch sizes. We select the best model on the validation set out of the 30 first epochs. Although this might push the performances even further, for all tasks except NLI, we don't apply any regularisation techniques such as weight decay, learning rate warm-up or discriminative fine-tuning. We show that fine-tuning CamemBERT in a straight-forward manner leads to state-of-the-art results on most tasks and outperforms the",
        "type": "Document"
      },
      {
        "id": "a34ba259-71d4-4abe-aba1-4c61f5e677b8",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1909.13375",
          "start_idx": 19611,
          "end_idx": 20052
        },
        "page_content": " to gain further insights on the effect of the components that we ourselves introduced. For ease of performance comparison, we followed NABERT+'s training settings: we used the BERT Adam optimizer from huggingface with default settings and a learning rate of $1e^{-5}$. The only difference was that we used a batch size of 12. We trained our base model for 20 epochs. For the large models we used a batch size of 3 with a learning rate of $5",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much do they outpeform previous results on the word discrimination task?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "4f366b00-7e35-42cd-8d5d-0a1a3c14dcfa",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1906.06448",
          "start_idx": 21919,
          "end_idx": 22444
        },
        "page_content": " also indicates the difficulty of problems from linguistics publications. Regarding non-monotone problems collected via crowdsourcing, there are very few non-monotone problems, so accuracy is 100%. Adding non-monotone problems to our test set is left for future work. Table 9 shows the evaluation results by type of linguistic phenomenon. While accuracy on problems involving NPIs and conditionals was improved on both upward and downward inferences, accuracy on problems involving conjunction and disjunction was improved on",
        "type": "Document"
      },
      {
        "id": "8a052545-2df4-4226-89af-f8c733567a26",
        "metadata": {
          "chunk_id": 33,
          "document_id": "1611.02550",
          "start_idx": 12001,
          "end_idx": 12521
        },
        "page_content": " respects our end objective, and can use more data since neither fully labeled data nor any minimum number of examples of each word should be needed.   Our end goal is to improve performance on downstream tasks requiring accurate word discrimination. In this paper we use an intermediate task that more directly tests whether same- and different-word pairs have the expected relationship. and that allows us to compare to a variety of prior work. Specifically, we use the word discrimination task of Carlin et al. BIBREF",
        "type": "Document"
      },
      {
        "id": "efd1c414-9715-432a-8115-683536f25ab0",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1611.02550",
          "start_idx": 12326,
          "end_idx": 12820
        },
        "page_content": " prior work. Specifically, we use the word discrimination task of Carlin et al. BIBREF20 , which is similar to a query-by-example task where the word segmentations are known. The evaluation consists of determining, for each pair of evaluation segments, whether they are examples of the same or different words, and measuring performance via the average precision (AP). We do this by measuring the cosine similarity between their acoustic word embeddings and declaring them to be the same if the",
        "type": "Document"
      },
      {
        "id": "a6b6c2ef-6c9c-4044-85e0-6973ec281318",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1809.10644",
          "start_idx": 7678,
          "end_idx": 8121
        },
        "page_content": ", the authors reported their trained best logistic regression model's results on their respective datasets. SR: Sexist/Racist BIBREF3 , HATE: Hate BIBREF4 HAR: Harassment BIBREF9  The approach we have developed establishes a new state of the art for classifying hate speech, outperforming previous results by as much as 12 F1 points. Table TABREF10 illustrates the robustness of our method, which often outperform previous results, measured by",
        "type": "Document"
      },
      {
        "id": "448c419e-98b5-4844-8de3-55646bd0d2d7",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1904.08386",
          "start_idx": 6277,
          "end_idx": 6716
        },
        "page_content": " is surprising as BERT is a bigger model trained on data from books (among other domains). Both ELMo and BERT outperform GloVe, which intuitively makes sense because the latter do not model the order or structure of the words in each description. While the purity of our methods is higher than that of a random clustering, it is still far below 1. To provide additional context to these results, we now switch to our \u201codd-one-out\u201d task and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many paraphrases are generated per question?",
    "llm_answer": "10\n",
    "context": [
      {
        "id": "2444b48a-8cb8-4ee0-af7a-7d9272dc90c9",
        "metadata": {
          "chunk_id": 68,
          "document_id": "1601.06068",
          "start_idx": 22413,
          "end_idx": 22941
        },
        "page_content": " to generate paraphrases for questions, and applied it to a question answering system based on semantic parsing. We showed that using paraphrases for a question answering system is a useful way to improve its performance. Our method is rather generic and can be applied to any question answering system. The authors would like to thank Nitin Madnani for his help with the implementation of the paraphrase classifier. We would like to thank our anonymous reviewers for their insightful comments. This research was supported by an",
        "type": "Document"
      },
      {
        "id": "3e885c82-4dac-4576-8ce1-0bd5323ce57d",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1601.06068",
          "start_idx": 5378,
          "end_idx": 5797
        },
        "page_content": " are further filtered with a classifier to improve the precision of the generated paraphrases. We train the L-PCFG $G_{\\mathrm {syn}}$ on the Paralex corpus BIBREF9 . Paralex is a large monolingual parallel corpus, containing 18 million pairs of question paraphrases with 2.4M distinct questions in the corpus. It is suitable for our task of generating paraphrases since its large scale makes our model robust for open-",
        "type": "Document"
      },
      {
        "id": "0a8bc7b2-6290-4d39-9672-772ec62ee2ca",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1601.06068",
          "start_idx": 19573,
          "end_idx": 19929
        },
        "page_content": "36 . In particular, we use Moses BIBREF37 to train a monolingual phrase-based MT system on the Paralex corpus. Finally, we use Moses decoder to generate 10-best distinct paraphrases for the test questions. For WebQuestions, we use 8 handcrafted part-of-speech patterns (e.g., the pattern (DT)?(JJ.? $\\mid $ NN.?){0,2}NN.? matches the noun phrase the big le",
        "type": "Document"
      },
      {
        "id": "8289efc6-c815-421d-80d2-8b126306d695",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1601.06068",
          "start_idx": 19293,
          "end_idx": 19699
        },
        "page_content": "g R.) and average F $_1$ (avg F $_1$ ) proposed by berantsemantic2013 as evaluation metrics. We use GraphParser without paraphrases as our baseline. This gives an idea about the impact of using paraphrases. We compare our paraphrasing models with monolingual machine translation based model for paraphrase generation BIBREF24 , BIBREF36 . In particular, we use Moses BIBREF37 to train a monolingual phrase-",
        "type": "Document"
      },
      {
        "id": "e7037786-0d49-48ce-a1fc-29f3f057d444",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1601.06068",
          "start_idx": 4050,
          "end_idx": 4482
        },
        "page_content": " these constructions, we refer the reader to cohen-13 and narayan-15. The rest of this section describes our paraphrase generation algorithm. We define our paraphrase generation task as a sampling problem from an L-PCFG $G_{\\mathrm {syn}}$ , which is estimated from a large corpus of parsed questions. Once this grammar is estimated, our algorithm follows a pipeline with two major steps. We first build a word lattice $W_q$ for the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How strong was the correlation between exercise and diabetes?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "6bc05370-ae82-46ff-a008-203e23be5192",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1709.07916",
          "start_idx": 11632,
          "end_idx": 12133
        },
        "page_content": " users' interest about posting their opinions, sharing information, and conversing about exercise & diabetes, exercise & diet, diabetes & diet, diabetes & obesity, and diet & obesity (Figure FIGREF9 ). The strongest correlation among the topics was determined to be between exercise and obesity ( INLINEFORM0 ). Other notable correlations were: diabetes and obesity ( INLINEFORM1 ), and diet and obesity ( INLINEFORM2 ). Diabetes, diet, exercise, and obesity are common public health related opinions.",
        "type": "Document"
      },
      {
        "id": "85b6241d-f834-44da-9356-302b928701cd",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1709.07916",
          "start_idx": 13327,
          "end_idx": 13877
        },
        "page_content": " diabetes and other diseases such as heart attack. The subtopic Alzheimer is also shown in the obesity subtopics. This overlap between categories prompts the discussion of research and linkages among obesity, diabetes, and Alzheimer's disease. Type 2 diabetes was another subtopic that was also expressed by users and scientifically documented in the literature. The inclusion of Yoga in posts about diabetes is interesting. While yoga would certainly be labeled as a form of fitness, when considering the post, it was insightful to see discussion on",
        "type": "Document"
      },
      {
        "id": "7adc957f-30ec-4f43-883a-1da9ca389a53",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1709.07916",
          "start_idx": 14506,
          "end_idx": 14993
        },
        "page_content": " data. The diet plans of celebrities were also considered influential to explaining and informing diet opinions of Twitter users BIBREF64 . Exercise themes show the Twitter users' association of exercise with \u201cbrain\" benefits such as increased memory and cognitive performance (Tables TABREF7 and TABREF8 ) BIBREF65 . The topics also confirm that exercising is associated with controlling diabetes and assisting with meal planning BIBREF66 , BIBREF9 , and obesity BIBREF67 . Additionally",
        "type": "Document"
      },
      {
        "id": "f4ff87fb-b9da-404d-b141-bfdf435afc81",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1709.07916",
          "start_idx": 788,
          "end_idx": 1243
        },
        "page_content": "IBREF4 . There are several comorbidities associated with being overweight or obese, such as diabetes BIBREF5 . The prevalence of diabetes in adults has risen globally from 4.7% in 1980 to 8.5% in 2014. Current projections estimate that by 2050, 29 million Americans will be diagnosed with type 2 diabetes, which is a 165% increase from the 11 million diagnosed in 2002 BIBREF6 . Studies show that there are strong relations among diabetes, diet, exercise,",
        "type": "Document"
      },
      {
        "id": "5a71054b-5a31-41ac-a425-c9017b0f1cae",
        "metadata": {
          "chunk_id": 41,
          "document_id": "1709.07916",
          "start_idx": 15848,
          "end_idx": 16405
        },
        "page_content": "7 and TABREF8 ). The scholarly literature has well documented the possible linkages between obesity and chronic diseases such as diabetes BIBREF1 as supported by the study results. The topic of children is another prominent subtopic associated with obesity. There has been an increasing number of opinions in regard to child obesity and national health campaigns that have been developed to encourage physical activity among children BIBREF70 . Alzheimer was also identified as a topic under obesity. Although considered a perplexing finding, recent studies",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How were topics of interest about DDEO identified?",
    "llm_answer": "LDA (Latent Dirichlet Allocation) topic modeling was used, analyzing unigrams (single words) associated with topics.  Inspection of the top unigrams for each topic revealed meaningful topics.\n",
    "context": [
      {
        "id": "1c74513d-99eb-43f3-9ac5-f0c8d5d1ef70",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1709.07916",
          "start_idx": 8702,
          "end_idx": 9132
        },
        "page_content": " TABREF7 ) shows that the highest and the lowest number of topics were related to exercise and diabetes (80 and 21 out of 222). Diet and Obesity had almost similar rates (58 and 63 out of 222). Each of the DDEO topics included several common subtopics including both DDEO and non-DDEO terms discovered by the LDA algorithm (Table TABREF7 ). Common subtopics for \u201cDiabetes\", in order of frequency, included type 2 diabetes, obesity",
        "type": "Document"
      },
      {
        "id": "a44c48dd-0aaf-4798-8fd9-0b69c470e6af",
        "metadata": {
          "chunk_id": 30,
          "document_id": "1709.07916",
          "start_idx": 11106,
          "end_idx": 11632
        },
        "page_content": " research and linkages among obesity, diabetes, and Alzheimer's disease. Type 2 diabetes was another subtopic expressed by users and scientifically documented in the literature. The main DDEO topics showed some level of interrelationship by appearing as subtopics of other DDEO topics. The words with italic and underline styles in Table 2 demonstrate the relation among the four DDEO areas. Our results show users' interest about posting their opinions, sharing information, and conversing about exercise & diabetes, exercise",
        "type": "Document"
      },
      {
        "id": "2bf9ec5a-06e8-4105-aa63-4afc642b312f",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1709.07916",
          "start_idx": 3695,
          "end_idx": 4288
        },
        "page_content": " population level. This study identifies the characteristics of the most common health opinions with respect to DDEO and discloses public perception of the relationship among diabetes, diet, exercise and obesity. These common public opinions/topics and perceptions can be used by providers and public health agencies to better understand the common opinions of their population denominators in regard to DDEO, and reflect upon those opinions accordingly. Our approach uses semantic and linguistics analyses for disclosing health characteristics of opinions in tweets containing DDEO words. The",
        "type": "Document"
      },
      {
        "id": "c52c4a6c-f82d-4371-854d-0f59a0f4ccc6",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1604.05781",
          "start_idx": 21504,
          "end_idx": 21944
        },
        "page_content": " terms co-occur across a set of documents. We used the popular topic modeling method Latent Dirichlet Allocation (LDA) BIBREF39 . We ranked unigrams by how strongly associated they were with the topic. Inspecting these unigrams we found that a 10-topic model discovered meaningful topics. See Methods for full details. The top unigrams for each topic are shown in Tab. 1 . Topics in the causal corpus tend to fall into three main categories",
        "type": "Document"
      },
      {
        "id": "6c8bdce1-82e1-454f-9496-48b03741c9d2",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1708.05873",
          "start_idx": 4026,
          "end_idx": 4560
        },
        "page_content": " the key international development topics discussed in the GD. We model topic prevalence in the context of the structural covariates. In addition, we control for region fixed effects and time trend. The aim is to allow the observed metadata to affect the frequency with which a topic is discussed in General Debate speeches. This allows us to test the degree of association between covariates (and region/time effects) and the average proportion of a document discussing a topic. We assess the optimal number of topics that need to be",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do their train their embeddings?",
    "llm_answer": "They use two main training approaches: one based on classifying the word using word labels of training segments, and another based on training \"Siamese\" networks.  In both cases, RNN-based embedding models are used.\n",
    "context": [
      {
        "id": "17902d4c-26a5-4f6d-8c8c-344c7f658d89",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1910.07181",
          "start_idx": 13094,
          "end_idx": 13495
        },
        "page_content": "formative ones. The final embedding $v_{(w, \\mathcal {C})}$ is then a linear combination of the embeddings obtained from each context, where the weight of each embedding is determined based on the self-attention layer. For further details on this mechanism, we refer to BIBREF19. Like previous work, we use mimicking BIBREF8 as a training objective. That is, given a frequent word $w$ with known embed",
        "type": "Document"
      },
      {
        "id": "3532fb8d-e948-484f-a9c0-eaece01fc329",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1611.02550",
          "start_idx": 10524,
          "end_idx": 11014
        },
        "page_content": " training, the embeddings should correspond to some measure of similarity of the word to the training words, measured via the posterior probabilities of the previously seen words. In the experiments below, we examine this assumption by analyzing performance on words that appear in the training data compared to those that do not. The second training approach, based on earlier work of Kamper et al. BIBREF13 , is to train \"Siamese\" networks BIBREF30 . In this approach, full supervision is",
        "type": "Document"
      },
      {
        "id": "863a262b-cd13-4dd7-954c-fb4306444389",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1611.02550",
          "start_idx": 9307,
          "end_idx": 9755
        },
        "page_content": " except for the final layer which depends on the form of supervision and loss used in training.  We train the RNN-based embedding models using a set of pre-segmented spoken words. We use two main training approaches, inspired by prior work but with some differences in the details. As in BIBREF13 , BIBREF10 , our first approach is to use the word labels of the training segments and train the networks to classify the word. In this case, the final",
        "type": "Document"
      },
      {
        "id": "e5514909-e8df-4ec2-bb76-67b2666fba58",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1910.05608",
          "start_idx": 9332,
          "end_idx": 9720
        },
        "page_content": "$ is the class weight, $y_i$ is the ground truth and $\\hat{y}_i$ is the output of the model. If the class weight is not set, we find that model cannot adjust parameters. The model tends to output all clean classes. We experiment 8 types of embedding in total: comment: CBOW embedding training in all dataset comment, each word is splited by space. Embedding size is 200. comment_bpe: CBOW",
        "type": "Document"
      },
      {
        "id": "d999f24b-4c79-4c30-b2c8-cc354ba6834a",
        "metadata": {
          "chunk_id": 40,
          "document_id": "1910.12618",
          "start_idx": 16003,
          "end_idx": 16518
        },
        "page_content": " concatenate all the vector representations of a sentence together beforehand and result in a network with too many parameters to be trained correctly with our number of available documents. Recall that we decided to train our own vector representation of words instead of using an already available one. In order to obtain the embedding, the texts are first converted into a sequence of integers: each word is given a number ranging from 1 to $V$, where $V$ is the vocabulary size (0 is used for padding or unknown",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they model travel behavior?",
    "llm_answer": "They use random utility maximization (RUM) models, which are predominantly numeric, linear, and often flat.  They also use Travel Behavior embeddings, a method that borrows from NLP to encode categorical variables.\n",
    "context": [
      {
        "id": "b4037fb4-269c-4846-9783-e2f237e6b42e",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1807.03367",
          "start_idx": 11074,
          "end_idx": 11628
        },
        "page_content": "task is localization: without such capabilities the guide can not tell whether the tourist reached the target location. In this work, we establish a minimal baseline for Talk The Walk by utilizing agents trained for localization. Specifically, we let trained tourist models undertake random walks, using the following protocol: at each step, the tourist communicates its observations and actions to the guide, who predicts the tourist's location. If the guide predicts that the tourist is at target, we evaluate its location. If successful, the task ends",
        "type": "Document"
      },
      {
        "id": "c72210d9-65f6-4d7e-bb07-bd292705baa0",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1909.00154",
          "start_idx": 19320,
          "end_idx": 19766
        },
        "page_content": " one considers the complicated challenges for opening or sharing travel survey datasets in our field. Of course, a major question arises: are behaviors that consistent across the world? There are certainly nuances across the world, but we believe that general patterns would emerge (e.g. a \u201cbusiness\" trip purpose will be closer to \u201cwork\" than \u201cleisure\", in a departure time choice model; \u201cstudent\" will be closer to \u201cunemployed\" than to \u201cretired",
        "type": "Document"
      },
      {
        "id": "43062274-1ab4-4211-9848-083f40ebf0af",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1807.03367",
          "start_idx": 11628,
          "end_idx": 12165
        },
        "page_content": " guide predicts that the tourist is at target, we evaluate its location. If successful, the task ends, otherwise we continue until there have been three wrong evaluations. The protocol is given as pseudo-code in Appendix SECREF12 . The designed navigation protocol relies on a trained localization model that predicts the tourist's location from a communicated message. Before we formalize this localization sub-task in Section UID21 , we further introduce two simplifying assumptions\u2014perfect perception and orientation-agnosticism\u2014so as",
        "type": "Document"
      },
      {
        "id": "79dfc018-9acb-4d8d-965a-0f32ac5ddfe5",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1909.00154",
          "start_idx": 3999,
          "end_idx": 4489
        },
        "page_content": " ownership versus departure time choice models for example. The key idea in this paper is to introduce a method, called Travel Behavior embeddings, that borrows much from the NLP concept. This method serves to encode categorical variables, and is dependent on the problem at hand. We will focus on mode choice, and test on a well-known dataset, by comparing with both dummy and PCA encoding. All the dataset and code are made openly available, and the reader can follow and generate results",
        "type": "Document"
      },
      {
        "id": "2031a68d-d1bc-48c7-841a-7c30a7b3c55c",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1909.00154",
          "start_idx": 5640,
          "end_idx": 6156
        },
        "page_content": ", that complexity can grow quick enough to justify an embeddings based method even if without the shockingly better performance observed in NLP applications. We are generally concerned with random utility maximization (RUM) models, for they have a dominant role in travel behavior modeling. The nature of such models is predominantly numeric, linear, and quite often strictly flat (notwithstanding hierarchical variations, such as nested models BIBREF1, hierarchical Bayes BIBREF2, or non-linear transformations). As",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do their interpret the coefficients?",
    "llm_answer": "Coefficients reflect the impact on load demand (e.g., positive for winter words increasing demand, negative for summer words decreasing it).  A word's presence causes a rise in the time series if the coefficient is positive, a decline if negative.  The magnitude of the coefficient indicates the strength of the impact.\n",
    "context": [
      {
        "id": "c5ef67c7-b8d6-4908-8776-9e8bfcbc3f04",
        "metadata": {
          "chunk_id": 78,
          "document_id": "1910.12618",
          "start_idx": 30707,
          "end_idx": 31243
        },
        "page_content": " value of the coefficients also reflects the impact on the load demand. For example January and February have the highest and very similar values, which concurs with the similarity between the months. Sunday has a much more negative coefficient than Saturday, since the demand significantly drops during the last day of the week. The important words also globally match between the LASSO and the RF, which is a proof of the consistency of our results (this is further explored afterwards in figure FIGREF43). Although not presented here",
        "type": "Document"
      },
      {
        "id": "9b7303d5-6316-4ca1-b5af-f717f4284739",
        "metadata": {
          "chunk_id": 47,
          "document_id": "2001.07209",
          "start_idx": 18369,
          "end_idx": 18689
        },
        "page_content": " of moral relevance change for word $w$; $f(w$) is its average frequency; $l(w)$ is its character length; $c(w)$ is its concreteness rating; $\\beta _f$, $\\beta _l$, $\\beta _c$, and $\\beta _0$ are the corresponding factor weights and intercept, respectively; and $\\epsilon \\sim \\mathcal {N}(0, \\sigma )$ is the regression",
        "type": "Document"
      },
      {
        "id": "ae55684e-2921-435e-bd77-cc78b30da3c7",
        "metadata": {
          "chunk_id": 48,
          "document_id": "2001.07209",
          "start_idx": 18566,
          "end_idx": 19074
        },
        "page_content": "on \\sim \\mathcal {N}(0, \\sigma )$ is the regression error term. Table TABREF27 shows the results of multiple linear regression. We observe that concreteness is a significant negative predictor of change toward moral relevance, suggesting that abstract concepts are more strongly associated with increasing moral relevance over time than concrete concepts. This significance persists under partial correlation test against the control factors ($p < 0.01$). We further verified the diachronic component of this",
        "type": "Document"
      },
      {
        "id": "1afef481-42fc-44cd-9c62-101923ee3d76",
        "metadata": {
          "chunk_id": 77,
          "document_id": "1910.12618",
          "start_idx": 30433,
          "end_idx": 30892
        },
        "page_content": " _w > 0$ then the presence of the word $w$ causes a rise the time series (respectively if $\\beta _w < 0$, it entails a decline). The results are plotted fig. FIGREF35 for the the UK. As one can see, the winter related words have positive coefficients, and thus increase the load demand as expected whereas the summer related ones decrease it. The value of the coefficients also reflects the impact on the load demand. For example January and February have the",
        "type": "Document"
      },
      {
        "id": "e90aa33a-7793-4e4a-866d-6a9370b352bb",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1910.14497",
          "start_idx": 3655,
          "end_idx": 4014
        },
        "page_content": "in B} cos(w,a)$, and $X$,$Y$,$A$, and $B$ are groups of words for which the association is measured. Possible values range from $-2$ to 2 depending on the association of the words groups, and a value of zero indicates $X$ and $Y$ are equally associated with $A$ and $B$. See BIBREF4 for further details on WEAT. The RIPA (relational inner product association)",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much do they outperform previous state-of-the-art models?",
    "llm_answer": "The answer is not found in the context.  While the context discusses improvements over baselines and previous models, it does not quantify the improvements against *state-of-the-art* models in a direct way.\n",
    "context": [
      {
        "id": "5e0f3225-070e-4250-8f70-93c083311ec4",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "8951670c-a290-4246-8bea-c038e66a5e74",
        "metadata": {
          "chunk_id": 62,
          "document_id": "1909.13375",
          "start_idx": 21089,
          "end_idx": 21507
        },
        "page_content": " available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1. All that, while keeping in mind we compare a base model to a large model (for reference, note the 8 point improvement between MTMSNbase and MTMSNlarge in both EM and F1). Our best model, large-squad, exhibits a huge improvement of 29.7 in EM and",
        "type": "Document"
      },
      {
        "id": "e60e0d3d-b592-4a3f-9ebd-b12edba8a33b",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1909.13375",
          "start_idx": 20788,
          "end_idx": 21299
        },
        "page_content": " For the other types, most of the improvement came from better preprocessing. A more detailed discussion could be found in Section (SECREF36). Notice that different BERTlarge models were used, so the comparison is less direct. Overall, our large models exhibits similar results to those of MTMSNlarge. For multi-span questions we achieve a significantly better performance. While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still",
        "type": "Document"
      },
      {
        "id": "5db43982-28a9-4d63-8047-9594784d578b",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1603.04553",
          "start_idx": 11106,
          "end_idx": 11513
        },
        "page_content": " the MIR model, we obtain significant improvements of 2.62% and 3.02% on CoNLL F1 score. To make a thorough empirical comparison with previous studies, Table 3 (below the dashed line) also shows the results of some state-of-the-art supervised coreference resolution systems \u2014 IMS: the second best system in the CoNLL 2012 shared task BIBREF28 ; Latent-Tree: the latent tree model BIBREF29 obtaining the best",
        "type": "Document"
      },
      {
        "id": "a34ba259-71d4-4abe-aba1-4c61f5e677b8",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1909.13375",
          "start_idx": 19611,
          "end_idx": 20052
        },
        "page_content": " to gain further insights on the effect of the components that we ourselves introduced. For ease of performance comparison, we followed NABERT+'s training settings: we used the BERT Adam optimizer from huggingface with default settings and a learning rate of $1e^{-5}$. The only difference was that we used a batch size of 12. We trained our base model for 20 epochs. For the large models we used a batch size of 3 with a learning rate of $5",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance difference between proposed method and state-of-the-arts on these datasets?",
    "llm_answer": "TRADE, the state-of-the-art model, performs poorly in this setting.  The proposed \"Ours with Mask and Ordered Triplets\" model outperforms the Baseline and Ablation models on all metrics in previously seen environments.\n",
    "context": [
      {
        "id": "bc03fd03-c242-49e0-bafb-22e9b13f22d8",
        "metadata": {
          "chunk_id": 72,
          "document_id": "2002.11893",
          "start_idx": 26889,
          "end_idx": 27376
        },
        "page_content": " system often records full user constraints, while the last one records relaxed constraints for recommendation. Thus the last one involves system policy, which is out of the scope of state tracking. We used the first query for these models and left state tracking with recommendation for future work. Result Analysis: We evaluated the joint state accuracy (percentage of exact matching) of these two models (Table TABREF31). TRADE, the state-of-the-art model on MultiWOZ, performs poorly",
        "type": "Document"
      },
      {
        "id": "95c62085-b478-4b5e-a4fa-049a6c8fc5d0",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1908.10449",
          "start_idx": 22880,
          "end_idx": 23381
        },
        "page_content": " hard time learning in this setting. As shown in Table TABREF16, both datasets have a vocabulary size of more than 100k. This is much larger than in the other two settings, where on average the length of questions is around 10. This suggests that the methods with better sample efficiency are needed to act in more realistic problem settings with huge action spaces. Experiments also show that a larger memory slot size always helps. Intuitively, with a memory mechanism (either implicit or explicit),",
        "type": "Document"
      },
      {
        "id": "d7a0b7aa-da14-4112-80c0-cc26c62fe83f",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "5c6209f3-cef8-4a17-88a8-a660a1a73e1e",
        "metadata": {
          "chunk_id": 76,
          "document_id": "1701.00185",
          "start_idx": 27376,
          "end_idx": 27736
        },
        "page_content": " and NMI performance of our proposed approaches and four baseline methods, K-means, SkipVec, RecNN and Para2vec based clustering methods. Intuitively, we get a general observation that (1) BoW based approaches, including K-means (TF) and K-means (TF-IDF), and SkipVec based approaches perform not well; (2) RecNN based approaches, both RecNN (Ave.) and RecNN (",
        "type": "Document"
      },
      {
        "id": "e3060c9f-09bf-4735-86e2-63d0d62382e7",
        "metadata": {
          "chunk_id": 53,
          "document_id": "1909.02764",
          "start_idx": 20698,
          "end_idx": 21118
        },
        "page_content": " to our data set. The results are shown in column \u201cSimple\u201d in Table TABREF19. We observe a clear drop in performance, with an average of F$_1$=48 %. The best performing model is again the one trained on TEC, en par with the one trained on the Figure8 data. The model trained on ISEAR performs second best in Experiment 1, it performs worst in Experiment 2. To adapt models trained on previously existing data sets to our",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What benchmarks are created?",
    "llm_answer": "UMBC corpus for Twitter NER, a Twitter NER corpus created by BIBREF21.  A random forest trained on four features (time of year, day of the week, national average temperature, and wind speed).\n",
    "context": [
      {
        "id": "0f0f86f5-59c3-4ea6-ac02-69201e3aefd7",
        "metadata": {
          "chunk_id": 52,
          "document_id": "1910.12618",
          "start_idx": 20588,
          "end_idx": 21147
        },
        "page_content": " one). Finally before application on the testing set, all the methods are re-trained from scratch using both the training and validation data. The goal of our experiments is to quantify how close one can get using textual data only when compared to numerical data. However the inputs of the numerical benchmark should be hence comparable to the information contained in the weather reports. Considering they mainly contain calendar (day of the week and month) as well as temperature and wind information, the benchmark of comparison is a random forest trained",
        "type": "Document"
      },
      {
        "id": "e8e03940-c32b-41c1-98fd-53467848266d",
        "metadata": {
          "chunk_id": 53,
          "document_id": "1910.12618",
          "start_idx": 21147,
          "end_idx": 21609
        },
        "page_content": " and month) as well as temperature and wind information, the benchmark of comparison is a random forest trained on four features only: the time of the year (whose value is 0 on January the 1st and 1 on December the 31st with a linear growth in between), the day of the week, the national average temperature and wind speed. The metrics of evaluation are the Mean Absolute Percentage Error (MAPE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and the",
        "type": "Document"
      },
      {
        "id": "68c9fa82-7b71-4d3b-8d4e-70ded7bf95e4",
        "metadata": {
          "chunk_id": 21,
          "document_id": "1701.02877",
          "start_idx": 7791,
          "end_idx": 8297
        },
        "page_content": " studying NER performance on this genre gained momentum. So far, there have been no big evaluation efforts, such as ACE and OntoNotes, resulting in substantial amounts of gold standard data. Instead, benchmark corpora were created as part of smaller challenges or individual projects. The first such corpus is the UMBC corpus for Twitter NER BIBREF33 , where researchers used crowdsourcing to obtain annotations for persons, locations and organisations. A further Twitter NER corpus was created by BIBREF21",
        "type": "Document"
      },
      {
        "id": "e23dc951-77e9-483d-84ce-d8a31bf62b89",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1910.12618",
          "start_idx": 19565,
          "end_idx": 20023
        },
        "page_content": " the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014 and 2015 serve as validation set (730 days). It is used to tune the hyperparameters of the different approaches. All the data from January the 1st 2016 (974 days for France and 1,096 for the UK) is used as test set, on which the final results are presented. Grid search is applied to find the best combination of values: for each hyperparameter,",
        "type": "Document"
      },
      {
        "id": "637d4fe3-c7c1-4e7c-a855-261351957759",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1701.02877",
          "start_idx": 4315,
          "end_idx": 4755
        },
        "page_content": " NER approaches, namely Stanford NER BIBREF34 , SENNA BIBREF35 and CRFSuite BIBREF36 (see Section \"NER Models and Features\" for details). Since the goal of this study is to compare NER performance on corpora from diverse domains and genres, seven benchmark NER corpora are included, spanning newswire, broadcast conversation, Web content, and social media (see Table 1 for details). These datasets were chosen such that they have been annot",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What percentage fewer errors did professional translations make?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "be114be3-f20c-4ef8-9cae-96029b6d9c45",
        "metadata": {
          "chunk_id": 88,
          "document_id": "2004.01694",
          "start_idx": 34017,
          "end_idx": 34587
        },
        "page_content": " in the field, and found that the choice of raters, the availability of linguistic context, and the creation of reference translations have a strong impact on perceived translation quality. As for the choice of raters, professional translators showed a significant preference for human translation, while non-expert raters did not. In terms of linguistic context, raters found human translation significantly more accurate than machine translation when evaluating full documents, but not when evaluating single sentences out of context. They also found human translation",
        "type": "Document"
      },
      {
        "id": "9d5516aa-fb06-4196-9c97-e012546769b3",
        "metadata": {
          "chunk_id": 89,
          "document_id": "2004.01694",
          "start_idx": 34369,
          "end_idx": 35012
        },
        "page_content": " when evaluating full documents, but not when evaluating single sentences out of context. They also found human translation significantly more fluent than machine translation, both when evaluating full documents and single sentences. Moreover, we showed that aggressive editing of human reference translations for target language fluency can decrease adequacy to the point that they become indistinguishable from machine translation, and that raters found human translations significantly better than machine translations of original source texts, but not of source texts that were translations themselves. Our results strongly suggest that in",
        "type": "Document"
      },
      {
        "id": "d51839d0-7090-4752-8230-0293735ca2f9",
        "metadata": {
          "chunk_id": 81,
          "document_id": "2004.01694",
          "start_idx": 31011,
          "end_idx": 31558
        },
        "page_content": " fluency, specifically in grammaticality. In professional translation workflows, texts are typically revised with a focus on target language fluency after an initial translation step. As shown in our experiment in Section SECREF24, aggressive revision can make translations more fluent but less accurate, to the degree that they become indistinguishable from MT in terms of accuracy (Table TABREF30). Raters show a significant preference for human over machine translations of texts that were originally written in the source language, but not for",
        "type": "Document"
      },
      {
        "id": "d4c52e39-5442-4fb4-a1f5-0a281229560f",
        "metadata": {
          "chunk_id": 58,
          "document_id": "2004.01694",
          "start_idx": 22356,
          "end_idx": 22869
        },
        "page_content": "3 perform this classification only for the machine-translated outputs, and thus the natural question of whether the mistakes that humans and computers make are qualitatively different is left unanswered. Our analysis was performed by one of the co-authors who is a bi-lingual native Chinese/English speaker. Sentences were shown in the context of the document, to make it easier to determine whether the translations were correct based on the context. The analysis was performed on one machine translation (MT$_1$",
        "type": "Document"
      },
      {
        "id": "70ab1b4d-972a-4831-96f9-6184f6a7d183",
        "metadata": {
          "chunk_id": 56,
          "document_id": "2004.01694",
          "start_idx": 21596,
          "end_idx": 22082
        },
        "page_content": " stronger preference for human translation in terms of fluency (compare Tables TABREF30 and TABREF21). In a direct comparison of the two human translations (Table TABREF30), we also find that H$_A$ is considered significantly more adequate than H$_B$, while there is no significant difference in fluency. To achieve a finer-grained understanding of what errors the evaluated translations exhibit, we perform a categorisation of 150 randomly sampled sentences based on the classification",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was the weakness in Hassan et al's evaluation design?",
    "llm_answer": "The weakness was that the design did not consider the human judges and lacked objectivity, potentially leading to mistakes due to out-of-context dialogs.\n",
    "context": [
      {
        "id": "70e1ea66-1171-4546-a976-9b33995d48d1",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1908.07816",
          "start_idx": 4017,
          "end_idx": 4509
        },
        "page_content": " relate. It is the first time such an approach is designed with consideration for the human judges. Our main goal is to increase the objectivity of the results and reduce judges' mistakes due to out-of-context dialogs they have to evaluate. The rest of the paper unfolds as follows. Section SECREF2 discusses some related work. In Section SECREF3, we give detailed description of the methodology. We present experimental results and some analysis in Section SECREF4. The paper is concluded in",
        "type": "Document"
      },
      {
        "id": "86d3c04c-e40e-4987-99b0-a1e9c68d7fea",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1904.08386",
          "start_idx": 9817,
          "end_idx": 10140
        },
        "page_content": " anonymous reviewers for their insightful comments. Additionally, we thank Nader Akoury, Garrett Bernstein, Chenghao Lv, Ari Kobren, Kalpesh Krishna, Saumya Lal, Tu Vu, Zhichao Yang, Mengxue Zhang and the UMass NLP group for suggestions that improved the paper's clarity, coverage of related work, and analysis experiments.",
        "type": "Document"
      },
      {
        "id": "748fb5f2-01c6-4839-9bc8-6212bf55e7d1",
        "metadata": {
          "chunk_id": 3,
          "document_id": "2004.01694",
          "start_idx": 1337,
          "end_idx": 1854
        },
        "page_content": " concur that these findings have demonstrated the need to critically re-evaluate the design of human MT evaluation. Our paper investigates three aspects of human MT evaluation, with a special focus on assessing human\u2013machine parity: the choice of raters, the use of linguistic context, and the creation of reference translations. We focus on the data shared by BIBREF3, and empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation. We find that for all three aspects",
        "type": "Document"
      },
      {
        "id": "2f087fa2-1aed-415a-a42c-95e05656d14d",
        "metadata": {
          "chunk_id": 4,
          "document_id": "2004.01694",
          "start_idx": 1628,
          "end_idx": 2197
        },
        "page_content": " changes in the evaluation design affect the outcome of the human evaluation. We find that for all three aspects, human translations are judged more favourably, and significantly better than MT, when we make changes that we believe strengthen the evaluation design. Based on our empirical findings, we formulate a set of recommendations for human MT evaluation in general, and assessing human\u2013machine parity in particular. All of our data are made publicly available for external validation and further analysis. We first review current methods to assess the quality of",
        "type": "Document"
      },
      {
        "id": "a856e2bf-d1b3-4975-ba4e-7e868fb4d666",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1908.07816",
          "start_idx": 22248,
          "end_idx": 22663
        },
        "page_content": "31 on the human evaluation results, showing the improvements of MEED are significant (with $p$-value $<0.01$). We present four sample dialogs in Table TABREF36, along with the responses generated by the three models. Dialog 1 and 2 are emotionally positive and dialog 3 and 4 are negative. For the first two examples, we can see that MEED is able to generate more emotional content (like \u201cfun\u201d and \u201ccongratulations\ufffd",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What evidence do they present that the model attends to shallow context clues?",
    "llm_answer": "The provided context shows examples of questions and their corresponding comments (e.g., \"snorkeling two days ago off the coast of dukhan...\").  A darker blue patch in a figure (Figure 5) indicates larger weight given to certain words, suggesting the model attends to these shallow context clues.\n",
    "context": [
      {
        "id": "f54ce504-5c6f-450d-8f87-6cf459c47cc0",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1910.07181",
          "start_idx": 12787,
          "end_idx": 13094
        },
        "page_content": " contexts $\\mathcal {C} = \\lbrace C_1, \\ldots , C_m\\rbrace $ and the corresponding embeddings $v_{(w,C_1)}, \\ldots , v_{(w,C_m)}$, we apply a self-attention mechanism to all embeddings, allowing the model to distinguish informative contexts from uninformative ones. The final embedding $v_{(w, \\mathcal {C})",
        "type": "Document"
      },
      {
        "id": "1f16990c-f633-44d1-bb4a-85729d948c24",
        "metadata": {
          "chunk_id": 64,
          "document_id": "2004.03034",
          "start_idx": 24560,
          "end_idx": 24939
        },
        "page_content": " the contextual models perform better even for the claims with limited context. This work was supported in part by NSF grants IIS-1815455 and SES-1741441. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of NSF or the U.S. Government.",
        "type": "Document"
      },
      {
        "id": "192f64d5-d7e6-4edd-9a99-9dc9e9a85cf5",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1603.07044",
          "start_idx": 16137,
          "end_idx": 16605
        },
        "page_content": " are shown in Figure 5 , and categorized into short, long, and noisy sentences for discussion. A darker blue patch refers to a larger weight relative to other words in the same sentence. Figure 5 illustrates two cQA examples whose questions are relatively short. The comments corresponding to these questions are \u201c...snorkeling two days ago off the coast of dukhan...\u201d and \u201cthe doha international airport...\u201d. We can observe that our model successfully learns to focus",
        "type": "Document"
      },
      {
        "id": "c9c62107-7b13-479a-a3d4-412d89dc50bd",
        "metadata": {
          "chunk_id": 3,
          "document_id": "2004.02393",
          "start_idx": 1273,
          "end_idx": 1817
        },
        "page_content": " may require different forms of evidence annotations. As a result, the supervision from these datasets can still be insufficient for training accurate models. Taking question answering with multi-hop reasoning as an example, annotating only supporting passages is not sufficient to show the reasoning processes due to the lack of necessary structural information (Figure FIGREF1). One example is the order of annotated evidence, which is crucial in logic reasoning and the importance of which has also been demonstrated in text-based QA BIBREF8",
        "type": "Document"
      },
      {
        "id": "63a833f5-a192-4207-8bfa-42ff3f39e483",
        "metadata": {
          "chunk_id": 63,
          "document_id": "2004.03034",
          "start_idx": 24164,
          "end_idx": 24784
        },
        "page_content": " this paper, we present a dataset of claims with their corresponding impact votes, and investigate the role of argumentative discourse context in argument impact classification. We experiment with various models to represent the claims and their context and find that incorporating the context information gives significant improvement in predicting argument impact. In our study, we find that flat representation of the context gives the best improvement in the performance and our analysis indicates that the contextual models perform better even for the claims with limited context. This work was supported in part by",
        "type": "Document"
      }
    ]
  },
  {
    "query": "In what way is the input restructured?",
    "llm_answer": "Input Module extracts vector representations of inputs at different levels of granularity by using several different encoders.  The final input embedding is a sum of token, segmentation, and position embeddings.\n",
    "context": [
      {
        "id": "7188814a-9cc0-45ad-afbe-3ffcaa35234f",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1706.07179",
          "start_idx": 2809,
          "end_idx": 3302
        },
        "page_content": " in this section. Figure 1 provides a high-level view of the model. The model is sequential in nature, consisting of the following steps: read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer. We model the dynamic memory in a fashion similar to Recurrent Entity Networks BIBREF17 and then equip it with an additional relational memory. There are three main components to the model: 1) input encoder 2) dynamic memory, and 3)",
        "type": "Document"
      },
      {
        "id": "7532cb71-8027-493d-8e44-73b7dff410f9",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1909.08859",
          "start_idx": 4906,
          "end_idx": 5447
        },
        "page_content": " question and the multiple choice answers are composed of images. Input Module extracts vector representations of inputs at different levels of granularity by using several different encoders. Reasoning Module scans the procedural text and tracks the states of the entities and their relations through a recurrent relational memory core unit BIBREF5. Attention Module computes context-aware query vectors and query-aware context vectors as well as query-aware memory vectors. Modeling Module employs two multi-layered RNNs to encode previous",
        "type": "Document"
      },
      {
        "id": "ad9d7cff-01ed-4a66-9570-050a05a20da0",
        "metadata": {
          "chunk_id": 17,
          "document_id": "2001.00137",
          "start_idx": 7115,
          "end_idx": 7610
        },
        "page_content": " by an embedding layer used for input representation, with the final input embedding being a sum of token embedddings, segmentation embeddings and position embeddings. The first one, token embedding layer, uses a vocabulary dictionary to convert each token into a more representative embedding. The segmentation embedding layer indicates which tokens constitute a sentence by signaling either 1 or 0. In our case, since our data are formed of single sentences, the segment is 1 until the first `",
        "type": "Document"
      },
      {
        "id": "45125e2a-384e-43c1-869a-fb7164dd076f",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1812.07023",
          "start_idx": 15144,
          "end_idx": 15594
        },
        "page_content": " INLINEFORM0 which is the input of the decoder. We first combine past utterances using a dialogue-level encoder which is a single-layer lstm recurrent neural network. The input to this encoder are the final hidden states of the utterance-level lstm. To combine the hidden states of these diverse modalities, we found concatenation to perform better on the validation set than averaging or the Hadamard product. The answer decoder consists of a single",
        "type": "Document"
      },
      {
        "id": "30c459b0-7d92-4279-b024-1221df3d8824",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1908.08419",
          "start_idx": 13807,
          "end_idx": 14293
        },
        "page_content": " the interaction between the characters. There are two types of gates in this network structure: reset gates and update gates. These two gated vectors determine the final output of the gated recurrent neural network, where the update gate helps the model determine what to be passed, and the reset gate primarily helps the model decide what to be cleared. In particular, the word embedding of a word with INLINEFORM0 characters can be computed as: DISPLAYFORM0  where INLINEFORM0 and IN",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What language is the Twitter content in?",
    "llm_answer": "English\n",
    "context": [
      {
        "id": "20df2585-9cf0-49d9-99da-00c30040240d",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1604.05781",
          "start_idx": 4432,
          "end_idx": 4838
        },
        "page_content": " BIBREF29 . A tweet will be referred to as a `document' for the rest of this work. All document text was processed the same way. Punctuation, XML characters, and hyperlinks were removed, as were Twitter-specific \u201cat-mentions\u201d and \u201chashtags\u201d (see also the Appendix). There is useful information here, but it is either not natural language text, or it is Twitter-specific, or both. Documents were broken into",
        "type": "Document"
      },
      {
        "id": "e2ec487e-c7fa-4e10-bbc5-56fd21e5dad6",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1912.13109",
          "start_idx": 5229,
          "end_idx": 5731
        },
        "page_content": "uations, username mentions, hastags, emoticons, numbers and lots of special characters. These were all cleaned up in a preprocessing cycle to clean the data. Stop words: Stop words corpus obtained from NLTK was used to eliminate most unproductive words which provide little information about individual tweets. Transliteration: Followed by above two processes, we translated Hinglish tweets into English words using a two phase process Transliteration: In phase I, we used translation API's provided by",
        "type": "Document"
      },
      {
        "id": "29490b2d-e149-4ad5-997b-a1f87c010383",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1604.05781",
          "start_idx": 3952,
          "end_idx": 4432
        },
        "page_content": " statements will be considered in future work.) We considered only English-language tweets for this study. To avoid cross-language effects, we kept only tweets with a user-reported language of `English' and, as a second constraint, individual tweets needed to match more English stopwords than any other language's set of stopwords. Stopwords considered for each language were determined using NLTK's database BIBREF29 . A tweet will be referred to as a `document' for the rest of",
        "type": "Document"
      },
      {
        "id": "fe2e7d7b-3751-420b-9915-aa2b6ed930d5",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1904.00648",
          "start_idx": 6273,
          "end_idx": 6789
        },
        "page_content": " two methods: on one side, we use the entities extracted from the radio schedule for generating candidates entities in the user-generated tweets, thanks to a matching algorithm based on time proximity and string similarity. On the other side, we create a statistical model capable of detecting entities directly from the UGC, aimed to model the informal language of the raw texts. In Figure 1, an overview of the system proposed is presented. In May 2018, we crawled Twitter using the Python library Tweepy, creating",
        "type": "Document"
      },
      {
        "id": "991b756b-f91a-4b3b-8517-d070e3c27789",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1602.08741",
          "start_idx": 6528,
          "end_idx": 6973
        },
        "page_content": " criteria: matching to one or more search keywords, produced by subset of users, or coming from certain geo location. The last one provides the full set of Tweets, although it is not available by default. In order to get Twitter \u201cfirehose\u201d one can contact Twitter, or buy this stream from third-parties. In our case the simplest approach would be to use \u201csample\u201d endpoint, but it provides Tweets in all possible languages from all over the World",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What evaluations did the authors use on their system?",
    "llm_answer": "Human evaluation of grammatical correctness, contextual coherence, emotional appropriateness; relative ranking; direct assessment; head-to-head evaluation.\n",
    "context": [
      {
        "id": "4af29e90-8384-43de-b8a7-e6abec434471",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1709.10217",
          "start_idx": 8906,
          "end_idx": 9404
        },
        "page_content": " 2, there are 7 submitted systems. However, only 4 systems can provide correct results or be connected in a right way at the test phase. Therefore, Table TABREF16 shows the complete results of the task 2. In this paper, we introduce the first evaluation of Chinese human-computer dialogue technology. In detail, we first present the two tasks of the evaluation as well as the evaluation metrics. We then describe the released data for evaluation. Finally, we also show the evaluation results of the",
        "type": "Document"
      },
      {
        "id": "7a50363e-7a41-4b51-87e8-93fae1cdc277",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1709.10217",
          "start_idx": 2447,
          "end_idx": 3021
        },
        "page_content": " dialogue systems, especially considering the language characteristics of Chinese, we organize the first evaluation of Chinese human-computer dialogue technology. In this paper, we will present the evaluation scheme and the released corpus in detail. The rest of this paper is as follows. In Section 2, we will briefly introduce the first evaluation of Chinese human-computer dialogue technology, which includes the descriptions and the evaluation metrics of the two tasks. We then present the evaluation data and final results in Section 3 and 4 respectively, following the",
        "type": "Document"
      },
      {
        "id": "c9d4aed9-a995-46d8-9f3c-7c00d5917dbc",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1908.07816",
          "start_idx": 20712,
          "end_idx": 21186
        },
        "page_content": " TABREF35 summarize the human evaluation results on the responses' grammatical correctness, contextual coherence, and emotional appropriateness, respectively. In the tables, we give the percentage of votes each model received for the three scores, the average score obtained with improvements over S2S, and the agreement score among the raters. Note that we report Fleiss' $\\kappa $ score BIBREF27 for contextual coherence and emotional appropriateness, and Finn's $r$ score",
        "type": "Document"
      },
      {
        "id": "67747abf-6bd0-4403-ae57-fb90db5a6572",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1910.10408",
          "start_idx": 21469,
          "end_idx": 21983
        },
        "page_content": " goal is to be able to generate shorter translations and as close as possible to the length of the source sentences, we focused the manual evaluation on the Short output class and aimed to verify possible losses in quality with respect to the baseline system. We ran a head-to-head evaluation on the first 10 sentences of each test talk, for a total of 270 sentences, by asking annotators to blindly rank the two system outputs (ties were also permitted) in terms of quality with respect to a reference translation",
        "type": "Document"
      },
      {
        "id": "51ef2acd-49b2-43dd-895f-11019035f087",
        "metadata": {
          "chunk_id": 7,
          "document_id": "2004.01694",
          "start_idx": 3098,
          "end_idx": 3549
        },
        "page_content": " to the translation hint by human post-editors or raters, respectively. As part of the large-scale evaluation campaign at WMT, two primary evaluation methods have been used in recent years: relative ranking and direct assessment BIBREF9. In the case of relative ranking, raters are presented with outputs from two or more systems, which they are asked to evaluate relative to each other (e.g., to determine system A is better than system B). Ties (e.g",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What accuracy does CNN model achieve?",
    "llm_answer": "The CNN model's accuracy drops from around 0.8 to 0.5 when the noise rate increases from 0 to 0.5.\n",
    "context": [
      {
        "id": "63f3746e-d4b5-480a-aa8f-0fcd9fefecc0",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1909.00124",
          "start_idx": 13391,
          "end_idx": 13735
        },
        "page_content": "0, $0.1$, $0.2$, $0.3$, $0.4$, $0.5$] are shown in Figure FIGREF13. From the figure, we can see that the test accuracy drops from around 0.8 to 0.5 when the noise rate increases from 0 to 0.5, but our NetAb outperforms CNN. The results clearly show that the performance of the CNN drops quite a lot with the noise rate increasing. Experiment 2:",
        "type": "Document"
      },
      {
        "id": "f904e2e0-cddb-4c77-b068-a17ebc2fc287",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1911.07228",
          "start_idx": 8371,
          "end_idx": 8784
        },
        "page_content": " 3 shows perfomance of the BLSTM-CNN-CRF model. In our experiments, we use three evaluation parameters (precision, recall, and F1 score) to access our experimental result. They will be described as follow in Table 3. The \"correctNE\", the number of correct label for entity that the model can found. The \"goldNE\", number of the real label annotated by annotator in the gold data. The \"foundNE\", number of the label",
        "type": "Document"
      },
      {
        "id": "6cf0a06f-34c6-44b2-a329-2285cf3a8bc8",
        "metadata": {
          "chunk_id": 57,
          "document_id": "1910.04269",
          "start_idx": 21961,
          "end_idx": 22224
        },
        "page_content": " which is a recently developed technique to prevent over\ufb01tting on test data.This approach achieved an accuracy of 95.4%. We also analysed how attention mechanism and recurrent layers impact the performance of networks. This approach achieved an accuracy of 95.0%.",
        "type": "Document"
      },
      {
        "id": "881bf34f-07fa-436f-9986-d561e4e902c4",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1810.09774",
          "start_idx": 14308,
          "end_idx": 14688
        },
        "page_content": " accuracy of 22.5 points. Our baselines for SNLI (90.4%) and SNLI + MultiNLI (90.6%) outperform the previous state-of-the-art accuracy for SNLI (90.1%) by BIBREF24 . To understand better the types of errors made by neural network models in NLI we looked at some example failure-pairs for selected models. Tables 5 and 6 contain some randomly selected failure-pairs for two models:",
        "type": "Document"
      },
      {
        "id": "88d70074-2e19-4109-8c84-f28320fec049",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1911.07228",
          "start_idx": 7484,
          "end_idx": 7865
        },
        "page_content": " about 200k words Based on state-of-the-art methods for NER, BLSTM-CNN-CRF is the end-to-end deep neural network model that achieves the best result on F-score BIBREF9. Therefore, we decide to conduct the experiment on this model and analyze the errors. We run experiment with the Ma and Hovy (2016) model BIBREF8, source code provided by (Motoki Sato) and analysis the errors from",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many documents are in the Indiscapes dataset?",
    "llm_answer": "508\n",
    "context": [
      {
        "id": "f1f6ca13-a5a7-492e-a79c-d832c82f52f7",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1912.07025",
          "start_idx": 3958,
          "end_idx": 4356
        },
        "page_content": "14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20. No publicly available large-scale, annotated dataset of historical Indic manuscripts exists to the best of our knowledge. In contrast with existing collections, our proposed dataset contains a much larger diversity in terms of document type (palm-leaf and early paper), scripts and annotated layout elements (see Tables TABREF5,TABREF",
        "type": "Document"
      },
      {
        "id": "42b3ff4e-a62c-4498-a7f9-0989543bfb23",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1912.07025",
          "start_idx": 8670,
          "end_idx": 9168
        },
        "page_content": " it truly an end-to-end system. Overall, our dataset contains 508 annotated Indic manuscripts. Some salient aspects of the dataset can be viewed in Table TABREF5 and a pictorial illustration of layout regions can be viewed in Figure FIGREF13. Note that multiple regions can overlap, unlike existing historical document datasets which typically contain disjoint region annotations. For the rest of the section, we discuss the challenges associated with annotating Indic manuscripts (Section SECREF9)",
        "type": "Document"
      },
      {
        "id": "82add19d-4a04-4694-b8a9-fdf21fc005ec",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1912.07025",
          "start_idx": 705,
          "end_idx": 1221
        },
        "page_content": " within these documents before it is lost forever. Surprisingly, no large-scale annotated Indic manuscript image datasets exist for the benefit of researchers in the community. In this paper, we take a significant step to address this gap by creating such a dataset. Given the large diversity in language, script and non-textual regional elements in these manuscripts, spatial layout parsing is crucial in enabling downstream applications such as OCR, word-spotting, style-and-content based retrieval and clustering.",
        "type": "Document"
      },
      {
        "id": "2d3bdfec-4ed5-456d-bd82-023fad798858",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "4565fdf8-dcca-4c1d-82f5-cf6399445668",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1912.07025",
          "start_idx": 6986,
          "end_idx": 7473
        },
        "page_content": "38. The Indic manuscript document images in our dataset are obtained from two sources. The first source is the publicly available Indic manuscript collection from University of Pennsylvania's Rare Book and Manuscript Library BIBREF39, also referred to as Penn-in-Hand (PIH). From the $2{,}880$ Indic manuscript book-sets, we carefully curated 193 manuscript images for annotation. Our curated selection aims to maximize the diversity of the dataset in terms of various attributes such as",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are simulated datasets collected?",
    "llm_answer": "A dataset of 8066 pairs of free-form natural language instructions and navigation plans for training, collected from 88 unique simulated environments.  A prototype dataset corresponding to videos from the Charades dataset, and an official dataset using the Charades sets.\n",
    "context": [
      {
        "id": "cdfc8855-8140-4102-951c-82a8fda5dfdf",
        "metadata": {
          "chunk_id": 57,
          "document_id": "1809.08731",
          "start_idx": 19179,
          "end_idx": 19597
        },
        "page_content": " scenarios; the reason we use them is that they allow for a proof of concept. In this setting, we train scikit's BIBREF18 support vector regression model (SVR) with the default parameters on predicting fluency, given WPSLOR and ROUGE-L-mult. We use 500 of our total 2955 examples for each of training and development, and the remaining 1955 for testing. Second, we simulate a setting in which we have only access to (i",
        "type": "Document"
      },
      {
        "id": "750cf7d4-eaf2-41cf-a5fe-3fdf7b919809",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1810.00663",
          "start_idx": 17810,
          "end_idx": 18361
        },
        "page_content": " . This dataset was created using Amazon Mechanical Turk and 100 maps of simulated indoor environments, each with 6 to 65 rooms. To the best of our knowledge, this is the first benchmark for comparing translation models in the context of behavioral robot navigation. As shown in Table TABREF16 , the dataset consists of 8066 pairs of free-form natural language instructions and navigation plans for training. This training data was collected from 88 unique simulated environments, totaling 6064 distinct navigation plans (2002 plans have two different",
        "type": "Document"
      },
      {
        "id": "46696be8-fc07-4b91-a6b4-1f1160ed56a3",
        "metadata": {
          "chunk_id": 52,
          "document_id": "1709.01256",
          "start_idx": 19441,
          "end_idx": 19942
        },
        "page_content": " We removed empty lines in the documents and trained word2vec embeddings on the entire corpus. We used the documents occurring in the first INLINEFORM0 of the revision period for INLINEFORM1 calibration, and the remaining documents for test. The generation process of the simulated data sets is designed to mimic the real world. Users open some existing documents in a file system, make some changes (e.g. addition, deletion or replacement), and save them as separate documents. These documents become",
        "type": "Document"
      },
      {
        "id": "e9c5de5b-1185-4e43-ac74-e5a3c94581a3",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1812.07023",
          "start_idx": 10725,
          "end_idx": 11271
        },
        "page_content": " prototype dataset, the training set of the avsd dataset corresponds to videos taken from the training set of the Charades dataset while the validation and test sets of the avsd dataset correspond to videos taken from the validation set of the Charades dataset. For the official dataset, training, validation and test sets are drawn from the corresponding Charades sets. The Charades dataset also provides additional annotations for the videos such as action, scene, and object annotations, which are considered to be external data sources by the",
        "type": "Document"
      },
      {
        "id": "75c8f911-af20-49c1-8b12-e4aecd8370c1",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1707.03904",
          "start_idx": 5304,
          "end_idx": 5850
        },
        "page_content": " subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short. We plan to release these annotations along with the datasets, and our retrieved documents for each question. Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What human evaluation metrics were used in the paper?",
    "llm_answer": "Relevance, conciseness, and readability, scored on a 1 to 5 scale.  Pearson correlation with human judgments.\n",
    "context": [
      {
        "id": "67a7695f-67b2-4acb-98f7-3e39a52c72ef",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1910.00825",
          "start_idx": 16888,
          "end_idx": 17413
        },
        "page_content": " our method's increased performance on automatic evaluation metrics entails better human perceived quality. We randomly select 100 test samples from MultiWOZ test set for evaluation. We recruit 150 crowd workers from Amazon Mechanical Turk. For each sample, we show the conversation, reference summary, as well as summaries generated by Pointer-Generator and SPNet to three different participants. The participants are asked to score each summary on three indicators: relevance, conciseness and readability on a 1 to 5 scale,",
        "type": "Document"
      },
      {
        "id": "748fb5f2-01c6-4839-9bc8-6212bf55e7d1",
        "metadata": {
          "chunk_id": 3,
          "document_id": "2004.01694",
          "start_idx": 1337,
          "end_idx": 1854
        },
        "page_content": " concur that these findings have demonstrated the need to critically re-evaluate the design of human MT evaluation. Our paper investigates three aspects of human MT evaluation, with a special focus on assessing human\u2013machine parity: the choice of raters, the use of linguistic context, and the creation of reference translations. We focus on the data shared by BIBREF3, and empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation. We find that for all three aspects",
        "type": "Document"
      },
      {
        "id": "deb96dd0-6ed3-4ea6-9537-59fa26e50f65",
        "metadata": {
          "chunk_id": 33,
          "document_id": "1809.08731",
          "start_idx": 11198,
          "end_idx": 11606
        },
        "page_content": "\\text{NCE}(S))$$   (Eq. 24)  Due to its popularity, we also performed initial experiments with BLEU BIBREF17 . Its correlation with human scores was so low that we do not consider it in our final experiments. Following earlier work BIBREF2 , we evaluate our metrics using Pearson correlation with human judgments. It is defined as the covariance divided by the product of the standard deviations:  $$\\rho _{X",
        "type": "Document"
      },
      {
        "id": "54d66760-2c41-465c-88e6-57d070d341e8",
        "metadata": {
          "chunk_id": 67,
          "document_id": "1809.08731",
          "start_idx": 22769,
          "end_idx": 23227
        },
        "page_content": " single reference. This was supported by isabelle-cherry-foster:2017:EMNLP2017, who proposed a so-called challenge set approach as an alternative. graham-EtAl:2016:COLING performed a large-scale evaluation of human-targeted metrics for machine translation, which can be seen as a compromise between human evaluation and fully automatic metrics. They also found fully automatic metrics to correlate only weakly or moderately with human judgments. bojar2016ten",
        "type": "Document"
      },
      {
        "id": "2d3984d1-024e-4608-a5c4-4823d3344fdc",
        "metadata": {
          "chunk_id": 85,
          "document_id": "2004.01694",
          "start_idx": 32633,
          "end_idx": 33097
        },
        "page_content": " a small number of human raters: five in the experiments of Sections SECREF3 and SECREF35, four per condition (adequacy and fluency) in Section SECREF4, and one in the fine-grained error analysis presented in Section SECREF24. Moreover, the results presented in this article are based on one text domain (news) and one language direction (Chinese to English), and while a large-scale evaluation with another language pair supports our findings (see above), further",
        "type": "Document"
      }
    ]
  },
  {
    "query": "For the purposes of this paper, how is something determined to be domain specific knowledge?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "319c9757-9bd8-4aed-9974-60604182d684",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1701.03214",
          "start_idx": 5368,
          "end_idx": 5787
        },
        "page_content": " (essentially open) in-domain data. The scientific domain MT was conducted on the Chinese-Japanese paper excerpt corpus (ASPEC-CJ) BIBREF11 , which is one subtask of the workshop on Asian translation (WAT) BIBREF15 . The ASPEC-CJ task uses 672315, 2090, and 2107 sentences for training, development, and testing, respectively. The Wikipedia domain task was conducted on a Chinese-Japanese corpus automatically extracted",
        "type": "Document"
      },
      {
        "id": "e9ff0ea6-7735-4c7f-b973-c089ea86b236",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1701.03214",
          "start_idx": 4004,
          "end_idx": 4439
        },
        "page_content": "-of-domain data is used for the in-domain data; while for the \u201cmulti domain\u201d and \u201cmixed fine tuning\u201d methods, we use a vocabulary obtained from the mixed in-domain and out-of-domain data for all the training stages. We conducted NMT domain adaptation experiments in two different settings as follows: Chinese-to-English translation was the focus of the high quality in-domain corpus setting. We utilized the resource rich patent out-of",
        "type": "Document"
      },
      {
        "id": "7d53bec7-0b4e-4947-bd50-293be84c55b5",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1905.06906",
          "start_idx": 5943,
          "end_idx": 6219
        },
        "page_content": " task in target domain. Domain Adaptation (DA) is defined by the target predictive function $D_{S}$8 calculated using the knowledge of $D_{S}$9 and $(x_{s_{1}},y_{s_{1}})$0 where $(x_{s_{1}},y_{s_{1}})$1 but $(x_{s_{1}},y_{s_{1}})$2 . It is imperative to note that the domains",
        "type": "Document"
      },
      {
        "id": "3343d9c8-a357-45f7-adff-993911df0c4f",
        "metadata": {
          "chunk_id": 1,
          "document_id": "1902.00330",
          "start_idx": 505,
          "end_idx": 1046
        },
        "page_content": " information retrieval (IE) and has many applications, such as knowledge base population (KBP), question answering (QA), etc. Existing EL methods can be divided into two categories: local model and global model. Local models concern mainly on contextual words surrounding the mentions, where mentions are disambiguated independently. These methods are not work well when the context information is not rich enough. Global models take into account the topical coherence among the referred entities within the same document, where mentions are",
        "type": "Document"
      },
      {
        "id": "95ee1d94-c198-49b3-b762-9125b49a935c",
        "metadata": {
          "chunk_id": 28,
          "document_id": "2002.11893",
          "start_idx": 10717,
          "end_idx": 11221
        },
        "page_content": " before the conversation, we specially design cross-domain informable slots (bold values) whose values refer to other sub-goals. Cross-domain informable slots utilize sub-goal id to connect different sub-goals. Thus the actual constraints vary according to the different contexts instead of being pre-specified. The values of common informable slots are sampled randomly from the database. Based on the informable slots, users are required to gather the values of requestable slots (blank values in Table",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What type of model are the ELMo representations used in?",
    "llm_answer": "CNNs and bi-directional LSTMs.\n",
    "context": [
      {
        "id": "ec807420-83f2-460d-a372-b3de1fbf9150",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1809.09795",
          "start_idx": 5437,
          "end_idx": 5886
        },
        "page_content": ", our architecture is based on Embeddings from Language Model or ELMo BIBREF10 . The ELMo layer allows to recover a rich 1,024-dimensional dense vector for each word. Using CNNs, each vector is built upon the characters that compose the underlying words. As ELMo also contains a deep bi-directional LSTM on top of this character-derived vectors, each word-level embedding contains contextual information from their surroundings. Concretely, we use a",
        "type": "Document"
      },
      {
        "id": "95031564-64b5-4309-9bee-9ae0ab6bef39",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1810.09774",
          "start_idx": 11025,
          "end_idx": 11459
        },
        "page_content": " , BIBREF2 and BIBREF0 respectively. For BERT we used the uncased 768-dimensional model (BERT-base). For ESIM + ELMo we used the AllenNLP BIBREF23 PyTorch implementation with the default settings and hyperparameter values. Table 4 contains all the experimental results. Our experiments show that, while all of the six models perform well when the test set is drawn from the same corpus as the training and development set, accuracy is",
        "type": "Document"
      },
      {
        "id": "1d46cce7-14e7-442d-bb5e-4d7eaa7afbe1",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1911.03894",
          "start_idx": 4523,
          "end_idx": 4978
        },
        "page_content": " have been released: ELMo models for Japanese, Portuguese, German and Basque and BERT for Simplified and Traditional Chinese and German. However, to the best of our knowledge, no particular effort has been made toward training models for languages other than English, at a scale similar to the latest English models (e.g. RoBERTa trained on more than 100GB of data). Our approach is based on RoBERTa BIBREF9, which replicates and improves the initial BERT",
        "type": "Document"
      },
      {
        "id": "5a56fb7d-1806-4c70-bb3f-7db05cb9d99f",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1809.09795",
          "start_idx": 2393,
          "end_idx": 2864
        },
        "page_content": " word representations, which have recently been used to achieve the state of the art on six NLP tasks, including sentiment analysis BIBREF10 . We test our proposed architecture on 7 different irony/sarcasm datasets derived from 3 different data sources, providing state-of-the-art performance in 6 of them and otherwise offering competitive results, showing the effectiveness of our proposal. We make our code available at https://github.com/epochx/elmo4irony. Apart from",
        "type": "Document"
      },
      {
        "id": "222e10ae-4255-4ab7-9aec-1ed9bf42eff8",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1810.09774",
          "start_idx": 9942,
          "end_idx": 10400
        },
        "page_content": " is particularly interesting in this context as it performed significantly better than other models in the Breaking NLI experiment conducted by BIBREF1 . The success of pre-trained language models in multiple NLP tasks make ESIM + ELMo and BERT interesting additions to this experiment. Table 3 lists the different models used in the experiments. For BiLSTM-max we used the Adam optimizer BIBREF21 , a learning rate of 5e-4 and batch size of 64. The learning",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does using phonetic feedback improve state-of-the-art systems?",
    "llm_answer": "The answer is not found in the context.  While the text highlights improvements from using phonetic feedback, it does not quantify the improvement relative to \"state-of-the-art\" systems.\n",
    "context": [
      {
        "id": "6a211ace-fc4a-4de9-bd9c-e230da613a02",
        "metadata": {
          "chunk_id": 31,
          "document_id": "2003.01769",
          "start_idx": 12435,
          "end_idx": 12978
        },
        "page_content": " well as the mimic setup. Compared to the previous experiment without parallel data, the presence of the spectral magnitude and time-domain losses likely keep the enhancement output more stable when joint training, at the cost of requiring parallel training data. We have shown that phonetic feedback is valuable for speech enhancement systems. In addition, we show that our approach to this feedback, the mimic loss framework, is useful in many scenarios: with and without the presence of parallel data, in both the enhancement and robust ASR",
        "type": "Document"
      },
      {
        "id": "175d8112-3d13-4cab-bfee-16cca561311f",
        "metadata": {
          "chunk_id": 1,
          "document_id": "2003.01769",
          "start_idx": 594,
          "end_idx": 1096
        },
        "page_content": " learn the phonetic patterns of the speech signal in the coordinated output of time-domain or time-frequency units. However, our hypothesis is that directly providing phonetic feedback can be a powerful additional signal for speech enhancement. For example, many local metrics will be more attuned to high-energy regions of speech, but not all phones of a language carry equal energy in production (compare /v/ to /ae/). Our proxy for phonetic intelligibility is a frozen automatic speech recognition (",
        "type": "Document"
      },
      {
        "id": "ed695636-c2f6-4d5a-b9ac-1f490be5610b",
        "metadata": {
          "chunk_id": 3,
          "document_id": "2003.01769",
          "start_idx": 1439,
          "end_idx": 1983
        },
        "page_content": " provides key linguistic insights to the enhancement model about what a recognizable phoneme looks like. When no parallel data is available, but transcripts are available, a loss is easily computed against hard senone labels and backpropagated to the enhancement model trained from scratch. Since the clean acoustic model is frozen, the only way for the enhancement model to improve the loss is to make a signal that is more recognizable to the acoustic model. The improvement by this model demonstrates the power of phonetic feedback; very few",
        "type": "Document"
      },
      {
        "id": "6e31b4c2-c658-424c-beb9-1da74995dcd2",
        "metadata": {
          "chunk_id": 4,
          "document_id": "2003.01769",
          "start_idx": 1755,
          "end_idx": 2293
        },
        "page_content": " to the acoustic model. The improvement by this model demonstrates the power of phonetic feedback; very few neural enhancement techniques until now have been able to achieve improvements without parallel data. When parallel data is available, mimic loss works by comparing the outputs of the acoustic model on clean speech with the outputs of the acoustic model on denoised speech. This is a more informative loss than the loss against hard senone labels, and is complimentary to local losses. We show that mimic loss can be applied to an",
        "type": "Document"
      },
      {
        "id": "99aa59ef-b5ee-40f0-a832-bcb982d5ab1f",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1909.02764",
          "start_idx": 5772,
          "end_idx": 6281
        },
        "page_content": "2004automatic compare local and global features in support vector machines. Next to such discriminative approaches, hidden Markov models are well-studied, however, there is no agreement on which feature-based classifier is most suitable BIBREF13. Similar to the facial expression modality, recent efforts on applying deep learning have been increased for acoustic speech processing. For instance, lee2015high use a recurrent neural network and palaz2015analysis apply a convolutional neural network to the raw",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what are the baselines?",
    "llm_answer": "content, content-network, image-based models;  BiRNN language model; GA model; LSTM baseline;  TF-IDF, S2S, IR, Proposed+IR\n",
    "context": [
      {
        "id": "c9d62915-21d1-42b6-a16c-8d9d28de6e0c",
        "metadata": {
          "chunk_id": 91,
          "document_id": "1902.06843",
          "start_idx": 35139,
          "end_idx": 35416
        },
        "page_content": "s by -0.27. Baselines: To test the efficacy of our multi-modal framework for detecting depressed users, we compare it against existing content, content-network, and image-based models (based on the aforementioned general image feature, facial presence, and facial expressions.)",
        "type": "Document"
      },
      {
        "id": "0d211bed-e5df-4233-8d02-e24d7ee9b65c",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1909.02635",
          "start_idx": 9050,
          "end_idx": 9548
        },
        "page_content": " (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo",
        "type": "Document"
      },
      {
        "id": "3f956a4e-f9f6-4339-b75c-d529dbc23a27",
        "metadata": {
          "chunk_id": 53,
          "document_id": "1707.03904",
          "start_idx": 20086,
          "end_idx": 20524
        },
        "page_content": " \u2013 finding the few most relevant ones will allow the reader to work best. In Tables 2 and 3 we compare all baselines when the context size is tuned to maximize the overall accuracy on the validation set. For Quasar-S the best performing baseline is the BiRNN language model, which achieves $33.6\\%$ accuracy. The GA model achieves $48.3\\%$ accuracy on the set of instances for which the answer is in context, however, a search accuracy of",
        "type": "Document"
      },
      {
        "id": "b2775552-5ca7-4d50-97ee-2a3b6b097611",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1810.09774",
          "start_idx": 6998,
          "end_idx": 7473
        },
        "page_content": " SNLI + MultiNLI we report the baseline using the SNLI test data. All the experimental combinations are listed in Table 1 . Examples from the selected datasets are provided in Table 2 . To be more precise, we vary three things: training dataset, model and testing dataset. We should qualify this though, since the three datasets we look at, can also be grouped by text domain/genre and type of data collection, with MultiNLI and SNLI using the same data collection style, and",
        "type": "Document"
      },
      {
        "id": "7d4754c7-4ea8-4d20-9fb6-3d1e50bf0cf6",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1809.04960",
          "start_idx": 22497,
          "end_idx": 22960
        },
        "page_content": " the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types of comments as described in the above retrieval evaluation setting: Correct, Plausible, Popular, and Random. We rank the candidate comment set with four models (TF-IDF, S2S, IR, and Proposed+IR), and record the types of top-1 comments. Figure FIGREF40 shows the percentage of different types of top-1 comments generated by each model. It shows",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the results of the experiment?",
    "llm_answer": "Table TABREF19 shows results for in-domain, out-of-domain, and transfer learning experiments.  A baseline model was trained and validated on corpora.\n",
    "context": [
      {
        "id": "78dd82c6-c6af-4015-b4e0-f73037eb863b",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1904.05862",
          "start_idx": 15343,
          "end_idx": 15454
        },
        "page_content": "letter++ experiments, and Tatiana Likhomanenko for providing convolutional language models for our experiments.",
        "type": "Document"
      },
      {
        "id": "c90edf37-111f-4559-92ba-a4c63cb24b6a",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1909.02764",
          "start_idx": 11227,
          "end_idx": 11795
        },
        "page_content": " which enables the experimenter to manually trigger prerecorded voice samples playing trough the in-car speakers and to bring new content to the center screen. Figure FIGREF4 shows the driving simulator. The experimental setting is comparable to an everyday driving task. Participants are told that the goal of the study is to evaluate and to improve an intelligent driving assistant. To increase the probability of emotions to arise, participants are instructed to reach the destination of the route as fast as possible while following traffic rules and speed limits.",
        "type": "Document"
      },
      {
        "id": "64a25737-469b-4e82-875c-ba81cb2e5018",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1701.00185",
          "start_idx": 3790,
          "end_idx": 4304
        },
        "page_content": " this paper we use a new short text dataset, Biomedical, in the experiment to verify the effectiveness of our approach. Third, we put much effort on studying the influence of various different semantic features integrated in our self-taught CNN framework, which is not involved in the conference paper. For the purpose of reproducibility, we make the datasets and software used in our experiments publicly available at the website. The remainder of this paper is organized as follows: In Section SECREF2 , we first",
        "type": "Document"
      },
      {
        "id": "5044ece2-096a-4ad7-9403-a68458623ed0",
        "metadata": {
          "chunk_id": 41,
          "document_id": "1910.11949",
          "start_idx": 15508,
          "end_idx": 15935
        },
        "page_content": " : i like to eat healthy $>$ have you seen the last chapter? Elisabot : no i have not . $>$ where do you live? Elisabot : i live in the mountains As most of the metrics correlate very weakly with human judgements in the non-technical domain BIBREF31 we decide to evaluate our system with a simple user study with two patients. We present the user interface built and the feedback obtained from the patients. We developed a user",
        "type": "Document"
      },
      {
        "id": "65333987-2358-483b-95cd-3ff028ff6a96",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How was the dataset collected?",
    "llm_answer": "The StackOverflow dataset is from Kaggle, randomly selecting 20,000 question titles from 20 tags.  The Biomedical dataset is from BioASQ, randomly selecting 20,000 paper titles from 20 different ones.  Other datasets are collected via crowdsourcing or from examinations.\n",
    "context": [
      {
        "id": "75c8f911-af20-49c1-8b12-e4aecd8370c1",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1707.03904",
          "start_idx": 5304,
          "end_idx": 5850
        },
        "page_content": " subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short. We plan to release these annotations along with the datasets, and our retrieved documents for each question. Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated",
        "type": "Document"
      },
      {
        "id": "af27ee8c-701c-4fe4-b55b-63a0e04b788a",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1701.00185",
          "start_idx": 18784,
          "end_idx": 19212
        },
        "page_content": " BIBREF41 . StackOverflow. We use the challenge data published in Kaggle.com. The raw dataset consists 3,370,528 samples through July 31st, 2012 to August 14, 2012. In our experiments, we randomly select 20,000 question titles from 20 different tags as in Table TABREF25 . Biomedical. We use the challenge data published in BioASQ's official website. In our experiments, we randomly select 20, 000 paper titles from 20 different",
        "type": "Document"
      },
      {
        "id": "fb4f7b01-7328-425d-9060-5b0bfa587873",
        "metadata": {
          "chunk_id": 75,
          "document_id": "1910.00458",
          "start_idx": 26170,
          "end_idx": 26674
        },
        "page_content": "26 are human-generated and based on source documents or summaries in free text format. However, since annotators tend to copy spans as answers BIBREF27, the majority of answers are still extractive in these datasets. The multi-choice QA datasets are collected either via crowd sourcing, or collected from examinations designed by educational experts BIBREF7. In this type of QA datasets, besides token matching, a significant portion of questions require multi-sentence reasoning and external knowledge B",
        "type": "Document"
      },
      {
        "id": "f420687c-1cd6-4839-be87-8508949281e2",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1707.03904",
          "start_idx": 14370,
          "end_idx": 14849
        },
        "page_content": "\ufffdopen-book\u201d setting, where they had access to a search engine over the short pseudo-documents extracted for each dataset (as described in Section \"Context Retrieval\" ). We decided to use short pseudo-documents for this exercise to reduce the burden of reading on the volunteers, though we note that the long pseudo-documents have greater coverage of answers. We also asked the volunteers to provide annotations to categorize the type of each question they were asked, and a label",
        "type": "Document"
      },
      {
        "id": "82c8dd79-f34e-4f24-a750-66441024005b",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1610.00879",
          "start_idx": 4217,
          "end_idx": 4654
        },
        "page_content": "damn', along with profane words. Heuristics other than these hashtags could have been used for dataset creation. For example, timestamps were a good option to account for time at which a tweet was posted. However, this could not be used because user's local times was not available, since very few users had geolocation enabled. The complete set of features is shown in Table TABREF7 . There are two sets of features: (a) N-gram features",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many annotators participated?",
    "llm_answer": "Four\n",
    "context": [
      {
        "id": "7ad84a74-dfae-4c36-99a3-0c3aa9c1ab59",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1703.05260",
          "start_idx": 17551,
          "end_idx": 18087
        },
        "page_content": " results were discussed with the authors. Throughout the whole annotation phase, annotators could discuss any emerging issues with the authors. All annotations were done by undergraduate students of computational linguistics. The annotation was rather time-consuming due to the complexity of the task, and thus we decided for single annotation mode. To assess annotation quality, a small sample of texts was annotated by all four annotators and their inter-annotator agreement was measured (see Section \"Inter-Annotator Agreement\" ). It",
        "type": "Document"
      },
      {
        "id": "86148570-c570-4e16-9eb0-b05217e1148b",
        "metadata": {
          "chunk_id": 33,
          "document_id": "1912.01673",
          "start_idx": 12785,
          "end_idx": 13244
        },
        "page_content": " how many annotators saw the same input sentence: 30 annotations were annotated by one person only, 30 annotations by two different persons etc. The last column shows the number of unique outputs obtained in that group. Across all cases, 96.8% of produced strings were unique. In line with instructions, the annotators were using the IMPOSSIBLE option scarcely (95 times, i.e. only 2%). It was also a case of 7 annotators only; the remaining 5 annotators were",
        "type": "Document"
      },
      {
        "id": "d57b48a2-0597-49c7-ba89-594a0726bee9",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1703.05260",
          "start_idx": 17876,
          "end_idx": 18350
        },
        "page_content": " inter-annotator agreement was measured (see Section \"Inter-Annotator Agreement\" ). It was found to be sufficiently high. Annotation of the corpus together with some pre- and post-processing of the data required about 500 hours of work. All stories were annotated with event and participant types (a total of 12,188 and 43,946 instances, respectively). On average there were 7 coreference chains per story with an average length of 6 tokens. After the first annotation round",
        "type": "Document"
      },
      {
        "id": "ad8f57e6-9103-48d1-bcd3-77aa60670c8e",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1906.06448",
          "start_idx": 11534,
          "end_idx": 12014
        },
        "page_content": " the premise and/or the hypothesis is ungrammatical or does not make sense. Workers were paid US$0.04 for each question, and each question was assigned to three workers. To collect high-quality annotation results, we imposed ten test questions on each worker, and removed workers who gave more than three wrong answers. We also set the minimum time it should take to complete each question to 200 seconds. 1,237 workers contributed to this task, and we annotated gold labels of 15",
        "type": "Document"
      },
      {
        "id": "51967a51-6d78-44f6-802a-a0611befeb24",
        "metadata": {
          "chunk_id": 67,
          "document_id": "1703.05260",
          "start_idx": 23974,
          "end_idx": 24460
        },
        "page_content": " 3 annotators (qualified majority vote) compared to the set of those pairs annotated by at least one person (see Figure 4 ). We take the result of 90.5% between annotators to be a good agreement. Figure 5 gives an overview of the number of event and participant types provided in the templates. Taking a flight and getting a haircut stand out with a large number of both event and participant types, which is due to the inherent complexity of the scenarios. In contrast, planting a tree",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they show their model discovers underlying syntactic structure?",
    "llm_answer": "The model autonomously discovered to avoid inter-word attention connections, and use the hidden states of space (separator) tokens to summarize previous information.  This is presented as strong proof of the model's understanding of latent data structure.  Unsupervised parsing results also outperform some baseline models, demonstrating a structure similar to human expert-provided structure.\n",
    "context": [
      {
        "id": "62e73640-ff45-4334-9977-87f9e84aced6",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1906.00180",
          "start_idx": 3986,
          "end_idx": 4571
        },
        "page_content": " consistently higher testing accuracy provides evidence that the recurrent networks are not only capable of recognizing FOL entailment relations between unseen sentences. They can also outperform the tree-shaped models on this task, although they do not use any of the symbolic structure that seemed to explain the success of their recursive predecessors. The recurrent classifiers have learned to apply their own strategies, which we will investigate in the remainder of this paper. Compositionality is the ability to interpret and generate a possibly infinite number of constructions",
        "type": "Document"
      },
      {
        "id": "b43f97c9-a459-4f8e-8c48-ccd33c7d6be0",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1711.02013",
          "start_idx": 18748,
          "end_idx": 19243
        },
        "page_content": " the last space step. The model autonomously discovered to avoid inter-word attention connection, and use the hidden states of space (separator) tokens to summarize previous information. This is strong proof that the model can understand the latent structure of data. As a result our model achieve state-of-the-art performance and significantly outperform baseline models. It is worth noting that HM-LSTM BIBREF6 also unsupervisedly induce similar structure from data. But discrete operations in",
        "type": "Document"
      },
      {
        "id": "25d6b104-f494-424e-a98f-77d643f000c2",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1906.00180",
          "start_idx": 16996,
          "end_idx": 17562
        },
        "page_content": " nontrivial hierarchical structure. The formal language we defined did not exploit the full expressive power of first-order predicate logic; nevertheless by using standard first-order predicate logic, a standard theorem prover, and a set-up where the training set only covers a tiny fraction of the space of possible logical expressions, our experiments avoid the problems observed in earlier attempts to demonstrate logical reasoning in recurrent networks. The experiments performed in the last few sections moreover show that the GRU and LSTM architectures exhibit",
        "type": "Document"
      },
      {
        "id": "de8099c0-ae1d-4506-895d-9019fcb7ebbb",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1711.02013",
          "start_idx": 3137,
          "end_idx": 3650
        },
        "page_content": "level and character-level language modeling. The model's unsupervised parsing outperforms some strong baseline models, demonstrating that the structure found by our model is similar to the intrinsic structure provided by human experts. The idea of introducing some structures, especially trees, into language understanding to help a downstream task has been explored in various ways. For example, BIBREF5 , BIBREF4 learn a bottom-up encoder, taking as an input a parse tree supplied from an external parser. There",
        "type": "Document"
      },
      {
        "id": "d9474388-f3d6-44c2-a88d-471b6c36b6d4",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1711.02013",
          "start_idx": 5188,
          "end_idx": 5657
        },
        "page_content": ", while ours control it softly through an attention mechanism. In terms of language modeling, syntactic language modeling can be dated back to BIBREF27 . BIBREF28 , BIBREF29 have also proposed language models with a top-down parsing mechanism. Recently BIBREF30 , BIBREF31 have introduced neural networks into this space. It learns both a discriminative and a generative model with top-down parsing, trained with a supervision signal from parsed sentences in the corpus",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they measure performance of language model tasks?",
    "llm_answer": "Accuracy is used to evaluate NLI performance.  For languages other than English, the test set is translated into English and used with an English classification model.  Other metrics, like TrueSkill, are used to evaluate translations.\n",
    "context": [
      {
        "id": "74d41b8d-63d7-479f-a82f-76142d721807",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1911.03894",
          "start_idx": 15623,
          "end_idx": 16121
        },
        "page_content": " is also machine translated for all languages. The dataset is composed of 122k train, 2490 valid and 5010 test examples. As usual, NLI performance is evaluated using accuracy. To evaluate a model on a language other than English (such as French), we consider the two following settings: TRANSLATE-TEST: The French test set is machine translated into English, and then used with an English classification model. This setting provides a reasonable, although imperfect, way to circumvent the fact that",
        "type": "Document"
      },
      {
        "id": "58e15f56-c72b-4674-af7a-104b8db45293",
        "metadata": {
          "chunk_id": 29,
          "document_id": "2004.01694",
          "start_idx": 11778,
          "end_idx": 12167
        },
        "page_content": "-experts were NLP researchers native in Chinese, working in an English-speaking country. The ratings are elicited with Appraise BIBREF21. We derive an overall score for each translation (H$_A$, MT$_1$, and MT$_2$) based on the rankings. We use the TrueSkill method adapted to MT evaluation BIBREF22 following its usage at WMT15, i. e., we run 1,000 iterations of the rankings recorded with",
        "type": "Document"
      },
      {
        "id": "b4048d90-aa2a-4fd2-8f5e-b7848534cb17",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1910.04269",
          "start_idx": 17760,
          "end_idx": 18217
        },
        "page_content": " of 1,500 samples for each of six languages. Out of 1,500 samples for each language, 1,200 were randomly selected as training dataset for that language and rest 300 as validation dataset using k-fold cross-validation. To sum up, we trained our model on 7,200 samples and validated it on 1800 samples comprising six languages. The results are discussed in next section. This paper discusses two end-to-end approaches which achieve state-of-the-art results in",
        "type": "Document"
      },
      {
        "id": "5a72bbab-2f01-467a-b6ba-1642e748f5b2",
        "metadata": {
          "chunk_id": 34,
          "document_id": "2001.00137",
          "start_idx": 13325,
          "end_idx": 13822
        },
        "page_content": " of our model's robustness to different rates of data incompleteness. The dataset used to evaluate the models' performance is the Chatbot Natural Language Unerstanding (NLU) Evaluation Corpus, introduced by Braun et al. BIBREF20 to test NLU services. It is a publicly available benchmark and is composed of sentences obtained from a German Telegram chatbot used to answer questions about public transport connections. The dataset has two intents, namely Departure Time and Find Connection with 100",
        "type": "Document"
      },
      {
        "id": "a60aac12-cbc5-4035-ad8e-42926b46c10d",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1910.05456",
          "start_idx": 12667,
          "end_idx": 13138
        },
        "page_content": " 1000, and 10000 examples, respectively. For all L1 languages, we train our models on the high-resource datasets with 10000 examples. For fine-tuning, we use the low-resource datasets. In Table TABREF18, we show the final test accuracy for all models and languages. Pretraining on EUS and NAV results in the weakest target language inflection models for ENG, which might be explained by those two languages being unrelated to ENG and making at least partial use of prefix",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are content clusters used to improve the prediction of incident severity?",
    "llm_answer": "Content clusters are used to train a supervised classifier that predicts the degree of harm of an incident based on features of the record (location, external category, medical specialty), and the text itself.\n",
    "context": [
      {
        "id": "65c4b13e-f3c9-4d34-84e0-8cdb0039dee3",
        "metadata": {
          "chunk_id": 140,
          "document_id": "1909.00183",
          "start_idx": 51827,
          "end_idx": 52413
        },
        "page_content": " higher agreement in the severe and deadly incidents. These results confirm that our method performs as well as the original annotators but is better at identifying risky events. We have applied a multiscale graph partitioning algorithm (Markov Stability) to extract content-based clusters of documents from a textual dataset of incident reports in an unsupervised manner at different levels of resolution. The method uses paragraph vectors to represent the records and analyses the ensuing similarity graph of documents through multi-resolution capabilities to capture clusters without",
        "type": "Document"
      },
      {
        "id": "26dbef4b-b98d-4aa9-8583-ce4704241ff5",
        "metadata": {
          "chunk_id": 117,
          "document_id": "1909.00183",
          "start_idx": 43409,
          "end_idx": 43881
        },
        "page_content": " clusters) does Infomap, which partitions graphs into small clique-like subgraphs BIBREF40, BIBREF56, provide a slightly improved $NMI$. Therefore, MS finds both relevant and high quality clusterings across all scales by sweeping the Markov time parameter. Here we approach the task of training a supervised classifier that predicts the degree of harm of an incident based on other features of the record (such as location, external category, and medical specialty) and on",
        "type": "Document"
      },
      {
        "id": "99e1de61-535d-4b1a-9c64-614398db0838",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1909.00183",
          "start_idx": 2360,
          "end_idx": 2871
        },
        "page_content": " system. Such tools can also offer unbiased insight into the root cause analysis of incidents that could improve the safety and quality of care and efficiency of healthcare services. In this work, we showcase an algorithmic methodology that detects content-based groups of records in an unsupervised manner, based only on the free (unstructured) textual descriptions of the incidents. To do so, we combine deep neural-network high-dimensional text-embedding algorithms with graph-theoretical methods for multisc",
        "type": "Document"
      },
      {
        "id": "1dc9ae1e-6f3f-4cf4-bb48-f6a13d030efe",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1909.00183",
          "start_idx": 3949,
          "end_idx": 4515
        },
        "page_content": " levels of resolution; hence the content categories emerge from the textual data, rather than from pre-designed classifications. The obtained results can help mitigate human error or effort in finding the right category in complex classification trees. We illustrate in our analysis the insight gained from this unsupervised, multi-resolution approach in this specialised corpus of medical records. As an additional application, we use machine learning methods for the prediction of the degree of harm of incidents directly from the text in the NRLS incident reports",
        "type": "Document"
      },
      {
        "id": "cf02ca4b-4c82-4a6e-8b27-25ee5273a6c2",
        "metadata": {
          "chunk_id": 150,
          "document_id": "1909.00183",
          "start_idx": 56239,
          "end_idx": 56836
        },
        "page_content": " itself. Additionally, the method allows for the discovery of emerging topics or classes of incidents directly from the data when such events do not fit existing categories by using methods for anomaly detection to decide whether new topic clusters should be created. This is a direction of future work. Further work also includes the use of our method to enable comparisons across healthcare organisations and also to monitor changes in their incident reports over time. Another interesting direction is to provide online classification suggestions to users based on the text they input as an aid",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What cluster identification method is used in this paper?",
    "llm_answer": "A simple clustering algorithm that iteratively proposes random exchanges of memberships.\n",
    "context": [
      {
        "id": "81e65ce8-56e3-4e97-97cf-bbbc278a177a",
        "metadata": {
          "chunk_id": 62,
          "document_id": "1703.04617",
          "start_idx": 16880,
          "end_idx": 17340
        },
        "page_content": " vector, in stead of just choosing the closest cluster. Then the discriminative vector $\\mathbf {\\delta _x}$ with regard to these most relevant clusters are computed. All this information is combined to obtain the discriminative information. In order to keep the full information of input, we also copy the input question $\\mathbf {x}$ , together with the acquired discriminative information, to a feed-forward layer to obtain a new representation $\\mathbf {x^",
        "type": "Document"
      },
      {
        "id": "72e0ff94-0bca-4dc7-9727-6387b8f5beb5",
        "metadata": {
          "chunk_id": 143,
          "document_id": "1909.00183",
          "start_idx": 53192,
          "end_idx": 53768
        },
        "page_content": ". FIGREF22 and FIGREF23). Our a posteriori analysis showed that the method recovers meaningful clusters of content as measured by the similarity of the groups against the hand-coded categories and by the intrinsic topic coherence of the clusters. The clusters have high medical content, thus providing complementary information to the externally imposed classification categories. Indeed, some of the most relevant and persistent communities emerge because of their highly homogeneous medical content, even if they cannot be mapped to standardised external categories. An area",
        "type": "Document"
      },
      {
        "id": "3bedb78c-6d0a-4b06-bbc9-62ae3ff514f3",
        "metadata": {
          "chunk_id": 64,
          "document_id": "1909.00183",
          "start_idx": 23950,
          "end_idx": 24472
        },
        "page_content": " represent graphs on the plane. This layout assigns a harmonic spring to each edge and finds through iterative rearrangements finds an arrangement on the plane that balances attractive and repulsive forces between nodes. Hence similar nodes tend to appear close together on this layout. We colour the nodes by either hand-coded categories (Figure FIGREF7) or multiscale MS communities (Figure FIGREF21). Spatially coherent colourings on this layout imply good clusters in terms of the similarity graph. Tracking membership",
        "type": "Document"
      },
      {
        "id": "5f2b0d28-39e5-4788-ad3c-8ff924aaf332",
        "metadata": {
          "chunk_id": 39,
          "document_id": "2003.06279",
          "start_idx": 15274,
          "end_idx": 15751
        },
        "page_content": " (DT), nearest neighbors (kNN), Naive Bayes (NB) and Support Vector Machines (SVM). We used some heuristics to optimize classifier parameters. Such techniques are described in the literature BIBREF49. The accuracy of the pattern recognition methods were evaluated using cross-validation BIBREF50. In summary, the methodology used in this paper encompasses the following steps: Network construction: here texts are mapped into a co-occurrence networks. Some variations exists in",
        "type": "Document"
      },
      {
        "id": "feef5430-9b43-4dbe-b6a5-71238daf7faa",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1904.08386",
          "start_idx": 4630,
          "end_idx": 5112
        },
        "page_content": "ating all possible cluster assignments is intractable given the large search space ( INLINEFORM0 possible assignments). We devise a simple clustering algorithm to approximate this process. First, we initialize with random cluster assignments and define \u201ccluster strength\u201d to be the relative difference between \u201cintra-group\u201d Euclidean distance and \u201cinter-group\u201d Euclidean distance. Then, we iteratively propose random exchanges of memberships, only accepting these proposals when the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Why did they think this was a good idea?",
    "llm_answer": "They believed the superior visual grounding of their method would produce natural language explanations justifying visual question answers, thus increasing the capabilities of AI.\n",
    "context": [
      {
        "id": "7bff0573-e942-49db-b94d-089865cb4035",
        "metadata": {
          "chunk_id": 42,
          "document_id": "2004.03744",
          "start_idx": 15146,
          "end_idx": 15649
        },
        "page_content": ", and use at least half of the highlighted words to write an explanation for their decision. The last two steps thus follow the quality control of crowd-sourced explanations introduced by Camburu BIBREF6. We also ensured that workers do not simply use a copy of the given hypothesis as explanation. We ensured all the above via in-browser checks before workers' submission. An example of collected explanations is given in Figure FIGREF21. To check the success of our crowdsourcing, we manually assessed",
        "type": "Document"
      },
      {
        "id": "ce89cd01-a5ce-4760-bef4-0c64e2ede190",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1912.01673",
          "start_idx": 10133,
          "end_idx": 10419
        },
        "page_content": ". This step was necessary to remove sentences that are: too unreal, out of this world, such as: Jedno fotonov\u00fd torp\u00e9do a je z tebe vesm\u00edrn\u00e1 topinka. \u201cOne photon torpedo and you're a space toast.\u201d photo captions (i.e. incomplete sentences), e.g.: Zvl\u00e1\u0161tn\u00ed ekv\u00e1dorsk\u00fd p\u0159\u00edpad Correa vs. Cr",
        "type": "Document"
      },
      {
        "id": "77574889-6856-4bab-80be-d11284dcaa61",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1808.00265",
          "start_idx": 25112,
          "end_idx": 25599
        },
        "page_content": " believe that the superior visual grounding provided by the proposed method can play a relevant role to generate natural language explanations to justify the answer to a given visual question. This scenario will help to demonstrate the relevance of our technique as a tool to increase the capabilities of AI based technologies to explain their decisions.  Acknowledgements: This work was partially funded by Oppo, Panasonic and the Millennium Institute for Foundational Research on Data.",
        "type": "Document"
      },
      {
        "id": "ad8f57e6-9103-48d1-bcd3-77aa60670c8e",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1906.06448",
          "start_idx": 11534,
          "end_idx": 12014
        },
        "page_content": " the premise and/or the hypothesis is ungrammatical or does not make sense. Workers were paid US$0.04 for each question, and each question was assigned to three workers. To collect high-quality annotation results, we imposed ten test questions on each worker, and removed workers who gave more than three wrong answers. We also set the minimum time it should take to complete each question to 200 seconds. 1,237 workers contributed to this task, and we annotated gold labels of 15",
        "type": "Document"
      },
      {
        "id": "07286068-8a8a-4163-ab81-4a9ab2ff7439",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1711.02013",
          "start_idx": 2121,
          "end_idx": 2586
        },
        "page_content": "supervised syntactic structure induction has been among the longstanding challenges of computational linguistic BIBREF11 , BIBREF12 , BIBREF13 . Researchers are interested in this problem for a variety of reasons: to be able to parse languages for which no annotated treebanks exist BIBREF14 ; to create a dependency structure to better suit a particular NLP application BIBREF10 ; to empirically argue for or against the poverty of the stimulus BIBREF15 , BIBREF16",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What QA models were used?",
    "llm_answer": "QA-BERT, XLM, XLNet, DistilBERT, and ALBERT.\n",
    "context": [
      {
        "id": "b01c22d7-80d3-43ea-b231-e7ab26106c1e",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1605.08675",
          "start_idx": 8762,
          "end_idx": 9243
        },
        "page_content": " QA system called Hipisek were presented by BIBREF12 . It bases on a fairly common scheme of transforming a question into a search query and finding the most appropriate sentence, satisfying question constrains. Unfortunately, a very small evaluation set (65 question) and an unspecified knowledge base (gathered by a web crawler) make it difficult to compare the results. In their later works BIBREF13 , BIBREF14 , the team concentrated on spatial reasoning using a knowledge base",
        "type": "Document"
      },
      {
        "id": "e806f9b4-6c35-4ef2-8965-b2375ef47ba1",
        "metadata": {
          "chunk_id": 72,
          "document_id": "2001.11268",
          "start_idx": 28640,
          "end_idx": 29140
        },
        "page_content": " systematic review automation. With our provided code and data, the QA-BERT model can be switched with more advanced transformer architectures, including XLM, XLNet, DistilBERT and ALBERT pre-trained models. More detailed investigations into multilingual predictions BIBREF26 pre-processing and predicting more than one PICO per sentence are reserved for future work. Limitations in the automatically annotated PubMed training data mostly consist of incomplete detection or noise P, I, and O entities",
        "type": "Document"
      },
      {
        "id": "b316c7e3-b45d-4b74-b8b3-1f14bbfcc131",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1804.03396",
          "start_idx": 9142,
          "end_idx": 9591
        },
        "page_content": " named QA4IE to overcome the weaknesses of existing IE systems. As we discussed above, the problem of step 1, 2 and 4 can be solved by existing work and we propose to solve the problem of step 3 with QA models. To train a high quality neural network QA model, we build a large IE benchmark in QA style named QA4IE benchmark which consists of 293K Wikipedia articles and 2 million golden relation triples with 636 different relation types. To adapt Q",
        "type": "Document"
      },
      {
        "id": "7c20b408-03a6-4bfb-81b6-30cfb1a7aba7",
        "metadata": {
          "chunk_id": 75,
          "document_id": "1905.08949",
          "start_idx": 27701,
          "end_idx": 28047
        },
        "page_content": "ers (VAEs) and GANs, such models are also used in open-ended VQG to bring \u201ccreativity\u201d into generated questions BIBREF77 , BIBREF78 , showing promising results. This also brings hope to address deep QG from text, as applied in NLG: e.g., SeqGAN BIBREF79 and LeakGAN BIBREF80 . Endowing a QG system with the ability to ask deep questions will help",
        "type": "Document"
      },
      {
        "id": "7279e53a-c1a9-4011-b3f6-b29a5568a810",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1908.10449",
          "start_idx": 12341,
          "end_idx": 12774
        },
        "page_content": " the information gathering procedure continues. In this section, we describe the high-level model structure and training strategies of QA-DQN. We refer readers to BIBREF18 for detailed information. We will release datasets and code in the near future. In this section, we use game step $t$ to denote one round of interaction between an agent with the iMRC environment. We use $o_t$ to denote text observation at game step $t$ and $q$",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do their results compare against other competitors in the PAN 2017 shared task on Author Profiling?",
    "llm_answer": "Their system, N-GrAM, was the best-performing out of the 22 systems submitted.\n",
    "context": [
      {
        "id": "9459e846-976a-438a-a8a8-4388dde6f2cf",
        "metadata": {
          "chunk_id": 40,
          "document_id": "1707.03764",
          "start_idx": 15462,
          "end_idx": 15940
        },
        "page_content": " margin. Our system significantly out-performed the baseline on the joint task, as the baseline scored significantly lower for the gender task than for the variety task. We conclude that, for the current author profiling task, a seemingly simple system using word and character n-grams and an SVM classifier proves very hard to beat. Indeed, N-GrAM turned out to be the best-performing out of the 22 systems submitted in this shared task. Using additional training data, `smart'",
        "type": "Document"
      },
      {
        "id": "bd0c44ff-05f9-4638-9aab-1e7257304fc3",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1707.03764",
          "start_idx": 6499,
          "end_idx": 7022
        },
        "page_content": " PAN 2017 training data using five-fold cross-validation, unless otherwise specified. We extended the training dataset by adding data and gender labels from the PAN 16 Author Profiling shared task BIBREF5 . However, the additional data consistently resulted in lower cross-validation scores than when using only the training data provided with the PAN 17 task. One possible explanation for this is that our unigram model captures aspects that are tied specifically to the PAN 17 dataset, because it contains topics that may",
        "type": "Document"
      },
      {
        "id": "766340f0-646a-47f1-bcad-feaa0bad2312",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1904.08386",
          "start_idx": 6559,
          "end_idx": 7060
        },
        "page_content": " to these results, we now switch to our \u201codd-one-out\u201d task and compare directly to human performance. For each triplet of cities, we identify the intruder as the city with the maximum Euclidean distance from the other two. Interestingly, crowd workers achieve only slightly higher accuracy than ELMo city representations; their interannotator agreement is also low, which indicates that close reading to analyze literary coherence between multiple texts is a difficult task, even for human annotators.",
        "type": "Document"
      },
      {
        "id": "45169d21-3106-4cf2-9383-56f6ce28e4d2",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1911.03243",
          "start_idx": 1181,
          "end_idx": 1742
        },
        "page_content": ", allowing for substantially more reliable performance evaluation of QA-SRL parsers. To address worker quality, we systematically screen workers, provide concise yet effective guidelines, and perform a short training procedure, all within a crowd-sourcing platform. To address coverage, we employ two independent workers plus an additional one for consolidation \u2014 similar to conventional expert-annotation practices. In addition to yielding 25% more roles, our coverage gain is demonstrated by evaluating against expertly annotated data and comparison with Prop",
        "type": "Document"
      },
      {
        "id": "89e03fe0-6dfe-4a5b-82a9-9d9b3ef84c1e",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1709.10217",
          "start_idx": 10224,
          "end_idx": 10506
        },
        "page_content": "qi Zhu and Xiaoming Shi from the research center for social computing and information retrieval for their support on the data annotation, establishing the system testing environment and the communication to the participants and help connect their systems to the testing environment.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "On which task does do model do worst?",
    "llm_answer": "Arithmetic problems.\n",
    "context": [
      {
        "id": "555cfecb-8c10-4956-b6a4-2602ab20419c",
        "metadata": {
          "chunk_id": 82,
          "document_id": "1909.00183",
          "start_idx": 30425,
          "end_idx": 30882
        },
        "page_content": " is carried out with five-fold cross validation, using 80% of the data to train the model and the remaining 20% to test it. As a measure of performance of the classifiers and models, we use the weighted average of the F1 score for all levels of DoH, which takes into account both precision and recall, i.e., both the exactness and completeness of the model. We showcase our methodology through the analysis of the text from NRLS patient incident reports. In",
        "type": "Document"
      },
      {
        "id": "d217561f-fa6f-41a9-805d-e8059ae44c72",
        "metadata": {
          "chunk_id": 73,
          "document_id": "1910.00458",
          "start_idx": 25409,
          "end_idx": 25967
        },
        "page_content": " do math? To investigate this question, we sampled some arithmetic questions that are correctly predicted by our model, made small alterations to the passage or question, and then checked whether our model can still make correct choices. We found our model is very fragile to these minor alterations, implicating that the model is actually not that good at arithmetic problems. We provided one interesting example in the Section 3 of the Supplementary Material. There are increasing interests in machine reading comprehension (MRC) for question answering (QA",
        "type": "Document"
      },
      {
        "id": "0fd8b447-bb11-4afd-8aa0-b8f09ddd6d71",
        "metadata": {
          "chunk_id": 57,
          "document_id": "1909.08859",
          "start_idx": 19980,
          "end_idx": 20459
        },
        "page_content": " a single model is trained to solve all the tasks at once, PRN and BIDAF w/ static memory perform comparably and give much better results than BIDAF. Note that the model performances in the multi-task training setting are worse than single-task performances. We believe that this is due to the nature of the tasks that some are more difficult than the others. We think that the performance could be improved by employing a carefully selected curriculum strategy BIBREF20. In Fig.",
        "type": "Document"
      },
      {
        "id": "1cbae296-557a-4234-8aec-11ec59a6978f",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1911.11744",
          "start_idx": 9878,
          "end_idx": 10434
        },
        "page_content": " on the generated scenario, combinations of these three features are necessary to distinguish the targets from each other, allowing for tasks of varying complexity. To train our model, we generated a dataset of 20,000 demonstrated 7 DOF trajectories (6 robot joints and 1 gripper dimension) in our simulated environment together with a sentence generator capable of creating natural task descriptions for each scenario. In order to create the language generator, we conducted an human-subject study to collect sentence templates of a placement task as well",
        "type": "Document"
      },
      {
        "id": "afb47eaf-a415-40a4-97af-9c5f021e6bf3",
        "metadata": {
          "chunk_id": 72,
          "document_id": "1902.09087",
          "start_idx": 26639,
          "end_idx": 27095
        },
        "page_content": " NLP tasks. Figure FIGREF37 shows a case study comparing models in different input levels. The word level model is relatively coarse in utilizing informations, and finds a sentence with the longest overlap (5 words, 12 characters). However, it does not realize that the question is about numbers of people, and the \u201cDaoHang\u201d(navigate) in question is a verb, but noun in the sentence. The character level model finds a long sentence which covers most of the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "On which task does do model do best?",
    "llm_answer": "PRN and BIDAF w/ static memory.\n",
    "context": [
      {
        "id": "555cfecb-8c10-4956-b6a4-2602ab20419c",
        "metadata": {
          "chunk_id": 82,
          "document_id": "1909.00183",
          "start_idx": 30425,
          "end_idx": 30882
        },
        "page_content": " is carried out with five-fold cross validation, using 80% of the data to train the model and the remaining 20% to test it. As a measure of performance of the classifiers and models, we use the weighted average of the F1 score for all levels of DoH, which takes into account both precision and recall, i.e., both the exactness and completeness of the model. We showcase our methodology through the analysis of the text from NRLS patient incident reports. In",
        "type": "Document"
      },
      {
        "id": "1cbae296-557a-4234-8aec-11ec59a6978f",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1911.11744",
          "start_idx": 9878,
          "end_idx": 10434
        },
        "page_content": " on the generated scenario, combinations of these three features are necessary to distinguish the targets from each other, allowing for tasks of varying complexity. To train our model, we generated a dataset of 20,000 demonstrated 7 DOF trajectories (6 robot joints and 1 gripper dimension) in our simulated environment together with a sentence generator capable of creating natural task descriptions for each scenario. In order to create the language generator, we conducted an human-subject study to collect sentence templates of a placement task as well",
        "type": "Document"
      },
      {
        "id": "0fd8b447-bb11-4afd-8aa0-b8f09ddd6d71",
        "metadata": {
          "chunk_id": 57,
          "document_id": "1909.08859",
          "start_idx": 19980,
          "end_idx": 20459
        },
        "page_content": " a single model is trained to solve all the tasks at once, PRN and BIDAF w/ static memory perform comparably and give much better results than BIDAF. Note that the model performances in the multi-task training setting are worse than single-task performances. We believe that this is due to the nature of the tasks that some are more difficult than the others. We think that the performance could be improved by employing a carefully selected curriculum strategy BIBREF20. In Fig.",
        "type": "Document"
      },
      {
        "id": "74ec3dfb-523a-43d2-abf0-bbb0c88867d6",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1910.09399",
          "start_idx": 12524,
          "end_idx": 12961
        },
        "page_content": " to become increasingly adept at its specific task until a threshold is reached. This is analogous to a min-max game between the two models, according to the following equation: In Eq. (DISPLAY_FORM10), $x$ denotes a multi-dimensional sample, e.g., an image, and $z$ denotes a multi-dimensional latent space vector, e.g., a multidimensional data point following a predefined distribution function such as that of normal distributions. $D",
        "type": "Document"
      },
      {
        "id": "bbd1cacc-f6c9-4958-9fff-426c34ae2d2a",
        "metadata": {
          "chunk_id": 39,
          "document_id": "2004.02393",
          "start_idx": 13193,
          "end_idx": 13770
        },
        "page_content": " ability to distinguish the more informative signals from distant supervision labels. By introducing additional inductive bias of orders, the conditional selection model further improves with a large margin. Finally, our cooperative game gives the best performance, showing that a trained Reasoner has the ability of ignoring entity links that are irrelevant to the reasoning chain. Table TABREF22 demonstrates the effect of selecting directions, together with the methods' recall on head passages and tail passages. The latter is evaluated on a subset of bridge-type questions",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does counterfactual data augmentation aim to tackle bias?",
    "llm_answer": "By swapping gendered words.\n",
    "context": [
      {
        "id": "3ee652c6-3fba-4742-a691-b433779e1140",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1911.03842",
          "start_idx": 7043,
          "end_idx": 7493
        },
        "page_content": " means, or by experts BIBREF21, BIBREF22, BIBREF23), and we run an existing dialogue safety classifier BIBREF24 to measure offensiveness of the dialogues. We explore both data augmentation and algorithmic methods to mitigate bias in generative Transformer dialogue models. We describe first our modeling setting and then the three proposed techniques for mitigating bias. Using (i) counterfactual data augmentation BIBREF25 to swap gendered words and",
        "type": "Document"
      },
      {
        "id": "050f9d56-ef67-44ec-8dec-6b336fa0cebe",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1907.05664",
          "start_idx": 14088,
          "end_idx": 14575
        },
        "page_content": " in our experiment: \u201cWould the important words in the input be deleted, we would have a different summary\". Such counterfactuals are however more difficult to define for image classification for example, where it could be applying a mask over an image, or just filtering a colour or a pattern. We believe that defining a counterfactual and testing it allows us to measure and evaluate the truthfulness of the attributions and thus weight how much we can trust them. In this work, we have",
        "type": "Document"
      },
      {
        "id": "72b89fd4-5571-43fe-9d82-418d2b2aad2e",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1911.03842",
          "start_idx": 7493,
          "end_idx": 7930
        },
        "page_content": " Using (i) counterfactual data augmentation BIBREF25 to swap gendered words and (ii) additional data collection with crowdworkers, we create a gender-balanced dataset. Further, (iii) we describe a controllable generation method which moderates the male and female gendered words it produces. Following BIBREF0, in all of our experiments we fine-tune a large, pre-trained Transformer encoder-decoder neural network on the dialogues in the",
        "type": "Document"
      },
      {
        "id": "4e648957-1d5f-4b42-8059-3a4cdb3ad840",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1812.06705",
          "start_idx": 0,
          "end_idx": 467
        },
        "page_content": "Deep neural network-based models are easy to overfit and result in losing their generalization due to limited size of training data. In order to address the issue, data augmentation methods are often applied to generate more training samples. Recent years have witnessed great success in applying data augmentation in the field of speech area BIBREF0 , BIBREF1 and computer vision BIBREF2 , BIBREF3 , BIBREF4 . Data augmentation in these areas can be easily performed",
        "type": "Document"
      },
      {
        "id": "4c99e3f1-a35b-412f-87fb-743c77bcc2dc",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1910.14497",
          "start_idx": 2950,
          "end_idx": 3356
        },
        "page_content": "ometric bias mitigation uses the cosine distances between words to both measure and remove gender bias BIBREF0. This method implicitly defines bias as a geometric asymmetry between words when projected onto a subspace, such as the gender subspace constructed from a set of gender pairs such as $\\mathcal {P} = \\lbrace (he,she),(man,woman),(king,queen)...\\rbrace $. The projection of a vector $v$ onto $B$ (",
        "type": "Document"
      }
    ]
  },
  {
    "query": "In the targeted data collection approach, what type of data is targetted?",
    "llm_answer": "Biomedical questions.\n",
    "context": [
      {
        "id": "54830273-3c90-417d-8898-e4c92c7fa94a",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1703.04617",
          "start_idx": 14584,
          "end_idx": 15140
        },
        "page_content": "ity. The previous models are often trained for all questions without explicitly discriminating different question types; however, for a target question, both the common features shared by all questions and the specific features for a specific type of question are further considered in this paper, as they could potentially obey different distributions. In this paper we further explicitly model different types of questions in the end-to-end training. We start from a simple way to first analyze the word frequency of all questions, and obtain top-10 most",
        "type": "Document"
      },
      {
        "id": "05d37bee-f941-488b-81c9-7aa089b2d5b4",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1909.00542",
          "start_idx": 2427,
          "end_idx": 2993
        },
        "page_content": " relevant concepts from designated terminologies and ontologies, relevant articles from PubMed, relevant snippets from the relevant articles, and relevant RDF triples from designated ontologies. Phase B provides a biomedical question and a list of relevant articles and snippets, and participant systems are expected to return the exact answers and the ideal answers. The training data is composed of the test data from all previous years, and amounts to 2,747 samples. There has been considerable research on the use of machine learning approaches for tasks related",
        "type": "Document"
      },
      {
        "id": "b70e6e5c-d4a2-4bbf-ac54-07de71fbedc5",
        "metadata": {
          "chunk_id": 21,
          "document_id": "2002.11893",
          "start_idx": 8062,
          "end_idx": 8606
        },
        "page_content": "domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context. To make workers understand the task more easily, we crafted templates to generate natural language descriptions for each structured goal. Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and",
        "type": "Document"
      },
      {
        "id": "2f75c416-8796-4da2-b08f-28e115972114",
        "metadata": {
          "chunk_id": 1,
          "document_id": "1902.09314",
          "start_idx": 437,
          "end_idx": 950
        },
        "page_content": " \u201cservice\u201d and \u201cfood\u201d are negative and positive respectively. A target is usually an entity or an entity aspect. In recent years, neural network models are designed to automatically learn useful low-dimensional representations from targets and contexts and obtain promising results BIBREF0 , BIBREF1 . However, these neural network models are still in infancy to deal with the fine-grained targeted sentiment classification task. Attention mechanism, which has been successfully used in machine translation BIBREF",
        "type": "Document"
      },
      {
        "id": "cf02ca4b-4c82-4a6e-8b27-25ee5273a6c2",
        "metadata": {
          "chunk_id": 150,
          "document_id": "1909.00183",
          "start_idx": 56239,
          "end_idx": 56836
        },
        "page_content": " itself. Additionally, the method allows for the discovery of emerging topics or classes of incidents directly from the data when such events do not fit existing categories by using methods for anomaly detection to decide whether new topic clusters should be created. This is a direction of future work. Further work also includes the use of our method to enable comparisons across healthcare organisations and also to monitor changes in their incident reports over time. Another interesting direction is to provide online classification suggestions to users based on the text they input as an aid",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they determine which words are informative?",
    "llm_answer": "Informative words are nouns and verbs.  Two are matched if their raw text, lemmatizations, WordNet synsets, or aliases are the same.\n",
    "context": [
      {
        "id": "93201066-cfea-4fc5-a0d2-a56c1624273e",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1808.00265",
          "start_idx": 15648,
          "end_idx": 16012
        },
        "page_content": " Informative words are all nouns and verbs, where two informative words are matched if at least one of the following conditions is met: (1) Their raw text as they appear in INLINEFORM5 or INLINEFORM6 are the same; (2) Their lemmatizations (using NLTK BIBREF22 ) are the same; (3) Their synsets in WordNet BIBREF23 are the same; (4) Their aliases (provided from VG)",
        "type": "Document"
      },
      {
        "id": "23d4c060-7ebe-492e-ae46-a038c856e5dd",
        "metadata": {
          "chunk_id": 1,
          "document_id": "2002.06644",
          "start_idx": 523,
          "end_idx": 1035
        },
        "page_content": " the news objectively , thus implying the prevalence of the bias. Therefore, when presenting factual information, it becomes necessary to differentiate subjective language from objective language. There has been considerable work on capturing subjectivity using text-classification models ranging from linguistic-feature-based modelsBIBREF1 to finetuned pre-trained word embeddings like BERTBIBREF2. The detection of bias-inducing words in a Wikipedia statement was explored in BIBREF1. The authors propose the \"",
        "type": "Document"
      },
      {
        "id": "efd1c414-9715-432a-8115-683536f25ab0",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1611.02550",
          "start_idx": 12326,
          "end_idx": 12820
        },
        "page_content": " prior work. Specifically, we use the word discrimination task of Carlin et al. BIBREF20 , which is similar to a query-by-example task where the word segmentations are known. The evaluation consists of determining, for each pair of evaluation segments, whether they are examples of the same or different words, and measuring performance via the average precision (AP). We do this by measuring the cosine similarity between their acoustic word embeddings and declaring them to be the same if the",
        "type": "Document"
      },
      {
        "id": "997a6380-5523-40cf-bb80-4f9c28e7375a",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1908.08419",
          "start_idx": 2923,
          "end_idx": 3393
        },
        "page_content": " score and sequence score, which effectively evaluates the segmentation decisions. Specifically, we combine information branch and gated neural network to determine if the segment is a legal word, i.e., word score. Meanwhile, we use the hidden layer output of the long short-term memory (LSTM) BIBREF11 to find out how the word is linked to its surroundings, i.e., link score. The final decision on the selection of labeling samples is made by calculating the average of",
        "type": "Document"
      },
      {
        "id": "b6eff740-7122-42a8-a557-16da36e47fb8",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1911.12569",
          "start_idx": 12784,
          "end_idx": 13301
        },
        "page_content": " each word. For example, the candidate terms for the word good are: great, nice awesome and superb. DT offers the primary attention mechanism external knowledge in the form of candidate terms. It assists the system to perform better when presented with unseen words during testing as the unseen words could have been a part of the DT expansion list. For example, the system may not come across the word superb during training but it can appear in the test set. Since the system has already seen the word superb in the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What improvement does the MOE model make over the SOTA on language modelling?",
    "llm_answer": "The context states that the larger MoE model (4096 experts) achieved a 24% lower perplexity on the test set, and that a model with similar computational budget to the best published model has a lower test perplexity after 10 epochs.\n",
    "context": [
      {
        "id": "74003cad-8770-476f-99d6-91a427090ee9",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1701.06538",
          "start_idx": 22654,
          "end_idx": 23101
        },
        "page_content": " training examples (about 3 billion sentence pairs). Our training time was shorter due to the lower computational budget of our model. Results for the single-pair GNMT models, the multilingual GNMT model and the multilingual MoE model are given in Table TABREF50 . The MoE model achieves 19% lower perplexity on the dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats the multilingual GNMT model on 11 of the",
        "type": "Document"
      },
      {
        "id": "89b84c5c-081c-42d6-95a3-6b1866ac7d66",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1701.06538",
          "start_idx": 23101,
          "end_idx": 23538
        },
        "page_content": "LEU score, the MoE model significantly beats the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English INLINEFORM0 Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number of real examples were highly oversampled in the training corpus.  This work is the first to",
        "type": "Document"
      },
      {
        "id": "83a1ee9d-203d-4e20-bed8-ee46423051b3",
        "metadata": {
          "chunk_id": 87,
          "document_id": "1701.06538",
          "start_idx": 32078,
          "end_idx": 32501
        },
        "page_content": " with a hidden layer of size 2048. MoE-143M uses a hierarchical MoE layer with 256 experts, each with a hidden layer of size 8192. Both models have 4B parameters in the MoE layers. We searched for the best INLINEFORM0 for each model, and trained each model for 10 epochs. The two models achieved test perplexity of INLINEFORM0 and INLINEFORM1 respectively, showing that even in the presence of a large MoE, more computation",
        "type": "Document"
      },
      {
        "id": "0bb6e430-f972-44fd-b74f-e1f77c679d54",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1701.06538",
          "start_idx": 16844,
          "end_idx": 17346
        },
        "page_content": " similarly to the computationally-matched baseline models, while the largest of the models (4096 experts) achieved an impressive 24% lower perplexity on the test set. In addition to the largest model from the previous section, we trained two more MoE models with similarly high capacity (4 billion parameters), but higher computation budgets. These models had larger LSTMs, and fewer but larger and experts. Details can be found in Appendix UID77 . Results of these three models form the bottom line of",
        "type": "Document"
      },
      {
        "id": "7dac3f56-f4b3-4057-b573-9c4758efa5ba",
        "metadata": {
          "chunk_id": 88,
          "document_id": "1701.06538",
          "start_idx": 32330,
          "end_idx": 32790
        },
        "page_content": " INLINEFORM1 respectively, showing that even in the presence of a large MoE, more computation is still useful. Results are reported at the bottom of Table TABREF76 . The larger of the two models has a similar computational budget to the best published model from the literature, and training times are similar. Comparing after 10 epochs, our model has a lower test perplexity by INLINEFORM2 . The models are similar in structure to the 8-million-operations-per",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the difference in performance between the interpretable system (e.g. vectors and cosine distance) and LSTM with ELMo system?",
    "llm_answer": "The context states that the ELMo-based model under-performs a first-occurrence baseline, suggesting the LSTM model with ELMo embeddings isn't learning complex entity semantics from long-term contexts as effectively as the baseline.  It also notes that the document-level, entity-first model performs better.  No direct comparison of interpretable systems to the LSTM-ELMo system is given.\n",
    "context": [
      {
        "id": "59adfde7-e978-4f32-8b53-6f679fc1795a",
        "metadata": {
          "chunk_id": 41,
          "document_id": "1909.02635",
          "start_idx": 15788,
          "end_idx": 16293
        },
        "page_content": " and precision are substantially increaesd compared to these baseline models. Interestingly, the ELMo-based model under-performs the first-occurrence baseline, indicating that the LSTM model is not learning much in terms of recognizing complex entity semantics grounded in long term contexts. Comparing the four variants of structuring input in proposed architectures as discussed in Section SECREF4, we observe that the document-level, entity-first model is the best performing variant. Given the left-to",
        "type": "Document"
      },
      {
        "id": "f72ee79c-af55-446d-9f3c-484128435ab1",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1909.02635",
          "start_idx": 9367,
          "end_idx": 9776
        },
        "page_content": " also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo$_{token}$ and ELMo$_{sent}$) BIBREF22 compared to GPT. Table TABREF10 compares the performance of the discussed models against the baselines, evaluating per-step entity prediction performance. Using the ground truth about ingredient's state, we also report the uncombined (UR) and combined (CR) recalls, which are per-timest",
        "type": "Document"
      },
      {
        "id": "7d0f87ba-a143-498c-8aaf-bcd05fedb8ca",
        "metadata": {
          "chunk_id": 33,
          "document_id": "1810.09774",
          "start_idx": 12517,
          "end_idx": 13001
        },
        "page_content": " The sentences included in SNLI are also much simpler compared to those in MultiNLI, as they are taken from the Flickr image captions. This might also explain why the difference in accuracy for all of the six models is lowest when the models are trained on MultiNLI and tested on SNLI. It is also very surprising that the model with the biggest difference in accuracy was ESIM + ELMo which includes a pre-trained ELMo language model. BERT performed significantly better than the other",
        "type": "Document"
      },
      {
        "id": "ec807420-83f2-460d-a372-b3de1fbf9150",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1809.09795",
          "start_idx": 5437,
          "end_idx": 5886
        },
        "page_content": ", our architecture is based on Embeddings from Language Model or ELMo BIBREF10 . The ELMo layer allows to recover a rich 1,024-dimensional dense vector for each word. Using CNNs, each vector is built upon the characters that compose the underlying words. As ELMo also contains a deep bi-directional LSTM on top of this character-derived vectors, each word-level embedding contains contextual information from their surroundings. Concretely, we use a",
        "type": "Document"
      },
      {
        "id": "9500ae35-9e38-42f9-b77e-65fae2e735a6",
        "metadata": {
          "chunk_id": 69,
          "document_id": "1707.02377",
          "start_idx": 25491,
          "end_idx": 25897
        },
        "page_content": "ecC is better at handling longer paragraphs, LSTM-based methods are superior for relatively short sentences (of length in the order of 10s). We would like to point out that Doc2VecC is much faster to train and test comparing to skip-thought vectors. It takes less than 2 hours to learn the embeddings on the large book corpus for Doc2VecC on a desktop with Intel i7 2.2Ghz cpu, in comparison to the 2 weeks",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which language-pair had the better performance?",
    "llm_answer": "English-French\n",
    "context": [
      {
        "id": "279617fc-59de-45c8-a31a-e920d05a0421",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1910.07481",
          "start_idx": 12134,
          "end_idx": 12501
        },
        "page_content": " language pair: English-French. En$\\rightarrow $Fr: The Document system obtained the best results considering all metrics on all test sets with strong statistical evidence. It surpassed the Baseline by 1.09 BLEU points and 0.85 TER points on tst2015, 0.75 BLEU points and 0.76 TER points on tst2014, and 0.48 BLEU points and 0.68 TER points on tst2013. Fr$\\rightarrow",
        "type": "Document"
      },
      {
        "id": "0826c6bc-326d-420e-860e-5b0683526906",
        "metadata": {
          "chunk_id": 66,
          "document_id": "2004.01694",
          "start_idx": 25377,
          "end_idx": 25845
        },
        "page_content": " MT$_1$ in the pairwise ranking study, and exhibits fewer translation errors in our error classification. H$_B$ has a substantially higher number of missing words than the other two translations, which agrees with the lower perceived adequacy in the pairwise ranking. However, the analysis not only supports the findings of the pairwise ranking study, but also adds nuance to it. Even though H$_B$ has the highest number of deletions, and does worse than the other two",
        "type": "Document"
      },
      {
        "id": "152f1159-a2e5-496e-8186-a20e1a91af70",
        "metadata": {
          "chunk_id": 73,
          "document_id": "1911.00069",
          "start_idx": 23531,
          "end_idx": 23962
        },
        "page_content": " model transfer performance. Among the 2 neural network models, the Bi-LSTM model achieves a better cross-lingual RE performance than the CNN model for 6 out of the 7 target languages. In terms of absolute performance, the Bi-LSTM model achieves over $40.0$ $F_1$ scores for German, Spanish, Portuguese and Chinese. In terms of relative performance, it reaches over $75\\%$ of the accuracy of the supervised target-language RE model",
        "type": "Document"
      },
      {
        "id": "f3ccffbc-d7d1-4c2d-8775-ff0159879924",
        "metadata": {
          "chunk_id": 67,
          "document_id": "1911.00069",
          "start_idx": 21384,
          "end_idx": 21798
        },
        "page_content": " be 1K. We compare the performance of cross-lingual RE model transfer under the following bilingual word embedding mappings: Regular-1K: the regular mapping learned in (DISPLAY_FORM14) using 1K word pairs; Orthogonal-1K: the orthogonal mapping with length normalization learned in (DISPLAY_FORM16) using 1K word pairs (in this case we train the English RE models with the normalized English word embeddings); Semi-",
        "type": "Document"
      },
      {
        "id": "14d93857-c1d6-41e9-b5fc-a045b281b81d",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1808.09409",
          "start_idx": 14113,
          "end_idx": 14619
        },
        "page_content": " and Russian L2 sentences, the syntax-based system has better performances though it sometimes works worse on the corresponding L1 sentences, indicating the syntax-based systems are more robust when handling learner texts. Furthermore, the neural-parser-based system achieves the best overall performance on the L2 data. Though performing slightly worse than the neural syntax-agnostic one on the L1 data, it has much smaller INLINEFORM0 F, showing that as the syntactic analysis improves, the performances",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which psycholinguistic and basic linguistic features are used?",
    "llm_answer": "Ratio of quotation marks, exclamation marks, modal verbs, stop words, type-token ratio, hedging, named entity types, POS n-grams, sentiment and subjectivity scores, spell-checking, readability features (Coleman-Liau, Flesch), argument lexicon features, word lengths, sentence lengths, word /usage/linguistic-features.  Emoticons and punctuation features are also included.\n",
    "context": [
      {
        "id": "6e0874d5-3cca-402e-83e2-9b4debf12a79",
        "metadata": {
          "chunk_id": 11,
          "document_id": "2001.05493",
          "start_idx": 4208,
          "end_idx": 4680
        },
        "page_content": " also handle different writing styles. Our approach is based on three main ideas: Deep-Text Learning. The goal is to learn long range associations, dependencies between regions of text, N-grams, key-patterns, topical information, and sequential dependencies. Exploiting psycho-linguistic features with basic linguistic features as meta-data. The main aim is to minimize the direct dependencies on in-depth grammatical structure of the language (i.e., to support code-mixed",
        "type": "Document"
      },
      {
        "id": "a6b29bb9-376b-44f0-98a0-79c305b4f0ce",
        "metadata": {
          "chunk_id": 39,
          "document_id": "2004.03034",
          "start_idx": 15301,
          "end_idx": 15680
        },
        "page_content": ", ratio of quotation marks, exclamation marks, modal verbs, stop words, type-token ratio, hedging BIBREF29, named entity types, POS n-grams, sentiment BIBREF30 and subjectivity scores BIBREF31, spell-checking, readibility features such as Coleman-Liau BIBREF32, Flesch BIBREF33, argument lexicon features BIBREF34 and surface features such as word lengths, sentence lengths, word",
        "type": "Document"
      },
      {
        "id": "69ddfb55-ac81-430f-80cc-16e0d2f4d93a",
        "metadata": {
          "chunk_id": 34,
          "document_id": "2001.05493",
          "start_idx": 12425,
          "end_idx": 12975
        },
        "page_content": "/usage/linguistic-features#pos-tagging We have identified a novel combination of features which are highly effective in aggression classification when applied in addition to the features obtained from the deep learning classifier at the classification layer. We have introduced two new features in addition to the previously available features. The first one is the Emotion Sensor Feature which use a statistical model to classify the words into 7 different classes based on the sentences obtained from twitter and blogs which contain total 1,185,540",
        "type": "Document"
      },
      {
        "id": "d68dcd96-8549-4ff2-b5fe-cc83031521b9",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1710.01492",
          "start_idx": 18363,
          "end_idx": 18698
        },
        "page_content": "., ;), :D), hashtags (#Brexit), user tags (e.g., @allenai_org), abbreviations (e.g., RT, BTW, F2F, OMG), elongated words (e.g., soooo, yaayyy), use of capitalization (e.g., proportion of ALL CAPS words), URLs, etc. Finally, the most important features are those based on the presence of words and phrases in sentiment polarity lexicons",
        "type": "Document"
      },
      {
        "id": "1712396c-2ebf-4288-9b09-f039fac51d82",
        "metadata": {
          "chunk_id": 12,
          "document_id": "2001.05493",
          "start_idx": 4503,
          "end_idx": 4930
        },
        "page_content": " in-depth grammatical structure of the language (i.e., to support code-mixed data). We have also included emoticons, and punctuation features with it. We use the term \"NLP Features\" to represent it in the entire paper. Dual embedding based on FastText and Glove. This dual embedding helps in high vocabulary coverage and to capture the rare and partially incorrect words in the text (specially by FastText BIBREF20). Our \"Deep-",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How have the differences in communication styles between Twitter and Facebook increase the complexity of the problem?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "03b47f89-d42b-420a-b81b-bde56754aefe",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1710.01492",
          "start_idx": 2556,
          "end_idx": 3007
        },
        "page_content": " challenging with its use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and #hashtags. In addition to the genre difference, there is also a difference in length: social media messages are generally short, often length-limited by design as in Twitter, i.e., a sentence or a headline rather than a full document. How to handle such challenges has only",
        "type": "Document"
      },
      {
        "id": "3cc1f098-806e-494b-9f68-4c17faeefd08",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1911.12569",
          "start_idx": 0,
          "end_idx": 587
        },
        "page_content": "The emergence of social media sites with limited character constraint has ushered in a new style of communication. Twitter users within 280 characters per tweet share meaningful and informative messages. These short messages have a powerful impact on how we perceive and interact with other human beings. Their compact nature allows them to be transmitted efficiently and assimilated easily. These short messages can shape people's thought and opinion. This makes them an interesting and important area of study. Tweets are not only important for an individual but also for the companies",
        "type": "Document"
      },
      {
        "id": "7a16c773-5d8e-41eb-8c85-aadb8ec2713a",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1809.09795",
          "start_idx": 11967,
          "end_idx": 12483
        },
        "page_content": "attention. This shows the effectiveness of using pre-trained character-based word representations, that allow us to recover many of the morpho-syntactic cues that tend to denote irony and sarcasm. Finally, our experiments showed that enlarging existing Twitter datasets by adding external soft-labeled data from the same media source does not yield improvements in the overall performance. This complies with the observations made by BIBREF18 . Since we have designed our augmentation tactics to maximize the overlap",
        "type": "Document"
      },
      {
        "id": "07410a12-b09e-4b8b-b6e9-974f97481d57",
        "metadata": {
          "chunk_id": 1,
          "document_id": "2001.06888",
          "start_idx": 525,
          "end_idx": 1067
        },
        "page_content": " in Twitter micro-blogging social media is oriented towards their daily life, first witness news-reporting and engaging in various events (sports, political stands etc.). According to studies, news in twitter is propagated and reported faster than conventional news media BIBREF1. Thus, extracting first hand news and entities occurring in this fast and versatile online media gives valuable information. However, abridged and noisy content of Tweets makes it even more difficult and challenging for tasks such as named entity recognition and",
        "type": "Document"
      },
      {
        "id": "d9d5940d-bfbd-4da1-826f-d3dce0804439",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1809.10644",
          "start_idx": 2893,
          "end_idx": 3457
        },
        "page_content": " for metadata of the Twitter user, that were trained jointly in interleaved fashion. Both networks used in combination - and especially when trained using transfer learning - achieved higher F1 scores than either neural network classifier alone. In contrast to the methods described above, our approach relies on a simple word embedding (SWEM)-based architecture BIBREF5 , reducing the number of required parameters and length of training required, while still yielding improved performance and resilience across related classification tasks. Moreover, our network",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What data/studies do the authors provide to support the assertion that the majority of aggressive conversations contain code-mixed languages?",
    "llm_answer": "The authors cite the TRAC-2018 dataset as providing code-mixed data, but don't state that a majority of aggressive conversations are code-mixed.\n",
    "context": [
      {
        "id": "017b9458-0be8-48d1-bd2f-4889519b70a0",
        "metadata": {
          "chunk_id": 5,
          "document_id": "2001.05493",
          "start_idx": 2052,
          "end_idx": 2570
        },
        "page_content": " different languages in a single utterance or sentence, whereas code-switching refers to the co-occurrence of speech extracts belonging to two different grammatical systemsBIBREF3. This language interchange makes the grammar more complex and thus it becomes tough to handle it by traditional algorithms. Thus the presence of high percentage of code-mixed content in social media text has increased the complexity of the aggression detection task. For example, the dataset provided by the organizers of TRAC-2018 BIBREF",
        "type": "Document"
      },
      {
        "id": "4ce958c7-5e5a-431e-b37d-02966afd7af6",
        "metadata": {
          "chunk_id": 23,
          "document_id": "2001.05493",
          "start_idx": 8516,
          "end_idx": 9051
        },
        "page_content": " training dataset provided by TRAC. This also proves the effectiveness of our approach. Our system outperforms all the previous state of the art approaches used for aggression identification on English code-mixed TRAC data, while being trained only from Facebook comments the system outperforms other approaches on the additional Twitter test set. The remaining part of this paper is organized as follows: Section SECREF2 is an overview of related work. Section SECREF3 presents the methodology and algorithmic details. Section SECREF4",
        "type": "Document"
      },
      {
        "id": "5895a776-5ffe-4a12-addf-54061bef69db",
        "metadata": {
          "chunk_id": 80,
          "document_id": "2001.05493",
          "start_idx": 26969,
          "end_idx": 27231
        },
        "page_content": " future, we are planning to pay attention on a generalized language model for code-mixed texts which can also handle Hindi-code-mixed and other multi-lingual code-mixed datasets (i.e., trying to reduce the dependencies on language-specific code-mixed resources).",
        "type": "Document"
      },
      {
        "id": "9478230e-3422-4766-b535-cc55374230bb",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1912.13109",
          "start_idx": 2336,
          "end_idx": 2800
        },
        "page_content": " large variation. Spelling variation: There is no agreement on the spellings of the words which are mixed with English. For example to express love, a code mixed spelling, specially when used social platforms might be pyaar, pyar or pyr. Dataset: Based on some earlier work, only available labelled dataset had 3189 rows of text messages of average length of 116 words and with a range of 1, 1295. Prior work addresses this concern by using Transfer Learning on an",
        "type": "Document"
      },
      {
        "id": "4d7ae159-ddaa-4ec2-bc16-fbd4e3f53980",
        "metadata": {
          "chunk_id": 10,
          "document_id": "2001.05493",
          "start_idx": 3764,
          "end_idx": 4208
        },
        "page_content": " systems either shows high performance on (a) Twitter dataset or (b) Facebook dataset (given in the TRAC-2018), but not on both English code-mixed datasets. This may be due to the text style or level of complexities of both datasets. So, we concentrated to develop a robust system for English code-mixed texts, and uni-lingual texts, which can also handle different writing styles. Our approach is based on three main ideas: Deep-Text Learning.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What datasets are used to evaluate the model?",
    "llm_answer": "Train, development, and test sets.  Swissmetro dataset.  Other corpora (Table TABREF12).\n",
    "context": [
      {
        "id": "cb26796d-fa6a-45cf-840c-0901678f1e04",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1909.00154",
          "start_idx": 24630,
          "end_idx": 25057
        },
        "page_content": " vectors) Development set: 20% of the dataset (2003 vectors) Test set: 20% of the dataset (2003 vectors) The PyLogit package BIBREF11 also uses Swissmetro as an example. Therefore, our model specifications will extend the default one from this package. We re-estimated this model with the train set and validated with testset. The results are shown in tables TABREF31 and TABREF32. Since we are comparing the models at the test",
        "type": "Document"
      },
      {
        "id": "11bed98c-eee8-466c-b192-b4d7bee2693b",
        "metadata": {
          "chunk_id": 56,
          "document_id": "1909.00154",
          "start_idx": 21118,
          "end_idx": 21619
        },
        "page_content": " different experiments in the paper (and report the respective mean and standard deviation statistics). Whenever we want to analyse a particular model (e.g. to check the coefficients of a choice model), we select the one with the highest likelihood at the development set (i.e. in practice, its out-of-sample generalization performance), and report its performance on the test set. Since a choice model will typically involve other variables than the categorical ones that we learn the embeddings for,",
        "type": "Document"
      },
      {
        "id": "65333987-2358-483b-95cd-3ff028ff6a96",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      },
      {
        "id": "9638118b-fae1-4140-a7d1-11bd33d21e74",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1909.00154",
          "start_idx": 23166,
          "end_idx": 23592
        },
        "page_content": "CCM), for example. We will apply the methodology to the well-known \u201cSwissmetro\" dataset. We will compare it with a dummy variables and PCA baselines. We will follow the 3-way experimental design mentioned before: split the dataset into train, development and test sets, so that the embeddings, PCA eigenvectors and the choice model are estimated from the same train and development sets, and validate it out-of-sample. For the",
        "type": "Document"
      },
      {
        "id": "8cb37777-611d-4a23-a724-65efee859d7d",
        "metadata": {
          "chunk_id": 124,
          "document_id": "1909.00183",
          "start_idx": 45834,
          "end_idx": 46293
        },
        "page_content": " We first train models using three categories from the reports: location (L), external hand-coded category (C), and medical specialty (S). We also compute the performance of models trained on text features, both TF-iDF and Doc2Vec. We also study models trained on a mixture of text and categories. Finally, we run Markov Stability as described above to obtain cluster labels for each dataset (O and PRC) at different resolutions (70, 45, 30 and 13 communities",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What baseline models do they compare against?",
    "llm_answer": "TF-IDF, S2S, IR,  NABERT+, Stanford, Multigraph, MIR\n",
    "context": [
      {
        "id": "7d4754c7-4ea8-4d20-9fb6-3d1e50bf0cf6",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1809.04960",
          "start_idx": 22497,
          "end_idx": 22960
        },
        "page_content": " the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types of comments as described in the above retrieval evaluation setting: Correct, Plausible, Popular, and Random. We rank the candidate comment set with four models (TF-IDF, S2S, IR, and Proposed+IR), and record the types of top-1 comments. Figure FIGREF40 shows the percentage of different types of top-1 comments generated by each model. It shows",
        "type": "Document"
      },
      {
        "id": "5e0f3225-070e-4250-8f70-93c083311ec4",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "d7a0b7aa-da14-4112-80c0-cc26c62fe83f",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "38d0d5d3-4be2-4a08-bc2e-e9f401074e5d",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1701.06538",
          "start_idx": 17346,
          "end_idx": 17826
        },
        "page_content": ". Details can be found in Appendix UID77 . Results of these three models form the bottom line of Figure FIGREF32 -right. Table TABREF33 compares the results of these models to the best previously-published result on this dataset . Even the fastest of these models beats the best published result (when controlling for the number of training epochs), despite requiring only 6% of the computation. We trained our models using TensorFlow BIBREF30 on clusters containing 16-32 Tesla K",
        "type": "Document"
      },
      {
        "id": "c686ef15-dda8-4ff8-a15a-946f0174a62e",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1603.04553",
          "start_idx": 10414,
          "end_idx": 10861
        },
        "page_content": " baseline with two deterministic systems, namely Stanford: the Stanford system BIBREF10 and Multigraph: the unsupervised multigraph system BIBREF26 , and one unsupervised system, namely MIR: the unsupervised system using most informative relations BIBREF27 . Our model outperforms the three baseline systems on all the evaluation metrics. Specifically, our model achieves improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford system,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the differences with previous applications of neural networks for this task?",
    "llm_answer": "The context mentions further exploration of feature extraction techniques (Constant-Q transform, FFT), improvements in neural network architectures (concatenating 1D and 2D ConvNets, deeper networks with skip connections and Inception modules), and experiments using various CNN variants (DCNN, grConv, AdaSent).  It also highlights the use of LSTMs and GRNNs for capturing contextual information, which are different from prior methods.\n",
    "context": [
      {
        "id": "577f6feb-1a55-497e-a9ae-b43551ab6a28",
        "metadata": {
          "chunk_id": 54,
          "document_id": "1910.04269",
          "start_idx": 20617,
          "end_idx": 21192
        },
        "page_content": " robust to variations which might be present in real world scenarios. There can be further exploration of various feature extraction techniques like Constant-Q transform and Fast Fourier Transform and assessment of their impact on Language Identification. There can be further improvements in neural network architectures like concatenating the high level features obtained from 1D-ConvNet and 2D-ConvNet, before performing classification. There can be experiments using deeper networks with skip connections and Inception modules. These are known to have positively impacted",
        "type": "Document"
      },
      {
        "id": "de50c9d5-2c01-47ea-83d4-dfc57698b782",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1908.08419",
          "start_idx": 4671,
          "end_idx": 5143
        },
        "page_content": " Deep Learning Methods. Recently, neural networks have been applied in CWS tasks. To name a few, Zheng et al. BIBREF14 used deep layers of neural networks to learn feature representations of characters. Chen et al. BIBREF6 adopted LSTM to capture the previous important information. Chen et al. BIBREF18 proposed a gated recursive neural network (GRNN), which contains reset and update gates to incorporate the complicated combinations of characters. Jiang and Tang BIBREF",
        "type": "Document"
      },
      {
        "id": "66e27b9c-5467-44d8-8a0e-fccef3b9e20e",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1812.07023",
          "start_idx": 0,
          "end_idx": 519
        },
        "page_content": "Deep neural networks have been successfully applied to several computer vision tasks such as image classification BIBREF0 , object detection BIBREF1 , video action classification BIBREF2 , etc. They have also been successfully applied to natural language processing tasks such as machine translation BIBREF3 , machine reading comprehension BIBREF4 , etc. There has also been an explosion of interest in tasks which combine multiple modalities such as audio, vision, and language together. Some popular multi-modal tasks",
        "type": "Document"
      },
      {
        "id": "2028670a-dfdd-426a-9b0f-dd1ec04e851c",
        "metadata": {
          "chunk_id": 72,
          "document_id": "1910.09399",
          "start_idx": 28127,
          "end_idx": 28588
        },
        "page_content": " machine learning algorithms, the recurrent neural networks (RNN) and generative adversarial networks (GANs), with the sole purpose of speeding the generation of text-to-image synthesis. black Deep learning shed some light to some of the most sophisticated advances in natural language representation, image synthesis BIBREF7, BIBREF8, BIBREF43, BIBREF35, and classification of generic data BIBREF44. However, a bulk of the latest breakthroughs in deep learning",
        "type": "Document"
      },
      {
        "id": "3317551c-5a55-4514-ab30-f166026be402",
        "metadata": {
          "chunk_id": 21,
          "document_id": "1701.00185",
          "start_idx": 8101,
          "end_idx": 8526
        },
        "page_content": ", CNN is better to learn non-biased implicit features which has been successfully exploited for many supervised NLP learning tasks as described in Section SECREF1 , and various CNN based variants are proposed in the recent works, such as Dynamic Convolutional Neural Network (DCNN) BIBREF10 , Gated Recursive Convolutional Neural Network (grConv) BIBREF31 and Self-Adaptive Hierarchical Sentence model (AdaSent) BIBREF32 . In",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much improvement is gained from the proposed approaches?",
    "llm_answer": "The provided text shows improvements in comparison to baseline methods and prior work, but does not quantify the *amount* of improvement gained.  It states that the proposed method outperforms some baselines, but lacks precise numerical values for the improvement.\n",
    "context": [
      {
        "id": "5f5a488a-b689-46d4-b32b-50ea0883fe6f",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1703.09684",
          "start_idx": 2106,
          "end_idx": 2596
        },
        "page_content": " evaluate both baseline and state-of-the-art VQA algorithms. We found that our proposed approach enables more nuanced comparisons of VQA algorithms, and helps us understand the benefits of specific techniques better. In addition, it also allowed us to answer several key questions about VQA algorithms, such as, `Is the generalization capacity of the algorithms hindered by the bias in the dataset?', `Does the use of spatial attention help answer specific question-types?', `How successful",
        "type": "Document"
      },
      {
        "id": "2d9e100c-8cb0-48b7-81d1-63d646c42530",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1909.00542",
          "start_idx": 8896,
          "end_idx": 9426
        },
        "page_content": " the results are compatible with the top part of the table and prior work suggesting that classification-based approaches improve over regression-based approaches. We also experiment with the use of reinforcement learning techniques. Again these experiments are based on BIBREF2, who uses REINFORCE to train a global policy. The policy predictor uses a simple feedforward network with a hidden layer. The results reported by BIBREF2 used ROUGE Recall and indicated no improvement with respect to deep learning architectures. Human",
        "type": "Document"
      },
      {
        "id": "b95b5f06-43bf-4e7b-beb5-99dfc997f83a",
        "metadata": {
          "chunk_id": 58,
          "document_id": "2002.00652",
          "start_idx": 18202,
          "end_idx": 18702
        },
        "page_content": " modeling methods, they do not significantly improve the performance as we expected. As mentioned in Section SECREF1, intuitively, methods which only use the precedent SQL enjoys better generalizability. To validate it, we further conduct an out-of-distribution experiment to assess the generalizability of different context modeling methods. Concretely, we select three representative methods and train them on questions at turn 1 and 2, whereas test them at turn 3, 4 and beyond. As shown in Figure",
        "type": "Document"
      },
      {
        "id": "5c6209f3-cef8-4a17-88a8-a660a1a73e1e",
        "metadata": {
          "chunk_id": 76,
          "document_id": "1701.00185",
          "start_idx": 27376,
          "end_idx": 27736
        },
        "page_content": " and NMI performance of our proposed approaches and four baseline methods, K-means, SkipVec, RecNN and Para2vec based clustering methods. Intuitively, we get a general observation that (1) BoW based approaches, including K-means (TF) and K-means (TF-IDF), and SkipVec based approaches perform not well; (2) RecNN based approaches, both RecNN (Ave.) and RecNN (",
        "type": "Document"
      },
      {
        "id": "309bcaa2-ef9d-45b3-9b52-81e934a26747",
        "metadata": {
          "chunk_id": 66,
          "document_id": "2003.07433",
          "start_idx": 25731,
          "end_idx": 26161
        },
        "page_content": "REF28 illustrates the comparisons between LAXARY and Coppersmith et. al. proposed method. Here we can see, the outperformance of our proposed method as well as the importance of $s-score$ estimation. We also illustrates the importance of $\\alpha -score$ and $S-score$ in Fig FIGREF30. Fig FIGREF30 illustrates that if we change the number of training samples (%), LAXARY models outperforms Coppersmith et. al. proposed model under",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is infinite-length sequence generation a result of training with maximum likelihood?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "6cbbf164-e7b3-49d6-884f-a93965ba12a6",
        "metadata": {
          "chunk_id": 98,
          "document_id": "2002.02492",
          "start_idx": 30976,
          "end_idx": 31467
        },
        "page_content": " Maximum likelihood learning fits the model $p_{\\theta }$ using the data distribution, whereas a decoded sequence from the trained model follows the distribution $q_{\\mathcal {F}}$ induced by a decoding algorithm. Based on this discrepancy, we make a strong conjecture: we cannot be guaranteed to obtain a good consistent sequence generator using maximum likelihood learning and greedy decoding. Sequence-level learning, however, uses a decoding algorithm during training BIBREF25, BIBREF26.",
        "type": "Document"
      },
      {
        "id": "41a07305-948a-4267-8649-3804aa7e8e37",
        "metadata": {
          "chunk_id": 0,
          "document_id": "2002.02492",
          "start_idx": 0,
          "end_idx": 477
        },
        "page_content": "Neural sequence models trained with maximum likelihood estimation (MLE) have become a standard approach to modeling sequences in a variety of natural language applications such as machine translation BIBREF0, dialogue modeling BIBREF1, and language modeling BIBREF2. Despite this success, MLE-trained neural sequence models have been shown to exhibit issues such as length bias BIBREF3, BIBREF4 and degenerate repetition BIBREF5. These issues are suspected to be related to the",
        "type": "Document"
      },
      {
        "id": "1dede3e1-c7e4-4653-8d8b-d3aed7923acc",
        "metadata": {
          "chunk_id": 77,
          "document_id": "2002.02492",
          "start_idx": 24026,
          "end_idx": 24555
        },
        "page_content": " is inconsistent. We use $L=1500$, which is more than 10 times the maximum training sequence length. In each experiment, we report the mean and standard deviation of metrics across 10 independent initializations. Unless specified otherwise, we report metrics using the test context distribution, since the train, valid, and randomly generated context distributions had similar results. We train recurrent language models for sequence completion with maximum likelihood, using the following loss on each sequence $Y=(c_1,\\ldots ,c",
        "type": "Document"
      },
      {
        "id": "5a148791-c5db-4f24-9e70-cd62701157ca",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1905.13413",
          "start_idx": 7437,
          "end_idx": 7950
        },
        "page_content": " also at assertion generation, suggesting that extractions of higher quality can be added as training samples to continue this training process iteratively. The resulting iterative learning procedure (alg:iter) incrementally includes extractions generated by the current model as training samples to optimize the binary classification loss to obtain a better model, and this procedure is continued until convergence. [t] training data $\\mathcal {D}$ , initial model $\\theta ^{(0)}$ model after convergence $\\theta",
        "type": "Document"
      },
      {
        "id": "98ffe82a-ee5c-4160-87ab-37728715cdde",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1909.02480",
          "start_idx": 4352,
          "end_idx": 4751
        },
        "page_content": " sequence in non-autoregressive models. However, in order to model all of the complexities of sequence generation to the point that we can read off all of the words in the output in an independent fashion (as in Eq. (DISPLAY_FORM6)), the prior distribution $p_{\\theta }(\\mathbf {z}|\\mathbf {x})$ will necessarily be quite complex. In this section, we describe generative flows BIBREF10, an effective",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is dataset for this challenge?",
    "llm_answer": "The answer is not found in the context.  While various datasets are mentioned, there's no single overall dataset size given for the *challenge*.\n",
    "context": [
      {
        "id": "ec846c61-b035-431a-88d4-858e8c47d3be",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1603.07044",
          "start_idx": 9966,
          "end_idx": 10403
        },
        "page_content": " for task A, there are a total number of 26,700 question-comment pairs. For task B, there are 2,670 question-question pairs. For task C, there are 26,700 question-comment pairs. The test dataset includes 50 questions, 500 related questions and 5,000 comments which do not overlap with the training set. To evaluate the performance, we use mean average precision (MAP) and F1 score. Table 2 shows the initial results using the RNN encoder",
        "type": "Document"
      },
      {
        "id": "2d3bdfec-4ed5-456d-bd82-023fad798858",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "af27ee8c-701c-4fe4-b55b-63a0e04b788a",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1701.00185",
          "start_idx": 18784,
          "end_idx": 19212
        },
        "page_content": " BIBREF41 . StackOverflow. We use the challenge data published in Kaggle.com. The raw dataset consists 3,370,528 samples through July 31st, 2012 to August 14, 2012. In our experiments, we randomly select 20,000 question titles from 20 different tags as in Table TABREF25 . Biomedical. We use the challenge data published in BioASQ's official website. In our experiments, we randomly select 20, 000 paper titles from 20 different",
        "type": "Document"
      },
      {
        "id": "62bfaadf-d388-4a07-b13b-c24fa0bf8a92",
        "metadata": {
          "chunk_id": 116,
          "document_id": "1605.08675",
          "start_idx": 42686,
          "end_idx": 43142
        },
        "page_content": " was involved in previous experiments BIBREF4 , BIBREF36 . Evaluation set bases on an open dataset for Polish QA systems, published by BIBREF56 . It has been gathered from Did you know... column, appearing in the main page of the Polish Wikipedia. It contains 4721 questions, from which 1000 have been analysed, which resulted in 576 satisfying the task constrains, given in chapter SECREF2 . Table TABREF85 shows a distribution of different question types",
        "type": "Document"
      },
      {
        "id": "f8ec0442-a15a-41b1-b2a7-4614f86ba6e5",
        "metadata": {
          "chunk_id": 53,
          "document_id": "1908.08419",
          "start_idx": 19170,
          "end_idx": 19642
        },
        "page_content": " from 3868 records as unlabeled set. Then, we manually annotate remaining 668 records as labeled set, which contains 1170 sentences. Finally, we divide labeled set into train set and test set with the ratio of 7:3 randomly. Statistics of datasets are listed in Table TABREF33 . To determine suitable parameters, we divide training set into two sets, the first 80% sentences as training set and the rest 20% sentences as validation set. Character embedding dimensions and K",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How better is performance of proposed model compared to baselines?",
    "llm_answer": "The proposed model outperforms baselines by a clear margin in new environments, and by a substantial amount in environments with sufficient training data.  It also outperforms baselines on previously seen environments, with particularly large improvements in Exact Match and Goal Match metrics.\n",
    "context": [
      {
        "id": "fc23cf7f-21ff-43cd-8d00-ef5914322400",
        "metadata": {
          "chunk_id": 68,
          "document_id": "1810.00663",
          "start_idx": 26280,
          "end_idx": 26763
        },
        "page_content": " room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM. Note that the average number of actions in the ground truth output sequences is 7.07 for the Test-New set. Our model's predictions are just INLINEFORM1 edits off on average from the correct navigation plans. This",
        "type": "Document"
      },
      {
        "id": "abc51b19-3db2-4cbf-ac46-ff97aa2fa1d1",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1603.07044",
          "start_idx": 14997,
          "end_idx": 15515
        },
        "page_content": " task A already had a substantial amount of training data. Table 4 gives the final comparison between different models (we only list the MAP score because it is the official score for the challenge). Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited",
        "type": "Document"
      },
      {
        "id": "d7a0b7aa-da14-4112-80c0-cc26c62fe83f",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "5e0f3225-070e-4250-8f70-93c083311ec4",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "5e5c6963-88aa-4a86-96ff-323aeb271bfb",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1810.00663",
          "start_idx": 25415,
          "end_idx": 25983
        },
        "page_content": " In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models. The previous section evaluated model performance on new instructions (and corresponding navigation plans) for environments that were previously seen at training time. Here, we examine whether the trained models succeed on environments that are completely new. The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What DCGs are used?",
    "llm_answer": "DGC-max, DGC-ave, and DGC-gated.\n",
    "context": [
      {
        "id": "2677d7aa-1729-49e0-bc20-7f5b07f749f2",
        "metadata": {
          "chunk_id": 30,
          "document_id": "1902.09087",
          "start_idx": 11366,
          "end_idx": 11812
        },
        "page_content": "12 , which use poolings on neighboring vertexes that ignore the semantic structure of n-grams. But to some situations, their formulations can be very similar to ours (See Appendix for derivation). For example, if we set the kernel size in LCNs to 3, use linear activations and suppose the pooling mode is average in both LCNs and DGCs, at each word in each layer, the DGCs compute the average of the first order neighbors together with the center",
        "type": "Document"
      },
      {
        "id": "f527b437-82e0-4b1d-87b9-6089a35c9f29",
        "metadata": {
          "chunk_id": 45,
          "document_id": "2001.06354",
          "start_idx": 14994,
          "end_idx": 15452
        },
        "page_content": ". For evaluation, the Visual Dialog task employs four metrics. NDCG is the primary metric of the Visual Dialog Challenge which considers multiple similar answers as correct ones. The other three are MRR, recall@k, and mean rank where they only consider the rank of a single answer. Our experiments show the scores of NDCG and non-NDCG metrics from our image-only and joint models have a trade-off relationship due to their different ability (as shown in Sec.",
        "type": "Document"
      },
      {
        "id": "4c27a9e7-3ba5-4e4a-be81-0e9d7b81f175",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1905.00563",
          "start_idx": 10486,
          "end_idx": 10702
        },
        "page_content": "{argmin} (G)$4 Ho $\\operatornamewithlimits{argmin} (G)$5 dd $\\operatornamewithlimits{argmin} (G)$6 d $\\operatornamewithlimits{argmin} (G)$7 s,r,s',r'd $\\operatornamewithlimits{argmin} (G)$8 s, r, o $\\operatornamewith",
        "type": "Document"
      },
      {
        "id": "ab5f70a9-64cc-4d54-a61b-cc7d6c3eb892",
        "metadata": {
          "chunk_id": 75,
          "document_id": "1910.09399",
          "start_idx": 29415,
          "end_idx": 29975
        },
        "page_content": " and training strategy that allows natural text to image synthesis. These contributions are primarily tested on the Caltech-UCSD Birds and Oxford-102 Flowers datasets. Each image in these datasets carry five text descriptions. These text descriptions were created by the research team when setting up the evaluation environment. The DC-GANs model is subsequently trained on several subcategories. Subcategories in this research represent the training and testing sub datasets. The performance shown by these experiments display a promising yet effective way to",
        "type": "Document"
      },
      {
        "id": "b688c17d-b5a7-4ded-9573-7514b790fc04",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1902.09087",
          "start_idx": 16940,
          "end_idx": 17407
        },
        "page_content": "REF18 are strong baselines that perform CNNs over directed graphs to produce high level representation for each vertex in the graph, which can be used to build a sentence representation via certain pooling operation. We therefore choose to compare with DGC-max (with maximum pooling), DGC-ave (with average pooling), and DGC-gated (with gated pooling), where the gate value is computed using the concatenation of the vertex vector and the center vertex vector through",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance difference of using a generated summary vs. a user-written one?",
    "llm_answer": "The models consistently perform better with a user-written gold summary compared to a system-generated summary.\n",
    "context": [
      {
        "id": "272aa74e-5499-45b7-abd7-e052e0d9f06f",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1911.02711",
          "start_idx": 17553,
          "end_idx": 18109
        },
        "page_content": " use summary information shows that the review summary is useful for sentiment classification. In addition, the same models work consistently better when the user written gold summary is used compared to a system generated summary, which is intuitively reasonable since the current state-of-the-art abstractive summarization models are far from perfect. Interestingly, as shown in the second section of the table, the gold summary itself does not lead to better sentiment accuracy compared with the review itself, which shows that summaries better serve as",
        "type": "Document"
      },
      {
        "id": "e5593341-cca7-472a-847e-8c6a3b02d388",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1910.00825",
          "start_idx": 0,
          "end_idx": 560
        },
        "page_content": "Summarization aims to condense a piece of text to a shorter version, retaining the critical information. On dialogs, summarization has various promising applications in the real world. For instance, the automatic doctor-patient interaction summary can save doctors' massive amount of time used for filling medical records. There is also a general demand for summarizing meetings in order to track project progress in the industry. Generally, multi-party conversations with interactive communication are more difficult to summarize than single-speaker documents",
        "type": "Document"
      },
      {
        "id": "056fcbd8-555a-4c40-9502-a9a7e36fb07e",
        "metadata": {
          "chunk_id": 33,
          "document_id": "1909.00578",
          "start_idx": 11017,
          "end_idx": 11604
        },
        "page_content": " stochastic learning algorithms are involved. An in-depth exploration of this point would involve further investigation, which will be part of future work. We propose a novel Quality Estimation model for summarization which does not require human references to estimate the quality of automatically produced summaries. Sum-QE successfully predicts qualitative aspects of summaries that recall-oriented evaluation metrics fail to approximate. Leveraging powerful BERT representations, it achieves high correlations with human scores for most linguistic qualities rated, on three different",
        "type": "Document"
      },
      {
        "id": "9036f9f4-99f4-4b41-a3e5-fdc241fa7e8f",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1909.00578",
          "start_idx": 12681,
          "end_idx": 13094
        },
        "page_content": ", not only summaries. It could thus be applied to other text generation tasks, such as natural language generation and sentence compression. We would like to thank the anonymous reviewers for their helpful feedback on this work. The work has been partly supported by the Research Center of the Athens University of Economics and Business, and by the French National Research Agency under project ANR-16-CE33-0013.",
        "type": "Document"
      },
      {
        "id": "54d45754-408d-45db-b82e-719410f7fdf3",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1909.00542",
          "start_idx": 4863,
          "end_idx": 5365
        },
        "page_content": " the target summaries are human-generated and they do not have a perfect mapping to the input sentences. In addition, some samples have multiple reference summaries. BIBREF11 showed that different data labelling approaches influence the quality of the final summary, and some labelling approaches may lead to better results than using regression. In this paper we experiment with the following labelling approaches: : Label as \u201csummary\u201d all sentences from the input text that have a ROUGE score above a",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What evaluation metrics did they look at?",
    "llm_answer": "Automatic evaluation metrics, human perceived quality scores (relevance, conciseness, readability), task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, task ease, user expertise, F1-score, Recall rate, F1 score, Overall accuracy.\n",
    "context": [
      {
        "id": "67a7695f-67b2-4acb-98f7-3e39a52c72ef",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1910.00825",
          "start_idx": 16888,
          "end_idx": 17413
        },
        "page_content": " our method's increased performance on automatic evaluation metrics entails better human perceived quality. We randomly select 100 test samples from MultiWOZ test set for evaluation. We recruit 150 crowd workers from Amazon Mechanical Turk. For each sample, we show the conversation, reference summary, as well as summaries generated by Pointer-Generator and SPNet to three different participants. The participants are asked to score each summary on three indicators: relevance, conciseness and readability on a 1 to 5 scale,",
        "type": "Document"
      },
      {
        "id": "6b1e72f3-8ee5-43ac-a062-f08c158e3d67",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1709.10217",
          "start_idx": 6394,
          "end_idx": 6850
        },
        "page_content": " evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the number of total tasks. User satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied, neutral, satisfied and very satisfied, respectively. Response fluency: There are three scores -1, 0, 1, which indicate nonfluency, neutral, fluency. Number of dialogue turns: The number of utterances in a task",
        "type": "Document"
      },
      {
        "id": "3ee506eb-b079-45ca-b493-363ac132a98d",
        "metadata": {
          "chunk_id": 55,
          "document_id": "2003.05995",
          "start_idx": 22449,
          "end_idx": 22776
        },
        "page_content": " that I needed?\u201d on a scale of 1 (no, not at all) to 7 (yes, completely). Task ease: \u201cHow easy was the task?\u201d on a scale of 1 (very easy) to 7 (very difficult). User expertise: \u201cIn this conversation, did you know what you could say or do at each point of the dialog?\u201d on a scale of 1 (no, not at all) to 7 (yes, completely). At",
        "type": "Document"
      },
      {
        "id": "b3036530-5228-47f4-b1f0-9a1687f209da",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1709.10217",
          "start_idx": 4141,
          "end_idx": 4657
        },
        "page_content": ", in which only the released data can be used for training and developing. The other is an open evaluation that allow to collect external data for training and developing. For task 1, we use F1-score as evaluation metric. For the task-oriented dialogue systems, the best way for evaluation is to use the online human-computer dialogue. After finishing an online human-computer dialogue with a dialogue system, the human then manually evaluate the system by using the metrics of user satisfaction degree, dialogue flu",
        "type": "Document"
      },
      {
        "id": "96ecbdb9-17ee-46e0-9f1c-9eef560fb526",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1912.13109",
          "start_idx": 10203,
          "end_idx": 10681
        },
        "page_content": " metrics - Recall rate, F1 score and Overall accuracy. Choice of model parameters were in the above models were inspired from previous work done but then were tuned to the best performance of the Test dataset. Following parameters were considered for tuning. Learning rate: Based on grid search the best performance was achieved when learning rate was set to 0.01. This value was arrived by a grid search on lr parameter. Number of Bidirectional LSTM units: A set of 32, 64, 128",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the datasets used for the task?",
    "llm_answer": "Stanford Sentiment Treebank (SST-2), Yelp dataset, Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), Quora Question Pairs, MCTest, CNN/Daily Mail, Children's Book Test, and a dataset for simulated indoor environments (using Amazon Mechanical Turk).\n",
    "context": [
      {
        "id": "4a2bbc22-5a05-441f-92ce-90e5ab0a95c8",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1910.00458",
          "start_idx": 17496,
          "end_idx": 17916
        },
        "page_content": "500). For the sentiment analysis category, we used the Stanford Sentiment Treebank (SST-2) dataset from the GLUE benchmark BIBREF22 (around 60k train examples) and the Yelp dataset (around 430k train examples). For the paraphrase category, three paraphrasing datasets are used from the GLUE benchmark: Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), and Quora Question Pairs (",
        "type": "Document"
      },
      {
        "id": "75c8f911-af20-49c1-8b12-e4aecd8370c1",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1707.03904",
          "start_idx": 5304,
          "end_idx": 5850
        },
        "page_content": " subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short. We plan to release these annotations along with the datasets, and our retrieved documents for each question. Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated",
        "type": "Document"
      },
      {
        "id": "032b9bed-6a38-456d-b02a-dea6816fa084",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1910.12618",
          "start_idx": 7204,
          "end_idx": 7727
        },
        "page_content": " section 5 concludes this paper and gives insight on future possible work. In order to prove the consistency of our work, experiments have been conducted on two data sets, one for France and the other for the UK. In this section details about the text and time series data are given, as well as the major preprocessing steps. Three types of time series are considered in our work: national net electricity consumption (also referred as load or demand), national temperature and wind speed. The load data sets were retrieved",
        "type": "Document"
      },
      {
        "id": "750cf7d4-eaf2-41cf-a5fe-3fdf7b919809",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1810.00663",
          "start_idx": 17810,
          "end_idx": 18361
        },
        "page_content": " . This dataset was created using Amazon Mechanical Turk and 100 maps of simulated indoor environments, each with 6 to 65 rooms. To the best of our knowledge, this is the first benchmark for comparing translation models in the context of behavioral robot navigation. As shown in Table TABREF16 , the dataset consists of 8066 pairs of free-form natural language instructions and navigation plans for training. This training data was collected from 88 unique simulated environments, totaling 6064 distinct navigation plans (2002 plans have two different",
        "type": "Document"
      },
      {
        "id": "2222ff1c-48c2-4593-8cbe-f96acd7fec63",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1703.04617",
          "start_idx": 1343,
          "end_idx": 1868
        },
        "page_content": " can help attain better results on our competitive baselines. Recent advance on reading comprehension and question answering has been closely associated with the availability of various datasets. BIBREF0 released the MCTest data consisting of 500 short, fictional open-domain stories and 2000 questions. The CNN/Daily Mail dataset BIBREF1 contains news articles for close style machine comprehension, in which only entities are removed and tested for comprehension. Children's Book Test (CBT) BIBREF2 leverages named entities",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the accuracy of the model for the six languages tested?",
    "llm_answer": "The accuracy of the model for the six languages is not found in the context.\n",
    "context": [
      {
        "id": "b4048d90-aa2a-4fd2-8f5e-b7848534cb17",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1910.04269",
          "start_idx": 17760,
          "end_idx": 18217
        },
        "page_content": " of 1,500 samples for each of six languages. Out of 1,500 samples for each language, 1,200 were randomly selected as training dataset for that language and rest 300 as validation dataset using k-fold cross-validation. To sum up, we trained our model on 7,200 samples and validated it on 1800 samples comprising six languages. The results are discussed in next section. This paper discusses two end-to-end approaches which achieve state-of-the-art results in",
        "type": "Document"
      },
      {
        "id": "1d539c4c-9ef1-4b04-82d9-1a43355eac55",
        "metadata": {
          "chunk_id": 59,
          "document_id": "2001.11268",
          "start_idx": 23352,
          "end_idx": 23963
        },
        "page_content": " this project's GitHub repository , including true class labels and a detailed breakdown of true and false predictions for each class. The highest proportion of false classification appears between the results and conclusion classes. The fine-tuned multilingual model showed marginally inferior classification scores on the exclusively English testing data. However, this model's contribution is not limited to the English language because its interior weights embed a shared vocabulary of 100 languages, including German and Chinese. Our evaluation of the multilingual model's capacity for language transfer is",
        "type": "Document"
      },
      {
        "id": "2cd2e8d6-346f-4395-ac7f-a99a403c307c",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1707.03764",
          "start_idx": 9602,
          "end_idx": 10104
        },
        "page_content": " model for the four languages. We trained the model for the gender label on English, Portuguese and Arabic and tested it on Spanish: the system scored 0.67 in accuracy. We looked at accuracy scores for the English gender and variety data more closely. We tried different representations of the tweet texts, to see what kind of words were most predictive of variety and gender. Specifically, we look at using only words that start with an uppercase letter, only words that start with a lowercase letter,",
        "type": "Document"
      },
      {
        "id": "74d41b8d-63d7-479f-a82f-76142d721807",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1911.03894",
          "start_idx": 15623,
          "end_idx": 16121
        },
        "page_content": " is also machine translated for all languages. The dataset is composed of 122k train, 2490 valid and 5010 test examples. As usual, NLI performance is evaluated using accuracy. To evaluate a model on a language other than English (such as French), we consider the two following settings: TRANSLATE-TEST: The French test set is machine translated into English, and then used with an English classification model. This setting provides a reasonable, although imperfect, way to circumvent the fact that",
        "type": "Document"
      },
      {
        "id": "4f366b00-7e35-42cd-8d5d-0a1a3c14dcfa",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1906.06448",
          "start_idx": 21919,
          "end_idx": 22444
        },
        "page_content": " also indicates the difficulty of problems from linguistics publications. Regarding non-monotone problems collected via crowdsourcing, there are very few non-monotone problems, so accuracy is 100%. Adding non-monotone problems to our test set is left for future work. Table 9 shows the evaluation results by type of linguistic phenomenon. While accuracy on problems involving NPIs and conditionals was improved on both upward and downward inferences, accuracy on problems involving conjunction and disjunction was improved on",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which models achieve state-of-the-art performances?",
    "llm_answer": "BERT-based models, specifically $RoBERTa_{large}$ and DistillRoBERTa.\n",
    "context": [
      {
        "id": "38d0d5d3-4be2-4a08-bc2e-e9f401074e5d",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1701.06538",
          "start_idx": 17346,
          "end_idx": 17826
        },
        "page_content": ". Details can be found in Appendix UID77 . Results of these three models form the bottom line of Figure FIGREF32 -right. Table TABREF33 compares the results of these models to the best previously-published result on this dataset . Even the fastest of these models beats the best published result (when controlling for the number of training epochs), despite requiring only 6% of the computation. We trained our models using TensorFlow BIBREF30 on clusters containing 16-32 Tesla K",
        "type": "Document"
      },
      {
        "id": "60373253-fa83-4f44-bcbd-a972e3cb2654",
        "metadata": {
          "chunk_id": 24,
          "document_id": "2002.11402",
          "start_idx": 8819,
          "end_idx": 9288
        },
        "page_content": "es other models in all scores. Spacy results are good despite not being trained for case-less data. In terms of F1 and overall stability Spacy did better than Stanford NER, on our News Validation set. Similarly, Stanford did well in Precision but could not catch up with Spacy and our model in terms of Recall. Flair overall performed poorly, but as said before these open-source models are not trained for our particular use-case. Lets check some examples for detailed",
        "type": "Document"
      },
      {
        "id": "76b8b551-5acb-4396-ad68-bd7052c2758b",
        "metadata": {
          "chunk_id": 17,
          "document_id": "2002.06644",
          "start_idx": 6467,
          "end_idx": 6878
        },
        "page_content": "ERT based models, and BERT-based ensemble models outperform the baselines for all the metrics. Among the optimized BERT based models, $RoBERTa_{large}$ outperforms all other non-ensemble models and the baselines for all metrics. It further achieves a maximum recall of $0.681$ for all the proposed models. We note that DistillRoBERTa, a distilled model, performs competitively, achieving $69.69\\%$ accuracy, and",
        "type": "Document"
      },
      {
        "id": "1cb0a728-23ef-4b61-bb76-e238aa19764e",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1606.08140",
          "start_idx": 12268,
          "end_idx": 12747
        },
        "page_content": " that the models employing path information generally achieve better results than models that do not use such information. In terms of models not exploiting path information or external information, the STransE model produces the highest filtered mean rank on WN18 and the highest filtered Hits@10 and mean reciprocal rank on FB15k. Compared to the closely related models SE, TransE, TransR, CTransR, TransD and TranSparse, our STransE model does better than these models on both",
        "type": "Document"
      },
      {
        "id": "dc948dec-3a23-4196-b1ee-c386b0464b4d",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1909.13375",
          "start_idx": 20052,
          "end_idx": 20444
        },
        "page_content": "s. For the large models we used a batch size of 3 with a learning rate of $5e^{-6}$ and trained for 5 epochs, except for the model without the single-span heads that was trained with a batch size of 2 for 7 epochs. F1 was used as our validation metric. All models were trained on a single GPU with 12-16GB of memory. Table TABREF24 shows the results on DROP's development set. Compared to our",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which competitive relational classification models do they test?",
    "llm_answer": "Spacy, Stanford NER, Flair, and a model by E. Tong et al. (BIBREF9).\n",
    "context": [
      {
        "id": "071cfc43-73e9-4066-8835-9d1dcc3c1cb2",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1906.03338",
          "start_idx": 2993,
          "end_idx": 3586
        },
        "page_content": " arguments, our work investigates how (de-)contextualization of argumentative units affects automatic argumentative relation classification models. In this section, we describe different formulations of the argumentative relation classification task and describe features used by our replicated model. In order to test our hypotheses, we propose to group all features into three distinct types. Now, we introduce a classification of three different prediction models used in the argumentative relation prediction literature. We will inspect all of them and show that all can suffer from severe",
        "type": "Document"
      },
      {
        "id": "e503bdfc-e870-4d27-90b5-b2a3cf6921f4",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1908.05434",
          "start_idx": 13265,
          "end_idx": 13751
        },
        "page_content": " comparison of our proposed model with commonly used ordinal regression models as well as the previous state-of-the-art classification model by E. Tong et al. ( BIBREF9 ). To assess the effect of each component in our model, we perform an ablation test where the components are swapped by their more standard alternatives one at a time. Next, we perform a qualitative analysis on the model predictions on the raw data, which are scraped from a different escort website than the one that",
        "type": "Document"
      },
      {
        "id": "60373253-fa83-4f44-bcbd-a972e3cb2654",
        "metadata": {
          "chunk_id": 24,
          "document_id": "2002.11402",
          "start_idx": 8819,
          "end_idx": 9288
        },
        "page_content": "es other models in all scores. Spacy results are good despite not being trained for case-less data. In terms of F1 and overall stability Spacy did better than Stanford NER, on our News Validation set. Similarly, Stanford did well in Precision but could not catch up with Spacy and our model in terms of Recall. Flair overall performed poorly, but as said before these open-source models are not trained for our particular use-case. Lets check some examples for detailed",
        "type": "Document"
      },
      {
        "id": "93766546-4291-49ca-9be0-ef7537240cc7",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1906.03338",
          "start_idx": 18109,
          "end_idx": 18674
        },
        "page_content": " ). This holds true for all three task formulations and provides further evidence that models \u2013 when given the option \u2013 put a strong focus on contextual clues while neglecting the information provided by the EAU span itself. While competitive systems for argumentative relation classification are considered to be robust, our experiments have shown that despite confidence-inspiring scores on unseen testing data, such systems can easily be fooled \u2013 they can deliver strong performance scores although the classifier does not have access to the content of the EAUs.",
        "type": "Document"
      },
      {
        "id": "00da59da-82d7-4382-9471-069e0d1df5c2",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1905.08949",
          "start_idx": 9823,
          "end_idx": 10318
        },
        "page_content": " cognitive level and answer type, along with their statistics. Among them, SQuAD was used by most groups as the benchmark to evaluate their NQG models. This provides a fair comparison between different techniques. However, it raises the issue that most NQG models work on factoid questions with answer as text span, leaving other types of QG problems less investigated, such as generating deep multi-choice questions. To overcome this, a wider variety of corpora should be benchmarked against in",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they gather human judgements for similarity between relations?",
    "llm_answer": "Nine undergraduate subjects were asked to rate the similarity of 360 pairs of relations from a subset of Wikidata, on a scale of 0 (no similarity) to 40 (high similarity).\n",
    "context": [
      {
        "id": "27179fdd-f36f-40ae-a118-3e33f137c568",
        "metadata": {
          "chunk_id": 67,
          "document_id": "1907.08937",
          "start_idx": 26256,
          "end_idx": 26730
        },
        "page_content": " of applications. We note that there are a wide range of future directions: (1) human prior knowledge could be incorporated into the similarity quantification; (2) similarity between relations could also be considered in multi-modal settings, e.g., extracting relations from images, videos, or even from audios; (3) by analyzing the distributions corresponding to different relations, one can also find some \u201cmeta-relations\u201d between relations, such as hypernymy and hyponymy",
        "type": "Document"
      },
      {
        "id": "27f73edc-9977-4d7d-a354-aa3f67e7fd48",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1907.08937",
          "start_idx": 2997,
          "end_idx": 3549
        },
        "page_content": " a framework for assessing the similarity between relations, our second contribution is that we have done a survey of applications. We present experiments and analysis aimed at answering five questions: (1) How well does the computed similarity score correlate with human judgment about the similarity between relations? How does our approach compare to other possible approaches based on other kinds of relation embeddings to define a similarity? (sec:relationship and sec:human-judgment) (2) Open IE models inevitably extract many redundant relations",
        "type": "Document"
      },
      {
        "id": "8b34466d-8a6e-474d-8a3b-2f92ec6d2368",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1907.08937",
          "start_idx": 25697,
          "end_idx": 26256
        },
        "page_content": " sparse data, which makes it difficult to achieve an effective and stable similarity metric. Motivated by this, we propose to measure relation similarity by leveraging their fact distribution so that we can identify nuances between similar relations, and merge those distant surface forms of the same relations, benefitting the tasks mentioned above. In this paper, we introduce an effective method to quantify the relation similarity and provide analysis and a survey of applications. We note that there are a wide range of future directions: (1) human prior",
        "type": "Document"
      },
      {
        "id": "e578623d-ba74-4a6a-92da-75053539acd6",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1907.08937",
          "start_idx": 25439,
          "end_idx": 25868
        },
        "page_content": "40 , BIBREF41 , BIBREF42 , BIBREF43 , and relation prediction BIBREF3 , BIBREF44 , BIBREF45 , BIBREF46 , BIBREF47 . For both semantic relations and general relations, identifying them is a crucial problem, requiring systems to provide a fine-grained relation similarity metric. However, the existing methods suffer from sparse data, which makes it difficult to achieve an effective and stable similarity metric. Motivated by this",
        "type": "Document"
      },
      {
        "id": "24f1ff88-5bc5-4b24-9175-83e0b2b3d42a",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1907.08937",
          "start_idx": 11316,
          "end_idx": 11814
        },
        "page_content": " extraction dataset obtained via crowdsourcing. We directly use these two dataset, no extra processing steps were applied. Following BIBREF11 , BIBREF12 and the vast amount of previous work on semantic similarity, we ask nine undergraduate subjects to assess the similarity of 360 pairs of relations from a subset of Wikidata BIBREF8 that are chosen to cover from high to low levels of similarity. In our experiment, subjects were asked to rate an integer similarity score from 0 (no similarity) to",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What text classification task is considered?",
    "llm_answer": "Question classification; predicting the degree of harm (DoH) among five possible values (1-5).\n",
    "context": [
      {
        "id": "e8dc8309-8ea6-47df-99e6-dc78e604f23f",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1605.08675",
          "start_idx": 18319,
          "end_idx": 18812
        },
        "page_content": " case of entity questions; a WordNet synset, to which a question focus belongs; necessary for DeepER. Search query \u2013 used to find possibly relevant documents, Question content \u2013 the words from question which are supposed to appear also in context of an answer. The task presented above, called question classification, is an example of text classification with very short texts. It could be tackled by a general-purpose classifier; for example, BIBREF11 used SVMs (Support Vector Machines) for",
        "type": "Document"
      },
      {
        "id": "49566145-f5a3-45f6-907d-eb980f2b4bbe",
        "metadata": {
          "chunk_id": 72,
          "document_id": "2003.06279",
          "start_idx": 27434,
          "end_idx": 28020
        },
        "page_content": "ding technique yielding the best result. All optimized results were obtained with the use of stopwords, global thresholding strategy and SVM as classification algorithm. A significant gain is more evident for intermediary text lengths. Textual classification remains one of the most important facets of the Natural Language Processing area. Here we studied a family of classification methods, the word co-occurrence networks. Despite this apparent simplicity, this model has been useful in several practical and theoretical scenarios. We proposed a modification of the traditional model",
        "type": "Document"
      },
      {
        "id": "4f5324be-1104-4f32-a242-2758c8163dce",
        "metadata": {
          "chunk_id": 81,
          "document_id": "1909.00183",
          "start_idx": 30160,
          "end_idx": 30649
        },
        "page_content": " the performance of the supervised classification. The supervised classification was carried out by training on features and text three classifiers commonly applied to text classification tasks BIBREF22, BIBREF23: a Ridge classifier, Support Vector Machines with a linear kernel, and Random Forests. The goal is to predict the degree of harm (DoH) among five possible values (1-5). The classification is carried out with five-fold cross validation, using 80% of the data to train the model",
        "type": "Document"
      },
      {
        "id": "827b5103-5709-4527-8813-320580551ea9",
        "metadata": {
          "chunk_id": 3,
          "document_id": "2003.06279",
          "start_idx": 1262,
          "end_idx": 1718
        },
        "page_content": " text classification tasks BIBREF8, BIBREF9, BIBREF10. The main advantage of the model is that it does not rely on deep semantical information to obtain competitive results. Another advantage of graph-based approaches is that, when combined with other approaches, it yields competitive results BIBREF11. A simple, yet recurrent text model is the well-known word co-occurrence network. After optional textual pre-processing steps, in a co-occurrence network",
        "type": "Document"
      },
      {
        "id": "5ad1e594-5ff9-48ac-b9b9-7c105b6f7eda",
        "metadata": {
          "chunk_id": 4,
          "document_id": "2001.00137",
          "start_idx": 1748,
          "end_idx": 2222
        },
        "page_content": " Language Processing (NLP) domain. Tasks such as text generation, sentence correction, image captioning and text classification, have been possible via models such as Convolutional Neural Networks and Recurrent Neural Networks BIBREF5, BIBREF6, BIBREF7. More recently, state-of-the-art results have been achieved with attention models, more specifically Transformers BIBREF8. Surprisingly, however, there is currently no research on incomplete text classification in the NLP",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What novel class of recurrent-like networks is proposed?",
    "llm_answer": "A neural network architecture that leverages both recurrence and one-dimensional convolutions.\n",
    "context": [
      {
        "id": "83186976-9ab8-42cc-a9fc-245478dd7118",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1911.00069",
          "start_idx": 12563,
          "end_idx": 12982
        },
        "page_content": ". Recurrent neural networks (RNNs) are a class of neural networks that operate on sequential data such as sequences of words. LSTM networks are a type of RNNs that have been invented to better capture long-range dependencies in sequential data. We pass the word embeddings $\\mathbf {x}_t$'s to a forward and a backward LSTM layer. A forward or backward LSTM layer consists of a set of recurrently connected blocks known",
        "type": "Document"
      },
      {
        "id": "38383edc-b8b7-4681-aa17-7396f5c2cf75",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1906.00180",
          "start_idx": 1317,
          "end_idx": 1867
        },
        "page_content": " rules that characterize it, and focus in particular on recurrent neural networks that are unambiguously `connectionist': trained recurrent nets do not rely on symbolic data and control structures such as trees and global variable binding, and can straightforwardly be implemented in biological networks BIBREF8 or neuromorphic hardware BIBREF9 . We report positive results on this challenge, and in the process develop a series of tests for compositional generalization that address the concerns of deep learning skeptics. The paper makes three main",
        "type": "Document"
      },
      {
        "id": "f79c6ab6-4cf0-4f8a-9f3f-d61ac1b53501",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1711.02013",
          "start_idx": 4391,
          "end_idx": 4818
        },
        "page_content": ", which can be dated back to 1990s (e.g. BIBREF23 , BIBREF24 , BIBREF25 ). The NARX RNN BIBREF25 is another example which used a feed forward net taking different inputs with predefined time delays to model long-term dependencies. More recently, BIBREF26 also used multiple layers of recurrent networks with different pre-defined updating frequencies. Instead, our model tries to learn the structure from data, rather than pred",
        "type": "Document"
      },
      {
        "id": "4796c61b-76c1-41c3-b84f-3c0f2a6752c7",
        "metadata": {
          "chunk_id": 105,
          "document_id": "1803.02839",
          "start_idx": 40577,
          "end_idx": 41079
        },
        "page_content": " to consider neural network solutions to higher-order generalizations: DISPLAYFORM0  Networks that solve Eq.( EQREF85 ) are recurrent-like. Updates to a hidden state will generally depend on states beyond the immediately preceding one; often, this dependence can be captured by evolving on the phase space of the hidden states, rather than on the sequences of the hidden states themselves. The latter results in a nested RNN structure for the recurrent-like cell, similar to the structure proposed in B",
        "type": "Document"
      },
      {
        "id": "10b5e7b6-7ae0-48f8-84aa-952a6dd05d7c",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1909.13362",
          "start_idx": 6723,
          "end_idx": 7196
        },
        "page_content": ", we present a neural network architecture that leverages both recurrence and one-dimensional convolutions. Recurrence enables our model to read a sequence much like a human would; a sequence with elements $abcd$ would be read one element at a time, updating a latent understanding after reading each $a$, $b$, $c$, and finally $d$. One-dimensional convolutions extract a spatial relationship between sequential elements. The $abcd$ example sequence may then be read as $ab",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much bigger is Switchboard-2000 than Switchboard-300 database?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "94725338-d50b-40f9-b9f8-72f74c10a7d7",
        "metadata": {
          "chunk_id": 46,
          "document_id": "2001.07263",
          "start_idx": 17384,
          "end_idx": 17775
        },
        "page_content": " others, the 2000-hour system reaches 8.7% and 7.4% WER on rt02 and rt04. We observe that the regularization techniques, which are extremely important on the 300h setup, are still beneficial but have a significantly smaller effect.  For comparison with results in the literature we refer to the Switchboard-300 results in BIBREF3, BIBREF7, BIBREF51, BIBREF52 and the Switchboard-2000 results",
        "type": "Document"
      },
      {
        "id": "7d420fd2-453e-4489-b5cc-0572559f68ae",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1811.00942",
          "start_idx": 6659,
          "end_idx": 7058
        },
        "page_content": "87K tokens for training, 70K for validation, and 78K for test, with a vocabulary size of 10,000. On the other hand, WT103 comprises 103 million tokens for training, 217K for validation, and 245K for test, spanning a vocabulary of 267K unique tokens. For the neural language model, we used a four-layer QRNN BIBREF10 , which achieves state-of-the-art results on a variety of datasets, such as WT103 B",
        "type": "Document"
      },
      {
        "id": "c0fe6731-dd47-4c02-b1f9-48e82006df2c",
        "metadata": {
          "chunk_id": 41,
          "document_id": "1910.00458",
          "start_idx": 13703,
          "end_idx": 14141
        },
        "page_content": " almost the same way BIBREF9 with only one difference that MC160 is around three times smaller than MC500. We can see from Table TABREF17 that both the BERT and RoBERTa baselines perform much worse on MC160 than MC500. We think the reason is that the data size of MC160 is not enough to well fine-tune the large models with a huge amount of trainable parameters. However, by leveraging the transfer learning techniques we proposed, we can",
        "type": "Document"
      },
      {
        "id": "01bd1200-c1dd-412c-a717-ff4db1af4e8c",
        "metadata": {
          "chunk_id": 47,
          "document_id": "2001.07263",
          "start_idx": 17775,
          "end_idx": 18103
        },
        "page_content": "IBREF7, BIBREF51, BIBREF52 and the Switchboard-2000 results in BIBREF50, BIBREF51, BIBREF53, BIBREF54, BIBREF55, BIBREF56. Our 300-hour model not only outperforms the previous best attention based encoder-decoder model BIBREF3 by a large margin, it also surpasses the best hybrid systems with multiple LMs BIBREF7. Our result on",
        "type": "Document"
      },
      {
        "id": "95c62085-b478-4b5e-a4fa-049a6c8fc5d0",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1908.10449",
          "start_idx": 22880,
          "end_idx": 23381
        },
        "page_content": " hard time learning in this setting. As shown in Table TABREF16, both datasets have a vocabulary size of more than 100k. This is much larger than in the other two settings, where on average the length of questions is around 10. This suggests that the methods with better sample efficiency are needed to act in more realistic problem settings with huge action spaces. Experiments also show that a larger memory slot size always helps. Intuitively, with a memory mechanism (either implicit or explicit),",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What domains are detected in this paper?",
    "llm_answer": "The provided text mentions \"Lifestyle\" and \"Scientific\" domains, and implies the detection of other domains from Wikipedia.\n",
    "context": [
      {
        "id": "3be6660a-4a1b-45c7-9905-2b27b4e2df1b",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1907.11499",
          "start_idx": 3251,
          "end_idx": 3868
        },
        "page_content": " involvement. Instead of learning from individually labeled segments, our model only requires document-level supervision and optionally prior domain knowledge and learns to introspectively judge the domain of constituent segments. Importantly, we do not require document-level domain annotations either since we obtain these via distant supervision by leveraging information drawn from Wikipedia. Our domain detection framework comprises two neural network modules; an encoder learns representations for words and sentences together with prior domain information if the latter is available (e.g., domain definitions),",
        "type": "Document"
      },
      {
        "id": "8348e336-9079-412a-a078-faa11a754824",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1907.11499",
          "start_idx": 6351,
          "end_idx": 6867
        },
        "page_content": " at hand. For example, the definition of the \u201cLifestyle\u201d domain is \u201cthe interests, opinions, behaviors, and behavioral orientations of an individual, group, or culture\u201d. Figure FIGREF5 provides an overview of our Domain Detection Network, which we call DetNet. The model comprises two modules; an encoder learns representations for words and sentences whilst incorporating prior domain information; a detector generates domain scores for words, sentences, and documents by selectively attending to previously encoded",
        "type": "Document"
      },
      {
        "id": "319c9757-9bd8-4aed-9974-60604182d684",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1701.03214",
          "start_idx": 5368,
          "end_idx": 5787
        },
        "page_content": " (essentially open) in-domain data. The scientific domain MT was conducted on the Chinese-Japanese paper excerpt corpus (ASPEC-CJ) BIBREF11 , which is one subtask of the workshop on Asian translation (WAT) BIBREF15 . The ASPEC-CJ task uses 672315, 2090, and 2107 sentences for training, development, and testing, respectively. The Wikipedia domain task was conducted on a Chinese-Japanese corpus automatically extracted",
        "type": "Document"
      },
      {
        "id": "c681f5c1-05bf-473a-93ee-d4875daf9705",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1907.11499",
          "start_idx": 3868,
          "end_idx": 4419
        },
        "page_content": " and sentences together with prior domain information if the latter is available (e.g., domain definitions), while a detector generates domain-specific scores for words, sentences, and documents. We obtain a segment-level domain predictor which is trained end-to-end on document-level labels using a hierarchical, attention-based neural architecture BIBREF15 . We conduct domain detection experiments on English and Chinese and measure system performance using both automatic and human-based evaluation. Experimental results show that our model outper",
        "type": "Document"
      },
      {
        "id": "4d40d9b3-3ffe-4d6b-9e2f-430159f4401f",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1907.11499",
          "start_idx": 5095,
          "end_idx": 5596
        },
        "page_content": " We formulate domain detection as a multilabel learning problem. Our model is trained on samples of document-label pairs. Each document consists of INLINEFORM0 sentences INLINEFORM1 and is associated with discrete labels INLINEFORM2 . In this work, domain labels are not annotated manually but extrapolated from Wikipedia (see Section SECREF6 for details). In a non-MIL framework, a model typically learns to predict document labels by directly conditioning on its sentence representations INLINEFORM3",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Why do they think this task is hard?  What is the baseline performance?",
    "llm_answer": "The task is hard because baselines lag behind human performance by 16.4% and 32.1% for Quasar-S and Quasar-T respectively.\n",
    "context": [
      {
        "id": "a98b3d51-106c-4190-b27f-eed96e43fcbb",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1707.03904",
          "start_idx": 20754,
          "end_idx": 21226
        },
        "page_content": " baselines, however, lag behind human performance by $16.4\\%$ and $32.1\\%$ for Quasar-S and Quasar-T respectively, indicating the strong potential for improvement. Interestingly, for human performance we observe that non-experts are able to match or beat the performance of experts when given access to the background corpus for searching the answers. We also emphasize that the human performance is limited by either the knowledge of the experts, or the usefulness of the",
        "type": "Document"
      },
      {
        "id": "7c3d09a4-736a-4647-96df-1b3dfe5eed0e",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1910.08210",
          "start_idx": 18169,
          "end_idx": 18684
        },
        "page_content": "-to-one group assignments and natural language templated descriptions. The performance across the curriculum is shown in Table TABREF32 (see Figure FIGREF58 in appendix SECREF12 for training curves of each stage). We see that curriculum learning is crucial to making progress on , and that initial policy training (first row of Table TABREF32) with additional complexities in any of the dimensions result in significantly worse performance. We take each of the 5 runs after training through the whole curriculum and",
        "type": "Document"
      },
      {
        "id": "f6fc3632-85ae-4401-b21d-36d939a2b100",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1906.06448",
          "start_idx": 9932,
          "end_idx": 10429
        },
        "page_content": " each set of substitutions, and each set was assigned to three workers. To remove low-quality examples, we set the minimum time it should take to complete each set to 200 seconds. The entry in our task was restricted to workers from native speaking English countries. 128 workers contributed to the task, and we created 15,339 hypotheses (7,179 upward examples and 8,160 downward examples). The gold label of each premise-hypothesis pair created in the previous task is automatically determined by",
        "type": "Document"
      },
      {
        "id": "8b8929af-c816-417f-9934-e809f4136e2b",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1706.07179",
          "start_idx": 5878,
          "end_idx": 6329
        },
        "page_content": " the graph. We evaluate the model's performance on the bAbI tasks BIBREF18 , a collection of 20 question answering tasks which have become a benchmark for evaluating memory-augmented neural networks. We compare the performance with the Recurrent Entity Networks model (EntNet) BIBREF17 . Performance is measured in terms of mean percentage error on the tasks. Training Details: We used Adam and did a grid search for the learning rate in {0.01, 0.005,",
        "type": "Document"
      },
      {
        "id": "ec846c61-b035-431a-88d4-858e8c47d3be",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1603.07044",
          "start_idx": 9966,
          "end_idx": 10403
        },
        "page_content": " for task A, there are a total number of 26,700 question-comment pairs. For task B, there are 2,670 question-question pairs. For task C, there are 26,700 question-comment pairs. The test dataset includes 50 questions, 500 related questions and 5,000 comments which do not overlap with the training set. To evaluate the performance, we use mean average precision (MAP) and F1 score. Table 2 shows the initial results using the RNN encoder",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they generate the synthetic dataset?",
    "llm_answer": "Users open existing documents, make changes, and save them as separate documents.  The generation process mimics real-world actions.\n",
    "context": [
      {
        "id": "46696be8-fc07-4b91-a6b4-1f1160ed56a3",
        "metadata": {
          "chunk_id": 52,
          "document_id": "1709.01256",
          "start_idx": 19441,
          "end_idx": 19942
        },
        "page_content": " We removed empty lines in the documents and trained word2vec embeddings on the entire corpus. We used the documents occurring in the first INLINEFORM0 of the revision period for INLINEFORM1 calibration, and the remaining documents for test. The generation process of the simulated data sets is designed to mimic the real world. Users open some existing documents in a file system, make some changes (e.g. addition, deletion or replacement), and save them as separate documents. These documents become",
        "type": "Document"
      },
      {
        "id": "a623c523-726b-4639-8351-15805b46e720",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1910.09399",
          "start_idx": 15104,
          "end_idx": 15618
        },
        "page_content": " the data space fall into some distribution in a very complex and high-dimensional feature space. Sampling from such a complex space is very difficult, so GANs instead train a generator to create synthetic images from a much more simple feature space (usually random noise) called the latent space. The generator network performs up-sampling of the latent space and is usually a deep neural network consisting of several convolutional and/or fully connected layers BIBREF21. The generator is trained using gradient",
        "type": "Document"
      },
      {
        "id": "39a59db2-6e27-4feb-b2d7-c8ad2e1fd6f2",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1906.00180",
          "start_idx": 2202,
          "end_idx": 2706
        },
        "page_content": " data generation process is inspired by BIBREF13 : an artificial language is defined, sentences are generated according to its grammar and the entailment relation between pairs of such sentences is established according to a fixed background logic. However, our language is significantly more complex, and instead of natural logic we use FOL. Our main model is a recurrent network, sketched in Figure 4 . It is a so-called `Siamese' network because it uses the same parameters to process the left and the",
        "type": "Document"
      },
      {
        "id": "b2416b9f-678b-4a0e-9b92-43b9778dbf1e",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1910.09399",
          "start_idx": 14628,
          "end_idx": 15104
        },
        "page_content": " in an unsupervised fashion BIBREF24. The following sections go into further detail regarding how the generator and discriminator are trained in GANs. Generator - In image synthesis, the generator network can be thought of as a mapping from one representation space (latent space) to another (actual data) BIBREF21. When it comes to image synthesis, all of the images in the data space fall into some distribution in a very complex and high-dimensional feature space. Sampling",
        "type": "Document"
      },
      {
        "id": "7bb7d20a-1968-47d3-914d-0675670094b0",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1910.09399",
          "start_idx": 3211,
          "end_idx": 3731
        },
        "page_content": " (GANs) consist of two neural networks paired with a discriminator and a generator. These two models compete with one another, with the generator attempting to produce synthetic/fake samples that will fool the discriminator and the discriminator attempting to differentiate between real (genuine) and synthetic samples. Because GANs' adversarial training aims to cause generators to produce images similar to the real (training) images, GANs can naturally be used to generate synthetic images (image synthesis), and this",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the average length of the claims?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "92c0086a-a7d3-45c4-a6cc-df984eaa2cc6",
        "metadata": {
          "chunk_id": 32,
          "document_id": "2004.03034",
          "start_idx": 12548,
          "end_idx": 13000
        },
        "page_content": ") for a particular claim C is defined by number of claims included in the argument path starting from the thesis until the claim C. For example, in Figure FIGREF1, the context length for O1 and S3 are 1 and 2 respectively. Table TABREF8 shows number of claims with the given range of context length for the claims with more than 5 votes and $60\\%$ agreement score. We observe that more than half of these claims have 3 or higher context length. Similar",
        "type": "Document"
      },
      {
        "id": "f5629c09-7dee-477d-b1e4-d97a73124da7",
        "metadata": {
          "chunk_id": 31,
          "document_id": "2004.03034",
          "start_idx": 12247,
          "end_idx": 12760
        },
        "page_content": " claim is timely and appropriate given its context. There are cases in the dataset where the same claim has different impact labels, when presented within a different context. Therefore, we claim that it is not sufficient to only study the linguistic characteristic of a claim to determine its impact, but it is also necessary to consider its context in determining the impact. Context length ($\\text{C}_{l}$) for a particular claim C is defined by number of claims included in the argument path starting from the",
        "type": "Document"
      },
      {
        "id": "f507266e-edfa-4f40-a05c-83887b674fe1",
        "metadata": {
          "chunk_id": 33,
          "document_id": "2004.03034",
          "start_idx": 13000,
          "end_idx": 13546
        },
        "page_content": " agreement score. We observe that more than half of these claims have 3 or higher context length. Similar to prior work, our aim is to understand the characteristics of impactful claims in argumentation. However, we hypothesize that the qualitative characteristics of arguments is not independent of the context in which they are presented. To understand the relationship between argument context and the impact of a claim, we aim to incorporate the context along with the claim itself in our predictive models. Prediction task. Given a claim, we",
        "type": "Document"
      },
      {
        "id": "d57b48a2-0597-49c7-ba89-594a0726bee9",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1703.05260",
          "start_idx": 17876,
          "end_idx": 18350
        },
        "page_content": " inter-annotator agreement was measured (see Section \"Inter-Annotator Agreement\" ). It was found to be sufficiently high. Annotation of the corpus together with some pre- and post-processing of the data required about 500 hours of work. All stories were annotated with event and participant types (a total of 12,188 and 43,946 instances, respectively). On average there were 7 coreference chains per story with an average length of 6 tokens. After the first annotation round",
        "type": "Document"
      },
      {
        "id": "aa51bc7a-d137-4342-81b8-af05dedabcf1",
        "metadata": {
          "chunk_id": 23,
          "document_id": "2004.03034",
          "start_idx": 9268,
          "end_idx": 9778
        },
        "page_content": " our study to the claims with at least 5 votes to have a more reliable assignment for the accumulated impact label for each claim. Impact label statistics. Table TABREF7 shows the distribution of the number of votes for each of the impact categories. The claims have $241,884$ total votes. The majority of the impact votes belong to medium impact category. We observe that users assign more high impact and very high impact votes than low impact and no impact votes respectively. When we restrict the claims to",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What debate topics are included in the dataset?",
    "llm_answer": "Politics, World, Society, Law, Ethics, War, Conflict in the Middle East, Latin America, Commonwealth, and others.  (e.g., iPhone6, a political candidate, a policy, an event).\n",
    "context": [
      {
        "id": "85e542a8-0eb1-4992-92d1-94606138b39d",
        "metadata": {
          "chunk_id": 33,
          "document_id": "1906.03538",
          "start_idx": 13163,
          "end_idx": 13614
        },
        "page_content": " Table TABREF19 . To better understand the topical breakdown of claims in the dataset, we crowdsource the set of \u201ctopics\u201d associated with each claim (e.g., Law, Ethics, etc.) We observe that, as expected, the three topics of Politics, World, and Society have the biggest portions (Figure FIGREF21 ). Additionally, the included claims touch upon 10+ different topics. Figure FIGREF22 depicts a few popular categories and sampled questions from each. We",
        "type": "Document"
      },
      {
        "id": "277f9efa-9e4e-4ecf-b30f-0957f14bc345",
        "metadata": {
          "chunk_id": 8,
          "document_id": "2004.03034",
          "start_idx": 3138,
          "end_idx": 3668
        },
        "page_content": ",000 claims that are part of a diverse collection of arguments on 741 controversial topics. The structure of the website dictates that each argument must present a supporting or opposing claim for its parent claim, and stay within the topic of the main thesis. Rather than being posts on a social media platform, these are community-curated claims. Furthermore, for each presented claim, the audience votes on its impact within the given line of reasoning. Critically then, the dataset includes the argument context for each claim",
        "type": "Document"
      },
      {
        "id": "2e6061ec-9257-4930-aec3-a5b0802274ef",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1708.05873",
          "start_idx": 8352,
          "end_idx": 8830
        },
        "page_content": "solution', `effort', `general', etc. Topic 9 - War. This topic directly relates to issues of war. The key words appear to be linked to discussions around ongoing wars. Topic 10 - Conflict in the Middle East. This topic clearly picks up issues related to the Middle East \u2013 particularly around peace and conflict in the Middle East. Topic 11 - Latin America. This is another topic with a regional focus, picking up on issues related to Latin America. Topic 12 - Commonwealth. This",
        "type": "Document"
      },
      {
        "id": "d12af3f2-8df8-4523-9ec2-a04b84ccab27",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1710.01492",
          "start_idx": 12206,
          "end_idx": 12623
        },
        "page_content": " them. A topic can be anything that people express opinions about, e.g., a product (e.g., iPhone6), a political candidate (e.g., Donald Trump), a policy (e.g., Obamacare), an event (e.g., Brexit), etc. For example, in (6) the author is positive about Donald Trump but negative about Hillary Clinton. A political analyzer would not be interested so much in the overall sentiment expressed in the tweet (even though one",
        "type": "Document"
      },
      {
        "id": "09734045-4143-4791-ae2e-4ef7a28f81c5",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1708.05873",
          "start_idx": 5623,
          "end_idx": 6152
        },
        "page_content": " positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence. The topic quality is usually evaluated by highest probability words, which is presented in Figure FIGREF4 . Figure FIGREF4 provides a list of the main topics (and the highest probability words associated these topics) that emerge from the STM of UN General Debate statements. In addition to the highest probability words, we use several other measures of key words (not presented here) to interpret the dimensions.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much, the proposed method improves BiDAF and DCN on SQuAD dataset?",
    "llm_answer": "The context does not contain the answer to how much the proposed method improves BiDAF and DCN on the SQuAD dataset.  It discusses improvements to BiDAF and other models, but not in comparison to the baseline or each other in terms of a quantifiable improvement.\n",
    "context": [
      {
        "id": "e70ded19-51b8-409d-9291-9ca407115473",
        "metadata": {
          "chunk_id": 74,
          "document_id": "2001.11268",
          "start_idx": 29425,
          "end_idx": 29920
        },
        "page_content": " use of original SQuAD domains to enrich our data. This was done in order to save computing resources, as an addition of 100 SQuAD domains resulted in training time increases of two hours, depending on various other parameter settings. Adjusted parameters include increased batch size, and decreased maximal context length in order to reduce training time. With this paper we aimed to explore state-of-the-art NLP methods to advance systematic review (semi)automation. Both of the presented fine",
        "type": "Document"
      },
      {
        "id": "044096e4-d669-4717-83bf-0b459cbcabae",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1803.09230",
          "start_idx": 12120,
          "end_idx": 12560
        },
        "page_content": "act match) computed on the dev set. The effect of character embedding on the BiDAF model is reported in Table 1 . We can notice that character embedding boosts up the performance by roughly 2% for both EM and F1 score. This is expected since character embedding can help deal with non-dictionary words by giving them a unique embedding. Next, we report the results of the model performances for baseline, BiDAF, Co-Attention, Hybrid and DCA",
        "type": "Document"
      },
      {
        "id": "f3a221b9-4d3d-4b00-b781-b975fa848801",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1803.09230",
          "start_idx": 12850,
          "end_idx": 13315
        },
        "page_content": " the results. Co-Attention with char embedding was giving us worse results so we put the best numbers we got for Co-Attention there. We would like to point out that the BiDAF model here doesn't include BiLSTM layer as present in original paper because the BiLSTM didn't give any advantage except for slowing down the training. Selected tensorboard visualizations are also shown in Figure 3 . Visualizations demonstrate that both hybrid and DCA models perform better",
        "type": "Document"
      },
      {
        "id": "81c1e249-37cb-42b2-b992-17e974e5671c",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1804.03396",
          "start_idx": 19961,
          "end_idx": 20345
        },
        "page_content": " SQuAD dataset are all performed with the open source code from BIBREF27 . We use 100 1D filters with width 5 to construct the CharCNN in our char embedding layer. We set the hidden size $d=100$ for all the hidden states in our model. The optimizer we use is the AdaDelta optimizer BIBREF45 with an initial learning rate of 2. A dropout BIBREF46 rate of 0.2 is applied in all the CNN,",
        "type": "Document"
      },
      {
        "id": "5f60f35c-49af-411d-9dd5-a573069248ea",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1707.03764",
          "start_idx": 6168,
          "end_idx": 6712
        },
        "page_content": " out to be detrimental to performance, when compared to the simpler system that we have described in Section SECREF2 . We divide our attempts according to the different ways we attempted to enhance performance: manipulating the data itself (adding more, and changing preprocessing), using a large variety of features, and changing strategies in modelling the problem by using different algorithms and paradigms. All reported results are on the PAN 2017 training data using five-fold cross-validation, unless otherwise specified. We extended the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the linguistic differences between each class?",
    "llm_answer": "The languages are divided into Indo-European and Sino-Tibetan families, a language isolate (Basque), and subfamilies within Indo-European (West Germanic, Romance).  There are differences in syllable structures, verbal morphology, plural morpheme acquisition, and past tense acquisition across languages.  Language varieties also differ within some languages (e.g., Portuguese, Spanish).\n",
    "context": [
      {
        "id": "6bc2bb18-6a66-4628-bef7-a0e1a75c3767",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1909.13362",
          "start_idx": 15624,
          "end_idx": 16095
        },
        "page_content": " language groupings within families. We selected six evaluation languages: English, Dutch, Italian, French, Basque, and Manipuri. These represent two language families (Indo-European, Sino-Tibetan), a language isolate thought to be unrelated to any existing language (Basque), and two different subfamilies within the Indo-European family (West Germanic, Romance). The primary constraint was the availability of syllabified datasets for training and testing. Table TABREF",
        "type": "Document"
      },
      {
        "id": "e8fb7d0e-86e1-4332-bb4a-d8109e4146e4",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1910.04269",
          "start_idx": 19553,
          "end_idx": 20023
        },
        "page_content": ", the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Kat",
        "type": "Document"
      },
      {
        "id": "d3f36301-652a-4b87-b5d5-336bdcdbf228",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1909.13362",
          "start_idx": 5671,
          "end_idx": 6191
        },
        "page_content": " each are closely related. Evaluating a model solely on these three languages, the approach taken in BIBREF13 and others, does not adequately test a model's generalized ability to learn diverse syllable structures. In this paper, we present a neural network that can syllabify phone sequences without introducing any fixed principles or rules of syllabification. We show that this novel approach to syllabification is language-agnostic by evaluating it on datasets of six languages, five from two major language families",
        "type": "Document"
      },
      {
        "id": "e1f561f8-ff33-4935-875e-125c4dba3aed",
        "metadata": {
          "chunk_id": 71,
          "document_id": "1910.05456",
          "start_idx": 25338,
          "end_idx": 25885
        },
        "page_content": "ques2011study investigated the effect of a stay abroad on Spanish L2 acquisition, including learning of its verbal morphology in English speakers. jia2003acquisition studied how Mandarin Chinese-speaking children learned the English plural morpheme. nicoladis2012young studied the English past tense acquisition in Chinese\u2013English and French\u2013English bilingual children. They found that, while both groups showed similar production accuracy, they differed slightly in the type of errors they made. Also considering the effect of the native language",
        "type": "Document"
      },
      {
        "id": "81a542f2-6b67-4a5a-b10e-0978ed74224f",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1707.03764",
          "start_idx": 794,
          "end_idx": 1328
        },
        "page_content": " and language variety from their tweets in four different languages: English, Spanish, Arabic, and Portuguese. Gender consists of a binary classification (male/female), whereas language variety differs per language, from 2 varieties for Portuguese (Brazilian and Portugal) to 7 varieties for Spanish (Argentina, Chile, Colombia, Mexico, Peru, Spain, Venezuela). The challenge is thus to classify users along two very different axes, and in four highly different languages \u2013 forcing participants to either build models that can capture",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what genres do they songs fall under?",
    "llm_answer": "Blues, Rap, Metal, Folk, R&B, Reggae, Country, and Religious.\n",
    "context": [
      {
        "id": "f285b6e6-9a7a-44e9-968a-e21d5b38cb55",
        "metadata": {
          "chunk_id": 33,
          "document_id": "2003.05377",
          "start_idx": 11875,
          "end_idx": 12353
        },
        "page_content": ". In some of the songs, songwriters try to address religious education, which could explain the confusion between those genres. The MPB (Brazilian Popular Music) genre was the most confused of all, which may indicate that song lyrics of this genre cover a wide range of subjects that intersect with other genres. In this work we constructed a dataset of $138,368$ Brazilian song lyrics distributed in 14 genres. We applied SVM, Random Forest, and a Bidirectional Long Short-Term",
        "type": "Document"
      },
      {
        "id": "788d0536-35ca-49e6-99d2-dfce03d0cc9b",
        "metadata": {
          "chunk_id": 7,
          "document_id": "2003.05377",
          "start_idx": 2865,
          "end_idx": 3346
        },
        "page_content": " performance gains on three different classification tasks: detecting genre, distinguishing the best and the worst songs, and determining the approximate publication time of a song. The experiments in genre classification focused on eight genres: Blues, Rap, Metal, Folk, R&B, Reggae, Country, and Religious. Only lyrics in English were included and they used an SVM with the default settings for the classification. Ying et al. BIBREF0 used Part-of-Speech (POS) features extracted",
        "type": "Document"
      },
      {
        "id": "69ffd625-139c-4ce3-8300-2f00adcc29ab",
        "metadata": {
          "chunk_id": 1,
          "document_id": "2003.05377",
          "start_idx": 511,
          "end_idx": 1044
        },
        "page_content": " their music collections BIBREF1. Automatic music genre classification based only on the lyrics is considered a challenging task in the field of Natural Language Processing (NLP). Music genres remain a poorly defined concept, and boundaries between genres still remain fuzzy, which makes the automatic classification problem a nontrivial task BIBREF1. Traditional approaches in text classification have applied algorithms such as Support Vector Machine (SVM) and Na\u00efve Bayes, combined with handcraft features (POS and chunk tags) and",
        "type": "Document"
      },
      {
        "id": "ea5957d1-2d20-4299-815f-22d8a0e06728",
        "metadata": {
          "chunk_id": 34,
          "document_id": "2003.05377",
          "start_idx": 12170,
          "end_idx": 12670
        },
        "page_content": " 14 genres. We applied SVM, Random Forest, and a Bidirectional Long Short-Term Memory (BLSTM) network combined with different word embeddings techniques to address the automatic genre classification task based only on the song lyrics. We compared the results between the different combinations of classifiers and word embedding techniques, concluding that our BLSTM combined with the Wang2Vec pre-trained model obtained the best F1-score classification result. Beside the dataset construction and the",
        "type": "Document"
      },
      {
        "id": "6b333bc7-dc5e-4730-9cbb-ba81d4697d89",
        "metadata": {
          "chunk_id": 0,
          "document_id": "2003.05377",
          "start_idx": 0,
          "end_idx": 511
        },
        "page_content": "Music is part of the day-to-day life of a huge number of people, and many works try to understand the best way to classify, recommend, and identify similarities between songs. Among the tasks that involve music classification, genre classification has been studied widely in recent years BIBREF0 since musical genres are the main top-level descriptors used by music dealers and librarians to organize their music collections BIBREF1. Automatic music genre classification based only on the lyrics is considered a",
        "type": "Document"
      }
    ]
  },
  {
    "query": "To what other competitive baselines is this approach compared?",
    "llm_answer": "Span models.\n",
    "context": [
      {
        "id": "7932e4c8-cb41-4b85-9fb3-5219b7ff790d",
        "metadata": {
          "chunk_id": 70,
          "document_id": "1804.03396",
          "start_idx": 21539,
          "end_idx": 22032
        },
        "page_content": " model on SQuAD dataset. Our model outperforms the previous sequence model but is not competitive with span models because it is designed to produce sequence answers in IE settings while baseline span models are designed to produce span answers for SQuAD dataset. The comparison between our QA model and two baseline QA models on our QA4IE benchmark is shown in Table 4 . For training of both baseline QA models, we use the same configuration of max input length as our model and tune the rest",
        "type": "Document"
      },
      {
        "id": "d7a0b7aa-da14-4112-80c0-cc26c62fe83f",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "5f5a488a-b689-46d4-b32b-50ea0883fe6f",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1703.09684",
          "start_idx": 2106,
          "end_idx": 2596
        },
        "page_content": " evaluate both baseline and state-of-the-art VQA algorithms. We found that our proposed approach enables more nuanced comparisons of VQA algorithms, and helps us understand the benefits of specific techniques better. In addition, it also allowed us to answer several key questions about VQA algorithms, such as, `Is the generalization capacity of the algorithms hindered by the bias in the dataset?', `Does the use of spatial attention help answer specific question-types?', `How successful",
        "type": "Document"
      },
      {
        "id": "1de5b118-edc0-495d-b24e-b130fd21dd29",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1910.08210",
          "start_idx": 16635,
          "end_idx": 17105
        },
        "page_content": " We compare to baselines and ablated variants on a simplified variant of in which there are one-to-one group assignments (no group), stationary monsters (no dyna), and no natural language templated descriptions (no nl). Figure FIGREF29 shows that compared to baselines and ablated variants, is more sample efficient and converges to higher performance. Moreover, no ablated variant is able to solve the tasks\u2014it is the combination of ablated features that enables to win",
        "type": "Document"
      },
      {
        "id": "22d31a7e-1e4f-46a6-b434-b68bbb6814bf",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1812.07023",
          "start_idx": 17433,
          "end_idx": 17962
        },
        "page_content": " set have been provided by the challenge organizers. For the prototype test set and for the ablation study presented in Table TABREF24 , we use the same code for evaluation metrics as used by BIBREF11 for fairness and comparability. We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below: Our primary architectural differences over the baseline model are: not concatenating the question, answer pairs before encoding them, the auxiliary decoder module,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is human evaluation performed, what was the criteria?",
    "llm_answer": "Human evaluators, divided into expert (15 Chinese literature bachelors) and amateur (15 other field bachelors) groups, graded generated poems from four perspectives: Fluency (grammatical and rhythmic well-formedness), Semantic coherence, Accuracy (fidelity to source), and Appropriateness (suitability to the context).  Evaluators were native Chinese speakers.\n",
    "context": [
      {
        "id": "2f087fa2-1aed-415a-a42c-95e05656d14d",
        "metadata": {
          "chunk_id": 4,
          "document_id": "2004.01694",
          "start_idx": 1628,
          "end_idx": 2197
        },
        "page_content": " changes in the evaluation design affect the outcome of the human evaluation. We find that for all three aspects, human translations are judged more favourably, and significantly better than MT, when we make changes that we believe strengthen the evaluation design. Based on our empirical findings, we formulate a set of recommendations for human MT evaluation in general, and assessing human\u2013machine parity in particular. All of our data are made publicly available for external validation and further analysis. We first review current methods to assess the quality of",
        "type": "Document"
      },
      {
        "id": "efc3bcef-ff25-486a-ad34-fc2f8998a97d",
        "metadata": {
          "chunk_id": 41,
          "document_id": "1909.00279",
          "start_idx": 15265,
          "end_idx": 15755
        },
        "page_content": "ators to join our human evaluation. The human evaluators were divided into two groups. The expert group contains 15 people who hold a bachelor degree in Chinese literature, and the amateur group contains 15 people who holds a bachelor degree in other fields. All 30 human evaluators are native Chinese speakers. We ask evaluators to grade each generated poem from four perspectives: 1) Fluency: Is the generated poem grammatically and rhythmically well formed, 2) Semantic coherence: Is the",
        "type": "Document"
      },
      {
        "id": "ed674135-c713-41cc-915c-42336218eb34",
        "metadata": {
          "chunk_id": 91,
          "document_id": "2004.01694",
          "start_idx": 35319,
          "end_idx": 35678
        },
        "page_content": " human evaluation of MT in general. Our recommendations have the aim of increasing the validity of MT evaluation, but we are aware of the high cost of having MT evaluation done by professional translators, and on the level of full documents. We welcome future research into alternative evaluation protocols that can demonstrate their validity at a lower cost.",
        "type": "Document"
      },
      {
        "id": "748fb5f2-01c6-4839-9bc8-6212bf55e7d1",
        "metadata": {
          "chunk_id": 3,
          "document_id": "2004.01694",
          "start_idx": 1337,
          "end_idx": 1854
        },
        "page_content": " concur that these findings have demonstrated the need to critically re-evaluate the design of human MT evaluation. Our paper investigates three aspects of human MT evaluation, with a special focus on assessing human\u2013machine parity: the choice of raters, the use of linguistic context, and the creation of reference translations. We focus on the data shared by BIBREF3, and empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation. We find that for all three aspects",
        "type": "Document"
      },
      {
        "id": "2d3984d1-024e-4608-a5c4-4823d3344fdc",
        "metadata": {
          "chunk_id": 85,
          "document_id": "2004.01694",
          "start_idx": 32633,
          "end_idx": 33097
        },
        "page_content": " a small number of human raters: five in the experiments of Sections SECREF3 and SECREF35, four per condition (adequacy and fluency) in Section SECREF4, and one in the fine-grained error analysis presented in Section SECREF24. Moreover, the results presented in this article are based on one text domain (news) and one language direction (Chinese to English), and while a large-scale evaluation with another language pair supports our findings (see above), further",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much better were results of the proposed models than base LSTM-RNN model?",
    "llm_answer": "The context states that LSTM models achieve similar test perplexity to previous work, but does not directly compare proposed models to a base LSTM-RNN model.  Therefore, the answer is not found in the context.\n",
    "context": [
      {
        "id": "d234fb0a-22fe-47d0-942b-072390bdb613",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1906.00180",
          "start_idx": 3251,
          "end_idx": 3690
        },
        "page_content": "IBREF27 and LSTM BIBREF28 . Training and testing accuracies after 50 training epochs, averaged over five different model runs, are shown in Table UID18 . All recurrent models outperform the summing baseline. Even the simplest recurrent network, the SRN, achieves higher training and testing accuracy scores than the tree-shaped matrix model. The GRU and LSTM even beat the tensor model. The LSTM obtains slightly lower scores than the GRU,",
        "type": "Document"
      },
      {
        "id": "6d01045a-d841-4321-8d95-32c31a189faa",
        "metadata": {
          "chunk_id": 81,
          "document_id": "2002.02492",
          "start_idx": 25396,
          "end_idx": 25780
        },
        "page_content": " RNNs implies that all of the recurrent language models that we train are consistent according to Lemma UNKREF23. Our LSTM models achieve similar test perplexity ($91.86 \\pm 0.4$) to those reported in previous work BIBREF24; see Appendix for further details. Additionally, we train self-terminating $\\tanh $-RNN and LSTM-RNN variants (Definition UNKREF33) at various values of $\\epsil",
        "type": "Document"
      },
      {
        "id": "f1a4b018-7b8c-478b-9ba0-3b72fdb7a4db",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1910.12618",
          "start_idx": 26085,
          "end_idx": 26516
        },
        "page_content": " of the NNs, with the embedding RNN edging the MLP TF-IDF one. However for the UK data set the RFs yield significantly better results on the test set than the NNs. This inversion of performance of the algorithms is possibly due to a change in the way the reports were written by the Met Office after August 2017, since the results of the MLP and RNN on the validation set (not shown here) were satisfactory and better than both RFs",
        "type": "Document"
      },
      {
        "id": "b52ace5f-2772-4963-912b-794ce33f4b7e",
        "metadata": {
          "chunk_id": 16,
          "document_id": "2001.05467",
          "start_idx": 6404,
          "end_idx": 6856
        },
        "page_content": " only feeding ground-truth tokens); the other is to let the model use its own predictions during greedy/beam-search decoding or sampling. We reason that the former is a much better estimation of the model's bias, because the latter will result in a cascading enlargement of the model bias due to the auto-regressive nature of LSTM-RNN models (i.e., the tokens fed to the decoder are themselves also polluted by the model's bias). Our early experimental",
        "type": "Document"
      },
      {
        "id": "36dc5e74-3ea0-4b03-9b0b-f67f735f5ffe",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1902.09314",
          "start_idx": 16362,
          "end_idx": 16777
        },
        "page_content": " is a lightweight alternative of recurrent models, we study the model size of each model on the Restaurant dataset. Statistical results are reported in Table TABREF37 . We implement all the compared models base on the same source code infrastructure, use the same hyperparameters, and run them on the same GPU . RNN-based and BERT-based models indeed have larger model size. ATAE-LSTM, IAN, RAM, and AEN-GloVe-BiLST",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which one of the four proposed models performed best?",
    "llm_answer": "\"Ours with Mask and Ordered Triplets\"\n",
    "context": [
      {
        "id": "38d0d5d3-4be2-4a08-bc2e-e9f401074e5d",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1701.06538",
          "start_idx": 17346,
          "end_idx": 17826
        },
        "page_content": ". Details can be found in Appendix UID77 . Results of these three models form the bottom line of Figure FIGREF32 -right. Table TABREF33 compares the results of these models to the best previously-published result on this dataset . Even the fastest of these models beats the best published result (when controlling for the number of training epochs), despite requiring only 6% of the computation. We trained our models using TensorFlow BIBREF30 on clusters containing 16-32 Tesla K",
        "type": "Document"
      },
      {
        "id": "d7a0b7aa-da14-4112-80c0-cc26c62fe83f",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "4e04ab32-f047-4b22-8b03-4e6977b298f9",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1901.02257",
          "start_idx": 18849,
          "end_idx": 19405
        },
        "page_content": " Our ensemble model surpasses the current state-of-the-art model with an accuracy of 84.84%. We got the final ensemble result by voting on 4 single models. Every single model uses the same architecture but different parameters. To study the effectiveness of each perspective, we conduct several experiments on the three single perspectives and their combination perspective. Table 3 presents their comparison results. The first group of models are based on the three single perspectives, and we can observe that the union perspective performs best compared",
        "type": "Document"
      },
      {
        "id": "3b61fd1a-0d50-4af7-bc74-5de4e141f81c",
        "metadata": {
          "chunk_id": 89,
          "document_id": "1905.06566",
          "start_idx": 24955,
          "end_idx": 25353
        },
        "page_content": " We also conducted human experiment with 20 randomly sampled documents from the CNNDM test set. We compared our model $\\text{\\sc Hibert}_M$ against Lead3, DCA, Latent, BERT and the human reference (Human). We asked the subjects to rank the outputs of these systems from best to worst. As shown in Table 4 , the output of $\\text{\\sc Hibert}_M$ is selected as the best in 30% of cases and we obtained",
        "type": "Document"
      },
      {
        "id": "13007813-fa11-43ad-84af-54b729317817",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1909.13362",
          "start_idx": 21575,
          "end_idx": 22056
        },
        "page_content": " model, which replaces the CRF output of the Base model with a softmax. A comparison of the results of these three models can be seen in Table TABREF25. This comparison empirically motivates the CRF output because Base almost always outperforms Base-Softmax. Of these three models, the Base model performed the best with the exception of the French and Manipuri datasets. The differences in the French results can be considered negligible because the accuracies are all near $100\\%",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much is proposed model better than baselines in performed experiments?",
    "llm_answer": "The proposed model outperforms baselines by a clear margin, with increases in EM and GM scores.  For example, the difference between the proposed model and the second best model is INLINEFORM0 EM and GM.  There is also an over INLINEFORM0 increase in EM and GM between the proposed model and the next best two models.\n",
    "context": [
      {
        "id": "fc23cf7f-21ff-43cd-8d00-ef5914322400",
        "metadata": {
          "chunk_id": 68,
          "document_id": "1810.00663",
          "start_idx": 26280,
          "end_idx": 26763
        },
        "page_content": " room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM. Note that the average number of actions in the ground truth output sequences is 7.07 for the Test-New set. Our model's predictions are just INLINEFORM1 edits off on average from the correct navigation plans. This",
        "type": "Document"
      },
      {
        "id": "5e5c6963-88aa-4a86-96ff-323aeb271bfb",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1810.00663",
          "start_idx": 25415,
          "end_idx": 25983
        },
        "page_content": " In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models. The previous section evaluated model performance on new instructions (and corresponding navigation plans) for environments that were previously seen at training time. Here, we examine whether the trained models succeed on environments that are completely new. The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as",
        "type": "Document"
      },
      {
        "id": "abc51b19-3db2-4cbf-ac46-ff97aa2fa1d1",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1603.07044",
          "start_idx": 14997,
          "end_idx": 15515
        },
        "page_content": " task A already had a substantial amount of training data. Table 4 gives the final comparison between different models (we only list the MAP score because it is the official score for the challenge). Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited",
        "type": "Document"
      },
      {
        "id": "d7a0b7aa-da14-4112-80c0-cc26c62fe83f",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "4d522ea5-86b3-42df-acaa-37f028f2bec5",
        "metadata": {
          "chunk_id": 138,
          "document_id": "2001.11381",
          "start_idx": 31860,
          "end_idx": 32196
        },
        "page_content": "ados. Dicho lo anterior, el Modelo 1 podr\u00eda ser considerado como nuestro propio baseline. A continuaci\u00f3n presentamos un protocolo de evaluaci\u00f3n manual de los resultados obtenidos. El experimento consisti\u00f3 en la generaci\u00f3n de 15 frases por cada uno de los tres modelos propuestos. Para cada modelo, se consideraron tres queries: $Q=$ {AM",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much is precission increased?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "9e5b0bbb-7fff-40fc-a541-c395ca9850cc",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1909.13375",
          "start_idx": 22494,
          "end_idx": 22947
        },
        "page_content": " writing, and not just on multi-span questions. In order to analyze the effect of each of our changes, we conduct ablation studies on the development set, depicted in Table TABREF26. Not using the simple preprocessing from Section SECREF17 resulted in a 2.5 point decrease in both EM and F1. The numeric questions were the most affected, with their performance dropping by 3.5 points. Given that number questions make up about 61% of the dataset, we can",
        "type": "Document"
      },
      {
        "id": "487566c1-6b22-466e-8847-fa293a6f80e8",
        "metadata": {
          "chunk_id": 83,
          "document_id": "1905.06566",
          "start_idx": 23289,
          "end_idx": 23633
        },
        "page_content": " by 0.5 ROUGE. We also implemented two baselines. One is the hierarchical transformer summarization model (HeriTransfomer; described in \"Extractive Summarization\" ) without pre-training. Note the setting for HeriTransfomer is ( $L=4$ , $H=300$ and $A=4$ ) . We can see that the pre-training (details in Section \"Pre-training\" ) leads to a +1.25",
        "type": "Document"
      },
      {
        "id": "f16e7f59-b65d-45bb-a64e-f30bc1a210c8",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1904.05862",
          "start_idx": 13711,
          "end_idx": 14238
        },
        "page_content": " increases. We suspect that this is because the training signal from the positive samples decreases as the number of negative samples increases. In this experiment, everything is kept equal except for the number of negative samples. Next, we analyze the effect of data augmentation through cropping audio sequences (\u00a7 SECREF11 ). When creating batches we crop sequences to a pre-defined maximum length. Table shows that a crop size of 150K frames results in the best performance. Not restricting the maximum length (None) gives",
        "type": "Document"
      },
      {
        "id": "cf7959be-bae2-46af-8950-08880cdb76b1",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1708.05873",
          "start_idx": 14464,
          "end_idx": 15020
        },
        "page_content": " largest populations. However, with Topic 7 there is an increase in expected topic proportion until countries have a population of around 300 million, after which there is a decline in discussion of Topic 7. For countries with populations larger than 500 million there is no effect of population on discussion of Topic 7. It is only with the very largest populations that we see a positive effect on discussion of Topic 7. We would also expect the extent to which states discuss international development in their GD statements to be impacted by the amount",
        "type": "Document"
      },
      {
        "id": "b744e65c-657e-4027-a1ce-6f063f2f2d46",
        "metadata": {
          "chunk_id": 67,
          "document_id": "1909.13375",
          "start_idx": 22947,
          "end_idx": 23384
        },
        "page_content": " 3.5 points. Given that number questions make up about 61% of the dataset, we can deduce that our improved number handling is responsible for about a 2.1 point gain, while the rest could be be attributed to the improved Wikipedia parsing. Although NER span cleaning (Section SECREF23) affected only 3% of the multi-span questions, it provided a solid improvement of 5.4 EM in multi-span questions and 1.5 EM in single-span questions. The",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What labels are in the dataset?",
    "llm_answer": "Location (LOC), Person (PER), Organization (ORG), Miscellaneous (TABREF15); Non-Aggressive (NAG), Overtly-Aggressive (OAG), Covertly-Aggressive (CAG); seven ordered levels of likelihood of human trafficking; seven classes based on structured headings (PubMed data).\n",
    "context": [
      {
        "id": "a7dd4b83-a423-4210-8cf5-a2555b3488d8",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1911.07228",
          "start_idx": 6150,
          "end_idx": 6611
        },
        "page_content": " corpus which are provided by VLSP 2016 - Named Entity Recognition. The dataset contains four different types of label: Location (LOC), Person (PER), Organization (ORG) and Miscellaneous - Name of an entity that do not belong to 3 types above (Table TABREF15). Although the corpus has more information about the POS and chunks, but we do not use them as features in our model. There are two folders with 267 text files of training data and 45 text files of test",
        "type": "Document"
      },
      {
        "id": "ef462452-43cb-4d23-a1e2-a85f202fea71",
        "metadata": {
          "chunk_id": 56,
          "document_id": "1907.11499",
          "start_idx": 22325,
          "end_idx": 22841
        },
        "page_content": " from bag labels, small amounts of instance labels are also available BIBREF23 . It would also be interesting to examine how the label space influences model performance, especially since in our scenario the labels are extrapolated from Wikipedia and might be naturally noisy and/or ambiguous. The authors would like to thank the anonymous reviewers and the action editor, Yusuke Miyao, for their valuable feedback. We acknowledge the financial support of the European Research Council (Lapata; award number 681760).",
        "type": "Document"
      },
      {
        "id": "3c55c14c-3d54-4eb0-8d28-bb34a1f1f37b",
        "metadata": {
          "chunk_id": 63,
          "document_id": "2001.05493",
          "start_idx": 21322,
          "end_idx": 21715
        },
        "page_content": " used the English code-mixed dataset provided by TRAC 2018. This dataset contains three labels, (a) Non-Aggressive(NAG), (b) Overtly-Aggressive (OAG) and (c) Covertly-Aggressive(CAG). The distribution of training, validation and test sets are described in Table TABREF56. Kaggle Dataset: This dataset contains 20001 tweets which are manually labeled. The labels are divided into two categories",
        "type": "Document"
      },
      {
        "id": "c20dcd0b-96f3-4385-bd83-c121b0e6449b",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1908.05434",
          "start_idx": 14746,
          "end_idx": 15226
        },
        "page_content": " the current raw dataset is sufficiently large. The labeled dataset is called Trafficking-10k. It consists of 12,350 ads from Backpage labeled by experts in human trafficking detection BIBREF9 . Each label is one of seven ordered levels of likelihood that the corresponding ad comes from a human trafficker. Descriptions and sample proportions of the labels are in Table TABREF11 . The original Trafficking-10K includes both texts and images, but as mentioned in Section SECREF1 ,",
        "type": "Document"
      },
      {
        "id": "c2e915fb-365a-4919-8b62-f55617bee57b",
        "metadata": {
          "chunk_id": 12,
          "document_id": "2001.11268",
          "start_idx": 4947,
          "end_idx": 5446
        },
        "page_content": " associated with labelling, its authors acquired sentence labels via automated annotation. Seven classes were assigned on the basis of structured headings within the text of each abstract. Table TABREF4 provides an overview of class abbreviations and their meaning.In the following we refer to it as the PubMed data. The LSTM itself yields impressive results with F1 scores for annotation of up to 0.85 for PIO elements, it generalizes across domains and assigns one label per sentence. We were able",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are customer satisfaction, customer frustration and overall problem resolution data collected?",
    "llm_answer": "The data consists of 216 satisfied and 500 unsatisfied customer conversations, 271 resolved and 425 unresolved problem conversations, and 534 frustrated and 229 not frustrated customer conversations.\n",
    "context": [
      {
        "id": "b2176739-9934-490e-8c54-8eb1f13543c5",
        "metadata": {
          "chunk_id": 98,
          "document_id": "1709.05413",
          "start_idx": 39019,
          "end_idx": 39532
        },
        "page_content": " that are satisfied/not frustrated and resolved problems) are shown at the top rows of the table, and undesirable outcomes (unsatisfied/frustrated customers and unresolved problems) are shown at the bottom rows. Our analysis helps zone in on how the use of certain dialogue acts may be likely to result in different outcomes. The weights we observe vary in the amount of insight provided: for example, offering extra help at the end of a conversation, or thanking the customer yields more satisfied customers, and",
        "type": "Document"
      },
      {
        "id": "d7dbea21-e54a-4db3-9f98-a67fe9d0353f",
        "metadata": {
          "chunk_id": 88,
          "document_id": "1709.05413",
          "start_idx": 34849,
          "end_idx": 35438
        },
        "page_content": " receive majority consensus for a label, or received majority vote of \"can't tell\". Our final conversation sets consist of 216 satisfied and 500 unsatisfied customer conversations, 271 resolved and 425 unresolved problem conversations, and 534 frustrated and 229 not frustrated customer conversations. We retain the inherent imbalance in the data to match the natural distribution observed. The clear excess of consensus of responses that indicate negative outcomes further motivates us to understand what sorts of dialogic patterns results in such outcomes. We run the experiment for each",
        "type": "Document"
      },
      {
        "id": "b09c3d6e-f4f7-41d2-a0d0-4f1b60755dfb",
        "metadata": {
          "chunk_id": 100,
          "document_id": "1709.05413",
          "start_idx": 39812,
          "end_idx": 40354
        },
        "page_content": " strong association with unsatisfied customers. Giving elaborate answers that are not a simple affirmative, negative, or response acknowledgement (i.e. Answer (Other)) towards the middle of a conversation leads to satisfied customers that are not frustrated. Likewise, requesting information towards the end of a conversation (implying that more information is still necessary at the termination of the dialogue) leads to unsatisfied and unresolved customers, with ratios of at least 4:1. By using the feature weights we derive from using our",
        "type": "Document"
      },
      {
        "id": "ee82fac2-22b3-46d9-9294-3f069e13801f",
        "metadata": {
          "chunk_id": 99,
          "document_id": "1709.05413",
          "start_idx": 39532,
          "end_idx": 40044
        },
        "page_content": " offering extra help at the end of a conversation, or thanking the customer yields more satisfied customers, and more resolved problems (with ratios of above 6:1). However, some outcomes are much more subtle: for example, asking yes-no questions early-on in a conversation is highly associated with problem resolution (ratio 3:1), but asking them at the end of a conversation has as similarly strong association with unsatisfied customers. Giving elaborate answers that are not a simple affirmative, negative, or",
        "type": "Document"
      },
      {
        "id": "f25408d8-0967-4344-a413-0f7c8f320aee",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1911.00133",
          "start_idx": 9726,
          "end_idx": 10257
        },
        "page_content": " perfect agreement on 39% of the data, and for another 32% the majority was 3/5 or less. This suggests that our data displays significant variation in how stress is expressed, which we explore in the next section. While all our data has the same genre and personal narrative style, we find distinctions among domains with which classification systems must contend in order to perform well, and distinctions between stressful and non-stressful data which may be useful when developing such systems. Posters in each subreddit express",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many improvements on the French-German translation benchmark?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "74d41b8d-63d7-479f-a82f-76142d721807",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1911.03894",
          "start_idx": 15623,
          "end_idx": 16121
        },
        "page_content": " is also machine translated for all languages. The dataset is composed of 122k train, 2490 valid and 5010 test examples. As usual, NLI performance is evaluated using accuracy. To evaluate a model on a language other than English (such as French), we consider the two following settings: TRANSLATE-TEST: The French test set is machine translated into English, and then used with an English classification model. This setting provides a reasonable, although imperfect, way to circumvent the fact that",
        "type": "Document"
      },
      {
        "id": "b6aa7266-b83f-4fe9-ac46-0693324540a1",
        "metadata": {
          "chunk_id": 30,
          "document_id": "1910.07481",
          "start_idx": 11703,
          "end_idx": 12134
        },
        "page_content": " best results, up to 0.85 BLEU points over the Baseline on the newstest2019 corpus. It also surpassed the Baselinee by 0.18 points on the newstest2017 with strong statistical significance, and by 0.15 BLEU points on the newstest2018 but this time with no statistical evidence. These encouraging results prompted us to extend experiments to another language pair: English-French. En$\\rightarrow $Fr: The Document system obtained the",
        "type": "Document"
      },
      {
        "id": "c40316f9-9131-44d3-bdfe-eb51fdc2e6c7",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1809.04686",
          "start_idx": 7910,
          "end_idx": 8389
        },
        "page_content": " does not provide a multilingual test set, we used the public translation engine Google Translate to translate the SST test set to French. Previous work by BIBREF40 has shown that replacing the human translated test set with a synthetic set (obtained by using Google Translate) produces only a small difference of around 1% absolute accuracy on their human-translated French SNLI test set. Therefore, the performance measured on our `pseudo' French SST test set is expected to be",
        "type": "Document"
      },
      {
        "id": "279617fc-59de-45c8-a31a-e920d05a0421",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1910.07481",
          "start_idx": 12134,
          "end_idx": 12501
        },
        "page_content": " language pair: English-French. En$\\rightarrow $Fr: The Document system obtained the best results considering all metrics on all test sets with strong statistical evidence. It surpassed the Baseline by 1.09 BLEU points and 0.85 TER points on tst2015, 0.75 BLEU points and 0.76 TER points on tst2014, and 0.48 BLEU points and 0.68 TER points on tst2013. Fr$\\rightarrow",
        "type": "Document"
      },
      {
        "id": "11e67a16-651d-4fa5-a286-a2507dc42e62",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1910.07481",
          "start_idx": 11497,
          "end_idx": 11877
        },
        "page_content": " Table TABREF7 contains results for both English to French and French to English translation tasks, models are evaluated on the tst2013, tst2014 and tst2015 test sets. En$\\rightarrow $De: The Baseline model obtained State-of-The-Art BLEU and TER results according to BIBREF23, BIBREF24. The Document system shows best results, up to 0.85 BLEU points over the Baseline on the newst",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they prevent the model complexity increasing with the increased number of slots?",
    "llm_answer": "They propose the Slot-Independent Model (SIM) which does not increase in complexity as the number of slots increases.\n",
    "context": [
      {
        "id": "dfbc38dc-020e-4230-8221-9e470ca343a9",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1909.00754",
          "start_idx": 16354,
          "end_idx": 16636
        },
        "page_content": "wise},\\\\\n\\end{array}\\right.\n\n$  where $O(x)$ means the Inference Time Complexity (ITC) of the variable $x$ . For a model having an ITC of $O(1)$ with respect to the number of slots $n$ , and values $m$ , the ITM will be a multiplier of 2.15x, while for an ITC of $O(n)$ , it will be",
        "type": "Document"
      },
      {
        "id": "9c850b2e-e9cf-41f2-b2c3-3de5f5b5468e",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1909.11833",
          "start_idx": 2381,
          "end_idx": 2917
        },
        "page_content": " of slot-specific modules. To this end, we propose the Slot-Independent Model (SIM). Our model complexity does not increase when the number of slots in dialogue tasks go up. Thus, SIM has many fewer parameters than existing dialogue state tracking models. To compensate for the exclusion of slot-specific parameters, we incorporate better feature representation of user utterance and dialogue states using syntactic information and convolutional neural networks (CNN). The refined representation, in addition to cross and self-attention",
        "type": "Document"
      },
      {
        "id": "290cbcdc-a2a7-4545-b00e-c4e5a1ed651c",
        "metadata": {
          "chunk_id": 56,
          "document_id": "1909.00754",
          "start_idx": 15551,
          "end_idx": 15976
        },
        "page_content": " dataset, MultiWoZ BIBREF9 . It has a more complex ontology with 7 domains and 25 predefined slots. Since the combined slot-value pairs representation of the belief states has to be applied for the model with $O(n)$ ITC, the total number of slots is 35. The statistics of these two datsets are shown in Table 2 . Based on the statistics from these two datasets, we can calculate the theoretical Inference Time Multiplier (ITM",
        "type": "Document"
      },
      {
        "id": "35f69dd2-7c57-4a7c-810e-a32701796b29",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1911.02711",
          "start_idx": 15887,
          "end_idx": 16402
        },
        "page_content": " results compared to a hidden size of 256 either. We see an evident improvement of our model when the hidden size increases from 128 to 256. However, the improvement becomes relatively small compared to a large increase in the number of parameters when the hidden size is further increased to 360. Therefore, we adopt 256 as the hidden size in our experiments. As Table TABREF29 shows, the accuracy increases when increasing layer numbers from 1 to 2. More layers do not increase the accuracy on development set. We",
        "type": "Document"
      },
      {
        "id": "9ddc2c2d-d7c5-45fb-b840-8298af67b7d9",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1806.00722",
          "start_idx": 13866,
          "end_idx": 14348
        },
        "page_content": ", we also construct a series of 8-layer models. We set the hidden number to be 192, such that both 4-layer models and 8-layer models have similar number of parameters. For dense structured models, we set the dimension of hidden states to be 96. Since NMT model usually allocates a large proportion of its parameters to the source/target sentence embedding and softmax matrix, we explore in our experiments to what extent decreasing the dimensions of the three parts would harm the B",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they measure model size?",
    "llm_answer": "The context does not contain information about how model size is measured.\n",
    "context": [
      {
        "id": "b99f9b51-787d-423c-8c49-bf72943f2462",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1701.02877",
          "start_idx": 9957,
          "end_idx": 10406
        },
        "page_content": " training and testing data sizes measured in number of NEs are listed in Table 2 . Separate models are then trained on the training parts of each corpus and evaluated on the development (if available) and test parts of the same corpus. If development parts are available, as they are for CoNLL (CoNLL Test A) and MUC (MUC 7 Dev), they are not merged with the training corpora for testing, as it was permitted to do in the context of those evaluation",
        "type": "Document"
      },
      {
        "id": "6ff4cc69-62fb-437c-b70d-24204028e6a2",
        "metadata": {
          "chunk_id": 54,
          "document_id": "1909.09484",
          "start_idx": 18909,
          "end_idx": 19263
        },
        "page_content": " size $d_{emb}$ is set to 300. For our proposed model, the hidden size of decoder in dialogue policy maker is 150. The vocabulary size $|V|$ is 540 for DSTC2 and 4712 for Maluuba. And the size of $k_t$ is set to 20. An Adam optimizer BIBREF22 is used for training our models and baselines, with a learning rate of 0.001 for supervised training and 0.0001",
        "type": "Document"
      },
      {
        "id": "16f56a1c-8d72-49ac-9c71-98bc034f4989",
        "metadata": {
          "chunk_id": 90,
          "document_id": "1701.06538",
          "start_idx": 33051,
          "end_idx": 33526
        },
        "page_content": " first level branching factors are 32, 32, 64, 128, 256 and 256, respectively. Models are trained on a cluster of 32 Tesla K40 GPUs, except for the last two models, which are trained on clusters of 64 and 128 GPUs so as to have enough memory for all the parameters. For all models, training batch sizes are approximately 2.5 million words. Models are trained once-through over about 100 billion words. We implement several memory optimizations in order to fit up to 1 billion",
        "type": "Document"
      },
      {
        "id": "cb26796d-fa6a-45cf-840c-0901678f1e04",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1909.00154",
          "start_idx": 24630,
          "end_idx": 25057
        },
        "page_content": " vectors) Development set: 20% of the dataset (2003 vectors) Test set: 20% of the dataset (2003 vectors) The PyLogit package BIBREF11 also uses Swissmetro as an example. Therefore, our model specifications will extend the default one from this package. We re-estimated this model with the train set and validated with testset. The results are shown in tables TABREF31 and TABREF32. Since we are comparing the models at the test",
        "type": "Document"
      },
      {
        "id": "18d485b0-b1af-4609-847c-b7449df93c05",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1910.10408",
          "start_idx": 12581,
          "end_idx": 13061
        },
        "page_content": "Enc) models are evaluated in a length matching condition, i.e. output length has to match input length. We report the relative (Rel) and absolute (Abs) strategies of the approach as discussed in Section SECREF10. In the small data condition, we additionally evaluated how the fine-tuning strategy compares with a model trained from scratch. In the large data condition, we added a setting that combines both the length-token and length-encoding strategies. To evaluate all models'",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which model architecture do they for sentence encoding?",
    "llm_answer": "BiLSTM-max with a hidden size of 600D per direction.\n",
    "context": [
      {
        "id": "d8e54048-dab1-4ece-993b-5fab9accbda8",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1810.09774",
          "start_idx": 9339,
          "end_idx": 9703
        },
        "page_content": " encoding models, cross-sentence attention models as well as fine-tuned pre-trained language models. For sentence encoding models, we chose a simple one-layer bidirectional LSTM with max pooling (BiLSTM-max) with the hidden size of 600D per direction, used e.g. in InferSent BIBREF17 , and HBMP BIBREF18 . For the other models, we have chosen ESIM BIBREF19 , which",
        "type": "Document"
      },
      {
        "id": "be443191-cfc0-4851-bcb3-01ef0116db1a",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1912.01673",
          "start_idx": 4013,
          "end_idx": 4472
        },
        "page_content": " few: BiLSTM with the max-pooling trained for natural language inference BIBREF13, masked language modeling and next sentence prediction using bidirectional Transformer BIBREF14, max-pooling last states of neural machine translation among many languages BIBREF15 or the encoder final state in attentionless neural machine translation BIBREF16. The most common way of evaluating methods of sentence embeddings is extrinsic, using so called `transfer tasks', i.",
        "type": "Document"
      },
      {
        "id": "a140cfb7-5453-470b-b63d-9bb6dd25e0af",
        "metadata": {
          "chunk_id": 30,
          "document_id": "1907.11499",
          "start_idx": 12138,
          "end_idx": 12567
        },
        "page_content": " we generate sentence labels and further assume that these apply to sentence words too. Again, we report INLINEFORM3 2prp+r INLINEFORM4 p INLINEFORM5 r INLINEFORM6  We show model performance against AMT domain labels in Table TABREF42 . Consistent with the automatic evaluation results, DetNet variants are the best performing models on the sentence-level task. On the Wikipedia datasets, DetNet INLINEFORM0 or DetNet INLINEFORM1",
        "type": "Document"
      },
      {
        "id": "76f81e16-451d-4de3-aeb7-c4518c71d023",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1908.06606",
          "start_idx": 11284,
          "end_idx": 11795
        },
        "page_content": "i$. To tell the model $Q$ and $X$ is two different sentence, a sentence type input is generated which is a binary label sequence to denote what sentence each character in the input belongs to. Positional encoding and mask matrix is also constructed automatically to bring in absolute position information and eliminate the impact of zero padding respectively. Then a hidden vector $V_s$ which contains both query and text information is generated through BERT-base model. Since BERT is trained on general corpus",
        "type": "Document"
      },
      {
        "id": "b2e05fa6-3da0-4920-8f98-e2721cc4f962",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1910.05608",
          "start_idx": 7191,
          "end_idx": 7689
        },
        "page_content": " architectures to handle data in many viewpoints. In our system, we use five different model architectures combining many types of CNN, and RNN. Each model will use some types of word embedding or handle directly sentence embedding to achieve the best general result. Source code of five models is extended from the GitHub repository The first model is TextCNN (figure FIGREF2) proposed in BIBREF11. It only contains CNN blocks following by some Dense layers. The output of multiple CNN blocks with",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which data sources do they use?",
    "llm_answer": "Online debate websites, source documents/summaries (free text), crowd sourcing, examinations designed by educational experts,  French meteorological office (M\u00e9t\u00e9o France), National Oceanic and Atmospheric Administration (NOAA) stations, Stanford Sentiment Treebank (SST-2), Yelp dataset, Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), and Quora Question Pairs.\n",
    "context": [
      {
        "id": "00ea7799-d261-4f01-b6cc-6b0dec5e75e6",
        "metadata": {
          "chunk_id": 64,
          "document_id": "1906.03538",
          "start_idx": 25144,
          "end_idx": 25628
        },
        "page_content": " US Defense Advanced Research Projects Agency (DARPA). The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. We provide brief statistics on the sources of different content in our dataset in Table TABREF46 . In particular, this table shows: the size of the data collected from online debate websites (step 1). the size of the data filtered out (step 2a). the size of the perspectives added",
        "type": "Document"
      },
      {
        "id": "fb4f7b01-7328-425d-9060-5b0bfa587873",
        "metadata": {
          "chunk_id": 75,
          "document_id": "1910.00458",
          "start_idx": 26170,
          "end_idx": 26674
        },
        "page_content": "26 are human-generated and based on source documents or summaries in free text format. However, since annotators tend to copy spans as answers BIBREF27, the majority of answers are still extractive in these datasets. The multi-choice QA datasets are collected either via crowd sourcing, or collected from examinations designed by educational experts BIBREF7. In this type of QA datasets, besides token matching, a significant portion of questions require multi-sentence reasoning and external knowledge B",
        "type": "Document"
      },
      {
        "id": "91672d1f-40de-4361-a563-5175325a46aa",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1703.09684",
          "start_idx": 2802,
          "end_idx": 3121
        },
        "page_content": " BIBREF3 , FM-IQA BIBREF4 , The VQA Dataset BIBREF1 , Visual7W BIBREF5 , and Visual Genome BIBREF6 . FM-IQA needs human judges and has not been widely used, so we do not discuss it further. Table 1 shows statistics for the other datasets. Following others BIBREF7 , BIBREF8 , BIBREF9 , we refer to the portion of The VQ",
        "type": "Document"
      },
      {
        "id": "e925a6ff-8825-4f0a-bef0-ce1426b042e5",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1910.12618",
          "start_idx": 9057,
          "end_idx": 9544
        },
        "page_content": " the French meteorological office, M\u00e9t\u00e9o France, while the British ones are scrapped from stations of the National Oceanic and Atmospheric Administration (NOAA). Available on the same time span as the consumption, they usually have a 3 hours temporal resolution but are averaged to a daily one as well. Finally the time series were scaled to the range $[0,1]$ before the training phase, and re-scaled during prediction time. Our work aims at predicting time series using exclusively text",
        "type": "Document"
      },
      {
        "id": "4a2bbc22-5a05-441f-92ce-90e5ab0a95c8",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1910.00458",
          "start_idx": 17496,
          "end_idx": 17916
        },
        "page_content": "500). For the sentiment analysis category, we used the Stanford Sentiment Treebank (SST-2) dataset from the GLUE benchmark BIBREF22 (around 60k train examples) and the Yelp dataset (around 430k train examples). For the paraphrase category, three paraphrasing datasets are used from the GLUE benchmark: Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), and Quora Question Pairs (",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How were breast cancer related posts compiled from the Twitter streaming API?",
    "llm_answer": "A primary feed searched for \"breast\" AND \"cancer\", and a secondary feed searched for \"cancer\".  A total of 48,113 tweets from these accounts were compiled.\n",
    "context": [
      {
        "id": "d9e15cf7-703f-4794-a57c-ed94d12fbeb6",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1805.09959",
          "start_idx": 2486,
          "end_idx": 3003
        },
        "page_content": "th, 2016 through December 9th, 2017. The primary feed for the analysis collected INLINEFORM0 million tweets containing the keywords `breast' AND `cancer'. See Figure FIGREF2 for detailed Twitter frequency statistics along with the user activity distribution. Our secondary feed searched just for the keyword `cancer' which served as a comparison ( INLINEFORM1 million tweets, see Appendix 1), and helped us collect additional tweets relevant to cancer from patients. The numeric account ID provided in tweets helps to",
        "type": "Document"
      },
      {
        "id": "b71ff686-a550-4f8f-b0fd-d06d552ca5fe",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1805.09959",
          "start_idx": 9408,
          "end_idx": 9892
        },
        "page_content": " 1,000 tweets that were classified as `relevant'. We uncovered three categories of immediate interest including: tweets authored by patients regarding their condition (21.6%), tweets from friends/family with a direct connection to a patient (21.9%), and survivors in remission (8.8%). We also found users posting diagnostic related inquiries (7.6%) about possible symptoms that could be linked to breast cancer, or were interested in receiving preventative check-ups. The rest (40.2%)",
        "type": "Document"
      },
      {
        "id": "4a494e57-32f1-4a06-81af-d50190b89234",
        "metadata": {
          "chunk_id": 30,
          "document_id": "1805.09959",
          "start_idx": 11796,
          "end_idx": 12298
        },
        "page_content": " A total of 48,113 tweets from these accounts were compiled from both the `cancer' (69%) and `breast' `cancer' (31%) feeds. We provided tweet frequency statistics in Figure FIGREF7 . This is an indicator that this population of breast cancer patients and survivors are actively tweeting about topics related to `cancer' including their experiences and complications. Next, we applied hedonometrics to compare the patient posts with all collected breast cancer tweets. We found that the surveyed patient",
        "type": "Document"
      },
      {
        "id": "3f4da77c-1907-4746-b597-b3e931c17e41",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1805.09959",
          "start_idx": 4913,
          "end_idx": 5432
        },
        "page_content": ",cancer\u201d and 65.2% of all tweets mentioning `cancer' while active. More information regarding the types of Twitter endpoints and calculating the sampling proportion of collected tweets is described in Appendix II. Our goal was to analyze content authored only by patients. To help ensure this outcome we removed posts containing a URL for classification, BIBREF19 . Twitter allows users to spread content from other users via `retweets'. We also removed these posts prior to classification to isolate tweets authored by",
        "type": "Document"
      },
      {
        "id": "2e764ab7-6545-40fb-9101-88d9786b1b98",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1805.09959",
          "start_idx": 11523,
          "end_idx": 11980
        },
        "page_content": " groups annotated the 1,331 tweets to identify patients and evaluate the classifier's results. The raters, showing high inter-rater reliability, individually evaluated each tweet as self-diagnostic of a breast cancer patient or survivor. The rater's independent annotations had a 96% agreement.  The classifier correctly identified 1,140 tweets (85.6%) from 845 profiles. A total of 48,113 tweets from these accounts were compiled from both the `cancer' (69",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What machine learning and NLP methods were used to sift tweets relevant to breast cancer experiences?",
    "llm_answer": "Convolutional Neural Network (CNN) with Google's Tensorflow interface.\n",
    "context": [
      {
        "id": "ea5fd626-9d1a-4d40-86ec-eb0f1e1d5fb5",
        "metadata": {
          "chunk_id": 70,
          "document_id": "1805.09959",
          "start_idx": 28710,
          "end_idx": 29108
        },
        "page_content": "', `raise', `awareness', `save', `support', and `survivor'. The word shift on the right compares breast cancer patient tweets to non filtered tweets. These were more negative ( INLINEFORM0 = 5.78 v. 6.01) due a relative increase in words like `fighting', `surgery', `against', `dying', `sick', `killing', `radiation', and `hospital'. This tool helped identify words that signal emotional themes and",
        "type": "Document"
      },
      {
        "id": "d9e15cf7-703f-4794-a57c-ed94d12fbeb6",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1805.09959",
          "start_idx": 2486,
          "end_idx": 3003
        },
        "page_content": "th, 2016 through December 9th, 2017. The primary feed for the analysis collected INLINEFORM0 million tweets containing the keywords `breast' AND `cancer'. See Figure FIGREF2 for detailed Twitter frequency statistics along with the user activity distribution. Our secondary feed searched just for the keyword `cancer' which served as a comparison ( INLINEFORM1 million tweets, see Appendix 1), and helped us collect additional tweets relevant to cancer from patients. The numeric account ID provided in tweets helps to",
        "type": "Document"
      },
      {
        "id": "9ca1585c-bb92-4bdc-97c8-8c6f7c20a450",
        "metadata": {
          "chunk_id": 56,
          "document_id": "1805.09959",
          "start_idx": 22950,
          "end_idx": 23576
        },
        "page_content": " breast cancer patients. Our framework provides a proof of concept for integrating machine learning with natural language processing as a tool to help connect healthcare providers with patient experiences. These methods can inform the medical community to provide more personalized treatment regimens by evaluating patient satisfaction using social listening. Twitter has also been shown as a useful medium for political support of healthcare policies as well as spreading awareness. Applying these analyses across other social media platforms could provide comparably rich data-sets. For instance, Instagram has been found to",
        "type": "Document"
      },
      {
        "id": "a2b27000-5481-4234-adbe-10cfc9fa5649",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1805.09959",
          "start_idx": 1277,
          "end_idx": 1849
        },
        "page_content": ". These self-reported diagnostic indicators allowed for a sentiment analysis of tweets authored by patients. However, this process was tedious, since the samples were hand verified and sifted through multiple keyword searches. Here, we aim to automate this process with machine learning context classifiers in order to build larger sets of patient self-reported outcomes in order to quantify the patent experience. Patients with breast cancer represent a majority of people affected by and living with cancer. As such, it becomes increasingly important to learn from their",
        "type": "Document"
      },
      {
        "id": "02f026f6-fe11-460e-af65-d1341e04cc51",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1805.09959",
          "start_idx": 10184,
          "end_idx": 10696
        },
        "page_content": "ated set of tweets to train a more sophisticated classifier to uncover self-diagnostic tweets from users describing their personal breast cancer experiences as current patients or survivors. We implemented the Convolutional Neural Network (CNN) with Google's Tensorflow interface, BIBREF26 . We adapted our framework from, BIBREF18 , but instead trained the CNN on these 1000 labeled cancer related tweets. The trained CNN was applied to predict patient self-diagnostic tweets from our breast cancer dataset. The",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What approach performs better in experiments global latent or sequence of fine-grained latent variables?",
    "llm_answer": "The GVT (global latent variable) achieves relatively lower reconstruction PPL, suggesting better performance.  The sequential latent variables (SVT) further improve reconstruction PPL.\n",
    "context": [
      {
        "id": "a59ae894-876c-4764-984c-2468e28764a4",
        "metadata": {
          "chunk_id": 17,
          "document_id": "2002.11402",
          "start_idx": 6246,
          "end_idx": 6761
        },
        "page_content": " hyper-parameter tuning which could have resulted in much more accurate results but at the same time could have taken very long time as well. So instead, chose and tweaked the parameters based on initial results. We trained two models, one with sequence length 512 to capture document level important n-grams and second with sequence length 64 to capture sentence/paragraph level important n-grams. Through experiments it was evident that, sequence length plays a vital role in deciding context and locally/globally",
        "type": "Document"
      },
      {
        "id": "b00aec72-c7c9-4b44-9c6c-9a590b299adf",
        "metadata": {
          "chunk_id": 59,
          "document_id": "2003.12738",
          "start_idx": 20839,
          "end_idx": 21332
        },
        "page_content": " latent variable into both models (CVAE and GVT) promote more diverse generation results and boost the diversity scores such as Dist-1, Dist-2, and Dist-3. Compare to baseline models, the GVT achieves relatively lower reconstruction PPL, which suggests that the global latent variable contains rich latent information (e.g., topic) for response generation. Meanwhile, the sequential latent variables of the SVT encode fine-grained latent information and further improve the reconstruction PPL.",
        "type": "Document"
      },
      {
        "id": "b3b5d592-adc2-498b-91a9-2bb3d8c38a5e",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1909.02480",
          "start_idx": 9856,
          "end_idx": 10312
        },
        "page_content": " as a simple normal distribution, which we found helps train very deep generative flows more stably. The motivation of introducing the latent variable $\\mathbf {z}$ into the model is to model the uncertainty in the generative process. Thus, it is preferable that $\\mathbf {z}$ capture contextual interdependence between tokens in $\\mathbf {y}$. However, there is an obvious local optimum where the posterior network generates a latent vector $\\mathbf {z}_t",
        "type": "Document"
      },
      {
        "id": "e64f386b-fde9-47d0-ae47-c59d174b523d",
        "metadata": {
          "chunk_id": 18,
          "document_id": "2002.11402",
          "start_idx": 6558,
          "end_idx": 7010
        },
        "page_content": " it was evident that, sequence length plays a vital role in deciding context and locally/globally important n-grams. Final output is a concatenation of both the model outputs. Trained the topic model on single 32gb NVidia-V100 and it took around 50 hours to train the model with sequence length 512. We had to take 256gb ram machine to accommodate all data in memory for faster read/write. Also, trained model with 64 sequence length in around 17 hours",
        "type": "Document"
      },
      {
        "id": "4ed15af4-4dbc-4a1e-8f4d-27c5a63bdb03",
        "metadata": {
          "chunk_id": 61,
          "document_id": "2003.12738",
          "start_idx": 21530,
          "end_idx": 22057
        },
        "page_content": " while in the combined dataset of Persona and ED, we observe performance drop of SVT compare to other models. This is because both Persona and ED are well designed and have lower entropy than MojiTalk which collected from Twitter. We hypothesize that the sequential latent variables have no advantage in term of similarity to single, fixed \"gold response\" when model low entropy response. Indeed, in open domain dialogue response generation, automatic metric is not always aligned with the human judgement BIBREF28. In contrast",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which translation system do they use to translate to English?",
    "llm_answer": "Google Translate\n",
    "context": [
      {
        "id": "7da029d3-0e6a-48d3-9dd2-3cfd37690294",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1910.04269",
          "start_idx": 6853,
          "end_idx": 7342
        },
        "page_content": " comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (",
        "type": "Document"
      },
      {
        "id": "3381329a-d2f5-42db-98c2-1e53471a6fa9",
        "metadata": {
          "chunk_id": 32,
          "document_id": "2001.05493",
          "start_idx": 11700,
          "end_idx": 12148
        },
        "page_content": " architectures discussed above. (see Figure FIGREF22. for block diagram of system architecture). We consider the text to be well formatted before applying the text to the embedding layer. First, we detect non-English text(which are few) and translate all of them to English using Google Translate. Still, there is some code mixed words like \"mc\", \"bc\" and other English abbreviations and spelling errors like \"nd\" in place of \"and\", \"u\" in place of",
        "type": "Document"
      },
      {
        "id": "5d1005c5-15ce-464a-be35-9420362fb62e",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1912.13109",
          "start_idx": 5540,
          "end_idx": 6048
        },
        "page_content": " using a two phase process Transliteration: In phase I, we used translation API's provided by Google translation services and exposed via a SDK, to transliteration the Hinglish messages to English messages. Translation: After transliteration, words that were specific to Hinglish were translated to English using an Hinglish-English dictionary. By doing this we converted the Hinglish message to and assortment of isolated words being presented in the message in a sequence that can also be represented using",
        "type": "Document"
      },
      {
        "id": "c589b050-81f0-46ff-82f5-9ae58d9f3b28",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1911.03894",
          "start_idx": 16382,
          "end_idx": 16738
        },
        "page_content": "BERT. For the TRANSLATE-TEST setting, we report results of the English RoBERTa to act as a reference. In the TRANSLATE-TRAIN setting, we report the best scores from previous literature along with ours. BiLSTM-max is the best model in the original XNLI paper, mBERT which has been reported in French in BIBREF52 and XLM (MLM+TLM) is the best-presented model",
        "type": "Document"
      },
      {
        "id": "76119453-0bc2-4587-b978-4afcfffadaa2",
        "metadata": {
          "chunk_id": 36,
          "document_id": "2004.01694",
          "start_idx": 14289,
          "end_idx": 14817
        },
        "page_content": " (with ties allowed) from two translation outputs: one produced by a professional translator, the other by a machine translation system. We do not show reference translations as one of the two options is itself a human translation. We use source sentences and documents from the WMT 2017 Chinese\u2013English test set (see Section SECREF8): documents are full news articles, and sentences are randomly drawn from these news articles, regardless of their position. We only consider articles from the test set that are native Chinese (",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which pre-trained English NER model do they use?",
    "llm_answer": "Flair's default NER model.\n",
    "context": [
      {
        "id": "dfd3bf97-eeac-470e-8b48-635d4bc64f52",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1906.01183",
          "start_idx": 7231,
          "end_idx": 7643
        },
        "page_content": " are trained on United Nation Parallel Corpus. For pre-trained English NER system, we use the default NER model of Flair. We train our NER model using vanilla SGD with no momentum for 150 epochs, with an initial learning rate of 0.1 and a learning rate annealing method in which the train loss does not fall in 3 consecutive epochs. The hidden size of BiLSTM model is set to 256 and mini-batch size is set to 16.",
        "type": "Document"
      },
      {
        "id": "0e173741-fb53-4be4-a1b2-53b92ebb919b",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1906.01183",
          "start_idx": 4269,
          "end_idx": 4709
        },
        "page_content": "trained English NER model We construct the English NER system following BIBREF7 . This system uses a bidirectional LSTM as a character-level language model to take context information for word embedding generation. The hidden states of the character language model (CharLM) are used to create contextualized word embeddings. The final embedding INLINEFORM0 is concatenated by the CharLM embedding INLINEFORM1 and GLOVE embedding INLINEFORM2",
        "type": "Document"
      },
      {
        "id": "ff2597e5-b26a-4916-a8d3-f08c944866b1",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1911.03894",
          "start_idx": 20718,
          "end_idx": 21200
        },
        "page_content": "-specific model BIBREF48, but our results suggest that the multilingual setting in which mBERT was trained is simply not enough to use it alone and fine-tune it for French NER, as it shows worse performance than even simple CRF models, suggesting that monolingual models could be better at NER. CamemBERT displays improved performance compared to prior work for the 4 downstream tasks considered. This confirms the hypothesis that pretrained language models can be effectively fine-",
        "type": "Document"
      },
      {
        "id": "1d21ff99-26a6-4bb6-b639-4e660d298ad4",
        "metadata": {
          "chunk_id": 34,
          "document_id": "2002.11402",
          "start_idx": 12596,
          "end_idx": 13151
        },
        "page_content": " this exercise, we were able to test out the best suitable model architecture and data preparation steps so that similar models could be trained for Indian languages. Building cased or caseless NERs for English was not the final goal and this has already been benchmarked and explored before in previous approaches explained in \"Related Work\" section. We didn't use traditional datasets for model performance comparisons & benchmarks. As mentioned before, all the comparisons are being done with open-source models and libraries from the productionization",
        "type": "Document"
      },
      {
        "id": "23663590-851a-4a15-adc3-b090f37d4016",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1701.02877",
          "start_idx": 18041,
          "end_idx": 18461
        },
        "page_content": " data, which might take a long time to gather and retrain. For the setup in BIBREF35 language models are trained for seven weeks on the English Wikipedia, Reuters RCV1 BIBREF46 and parts of the Wall Street Journal, and results are reported over the CoNLL 2003 NER dataset. Reuters RCV1 is chosen as unlabelled data because the English CoNLL 2003 corpus is created from the Reuters RCV1 corpus. For this paper, we use the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much of the ASR grapheme set is shared between languages?",
    "llm_answer": "The context states that the languages share the common basic Latin alphabet and that Hindi and Marathi use the Devanagari script.  It does not quantify the degree of shared characters beyond this.\n",
    "context": [
      {
        "id": "de5b5365-3793-4d8d-a60e-083f62a93802",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1909.06522",
          "start_idx": 1526,
          "end_idx": 1963
        },
        "page_content": " known at decoding time, and the language specific lexicon and language model applied. In the absence of a phonetic lexicon, building graphemic systems has shown comparable performance to phonetic lexicon-based approaches in extensive monolingual evaluations BIBREF11, BIBREF12, BIBREF13. Recent advances in end-to-end ASR models have attempted to take the union of multiple language-specific grapheme (i.e. orthographic character) sets,",
        "type": "Document"
      },
      {
        "id": "156d6eb2-2190-4349-a599-11a1f08b2e36",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1909.06522",
          "start_idx": 1963,
          "end_idx": 2391
        },
        "page_content": " union of multiple language-specific grapheme (i.e. orthographic character) sets, and use such union as a universal grapheme set for a single sequence-to-sequence ASR model BIBREF14, BIBREF15, BIBREF16. It allows for learning a grapheme-based model jointly on data from multiple languages, and performing ASR on within training set languages. In various cases it can produce performance gains over monolingual modeling that uses",
        "type": "Document"
      },
      {
        "id": "b422f042-c97a-45db-81cc-4d0f651951f9",
        "metadata": {
          "chunk_id": 33,
          "document_id": "1909.06522",
          "start_idx": 12081,
          "end_idx": 12561
        },
        "page_content": " these 7 languages have little overlap except that (i) they all include common basic Latin alphabet, and (ii) both Hindi and Marathi use Devanagari script. We took the union of 7 character sets therein as the multilingual grapheme set (Section SECREF2), which contained 432 characters. In addition, we deliberately split 7 languages into two groups, such that the languages within each group were more closely related in terms of language family, orthography or phonology. We thus",
        "type": "Document"
      },
      {
        "id": "d0daecc2-e3f1-4ce5-b68b-437c878a53a0",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1909.06522",
          "start_idx": 17876,
          "end_idx": 18409
        },
        "page_content": " of 11.0% on Kannada and 18.4% on Hindi. We have presented a multilingual grapheme-based ASR model can effectively perform language-independent recognition on any within training set languages, and substantially outperform each monolingual ASR alternative. Various data augmentation techniques can yield further complementary improvements. Such single multilingual model can not only provide better ASR performance, but also serves as an alternative to the standard production deployment that typically includes extensive monolingual",
        "type": "Document"
      },
      {
        "id": "e86ad7c0-37ac-4e30-bd91-5cc2bb699ae6",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1909.06522",
          "start_idx": 12770,
          "end_idx": 13169
        },
        "page_content": " hours, which are referred to as 7lang, 4lang, and 3lang respectively. Note that Kannada, Malayalam and Tamil are Dravidian languages, which have rich agglutinative inflectional morphology BIBREF2 and resulted in around 10% OOV token rates on test sets (Hindi had the lowest OOV rate as 2-3%). Such experimental setup was designed to answer the questions: If a single graphemic ASR model could scale",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the languages used to test the model?",
    "llm_answer": "English, French, German, Spanish, Russian, Italian.\n",
    "context": [
      {
        "id": "7da029d3-0e6a-48d3-9dd2-3cfd37690294",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1910.04269",
          "start_idx": 6853,
          "end_idx": 7342
        },
        "page_content": " comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (",
        "type": "Document"
      },
      {
        "id": "74d41b8d-63d7-479f-a82f-76142d721807",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1911.03894",
          "start_idx": 15623,
          "end_idx": 16121
        },
        "page_content": " is also machine translated for all languages. The dataset is composed of 122k train, 2490 valid and 5010 test examples. As usual, NLI performance is evaluated using accuracy. To evaluate a model on a language other than English (such as French), we consider the two following settings: TRANSLATE-TEST: The French test set is machine translated into English, and then used with an English classification model. This setting provides a reasonable, although imperfect, way to circumvent the fact that",
        "type": "Document"
      },
      {
        "id": "3e903df4-a13b-4f8f-8458-741dc0ce7a03",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1910.07481",
          "start_idx": 7033,
          "end_idx": 7506
        },
        "page_content": ". We consider two different models for each language pair: the Baseline and the Document model. We evaluate them on 3 test sets and report BLEU and TER scores. All experiments are run 8 times with different seeds, we report averaged results and p-values for each experiment. Translation tasks are English to German, proposed in the first document-level translation task at WMT 2019 BIBREF17, English to French and French to English, following the IWSLT translation task BIB",
        "type": "Document"
      },
      {
        "id": "7367d0f1-0e76-4843-a7a9-1611742d90a2",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1809.04686",
          "start_idx": 9144,
          "end_idx": 9613
        },
        "page_content": "40 chose a subset of the SNLI test set (1332 sentences) and professionally translated it into four major languages - Arabic, French, Russian, and Spanish. We use the French test set for evaluation in Section \"Zero-Shot Classification Results\" and \"Analyses\" . Here, we first describe the model and training details of the base multilingual NMT model whose encoder is reused in all other tasks. Then we provide details about the task-specific classifiers. For each task,",
        "type": "Document"
      },
      {
        "id": "e8fb7d0e-86e1-4332-bb4a-d8109e4146e4",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1910.04269",
          "start_idx": 19553,
          "end_idx": 20023
        },
        "page_content": ", the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Kat",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which language has the lowest error rate reduction?",
    "llm_answer": "Sinhala\n",
    "context": [
      {
        "id": "780efdd4-9ed8-4b98-875a-348167bf37f8",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1909.06522",
          "start_idx": 15207,
          "end_idx": 15689
        },
        "page_content": "annada and Hindi, via language-specific lexicon and language model at decoding time. We found that, the language-specific decoding provided only moderate gains, presumably as discussed above, the language-independent decoding had given the mismatched-language word token rates as sufficiently low as 1-3%. Additionally, the multilingual ASR of 4lang and 3lang (Section SECREF15) achieved the same, or even slightly better performance as compared to the ASR of 7lang, suggesting that",
        "type": "Document"
      },
      {
        "id": "d25225cd-30f3-457e-813e-40e9e4ed5a8f",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1909.06522",
          "start_idx": 14189,
          "end_idx": 14631
        },
        "page_content": " validation set was used for evaluation on test set. ASR word error rate (WER%) results are shown in Table TABREF11. We found that, although not explicitly given any information on test language identities, multilingual ASR with language-independent decoding (Section SECREF3) - trained on 3, 4, or 7 languages - substantially outperformed each monolingual ASR in all cases, and on average led to relative WER reductions between 4.6% (Sinhala",
        "type": "Document"
      },
      {
        "id": "a5866b64-a8aa-4a76-a4b4-db8f4bf9920c",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1909.13362",
          "start_idx": 22727,
          "end_idx": 23135
        },
        "page_content": "IBREF12. Best-known results are also obtained on the Italian, French, and Basque datasets. Our reported accuracy of $94.9 \\pm 0.3\\%$ on the Manipuri dataset is furthest from state of the art. We suspect this to be due to having limited amounts of training data; the $97.5\\%$ accurate system from BIBREF35 supplemented their data-driven approach with rules of syllabification. Examples from the outputs of the",
        "type": "Document"
      },
      {
        "id": "8aa90678-eb38-4acd-8ee0-7508dbccb8bc",
        "metadata": {
          "chunk_id": 62,
          "document_id": "1910.12618",
          "start_idx": 24323,
          "end_idx": 24815
        },
        "page_content": " vocabulary. We also considered an aggregated forecaster (abridged Agg), consisting of the average of the two best individual ones in terms of RMSE. All the neural network methods have a reduced vocabulary size $V^*$. The results for the French and UK data are respectively given by tables TABREF26 and TABREF27. Our empirical results show that for the electricity consumption prediction task, the order of magnitude of the relative error is around 5%, independently of the language, encoding",
        "type": "Document"
      },
      {
        "id": "36d38442-8168-493d-b526-a41d12e33988",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1910.05456",
          "start_idx": 13392,
          "end_idx": 13836
        },
        "page_content": " best, followed closely by ITA. While the good performance of HUN as a source language is still unexpected, ITA is closely related to SPA, which could explain the high accuracy of the final model. As for ENG, pretraining on EUS and NAV yields the worst final models \u2013 importantly, accuracy is over $15\\%$ lower than for QVH, which is also an unrelated language. This again suggests that the prefixing morphology of EUS and NAV might play a role",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is moral bias measured?",
    "llm_answer": "Moral bias is measured via cosine-similarity, clustering, or semantic search, and by computing the moral subspace of the text embedding, using PCA on selected atomic actions.  The correlation between the WEAT value and the moral bias is calculated using Pearson's Correlation Coefficient.\n",
    "context": [
      {
        "id": "82c7f328-2228-42c8-b4a3-717032e7b758",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1912.05238",
          "start_idx": 16326,
          "end_idx": 16786
        },
        "page_content": " via cosine-similarity, clustering or semantic search\u2014 improves the scores of the extracted moral actions. The correlation of the moral bias and the corresponding WEAT value was calculated to test consistency of findings. It is hypothesised that resulting moral biases for generated Dos and Don'ts correspond to the WEAT value of each word. The correlation was tested by means of Pearson's Correlation Coefficient: where $m_x$ and $m_y$ are the the means of $X",
        "type": "Document"
      },
      {
        "id": "83cc2817-2337-48da-90be-6877dd7a8a62",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1912.05238",
          "start_idx": 12882,
          "end_idx": 13363
        },
        "page_content": " are labelled beforehand. Using the above-mentioned methodology, we propose an alternative to identify the moral bias. Inspired by BIBREF13, we first compute the moral subspace of the text embedding. Instead of the difference vectors of the question/answer pairs, we compute the PCA on selected atomic actions \u2014we expect that these actions represent Dos and Don'ts (cf. Appendix). We formulate the actions as questions, i.e. using question templates, and compute the mean embedding",
        "type": "Document"
      },
      {
        "id": "4f457aaf-7a18-4071-bdf6-740288092c85",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1912.05238",
          "start_idx": 17001,
          "end_idx": 17399
        },
        "page_content": " two or three starlets. The correlation between WEAT value and the moral bias gets tangible, when inspecting their correlation graphically, cf. Fig. FIGREF4. The concrete bias scores can be found in the Appendix, Fig. TABREF28 and TABREF29. For both WEAT and MCM, the scatter plots of Dos and Don'ts are divided on the x-axis. The Pearson's Correlation Coefficient using USE as embedding (Top) $r =",
        "type": "Document"
      },
      {
        "id": "4b33487c-87f6-4eab-8efa-c40a7aa9c63b",
        "metadata": {
          "chunk_id": 77,
          "document_id": "1912.05238",
          "start_idx": 28286,
          "end_idx": 28685
        },
        "page_content": " on the first two top PCs. The top PC (the $X$ axis) defines the moral direction $m$ (bias). The context-based actions which were tested using the moral subspace projection are listed in Tab. TABREF27. The resulting moral direction $m$ (or bias) for both the atomic and context-based actions can be found in Tab. TABREF30. We also list the results using the sentence embedding USE instead of BERT. $",
        "type": "Document"
      },
      {
        "id": "931d9986-71e2-4aff-bae0-5a2d260a7e83",
        "metadata": {
          "chunk_id": 56,
          "document_id": "1912.05238",
          "start_idx": 20871,
          "end_idx": 21397
        },
        "page_content": " than previous embedding methods. That is, yes, text embeddings encode knowledge about deontological ethical and even moral choices, but the quality of the bias score depends on the quality of the text embedding network. Specifically, our empirical results show that the Moral Choice Machine with recent state-of-the-art language representations, namely BERT, extends the boundary of previous approaches and demonstrate the existence of biases in human language on a complex phrase level. Moreover, we identified for the first",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What metrics are used for evaluation?",
    "llm_answer": "Task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, F1-score, BLEU, ROUGE, METEOR, WPSLOR, ROUGE-LM.\n",
    "context": [
      {
        "id": "6b1e72f3-8ee5-43ac-a062-f08c158e3d67",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1709.10217",
          "start_idx": 6394,
          "end_idx": 6850
        },
        "page_content": " evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the number of total tasks. User satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied, neutral, satisfied and very satisfied, respectively. Response fluency: There are three scores -1, 0, 1, which indicate nonfluency, neutral, fluency. Number of dialogue turns: The number of utterances in a task",
        "type": "Document"
      },
      {
        "id": "b3036530-5228-47f4-b1f0-9a1687f209da",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1709.10217",
          "start_idx": 4141,
          "end_idx": 4657
        },
        "page_content": ", in which only the released data can be used for training and developing. The other is an open evaluation that allow to collect external data for training and developing. For task 1, we use F1-score as evaluation metric. For the task-oriented dialogue systems, the best way for evaluation is to use the online human-computer dialogue. After finishing an online human-computer dialogue with a dialogue system, the human then manually evaluate the system by using the metrics of user satisfaction degree, dialogue flu",
        "type": "Document"
      },
      {
        "id": "d88a1149-0827-4f42-b9b1-620cfd250358",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1905.08949",
          "start_idx": 10602,
          "end_idx": 11043
        },
        "page_content": " they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks. As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE B",
        "type": "Document"
      },
      {
        "id": "1d812102-98f4-45bc-b355-c1653ccfaf7e",
        "metadata": {
          "chunk_id": 72,
          "document_id": "1809.08731",
          "start_idx": 24617,
          "end_idx": 25067
        },
        "page_content": " metrics on a benchmark dataset. Additionally, they can be applied even in settings where no references are available, or would be costly to obtain. Finally, for given references, we proposed the reference-based metric ROUGE-LM, which consists of a combination of WPSLOR and ROUGE. Thus, we were able to obtain an even more accurate fluency evaluation. We would like to thank Sebastian Ebert and Samuel Bowman for their detailed and helpful feedback.",
        "type": "Document"
      },
      {
        "id": "a669a759-4988-4843-98f8-c12f54d6dcaa",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1906.03538",
          "start_idx": 17710,
          "end_idx": 18208
        },
        "page_content": "6 and INLINEFORM7 , respectively. The metrics are averaged across all the perspectives INLINEFORM8 participating in the test set. The goal is to get estimates of the overall performance of the systems. Instead of creating a complex measure that would take all the aspects into account, we approximate the overall performance by multiplying the disjoint measures in INLINEFORM0 , INLINEFORM1 and INLINEFORM2 . While this gives an estimate on the overall quality, it ignores the pipeline structure of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much training data is used?",
    "llm_answer": "2,838 (or 7934)\n",
    "context": [
      {
        "id": "7c07de9a-2536-4a2c-9fe3-ac95785f8273",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1911.00133",
          "start_idx": 14430,
          "end_idx": 14888
        },
        "page_content": " content. This results in a total of 2,838 train data points (51.6% labeled stressful) and 715 test data points (52.4% labeled stressful). Because our data is relatively small, we train our traditional supervised models with 10-fold cross-validation; for our neural models, we break off a further random 10% of the training data for validation and average the predictions of 10 randomly-initialized trained models. In addition to the words of the posts (both",
        "type": "Document"
      },
      {
        "id": "9d6e4c30-d037-4a07-a951-0e7d1761e751",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1912.13109",
          "start_idx": 7657,
          "end_idx": 8136
        },
        "page_content": " above few data augmentation techniques were applied to boost the learning of the network. Before applying augmentation, a train-test split of 78%-22% was done from the original, cleansed data set. Thus, 700 tweets/messages were held out for testing. All model evaluation were done in on the test set that got generated by this process. The results presented in this report are based on the performance of the model on the test set. The training set of 2489 messages were however",
        "type": "Document"
      },
      {
        "id": "001abd8b-7ba1-407b-8110-49e732f239a9",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1910.12618",
          "start_idx": 19089,
          "end_idx": 19565
        },
        "page_content": " maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014",
        "type": "Document"
      },
      {
        "id": "6f04ea61-adc5-4abb-9261-abc554ebac1b",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1711.02013",
          "start_idx": 20658,
          "end_idx": 21116
        },
        "page_content": "ings, between recurrent layers, and on recurrent states were (0.7, 0.5, 0.5) respectively. dataset contains 17M training tokens and has a vocabulary size of 44k words. The dataset is partitioned into a training set (first 99M characters) and a development set (last 1M characters) that is used to report performance. As this dataset contains various articles from Wikipedia, the longer term information (such as current topic) plays a bigger role than in the",
        "type": "Document"
      },
      {
        "id": "1e2f925a-9a0d-4318-af38-ee9ae9d2a426",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1912.13109",
          "start_idx": 7921,
          "end_idx": 8471
        },
        "page_content": " on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does their model differ from BERT?",
    "llm_answer": "The input embeddings are the sum of token embeddings, segmentation embeddings, and position embeddings.  Segmentation embeddings use learned sentence A and sentence B embeddings for the first and second sentences, respectively.  ALBERT uses factorized embedding parameterization and cross-layer parameter sharing for parameter reduction.  Distilled BERT-based models are also described.\n",
    "context": [
      {
        "id": "3dc17a8d-13b0-41b4-b7db-e6cf42f7f0a4",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1909.00578",
          "start_idx": 10519,
          "end_idx": 11017
        },
        "page_content": " have a larger receptive field as well. The BERT multi-task versions perform better with highly correlated qualities like $\\mathcal {Q}4$ and $\\mathcal {Q}5$ (as illustrated in Figures 2 to 4 in the supplementary material). However, there is not a clear winner among them. Mathematical equivalence does not lead to deterministic results, especially when random initialization and stochastic learning algorithms are involved. An in-depth exploration of this point would involve further investigation",
        "type": "Document"
      },
      {
        "id": "3a417c27-56d4-4884-9808-a1c551d116a4",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1812.06705",
          "start_idx": 10253,
          "end_idx": 10716
        },
        "page_content": " BERT shares the same model architecture with the original BERT. The differences are the input representation and training procedure. The input embeddings of BERT are the sum of the token embeddings, the segmentation embeddings and the position embeddings. For the segmentation embeddings in BERT, a learned sentence A embedding is added to every token of the first sentence, and if a second sentence exists, a sentence B embedding will be added to every token of",
        "type": "Document"
      },
      {
        "id": "e20528dd-42d6-43a4-8089-2d19f634e2f7",
        "metadata": {
          "chunk_id": 10,
          "document_id": "2002.06644",
          "start_idx": 3849,
          "end_idx": 4295
        },
        "page_content": " as twelve times as compared to $BERT_{large}$, with bigger batches, and longer sequences. ALBERT, introduced in BIBREF7, uses factorized embedding parameterization and cross-layer parameter sharing for parameter reduction. These optimizations have led both the models to outperform $BERT_{large}$ in various benchmarking tests, like GLUE for text classification and SQuAD for Question Answering. Distilled BERT-based models: Secondly, we propose",
        "type": "Document"
      },
      {
        "id": "a1690506-dec4-411a-bde5-0ea265a08f04",
        "metadata": {
          "chunk_id": 86,
          "document_id": "1905.06566",
          "start_idx": 24100,
          "end_idx": 24427
        },
        "page_content": "SEP] tokens) into BERT and use the representation at [CLS] token to classify each sentence. Our model $\\text{\\sc Hibert}_S$1 outperforms BERT by 0.4 to 0.5 ROUGE despite with only half the number of model parameters ( $\\text{\\sc Hibert}_S$2 54.6M v.s. BERT 110M). Results on the NYT50 dataset show the similar trends (see Table",
        "type": "Document"
      },
      {
        "id": "a2d13af7-7d66-4e58-af1c-f9dc74e9f848",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1905.13497",
          "start_idx": 3655,
          "end_idx": 4034
        },
        "page_content": "iguation dataset, PDP-60, our method achieves 68.3% accuracy, which is better than the state-of-art accuracy of 66.7%. On a WSC dataset, WSC-273, our method achieves 60.3%. As of today, state-of-the-art accuracy on the WSC-273 for single model performance is around 57%, BIBREF14 and BIBREF13 . These results suggest that BERT implicitly learns to establish complex relationships",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does explicit constraint on the KL divergence term that authors propose looks like?",
    "llm_answer": "$|D_{KL}\\big (q_\\phi ({z}|{x}) || p({z})\\big )-C|$ where $C \\ge 0$.\n",
    "context": [
      {
        "id": "2f4e3231-1990-4a35-8506-b50aca528b57",
        "metadata": {
          "chunk_id": 76,
          "document_id": "1909.13668",
          "start_idx": 24751,
          "end_idx": 25130
        },
        "page_content": " term ($|D_{KL}\\big (q_\\phi ({z}|{x}) || p({z})\\big )-C|$). We experimented with a range of constraints ($C$) on the KL term and various powerful and weak decoder architectures (LSTM, GRU, and CNN), and empirically confirmed that in all cases the constraint was satisfied. We showed that the higher value of KL encourages not only divergence from the prior distribution, but also",
        "type": "Document"
      },
      {
        "id": "23ca1fd1-03c0-420f-9d03-57185cc16741",
        "metadata": {
          "chunk_id": 30,
          "document_id": "1909.13668",
          "start_idx": 9356,
          "end_idx": 9661
        },
        "page_content": " the results for $\\beta _C$-VAEGRU, $\\beta _C$-VAELSTM, and $\\beta _C$-VAECNN models on Yahoo and Yelp corpora. In all our experiments we found that $C\\!-\\!1\\!\\le KL\\!\\le \\! C\\!+\\!1$, demonstrating that the objective function effectively imposed the desired constraint on KL term. Hence, setting any $C>0$",
        "type": "Document"
      },
      {
        "id": "83dced40-5ddb-42d2-bfc9-4757bcc483d0",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1909.00430",
          "start_idx": 6876,
          "end_idx": 7257
        },
        "page_content": " would like to set INLINEFORM3 such that INLINEFORM4 and INLINEFORM5 are close. BIBREF0 suggest to use KL-divergence for this. KL-divergence is composed of two parts: INLINEFORM6 INLINEFORM7  Since INLINEFORM0 is constant, we only need to minimize INLINEFORM1 , therefore the loss function becomes: DISPLAYFORM0  Notice that computing INLINEFORM0 requires summation over INLINEFORM",
        "type": "Document"
      },
      {
        "id": "18801d32-0ee6-4e0e-94b0-8c68653e3493",
        "metadata": {
          "chunk_id": 75,
          "document_id": "1909.13668",
          "start_idx": 24489,
          "end_idx": 24872
        },
        "page_content": " variant of $\\beta $-VAE objective, which permits explicit control of the KL term, and treats KL as a mechanism to control the amount of information transmitted between the encoder and decoder. The immediate impact of the explicit constraint is avoiding the collapse issue ($D_{KL}=0$) by setting a non-zero positive constraint ($C\\ge 0$) on the KL term ($|D_{KL}\\big (q_\\phi ({z}|{x",
        "type": "Document"
      },
      {
        "id": "092976dd-f0b2-4d97-a6a4-825f2c91d0b2",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1909.13668",
          "start_idx": 4319,
          "end_idx": 4609
        },
        "page_content": " := - \\big \\langle \\log p_\\theta ({x}|{z}) \\big \\rangle $), while the KL term quantifies the amount of compression (rate; $R := D_{KL}[q_\\phi ({z}|{x})|| p({z})]$) by measuring the divergence between a channel that transmits zero bit of information about $x$, denoted by $p(z)$, and the enc",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what was the baseline?",
    "llm_answer": "Content, content-network, and image-based models.  Also, a LSTM baseline and a hierarchical transformer summarization model (HeriTransfomer).  Model 1 is also a baseline.\n",
    "context": [
      {
        "id": "c9d62915-21d1-42b6-a16c-8d9d28de6e0c",
        "metadata": {
          "chunk_id": 91,
          "document_id": "1902.06843",
          "start_idx": 35139,
          "end_idx": 35416
        },
        "page_content": "s by -0.27. Baselines: To test the efficacy of our multi-modal framework for detecting depressed users, we compare it against existing content, content-network, and image-based models (based on the aforementioned general image feature, facial presence, and facial expressions.)",
        "type": "Document"
      },
      {
        "id": "4d522ea5-86b3-42df-acaa-37f028f2bec5",
        "metadata": {
          "chunk_id": 138,
          "document_id": "2001.11381",
          "start_idx": 31860,
          "end_idx": 32196
        },
        "page_content": "ados. Dicho lo anterior, el Modelo 1 podr\u00eda ser considerado como nuestro propio baseline. A continuaci\u00f3n presentamos un protocolo de evaluaci\u00f3n manual de los resultados obtenidos. El experimento consisti\u00f3 en la generaci\u00f3n de 15 frases por cada uno de los tres modelos propuestos. Para cada modelo, se consideraron tres queries: $Q=$ {AM",
        "type": "Document"
      },
      {
        "id": "0d211bed-e5df-4233-8d02-e24d7ee9b65c",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1909.02635",
          "start_idx": 9050,
          "end_idx": 9548
        },
        "page_content": " (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo",
        "type": "Document"
      },
      {
        "id": "487566c1-6b22-466e-8847-fa293a6f80e8",
        "metadata": {
          "chunk_id": 83,
          "document_id": "1905.06566",
          "start_idx": 23289,
          "end_idx": 23633
        },
        "page_content": " by 0.5 ROUGE. We also implemented two baselines. One is the hierarchical transformer summarization model (HeriTransfomer; described in \"Extractive Summarization\" ) without pre-training. Note the setting for HeriTransfomer is ( $L=4$ , $H=300$ and $A=4$ ) . We can see that the pre-training (details in Section \"Pre-training\" ) leads to a +1.25",
        "type": "Document"
      },
      {
        "id": "b7568416-2511-4e7d-b42a-905e8dea8f55",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1909.09484",
          "start_idx": 19263,
          "end_idx": 19734
        },
        "page_content": " models and baselines, with a learning rate of 0.001 for supervised training and 0.0001 for reinforcement learning. In reinforcement learning, the decay parameter $\\lambda $ is set to 0.8. The weight decay is set to 0.001. And early stopping is performed on developing set. The experimental results of the proposed model and baselines will be analyzed from the following aspects. BPRA Results: As shown in Table TABREF35, most of the models have similar performance on BP",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is dataset used?",
    "llm_answer": "The sizes of various datasets vary.  Some are in the hundreds of thousands (e.g., 287,226 documents), others are in millions (e.g., 17M training tokens).  Information on total dataset size isn't given for all datasets.\n",
    "context": [
      {
        "id": "2d3bdfec-4ed5-456d-bd82-023fad798858",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "001abd8b-7ba1-407b-8110-49e732f239a9",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1910.12618",
          "start_idx": 19089,
          "end_idx": 19565
        },
        "page_content": " maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014",
        "type": "Document"
      },
      {
        "id": "9ccb7945-9a96-417a-b52e-68855c4910b1",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "6f04ea61-adc5-4abb-9261-abc554ebac1b",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1711.02013",
          "start_idx": 20658,
          "end_idx": 21116
        },
        "page_content": "ings, between recurrent layers, and on recurrent states were (0.7, 0.5, 0.5) respectively. dataset contains 17M training tokens and has a vocabulary size of 44k words. The dataset is partitioned into a training set (first 99M characters) and a development set (last 1M characters) that is used to report performance. As this dataset contains various articles from Wikipedia, the longer term information (such as current topic) plays a bigger role than in the",
        "type": "Document"
      },
      {
        "id": "cc9faf95-ff74-4532-ad40-1937aafe18a1",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1707.03904",
          "start_idx": 12758,
          "end_idx": 13269
        },
        "page_content": " to evaluate the performance of the reading system independently from the search system, while the full set allows us to evaluate the performance of Quasar as a whole. We also split the full set into training, validation and test sets. The final size of each data subset after all discards is listed in Table 1 . Evaluation is straightforward on Quasar-S since each answer comes from a fixed output vocabulary of entities, and we report the average accuracy of predictions as the evaluation metric. For Quasar-T",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much better does this baseline neural model do?",
    "llm_answer": "The baseline neural model outperforms the EntNet model.\n",
    "context": [
      {
        "id": "fc23cf7f-21ff-43cd-8d00-ef5914322400",
        "metadata": {
          "chunk_id": 68,
          "document_id": "1810.00663",
          "start_idx": 26280,
          "end_idx": 26763
        },
        "page_content": " room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM. Note that the average number of actions in the ground truth output sequences is 7.07 for the Test-New set. Our model's predictions are just INLINEFORM1 edits off on average from the correct navigation plans. This",
        "type": "Document"
      },
      {
        "id": "8937964f-11b5-487b-9960-db106025360c",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1909.02764",
          "start_idx": 17421,
          "end_idx": 17831
        },
        "page_content": " layer. This setup is inspired by BIBREF38. We use a dropout rate of 0.3 in all layers and optimize with Adam BIBREF39 with a learning rate of $10^{-5}$ (These parameters are the same for all further experiments). We build on top of the Keras library with the TensorFlow backend. We consider this setup our baseline model. We train models on a variety of corpora, namely the common format published by BIBREF27",
        "type": "Document"
      },
      {
        "id": "26654c16-c460-4900-b78d-5ad48b826e37",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1909.00542",
          "start_idx": 15059,
          "end_idx": 15516
        },
        "page_content": " the results and details of the runs submitted to BioASQ. The table uses ROUGE-SU4 Recall since this is the metric available at the time of writing this paper. However, note that, as explained in Section SECREF6, these results might differ from the final human evaluation results. Therefore we do not comment on the results, other than observing that the \u201cfirst $n$\u201d baseline produces the same results as the neural regressor. As mentioned in Section SECREF",
        "type": "Document"
      },
      {
        "id": "d7a0b7aa-da14-4112-80c0-cc26c62fe83f",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "d6d5b86e-164a-44b9-81e4-75eea14c84ca",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1706.07179",
          "start_idx": 6896,
          "end_idx": 7300
        },
        "page_content": ". The baseline EntNet model was run for 10 times for each task BIBREF17 . The results are shown in Table 1 . The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the EntNet model BIBREF17 . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the EntNet model achieves 0% error on 7 of the tasks. We demonstrated an end-to-end trained",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the baseline models?",
    "llm_answer": "TF-IDF, S2S, IR, and Proposed+IR\n",
    "context": [
      {
        "id": "7d4754c7-4ea8-4d20-9fb6-3d1e50bf0cf6",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1809.04960",
          "start_idx": 22497,
          "end_idx": 22960
        },
        "page_content": " the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types of comments as described in the above retrieval evaluation setting: Correct, Plausible, Popular, and Random. We rank the candidate comment set with four models (TF-IDF, S2S, IR, and Proposed+IR), and record the types of top-1 comments. Figure FIGREF40 shows the percentage of different types of top-1 comments generated by each model. It shows",
        "type": "Document"
      },
      {
        "id": "4d522ea5-86b3-42df-acaa-37f028f2bec5",
        "metadata": {
          "chunk_id": 138,
          "document_id": "2001.11381",
          "start_idx": 31860,
          "end_idx": 32196
        },
        "page_content": "ados. Dicho lo anterior, el Modelo 1 podr\u00eda ser considerado como nuestro propio baseline. A continuaci\u00f3n presentamos un protocolo de evaluaci\u00f3n manual de los resultados obtenidos. El experimento consisti\u00f3 en la generaci\u00f3n de 15 frases por cada uno de los tres modelos propuestos. Para cada modelo, se consideraron tres queries: $Q=$ {AM",
        "type": "Document"
      },
      {
        "id": "7562b134-7abd-4e6d-a542-e9dd2de17983",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1905.11037",
          "start_idx": 5306,
          "end_idx": 5824
        },
        "page_content": "ered perceptron, convolutional neural networks and long short-term memory networks. We outline the essentials of each of these models, but will treat them as black boxes. In a related line, kaushik2018much discuss the need of providing rigorous baselines that help better understand the improvement coming from future and complex models, and also the need of not demanding architectural novelty when introducing new datasets. Although not done in this work, an alternative (but also natural) way to address the task is",
        "type": "Document"
      },
      {
        "id": "b7568416-2511-4e7d-b42a-905e8dea8f55",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1909.09484",
          "start_idx": 19263,
          "end_idx": 19734
        },
        "page_content": " models and baselines, with a learning rate of 0.001 for supervised training and 0.0001 for reinforcement learning. In reinforcement learning, the decay parameter $\\lambda $ is set to 0.8. The weight decay is set to 0.001. And early stopping is performed on developing set. The experimental results of the proposed model and baselines will be analyzed from the following aspects. BPRA Results: As shown in Table TABREF35, most of the models have similar performance on BP",
        "type": "Document"
      },
      {
        "id": "8937964f-11b5-487b-9960-db106025360c",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1909.02764",
          "start_idx": 17421,
          "end_idx": 17831
        },
        "page_content": " layer. This setup is inspired by BIBREF38. We use a dropout rate of 0.3 in all layers and optimize with Adam BIBREF39 with a learning rate of $10^{-5}$ (These parameters are the same for all further experiments). We build on top of the Keras library with the TensorFlow backend. We consider this setup our baseline model. We train models on a variety of corpora, namely the common format published by BIBREF27",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much of MGNC-CNN out perform the baselines?",
    "llm_answer": "The context states that MGNC-CNN and MG-CNN outperform baseline methods, but does not give a specific performance difference.\n",
    "context": [
      {
        "id": "545f70fa-8008-4936-a00c-bd451c43f170",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1603.00968",
          "start_idx": 12591,
          "end_idx": 13006
        },
        "page_content": "IBREF11 for comparison; this represents the state-of-the-art on the three datasets other than TREC. We can see that MGNC-CNN and MG-CNN always outperform baseline methods (including C-CNN), and MGNC-CNN is usually better than MG-CNN. And on the Subj dataset, MG-CNN actually achieves slightly better results than BIBREF11 , with far less complexity and required training time (MGNC-CNN performs comparably, although",
        "type": "Document"
      },
      {
        "id": "debcc44a-3c46-46a7-b940-9f9018e04372",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1603.00968",
          "start_idx": 13006,
          "end_idx": 13420
        },
        "page_content": "11 , with far less complexity and required training time (MGNC-CNN performs comparably, although no better, here). On the TREC dataset, the best-ever accuracy we are aware of is 96.0% BIBREF21 , which falls within the range of the result of our MGNC-CNN model with three word embeddings. On the irony dataset, our model with three embeddings achieves 4% improvement (in terms of AUC) compared to the baseline model",
        "type": "Document"
      },
      {
        "id": "f1339450-dc9e-4e11-9b9a-6fbcd07c7074",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1603.00968",
          "start_idx": 11217,
          "end_idx": 11651
        },
        "page_content": " INLINEFORM1 on a validation set. For instantiations of MGNC-CNN in which we exploited two embeddings, we tuned both INLINEFORM2 , and INLINEFORM3 ; where we used three embedding sets, we tuned INLINEFORM4 and INLINEFORM5 . We used standard train/test splits for those datasets that had them. Otherwise, we performed 10-fold cross validation, creating nested development sets with which to tune hyperparameters. For all experiments we",
        "type": "Document"
      },
      {
        "id": "a2d9d185-aa9e-429c-a8ae-d7bb6c5fd8dd",
        "metadata": {
          "chunk_id": 16,
          "document_id": "2002.06644",
          "start_idx": 6017,
          "end_idx": 6467
        },
        "page_content": " along with a recurrent dropout of $0.2$ in two 64 unit sized stacked BiLSTMs, using softmax activation layer as the final dense layer. tab:experimental-results shows the performance of different models on the WNC corpus evaluated on the following four metrics: Precision, Recall, F1, and Accuracy. Our proposed methodology, the use of finetuned optimized BERT based models, and BERT-based ensemble models outperform the baselines for all the metrics",
        "type": "Document"
      },
      {
        "id": "6b7e8030-aa3f-4fb4-bece-117ccc021b08",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1910.07481",
          "start_idx": 9488,
          "end_idx": 9899
        },
        "page_content": " layers, $d_{ff} = 2048$ as the inner-layer dimensionality, $h = 8$ attention heads, $d_k = 64$ as queries and keys dimension and $Pdrop = 0.1$ as dropout probability. All experiments, including baselines, are run over 600k training steps with a batch size of approximately 3000 tokens. For all language pairs we trained a Baseline and a Document model. The Baseline is trained on a standard parallel corpus and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is increase in percentage of humor contained in headlines generated with TitleStylist method (w.r.t. baselines)?",
    "llm_answer": "Answer not found in the context.\n",
    "context": [
      {
        "id": "11ee0661-8f8b-418f-8a0b-aeddf8c82342",
        "metadata": {
          "chunk_id": 8,
          "document_id": "2004.01980",
          "start_idx": 2990,
          "end_idx": 3543
        },
        "page_content": " without any supervised style-specific article-headline paired data. Through both automatic and human evaluation, we demonstrated that our proposed TitleStylist can generate relevant, fluent headlines with three styles (humor, romance, and clickbait), and they are even more attractive than human-written ones. Our model can flexibly incorporate multiple styles, thus efficiently and automatically providing humans with various creative headline options for references and inspiring them to think out of the box. Our work is related to summarization and",
        "type": "Document"
      },
      {
        "id": "0bec2864-a155-41d4-9d65-7bb089955a88",
        "metadata": {
          "chunk_id": 7,
          "document_id": "2004.01980",
          "start_idx": 2673,
          "end_idx": 3201
        },
        "page_content": " the model on three tasks: humorous, romantic, and click-baity headline generation. Both automatic and human evaluations show that TitleStylist can generate headlines with the desired styles that appeal more to human readers, as in Figure FIGREF2. The main contributions of our paper are listed below: To the best of our knowledge, it is the first research on the generation of attractive news headlines with styles without any supervised style-specific article-headline paired data. Through both automatic and human evaluation,",
        "type": "Document"
      },
      {
        "id": "5452ee0a-c3bf-4e23-8ae4-3a773c5ce559",
        "metadata": {
          "chunk_id": 60,
          "document_id": "2004.01980",
          "start_idx": 20888,
          "end_idx": 21374
        },
        "page_content": "-weighting the original ones, which agrees with the fact that click-baity headlines are better at drawing readers' attention. To be noted, although we learned the \u201cClickbait\u201d style into our summarization system, we still made sure that we are generating relevant headlines instead of too exaggerated ones, which can be verified by our relevance scores. The human-annotated fluency scores in Table TABREF51 verified that our TitleStylist generated headlines are comparable or superior to",
        "type": "Document"
      },
      {
        "id": "8821f6ea-eb5b-45b6-a897-351140669949",
        "metadata": {
          "chunk_id": 59,
          "document_id": "2004.01980",
          "start_idx": 20574,
          "end_idx": 21076
        },
        "page_content": ") Our TitleStylist can generate more attractive headlines over the NHG and Multitask baselines for all three styles, demonstrating that adapting the model to these styles could improve the attraction and specialization of some parameters in the model for different styles can further enhance the attraction. (3) Adapting the model to the \u201cClickbait\u201d style could create the most attractive headlines, even out-weighting the original ones, which agrees with the fact that click-baity headlines are better",
        "type": "Document"
      },
      {
        "id": "765104d4-ee56-412b-a898-04d9d0d38793",
        "metadata": {
          "chunk_id": 5,
          "document_id": "2004.01980",
          "start_idx": 1921,
          "end_idx": 2391
        },
        "page_content": " of style in headline generation. We present a novel headline generation model, TitleStylist, to produce enticing titles with target styles including humorous, romantic, and click-baity. Our model leverages a multitasking framework to train both a summarization model on headline-article pairs, and a Denoising Autoencoder (DAE) on a style corpus. In particular, based on the transformer architecture BIBREF6, we use the style-dependent layer normalization and the style",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the languages they consider in this paper?",
    "llm_answer": "English, French, German, Spanish, Russian, Italian, Basque, Hungarian, Navajo, Turkish, Quechua, Dutch, Arabic, and Zulu,  Manipuri.\n",
    "context": [
      {
        "id": "7da029d3-0e6a-48d3-9dd2-3cfd37690294",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1910.04269",
          "start_idx": 6853,
          "end_idx": 7342
        },
        "page_content": " comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (",
        "type": "Document"
      },
      {
        "id": "e8fb7d0e-86e1-4332-bb4a-d8109e4146e4",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1910.04269",
          "start_idx": 19553,
          "end_idx": 20023
        },
        "page_content": ", the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Kat",
        "type": "Document"
      },
      {
        "id": "10b18f90-1375-4a41-9719-9a76bdd2287c",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1910.05456",
          "start_idx": 1974,
          "end_idx": 2423
        },
        "page_content": " on the L1, i.e., the \"native language\", in neural network models. To this goal, we select a diverse set of eight source languages from different language families \u2013 Basque, French, German, Hungarian, Italian, Navajo, Turkish, and Quechua \u2013 and three target languages \u2013 English, Spanish and Zulu. We pretrain a neural sequence-to-sequence architecture on each of the source languages and then fine-tune the resulting models on small datasets in each",
        "type": "Document"
      },
      {
        "id": "6bc2bb18-6a66-4628-bef7-a0e1a75c3767",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1909.13362",
          "start_idx": 15624,
          "end_idx": 16095
        },
        "page_content": " language groupings within families. We selected six evaluation languages: English, Dutch, Italian, French, Basque, and Manipuri. These represent two language families (Indo-European, Sino-Tibetan), a language isolate thought to be unrelated to any existing language (Basque), and two different subfamilies within the Indo-European family (West Germanic, Romance). The primary constraint was the availability of syllabified datasets for training and testing. Table TABREF",
        "type": "Document"
      },
      {
        "id": "7367d0f1-0e76-4843-a7a9-1611742d90a2",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1809.04686",
          "start_idx": 9144,
          "end_idx": 9613
        },
        "page_content": "40 chose a subset of the SNLI test set (1332 sentences) and professionally translated it into four major languages - Arabic, French, Russian, and Spanish. We use the French test set for evaluation in Section \"Zero-Shot Classification Results\" and \"Analyses\" . Here, we first describe the model and training details of the base multilingual NMT model whose encoder is reused in all other tasks. Then we provide details about the task-specific classifiers. For each task,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Did they experiment with tasks other than word problems in math?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "a81552e5-de8f-4ff3-9bba-b3048505764a",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1905.13497",
          "start_idx": 776,
          "end_idx": 1212
        },
        "page_content": " reasoning. Particularly, it is not clear whether BERT shed light on solving tasks such as the Pronoun Disambiguation Problem (PDP) and Winograd Schema Challenge (WSC). These tasks have been proposed as potential alternatives to the Turing Test, because they are formulated to be robust to statistics of word co-occurrence BIBREF4 . Below is a popular example from the binary-choice pronoun coreference problem BIBREF5 of WSC: Sentence:",
        "type": "Document"
      },
      {
        "id": "d217561f-fa6f-41a9-805d-e8059ae44c72",
        "metadata": {
          "chunk_id": 73,
          "document_id": "1910.00458",
          "start_idx": 25409,
          "end_idx": 25967
        },
        "page_content": " do math? To investigate this question, we sampled some arithmetic questions that are correctly predicted by our model, made small alterations to the passage or question, and then checked whether our model can still make correct choices. We found our model is very fragile to these minor alterations, implicating that the model is actually not that good at arithmetic problems. We provided one interesting example in the Section 3 of the Supplementary Material. There are increasing interests in machine reading comprehension (MRC) for question answering (QA",
        "type": "Document"
      },
      {
        "id": "ad8f57e6-9103-48d1-bcd3-77aa60670c8e",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1906.06448",
          "start_idx": 11534,
          "end_idx": 12014
        },
        "page_content": " the premise and/or the hypothesis is ungrammatical or does not make sense. Workers were paid US$0.04 for each question, and each question was assigned to three workers. To collect high-quality annotation results, we imposed ten test questions on each worker, and removed workers who gave more than three wrong answers. We also set the minimum time it should take to complete each question to 200 seconds. 1,237 workers contributed to this task, and we annotated gold labels of 15",
        "type": "Document"
      },
      {
        "id": "dff2affc-da5c-4a35-b103-f34de0b9910c",
        "metadata": {
          "chunk_id": 89,
          "document_id": "1807.03367",
          "start_idx": 33979,
          "end_idx": 34422
        },
        "page_content": "oriented dialogue research and situated navigation. Here, we describe related previous work in these areas. paragraph4 0.1ex plus0.1ex minus.1ex-1em Related tasks There has been a long line of work involving related tasks. Early work on task-oriented dialogue dates back to the early 90s with the introduction of the Map Task BIBREF11 and Maze Game BIBREF25 corpora. Recent efforts have led to larger-scale goal-oriented dialogue datasets, for",
        "type": "Document"
      },
      {
        "id": "a65816cb-79fa-4d16-9edd-0a230ce33eca",
        "metadata": {
          "chunk_id": 1,
          "document_id": "1605.08675",
          "start_idx": 494,
          "end_idx": 1010
        },
        "page_content": " in Natural Language Processing (NLP) community as it provides a way to employ numerous NLP tools in an exploitable end-user system. It has resulted in valuable contributions within TREC competitions BIBREF1 and, quite recently, in a system called IBM Watson BIBREF2 , successfully competing with humans in the task. However, the problem remains far from solved. Firstly, solutions designed for English are not always easily transferable to other languages with more complex syntax rules and less resources available",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What evaluation metrics are used?",
    "llm_answer": "Task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, F1-score, BPRA,  belief per-response accuracy.\n",
    "context": [
      {
        "id": "6b1e72f3-8ee5-43ac-a062-f08c158e3d67",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1709.10217",
          "start_idx": 6394,
          "end_idx": 6850
        },
        "page_content": " evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the number of total tasks. User satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied, neutral, satisfied and very satisfied, respectively. Response fluency: There are three scores -1, 0, 1, which indicate nonfluency, neutral, fluency. Number of dialogue turns: The number of utterances in a task",
        "type": "Document"
      },
      {
        "id": "d88a1149-0827-4f42-b9b1-620cfd250358",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1905.08949",
          "start_idx": 10602,
          "end_idx": 11043
        },
        "page_content": " they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks. As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE B",
        "type": "Document"
      },
      {
        "id": "b3036530-5228-47f4-b1f0-9a1687f209da",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1709.10217",
          "start_idx": 4141,
          "end_idx": 4657
        },
        "page_content": ", in which only the released data can be used for training and developing. The other is an open evaluation that allow to collect external data for training and developing. For task 1, we use F1-score as evaluation metric. For the task-oriented dialogue systems, the best way for evaluation is to use the online human-computer dialogue. After finishing an online human-computer dialogue with a dialogue system, the human then manually evaluate the system by using the metrics of user satisfaction degree, dialogue flu",
        "type": "Document"
      },
      {
        "id": "a669a759-4988-4843-98f8-c12f54d6dcaa",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1906.03538",
          "start_idx": 17710,
          "end_idx": 18208
        },
        "page_content": "6 and INLINEFORM7 , respectively. The metrics are averaged across all the perspectives INLINEFORM8 participating in the test set. The goal is to get estimates of the overall performance of the systems. Instead of creating a complex measure that would take all the aspects into account, we approximate the overall performance by multiplying the disjoint measures in INLINEFORM0 , INLINEFORM1 and INLINEFORM2 . While this gives an estimate on the overall quality, it ignores the pipeline structure of",
        "type": "Document"
      },
      {
        "id": "72b50da8-6c06-4809-91ff-95e6cb664a0d",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1909.09484",
          "start_idx": 15874,
          "end_idx": 16378
        },
        "page_content": " of the dialogue state tracker, it aims to show the impact of the dialogue state tracker on the dialogue policy maker; (2) the accuracy of dialogue policy maker, it aims to explain the performance of different methods of constructing dialogue policy; (3) the quality of the final response, it aims to explain the impact of the dialogue policy on the final dialogue response. The evaluation metrics are listed as follows: BPRA: Belief Per-Response Accuracy (BPRA) tests the ability to generate the correct",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What kind of Youtube video transcripts did they use?",
    "llm_answer": "Anonymized, transcribed Voice Search data.\n",
    "context": [
      {
        "id": "f7d07650-3cfe-48fe-9de2-f403ac8ce727",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1911.13087",
          "start_idx": 2266,
          "end_idx": 2709
        },
        "page_content": " includes three lines to indicate the possible pauses at the beginning and end of the sentences and also after each word. This file includes the list of files in which the narrated sentences have been recorded. The recorded files are in wav formats. However, in the file IDs, the extension is omitted. A sample of the file content is given below. The test directory is the directory in which the files are located. test/T1-1-50-01 test/T1-1-50",
        "type": "Document"
      },
      {
        "id": "3b77cde1-7c3c-44b6-9a21-2d367e0d3b85",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1911.13087",
          "start_idx": 3610,
          "end_idx": 4109
        },
        "page_content": " school in the Kurdistan Region of Iraq. The reason that we have taken only 200 sentences is to have a smaller dictionary and also to increase the repetition of each word in the narrated speech. We transformed the corpus sentences, which are in Persian-Arabic script, into the format which complies with the suggested phones for the related Sorani letters (see Section SECREF6). Two thousand narration files were created. We used Audacity to record the narrations. We used a normal laptop in a quiet",
        "type": "Document"
      },
      {
        "id": "41d905d3-bd50-45b8-8d2b-554d3a7f895f",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1909.06522",
          "start_idx": 8980,
          "end_idx": 9471
        },
        "page_content": "ated 10-second sound clips from YouTube videos BIBREF28. Note that in our video datasets, video lengths vary between 10 seconds and 5 minutes, with an average duration of about 2 minutes. Rather than constantly repeating the 10-second sound clip to match the original minute-long audio, we superpose each sound clip on the short utterances via audio segmentation. Specifically, we first use an initial bootstrap model to align each original long audio, and segment each audio into around 10-",
        "type": "Document"
      },
      {
        "id": "baebdcda-6a2e-453d-b187-3a74dffd3a8e",
        "metadata": {
          "chunk_id": 37,
          "document_id": "2002.11268",
          "start_idx": 10875,
          "end_idx": 11324
        },
        "page_content": "K, 1M and 21M utterance-level {audio, transcript} pairs taken from anonymized, transcribed Voice Search data. These fine-tuning sets roughly correspond to 10 hours, 100 hours, 1000 hours and 21,000 hours of audio, respectively. The following data sources were used to choose scaling factors and/or evaluate the final model performance. Source-domain Eval Set (YouTube). The in-domain performance of the YouTube-trained RNN-T baseline was measured on",
        "type": "Document"
      },
      {
        "id": "20df2585-9cf0-49d9-99da-00c30040240d",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1604.05781",
          "start_idx": 4432,
          "end_idx": 4838
        },
        "page_content": " BIBREF29 . A tweet will be referred to as a `document' for the rest of this work. All document text was processed the same way. Punctuation, XML characters, and hyperlinks were removed, as were Twitter-specific \u201cat-mentions\u201d and \u201chashtags\u201d (see also the Appendix). There is useful information here, but it is either not natural language text, or it is Twitter-specific, or both. Documents were broken into",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What makes it a more reliable metric?",
    "llm_answer": "It has an interpretable meaning: the expected error made by a given metric as compared to the human rating.\n",
    "context": [
      {
        "id": "85a66ada-4302-4eda-8b84-478df80153a6",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1809.08731",
          "start_idx": 12336,
          "end_idx": 12724
        },
        "page_content": "$ , it is a non-symmetric quasi-metric. We apply it with $Y$ being the human ratings. An additional advantage as compared to Pearson is that it has an interpretable meaning: the expected error made by a given metric as compared to the human rating. As shown in Table 3 , WordSLOR and WPSLOR correlate best with human judgments: WordSLOR (respectively WPSLOR) has a $0.025$ (respectively $",
        "type": "Document"
      },
      {
        "id": "1f5fe908-54f7-4a13-a408-734b1e7a226b",
        "metadata": {
          "chunk_id": 9,
          "document_id": "2001.06354",
          "start_idx": 3605,
          "end_idx": 4142
        },
        "page_content": " model's generalization ability (because it allows multiple similar answers), while the other metrics measure a model's preciseness, we interpret the results of these above experiments to mean that a model with more history information tends to predict correct answers by memorizing keywords or patterns in the history while a model with less history information (i.e., the image-only model) is better at generalization by avoiding relying on such exact-match extracted information. We think that an ideal model should have more balanced",
        "type": "Document"
      },
      {
        "id": "79eb9a70-87b9-4874-b3c5-81af4b58cf5a",
        "metadata": {
          "chunk_id": 35,
          "document_id": "2001.09332",
          "start_idx": 12665,
          "end_idx": 13168
        },
        "page_content": " related to one or more words absent from the vocabulary). As it can be seen, regardless of the metric used, our model has significantly better results than the other two models, both overall and within the two macro-areas. Furthermore, the other two models seem to be more subject to the metric used, perhaps due to a stabilization not yet reached for the few training epochs. For a complete comparison, both models were also tested considering only the subset of the analogies in common with our model",
        "type": "Document"
      },
      {
        "id": "51b1e3f8-9b61-465f-b0e8-a9b24dbb7303",
        "metadata": {
          "chunk_id": 93,
          "document_id": "2002.02492",
          "start_idx": 29289,
          "end_idx": 29596
        },
        "page_content": " perplexity. When $\\epsilon $ is too large ($\\epsilon =10^{-2}$), perplexity degrades. When $\\epsilon $ is too small ($\\epsilon =10^{-4}$), the lower-bound grows slowly, so $\\left<\\text{eos}\\right>$ is not guaranteed to be top-ranked within $L$ steps, and the metrics resemble the baseline's. An $\\epsilon $",
        "type": "Document"
      },
      {
        "id": "1d812102-98f4-45bc-b355-c1653ccfaf7e",
        "metadata": {
          "chunk_id": 72,
          "document_id": "1809.08731",
          "start_idx": 24617,
          "end_idx": 25067
        },
        "page_content": " metrics on a benchmark dataset. Additionally, they can be applied even in settings where no references are available, or would be costly to obtain. Finally, for given references, we proposed the reference-based metric ROUGE-LM, which consists of a combination of WPSLOR and ROUGE. Thus, we were able to obtain an even more accurate fluency evaluation. We would like to thank Sebastian Ebert and Samuel Bowman for their detailed and helpful feedback.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much in experiments is performance improved for models trained with generated adversarial examples?",
    "llm_answer": "The performance of all target models raises significantly after adversarial training.\n",
    "context": [
      {
        "id": "c2fb5d70-19e9-4567-a481-45842aab191b",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1909.02560",
          "start_idx": 24644,
          "end_idx": 25212
        },
        "page_content": "ially trained models, as shown in Table TABREF18. After adversarial training, the performance of all the target models raises significantly, while that on the original examples remain comparable. Note that since the focus of this paper is on model robustness which can hardly be reflected in original data, we do not expect performance improvement on original data. The results demonstrate that adversarial training with our adversarial examples can significantly improve the robustness we focus on without remarkably hurting the performance on original data. Moreover",
        "type": "Document"
      },
      {
        "id": "ddb2671f-d3dd-445e-a4db-ad7cd92a31b1",
        "metadata": {
          "chunk_id": 75,
          "document_id": "1909.02560",
          "start_idx": 29007,
          "end_idx": 29299
        },
        "page_content": " are much less affected and the modified sentences retain a good grammarticality. We also show that model robustness can be improved using adversarial training with our adversarial examples. Moreover, our adversarial examples can foster future research for further improving model robustness.",
        "type": "Document"
      },
      {
        "id": "9a1da164-7563-47d0-9069-4cf6a7d61b22",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1909.02560",
          "start_idx": 21491,
          "end_idx": 22071
        },
        "page_content": " robustness issues. Moreover, we present some generated adversarial examples in the appendix. We notice that the original models are more vulnerable to unmatched adversarial examples, because there are generally more replaceable position choices during the generation. Nevertheless, the results of the matched case are also sufficiently strong to reveal the robustness issues. We do not quantitatively compare the performance drop of the target models on the adversarial examples with previous work, because we generate a new type of adversarial examples that previous methods are",
        "type": "Document"
      },
      {
        "id": "cbe21d1b-d237-4772-a063-0a45df0c10e0",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1909.02560",
          "start_idx": 22071,
          "end_idx": 22669
        },
        "page_content": "arial examples with previous work, because we generate a new type of adversarial examples that previous methods are not capable of. We have different experiment settings, including original example sampling and constraints on adversarial modifications, which are tailored to the robustness issues we study. Performance drop on different kinds of adversarial examples with little overlap is not comparable, and thus surpassing other adversarial examples on model performance drop is unnecessary and irrelevant to support our contributions. Therefore, such comparisons are not included in this paper.",
        "type": "Document"
      },
      {
        "id": "90a9a850-775a-479f-a9f3-99dbdd7c83f4",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1909.02560",
          "start_idx": 23850,
          "end_idx": 24336
        },
        "page_content": "arial examples is also small, suggesting that most adversarial examples retain a good grammaticality. This verifies the validity of our adversarial examples. Adversarial training can often improve model robustness BIBREF25, BIBREF27. We also fine-tune the target models using adversarial training. At each training step, we train the model with a batch of original examples along with adversarial examples with balanced labels. The adversarial examples account for around 10% in a batch",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the delta-softmax calculated?",
    "llm_answer": "$\\Delta_s$ (delta-softmax) is the softmax probability of the correct relation for the EDU pair.\n",
    "context": [
      {
        "id": "21e0fbbb-f245-470d-bdad-9312b6c3b4db",
        "metadata": {
          "chunk_id": 112,
          "document_id": "2001.02380",
          "start_idx": 42766,
          "end_idx": 43134
        },
        "page_content": " in softmax probability of the correct relation as our signaling strength metric for the model. We call this metric ${\\Delta }_s$ (for delta-softmax), which can be written as: where $rel$ is the true relation of the EDU pair, $t_i$ represents the token at index $i$ of $N$ tokens, and $X_{mask=i}$ represents the input sequence with the masked position $i$ (for $i \\in",
        "type": "Document"
      },
      {
        "id": "6073a498-3e36-460b-9a11-e8d183cb73cb",
        "metadata": {
          "chunk_id": 40,
          "document_id": "1908.06606",
          "start_idx": 14117,
          "end_idx": 14461
        },
        "page_content": "_s$ denotes the length of sequence. Then we permute the two dimensions for softmax calculation. The calculation process of loss function can be defined as followed. where $O_s = softmax(permute(H_f)_0)$ denotes the probability score of each word to be the start word and similarly $O_e = softmax(permute(H_f)_1)$ denotes the end. $y_s$ and $y_e",
        "type": "Document"
      },
      {
        "id": "ea07dd43-9fa3-42ed-bbca-056cdfa113ae",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1909.00154",
          "start_idx": 12833,
          "end_idx": 13177
        },
        "page_content": ") for the softmax layer, so $B_c$ is simply the coefficients (row) vector for output class (alternative) $c$, and $\\alpha _c$ is the corresponding intercept. The typical loss function used in such models is called the categorical cross entropy: Where $\\delta _{i}$ is the kronecker delta ($\\delta _{true}=1; \\delta _{false}=0$), and $\\mathcal {",
        "type": "Document"
      },
      {
        "id": "4a0a9a6e-8bee-41d4-a41a-e9427e87d4e5",
        "metadata": {
          "chunk_id": 47,
          "document_id": "2001.05493",
          "start_idx": 16703,
          "end_idx": 16976
        },
        "page_content": "y_{i1}^*$ be the softmax probabilities, specifically for class label $k$ is given as: where $K$ is the number of classes, $W_{dpcnn}$ and $b_{dpcnn}$ are the weight matrix and bias respectively. Given a sequence $s_i = [x_{1}, x_{2}, x_{3},....x_{n}]$ where $x_{j} \\in R^d$",
        "type": "Document"
      },
      {
        "id": "888a80e2-48d9-49f6-b650-039190c50a16",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1909.13362",
          "start_idx": 13130,
          "end_idx": 13483
        },
        "page_content": " for each $o_i$: The softmax normalizes each $o_i$ to a probability distribution over the two discrete class labels. We can then model $p(y|o)$ by multiplying the maximum of each $s_i$ together: When using the softmax, $p(y|o)$ is calculated under the limiting assumption that each $o_i$ is independent. To more accurately model $p(y|o)$, we replace the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which two datasets does the resource come from?",
    "llm_answer": "REF0 and ASQ\n",
    "context": [
      {
        "id": "e8b23b74-a88e-479c-8b7b-d192a95ec8c1",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1901.02257",
          "start_idx": 6056,
          "end_idx": 6539
        },
        "page_content": "REF0 dataset concerns answering the question which requires using commonsense knowledge. including Wikipedia articles, examinations, narrative stories, news articles. Answering questions in these datasets. Meanwhile, the question types and answer types vary differently. The answer type multiple choice, span-answer, exact match Many architectures on MRC follow the process of representation, attention, fusion, and aggregation BIBREF12 , BIBREF2 , BIBREF13 , BIBREF14 , BIBREF15 , B",
        "type": "Document"
      },
      {
        "id": "5aae2354-e125-4179-9260-e8eac191d87c",
        "metadata": {
          "chunk_id": 99,
          "document_id": "1701.00185",
          "start_idx": 35418,
          "end_idx": 35678
        },
        "page_content": "ASQ for making the datasets available. This work is supported by the National Natural Science Foundation of China (No. 61602479, No. 61303172, No. 61403385) and the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDB02070005).",
        "type": "Document"
      },
      {
        "id": "75c8f911-af20-49c1-8b12-e4aecd8370c1",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1707.03904",
          "start_idx": 5304,
          "end_idx": 5850
        },
        "page_content": " subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short. We plan to release these annotations along with the datasets, and our retrieved documents for each question. Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated",
        "type": "Document"
      },
      {
        "id": "763eee49-3f73-492a-b846-25f755d86b8f",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1711.11221",
          "start_idx": 6905,
          "end_idx": 7300
        },
        "page_content": " in the target-side vocabulary of RNNSearch*. In order to build the topic cache, we first use an off-the-shelf LDA topic tool to learn topic distributions of source- and target-side documents separately. Then we estimate a topic projection distribution over all target-side topics $p(z_t|z_s)$ for each source topic $z_s$ by collecting events and accumulating counts of $(z_s, z_t)$ from aligned",
        "type": "Document"
      },
      {
        "id": "f420687c-1cd6-4839-be87-8508949281e2",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1707.03904",
          "start_idx": 14370,
          "end_idx": 14849
        },
        "page_content": "\ufffdopen-book\u201d setting, where they had access to a search engine over the short pseudo-documents extracted for each dataset (as described in Section \"Context Retrieval\" ). We decided to use short pseudo-documents for this exercise to reduce the burden of reading on the volunteers, though we note that the long pseudo-documents have greater coverage of answers. We also asked the volunteers to provide annotations to categorize the type of each question they were asked, and a label",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the size of the second dataset?",
    "llm_answer": "141 symptom-prescription pairs\n",
    "context": [
      {
        "id": "84b1300d-32c7-49fd-ba72-e11fccc22e88",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1801.09030",
          "start_idx": 17043,
          "end_idx": 17552
        },
        "page_content": " lower quality, as this dataset was parsed with simple rules, which may not cover all exceptions. The second test dataset has been proofread and all of the prescriptions are the most classical and influential ones in the history. So the quality is much better than the first one. However, the number of the cases is limited. There are 141 symptom-prescription pairs in the second dataset. Thus we use two test sets to do evaluation to take the advantages of both data magnitude and quality. In our experiments",
        "type": "Document"
      },
      {
        "id": "95c62085-b478-4b5e-a4fa-049a6c8fc5d0",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1908.10449",
          "start_idx": 22880,
          "end_idx": 23381
        },
        "page_content": " hard time learning in this setting. As shown in Table TABREF16, both datasets have a vocabulary size of more than 100k. This is much larger than in the other two settings, where on average the length of questions is around 10. This suggests that the methods with better sample efficiency are needed to act in more realistic problem settings with huge action spaces. Experiments also show that a larger memory slot size always helps. Intuitively, with a memory mechanism (either implicit or explicit),",
        "type": "Document"
      },
      {
        "id": "cc9faf95-ff74-4532-ad40-1937aafe18a1",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1707.03904",
          "start_idx": 12758,
          "end_idx": 13269
        },
        "page_content": " to evaluate the performance of the reading system independently from the search system, while the full set allows us to evaluate the performance of Quasar as a whole. We also split the full set into training, validation and test sets. The final size of each data subset after all discards is listed in Table 1 . Evaluation is straightforward on Quasar-S since each answer comes from a fixed output vocabulary of entities, and we report the average accuracy of predictions as the evaluation metric. For Quasar-T",
        "type": "Document"
      },
      {
        "id": "95bf2b84-1a56-4eed-b193-4a34d2c76c4c",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1706.08032",
          "start_idx": 14264,
          "end_idx": 14661
        },
        "page_content": " datasets in Table III. for all datasets, the filter window size ( INLINEFORM0 ) is 7 with 6 feature maps each for the first wide convolution layer, the second wide convolution layer has a filter window size of 5 with 14 feature maps each. Dropout rate ( INLINEFORM1 ) is 0.5, INLINEFORM2 constraint, learning rate is 0.1 and momentum of 0.9. Mini-batch size for STS Corpus is 100 and others are 4",
        "type": "Document"
      },
      {
        "id": "ed0bc156-89c2-4b94-9e58-2194484d059d",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1709.05404",
          "start_idx": 17579,
          "end_idx": 18022
        },
        "page_content": "-validation setup) for each dataset while holding the number of posts per class constant. Figure FIGREF26 shows the performance of each of the Gen, RQ, and Hyp datasets at intervals of 100 posts per class (up to the maximum size of 582 posts per class for Hyp, and 851 posts per class for RQ). From the graph, we can see that as a general trend, the datasets benefit from larger dataset sizes. Interestingly, the results for the RQ dataset are",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How large is the first dataset?",
    "llm_answer": "287,226 documents\n",
    "context": [
      {
        "id": "95c62085-b478-4b5e-a4fa-049a6c8fc5d0",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1908.10449",
          "start_idx": 22880,
          "end_idx": 23381
        },
        "page_content": " hard time learning in this setting. As shown in Table TABREF16, both datasets have a vocabulary size of more than 100k. This is much larger than in the other two settings, where on average the length of questions is around 10. This suggests that the methods with better sample efficiency are needed to act in more realistic problem settings with huge action spaces. Experiments also show that a larger memory slot size always helps. Intuitively, with a memory mechanism (either implicit or explicit),",
        "type": "Document"
      },
      {
        "id": "9ccb7945-9a96-417a-b52e-68855c4910b1",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "ed0bc156-89c2-4b94-9e58-2194484d059d",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1709.05404",
          "start_idx": 17579,
          "end_idx": 18022
        },
        "page_content": "-validation setup) for each dataset while holding the number of posts per class constant. Figure FIGREF26 shows the performance of each of the Gen, RQ, and Hyp datasets at intervals of 100 posts per class (up to the maximum size of 582 posts per class for Hyp, and 851 posts per class for RQ). From the graph, we can see that as a general trend, the datasets benefit from larger dataset sizes. Interestingly, the results for the RQ dataset are",
        "type": "Document"
      },
      {
        "id": "2d3bdfec-4ed5-456d-bd82-023fad798858",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "cc9faf95-ff74-4532-ad40-1937aafe18a1",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1707.03904",
          "start_idx": 12758,
          "end_idx": 13269
        },
        "page_content": " to evaluate the performance of the reading system independently from the search system, while the full set allows us to evaluate the performance of Quasar as a whole. We also split the full set into training, validation and test sets. The final size of each data subset after all discards is listed in Table 1 . Evaluation is straightforward on Quasar-S since each answer comes from a fixed output vocabulary of entities, and we report the average accuracy of predictions as the evaluation metric. For Quasar-T",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Who was the top-scoring team?",
    "llm_answer": "AM\n",
    "context": [
      {
        "id": "e5a534fd-34ac-420d-8a2e-47e201636197",
        "metadata": {
          "chunk_id": 134,
          "document_id": "2001.11381",
          "start_idx": 30919,
          "end_idx": 31185
        },
        "page_content": "imizaci\u00f3n-maximizaci\u00f3n) para calcular un score $S_i$, seg\u00fan la ecuaci\u00f3n (DISPLAY_FORM36): Mientras m\u00e1s elevado sea el valor $S_i$, mejor obedece a nuestros objetivos: acercarse al $query$ y alejarse de la sem\u00e1ntica original. Finalmente ordenamos en forma decreciente",
        "type": "Document"
      },
      {
        "id": "7949fac7-20d1-429d-9eda-4d2a4691114c",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1707.03764",
          "start_idx": 14249,
          "end_idx": 14661
        },
        "page_content": "AM came first in the shared task, with a score of 0.8253 for gender 0.9184 for variety, a joint score of 0.8361 and an average score of 0.8599 (final rankings were taken from this average score BIBREF0 ). For the global scores, all languages are combined. We present finer-grained scores showing the breakdown per language in Table TABREF24 . We compare our gender and variety accuracies against the LDR-baseline",
        "type": "Document"
      },
      {
        "id": "3d8d0032-45f5-40f2-8099-676e3c20b038",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1704.05572",
          "start_idx": 4739,
          "end_idx": 5005
        },
        "page_content": "it {tf-idf}(t, q)=\\sum _{x \\in t\\cap q} idf(x)\n$   where $N$ is the number of tuples in the KB and $n_x$ are the number of tuples containing $x$ . We normalize the tf-idf score by the number of tokens in $t$ and $q$ . We finally take the 50 top-scoring tuples $T_{qa",
        "type": "Document"
      },
      {
        "id": "56291e55-f14f-457b-9a4d-93e12da7cabb",
        "metadata": {
          "chunk_id": 94,
          "document_id": "1905.00563",
          "start_idx": 29074,
          "end_idx": 29259
        },
        "page_content": "1-) (Hs + (1-) r',or',o)-1 r',o) r,o and then calculate the change in the score as: (s,r,o)-(s, r, o)=  s,r.(-) +(-).r,o =  s,r ((1-) (Ho + (1-) s,r's,r')-1 s,r')+  ((1-) (Hs + (1-) r',",
        "type": "Document"
      },
      {
        "id": "21414e1f-c4d1-4401-9868-1399268a2874",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1601.06068",
          "start_idx": 16772,
          "end_idx": 17063
        },
        "page_content": " the knowledge base). Following reddylargescale2014, we use beam search to find the highest scoring tuple of paraphrase, ungrounded and grounded graphs $(\\hat{p}, \\hat{u}, \\hat{g})$ under the model $\\theta \\in \\mathbb {R}^n$ : $\n({\\hat{p},\\hat{u},\\hat{g}}) = \\operatornamewithlimits{arg\\,max",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Why is big data not appropriate for this task?",
    "llm_answer": "Big datasets are not always available, particularly in computational social science NLP applications where the data of interest are often not obtained from large-scale sources.  Also, the size of a dataset does not correspond to its degree of relevance for a particular analysis; even very large corpora can have idiosyncrasies.\n",
    "context": [
      {
        "id": "1fca2be8-e4e6-4c72-aa39-a3d3dc4a2200",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1705.07368",
          "start_idx": 1597,
          "end_idx": 2093
        },
        "page_content": " I offer a somewhat contrarian perspective to the currently prevailing trend of big data optimism, as exemplified by the work of BIBREF0 , BIBREF1 , BIBREF3 , and others, who argue that massive datasets are sufficient to allow language models to automatically resolve many challenging NLP tasks. Note that \u201cbig\u201d datasets are not always available, particularly in computational social science NLP applications, where the data of interest are often not obtained from large scale sources such as the",
        "type": "Document"
      },
      {
        "id": "72439b3f-a2d7-4aea-91c2-a626e163fd50",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1705.07368",
          "start_idx": 1280,
          "end_idx": 1811
        },
        "page_content": ". Word embeddings have risen in popularity for NLP applications due to the success of models designed specifically for the big data setting. In particular, BIBREF0 , BIBREF1 showed that very simple word embedding models with high-dimensional representations can scale up to massive datasets, allowing them to outperform more sophisticated neural network language models which can process fewer documents. In this work, I offer a somewhat contrarian perspective to the currently prevailing trend of big data optimism, as exemplified",
        "type": "Document"
      },
      {
        "id": "d6f85df5-a5be-455d-9de2-cb19a173c3e0",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1812.07023",
          "start_idx": 19554,
          "end_idx": 19699
        },
        "page_content": " intersection between language and video are currently constrained by the quality of video features, and emphasizes the need for larger datasets.",
        "type": "Document"
      },
      {
        "id": "73aa34ac-b7ad-4c92-b1ed-0a0ea90e8191",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1705.07368",
          "start_idx": 10677,
          "end_idx": 11190
        },
        "page_content": " following reasons: Of course, the model also requires some new parameters to be learned, namely the mixed membership proportions INLINEFORM0 . Based on topic modeling, I hypothesized that with care, these added parameters need not adversely affect performance in the small-medium data regime, for two reasons: 1) we can use a Bayesian approach to effectively manage uncertainty in them, and to marginalize them out, which prevents them being a bottleneck during training; and 2) at test time, using the posterior",
        "type": "Document"
      },
      {
        "id": "98e3bc53-9fe9-430c-8a89-b0d23750508b",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1705.07368",
          "start_idx": 2340,
          "end_idx": 2763
        },
        "page_content": " embedding models on a generic large corpus such as Wikipedia, and use the embeddings for NLP tasks on the target dataset, cf. BIBREF3 , BIBREF0 , BIBREF16 , BIBREF17 . However, as we shall see here, this standard practice might not always be effective, as the size of a dataset does not correspond to its degree of relevance for a particular analysis. Even very large corpora have idiosyncrasies that can make their embedd",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is an example of a computational social science NLP task?",
    "llm_answer": "Answer not found in the context.\n",
    "context": [
      {
        "id": "ed73ee41-ea6c-4512-9028-8bb5ae085912",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1901.02257",
          "start_idx": 1344,
          "end_idx": 1854
        },
        "page_content": "erences from multiple sentences from the passage and requireswith the commonsense knowledge that does not appear in the passage explicitly. Table 1 shows an example of CRC. Content: Previous Research 1. Category the methods in SemEval2018 task 11 2. Describe the first method 3. Describe the second method 4. State that your work is belong to which method Most studies on CRC task are neural network based (NN-based) models, which typically have the following characteristics. Firstly, word representations are",
        "type": "Document"
      },
      {
        "id": "d8cde9ab-6a82-4343-b3e3-beb878751b60",
        "metadata": {
          "chunk_id": 1,
          "document_id": "1803.09230",
          "start_idx": 578,
          "end_idx": 1094
        },
        "page_content": ", research in the field of Natural Language Processing (NLP) has massively benefited from neural architectures. Those approaches have outperformed former state-of-the-art non-neural machine learning model families while needing far less human intervention since they don't require any manual feature engineering. A subset of NLP research focuses on building systems that are able to answer questions about a given document. To jointly expand the current best practice, the Stanford Question Answering Dataset (SQuAD)",
        "type": "Document"
      },
      {
        "id": "5ad1e594-5ff9-48ac-b9b9-7c105b6f7eda",
        "metadata": {
          "chunk_id": 4,
          "document_id": "2001.00137",
          "start_idx": 1748,
          "end_idx": 2222
        },
        "page_content": " Language Processing (NLP) domain. Tasks such as text generation, sentence correction, image captioning and text classification, have been possible via models such as Convolutional Neural Networks and Recurrent Neural Networks BIBREF5, BIBREF6, BIBREF7. More recently, state-of-the-art results have been achieved with attention models, more specifically Transformers BIBREF8. Surprisingly, however, there is currently no research on incomplete text classification in the NLP",
        "type": "Document"
      },
      {
        "id": "3090caa2-641a-44fc-adbf-df390d6cafa5",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1905.08949",
          "start_idx": 5794,
          "end_idx": 6247
        },
        "page_content": " be further explored. The change of learning paradigm in NQG era is also represented by multi-task learning with other NLP tasks, for which we discuss in Section \"Multi-task Learning\" . Question generation is an NLG task for which the input has a wealth of possibilities depending on applications. While a host of input modalities have been considered in other NLG tasks, such as text summarization BIBREF24 , image captioning BIBREF25 and table-to-text",
        "type": "Document"
      },
      {
        "id": "42001090-59a7-45a8-b27c-9455c2ef401f",
        "metadata": {
          "chunk_id": 1,
          "document_id": "1910.05456",
          "start_idx": 424,
          "end_idx": 880
        },
        "page_content": " whether it is constraints imposed by the phonological or by the morphosyntactic attributes of the L1 that are more important during the process of learning an L2's morphosyntax. Within the area of natural language processing (NLP) research, experimenting on neural network models just as if they were human subjects has recently been gaining popularity BIBREF3, BIBREF4, BIBREF5. Often, so-called probing tasks are used, which require a specific subset of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which major geographical regions are studied?",
    "llm_answer": "Latin America and the Caribbean (LCN), South Asia (SAS), Sub-Saharan Africa (SSA), Europe and Central Asia (ECS), Middle East and North Africa (MEA), East Asia and the Pacific (EAS), North America (NAC).\n",
    "context": [
      {
        "id": "6db039d4-5797-4496-864b-4014c7c76a94",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1708.05873",
          "start_idx": 17963,
          "end_idx": 18387
        },
        "page_content": " of regions: Latin America and the Caribbean (LCN), South Asia (SAS), Sub-Saharan Africa (SSA), Europe and Central Asia (ECS), Middle East and North Africa (MEA), East Asia and the Pacific (EAS), North America (NAC). The figure shows that states in South Asia, and Latin America and the Caribbean are likely to discuss Topic 2 the most. States in South Asia and East Asia and the Pacific discuss Topic 7 the most. The figure",
        "type": "Document"
      },
      {
        "id": "75339e0c-cf77-412a-9bc4-ba5062f2b7cc",
        "metadata": {
          "chunk_id": 21,
          "document_id": "1809.02494",
          "start_idx": 8157,
          "end_idx": 8743
        },
        "page_content": " highlight the creation of models of geographical descriptors, comparing models between both subject groups, studying combinations of models of cardinal directions, and researching on geographical referring expression generation. Furthermore, insights about the semantics of geographical concepts could be inferred under a more thorough analysis. One of the inconveniences that our data sets present is the appearance of the issues described in Sec. SECREF10 . It could be necessary to filter some of the answers according to different criteria (e.g., deviation of the centroid location",
        "type": "Document"
      },
      {
        "id": "5bcc41f0-dd05-4313-bee9-eaf8b1407eea",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1809.02494",
          "start_idx": 1540,
          "end_idx": 2054
        },
        "page_content": " However, we believe it can be useful for other related research purposes as well. The resource is composed of data from two different surveys. In both surveys subjects were asked to draw on a map (displayed under a Mercator projection) a polygon representing a given geographical descriptor, in the context of the geography of Galicia in Northwestern Spain (see Fig. FIGREF1 ). However, the surveys were run with different purposes, and the subject groups that participated in each survey and the list of descript",
        "type": "Document"
      },
      {
        "id": "219ea24a-3f7e-4950-9c1b-d3b6ff08005c",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1809.02494",
          "start_idx": 9296,
          "end_idx": 9658
        },
        "page_content": " cardinal points, coastal areas, etc. The proposal of a shared task could help achieve this objective. This research was supported by the Spanish Ministry of Economy and Competitiveness (grants TIN2014-56633-C3-1-R and TIN2017-84796-C2-1-R) and the Galician Ministry of Education (grants GRC2014/030 and \"accreditation 2016-2019, ED431G/08\"). All grants were co-",
        "type": "Document"
      },
      {
        "id": "46161b79-8747-4928-8141-f5a96b4e8246",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1809.02494",
          "start_idx": 4387,
          "end_idx": 4877
        },
        "page_content": " the survey. Comparing the visualizations of both data sets we have observed that this is clearly the case; the polygons drawn by the experts are more concentrated and therefore there is a higher agreement among them. On top of these differences, some students provided unexpected drawings in terms of shape, size, or location of the polygon for several descriptors. If we focus on single descriptors, one interesting outcome is that some of the answers for \u201cNorthern Galicia\u201d and \u201cSouthern",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How strong is the correlation between the prevalence of the #MeToo movement and official reports [of sexual harassment]?",
    "llm_answer": "The change in the number of reported cases constitutes a considerable change in the number of #MeToo users on Twitter.  (p-value is extremely small at 5.7e-13).\n",
    "context": [
      {
        "id": "aaa3263f-11ec-45d1-94c4-6a3802bc750d",
        "metadata": {
          "chunk_id": 2,
          "document_id": "2001.05970",
          "start_idx": 848,
          "end_idx": 1382
        },
        "page_content": " sexual harassment risk of the entire student body. Alternatively, social media opens up new opportunities to gather a larger and more comprehensive amount of data and mitigate the risk of false or inaccurate narratives from the studied subjects. On October 15 of 2017, prominent Hollywood actress Alyssa Milano, by accusing Oscar-winning film producer, Harvey Weinstein, for multiple sexual impropriety attempts on herself and many other women in the film industry, ignited the \"MeToo\" trend on social media that called for women and",
        "type": "Document"
      },
      {
        "id": "3b39843a-45e0-43bf-80b1-f67ea3fada83",
        "metadata": {
          "chunk_id": 4,
          "document_id": "2001.05970",
          "start_idx": 1672,
          "end_idx": 2274
        },
        "page_content": " propose a new approach to characterizing sexual harassment by mining the tweets from college users with the hash-tag #metoo on Twitter. Our main contributions are several folds. We investigate campus sexual harassment using a big-data approach by collecting data from Twitter. We employ traditional topic modeling and linear regression methods on a new dataset to highlight patterns of the ongoing troubling social behaviors at both institutional and individual levels. We propose a novel approach to combining domain-general deep semantic parsing and sentiment analysis to dissect personal narratives",
        "type": "Document"
      },
      {
        "id": "6876dd2b-ef88-4f9d-b348-0257b2d4fa5b",
        "metadata": {
          "chunk_id": 34,
          "document_id": "2001.05970",
          "start_idx": 13447,
          "end_idx": 13945
        },
        "page_content": " will further examine the emotion features in the latter results. Observing the results of the linear regression in Table 2, we find the normalized governmental reported cases count and regional feature to be statistically significant on the sexual harassment rate in the Twitter data ($p-value<0.05$). Specifically, the change in the number of reported cases constitutes a considerable change in the number of #MeToo users on Twitter as p-value is extremely small at $5.7e-13$. This corresponds to",
        "type": "Document"
      },
      {
        "id": "a73fb3ae-6a2d-4d5b-898c-362a141f8233",
        "metadata": {
          "chunk_id": 32,
          "document_id": "2001.05970",
          "start_idx": 12717,
          "end_idx": 13188
        },
        "page_content": " of LDA on #MeToo tweets of college users (Table 1) fall into the same pattern as the research of Modrek and Chakalov (2019), which suggests that a large portion of #MeToo tweets on Twitter focuses on sharing personal traumatic stories about sexual harassment BIBREF10. In fact, in our top 5 topics, Topics 1 and 5 mainly depict gruesome stories and childhood or college time experience. This finding seems to support the validity of the Twitter sample of Modrek and Chak",
        "type": "Document"
      },
      {
        "id": "6dd81658-d3cf-42fd-90af-a3c59a3c1b46",
        "metadata": {
          "chunk_id": 3,
          "document_id": "2001.05970",
          "start_idx": 1382,
          "end_idx": 1876
        },
        "page_content": " in the film industry, ignited the \"MeToo\" trend on social media that called for women and men to share their own sexual harassment experience. According to CNN, over 1.7 million users had used the hash-tag in 85 countries. Benefiting from the tremendous amount of data supplied by this trend and the existing state-of-the-art semantic parser and generative statistical models, we propose a new approach to characterizing sexual harassment by mining the tweets from college users with the hash-",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are the topics embedded in the #MeToo tweets extracted?",
    "llm_answer": "Latent Dirichlet Allocation (LDA) is used to label universal topics.  The optimal topic number is chosen based on the highest coherence score.\n",
    "context": [
      {
        "id": "13d5ad2c-fcb0-43da-84d6-ff4c8e16fd19",
        "metadata": {
          "chunk_id": 20,
          "document_id": "2001.05970",
          "start_idx": 8131,
          "end_idx": 8655
        },
        "page_content": ". We determine the optimal topic number by selecting the one with the highest coherence score. Since certain words frequently appear in those #MeToo tweets (e.g., sexual harassment, men, women, story, etc.), we transform our corpus using TF-IDF, a term-weighting scheme that discounts the influence of common terms. Learning deep meaning representations, which enables the preservation of rich semantic content of entities, meaning ambiguity resolution and partial relational understanding of texts, is one of the challenges",
        "type": "Document"
      },
      {
        "id": "20df2585-9cf0-49d9-99da-00c30040240d",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1604.05781",
          "start_idx": 4432,
          "end_idx": 4838
        },
        "page_content": " BIBREF29 . A tweet will be referred to as a `document' for the rest of this work. All document text was processed the same way. Punctuation, XML characters, and hyperlinks were removed, as were Twitter-specific \u201cat-mentions\u201d and \u201chashtags\u201d (see also the Appendix). There is useful information here, but it is either not natural language text, or it is Twitter-specific, or both. Documents were broken into",
        "type": "Document"
      },
      {
        "id": "a73fb3ae-6a2d-4d5b-898c-362a141f8233",
        "metadata": {
          "chunk_id": 32,
          "document_id": "2001.05970",
          "start_idx": 12717,
          "end_idx": 13188
        },
        "page_content": " of LDA on #MeToo tweets of college users (Table 1) fall into the same pattern as the research of Modrek and Chakalov (2019), which suggests that a large portion of #MeToo tweets on Twitter focuses on sharing personal traumatic stories about sexual harassment BIBREF10. In fact, in our top 5 topics, Topics 1 and 5 mainly depict gruesome stories and childhood or college time experience. This finding seems to support the validity of the Twitter sample of Modrek and Chak",
        "type": "Document"
      },
      {
        "id": "a0f3b956-dcec-4d77-8043-43c54cd2f633",
        "metadata": {
          "chunk_id": 19,
          "document_id": "2001.05970",
          "start_idx": 7862,
          "end_idx": 8316
        },
        "page_content": " context (peer-to-peer, school employee or work employer, and third-parties). These labels are based on definitions by the U.S. Dept. of Education BIBREF14. In order to understand the latent topics of those #MeToo tweets for college followers, we first utilize Latent Dirichlet Allocation (LDA) to label universal topics demonstrated by the users. We determine the optimal topic number by selecting the one with the highest coherence score. Since certain",
        "type": "Document"
      },
      {
        "id": "6274377a-5c56-44ad-b61c-000b559c9de0",
        "metadata": {
          "chunk_id": 18,
          "document_id": "2001.05970",
          "start_idx": 7394,
          "end_idx": 7862
        },
        "page_content": " users who posted #MeToo tweets per student enrolled) for each individual college. Per our topic modeling results, we decide to look deeper into the narratives of #MeToo users who reveal their personal stories. We examine 6,760 tweets from the most relevant topic of our LDA model, and categorize them based on the following metrics: harassment types (verbal, physical, and visual abuse) and context (peer-to-peer, school employee or work employer, and third-parties).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which geographical regions correlate to the trend?",
    "llm_answer": "South Asia, Latin America and the Caribbean, East Asia and the Pacific.  (and the UK, exhibiting a decreasing trend)\n",
    "context": [
      {
        "id": "6db039d4-5797-4496-864b-4014c7c76a94",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1708.05873",
          "start_idx": 17963,
          "end_idx": 18387
        },
        "page_content": " of regions: Latin America and the Caribbean (LCN), South Asia (SAS), Sub-Saharan Africa (SSA), Europe and Central Asia (ECS), Middle East and North Africa (MEA), East Asia and the Pacific (EAS), North America (NAC). The figure shows that states in South Asia, and Latin America and the Caribbean are likely to discuss Topic 2 the most. States in South Asia and East Asia and the Pacific discuss Topic 7 the most. The figure",
        "type": "Document"
      },
      {
        "id": "a6f80e37-fc25-44d1-98ed-de73a01cb813",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1904.07342",
          "start_idx": 7626,
          "end_idx": 8166
        },
        "page_content": " of positive tweets emanate from traditionally liberal hubs (e.g. San Francisco, Los Angeles, Austin), while most negative tweets come from the Philadelphia metropolitan area. These regions aside, rural areas tended to see more negative sentiment tweeters post-event, whereas urban regions saw more positive sentiment tweeters; however, overall average climate change sentiment pre- and post-event was relatively stable geographically. This map further confirms findings that coastal cities tend to be more aware of climate change BIBREF8 .",
        "type": "Document"
      },
      {
        "id": "f7353b21-a471-4d52-9274-e3fdfb44ac90",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1904.07342",
          "start_idx": 6890,
          "end_idx": 7375
        },
        "page_content": " on a subset with geographic user information and compare results to the prior literature. In Figure FIGREF3 , we map 4-clustering results on three dimensions: predicted sentiments, latitude, and longitude. The clusters correspond to four major regions of the U.S.: the Northeast (green), Southeast (yellow), Midwest (blue), and West Coast (purple); centroids are designated by crosses. Average sentiments within each cluster confirm prior knowledge BIBREF1 : the Southeast and Midwest",
        "type": "Document"
      },
      {
        "id": "75339e0c-cf77-412a-9bc4-ba5062f2b7cc",
        "metadata": {
          "chunk_id": 21,
          "document_id": "1809.02494",
          "start_idx": 8157,
          "end_idx": 8743
        },
        "page_content": " highlight the creation of models of geographical descriptors, comparing models between both subject groups, studying combinations of models of cardinal directions, and researching on geographical referring expression generation. Furthermore, insights about the semantics of geographical concepts could be inferred under a more thorough analysis. One of the inconveniences that our data sets present is the appearance of the issues described in Sec. SECREF10 . It could be necessary to filter some of the answers according to different criteria (e.g., deviation of the centroid location",
        "type": "Document"
      },
      {
        "id": "9dbf9cd4-93f2-4f10-9d89-1e2404569554",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1910.12618",
          "start_idx": 8238,
          "end_idx": 8726
        },
        "page_content": " for France it seems marginal (fig. FIGREF2), there is a strong decreasing trend for the United-Kingdom (fig. FIGREF2). Such a strong non-stationarity of the time series would cause problems for the forecasting process, since the learnt demand levels would differ significantly from the upcoming ones. Therefore a linear regression was used to approximate the decreasing trend of the net consumption in the UK. It is then subtracted before the training of the methods, and then re-added a",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What two components are included in their proposed framework?",
    "llm_answer": "A deep learning classifier and a tweet processor.\n",
    "context": [
      {
        "id": "6bd45ba5-5818-4bf7-9706-f82d1b94eeae",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1603.04553",
          "start_idx": 11513,
          "end_idx": 11902
        },
        "page_content": "IBREF28 ; Latent-Tree: the latent tree model BIBREF29 obtaining the best results in the shared task; Berkeley: the Berkeley system with the final feature set BIBREF12 ; LaSO: the structured perceptron system with non-local features BIBREF30 ; Latent-Strc: the latent structure system BIBREF31 ; Model-Stack: the entity-centric system with model stacking BIBREF32 ; and Non-Linear: the non-",
        "type": "Document"
      },
      {
        "id": "4b093c43-a83c-4e8f-b5dd-c4989a94e81f",
        "metadata": {
          "chunk_id": 84,
          "document_id": "1703.05260",
          "start_idx": 30689,
          "end_idx": 30773
        },
        "page_content": " Foundation (DFG) as part of SFB 1102 'Information Density and Linguistic Encoding'.",
        "type": "Document"
      },
      {
        "id": "c99d5551-82aa-4a01-8ce1-12f1a9e0fbeb",
        "metadata": {
          "chunk_id": 47,
          "document_id": "2002.00876",
          "start_idx": 16529,
          "end_idx": 17069
        },
        "page_content": " we hope to support research and production applications employing structured models. We also believe the library provides a strong foundation for building generic tools for interpretablity, control, and visualization through its probabilistic API. Finally, we hope to explore further optimizations to make core algorithms competitive with highly-optimized neural network components. We thank Yoon Kim, Xiang Lisa Li, Sebastian Gehrmann, Yuntian Deng, and Justin Chiu for discussion and feedback on the project. The project was supported by",
        "type": "Document"
      },
      {
        "id": "b5200551-ff5d-43d8-96f3-3364a0e0c560",
        "metadata": {
          "chunk_id": 31,
          "document_id": "2001.05493",
          "start_idx": 11386,
          "end_idx": 11895
        },
        "page_content": ". Section SECREF26 describes the computation of NLP features. In Sections SECREF30, SECREF34 and SECREF45 we have described the architecture of different deep learning models like Deep Pyramid CNN, Disconnected RNN and Pooled BiLSTM respectively. Finally, in Section SECREF49, we describe model averaging based classification model which combines the prediction probabilities from three deep learninig architectures discussed above. (see Figure FIGREF22. for block diagram of system architecture). We consider",
        "type": "Document"
      },
      {
        "id": "27f6b6b2-d12e-473e-8942-f97e3c4c1ff0",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1706.08032",
          "start_idx": 2368,
          "end_idx": 2886
        },
        "page_content": " unnecessary parts of a tweet. The paper makes the following contributions: The organization of the present paper is as follows: In section 2, we describe the model architecture which introduces the structure of the model. We explain the basic idea of model and the way of constructing the model. Section 3 show results and analysis and section 4 summarize this paper. Our proposed model consists of a deep learning classifier and a tweet processor. The deep learning classifier is a combination of DeepCNN and Bi-LSTM",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which modifications do they make to well-established Seq2seq architectures?",
    "llm_answer": "They use various regularization techniques, a single-head LSTM attention mechanism, and consider the specific problem of TTS to modify the Seq2seq model for better performance.  They also employ variational-based methods to increase generation diversity.\n",
    "context": [
      {
        "id": "05d0377a-e874-4b4e-b18d-04a9e2e467ef",
        "metadata": {
          "chunk_id": 3,
          "document_id": "2001.07263",
          "start_idx": 1190,
          "end_idx": 1675
        },
        "page_content": " problems; a detailed list can be found in Sec. SECREF2. Recently, increasingly complex attention mechanisms have been proposed to improve seq2seq model performance, including stacking self and regular attention layers and using multiple attention heads in the encoder and decoder BIBREF4, BIBREF8. We show that consistent application of various regularization techniques brings a simple, single-head LSTM attention based encoder-decoder model to state-of-the-art performance on Switch",
        "type": "Document"
      },
      {
        "id": "78853cb7-1bf5-4182-93ab-866975328858",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1903.07398",
          "start_idx": 7684,
          "end_idx": 8197
        },
        "page_content": " proposed improvements come from the observation that employing generic Seq2seq models for TTS application misses out on further optimization that can be achieved when we consider the specific problem of TTS. Specifically, we notice that in TTS, unlike in applications like machine translation, the Seq2Seq attention mechanism should be mostly monotonic. In other words, when one reads a sequence of text, it is natural to assume that the text position progress nearly linearly in time with the sequence of output",
        "type": "Document"
      },
      {
        "id": "7b813b42-9872-4cf9-a2a5-fbad9455f828",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1909.08824",
          "start_idx": 18096,
          "end_idx": 18537
        },
        "page_content": "q2Seq and variational-based methods, including Variational Seq2Seq, VRNMT, CWVAE-unpretrained and CWVAE shows that, variational-based methods could increase the diversity of generations. This confirms one of our motivations that variational-based methods could capture the latent semantic distribution within targets and increase the diversity of If-Then reasoning. (2) Comparing CWVAE-unpretrained with other baseline methods shows that, in",
        "type": "Document"
      },
      {
        "id": "e6112d7d-1ac2-47c5-ab32-7b3073dd6370",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1905.08949",
          "start_idx": 4526,
          "end_idx": 5007
        },
        "page_content": " \u201chow\u201d in an unified framework. The majority of current NQG models follow the sequence-to-sequence (Seq2Seq) framework that use a unified representation and joint learning of content selection (via the encoder) and question construction (via the decoder). In this framework, traditional parsing-based content selection has been replaced by more flexible approaches such as attention BIBREF22 and copying mechanism BIBREF23 . Question construction has become completely data-driven,",
        "type": "Document"
      },
      {
        "id": "6175b575-2a4d-4030-8f6f-c80cd15d1e19",
        "metadata": {
          "chunk_id": 1,
          "document_id": "1909.02480",
          "start_idx": 279,
          "end_idx": 619
        },
        "page_content": "ldots , x_{T^{\\prime }}\\rbrace $ using conditional probabilities $P_\\theta (\\mathbf {y}|\\mathbf {x})$ predicted by neural networks (parameterized by $\\theta $). Most seq2seq models are autoregressive, meaning that they factorize the joint probability of the output sequence given the input sequence $P_\\theta (\\mathbf {y}|\\mathbf {x})$ into",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How was speed measured?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "11196771-2eb6-4011-91a7-24dca558ad39",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1909.02764",
          "start_idx": 11795,
          "end_idx": 12335
        },
        "page_content": " instructed to reach the destination of the route as fast as possible while following traffic rules and speed limits. They are informed that the time needed for the task would be compared to other participants. The route comprises highways, rural roads, and city streets. A navigation system with voice commands and information on the screen keeps the participants on the predefined track. To trigger emotion changes in the participant, we use the following events: (i) a car on the right lane cutting off to the left lane when participants",
        "type": "Document"
      },
      {
        "id": "1d812102-98f4-45bc-b355-c1653ccfaf7e",
        "metadata": {
          "chunk_id": 72,
          "document_id": "1809.08731",
          "start_idx": 24617,
          "end_idx": 25067
        },
        "page_content": " metrics on a benchmark dataset. Additionally, they can be applied even in settings where no references are available, or would be costly to obtain. Finally, for given references, we proposed the reference-based metric ROUGE-LM, which consists of a combination of WPSLOR and ROUGE. Thus, we were able to obtain an even more accurate fluency evaluation. We would like to thank Sebastian Ebert and Samuel Bowman for their detailed and helpful feedback.",
        "type": "Document"
      },
      {
        "id": "0f0f86f5-59c3-4ea6-ac02-69201e3aefd7",
        "metadata": {
          "chunk_id": 52,
          "document_id": "1910.12618",
          "start_idx": 20588,
          "end_idx": 21147
        },
        "page_content": " one). Finally before application on the testing set, all the methods are re-trained from scratch using both the training and validation data. The goal of our experiments is to quantify how close one can get using textual data only when compared to numerical data. However the inputs of the numerical benchmark should be hence comparable to the information contained in the weather reports. Considering they mainly contain calendar (day of the week and month) as well as temperature and wind information, the benchmark of comparison is a random forest trained",
        "type": "Document"
      },
      {
        "id": "1b24ee13-3f49-4378-98d4-18369d5373df",
        "metadata": {
          "chunk_id": 85,
          "document_id": "1910.12618",
          "start_idx": 33548,
          "end_idx": 34010
        },
        "page_content": " are given in the appendix tables TABREF57 and TABREF58. The first row corresponds to the reference word vector $\\overrightarrow{w_1}$ used to calculate the distance from (thus the distance is always zero), while the following ones are the 9 closest to it. The two last rows correspond to words we deemed important to check the distance with (an antagonistic one or relevant one not in the top 9 for instance). The results of the experiments are very similar for",
        "type": "Document"
      },
      {
        "id": "75cac52d-76c0-4b0e-ab63-d3971582c9df",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1709.01256",
          "start_idx": 17240,
          "end_idx": 17745
        },
        "page_content": " running, new documents become available and INLINEFORM0 can be periodically updated by using the same method. This section reports the results of the experiments conducted on two data sets for evaluating the performances of wDTW and wTED against other baseline methods. We denote the following distance/similarity measures. wDTW: Our semantic distance measure explained in Section SECREF21 . wTED: Our semantic distance measure explained in Section SECREF23 . WMD: The Word Mover's Distance introduced in",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What were their accuracy results on the task?",
    "llm_answer": "Accuracy results vary: 100% on non-monotone problems, 95.4% with a specific approach, and 95.0% with another approach.\n",
    "context": [
      {
        "id": "4f366b00-7e35-42cd-8d5d-0a1a3c14dcfa",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1906.06448",
          "start_idx": 21919,
          "end_idx": 22444
        },
        "page_content": " also indicates the difficulty of problems from linguistics publications. Regarding non-monotone problems collected via crowdsourcing, there are very few non-monotone problems, so accuracy is 100%. Adding non-monotone problems to our test set is left for future work. Table 9 shows the evaluation results by type of linguistic phenomenon. While accuracy on problems involving NPIs and conditionals was improved on both upward and downward inferences, accuracy on problems involving conjunction and disjunction was improved on",
        "type": "Document"
      },
      {
        "id": "ec846c61-b035-431a-88d4-858e8c47d3be",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1603.07044",
          "start_idx": 9966,
          "end_idx": 10403
        },
        "page_content": " for task A, there are a total number of 26,700 question-comment pairs. For task B, there are 2,670 question-question pairs. For task C, there are 26,700 question-comment pairs. The test dataset includes 50 questions, 500 related questions and 5,000 comments which do not overlap with the training set. To evaluate the performance, we use mean average precision (MAP) and F1 score. Table 2 shows the initial results using the RNN encoder",
        "type": "Document"
      },
      {
        "id": "50f7917c-6e8b-4b83-9208-501c663a60b1",
        "metadata": {
          "chunk_id": 73,
          "document_id": "2004.03744",
          "start_idx": 26592,
          "end_idx": 27055
        },
        "page_content": " a standard deviation of 5.54. We required the workers to have a previous approval rate above 90%. No restriction was put on the workers' location. Each assignment consisted of a set of 10 image-sentence pairs. For each pair, the participant was asked to (a) choose a label, (b) highlight words in the sentence that led to their decision, and (c) explain their decision in a comprehensive and concise manner, using a subset of the words that they highlighted. The",
        "type": "Document"
      },
      {
        "id": "6cf0a06f-34c6-44b2-a329-2285cf3a8bc8",
        "metadata": {
          "chunk_id": 57,
          "document_id": "1910.04269",
          "start_idx": 21961,
          "end_idx": 22224
        },
        "page_content": " which is a recently developed technique to prevent over\ufb01tting on test data.This approach achieved an accuracy of 95.4%. We also analysed how attention mechanism and recurrent layers impact the performance of networks. This approach achieved an accuracy of 95.0%.",
        "type": "Document"
      },
      {
        "id": "9ae07154-652f-4b7c-8b79-a3f2dbc110ae",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1803.09230",
          "start_idx": 16584,
          "end_idx": 17109
        },
        "page_content": " the task challenging but we were extremely satisfied with our own personal learning curve. We are sure that with even more time, we could significantly improve our model from the baseline enhancement we achieved so far. All in all, we believe that the experience of this project, will be of utmost value for our future professional work. First of all, we would like to thank the course instructor Richard Socher for making the class highly informative and a great learning experience. We would also like to thank the TAs for",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What two types the Chinese reading comprehension dataset consists of?",
    "llm_answer": "People Daily news datasets and Children's Fairy Tale datasets.\n",
    "context": [
      {
        "id": "03538ff4-bcd5-4949-8f5b-5464b2a843b9",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1709.08299",
          "start_idx": 4080,
          "end_idx": 4586
        },
        "page_content": " The previously mentioned datasets are all in English. To add diversities to the reading comprehension datasets, Cui et al. cui-etal-2016 proposed the first Chinese cloze-style reading comprehension dataset: People Daily & Children's Fairy Tale, including People Daily news datasets and Children's Fairy Tale datasets. They also generate the data in an automatic manner, which is similar to the previous datasets. They choose short articles (several hundreds of words) as Document and remove a word from it",
        "type": "Document"
      },
      {
        "id": "c2530727-660e-4673-90fd-0e50543588d8",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1806.09103",
          "start_idx": 3703,
          "end_idx": 4224
        },
        "page_content": " and low-frequency words, we employ a short list mechanism. Our evaluation will be performed on three public Chinese reading comprehension datasets and one English benchmark dataset for showing our method is also effective in multi-lingual case. The concerned reading comprehension task can be roughly categorized as user-query type and cloze-style according to the answer form. Answers in the former are usually a span of texts while in the cloze-style task, the answers are words or phrases which lets the latter be the",
        "type": "Document"
      },
      {
        "id": "2969e0cb-faf3-4840-b93a-c9bcb216a193",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1709.08299",
          "start_idx": 12705,
          "end_idx": 13259
        },
        "page_content": " comprehension dataset for the 1st Evaluation on Chinese Machine Reading Comprehension (CMRC-2017), consisting large-scale automatically generated training set and human-annotated validation and test set. Many participants have verified their algorithms on this dataset and tested on the hidden test set for final evaluation. The experimental results show that the neural baselines are tough to beat and there is still much room for using complicated transfer learning method to better solve the User Query Task. We hope the release of the full dataset (",
        "type": "Document"
      },
      {
        "id": "97c1f908-7d6d-4a4b-9c0c-569ee026bf32",
        "metadata": {
          "chunk_id": 1,
          "document_id": "1709.08299",
          "start_idx": 556,
          "end_idx": 1058
        },
        "page_content": " sentence according to the document material. Several cloze-style reading comprehension datasets are publicly available, such as CNN/Daily Mail BIBREF0 , Children's Book Test BIBREF1 , People Daily and Children's Fairy Tale BIBREF2 . In this paper, we provide a new Chinese reading comprehension dataset, which has the following features We also host the 1st Evaluation on Chinese Machine Reading Comprehension (CMRC2017), which has attracted over 30 participants and finally there were 17 participants",
        "type": "Document"
      },
      {
        "id": "d7f384a9-7da0-42fc-bc51-9ae7837a98b0",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1806.09103",
          "start_idx": 12961,
          "end_idx": 13401
        },
        "page_content": " current cloze-style datasets for English reading comprehension, such as CBT, Daily Mail and CNN BIBREF0 , the three Chinese datasets do not provide candidate answers. Thus, the model has to find the correct answer from the entire document. Besides, we also use the Children's Book Test (CBT) dataset BIBREF1 to test the generalization ability in multi-lingual case. We only focus on subsets where the answer is either a common noun (CN) or",
        "type": "Document"
      }
    ]
  },
  {
    "query": "For which languages most of the existing MRC datasets are created?",
    "llm_answer": "English, German, Spanish, Italian, Japanese, and Portuguese.\n",
    "context": [
      {
        "id": "e8ffc8fd-73a2-4096-87aa-87f5967c213f",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1910.11493",
          "start_idx": 5173,
          "end_idx": 5630
        },
        "page_content": " languages. Data for all but four languages (Basque, Kurmanji, Murrinhpatha, and Sorani) are extracted from English Wiktionary, a large multi-lingual crowd-sourced dictionary with morphological paradigms for many lemmata. 20 of the 100 language pairs are either distantly related or unrelated; this allows speculation into the relative importance of data quantity and linguistic relatedness. For each language, the basic data consists of triples of the form",
        "type": "Document"
      },
      {
        "id": "de1e2f56-6dcd-45a5-bf47-c2243ae92a99",
        "metadata": {
          "chunk_id": 56,
          "document_id": "1911.00069",
          "start_idx": 17396,
          "end_idx": 17813
        },
        "page_content": "11. Our in-house dataset includes manually annotated RE data for 6 languages: English, German, Spanish, Italian, Japanese and Portuguese. It defines 56 entity types (e.g., Person, Organization, Geo-Political Entity, Location, Facility, Time, Event_Violence, etc.) and 53 relation types between the entities (e.g., AgentOf, LocatedAt, PartOf, TimeOf, AffectedBy, etc.). The ACE05 dataset includes manually annotated RE",
        "type": "Document"
      },
      {
        "id": "2304b6a3-a5d8-4009-af97-fffe36eae8f2",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1909.06522",
          "start_idx": 11616,
          "end_idx": 12081
        },
        "page_content": ": live streams from users. UserVOD (video on demand): was-live videos. For each language, the train and test set size are described in Table TABREF10, and most training data were Pages. On each language we also had a small validation set for model parameter tuning. Each monolingual ASR baseline was trained on language-specific data only. The character sets of these 7 languages have little overlap except that (i) they all include common basic Latin alphabet, and",
        "type": "Document"
      },
      {
        "id": "d53c2579-5aa2-4ec6-95c4-5b151b4491d3",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1909.06522",
          "start_idx": 2193,
          "end_idx": 2640
        },
        "page_content": " within training set languages. In various cases it can produce performance gains over monolingual modeling that uses in-language data only. In our work, we aim to examine the same approach of building a multilingual graphemic lexicon, while using a standard hybrid ASR system \u2013 based on Bidirectional Long Short-Term Memory (BLSTM) and HMM \u2013 learned with lattice-free maximum mutual information (MMI) objective BIBREF17. Our initial attempt is on",
        "type": "Document"
      },
      {
        "id": "0411c0de-62c6-43bd-bf07-8716d95e72f2",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1910.11493",
          "start_idx": 18674,
          "end_idx": 19179
        },
        "page_content": " provides an interesting avenue for multilinguality. Further, our shared task was designed to only leverage a single helper language, though many may exist with lexical or morphological overlap with the target language. Techniques like those of BIBREF25 may aid in designing universal inflection architectures. Neither task this year included unannotated monolingual corpora. Using such data is well-motivated from an L1-learning point of view, and may affect the performance of low-resource data settings",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which sentiment analysis tasks are addressed?",
    "llm_answer": "Tweet-level sentiment, clinical narrative sentiment (positive/negative phenomena within risk factor domains).\n",
    "context": [
      {
        "id": "4820a48a-e57a-4cb8-b667-da1bae62e6f5",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1911.12569",
          "start_idx": 21417,
          "end_idx": 21953
        },
        "page_content": ". Figure FIGREF21 illustrates the performance of various models for sentiment analysis. As a concrete example which justifies the utility of emotion analysis in sentiment analysis is shown below. @realMessi he is a real sportsman and deserves to be the skipper. The gold labels for the example are anticipation, joy and trust emotion with a positive sentiment. Our system S2 (single task system for sentiment analysis with primary and secondary attention) had incorrectly labeled this example with a negative sentiment and the E2 system",
        "type": "Document"
      },
      {
        "id": "9f3bc3df-9609-4c6d-b153-90450c4a5800",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1710.01492",
          "start_idx": 4787,
          "end_idx": 5328
        },
        "page_content": ", which is needed to train the high number of parameters of such complex models. Finally, as methods for sentiment analysis mature, more attention is also being paid to linguistic structure and to multi-linguality and cross-linguality. Sentiment analysis emerged as a popular research direction in the early 2000s. Initially, it was regarded as standard document classification into topics such as business, sport, and politics BIBREF10 . However, researchers soon realized that it was quite different from standard document classification B",
        "type": "Document"
      },
      {
        "id": "243fbc87-04cc-41de-9b8b-5694903be2df",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1710.01492",
          "start_idx": 8983,
          "end_idx": 9412
        },
        "page_content": " implicit event polarity BIBREF35 , stance in tweets BIBREF36 , out-of-context sentiment intensity of phrases BIBREF37 , and emotion detection BIBREF38 . Some of these tasks featured languages other than English. Tweet-level sentiment. The simplest and also the most popular task of sentiment analysis on Twitter is to determine the overall sentiment expressed by the author of a tweet BIBREF30 , BIBREF28 , BIBREF26 , BIBREF29 ,",
        "type": "Document"
      },
      {
        "id": "127c8c21-6987-40ef-9c99-4b51c399a68d",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1809.05752",
          "start_idx": 23383,
          "end_idx": 23945
        },
        "page_content": " These include a deeper analysis of clinical narratives in EHRs: our next task will be to extend our EHR data pipeline by distinguishing between clinically positive and negative phenomena within each risk factor domain. This will involve a series of annotation tasks that will allow us to generate lexicon-based and corpus-based sentiment analysis tools. We can then use these clinical sentiment scores to generate a gradient of patient improvement or deterioration over time. We will also take into account structured data that have been collected on the target",
        "type": "Document"
      },
      {
        "id": "b3387cba-c340-4bee-a9b6-af8fe9fea458",
        "metadata": {
          "chunk_id": 54,
          "document_id": "1911.12569",
          "start_idx": 20838,
          "end_idx": 21417
        },
        "page_content": " emotional information increases the performance of the main task, i.e. sentiment analysis when these two are jointly performed. Experimental results help us to establish the fact that emotion analysis benefits sentiment analysis. The implicit sentiment attached to the emotion words assists the multi-task system. Emotion such as joy and trust are inherently associated with a positive sentiment whereas, anger, disgust, fear and sadness bear a negative sentiment. Figure FIGREF21 illustrates the performance of various models for sentiment analysis. As a concrete example which",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which 3 NLP areas are cited the most?",
    "llm_answer": "Language, machine translation, and lexical semantics.\n",
    "context": [
      {
        "id": "a404c960-3e30-4028-ba42-be02df26e806",
        "metadata": {
          "chunk_id": 139,
          "document_id": "1911.03562",
          "start_idx": 54039,
          "end_idx": 54519
        },
        "page_content": " extraction, document summarization, spoken dialogue, cross lingual (research), dialogue, systems, language generation. (Of course, note that some of the 59 areas, as estimated using title term bigrams, are overlapping. Also, we did not include large scale in the list above because the difference in averages is very small and it is not really an area of research.) Thus, the citation gap is common across a majority of the high-citations areas within NLP. This work examined the",
        "type": "Document"
      },
      {
        "id": "3baadb13-991f-4872-851b-66e3649bd0a7",
        "metadata": {
          "chunk_id": 140,
          "document_id": "1911.03562",
          "start_idx": 54519,
          "end_idx": 55001
        },
        "page_content": " is common across a majority of the high-citations areas within NLP. This work examined the ACL Anthology to identify broad trends in productivity, focus, and impact. We examined several questions such as: who and how many of us are publishing? what are we publishing on? where and in what form are we publishing? and what is the impact of our publications? Particular attention was paid to the demographics and inclusiveness of the NLP community. Notably, we showed that only about",
        "type": "Document"
      },
      {
        "id": "79c0ba3f-6284-437e-872f-a67ec94ff212",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1911.03562",
          "start_idx": 21972,
          "end_idx": 22500
        },
        "page_content": " the unigram in that year and the two years before it is at least ten. The period before 1980 is not included because of the small number of papers. Discussion: Appropriately enough, the most common term in the titles of NLP papers is language. Presence of high-ranking terms pertaining to machine translation suggest that it is the area of research that has received considerable attention. Other areas associated with the high-frequency title terms include lexical semantics, named entity recognition, question answering, word",
        "type": "Document"
      },
      {
        "id": "873136f9-7362-42e9-ad90-23c99f869ead",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1911.03562",
          "start_idx": 729,
          "end_idx": 1219
        },
        "page_content": " what are we publishing on? where and in what form are we publishing? and what is the impact of our publications? The answers are usually in the form of numbers, graphs, and inter-connected visualizations. We focus on the following aspects of NLP research: size, demographics, areas of research, impact, and correlation of citations with demographic attributes (age and gender).  Target Audience: The analyses presented here are likely to be of interest to any NLP researcher. This might be",
        "type": "Document"
      },
      {
        "id": "be67d08b-bbf8-4fb7-af2a-9e0fac99f0ea",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1909.02776",
          "start_idx": 7691,
          "end_idx": 8178
        },
        "page_content": " differences in the structure and context of each section. All the above studies imply the significance of document context in ranking. Nevertheless, it has not been given enough attention in the NLP community, and even sometimes is neglected. For instance, authors in BIBREF30 suggest the use of a wide range of various features. Among these, seventeen part-of-speech based sentences features have been introduced, all of which are sentence-normalized, but not document-normalized, i.e.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which journal and conference are cited the most in recent years?",
    "llm_answer": "The context states that CL Journal, EMNLP, and ACL have the best track record for accepting papers that have gone on to receive 1000 or more citations.  It also notes that CL Journal has a particularly high percentage of papers with 100 or more citations.  However, it does not state which is cited *most* overall in recent years.\n",
    "context": [
      {
        "id": "b40ae2e4-8094-40e7-b7ec-b3640219db8d",
        "metadata": {
          "chunk_id": 79,
          "document_id": "1911.03562",
          "start_idx": 30434,
          "end_idx": 30877
        },
        "page_content": " the paper title, authors, year of publication, publication venue, and #citations. Discussion: With time, not only have the number of papers grown, but also the number of high-citation papers. We see a marked jump in the 1990s over the previous decades, but the 2000s are the most notable in terms of the high number of citations. The 2010s papers will likely surpass the 2000s papers in the years to come. Q. What are the most cited papers in",
        "type": "Document"
      },
      {
        "id": "b7a89c80-1351-4fbc-ad8c-9b02a940f5b3",
        "metadata": {
          "chunk_id": 105,
          "document_id": "1911.03562",
          "start_idx": 40431,
          "end_idx": 40920
        },
        "page_content": " marked in CL journal, and thus they were not removed from analysis. Also, letters to editors, which are more common in CL journal, tend to often obtain 0 citations. CL, EMNLP, and ACL have the best track record for accepting papers that have gone on to receive 1000 or more citations. *Sem, the semantics conference, seems to have notably lower percentage of high-citation papers, even though it has fairly competitive acceptance rates. Instead of percentage, if one considers raw numbers",
        "type": "Document"
      },
      {
        "id": "ac7cf6a9-d804-495c-a266-6a4758df9ac0",
        "metadata": {
          "chunk_id": 104,
          "document_id": "1911.03562",
          "start_idx": 40114,
          "end_idx": 40632
        },
        "page_content": " or more citations. The percentages are markedly lower (between 30 and 70%) for the other conferences shown above, and even lower for some other conferences (not shown above). CL Journal is particularly notable for the largest percentage of papers with 100 or more citations. The somewhat high percentage of papers that are never cited (4.3%) are likely because some of the book reviews from earlier years are not explicitly marked in CL journal, and thus they were not removed from analysis. Also, letters to editors,",
        "type": "Document"
      },
      {
        "id": "0cfe56a8-4769-4181-90cf-da5bce6a1e1e",
        "metadata": {
          "chunk_id": 99,
          "document_id": "1911.03562",
          "start_idx": 38236,
          "end_idx": 38701
        },
        "page_content": " become competitive with some top-tier conferences such as EACL and COLING. Q. What percent of the AA\u2019 papers that were published in 2016 or earlier are cited more than 1000 times? How many more than 10 times? How many papers are cited 0 times? A. Google Scholar invented the i-10 index as another measure of author research impact. It stands for the number of papers by an author that received ten or more citations. (Ten here is somewhat arbitrary, but reasonable",
        "type": "Document"
      },
      {
        "id": "83ecf8de-d883-4e74-bfb7-a1dd6f89a8ab",
        "metadata": {
          "chunk_id": 90,
          "document_id": "1911.03562",
          "start_idx": 34616,
          "end_idx": 35134
        },
        "page_content": " 26,949 such papers. Figures FIGREF42 and FIGREF43 show the average citations by paper type when considering papers published 1965\u20132016 and 2010\u20132016, respectively. Figures FIGREF45 and FIGREF46 show the medians. Discussion: Journal papers have much higher average and median citations than other papers, but the gap between them and top-tier conferences is markedly reduced when considering papers published since 2010. System demo papers have the third highest average citations; however, shared task papers have the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which 5 languages appear most frequently in AA paper titles?",
    "llm_answer": "Chinese, Arabic, Korean, Japanese, and Hindi\n",
    "context": [
      {
        "id": "b093209a-9929-4e7f-8f39-47532680a92d",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1911.03562",
          "start_idx": 15948,
          "end_idx": 16450
        },
        "page_content": " language, then the language is mentioned in the title. We collected a list of 122 languages indexed by Wiktionary and looked for the presence of these words in the titles of AA papers. (Of course there are hundreds of other lesser known languages as well, but here we wanted to see the representation of these more prominent languages in NLP literature.) Figure FIGREF27 is a treemap of the 122 languages arranged alphabetically and shaded such that languages that appear more often in AA paper titles",
        "type": "Document"
      },
      {
        "id": "6c9d5151-0cab-42a2-b132-bba5cd59f13e",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1911.03562",
          "start_idx": 16450,
          "end_idx": 16950
        },
        "page_content": " the 122 languages arranged alphabetically and shaded such that languages that appear more often in AA paper titles have a darker shade of green. Discussion: Even though the amount of work done on English is much larger than that on any other language, often the word English does not appear in the title, and this explains why English is not the first (but the second-most) common language name to appear in the titles. This is likely due to the fact that many papers fail to mention the language of",
        "type": "Document"
      },
      {
        "id": "9c487f28-5722-4c9d-a9df-1231d2adb447",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1911.03562",
          "start_idx": 17227,
          "end_idx": 17704
        },
        "page_content": " on how central it is to the paper. We can see from the treemap that the most widely spoken Asian and Western European languages enjoy good representation in AA. These include: Chinese, Arabic, Korean, Japanese, and Hindi (Asian) as well as French, German, Swedish, Spanish, Portuguese, and Italian (European). This is followed by the relatively less widely spoken European languages (such as Russian, Polish, Norwegian, Romanian, Dutch, and Czech) and Asian languages (such as",
        "type": "Document"
      },
      {
        "id": "7f25b8b2-2a11-4c9c-830c-972ca89b4b9f",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1911.03562",
          "start_idx": 16740,
          "end_idx": 17227
        },
        "page_content": " in the titles. This is likely due to the fact that many papers fail to mention the language of study or the language of the datasets used if it is English. There is growing realization in the community that this is not quite right. However, the language of study can be named in other less prominent places than the title, for example the abstract, introduction, or when the datasets are introduced, depending on how central it is to the paper. We can see from the treemap that the most",
        "type": "Document"
      },
      {
        "id": "79c0ba3f-6284-437e-872f-a67ec94ff212",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1911.03562",
          "start_idx": 21972,
          "end_idx": 22500
        },
        "page_content": " the unigram in that year and the two years before it is at least ten. The period before 1980 is not included because of the small number of papers. Discussion: Appropriately enough, the most common term in the titles of NLP papers is language. Presence of high-ranking terms pertaining to machine translation suggest that it is the area of research that has received considerable attention. Other areas associated with the high-frequency title terms include lexical semantics, named entity recognition, question answering, word",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much F1 was improved after adding skip connections?",
    "llm_answer": "The skip connections improved scores further.  However, the amount of improvement is not specified.\n",
    "context": [
      {
        "id": "f658ba1c-2e6f-4097-8f60-b0df34156a44",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1912.10435",
          "start_idx": 16280,
          "end_idx": 16800
        },
        "page_content": " than the original BERT attention. The skip connections improved the scores further by adding the BERT embeddings back in to the coattention vectors and providing information that may have been lost by the C2Q/Q2C network in addition to providing a convenient path for backpropagation to the BERT embedding layers. The skip connection containing the transformer provides minimal gains while adding a significant overhead to runtime. Therefore, we built the final convolutional experiments on the Simple Skip architecture",
        "type": "Document"
      },
      {
        "id": "274b1836-7a53-4a54-96da-d74a5fcb9cf6",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1912.10435",
          "start_idx": 12841,
          "end_idx": 13286
        },
        "page_content": " fit the model on 2 GPUs (3 F1 score improvement by itself). Max Sequence length - After initial experiments with default sequence length (context + query token) 384, we switched to a sequence length of 512. This gave us a 0.6 F1 improvement on our model. Batch Size - Default: 12, We had to use a batch size of 6 for all our experiments due to resource constraints and out of memory issues on the GPU for any larger batch size. Number of epochs",
        "type": "Document"
      },
      {
        "id": "136b69ee-3abe-47e6-a4e9-0411a528baca",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1912.10435",
          "start_idx": 15804,
          "end_idx": 16280
        },
        "page_content": " C2Q/Q2C network produced a significant improvement in the No Answer F1 score while causing a symmetric drop in the Has Answer F1 score. The C2Q/Q2C network attends the context relative to the query and vice versa instead of as a concatenated whole. This method of attention provides more information regarding whether there is an answer to the question in the context than the original BERT attention. The skip connections improved the scores further by adding the BERT embed",
        "type": "Document"
      },
      {
        "id": "a4f90e9f-64ef-495d-9b65-6d054c5b7ce7",
        "metadata": {
          "chunk_id": 64,
          "document_id": "2001.11268",
          "start_idx": 25325,
          "end_idx": 25807
        },
        "page_content": " observed increased performance when adding additional data. For classifying I entities, an increase from 20 to 200 additional SQuAD domains resulted in an increase of 8% for the F1 score, whereas the increase for the O domain was less than 1%. After training a model with 200 additional SQuAD domains, we also evaluated it on the original SQuAD development set and obtained a F1 score of 0.72 for this general reading comprehension task. In this evaluation, the F1 scores represent",
        "type": "Document"
      },
      {
        "id": "65f824b8-0056-41d9-b4ed-df607eb76ec0",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1912.10435",
          "start_idx": 17060,
          "end_idx": 17540
        },
        "page_content": " and 3 are described in detail in Section 3.6. Each of the models built on BERT large used our augmented dataset in addition to the coattention architecture, simple skip connection, and separate start and end logit logic. The Model 1 results show that a moderately augmented (35%) data set helps the training since both unaugmented and highly augmented (50%) models did not perform as well. It seems that adding too much augmented data reduces the F1 because the augmented data is",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much gain does the model achieve with pretraining MVCNN?",
    "llm_answer": "+1.25 ROUGE improvement.\n",
    "context": [
      {
        "id": "3c1b83ec-2172-4ee6-81a5-4bab17827498",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1603.04513",
          "start_idx": 3855,
          "end_idx": 4308
        },
        "page_content": " sets are available. MVCNN has a large number of parameters, so that overfitting is a danger when they are trained on small training sets. We address this problem by pretraining MVCNN on unlabeled data. These pretrained weights can then be fine-tuned for the specific classification task. In sum, we attribute the success of MVCNN to: (i) designing variable-size convolution filters to extract variable-range features of sentences and (ii) exploring the",
        "type": "Document"
      },
      {
        "id": "c54ef979-72d8-4d5e-92f0-4a9b1d549d94",
        "metadata": {
          "chunk_id": 28,
          "document_id": "2003.01769",
          "start_idx": 11190,
          "end_idx": 11648
        },
        "page_content": " is critical since it requires the enhancement model to produce speech the acoustic model sees as \u201cclean.\u201d In addition to the setting without any parallel data, we show results given parallel data. In Table TABREF10 we demonstrate that training the AECNN framework with mimic loss improves intelligibility over both the model trained with only time-domain loss (AECNN-T), as well as the model trained with both time-domain and spectral-domain losses (AECNN-T",
        "type": "Document"
      },
      {
        "id": "f211c3e4-221b-4f56-96a4-572eddacada7",
        "metadata": {
          "chunk_id": 84,
          "document_id": "1905.06566",
          "start_idx": 23481,
          "end_idx": 23825
        },
        "page_content": " pre-training (details in Section \"Pre-training\" ) leads to a +1.25 ROUGE improvement. Another baseline is based on a pre-trained BERT BIBREF0 and finetuned on the CNNDM dataset. We used the $\\text{BERT}_{\\text{base}}$ model because our 16G RAM V100 GPU cannot fit $\\text{BERT}_{\\text{large}}$ for the summarization task even with batch size of",
        "type": "Document"
      },
      {
        "id": "1e2f925a-9a0d-4318-af38-ee9ae9d2a426",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1912.13109",
          "start_idx": 7921,
          "end_idx": 8471
        },
        "page_content": " on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we",
        "type": "Document"
      },
      {
        "id": "a34ba259-71d4-4abe-aba1-4c61f5e677b8",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1909.13375",
          "start_idx": 19611,
          "end_idx": 20052
        },
        "page_content": " to gain further insights on the effect of the components that we ourselves introduced. For ease of performance comparison, we followed NABERT+'s training settings: we used the BERT Adam optimizer from huggingface with default settings and a learning rate of $1e^{-5}$. The only difference was that we used a batch size of 12. We trained our base model for 20 epochs. For the large models we used a batch size of 3 with a learning rate of $5",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the highest accuracy score achieved?",
    "llm_answer": "93.06% (unnormalized) for scene recognition.\n",
    "context": [
      {
        "id": "ad156fd3-1eaf-4e70-8f56-1f3024106d99",
        "metadata": {
          "chunk_id": 58,
          "document_id": "2001.11268",
          "start_idx": 22809,
          "end_idx": 23352
        },
        "page_content": " precision scores in ranges between 0.92 and 0.97 are possible with F1 scores not dropping below 0.84 for the main classes of interest. In practice, the detachment between model predictions and assignment of labels means that a reviewer who wishes to switch between high recall and high precision results can do so very quickly, without obtaining new predictions from the model itself. More visualizations can be found in this project's GitHub repository , including true class labels and a detailed breakdown of true and false predictions for",
        "type": "Document"
      },
      {
        "id": "ce9a5da7-b644-459d-89dc-6f6485bbc871",
        "metadata": {
          "chunk_id": 62,
          "document_id": "1701.02877",
          "start_idx": 22362,
          "end_idx": 22802
        },
        "page_content": " data gives little to no improvement, whereas adding positive examples drastically improves performance. Table UID9 shows results with size normalised precision (P), recall (R), and F1-Score (F1). The five lowest P, R and F1 values per method (CRFSuite, Stanford NER, SENNA) are in bold to highlight underperformers. Results for all corpora are summed with macro average. Comparing the different methods, the highest F1 results are achieved",
        "type": "Document"
      },
      {
        "id": "0b9c40d2-cf4a-4254-9e98-5f7c70ba79be",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1910.03814",
          "start_idx": 19558,
          "end_idx": 20033
        },
        "page_content": " results for the LSTM, for the Davison method proposed in BIBREF7 trained with MMHS150K, and for random scores. Fig. FIGREF32 shows the Precision vs Recall plot and the ROC curve (which plots the True Positive Rate vs the False Positive Rate) of the different models. First, notice that given the subjectivity of the task and the discrepancies between annotators, getting optimal scores in the evaluation metrics is virtually impossible. However, a system with relatively low",
        "type": "Document"
      },
      {
        "id": "ac014505-5d0a-490b-b1f0-b7eac35b4729",
        "metadata": {
          "chunk_id": 68,
          "document_id": "1703.09684",
          "start_idx": 25413,
          "end_idx": 25866
        },
        "page_content": " distribution to obtain high scores. We found that for MCB-A, the accuracy on subordinate object recognition drops from 85.54% with unnormalized to 23.22% with normalized, and for scene recognition it drops from 93.06% (unnormalized) to 38.53% (normalized). Both these categories have a heavily skewed answer distribution; the top-25 answers in subordinate object recognition and the top-5 answers in scene recognition cover over 80% of all questions in",
        "type": "Document"
      },
      {
        "id": "d25b8c73-b992-4fc3-ac90-b4f525865c48",
        "metadata": {
          "chunk_id": 62,
          "document_id": "1703.09684",
          "start_idx": 23131,
          "end_idx": 23590
        },
        "page_content": " Accuracy scores are given for each of the 12 question-types in Table 3 , and scores that are normalized by using mean-per-unique-answer are given in appendix Table 5 . By inspecting Table 3 , we can see that some question-types are comparatively easy ( $>90$ %) under MPT: scene recognition, sport recognition, and object presence. High accuracy is also achieved on absurd, which we discuss in greater detail in Sec. \"Effects of Including Absurd Questions\" .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the three datasets used in the paper?",
    "llm_answer": "France, UK, Biomedical\n",
    "context": [
      {
        "id": "032b9bed-6a38-456d-b02a-dea6816fa084",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1910.12618",
          "start_idx": 7204,
          "end_idx": 7727
        },
        "page_content": " section 5 concludes this paper and gives insight on future possible work. In order to prove the consistency of our work, experiments have been conducted on two data sets, one for France and the other for the UK. In this section details about the text and time series data are given, as well as the major preprocessing steps. Three types of time series are considered in our work: national net electricity consumption (also referred as load or demand), national temperature and wind speed. The load data sets were retrieved",
        "type": "Document"
      },
      {
        "id": "64a25737-469b-4e82-875c-ba81cb2e5018",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1701.00185",
          "start_idx": 3790,
          "end_idx": 4304
        },
        "page_content": " this paper we use a new short text dataset, Biomedical, in the experiment to verify the effectiveness of our approach. Third, we put much effort on studying the influence of various different semantic features integrated in our self-taught CNN framework, which is not involved in the conference paper. For the purpose of reproducibility, we make the datasets and software used in our experiments publicly available at the website. The remainder of this paper is organized as follows: In Section SECREF2 , we first",
        "type": "Document"
      },
      {
        "id": "ffcdee05-7820-4632-b45a-84ec7ddff98e",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1604.05781",
          "start_idx": 3160,
          "end_idx": 3653
        },
        "page_content": " organized as follows: In Sec. \"Materials and Methods\" we discuss our materials and methods, including the dataset we studied, how we preprocessed that data and extracted a `causal' corpus and a corresponding `control' corpus, and the details of the statistical and language analysis tools we studied these corpora with. In Sec. \"Results\" we present results using these tools to compare the causal statements to control statements. We conclude with a discussion in Sec. \"Discussion\" . Data was",
        "type": "Document"
      },
      {
        "id": "39c7530e-8090-4406-a7ea-bdb1339acbba",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1911.03562",
          "start_idx": 1524,
          "end_idx": 2018
        },
        "page_content": " are based on information about the papers taken directly from AA (as of June 2019) and citation information extracted from Google Scholar (as of June 2019). Thus, all subsequent papers and citations are not included in the analysis. A fresh data collection is planned for January 2020.  Interactive Visualizations: The visualizations we are developing for this work (using Tableau) are interactive\u2014so one can hover, click to select and filter, move sliders, etc. Since this work is high in the",
        "type": "Document"
      },
      {
        "id": "7bb4c95c-4811-4e15-ab8e-deca3c53de73",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1709.08299",
          "start_idx": 9435,
          "end_idx": 10003
        },
        "page_content": " questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models. The",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is improvement in accuracy for short Jokes in relation other types of jokes?",
    "llm_answer": "A jump of 8 percent from the most recent work done with CNNs (Table 4).\n",
    "context": [
      {
        "id": "3f931445-8b63-4643-9400-fbe502c958bf",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1909.00252",
          "start_idx": 8852,
          "end_idx": 9334
        },
        "page_content": " joke was most important for humor. We found that all of the models, including humans, relied more on the punchline of the joke in their predictions (Table 2). Thus, it seems that although both parts of the joke are needed for it to be humorous, the punchline carries higher weight than the body. We hypothesize that this is due to the variations found in the different joke bodies: some take paragraphs to set up the joke, while others are less than a sentence. Our experiment with",
        "type": "Document"
      },
      {
        "id": "bd744a36-2fb1-4ed6-9ac5-4469eaadda56",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1909.00252",
          "start_idx": 6079,
          "end_idx": 6554
        },
        "page_content": " learn in a more balanced way despite the uneven amount of non-humorous jokes. Our validation and test sets were composed of the remaining 25%, downsampling the data into a 50/50 class split so that the accuracy metric could be balanced and easily understood. To show how our model compares to the previous work done, we also test on the Short Joke and Pun datasets mentioned in the Data section. For these datasets we will use the metrics (Accuracy, Precision, Recall, and F",
        "type": "Document"
      },
      {
        "id": "af7b49f3-e2a4-4bca-a1c6-43b0ed157497",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1909.00252",
          "start_idx": 3554,
          "end_idx": 4026
        },
        "page_content": " We used this natural divide as the cutoff to decide what qualified as a funny joke, giving us 13884 not-funny jokes and 2025 funny jokes. The Short Jokes dataset, found on Kaggle, contains 231,657 short jokes scraped from various joke websites with lengths ranging from 10 to 200 characters. The previous work by BIBREF4 combined this dataset with the WMT162 English news crawl. Although their exact combined dataset is not publicly available, we used the same method and",
        "type": "Document"
      },
      {
        "id": "7463f319-5f01-46e7-b621-0ef1e429fd0f",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1909.00252",
          "start_idx": 9136,
          "end_idx": 9591
        },
        "page_content": " some take paragraphs to set up the joke, while others are less than a sentence. Our experiment with the Short Jokes dataset found the Transformer model's accuracy and F1 score to be 0.986. This was a jump of 8 percent from the most recent work done with CNNs (Table 4). The results on the Pun of the Day dataset are shown in Table 3 above. It shows an accuracy of 93 percent, close to 4 percent greater accuracy than the best CNN model proposed. Although",
        "type": "Document"
      },
      {
        "id": "3946b28d-97b9-4916-a3f3-d48d31e10a97",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1909.00252",
          "start_idx": 11191,
          "end_idx": 11743
        },
        "page_content": " the idea of using machine learning tools, specifically a Transformer neural network architecture, to discern what jokes are funny and what jokes are not. This proposed model does not require any human interaction to determine, aside from the text of the joke itself, which jokes are humorous. This architecture can predict the level of humor for a specific audience to a higher degree than a general audience consensus. We also showed that this model has increased capability in joke identification as a result, with higher accuracy and F1 scores than",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What baseline did they compare Entity-GCN to?",
    "llm_answer": "Many techniques that use expensive question-aware recurrent document encoders.  BiDAF.\n",
    "context": [
      {
        "id": "ef054914-a25b-4a13-8502-c9c8efdefa76",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1808.09920",
          "start_idx": 4290,
          "end_idx": 4787
        },
        "page_content": " Entity-GCN still performs on par with many techniques that use expensive question-aware recurrent document encoders. Despite not using recurrent document encoders, the full Entity-GCN model achieves over 2% improvement over the best previously-published results. As our model is efficient, we also reported results of an ensemble which brings further 3.6% of improvement and only 3% below the human performance reported by BIBREF0 . Our contributions can be summarized as follows: In this section",
        "type": "Document"
      },
      {
        "id": "17feda8b-84c9-44a4-a590-58fb4d9e1c2e",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1808.09920",
          "start_idx": 22186,
          "end_idx": 22698
        },
        "page_content": " advantage. This is our best system evaluating on the development. Since Entity-GCN seems to gain little advantage using the coreference system, we report test results both with and without using it. Surprisingly, with coreference, we observe performance degradation on the test set. It is likely that the test documents are harder for the coreference system. We do perform one last ablation, namely, we replace our heuristic for assigning edges and their labels by a model component that predicts them. The last",
        "type": "Document"
      },
      {
        "id": "9970ed1f-918c-402d-88a8-154189c275de",
        "metadata": {
          "chunk_id": 76,
          "document_id": "1808.09920",
          "start_idx": 25894,
          "end_idx": 26375
        },
        "page_content": " Pearson's correlation). This is important as document sets can be large in practical applications. See Figure 3 in the supplemental material for plots. In Table 6 , we report three samples from WikiHop development set where out Entity-GCN fails. In particular, we show two instances where our model presents high confidence on the answer, and one where is not. We commented these samples explaining why our model might fail in these cases. In previous work, BiDAF BIBREF3 , FastQA",
        "type": "Document"
      },
      {
        "id": "c79ba06f-fbda-4ea2-862c-6fa44d16d8f7",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1808.09920",
          "start_idx": 3800,
          "end_idx": 4290
        },
        "page_content": ". This implies that only a lightweight computation has to be performed online, both at train and test time, whereas the rest is preprocessed. Even in the somewhat contrived WikiHop setting, where fairly small sets of candidates are provided, the model is at least 5 times faster to train than BiDAF. Interestingly, when we substitute ELMo with simple pre-trained word embeddings, Entity-GCN still performs on par with many techniques that use expensive question-aware recurrent document enc",
        "type": "Document"
      },
      {
        "id": "7af44895-0514-45fa-a3ae-37ebb33e1129",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1808.09920",
          "start_idx": 21894,
          "end_idx": 22391
        },
        "page_content": " document. Secondly, we notice that coreference links and complement edges seem to play a more marginal role. Though it may be surprising for coreference edges, recall that the MATCH heuristic already captures the easiest coreference cases, and for the rest the out-of-domain coreference system may not be reliable. Still, modelling all these different relations together gives our Entity-GCN a clear advantage. This is our best system evaluating on the development. Since Entity-GCN seems to gain",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How did they get relations between mentions?",
    "llm_answer": "The provided text describes using predictions from a coreference resolution system to add mentions of elements.  It also describes associating mentions by encoding selected entity information.  The exact method for getting relations between mentions isn't fully explained.\n",
    "context": [
      {
        "id": "116d63ff-ea12-4da6-a510-80a348767ca0",
        "metadata": {
          "chunk_id": 99,
          "document_id": "1902.00330",
          "start_idx": 33470,
          "end_idx": 33958
        },
        "page_content": " the pairs of mentions at most once, they assume each mention only needs to be consistent with one another mention in the document. The limitation of their method is that the consistency information is too sparse, resulting in low confidence. Similar to us, Guo et al. BIBREF18 also sort mentions according to the difficulty of disambiguation, but they did not make full use of the information of previously referred entities for the subsequent entity disambiguation. Nguyen et al. BIBREF",
        "type": "Document"
      },
      {
        "id": "d8cb4aa8-5614-4d4d-bdba-cf0d4ad9d238",
        "metadata": {
          "chunk_id": 87,
          "document_id": "1902.00330",
          "start_idx": 28375,
          "end_idx": 28918
        },
        "page_content": " most of datasets, indicating that it is effective to place the mention that with a higher local similarity in front of the sequence. It is worth noting that the effect of ranking mentions is not obvious on the MSNBC dataset, the reason is that most of mentions in MSNBC have similar local similarities, the order of disambiguation has little effect on the final result. Most of previous methods mainly use the similarities between entities to correlate each other, but our model associates them by encoding the selected entity information. To",
        "type": "Document"
      },
      {
        "id": "f5de65bc-7cfa-40cd-800c-b44a0af4fad2",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1808.09920",
          "start_idx": 7321,
          "end_idx": 7717
        },
        "page_content": "cup \\lbrace s\\rbrace $ and create one node per mention. This process is based on the following heuristic: we consider mentions spans in $S_q$ exactly matching an element of $C_q \\cup \\lbrace s\\rbrace $ . Admittedly, this is a rather simple strategy which may suffer from low recall. we use predictions from a coreference resolution system to add mentions of elements in $C_q \\cup \\lbrace s\\rbrace",
        "type": "Document"
      },
      {
        "id": "32162fa0-a4d2-41a9-a298-ea422b680ad8",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1603.04553",
          "start_idx": 4332,
          "end_idx": 4638
        },
        "page_content": " separately for each mention: $\nc^*_j = \\operatornamewithlimits{argmax}\\limits _{c_j} P(m_j|m_{c_j}) P(c_j|j)\n$  The model is a so-called ranking model because it is able to identify the most probable candidate antecedent given a mention to be resolved. According to previous work BIBREF17 , BIBREF18 , BIB",
        "type": "Document"
      },
      {
        "id": "31dd0252-7e67-44b8-914c-0cb67096c7f8",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1605.08675",
          "start_idx": 15409,
          "end_idx": 15878
        },
        "page_content": " extracted (ENTITY RECOGNITION). For each of them, a context is generated (CONTEXT GENERATION). Finally, a distance between a question content and the entity context is computed to asses its relevance (DISTANCE MEASURE). All the mentions and their distance scores are stored and, after no more documents are left, used to select the best match (BEST ENTITY SELECTION). The system returns the entity, supplied with information about a supporting sentence and a document,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How did they detect entity mentions?",
    "llm_answer": "The basic rules used to detect mentions are similar to those of Lee:2013:CL, except that a set of filtering rules is used to discard extracted entities.\n",
    "context": [
      {
        "id": "71add7a9-936e-44bd-ae98-fa4f6890b234",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1603.04553",
          "start_idx": 8719,
          "end_idx": 9194
        },
        "page_content": "bergsma:2005 and charniak-elsner:2009 applied EM for pronoun anaphora resolution. ng:2008:EMNLP probabilistically induced coreference partitions via EM clustering. Recently, moosavi2014 proposed an unsupervised model utilizing the most informative relations and achieved competitive performance with the Stanford system. The basic rules we used to detect mentions are similar to those of Lee:2013:CL, except that their system uses a set of filtering rules designed to discard",
        "type": "Document"
      },
      {
        "id": "31dd0252-7e67-44b8-914c-0cb67096c7f8",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1605.08675",
          "start_idx": 15409,
          "end_idx": 15878
        },
        "page_content": " extracted (ENTITY RECOGNITION). For each of them, a context is generated (CONTEXT GENERATION). Finally, a distance between a question content and the entity context is computed to asses its relevance (DISTANCE MEASURE). All the mentions and their distance scores are stored and, after no more documents are left, used to select the best match (BEST ENTITY SELECTION). The system returns the entity, supplied with information about a supporting sentence and a document,",
        "type": "Document"
      },
      {
        "id": "ec5a629b-65ba-4d08-9fa1-43e9732f6b93",
        "metadata": {
          "chunk_id": 105,
          "document_id": "1902.00330",
          "start_idx": 35978,
          "end_idx": 36422
        },
        "page_content": " In experiments, we evaluate our method on AIDA-B and other well-known datasets, the results show that our system outperforms state-of-the-art solutions. In the future, we would like to use reinforcement learning to detect mentions and determine which mention should be firstly disambiguated in the document. This research is supported by the GS501100001809National Key Research and Development Program of China (No. GS5011000018092018YFB100470",
        "type": "Document"
      },
      {
        "id": "2915e0ed-60b5-4b39-a7b3-b5401417e407",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1605.08675",
          "start_idx": 25068,
          "end_idx": 25585
        },
        "page_content": "ERF, Liner2 and Quant) or novel DeepER; this choice affects its overall performance. Experiments showing precision and recall of the whole system with respect to applied entity recognition technique are demonstrated in section SECREF88 . An entity recognition step is performed within the question answering process and aims at selecting all entity mentions in a given annotated document. Before it begins, the entity library is read into a PATRICIA trie, a very efficient prefix tree. In this structure, every entity",
        "type": "Document"
      },
      {
        "id": "8b31f51a-97bb-40d4-ac4b-8f8ca6081eec",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1902.00330",
          "start_idx": 7750,
          "end_idx": 8140
        },
        "page_content": " and the corresponding candidate set $\\lbrace e_t^1, e_t^2,..., \\\\ e_t^k\\rbrace $ , we aim to get their local representation based on the mention context and the candidate entity description. For each mention, we firstly select its $n$ surrounding words, and represent them as word embedding using a pre-trained lookup table BIBREF11 . Then, we use Long Short-Term Memory (LSTM) networks to",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What performance does the Entity-GCN get on WIKIHOP?",
    "llm_answer": "Entity-GCN achieves over 2% improvement over the best previously-published results, and an ensemble brings a further 3.6% improvement, only 3% below human performance.\n",
    "context": [
      {
        "id": "c79ba06f-fbda-4ea2-862c-6fa44d16d8f7",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1808.09920",
          "start_idx": 3800,
          "end_idx": 4290
        },
        "page_content": ". This implies that only a lightweight computation has to be performed online, both at train and test time, whereas the rest is preprocessed. Even in the somewhat contrived WikiHop setting, where fairly small sets of candidates are provided, the model is at least 5 times faster to train than BiDAF. Interestingly, when we substitute ELMo with simple pre-trained word embeddings, Entity-GCN still performs on par with many techniques that use expensive question-aware recurrent document enc",
        "type": "Document"
      },
      {
        "id": "c471a91a-35d8-4b63-a1fa-e5096bd17211",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1808.09920",
          "start_idx": 19847,
          "end_idx": 20268
        },
        "page_content": " this experiment we investigate the effect of the different relations available in the entity graph and processed by the R-GCN module. We start off by testing our stronger encoder (i.e., ELMo) in absence of edges connecting mentions in the supporting documents (i.e., using only self-loops \u2013 No R-GCN in Table 3 ). The results suggest that WikipHop genuinely requires multihop inference, as our best model is 6.1% and 8.4",
        "type": "Document"
      },
      {
        "id": "ef054914-a25b-4a13-8502-c9c8efdefa76",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1808.09920",
          "start_idx": 4290,
          "end_idx": 4787
        },
        "page_content": " Entity-GCN still performs on par with many techniques that use expensive question-aware recurrent document encoders. Despite not using recurrent document encoders, the full Entity-GCN model achieves over 2% improvement over the best previously-published results. As our model is efficient, we also reported results of an ensemble which brings further 3.6% of improvement and only 3% below the human performance reported by BIBREF0 . Our contributions can be summarized as follows: In this section",
        "type": "Document"
      },
      {
        "id": "9970ed1f-918c-402d-88a8-154189c275de",
        "metadata": {
          "chunk_id": 76,
          "document_id": "1808.09920",
          "start_idx": 25894,
          "end_idx": 26375
        },
        "page_content": " Pearson's correlation). This is important as document sets can be large in practical applications. See Figure 3 in the supplemental material for plots. In Table 6 , we report three samples from WikiHop development set where out Entity-GCN fails. In particular, we show two instances where our model presents high confidence on the answer, and one where is not. We commented these samples explaining why our model might fail in these cases. In previous work, BiDAF BIBREF3 , FastQA",
        "type": "Document"
      },
      {
        "id": "a77dade0-f730-45a3-894b-c84f9e07b4d6",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1808.09920",
          "start_idx": 4572,
          "end_idx": 5066
        },
        "page_content": " human performance reported by BIBREF0 . Our contributions can be summarized as follows: In this section we explain our method. We first introduce the dataset we focus on, WikiHop by BIBREF0 , as well as the task abstraction. We then present the building blocks that make up our Entity-GCN model, namely, an entity graph used to relate mentions to entities within and across documents, a document encoder used to obtain representations of mentions in context, and a relational graph convolution",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What document context was added?",
    "llm_answer": "A list of ranked and scored pseudodocuments relevant to the question.\n",
    "context": [
      {
        "id": "b4d3e65f-0959-4347-ad5e-3ae280bfc8ef",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1910.07481",
          "start_idx": 7995,
          "end_idx": 8544
        },
        "page_content": " means that only a fraction of the source training data contains document context. The enhanced model learns to use document information only when it is available. All test sets contain well delimited documents, Baseline models are evaluated on standard corpora while Document models are evaluated on the same standard corpora that have been augmented with document context. We evaluate the English-German systems on newstest2017, newstest2018 and newstest2019 where documents consist of newspaper articles to keep consistency with the training data",
        "type": "Document"
      },
      {
        "id": "3b8f986c-dc64-4e5b-8c94-12eedad19de6",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1910.07481",
          "start_idx": 13011,
          "end_idx": 13548
        },
        "page_content": " document embeddings described by Eq. DISPLAY_FORM2 must be preserved for the model to fully benefit from document context. In this analysis we present some of the many cases that suggest the Document model can handle ambiguous situations. These examples are often isolated sentences where even a human translator could not predict the good translation without looking at the document, making it almost impossible for the Baseline model which is blind to the context. Table TABREF10 contains an extract of these interesting cases for the",
        "type": "Document"
      },
      {
        "id": "a70f7c4b-36fd-4e47-bb52-3cc41ed4870a",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1906.03338",
          "start_idx": 16306,
          "end_idx": 16766
        },
        "page_content": " discourse markers indicating an opposite class. In other words, in this setting we want to examine effects when porting a context-sensitive system to a multi-document setting. For example, as seen in Figure FIGREF42 , the context of an argumentative unit may change from \u201cHowever\u201d to \u201cMoreover\u201d \u2013 which can happen naturally in open debates. The results are displayed in Figure FIGREF43 . In the standard setting (Figure FIGREF43 ), the models that have access",
        "type": "Document"
      },
      {
        "id": "413a787d-69f8-4727-ae72-a67d37f2c2ea",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1909.02776",
          "start_idx": 7372,
          "end_idx": 7877
        },
        "page_content": " well.\u201d BIBREF35. Berenjkoub et al studied the effectiveness of various subsets of features in summarization of distinct sections of scientific papers BIBREF36. They showed that some features work well only in some specific portion of text, for example, on the abstract section, while others perform better on the methodology section. This could be considered to be a consequence of differences in the structure and context of each section. All the above studies imply the significance of document context",
        "type": "Document"
      },
      {
        "id": "c06dec6b-c6ba-40a3-86b9-c972a0990c11",
        "metadata": {
          "chunk_id": 21,
          "document_id": "1707.03904",
          "start_idx": 8059,
          "end_idx": 8503
        },
        "page_content": "Scale $485 day + Picasso Painting\u201d) to debatable (\u201cAccording to Earth Medicine what's the birth totem for march\u201d \u201cThe Falcon\u201d) The context document for each record consists of a list of ranked and scored pseudodocuments relevant to the question. Context documents for each query were generated in a two-phase fashion, first collecting a large pool of semirelevant text, then filling a temporary index with short or long pseudodocuments from the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What were the results of the first experiment?",
    "llm_answer": "The accuracies of classifiers trained on the filtered datasets were very similar to the accuracy of the classifier on the unfiltered dataset.\n",
    "context": [
      {
        "id": "fcfada0a-4a28-4ceb-b822-a1bb0d8ac606",
        "metadata": {
          "chunk_id": 118,
          "document_id": "1803.02839",
          "start_idx": 45558,
          "end_idx": 45772
        },
        "page_content": " mathematical consistency of the experiments, and provided useful insights into the results. Mark prompted the investigation into potential emergent monoid structures since they appear frequently in state machines.",
        "type": "Document"
      },
      {
        "id": "78dd82c6-c6af-4015-b4e0-f73037eb863b",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1904.05862",
          "start_idx": 15343,
          "end_idx": 15454
        },
        "page_content": "letter++ experiments, and Tatiana Likhomanenko for providing convolutional language models for our experiments.",
        "type": "Document"
      },
      {
        "id": "32fc7928-368d-47ad-9739-5c2dad16beb4",
        "metadata": {
          "chunk_id": 80,
          "document_id": "1607.06025",
          "start_idx": 30690,
          "end_idx": 31233
        },
        "page_content": " error rate and the accuracy of the classifier. This observation led us to the experiment, where the generated dataset was filtered according to the prediction probability of the discriminative model. Two disjoint filtered datasets were created. One with hypotheses that had high probability that they come from the original distribution and the other one with low probability. However, the accuracies of classifiers trained on these datasets were very similar to the accuracy of the classifier on the unfiltered dataset. Similar test was also",
        "type": "Document"
      },
      {
        "id": "3b942450-5125-4b16-8163-c4e7540353a0",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1802.05322",
          "start_idx": 13905,
          "end_idx": 14341
        },
        "page_content": " explanation of the whole process needed to conduct an experiment like this. The process includes downloading a data set, web scraping for extra information, data preprocessing, model tuning and evaluation of the results. Action Adult Adventure Animation Biography Comedy Crime Documentary Drama Family Fantasy Film-Noir Game-Show History Horror Music Musical Mystery Reality-TV Romance Sci-Fi Short Sport Talk-Show Thriller War Western",
        "type": "Document"
      },
      {
        "id": "ffcdee05-7820-4632-b45a-84ec7ddff98e",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1604.05781",
          "start_idx": 3160,
          "end_idx": 3653
        },
        "page_content": " organized as follows: In Sec. \"Materials and Methods\" we discuss our materials and methods, including the dataset we studied, how we preprocessed that data and extracted a `causal' corpus and a corresponding `control' corpus, and the details of the statistical and language analysis tools we studied these corpora with. In Sec. \"Results\" we present results using these tools to compare the causal statements to control statements. We conclude with a discussion in Sec. \"Discussion\" . Data was",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does their model outperform existing methods?",
    "llm_answer": "Their model surpasses the current state-of-the-art model with an accuracy of 84.84%.\n",
    "context": [
      {
        "id": "38d0d5d3-4be2-4a08-bc2e-e9f401074e5d",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1701.06538",
          "start_idx": 17346,
          "end_idx": 17826
        },
        "page_content": ". Details can be found in Appendix UID77 . Results of these three models form the bottom line of Figure FIGREF32 -right. Table TABREF33 compares the results of these models to the best previously-published result on this dataset . Even the fastest of these models beats the best published result (when controlling for the number of training epochs), despite requiring only 6% of the computation. We trained our models using TensorFlow BIBREF30 on clusters containing 16-32 Tesla K",
        "type": "Document"
      },
      {
        "id": "e60e0d3d-b592-4a3f-9ebd-b12edba8a33b",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1909.13375",
          "start_idx": 20788,
          "end_idx": 21299
        },
        "page_content": " For the other types, most of the improvement came from better preprocessing. A more detailed discussion could be found in Section (SECREF36). Notice that different BERTlarge models were used, so the comparison is less direct. Overall, our large models exhibits similar results to those of MTMSNlarge. For multi-span questions we achieve a significantly better performance. While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still",
        "type": "Document"
      },
      {
        "id": "5e0f3225-070e-4250-8f70-93c083311ec4",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "76b8b551-5acb-4396-ad68-bd7052c2758b",
        "metadata": {
          "chunk_id": 17,
          "document_id": "2002.06644",
          "start_idx": 6467,
          "end_idx": 6878
        },
        "page_content": "ERT based models, and BERT-based ensemble models outperform the baselines for all the metrics. Among the optimized BERT based models, $RoBERTa_{large}$ outperforms all other non-ensemble models and the baselines for all metrics. It further achieves a maximum recall of $0.681$ for all the proposed models. We note that DistillRoBERTa, a distilled model, performs competitively, achieving $69.69\\%$ accuracy, and",
        "type": "Document"
      },
      {
        "id": "4e04ab32-f047-4b22-8b03-4e6977b298f9",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1901.02257",
          "start_idx": 18849,
          "end_idx": 19405
        },
        "page_content": " Our ensemble model surpasses the current state-of-the-art model with an accuracy of 84.84%. We got the final ensemble result by voting on 4 single models. Every single model uses the same architecture but different parameters. To study the effectiveness of each perspective, we conduct several experiments on the three single perspectives and their combination perspective. Table 3 presents their comparison results. The first group of models are based on the three single perspectives, and we can observe that the union perspective performs best compared",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance of their model?",
    "llm_answer": "The models surpass previously published results, even with significantly less computation.  Base models outperform the NABERT+ baseline.  Large models show substantial improvement over base models, especially in multi-span questions.  Results vary depending on the dataset (e.g., 73% on TEC, 57% on AMMER).  Comparing to MTMSNlarge, performance is similar, but better for multi-span questions.  Models have similar computational budget and training times to the best published models.\n",
    "context": [
      {
        "id": "38d0d5d3-4be2-4a08-bc2e-e9f401074e5d",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1701.06538",
          "start_idx": 17346,
          "end_idx": 17826
        },
        "page_content": ". Details can be found in Appendix UID77 . Results of these three models form the bottom line of Figure FIGREF32 -right. Table TABREF33 compares the results of these models to the best previously-published result on this dataset . Even the fastest of these models beats the best published result (when controlling for the number of training epochs), despite requiring only 6% of the computation. We trained our models using TensorFlow BIBREF30 on clusters containing 16-32 Tesla K",
        "type": "Document"
      },
      {
        "id": "5e0f3225-070e-4250-8f70-93c083311ec4",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "0ab3fdf3-8ea1-480c-8cac-92715fc9d565",
        "metadata": {
          "chunk_id": 52,
          "document_id": "1909.02764",
          "start_idx": 20420,
          "end_idx": 20841
        },
        "page_content": "=73 % on TEC. The model trained on our AMMER corpus achieves an F1 score of 57%. This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set. These results are mostly in line with Bostan2018. Now we analyze how well the models trained in Experiment 1 perform when applied to our data set. The results are shown in column \u201cSimple\u201d in Table TAB",
        "type": "Document"
      },
      {
        "id": "e60e0d3d-b592-4a3f-9ebd-b12edba8a33b",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1909.13375",
          "start_idx": 20788,
          "end_idx": 21299
        },
        "page_content": " For the other types, most of the improvement came from better preprocessing. A more detailed discussion could be found in Section (SECREF36). Notice that different BERTlarge models were used, so the comparison is less direct. Overall, our large models exhibits similar results to those of MTMSNlarge. For multi-span questions we achieve a significantly better performance. While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still",
        "type": "Document"
      },
      {
        "id": "7dac3f56-f4b3-4057-b573-9c4758efa5ba",
        "metadata": {
          "chunk_id": 88,
          "document_id": "1701.06538",
          "start_idx": 32330,
          "end_idx": 32790
        },
        "page_content": " INLINEFORM1 respectively, showing that even in the presence of a large MoE, more computation is still useful. Results are reported at the bottom of Table TABREF76 . The larger of the two models has a similar computational budget to the best published model from the literature, and training times are similar. Comparing after 10 epochs, our model has a lower test perplexity by INLINEFORM2 . The models are similar in structure to the 8-million-operations-per",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they damage different neural modules?",
    "llm_answer": "Damage to the analog of Broca's area (the LSTMs) is described.  The context does not detail damage to other neural modules.\n",
    "context": [
      {
        "id": "e33387de-f32b-4349-9a2a-08565fc221f0",
        "metadata": {
          "chunk_id": 7,
          "document_id": "2001.07263",
          "start_idx": 2754,
          "end_idx": 3239
        },
        "page_content": "ization by suppressing some of the effects of static noise on the targets. Dropout randomly deactivates neurons with a predefined probability in every training step BIBREF13 to reduce co-adaptation of neurons. DropConnect, which is similar in spirit to dropout, randomly deactivates connections between neurons by temporarily zeroing out weights BIBREF14. Zoneout, which is also inspired by dropout and was especially developed for recurrent models BIBREF15, stochastically forces some",
        "type": "Document"
      },
      {
        "id": "5f04e62f-d78e-478d-a78c-85260ba56d69",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1911.11744",
          "start_idx": 9158,
          "end_idx": 9592
        },
        "page_content": " $$ after being processed in a single perceptron with weight $_G$ and bias $_G$. These parameters are then used in the third part of the network, which is a DMP BIBREF0, allowing us leverage a large body of research regarding their behavior and stability, while also allowing other extensions of DMPs BIBREF5, BIBREF24, BIBREF25 to be incorporated to our framework. We evaluate our model in a simulated binning task in which the robot",
        "type": "Document"
      },
      {
        "id": "dd1bf791-9bb1-437a-bc06-043e6bf3256b",
        "metadata": {
          "chunk_id": 29,
          "document_id": "2002.08899",
          "start_idx": 10466,
          "end_idx": 10912
        },
        "page_content": " damaged to resemble the damage that causes aphasia. To avoid cherry-picking the results, we arbitrarily chose an input for each domain, subject to the following constraint. The input is not in the train set and the undamaged LLA-LSTM model produces a translation that we judge to be correct. For all inputs that we chose, damage to the analog of Broca's area (the LSTMs) results in an output that describes content only if it is described by the",
        "type": "Document"
      },
      {
        "id": "c2fb5d70-19e9-4567-a481-45842aab191b",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1909.02560",
          "start_idx": 24644,
          "end_idx": 25212
        },
        "page_content": "ially trained models, as shown in Table TABREF18. After adversarial training, the performance of all the target models raises significantly, while that on the original examples remain comparable. Note that since the focus of this paper is on model robustness which can hardly be reflected in original data, we do not expect performance improvement on original data. The results demonstrate that adversarial training with our adversarial examples can significantly improve the robustness we focus on without remarkably hurting the performance on original data. Moreover",
        "type": "Document"
      },
      {
        "id": "82f27514-8bda-46c4-aa7b-304508efcf55",
        "metadata": {
          "chunk_id": 28,
          "document_id": "2001.06888",
          "start_idx": 10924,
          "end_idx": 11447
        },
        "page_content": " and reduces spelling noise. It also helps the model to have an embedding when pretrained word embedding is empty or initialized as random for new words. These words are encountered when they were not present in the training set, thus, in the test phase, model fails to provide a useful embedding. Multimodal learning has become an emerging research interest and with the rise of deep learning techniques, it has become more visible in different research areas ranging from medical imaging to image segmentation and natural",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How long are the datasets?",
    "llm_answer": "The context describes various datasets with varying sizes.  No single answer for \"how long\" is provided.\n",
    "context": [
      {
        "id": "001abd8b-7ba1-407b-8110-49e732f239a9",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1910.12618",
          "start_idx": 19089,
          "end_idx": 19565
        },
        "page_content": " maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014",
        "type": "Document"
      },
      {
        "id": "c074198c-0ba3-4756-8dd2-e14dd86518bc",
        "metadata": {
          "chunk_id": 39,
          "document_id": "2004.01980",
          "start_idx": 13028,
          "end_idx": 13433
        },
        "page_content": " romantic, and click-baity text. The average sentence length in the NYT, CNN, Humor, Romance, and Clickbait datasets are 8.8, 9.2, 12.6, 11.6 and 8.7 words, respectively. The source dataset contains news articles paired with corresponding headlines. To enrich the training corpus, we combine two datasets: the New York Times (56K) and CNN (90K). After combining these two datasets, we randomly selected 3,",
        "type": "Document"
      },
      {
        "id": "6f04ea61-adc5-4abb-9261-abc554ebac1b",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1711.02013",
          "start_idx": 20658,
          "end_idx": 21116
        },
        "page_content": "ings, between recurrent layers, and on recurrent states were (0.7, 0.5, 0.5) respectively. dataset contains 17M training tokens and has a vocabulary size of 44k words. The dataset is partitioned into a training set (first 99M characters) and a development set (last 1M characters) that is used to report performance. As this dataset contains various articles from Wikipedia, the longer term information (such as current topic) plays a bigger role than in the",
        "type": "Document"
      },
      {
        "id": "2d3bdfec-4ed5-456d-bd82-023fad798858",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "b2a50f9d-7c05-4ecd-b65d-87cb9f0f75d3",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1906.03538",
          "start_idx": 12505,
          "end_idx": 12830
        },
        "page_content": ". We now provide a brief summary of [wave]390P[wave]415e[wave]440r[wave]465s[wave]485p[wave]525e[wave]535c[wave]595t[wave]610r[wave]635u[wave]660m. The dataset contains about INLINEFORM0 claims with a significant length diversity (Table TABREF19 ). Additionally, the dataset comes with INLINEFORM1 perspectives, most of which",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the sources of the data?",
    "llm_answer": "Online debate websites, Twitter data, and Arabic WikiNews.\n",
    "context": [
      {
        "id": "00ea7799-d261-4f01-b6cc-6b0dec5e75e6",
        "metadata": {
          "chunk_id": 64,
          "document_id": "1906.03538",
          "start_idx": 25144,
          "end_idx": 25628
        },
        "page_content": " US Defense Advanced Research Projects Agency (DARPA). The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. We provide brief statistics on the sources of different content in our dataset in Table TABREF46 . In particular, this table shows: the size of the data collected from online debate websites (step 1). the size of the data filtered out (step 2a). the size of the perspectives added",
        "type": "Document"
      },
      {
        "id": "2870538f-2c2a-4761-b1d7-7d431f1d1212",
        "metadata": {
          "chunk_id": 6,
          "document_id": "2001.05970",
          "start_idx": 2564,
          "end_idx": 3064
        },
        "page_content": " methods of gathering data are limited in scale and might be influenced by the psychological and cognitive tendencies of respondents not to provide faithful answers BIBREF5. The ubiquity of social media has motivated various research on widely-debated social topics such as gang violence, hate code, or presidential election using Twitter data BIBREF6, BIBREF7, BIBREF8, BIBREF9. Recently, researchers have taken the earliest steps to understand sexual harassment using textual data on Twitter. Using",
        "type": "Document"
      },
      {
        "id": "85e542a8-0eb1-4992-92d1-94606138b39d",
        "metadata": {
          "chunk_id": 33,
          "document_id": "1906.03538",
          "start_idx": 13163,
          "end_idx": 13614
        },
        "page_content": " Table TABREF19 . To better understand the topical breakdown of claims in the dataset, we crowdsource the set of \u201ctopics\u201d associated with each claim (e.g., Law, Ethics, etc.) We observe that, as expected, the three topics of Politics, World, and Society have the biggest portions (Figure FIGREF21 ). Additionally, the included claims touch upon 10+ different topics. Figure FIGREF22 depicts a few popular categories and sampled questions from each. We",
        "type": "Document"
      },
      {
        "id": "032b9bed-6a38-456d-b02a-dea6816fa084",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1910.12618",
          "start_idx": 7204,
          "end_idx": 7727
        },
        "page_content": " section 5 concludes this paper and gives insight on future possible work. In order to prove the consistency of our work, experiments have been conducted on two data sets, one for France and the other for the UK. In this section details about the text and time series data are given, as well as the major preprocessing steps. Three types of time series are considered in our work: national net electricity consumption (also referred as load or demand), national temperature and wind speed. The load data sets were retrieved",
        "type": "Document"
      },
      {
        "id": "06089622-0d0c-4172-adf5-98184f67dcea",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1710.06700",
          "start_idx": 5155,
          "end_idx": 5651
        },
        "page_content": " allowing researches to evaluate using the same dataset that we used, and reproduce same experiments. To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site https://ar.wikinews.org/wiki. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the new labeling strategy?",
    "llm_answer": "A two-stage labeling strategy for sentiment texts.  In the first stage, annotators label short texts with pure sentiment; in the second stage, a smaller number of texts with mixed sentiment are labeled by multiple annotators.\n",
    "context": [
      {
        "id": "631696ff-19ee-4ffe-9872-0c081a4d9a16",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1803.07771",
          "start_idx": 1808,
          "end_idx": 2303
        },
        "page_content": " issues with a new methodology. First, we introduce a two-stage labeling strategy for sentiment texts. In the first stage, annotators are invited to label a large number of short texts with relatively pure sentiment orientations. Each sample is labeled by only one annotator. In the second stage, a relatively small number of text samples with mixed sentiment orientations are annotated, and each sample is labeled by multiple annotators. Second, we propose a two-level long short-term memory (L",
        "type": "Document"
      },
      {
        "id": "377f27c4-7469-4ecb-b822-4e6f673cd8bc",
        "metadata": {
          "chunk_id": 92,
          "document_id": "1803.07771",
          "start_idx": 32711,
          "end_idx": 33231
        },
        "page_content": " are difficult for humans to label in text sentiment classification. In this study, a new labeling strategy is introduced to partition texts into those with pure and mixed sentiment orientations. These two categories of texts are labeled using different processes. A two-level network is accordingly proposed to utilize the two labeled data in our two-stage labeling strategy. Lexical cues (e.g., polar words, POS, conjunction words) are particularly useful in sentiment analysis. These lexical cues are used in our two-",
        "type": "Document"
      },
      {
        "id": "41356c46-4144-4d8b-bfb1-26f13ecb571c",
        "metadata": {
          "chunk_id": 64,
          "document_id": "1803.07771",
          "start_idx": 22848,
          "end_idx": 23303
        },
        "page_content": "stage labeling strategy. In the first stage, only one user is invited to label each clause sample as the sentiment orientations for clauses (or sub-sentences) are easy to label. In the second stage, five users are invited to label each text sample in the three raw data sets. The average score of the five users on each sample is calculated. Samples with average scores located in [0.6, 1] are labeled as \u201cpositive\". Samples with average scores located in",
        "type": "Document"
      },
      {
        "id": "61e02130-2d49-4b93-a090-4ddf583b91b5",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1909.13362",
          "start_idx": 6449,
          "end_idx": 6905
        },
        "page_content": "-of-speech tagging, named-entity recognition, and chunking BIBREF17. Neural networks have recently outpaced more traditional methods in sequence labeling tasks. These neural-based approaches are taking the place of HMMs, maximum entropy Markov models (MEMM), and conditional random fields (CRF) BIBREF18. In the following section and in Fig. FIGREF1, we present a neural network architecture that leverages both recurrence and one-dimensional convolutions.",
        "type": "Document"
      },
      {
        "id": "a57ee035-d944-4102-b8d4-a57c09e97aa2",
        "metadata": {
          "chunk_id": 74,
          "document_id": "2004.03744",
          "start_idx": 26850,
          "end_idx": 27381
        },
        "page_content": " their decision in a comprehensive and concise manner, using a subset of the words that they highlighted. The instructions are shown in Figure FIGREF42. Workers were also guided with three annotated examples, one for each label. For each assignment of 10 questions, one trusted annotation with gold standard label was inserted at a random position, as a measure to control the quality of label annotation. Each assignment was completed by three different workers. An example of question is shown in Figure FIGREF8 in the core paper",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are their changes evaluated?",
    "llm_answer": "Human translations are judged more favorably than MT when evaluation design changes are made.\n",
    "context": [
      {
        "id": "970623a6-44e3-4132-960c-df4ced9a1eca",
        "metadata": {
          "chunk_id": 86,
          "document_id": "1705.09665",
          "start_idx": 35314,
          "end_idx": 35882
        },
        "page_content": " future research is to examine the temporal dynamics in the multi-community landscape. By averaging our measures of distinctiveness and dynamicity across time, our present study treated community identity as a static property. However, as communities experience internal changes and respond to external events, we can expect the nature of their identity to shift as well. For instance, the relative consistency of harrypotter may be disrupted by the release of a new novel, while Seahawks may foster different identities during and between football seasons. Conversely",
        "type": "Document"
      },
      {
        "id": "2f087fa2-1aed-415a-a42c-95e05656d14d",
        "metadata": {
          "chunk_id": 4,
          "document_id": "2004.01694",
          "start_idx": 1628,
          "end_idx": 2197
        },
        "page_content": " changes in the evaluation design affect the outcome of the human evaluation. We find that for all three aspects, human translations are judged more favourably, and significantly better than MT, when we make changes that we believe strengthen the evaluation design. Based on our empirical findings, we formulate a set of recommendations for human MT evaluation in general, and assessing human\u2013machine parity in particular. All of our data are made publicly available for external validation and further analysis. We first review current methods to assess the quality of",
        "type": "Document"
      },
      {
        "id": "ddc891f7-8efa-46f5-af8a-dd20a1375322",
        "metadata": {
          "chunk_id": 53,
          "document_id": "1709.01256",
          "start_idx": 19942,
          "end_idx": 20473
        },
        "page_content": "e.g. addition, deletion or replacement), and save them as separate documents. These documents become revisions of the original documents. We started from an initial corpus that did not have revisions, and kept adding new documents and revising existing documents. Similar to a file system, at any moment new documents could be added and/or some of the current documents could be revised. The revision operations we used were deletion, addition and replacement of words, sentences, paragraphs, section names and document titles. The",
        "type": "Document"
      },
      {
        "id": "bc4a0f60-c159-4667-a613-4d43b77dc4ee",
        "metadata": {
          "chunk_id": 21,
          "document_id": "1912.01673",
          "start_idx": 8152,
          "end_idx": 8670
        },
        "page_content": " several of its possible alternations, see tab:firstroundexamples. Unfortunately, these examples turned out to be highly influential on the annotators' decisions and they correspond to almost two thirds of all of modifications gathered in the first round. Other very common transformations include change of a word order or transformation into a interrogative/imperative sentence. Other interesting modification were also proposed such as change into a fairy-tale style, excessive use of diminutives/vulgarisms or dada",
        "type": "Document"
      },
      {
        "id": "b8783fe6-678c-4111-87dd-8bd79d609045",
        "metadata": {
          "chunk_id": 1,
          "document_id": "1709.01256",
          "start_idx": 578,
          "end_idx": 1123
        },
        "page_content": " It is worthwhile to develop a system that is able to intelligently identify, manage and represent revisions. Given a collection of text documents, our study identifies revision relationships in a completely unsupervised way. For each document in a corpus we only use its content and the last modified timestamp. We assume that a document can be revised by many users, but that the documents are not merged together. We consider collaborative editing as revising documents one by one. The two research problems that are most relevant to document",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the six target languages?",
    "llm_answer": "English, Spanish, and Zulu.\n",
    "context": [
      {
        "id": "10b18f90-1375-4a41-9719-9a76bdd2287c",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1910.05456",
          "start_idx": 1974,
          "end_idx": 2423
        },
        "page_content": " on the L1, i.e., the \"native language\", in neural network models. To this goal, we select a diverse set of eight source languages from different language families \u2013 Basque, French, German, Hungarian, Italian, Navajo, Turkish, and Quechua \u2013 and three target languages \u2013 English, Spanish and Zulu. We pretrain a neural sequence-to-sequence architecture on each of the source languages and then fine-tune the resulting models on small datasets in each",
        "type": "Document"
      },
      {
        "id": "910e2fe4-f2a4-42fb-afd4-880f58ce8885",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1910.05456",
          "start_idx": 9884,
          "end_idx": 10340
        },
        "page_content": " additionally makes use of internal stem changes (e.g., o $\\rightarrow $ ue). Since English and Spanish are both Indo-European languages, and, thus, relatively similar, we further add a third, unrelated target language. We choose Zulu (ZUL), a Bantoid language. In contrast to the first two, it is strongly prefixing. For pretraining, we choose languages with different degrees of relatedness and varying morphological similarity to English, Spanish, and Z",
        "type": "Document"
      },
      {
        "id": "e8fb7d0e-86e1-4332-bb4a-d8109e4146e4",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1910.04269",
          "start_idx": 19553,
          "end_idx": 20023
        },
        "page_content": ", the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Kat",
        "type": "Document"
      },
      {
        "id": "6bc2bb18-6a66-4628-bef7-a0e1a75c3767",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1909.13362",
          "start_idx": 15624,
          "end_idx": 16095
        },
        "page_content": " language groupings within families. We selected six evaluation languages: English, Dutch, Italian, French, Basque, and Manipuri. These represent two language families (Indo-European, Sino-Tibetan), a language isolate thought to be unrelated to any existing language (Basque), and two different subfamilies within the Indo-European family (West Germanic, Romance). The primary constraint was the availability of syllabified datasets for training and testing. Table TABREF",
        "type": "Document"
      },
      {
        "id": "7da029d3-0e6a-48d3-9dd2-3cfd37690294",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1910.04269",
          "start_idx": 6853,
          "end_idx": 7342
        },
        "page_content": " comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the size of the released dataset?",
    "llm_answer": "287,226 documents for training, 13,368 for validation, and 11,490 for test.\n",
    "context": [
      {
        "id": "2d3bdfec-4ed5-456d-bd82-023fad798858",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "9ccb7945-9a96-417a-b52e-68855c4910b1",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "6b7f6ca6-24d5-4144-9cfd-92be2de31f39",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "6f04ea61-adc5-4abb-9261-abc554ebac1b",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1711.02013",
          "start_idx": 20658,
          "end_idx": 21116
        },
        "page_content": "ings, between recurrent layers, and on recurrent states were (0.7, 0.5, 0.5) respectively. dataset contains 17M training tokens and has a vocabulary size of 44k words. The dataset is partitioned into a training set (first 99M characters) and a development set (last 1M characters) that is used to report performance. As this dataset contains various articles from Wikipedia, the longer term information (such as current topic) plays a bigger role than in the",
        "type": "Document"
      },
      {
        "id": "95bf2b84-1a56-4eed-b193-4a34d2c76c4c",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1706.08032",
          "start_idx": 14264,
          "end_idx": 14661
        },
        "page_content": " datasets in Table III. for all datasets, the filter window size ( INLINEFORM0 ) is 7 with 6 feature maps each for the first wide convolution layer, the second wide convolution layer has a filter window size of 5 with 14 feature maps each. Dropout rate ( INLINEFORM1 ) is 0.5, INLINEFORM2 constraint, learning rate is 0.1 and momentum of 0.9. Mini-batch size for STS Corpus is 100 and others are 4",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which OpenIE systems were used?",
    "llm_answer": "OpenIE 4, MinIE, OLLIE, and Reverb.\n",
    "context": [
      {
        "id": "f20de040-0bb4-4ee4-8a86-6dfb689f156a",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1802.05574",
          "start_idx": 5763,
          "end_idx": 6247
        },
        "page_content": " most recent version of Open IE (version 5) is focused on n-ary relations. For ease of judgement, we focused on binary relations. Additionally, both systems support the detection of negative relations. In terms of settings, we used the off the shelf settings for OpenIE 4. For MinIE, we used their \u201csafe mode\" option, which uses slightly more aggressive minimization than the standard setting. In the recent evaluation of MiniIE, this setting performed roughly on par with the default",
        "type": "Document"
      },
      {
        "id": "47800e8e-8c81-414b-bd8a-c0cb42345be9",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1905.07471",
          "start_idx": 0,
          "end_idx": 447
        },
        "page_content": "Open Information Extraction (OpenIE) is the NLP task of generating (subject, relation, object) tuples from unstructured text e.g. \u201cFed chair Powell indicates rate hike\u201d outputs (Powell, indicates, rate hike). The modifier open is used to contrast IE research in which the relation belongs to a fixed set. OpenIE has been shown to be useful for several downstream applications such as knowledge base construction BIBREF0 , textual entailment BIBREF",
        "type": "Document"
      },
      {
        "id": "7eb6f87c-ca9c-4a6e-9da6-ef82ef175ff1",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1802.05574",
          "start_idx": 5024,
          "end_idx": 5440
        },
        "page_content": " two popular OIE systems OLLIE BIBREF10 and Reverb BIBREF10 . We view this as a baseline system. The second was MinIE BIBREF7 , which is reported as performing better than OLLIE, ClauseIE BIBREF9 and Stanford OIE BIBREF9 . MinIE focuses on the notion of minimization - producing compact extractions from sentences. In our experience using OIE on scientific text, we have found that these systems often produce overly",
        "type": "Document"
      },
      {
        "id": "c2d03061-f954-4795-b46d-69106baef316",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1905.07471",
          "start_idx": 3391,
          "end_idx": 3748
        },
        "page_content": "IBREF8 , ReVerb BIBREF9 , OLLIE BIBREF10 , Stanford OpenIE BIBREF11 , and PropS BIBREF12 . Stanovsky et al. SupervisedOIE frame OpenIE as a BIO-tagging problem and train an LSTM to tag an input sentence. Tuples can be derived from the tagger, input, and BIO CFG parser. This method outperforms traditional systems, though the tagging scheme inherently const",
        "type": "Document"
      },
      {
        "id": "0ac30099-870a-4e83-b97f-09989be9ff54",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1905.07471",
          "start_idx": 4053,
          "end_idx": 4538
        },
        "page_content": " The system is inhibited by its synthetic training data which is bootstrapped from a rule-based system. Due to the lack of large datasets for OpenIE, previous works have focused on generating datasets from other tasks. These have included QA-SRL datasets BIBREF14 and QAMR datasets BIBREF6 . These methods are limited by the size of the source training data which are an order of magnitude smaller than existing reading comprehension datasets. Span-based Question-Answer datasets are a",
        "type": "Document"
      }
    ]
  },
  {
    "query": "how are the bidirectional lms obtained?",
    "llm_answer": "A bidirectional LSTM (BiLSTM) is formed by concatenating an LSTM moving forward through time with an LSTM moving backward through time.\n",
    "context": [
      {
        "id": "f6098347-a188-41a0-a901-b5f39bdd0646",
        "metadata": {
          "chunk_id": 20,
          "document_id": "2003.05377",
          "start_idx": 7538,
          "end_idx": 7792
        },
        "page_content": " for $h_{t-1}$ input; $U_f$, $U_i$, $U_C$, $U_o$ are the weight matrices for $x_t$ input; and $b_f$, $b_i$, $b_C$, $b_o$ are the bias vectors. Basically, a Bidirectional LSTM network consists of using two LSTM networks: a forward LSTM and a backward LSTM",
        "type": "Document"
      },
      {
        "id": "ec995174-d6ae-4170-9adf-3f57fdc6856c",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1902.10525",
          "start_idx": 15990,
          "end_idx": 16460
        },
        "page_content": " bidirectional LSTMs, i.e. we process the input sequence forward and backward and merge the output states of each layer before feeding them to the next layer. The exact number of layers and nodes is determined empirically for each script. We give an overview of the impact of the number of nodes and layers in section SECREF4 . We also list the configurations for several scripts in our production system, as of this writing. The output of the LSTM layers at each timest",
        "type": "Document"
      },
      {
        "id": "e435ab24-c0e5-43c0-b913-5860975a3aa1",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1909.13362",
          "start_idx": 9631,
          "end_idx": 10052
        },
        "page_content": " Thus, processing a phonetic sequence in both the forward and backwards directions provides an improved context for assigning syllable boundaries. A bidirectional LSTM (BiLSTM) is formed when an LSTM moving forward through time is concatenated with an LSTM moving backward through time BIBREF23. We use the LSTM network as follows. The $x$ vector is fed through the LSTM network which outputs a vector $\\overrightarrow{h_",
        "type": "Document"
      },
      {
        "id": "5e019aa2-6b4c-462a-90ba-727e9c24977e",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1812.06705",
          "start_idx": 9029,
          "end_idx": 9352
        },
        "page_content": "i=1}^{N}p(t_i|t_{i+1}, t_{i+2},..., t_N).$$   (Eq. 9)  Traditionally, a bidirectional language model a shallow concatenation of independently trained forward and backward LMs. In order to train a deep bidirectional language model, BERT proposed Masked Language Model (MLM) task, which was also referred to Cloze Task BIBREF",
        "type": "Document"
      },
      {
        "id": "1673edab-541a-4113-b9f8-eb59b1cec4db",
        "metadata": {
          "chunk_id": 86,
          "document_id": "2001.02380",
          "start_idx": 33875,
          "end_idx": 34249
        },
        "page_content": " from the composed output of the function $h$ across time steps $0 \\ldots t$, $\\delta \\in \\lbrace b,f\\rbrace $ is the direction of the respective LSTMs, $c_t^\\delta $ is the recurrent context in each direction and $\\theta = {W,b}$ gives the model weights and bias parameters (see BIBREF46 for details). Note that although the output of the system is ostensibly a probability",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what metrics are used in evaluation?",
    "llm_answer": "Manual evaluation, ROUGE, BLEU, METEOR, task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, F1-score, BPRA.\n",
    "context": [
      {
        "id": "08e94578-b4df-43f1-b96f-4db730b4d02a",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1607.06025",
          "start_idx": 10088,
          "end_idx": 10607
        },
        "page_content": "REF37 , the evaluation metrics of text-generating models fall into three categories: manual evaluation, automatic evaluation metrics, task-based evaluation. In evaluation based on human judgment each generated textual example is inspected manually. The automatic evaluation metrics, like ROUGE, BLEU and METEOR, compare human texts and generated texts. BIBREF38 shows METEOR has the strongest correlation with human judgments in image description evaluation. The last category is task-based evaluation, where the impact",
        "type": "Document"
      },
      {
        "id": "6b1e72f3-8ee5-43ac-a062-f08c158e3d67",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1709.10217",
          "start_idx": 6394,
          "end_idx": 6850
        },
        "page_content": " evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the number of total tasks. User satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied, neutral, satisfied and very satisfied, respectively. Response fluency: There are three scores -1, 0, 1, which indicate nonfluency, neutral, fluency. Number of dialogue turns: The number of utterances in a task",
        "type": "Document"
      },
      {
        "id": "b3036530-5228-47f4-b1f0-9a1687f209da",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1709.10217",
          "start_idx": 4141,
          "end_idx": 4657
        },
        "page_content": ", in which only the released data can be used for training and developing. The other is an open evaluation that allow to collect external data for training and developing. For task 1, we use F1-score as evaluation metric. For the task-oriented dialogue systems, the best way for evaluation is to use the online human-computer dialogue. After finishing an online human-computer dialogue with a dialogue system, the human then manually evaluate the system by using the metrics of user satisfaction degree, dialogue flu",
        "type": "Document"
      },
      {
        "id": "a669a759-4988-4843-98f8-c12f54d6dcaa",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1906.03538",
          "start_idx": 17710,
          "end_idx": 18208
        },
        "page_content": "6 and INLINEFORM7 , respectively. The metrics are averaged across all the perspectives INLINEFORM8 participating in the test set. The goal is to get estimates of the overall performance of the systems. Instead of creating a complex measure that would take all the aspects into account, we approximate the overall performance by multiplying the disjoint measures in INLINEFORM0 , INLINEFORM1 and INLINEFORM2 . While this gives an estimate on the overall quality, it ignores the pipeline structure of",
        "type": "Document"
      },
      {
        "id": "72b50da8-6c06-4809-91ff-95e6cb664a0d",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1909.09484",
          "start_idx": 15874,
          "end_idx": 16378
        },
        "page_content": " of the dialogue state tracker, it aims to show the impact of the dialogue state tracker on the dialogue policy maker; (2) the accuracy of dialogue policy maker, it aims to explain the performance of different methods of constructing dialogue policy; (3) the quality of the final response, it aims to explain the impact of the dialogue policy on the final dialogue response. The evaluation metrics are listed as follows: BPRA: Belief Per-Response Accuracy (BPRA) tests the ability to generate the correct",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what results do they achieve?",
    "llm_answer": "Classification produces better results than regression, but the quality depends on data annotation.  Precision and F1 correlate better with human evaluation than Recall.  Reinforcement learning gives promising results, especially in BioASQ human evaluations.\n",
    "context": [
      {
        "id": "1f140aea-67fe-42b4-983e-2e4e083c362a",
        "metadata": {
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "d63c1e3c-bd64-40b5-a740-854f75384d1b",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1803.02839",
          "start_idx": 1481,
          "end_idx": 1849
        },
        "page_content": " , BIBREF7 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF10 , BIBREF11 , BIBREF8 , BIBREF9 . Often, the improvement simply mitigates the exploding/vanishing gradient problem BIBREF18 , BIBREF19 , but, for many tasks, the improvement is more capable of generalizing the network's training for that task. Understanding better how and why certain networks excel at certain NLP",
        "type": "Document"
      },
      {
        "id": "fcfada0a-4a28-4ceb-b822-a1bb0d8ac606",
        "metadata": {
          "chunk_id": 118,
          "document_id": "1803.02839",
          "start_idx": 45558,
          "end_idx": 45772
        },
        "page_content": " mathematical consistency of the experiments, and provided useful insights into the results. Mark prompted the investigation into potential emergent monoid structures since they appear frequently in state machines.",
        "type": "Document"
      },
      {
        "id": "1a6adc72-1767-4e4a-be77-138554d2b622",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1909.00542",
          "start_idx": 734,
          "end_idx": 1323
        },
        "page_content": " the training procedure was based on a regression set up, this year we experiment with various classification set ups. The main contributions of this paper are: We compare classification and regression approaches and show that classification produces better results than regression but the quality of the results depends on the approach followed to annotate the data labels. We conduct correlation analysis between various ROUGE evaluation metrics and the human evaluations conducted at BioASQ and show that Precision and F1 correlate better than Recall. Section SECREF2 briefly introduces",
        "type": "Document"
      },
      {
        "id": "bd9e9b93-3c0e-4e65-b722-95063c9d5e85",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1909.00542",
          "start_idx": 16697,
          "end_idx": 17106
        },
        "page_content": " for the reward function used by a reinforcement learning system. Reinforcement learning gives promising results, especially in human evaluations made on the runs submitted to BioASQ 6b. This year we introduced very small changes to the runs using reinforcement learning, and will aim to explore more complex reinforcement learning strategies and more complex neural models in the policy and value estimators.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what previous systems were compared to?",
    "llm_answer": "Four baseline systems, including two simple ones and two neural network models.  Also, participant systems in a competition.\n",
    "context": [
      {
        "id": "d5fefd6e-3db2-43fe-a779-06d1fa6e6d35",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1707.03764",
          "start_idx": 5731,
          "end_idx": 6168
        },
        "page_content": " that distinguishing words include both time-specific ones, like \u201cgilmore\u201d and \u201cimacelebrityau\u201d, and general words from everyday life, which are less likely to be subject to time-specific trends, like \u201cplayer\u201d, and \u201cchocolate\u201d. This section is meant to highlight all of the potential contributions to the systems which turned out to be detrimental to performance, when compared to the simpler system that we have described in Section SEC",
        "type": "Document"
      },
      {
        "id": "7bb4c95c-4811-4e15-ab8e-deca3c53de73",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1709.08299",
          "start_idx": 9435,
          "end_idx": 10003
        },
        "page_content": " questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models. The",
        "type": "Document"
      },
      {
        "id": "81ea8b55-e4f9-4f1a-844e-e7e1495fe5a5",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1701.03214",
          "start_idx": 8774,
          "end_idx": 9206
        },
        "page_content": " tables, the numbers in bold indicate the best system and all systems that were not significantly different from the best system. The significance tests were performed using the bootstrap resampling method BIBREF22 at $p < 0.05$ . We can see that without domain adaptation, the SMT systems perform significantly better than the NMT system on the resource poor domains, i.e., IWSLT-CE and WIKI-CJ; while on the resource rich domains,",
        "type": "Document"
      },
      {
        "id": "8005ecd6-6854-40ee-8c5b-c296aeb8fee3",
        "metadata": {
          "chunk_id": 141,
          "document_id": "1701.02877",
          "start_idx": 50827,
          "end_idx": 51373
        },
        "page_content": " The other genre that was particularly difficult for the systems was ACE Usenet. This is a form of user-generated content, not intended for publication but rather discussion among communities. In this sense, it is social media, and so it is not surprising that system performance on ACE UN resembles performance on social media more than other genres. Crucially, the computationally-cheap memorisation method actually acts as a reasonable predictor of the performance of other methods. This suggests that high entity diversity predicts difficulty",
        "type": "Document"
      },
      {
        "id": "abc51b19-3db2-4cbf-ac46-ff97aa2fa1d1",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1603.07044",
          "start_idx": 14997,
          "end_idx": 15515
        },
        "page_content": " task A already had a substantial amount of training data. Table 4 gives the final comparison between different models (we only list the MAP score because it is the official score for the challenge). Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Are this models usually semi/supervised or unsupervised?",
    "llm_answer": "Supervised.\n",
    "context": [
      {
        "id": "ed385b18-6171-42af-9672-281f1df69e11",
        "metadata": {
          "chunk_id": 53,
          "document_id": "1710.01492",
          "start_idx": 19472,
          "end_idx": 19941
        },
        "page_content": " particular, the best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks BIBREF53 , BIBREF54 . Conversely, kernel machines seem to be less frequently used than in the past, and the use of learning methods other than the ones mentioned above is at this point scarce. All these models are examples of supervised learning as they need labeled training data. Semi-supervised learning. We should note two things about the use of deep neural",
        "type": "Document"
      },
      {
        "id": "4ed2dab4-0335-4b56-9c22-9fffd20771bb",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1905.11901",
          "start_idx": 11183,
          "end_idx": 11687
        },
        "page_content": " data, our results are also relevant for work on using auxiliary data to improve low-resource MT. Supervised systems serve as an important baseline to judge the effectiveness of semisupervised or unsupervised approaches, and the quality of supervised systems trained on little data can directly impact semi-supervised workflows, for instance for the back-translation of monolingual data. Rico Sennrich has received funding from the Swiss National Science Foundation in the project CoNTra (grant number 10",
        "type": "Document"
      },
      {
        "id": "f8895a8e-a536-4d76-92d4-69dfbf588a3e",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1808.00265",
          "start_idx": 2296,
          "end_idx": 2873
        },
        "page_content": " have been treated as output variables that receive direct supervision from human annotations. Unfortunately, both of these approaches have disadvantages. First, unsupervised training of attention tends to lead to models that cannot ground their decision in the image in a human interpretable manner. Second, supervised training of attention is difficult and expensive: human annotators may consider different regions to be relevant for the question at hand, which entails ambiguity and increased annotation cost. Our goal is to leverage the best of both worlds by providing VQ",
        "type": "Document"
      },
      {
        "id": "2fa48409-f9ab-4c2c-a2df-c320d2690f61",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1809.04960",
          "start_idx": 7828,
          "end_idx": 8288
        },
        "page_content": " semi-supervised training is used to combine the advantage of both supervised and unsupervised learning. Given an article, the retrieval-based method aims to retrieve a comment from a large pool of candidate comments. The article consists of a title INLINEFORM0 and a body INLINEFORM1 . The comment pool is formed from a large scale of candidate comments INLINEFORM2 , where INLINEFORM3 is the number of the unique comments in the pool. In this work, we have 4",
        "type": "Document"
      },
      {
        "id": "15451d71-c31c-4b42-9406-c08ea2c9c494",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1904.05862",
          "start_idx": 841,
          "end_idx": 1272
        },
        "page_content": " . Unsupervised pre-training for computer vision has also shown promise BIBREF6 . In natural language processing (NLP), unsupervised pre-training of language models BIBREF7 , BIBREF8 , BIBREF9 improved many tasks such as text classification, phrase structure parsing and machine translation BIBREF10 , BIBREF11 . In speech processing, pre-training has focused on emotion recogniton BIBREF12 , speaker identification BIBREF13 , phon",
        "type": "Document"
      }
    ]
  },
  {
    "query": "When they say \"comparable performance\", how much of a performance drop do these new embeddings result in?",
    "llm_answer": "The answer is not found in the context.  While the text discusses performance improvements and comparisons, it does not quantify the *drop* in performance relative to a baseline or other models in terms of a numerical value.\n",
    "context": [
      {
        "id": "7ec87c37-7c39-4d71-b8a8-9183d587c8c1",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1603.00968",
          "start_idx": 13234,
          "end_idx": 13676
        },
        "page_content": " three embeddings achieves 4% improvement (in terms of AUC) compared to the baseline model. On SST-1 and SST-2, our model performs slightly worse than BIBREF11 . However, we again note that their performance is achieved using a much more complex model which involves pre-training and mutual-learning steps. This model takes days to train, whereas our model requires on the order of an hour. We note that the method proposed by Astudillo et al",
        "type": "Document"
      },
      {
        "id": "c01838f7-fe47-43f3-af9e-5b4e5b3a1796",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1603.04513",
          "start_idx": 7007,
          "end_idx": 7518
        },
        "page_content": "through data and user query data, showing that combining them gets stronger results than using individual word embeddings in web search ranking and word similarity task. However, these two papers either learned word representations on the same corpus BIBREF13 or enhanced the embedding quality by extending training corpora, not learning algorithms BIBREF17 . In our work, there is no limit to the type of embedding versions we can use and they leverage not only the diversity of corpora, but also the different",
        "type": "Document"
      },
      {
        "id": "4a70824c-6b13-4557-bb11-1989da09e907",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1611.02550",
          "start_idx": 22235,
          "end_idx": 22732
        },
        "page_content": "REF13 for all dimensionalities INLINEFORM0 .  We might expect the learned embeddings to be more accurate for words that are seen in training than for ones that are not. Fig. FIGREF11 measures this effect by showing performance as a function of the number of occurrences of the dev words in the training set. Indeed, both model types are much more successful for in-vocabulary words, and their performance improves the higher the training frequency of the words. However, performance increases more",
        "type": "Document"
      },
      {
        "id": "017c4fb8-f7e2-4afd-a6fd-6b1b3afe2c1a",
        "metadata": {
          "chunk_id": 85,
          "document_id": "1911.12579",
          "start_idx": 30862,
          "end_idx": 31340
        },
        "page_content": " better results for training all the embedding models. Dimensions ($D$): We evaluate and compare the quality of $100-D$, $200-D$, and $300-D$ using WordSim353 on different $ws$, and the optimal $300-D$ are evaluated with cosine similarity matrix for querying nearest neighboring words and calculating the similarity between word pairs. The embedding dimensions have little affect on the quality of the intrinsic evaluation process. However, the selection of embedding dimensions",
        "type": "Document"
      },
      {
        "id": "dcaff502-ec4b-426f-89a3-4b6ec30bfb78",
        "metadata": {
          "chunk_id": 54,
          "document_id": "1808.09920",
          "start_idx": 17788,
          "end_idx": 18256
        },
        "page_content": ")$ from each model. To help determine the sources of improvements, we perform an ablation study using the publicly available validation set (see Table 3 ). We perform two groups of ablation, one on the embedding layer, to study the effect of ELMo, and one on the edges, to study how different relations affect the overall model performance. We argue that ELMo is crucial, since we do not rely on any other context encoder. However, it is interesting to explore how our",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What types of word representations are they evaluating?",
    "llm_answer": "GloVe word embedding, character-CNN embedding, Part-Of-Speech (POS) embedding, Named-Entity-Recognition (NER) embedding, and exact match feature.\n",
    "context": [
      {
        "id": "880e227f-bac3-4a7c-b439-e4b9c5e8a2bf",
        "metadata": {
          "chunk_id": 62,
          "document_id": "1804.00079",
          "start_idx": 23747,
          "end_idx": 24271
        },
        "page_content": " tasks that are typically used to evaluate sentence representations. We provide a condensed summary and refer readers to their work for a more thorough description. We evaluate on text classification benchmarks - sentiment classification on movie reviews (MR), product reviews (CR) and Stanford sentiment (SST), question type classification (TREC), subjectivity/objectivity classification (SUBJ) and opinion polarity (MPQA). Representations are used to train a logistic regression classifier with 10-fold cross validation to",
        "type": "Document"
      },
      {
        "id": "7437b32e-92b3-411e-8093-242a2dd05f1b",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1604.05781",
          "start_idx": 13542,
          "end_idx": 13981
        },
        "page_content": " for time of year (see Methods). We applied Parts-of-Speech (POS) and Named Entity (NE) taggers to all these texts. Some post-processed and tagged example documents, both causal and control, are shown in Fig. 1 A. We also applied sentiment analysis methods to these documents (Methods) and we have highlighted very positive and very negative words throughout Fig. 1 . In Fig. 1 B we present odds ratios for how frequently unigrams (words),",
        "type": "Document"
      },
      {
        "id": "c3174ecf-1054-4d75-82dc-6c488c7b1e87",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1804.00079",
          "start_idx": 13363,
          "end_idx": 13834
        },
        "page_content": " addition, we also evaluate the quality of our learned individual word representations using standard benchmarks BIBREF36 , BIBREF37 . The choice of transfer tasks and evaluation framework are borrowed largely from BIBREF9 . We provide a condensed summary of the tasks in section SECREF10 in the Appendix but refer readers to their paper for a more detailed description. https://github.com/kudkudak/word-embeddings-benchmarks/wiki Table 2 presents the results of training",
        "type": "Document"
      },
      {
        "id": "9d96b20c-418b-4aef-a7d5-a0a95953dd6e",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1910.12618",
          "start_idx": 15726,
          "end_idx": 16229
        },
        "page_content": "F has a natural interpretation. Furthermore the RF and LASSO methods give the possibility to interpret marginal effects and analyze the importance of features, and thus to find the words which affect the time series the most. As for the word embedding, recurrent or convolutional neural networks (respectively RNN and CNN) were used with them. MLPs are not used, for they would require to concatenate all the vector representations of a sentence together beforehand and result in a network with too many",
        "type": "Document"
      },
      {
        "id": "5c1b46fa-6247-41b1-8176-b4426c503ac9",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1909.11833",
          "start_idx": 6253,
          "end_idx": 6534
        },
        "page_content": ")$. For each word $w_i$, we use GloVe word embedding $e_i$, character-CNN embedding $c_i$, Part-Of-Speech (POS) embedding $\\operatorname{POS}_i$, Named-Entity-Recognition (NER) embedding $\\operatorname{NER}_i$ and exact match feature $\\operatorname{EM}_i$. The POS and NER tags are",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What type of recurrent layers does the model use?",
    "llm_answer": "Deep RNN, LSTM, and quasi-recurrent layers.\n",
    "context": [
      {
        "id": "d289dfe0-f9fb-41cf-b558-cbd5f91b3857",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1710.01507",
          "start_idx": 7820,
          "end_idx": 8342
        },
        "page_content": ") and extract CNN features. We use the output of the fully-connected layer (FC7), which has 4096 dimensions, as feature representations for our architecture. We now go into detail about the components of the model, individual and combined, and how the parameters are learned. Recurrent Neural Network (RNN) is a class of artificial neural networks which utilizes sequential information and maintains history through its intermediate layers. A standard RNN has an internal state whose output at every time-step which can be",
        "type": "Document"
      },
      {
        "id": "7188814a-9cc0-45ad-afbe-3ffcaa35234f",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1706.07179",
          "start_idx": 2809,
          "end_idx": 3302
        },
        "page_content": " in this section. Figure 1 provides a high-level view of the model. The model is sequential in nature, consisting of the following steps: read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer. We model the dynamic memory in a fashion similar to Recurrent Entity Networks BIBREF17 and then equip it with an additional relational memory. There are three main components to the model: 1) input encoder 2) dynamic memory, and 3)",
        "type": "Document"
      },
      {
        "id": "4d96134c-cb91-41f9-987c-01edc0ace398",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1611.02550",
          "start_idx": 5793,
          "end_idx": 6236
        },
        "page_content": " FIGREF1 . The model consists of a deep RNN with some number INLINEFORM3 of stacked layers, whose final hidden state vector is passed as input to a set of INLINEFORM4 of fully connected layers; the output of the final fully connected layer is the embedding INLINEFORM5 . The RNN hidden state at each time frame can be viewed as a representation of the input seen thus far, and its value in the last time frame INLINEFORM0 could itself serve as",
        "type": "Document"
      },
      {
        "id": "5fc33f61-0564-4bbb-811f-82dc0bda9fcc",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1811.00942",
          "start_idx": 4545,
          "end_idx": 4922
        },
        "page_content": " introduced. BIBREF4 name their three-layer LSTM model trained with such tricks, \u201cAWD-LSTM.\u201d Quasi-Recurrent Neural Networks. Quasi-recurrent neural networks (QRNNs; BIBREF10 ) achieve current state of the art in word-level language modeling BIBREF11 . A quasi-recurrent layer comprises two separate parts: a convolution layer with three weights, and a recurrent pooling layer.",
        "type": "Document"
      },
      {
        "id": "26fdc38f-842b-4ef6-9e26-d6482bc535be",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1910.04269",
          "start_idx": 12971,
          "end_idx": 13505
        },
        "page_content": " its hyperparameter. We took some specific design choices for this model, which are as follows: We added residual connections with each convolutional layer. Residual connections in a way makes the model selective of the contributing layers, determines the optimal number of layers required for training and solves the problem of vanishing gradients. Residual connections or skip connections skip training of those layers that do not contribute much in the overall outcome of model. We added spatial attention BIBREF27 networks to help",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is a word confusion network?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "0a3611e5-3036-4d25-a669-3517dc7d6a94",
        "metadata": {
          "chunk_id": 1,
          "document_id": "1905.10810",
          "start_idx": 570,
          "end_idx": 1103
        },
        "page_content": " of isolated correction of non-word errors. They are traditionally separated in error correction literature BIBREF0 . Non-word errors are here incorrect word forms that not only differ from what was intended, but also do not constitute another, existing word themselves. Much of the initial research on error correction focused on this simple task, tackled without means of taking the context of the nearest words into account. It is true that, especially in the case of neural networks, it is often possible and desirable to combine",
        "type": "Document"
      },
      {
        "id": "1b9bb10c-a5c7-4e17-a724-f9d3e3ba65af",
        "metadata": {
          "chunk_id": 4,
          "document_id": "2003.06279",
          "start_idx": 1563,
          "end_idx": 2079
        },
        "page_content": "-occurrence network. After optional textual pre-processing steps, in a co-occurrence network each different word becomes a node and edges are established via co-occurrence in a desired window. A common strategy connects only adjacent words in the so called word adjacency networks. While the co-occurrence representation yields good results in classification scenarios, some important features are not considered in the model. For example, long-range syntactical links, though less frequent than adjacent syntactical",
        "type": "Document"
      },
      {
        "id": "ba2fc0f5-517d-4652-95be-2abd95a7c76f",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1907.11499",
          "start_idx": 8020,
          "end_idx": 8555
        },
        "page_content": " recurrent neural networks. It eliminates recurrence in favor of applying a self-attention mechanism which directly models relationships between all words in a sentence, regardless of their position. DetNet adopts three detectors corresponding to words, sentences, and documents: INLINEFORM0   WordDet first produces word domain scores using both lexical semantic information INLINEFORM0 and prior (domain) knowledge INLINEFORM1 ; SentDet yields domain scores for sentences while integrating downstream instance signals INLINEFORM2 and",
        "type": "Document"
      },
      {
        "id": "98d74eb6-5ed7-483c-a8be-10a29bf9f1ec",
        "metadata": {
          "chunk_id": 71,
          "document_id": "1705.07368",
          "start_idx": 26965,
          "end_idx": 27424
        },
        "page_content": " of underlying clusters is learned from data via Bayesian inference, have also been proposed BIBREF44 . In this work, dictionary words are assigned a mixed membership distribution over a set of shared latent vector space embeddings. Each instantiation of a dictionary word (an \u201cinput\u201d word) is assigned to one of the shared embeddings based on its dictionary word's membership vector. The words in its context (\u201coutput\u201d words) are assumed to be drawn based on",
        "type": "Document"
      },
      {
        "id": "14d6b31f-9da3-4604-8ac7-b19f1bccefdb",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1911.12569",
          "start_idx": 9725,
          "end_idx": 10201
        },
        "page_content": " whether they tend to occur in a similar context. It provides a word expansion list for words based on their contextual similarity. We use the top-4 words for each word as their candidate terms. We only use the top-4 words for each word as we observed that the expansion list with more words started to contain the antonyms of the current word which empirically reduced the system performance. Word embeddings of these four candidate terms and the hidden state vector $h_t$ of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What evaluation metrics were used in the experiment?",
    "llm_answer": "micro-F1, EM-score, F$_1$-score, relevance, attraction, fluency, style strength\n",
    "context": [
      {
        "id": "447cbf78-0bad-4440-920c-4bdb449fa2bb",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1909.07734",
          "start_idx": 6722,
          "end_idx": 7207
        },
        "page_content": ", sadness, anger, and neutral. Evaluation of submissions was done using only utterances with these four labels. Utterances with labels other than the above four (i.e., surprise, disgust, fear or non-neutral) were discarded and not used in the evaluation. Scripts for verifying and evaluating the submissions were made available online. We used micro-F1 as the comparison metric. A total of eleven teams submitted their evaluations, and are presented in the online leaderboard. Seven of",
        "type": "Document"
      },
      {
        "id": "29e16318-4e30-4cb6-a2f8-faf934e75bc7",
        "metadata": {
          "chunk_id": 93,
          "document_id": "1804.03396",
          "start_idx": 29645,
          "end_idx": 30157
        },
        "page_content": " $= 93.53$ and the IE measurements can be found in Figure 3 . Most of the error cases are the fourth case which is acceptable by human annotators. Note that our framework takes the whole document as the input while the baseline systems take the individual sentence as the input, which means the experiment setting is much more difficult for our framework. Finally, we perform a human evaluation on our QA4IE benchmark to verify the reliability of former experiments. The evaluation metrics are as follows: Triple",
        "type": "Document"
      },
      {
        "id": "e929c104-c4ce-40d5-aca7-762538286335",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1908.06606",
          "start_idx": 15988,
          "end_idx": 16383
        },
        "page_content": ") and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage",
        "type": "Document"
      },
      {
        "id": "727bd5dd-0dd4-469a-85d1-e9618fc62c3a",
        "metadata": {
          "chunk_id": 55,
          "document_id": "2004.01980",
          "start_idx": 18999,
          "end_idx": 19503
        },
        "page_content": " style corpus, and the sampling strategy follows the uniform distribution with the probability being equal to $\\lambda $. The human evaluation is to have a comprehensive measurement of the performances. We conduct experiments on four criteria, relevance, attraction, fluency, and style strength. We summarize the human evaluation results on the first three criteria in Table TABREF51, and the last criteria in Table TABREF57. Note that through automatic evaluation, the baselines NST, Fine-tuned, and Gig",
        "type": "Document"
      },
      {
        "id": "a669a759-4988-4843-98f8-c12f54d6dcaa",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1906.03538",
          "start_idx": 17710,
          "end_idx": 18208
        },
        "page_content": "6 and INLINEFORM7 , respectively. The metrics are averaged across all the perspectives INLINEFORM8 participating in the test set. The goal is to get estimates of the overall performance of the systems. Instead of creating a complex measure that would take all the aspects into account, we approximate the overall performance by multiplying the disjoint measures in INLINEFORM0 , INLINEFORM1 and INLINEFORM2 . While this gives an estimate on the overall quality, it ignores the pipeline structure of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What kind of instructional videos are in the dataset?",
    "llm_answer": "Screencast tutorial videos with spoken narratives for a photo-editing software.\n",
    "context": [
      {
        "id": "985dd687-bab3-4309-816f-4f31f8da00e6",
        "metadata": {
          "chunk_id": 53,
          "document_id": "1912.01046",
          "start_idx": 20739,
          "end_idx": 21318
        },
        "page_content": ", a new type of dataset used to find answer spans in tutorial videos. Our data collection method for question-answer pairs on instructional video can be further adopted to other domains where the answers involve multiple steps and are part of an overall goal, such as cooking or educational videos. We have shown that current baseline models for finding the answer spans are not sufficient for achieving high accuracy and hope that by releasing this new dataset and task, more appropriate question answering models can be developed for question answering on instructional videos.",
        "type": "Document"
      },
      {
        "id": "b1960429-2aba-4ae7-a91c-0737476b2172",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1912.01046",
          "start_idx": 5574,
          "end_idx": 6121
        },
        "page_content": " collection process . Our dataset consists of 76 tutorial videos pertaining to an image editing software. All of the videos include spoken instructions which are transcribed and manually segmented into multiple segments. Specifically, we asked the annotators to manually divide each video into multiple segments such that each of the segments can serve as an answer to any question. For example, Fig. FIGREF1 shows example segments marked in red (each which are a complete unit as an answer span). Each sentence is associated with the starting and",
        "type": "Document"
      },
      {
        "id": "fcee1bd0-617b-4376-ad49-86ff9fae716e",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1912.01046",
          "start_idx": 5276,
          "end_idx": 5797
        },
        "page_content": " augment with a visual cue detector on food entities. Their task focuses on procedure-based cooking videos, and contrary to our task is primarily a text alignment task. In our task we aim to answer questions-using the transcripts-on instructional-style videos, in which the answer can involve steps not mentioned in the question. In this section, we introduce the TutorialVQA dataset and describe the data collection process . Our dataset consists of 76 tutorial videos pertaining to an image editing software. All of the",
        "type": "Document"
      },
      {
        "id": "5eed01cc-cb49-4423-945d-c421c018ff99",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1912.01046",
          "start_idx": 2345,
          "end_idx": 2865
        },
        "page_content": ", as illustrated in Figure FIGREF1. The remainder of this paper is structured as follows. Section SECREF3 introduces TutorialVQA dataset as a case study of our proposed problem. The dataset includes about 6,000 triples, comprised of videos, questions, and answer spans manually collected from screencast tutorial videos with spoken narratives for a photo-editing software. Section SECREF4 presents the baseline models and their experiment details on the sentence-level prediction and video segment retrieval tasks on our",
        "type": "Document"
      },
      {
        "id": "e9c5de5b-1185-4e43-ac74-e5a3c94581a3",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1812.07023",
          "start_idx": 10725,
          "end_idx": 11271
        },
        "page_content": " prototype dataset, the training set of the avsd dataset corresponds to videos taken from the training set of the Charades dataset while the validation and test sets of the avsd dataset correspond to videos taken from the validation set of the Charades dataset. For the official dataset, training, validation and test sets are drawn from the corresponding Charades sets. The Charades dataset also provides additional annotations for the videos such as action, scene, and object annotations, which are considered to be external data sources by the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What baseline algorithms were presented?",
    "llm_answer": "Two simple ones and two neural network models.\n",
    "context": [
      {
        "id": "7bb4c95c-4811-4e15-ab8e-deca3c53de73",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1709.08299",
          "start_idx": 9435,
          "end_idx": 10003
        },
        "page_content": " questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models. The",
        "type": "Document"
      },
      {
        "id": "a4a4c58b-d1f1-46a1-9912-24f7a070ad68",
        "metadata": {
          "chunk_id": 43,
          "document_id": "2001.00137",
          "start_idx": 16781,
          "end_idx": 17197
        },
        "page_content": " the online platforms. Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector",
        "type": "Document"
      },
      {
        "id": "7562b134-7abd-4e6d-a542-e9dd2de17983",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1905.11037",
          "start_idx": 5306,
          "end_idx": 5824
        },
        "page_content": "ered perceptron, convolutional neural networks and long short-term memory networks. We outline the essentials of each of these models, but will treat them as black boxes. In a related line, kaushik2018much discuss the need of providing rigorous baselines that help better understand the improvement coming from future and complex models, and also the need of not demanding architectural novelty when introducing new datasets. Although not done in this work, an alternative (but also natural) way to address the task is",
        "type": "Document"
      },
      {
        "id": "b7568416-2511-4e7d-b42a-905e8dea8f55",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1909.09484",
          "start_idx": 19263,
          "end_idx": 19734
        },
        "page_content": " models and baselines, with a learning rate of 0.001 for supervised training and 0.0001 for reinforcement learning. In reinforcement learning, the decay parameter $\\lambda $ is set to 0.8. The weight decay is set to 0.001. And early stopping is performed on developing set. The experimental results of the proposed model and baselines will be analyzed from the following aspects. BPRA Results: As shown in Table TABREF35, most of the models have similar performance on BP",
        "type": "Document"
      },
      {
        "id": "a6368e04-3287-4ba7-bbe3-740fe1258f61",
        "metadata": {
          "chunk_id": 82,
          "document_id": "1603.04513",
          "start_idx": 26830,
          "end_idx": 27230
        },
        "page_content": "ational aspect of the system. All results in the five blocks are with respect to row 34, \u201cMVCNN (overall)\u201d; e.g., row 19 shows what happens when HLBL is removed from row 34, row 28 shows what happens when mutual learning is removed from row 34 etc. The block \u201cbaselines\u201d (1\u201318) lists some systems representative of previous work on the corresponding datasets, including the state-of-the-art systems (",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does TP-N2F compare to LSTM-based Seq2Seq in terms of training and inference speed?",
    "llm_answer": "Answer not found in the context.\n",
    "context": [
      {
        "id": "e4b23bc7-6984-48c3-a2c8-af65c677c7dd",
        "metadata": {
          "chunk_id": 73,
          "document_id": "1910.02339",
          "start_idx": 21875,
          "end_idx": 22293
        },
        "page_content": " engine) and operation sequence accuracy (where the generated operation sequence must match the ground truth sequence exactly). TP-N2F is compared to a baseline provided by the seq2prog model in BIBREF16, an LSTM-based seq2seq model with attention. Our model outperforms both the original seq2prog, designated SEQ2PROG-orig, and the best reimplemented seq2prog after an extensive hyperparameter search, designated SEQ2",
        "type": "Document"
      },
      {
        "id": "f0e2f4cd-da80-409b-b10d-b20c53223323",
        "metadata": {
          "chunk_id": 78,
          "document_id": "1910.02339",
          "start_idx": 23532,
          "end_idx": 23901
        },
        "page_content": " (M-Acc). AlgoLisp has about 10% noisy data (details in the Appendix), so we report results both on the full test set and the cleaned test set (in which all noisy testing samples are removed). TP-N2F is compared with an LSTM seq2seq with attention model, the Seq2Tree model in BIBREF17, and a seq2seq model with a pre-trained tree decoder from the Tree2Tree autoencoder",
        "type": "Document"
      },
      {
        "id": "60c24a48-7e9a-4105-a38a-ebfb92fa9a66",
        "metadata": {
          "chunk_id": 79,
          "document_id": "1910.02339",
          "start_idx": 23901,
          "end_idx": 24241
        },
        "page_content": "2seq model with a pre-trained tree decoder from the Tree2Tree autoencoder (SAPS) reported in BIBREF18. As shown in Table TABREF18, TP-N2F outperforms all existing models on both the full test set and the cleaned test set. Ablation experiments with TP2LSTM and LSTM2TP show that, for this task, the TP-N2F Decoder is more helpful than TP-N2F",
        "type": "Document"
      },
      {
        "id": "b35f43e4-2a22-47b5-820a-ecccf47084ac",
        "metadata": {
          "chunk_id": 77,
          "document_id": "1803.07771",
          "start_idx": 27491,
          "end_idx": 27849
        },
        "page_content": " two-level network (without lexicon embedding) with character embedding, Tl-LSTM-C, outperforms all the other involved algorithms. On the travel and the mobile corpora, TI-LSTM-W outperforms Bi-LSTM-W. The results in Table 4 indicate that the performances of Tl-LSTM on the mixed training and test data (R+C) are better than those of Bi-LSTM. This comparison",
        "type": "Document"
      },
      {
        "id": "d5a36662-9412-4346-9e05-56c723dbf62d",
        "metadata": {
          "chunk_id": 74,
          "document_id": "1910.02339",
          "start_idx": 22080,
          "end_idx": 22455
        },
        "page_content": " best reimplemented seq2prog after an extensive hyperparameter search, designated SEQ2PROG-best. Table TABREF16 presents the results. To verify the importance of the TP-N2F encoder and decoder, we conducted experiments to replace either the encoder with a standard LSTM (denoted LSTM2TP) or the decoder with a standard attentional LSTM (denoted TP2LSTM). We observe that both",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance proposed model achieved on AlgoList benchmark?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "d7a0b7aa-da14-4112-80c0-cc26c62fe83f",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "38d0d5d3-4be2-4a08-bc2e-e9f401074e5d",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1701.06538",
          "start_idx": 17346,
          "end_idx": 17826
        },
        "page_content": ". Details can be found in Appendix UID77 . Results of these three models form the bottom line of Figure FIGREF32 -right. Table TABREF33 compares the results of these models to the best previously-published result on this dataset . Even the fastest of these models beats the best published result (when controlling for the number of training epochs), despite requiring only 6% of the computation. We trained our models using TensorFlow BIBREF30 on clusters containing 16-32 Tesla K",
        "type": "Document"
      },
      {
        "id": "734c5a98-cf31-4b02-b4fd-50657ce843f1",
        "metadata": {
          "chunk_id": 93,
          "document_id": "1910.02339",
          "start_idx": 29094,
          "end_idx": 29547
        },
        "page_content": " Appendix A.4. AlgoLisp provides an execution script to run the generated program and has three evaluation metrics: accuracy of passing all test cases (Acc), accuracy of passing 50% of test cases (50p-Acc), and accuracy of generating an exactly matched program (M-Acc). AlgoLisp has about 10% noise data (where the execution script fails to pass all test cases on the ground truth program), so we report results both on the full test set and the cleaned",
        "type": "Document"
      },
      {
        "id": "1e2f925a-9a0d-4318-af38-ee9ae9d2a426",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1912.13109",
          "start_idx": 7921,
          "end_idx": 8471
        },
        "page_content": " on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we",
        "type": "Document"
      },
      {
        "id": "b33efcd4-e77c-4412-a68c-1439b71714e1",
        "metadata": {
          "chunk_id": 94,
          "document_id": "1701.06538",
          "start_idx": 34529,
          "end_idx": 34995
        },
        "page_content": "ity after 100 billion training words is 39% lower for the 68-billion-parameter MoE model than for the baseline model. It is notable that the measured computational efficiency of the largest model (0.30 TFLOPS/GPU) is very low compared to the other models. This is likely a result of the fact that, for purposes of comparison to the other models, we did not increase the training batch size proportionally to the number of GPUs. For comparison, we include results for",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance proposed model achieved on MathQA?",
    "llm_answer": "The provided text states the model has a lower test perplexity after 10 epochs compared to other models, but does not give specific performance numbers.\n",
    "context": [
      {
        "id": "b4f3fedf-d2c6-449f-9d7a-c91af200ab34",
        "metadata": {
          "chunk_id": 96,
          "document_id": "1910.02339",
          "start_idx": 29959,
          "end_idx": 30212
        },
        "page_content": " MathQA dataset, we use $n_{\\mathrm {F}}= 150$, $n_{\\mathrm {R}}= 50$, $d_{\\mathrm {F}}= 30$, $d_{\\mathrm {R}}= 20$, $d_{Rel} = 20$, $d_{Arg} = 10$, $d_{Pos} = 5$ and we train the model for 60 epochs with learning rate 0.00115. The reasoning module only",
        "type": "Document"
      },
      {
        "id": "950bacfe-9298-4c06-8732-85c9eb9e4e95",
        "metadata": {
          "chunk_id": 89,
          "document_id": "1910.02339",
          "start_idx": 27544,
          "end_idx": 28014
        },
        "page_content": ". The MathQA dataset consists of about 37k math word problems ((80/12/8)% training/dev/testing problems), each with a corresponding list of multi-choice options and an straight-line operation sequence program to solve the problem. An example from the dataset is presented in the Appendix A.4. In this task, TP-N2F is deployed to generate the operation sequence given the question. The generated operations are executed to generate the solution for the given math problem",
        "type": "Document"
      },
      {
        "id": "7dac3f56-f4b3-4057-b573-9c4758efa5ba",
        "metadata": {
          "chunk_id": 88,
          "document_id": "1701.06538",
          "start_idx": 32330,
          "end_idx": 32790
        },
        "page_content": " INLINEFORM1 respectively, showing that even in the presence of a large MoE, more computation is still useful. Results are reported at the bottom of Table TABREF76 . The larger of the two models has a similar computational budget to the best published model from the literature, and training times are similar. Comparing after 10 epochs, our model has a lower test perplexity by INLINEFORM2 . The models are similar in structure to the 8-million-operations-per",
        "type": "Document"
      },
      {
        "id": "b33efcd4-e77c-4412-a68c-1439b71714e1",
        "metadata": {
          "chunk_id": 94,
          "document_id": "1701.06538",
          "start_idx": 34529,
          "end_idx": 34995
        },
        "page_content": "ity after 100 billion training words is 39% lower for the 68-billion-parameter MoE model than for the baseline model. It is notable that the measured computational efficiency of the largest model (0.30 TFLOPS/GPU) is very low compared to the other models. This is likely a result of the fact that, for purposes of comparison to the other models, we did not increase the training batch size proportionally to the number of GPUs. For comparison, we include results for",
        "type": "Document"
      },
      {
        "id": "bb555808-f337-44f3-bc78-38e51d51355f",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1703.09684",
          "start_idx": 19045,
          "end_idx": 19508
        },
        "page_content": " average them together for the question-type. To compute overall performance, we compute the arithmetic normalized mean per-type (N-MPT) and harmonic N-MPT scores. A large discrepancy between unnormalized and normalized scores suggests an algorithm is not generalizing to rarer answers. While there are alternative formulations (e.g., BIBREF4 , BIBREF19 ), the majority of VQA systems formulate it as a classification problem in which the system is given an image",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What previous methods is the proposed method compared against?",
    "llm_answer": "Coppersmith et al.'s method, K-means, SkipVec, RecNN, and Para2vec.\n",
    "context": [
      {
        "id": "1f140aea-67fe-42b4-983e-2e4e083c362a",
        "metadata": {
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "309bcaa2-ef9d-45b3-9b52-81e934a26747",
        "metadata": {
          "chunk_id": 66,
          "document_id": "2003.07433",
          "start_idx": 25731,
          "end_idx": 26161
        },
        "page_content": "REF28 illustrates the comparisons between LAXARY and Coppersmith et. al. proposed method. Here we can see, the outperformance of our proposed method as well as the importance of $s-score$ estimation. We also illustrates the importance of $\\alpha -score$ and $S-score$ in Fig FIGREF30. Fig FIGREF30 illustrates that if we change the number of training samples (%), LAXARY models outperforms Coppersmith et. al. proposed model under",
        "type": "Document"
      },
      {
        "id": "5c6209f3-cef8-4a17-88a8-a660a1a73e1e",
        "metadata": {
          "chunk_id": 76,
          "document_id": "1701.00185",
          "start_idx": 27376,
          "end_idx": 27736
        },
        "page_content": " and NMI performance of our proposed approaches and four baseline methods, K-means, SkipVec, RecNN and Para2vec based clustering methods. Intuitively, we get a general observation that (1) BoW based approaches, including K-means (TF) and K-means (TF-IDF), and SkipVec based approaches perform not well; (2) RecNN based approaches, both RecNN (Ave.) and RecNN (",
        "type": "Document"
      },
      {
        "id": "d19a44f7-889e-49e1-a1e6-4b17ed54a1e5",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1908.07816",
          "start_idx": 4509,
          "end_idx": 5017
        },
        "page_content": " methodology. We present experimental results and some analysis in Section SECREF4. The paper is concluded in Section SECREF5, followed by some future work we plan to do. Many early open-domain dialog systems are rule-based and often require expert knowledge to develop. More recent work in response generation seeks data-driven solutions, leveraging on machine learning techniques and the availability of data. Ritter et al. BIBREF14 first applied statistical machine translation (SMT) methods to this area.",
        "type": "Document"
      },
      {
        "id": "b495585b-0fa7-47cf-8952-6efb7d7701f9",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1910.11949",
          "start_idx": 2984,
          "end_idx": 3467
        },
        "page_content": "REF8 by Joseph Weizenbaum at MIT. Its implementation consisted in pattern matching and substitution methodology. Recently, data driven approaches have drawn significant attention. Existing work along this line includes retrieval-based methods BIBREF9BIBREF10 and generation-based methodsBIBREF11BIBREF12. In this work we focus on generative models, where sequence-to-sequence algorithm that uses RNNs to encode and decode inputs into responses is a current best practice. Our convers",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the baseline model used?",
    "llm_answer": "Modelo 1\n",
    "context": [
      {
        "id": "4d522ea5-86b3-42df-acaa-37f028f2bec5",
        "metadata": {
          "chunk_id": 138,
          "document_id": "2001.11381",
          "start_idx": 31860,
          "end_idx": 32196
        },
        "page_content": "ados. Dicho lo anterior, el Modelo 1 podr\u00eda ser considerado como nuestro propio baseline. A continuaci\u00f3n presentamos un protocolo de evaluaci\u00f3n manual de los resultados obtenidos. El experimento consisti\u00f3 en la generaci\u00f3n de 15 frases por cada uno de los tres modelos propuestos. Para cada modelo, se consideraron tres queries: $Q=$ {AM",
        "type": "Document"
      },
      {
        "id": "b7568416-2511-4e7d-b42a-905e8dea8f55",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1909.09484",
          "start_idx": 19263,
          "end_idx": 19734
        },
        "page_content": " models and baselines, with a learning rate of 0.001 for supervised training and 0.0001 for reinforcement learning. In reinforcement learning, the decay parameter $\\lambda $ is set to 0.8. The weight decay is set to 0.001. And early stopping is performed on developing set. The experimental results of the proposed model and baselines will be analyzed from the following aspects. BPRA Results: As shown in Table TABREF35, most of the models have similar performance on BP",
        "type": "Document"
      },
      {
        "id": "0d211bed-e5df-4233-8d02-e24d7ee9b65c",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1909.02635",
          "start_idx": 9050,
          "end_idx": 9548
        },
        "page_content": " (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo",
        "type": "Document"
      },
      {
        "id": "7d4754c7-4ea8-4d20-9fb6-3d1e50bf0cf6",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1809.04960",
          "start_idx": 22497,
          "end_idx": 22960
        },
        "page_content": " the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types of comments as described in the above retrieval evaluation setting: Correct, Plausible, Popular, and Random. We rank the candidate comment set with four models (TF-IDF, S2S, IR, and Proposed+IR), and record the types of top-1 comments. Figure FIGREF40 shows the percentage of different types of top-1 comments generated by each model. It shows",
        "type": "Document"
      },
      {
        "id": "f297d78b-0fdd-4eee-ae9e-9e75a2e66130",
        "metadata": {
          "chunk_id": 140,
          "document_id": "2001.02380",
          "start_idx": 51470,
          "end_idx": 51956
        },
        "page_content": " baselines. The left, middle and right panels in Figure FIGREF40 correspond to measurements when all signals are included, only cases contained entirely in the head EDUs shown to the model, and only DMs, respectively. The scenario on the left is rather unreasonable and is included only for completeness: here the model is also penalized for not detecting signals such as lexical chains, part of which is outside the units that the model is being shown. An example of such a case can be",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What domains are present in the data?",
    "llm_answer": "20 domains are listed in Table TABREF10.\n",
    "context": [
      {
        "id": "8348e336-9079-412a-a078-faa11a754824",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1907.11499",
          "start_idx": 6351,
          "end_idx": 6867
        },
        "page_content": " at hand. For example, the definition of the \u201cLifestyle\u201d domain is \u201cthe interests, opinions, behaviors, and behavioral orientations of an individual, group, or culture\u201d. Figure FIGREF5 provides an overview of our Domain Detection Network, which we call DetNet. The model comprises two modules; an encoder learns representations for words and sentences whilst incorporating prior domain information; a detector generates domain scores for words, sentences, and documents by selectively attending to previously encoded",
        "type": "Document"
      },
      {
        "id": "40f64673-65b3-488c-bf5e-698652b3ef8a",
        "metadata": {
          "chunk_id": 29,
          "document_id": "2002.01359",
          "start_idx": 12314,
          "end_idx": 12843
        },
        "page_content": " common approach utilized in other datasets BIBREF0. It has been shown that such datasets suffer from substantial annotation errors BIBREF18. Thirdly, using a simulator greatly simplifies the data collection task and instructions as only paraphrasing is needed to achieve a natural dialogue. This is particularly important for creating a large dataset spanning multiple domains. The 20 domains present across the train, dev and test datasets are listed in Table TABREF10, as are the details regarding which domains are present in",
        "type": "Document"
      },
      {
        "id": "3be6660a-4a1b-45c7-9905-2b27b4e2df1b",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1907.11499",
          "start_idx": 3251,
          "end_idx": 3868
        },
        "page_content": " involvement. Instead of learning from individually labeled segments, our model only requires document-level supervision and optionally prior domain knowledge and learns to introspectively judge the domain of constituent segments. Importantly, we do not require document-level domain annotations either since we obtain these via distant supervision by leveraging information drawn from Wikipedia. Our domain detection framework comprises two neural network modules; an encoder learns representations for words and sentences together with prior domain information if the latter is available (e.g., domain definitions),",
        "type": "Document"
      },
      {
        "id": "319c9757-9bd8-4aed-9974-60604182d684",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1701.03214",
          "start_idx": 5368,
          "end_idx": 5787
        },
        "page_content": " (essentially open) in-domain data. The scientific domain MT was conducted on the Chinese-Japanese paper excerpt corpus (ASPEC-CJ) BIBREF11 , which is one subtask of the workshop on Asian translation (WAT) BIBREF15 . The ASPEC-CJ task uses 672315, 2090, and 2107 sentences for training, development, and testing, respectively. The Wikipedia domain task was conducted on a Chinese-Japanese corpus automatically extracted",
        "type": "Document"
      },
      {
        "id": "e9ff0ea6-7735-4c7f-b973-c089ea86b236",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1701.03214",
          "start_idx": 4004,
          "end_idx": 4439
        },
        "page_content": "-of-domain data is used for the in-domain data; while for the \u201cmulti domain\u201d and \u201cmixed fine tuning\u201d methods, we use a vocabulary obtained from the mixed in-domain and out-of-domain data for all the training stages. We conducted NMT domain adaptation experiments in two different settings as follows: Chinese-to-English translation was the focus of the high quality in-domain corpus setting. We utilized the resource rich patent out-of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many texts/datapoints are in the SemEval, TASS and SENTIPOLC datasets?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "6b7f6ca6-24d5-4144-9cfd-92be2de31f39",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "4a2bbc22-5a05-441f-92ce-90e5ab0a95c8",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1910.00458",
          "start_idx": 17496,
          "end_idx": 17916
        },
        "page_content": "500). For the sentiment analysis category, we used the Stanford Sentiment Treebank (SST-2) dataset from the GLUE benchmark BIBREF22 (around 60k train examples) and the Yelp dataset (around 430k train examples). For the paraphrase category, three paraphrasing datasets are used from the GLUE benchmark: Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), and Quora Question Pairs (",
        "type": "Document"
      },
      {
        "id": "2d3bdfec-4ed5-456d-bd82-023fad798858",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "a7dd4b83-a423-4210-8cf5-a2555b3488d8",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1911.07228",
          "start_idx": 6150,
          "end_idx": 6611
        },
        "page_content": " corpus which are provided by VLSP 2016 - Named Entity Recognition. The dataset contains four different types of label: Location (LOC), Person (PER), Organization (ORG) and Miscellaneous - Name of an entity that do not belong to 3 types above (Table TABREF15). Although the corpus has more information about the POS and chunks, but we do not use them as features in our model. There are two folders with 267 text files of training data and 45 text files of test",
        "type": "Document"
      },
      {
        "id": "6f04ea61-adc5-4abb-9261-abc554ebac1b",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1711.02013",
          "start_idx": 20658,
          "end_idx": 21116
        },
        "page_content": "ings, between recurrent layers, and on recurrent states were (0.7, 0.5, 0.5) respectively. dataset contains 17M training tokens and has a vocabulary size of 44k words. The dataset is partitioned into a training set (first 99M characters) and a development set (last 1M characters) that is used to report performance. As this dataset contains various articles from Wikipedia, the longer term information (such as current topic) plays a bigger role than in the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "In which languages did the approach outperform the reported results?",
    "llm_answer": "Arabic, Portuguese, Russian, and Swedish.\n",
    "context": [
      {
        "id": "10a35031-f90d-415f-aa3f-734db4bbcba6",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1612.05270",
          "start_idx": 1650,
          "end_idx": 2169
        },
        "page_content": " tested in eight different languages: Spanish, English, Italian, Arabic, German, Portuguese, Russian and Swedish. We compare our approach ranking in three international contests: TASS'15, SemEval'15-16 and SENTIPOLC'14, for Spanish, English and Italian respectively; the remaining languages are compared directly with the results reported in the literature. The experimental results locate our approach in good positions for all considered competitions; and excellent results in the other five languages tested. Finally",
        "type": "Document"
      },
      {
        "id": "05489258-3cbf-4bf5-86c1-af65b5a78974",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1910.04269",
          "start_idx": 6552,
          "end_idx": 7071
        },
        "page_content": " for feature extraction. An elaborate analysis of how deep architectures outperform their shallow counterparts is presented in this reseacrch. The difficulties in classifying perceptually similar languages like German and English were also put forward in this work. It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize",
        "type": "Document"
      },
      {
        "id": "d4a75364-55c2-4bc7-bc93-3b9f77c61931",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1806.09103",
          "start_idx": 16614,
          "end_idx": 17106
        },
        "page_content": " quite different, the results indicate that our model also works effectively for out-of-domain learning. To verify if our method can only work for Chinese, we also evaluate the effectiveness of the proposed method on benchmark English dataset. We use CBT dataset as our testbed to evaluate the performance. For a fair comparison, we simply set the same parameters as before. Table TABREF22 shows the results. We observe that our model outperforms most of the previously public works, with 2.4",
        "type": "Document"
      },
      {
        "id": "040c9fdb-ed2a-4139-b3f6-54120128868e",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1612.05270",
          "start_idx": 1304,
          "end_idx": 1873
        },
        "page_content": " given languages. Furthermore, our second contribution is to create a well-performing baseline to compare new sentiment classifiers in a broad range of languages or to bootstrap new sentiment analysis systems. Our approach is based on selecting the best text-transforming techniques that optimize some performance measures where the chosen techniques are robust to typical writing errors. In this context, we propose a robust multilingual sentiment analysis method, tested in eight different languages: Spanish, English, Italian, Arabic, German, Portuguese, Russian and",
        "type": "Document"
      },
      {
        "id": "4f366b00-7e35-42cd-8d5d-0a1a3c14dcfa",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1906.06448",
          "start_idx": 21919,
          "end_idx": 22444
        },
        "page_content": " also indicates the difficulty of problems from linguistics publications. Regarding non-monotone problems collected via crowdsourcing, there are very few non-monotone problems, so accuracy is 100%. Adding non-monotone problems to our test set is left for future work. Table 9 shows the evaluation results by type of linguistic phenomenon. While accuracy on problems involving NPIs and conditionals was improved on both upward and downward inferences, accuracy on problems involving conjunction and disjunction was improved on",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which is the baseline model?",
    "llm_answer": "Model 1\n",
    "context": [
      {
        "id": "4d522ea5-86b3-42df-acaa-37f028f2bec5",
        "metadata": {
          "chunk_id": 138,
          "document_id": "2001.11381",
          "start_idx": 31860,
          "end_idx": 32196
        },
        "page_content": "ados. Dicho lo anterior, el Modelo 1 podr\u00eda ser considerado como nuestro propio baseline. A continuaci\u00f3n presentamos un protocolo de evaluaci\u00f3n manual de los resultados obtenidos. El experimento consisti\u00f3 en la generaci\u00f3n de 15 frases por cada uno de los tres modelos propuestos. Para cada modelo, se consideraron tres queries: $Q=$ {AM",
        "type": "Document"
      },
      {
        "id": "b7568416-2511-4e7d-b42a-905e8dea8f55",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1909.09484",
          "start_idx": 19263,
          "end_idx": 19734
        },
        "page_content": " models and baselines, with a learning rate of 0.001 for supervised training and 0.0001 for reinforcement learning. In reinforcement learning, the decay parameter $\\lambda $ is set to 0.8. The weight decay is set to 0.001. And early stopping is performed on developing set. The experimental results of the proposed model and baselines will be analyzed from the following aspects. BPRA Results: As shown in Table TABREF35, most of the models have similar performance on BP",
        "type": "Document"
      },
      {
        "id": "7d4754c7-4ea8-4d20-9fb6-3d1e50bf0cf6",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1809.04960",
          "start_idx": 22497,
          "end_idx": 22960
        },
        "page_content": " the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types of comments as described in the above retrieval evaluation setting: Correct, Plausible, Popular, and Random. We rank the candidate comment set with four models (TF-IDF, S2S, IR, and Proposed+IR), and record the types of top-1 comments. Figure FIGREF40 shows the percentage of different types of top-1 comments generated by each model. It shows",
        "type": "Document"
      },
      {
        "id": "0d211bed-e5df-4233-8d02-e24d7ee9b65c",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1909.02635",
          "start_idx": 9050,
          "end_idx": 9548
        },
        "page_content": " (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo",
        "type": "Document"
      },
      {
        "id": "60414dd4-91fe-4b57-9229-8a044a37d837",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1910.07481",
          "start_idx": 13842,
          "end_idx": 14345
        },
        "page_content": "\". The first example in Table TABREF10 perfectly illustrate this ambiguity: the context clearly indicates that \"lui\" in the source sentence refers to \"ma fille\", which is located three sentences above, and should be translated into \"her\". In this case, the Baseline model predict the personal pronoun \"him\" while the Document model correctly predicts \"her\". It seems that the Baseline model does not benefit from any valuable information in the source sentence. Some might argue that the source sentence",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the main contribution of the paper? ",
    "llm_answer": "A novel approach applying NLP methods to a new dataset of UN speeches to uncover main development topics, structural factors influencing discussion, and state preferences regarding the international development agenda.  The paper demonstrates how text analytic approaches can help understand global governance.\n",
    "context": [
      {
        "id": "86d3c04c-e40e-4987-99b0-a1e9c68d7fea",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1904.08386",
          "start_idx": 9817,
          "end_idx": 10140
        },
        "page_content": " anonymous reviewers for their insightful comments. Additionally, we thank Nader Akoury, Garrett Bernstein, Chenghao Lv, Ari Kobren, Kalpesh Krishna, Saumya Lal, Tu Vu, Zhichao Yang, Mengxue Zhang and the UMass NLP group for suggestions that improved the paper's clarity, coverage of related work, and analysis experiments.",
        "type": "Document"
      },
      {
        "id": "1013d002-0cd6-4f7c-a881-2f21815815c1",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1708.05873",
          "start_idx": 19748,
          "end_idx": 20303
        },
        "page_content": " a novel approach that applies NLP methods to a new dataset of speeches in the UN General Debate, this paper has uncovered the main development topics discussed by governments in the UN, and the structural factors that influence the degree to which governments discuss international development. In doing so, the paper has shed some light on state preferences regarding the international development agenda in the UN. The paper more broadly demonstrates how text analytic approaches can help us to better understand different aspects of global governance.",
        "type": "Document"
      },
      {
        "id": "98c4f7a3-00be-42b7-8b98-e4740bf9a35a",
        "metadata": {
          "chunk_id": 169,
          "document_id": "1910.09399",
          "start_idx": 65581,
          "end_idx": 66035
        },
        "page_content": " to contribute directly or indirectly to the expansion of the vast computer vision AI field. Looking into the future, an excellent extension from the works surveyed in this paper would be to give more independence to the several learning methods (e.g. less human intervention) involved in the studies as well as increasing the size of the output images. The authors declare that there is no conflict of interest regarding the publication of this article.",
        "type": "Document"
      },
      {
        "id": "70e1ea66-1171-4546-a976-9b33995d48d1",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1908.07816",
          "start_idx": 4017,
          "end_idx": 4509
        },
        "page_content": " relate. It is the first time such an approach is designed with consideration for the human judges. Our main goal is to increase the objectivity of the results and reduce judges' mistakes due to out-of-context dialogs they have to evaluate. The rest of the paper unfolds as follows. Section SECREF2 discusses some related work. In Section SECREF3, we give detailed description of the methodology. We present experimental results and some analysis in Section SECREF4. The paper is concluded in",
        "type": "Document"
      },
      {
        "id": "4eeef17b-6ce3-45ae-9c9f-b3b15657e286",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1909.08167",
          "start_idx": 4273,
          "end_idx": 4685
        },
        "page_content": "3 and the adversarial-learning-based DANN model BIBREF2 as an example, respectively. In summary, the contributions of this paper include: ($\\mathbf {i}$) We theoretically and empirically analyse the problem of DIRL for domain adaptation when the marginal distribution $\\rm {P}(\\rm {Y})$ shifts across domains. ($\\mathbf {ii}$) We proposed a novel method to address the problem and show how to incorporate it with",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they get the formal languages?",
    "llm_answer": "The context states the formal languages are $a^n b^n$, $a^n b^n c^n$, and $a^n b^n c^n d^n$.\n",
    "context": [
      {
        "id": "10b18f90-1375-4a41-9719-9a76bdd2287c",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1910.05456",
          "start_idx": 1974,
          "end_idx": 2423
        },
        "page_content": " on the L1, i.e., the \"native language\", in neural network models. To this goal, we select a diverse set of eight source languages from different language families \u2013 Basque, French, German, Hungarian, Italian, Navajo, Turkish, and Quechua \u2013 and three target languages \u2013 English, Spanish and Zulu. We pretrain a neural sequence-to-sequence architecture on each of the source languages and then fine-tune the resulting models on small datasets in each",
        "type": "Document"
      },
      {
        "id": "b63f6629-8862-4f50-bd94-e80c3317d585",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1809.08510",
          "start_idx": 17158,
          "end_idx": 17644
        },
        "page_content": ", Russian, Arabic, Chinese, German, Spanish, French. We maintain the same procedure as described above. The hidden size of the language model was increased to 1024 with 16K BPE tokens being used. The first model was trained on English Russian, second was trained on English Russian Arabic and so on. For arabic we still trained from left to right even though naturally the language is read from right to left. We report the results in Figure 5 . As the number of languages increases the",
        "type": "Document"
      },
      {
        "id": "e52db55f-bfa1-4f72-9823-066e0ef9697f",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1911.03894",
          "start_idx": 1502,
          "end_idx": 1983
        },
        "page_content": " as the investigation of their language modeling capacity, something that remains to be investigated in the case of, for instance, morphologically rich languages. We take advantage of the newly available multilingual corpus OSCAR BIBREF13 and train a monolingual language model for French using the RoBERTa architecture. We pretrain the model - which we dub CamemBERT- and evaluate it in four different downstream tasks for French: part-of-speech (POS) tagging, dependency parsing,",
        "type": "Document"
      },
      {
        "id": "d8d4b415-62b8-495e-bc64-e1b87f15beb3",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1811.01001",
          "start_idx": 20923,
          "end_idx": 21274
        },
        "page_content": " for $a^n b^n$ ; 2, 3, 4, and 36 hidden units for $a^n b^n c^n$ ; and 3, 4, 5, and 36 hidden units for $a^n b^n c^n d^n$ . The 36 hidden unit case represents an over-parameterized network with more than enough theoretical capacity to recognize all these languages. Figure 4 shows the generalization graphs for the three formal languages trained with L",
        "type": "Document"
      },
      {
        "id": "ef7cf75c-9501-4dc6-a4c6-bf811f2b08db",
        "metadata": {
          "chunk_id": 62,
          "document_id": "1811.01001",
          "start_idx": 21111,
          "end_idx": 21578
        },
        "page_content": " recognize all these languages. Figure 4 shows the generalization graphs for the three formal languages trained with LSTM models under different training windows. We note that enlarging the training length-window, naturally, enables an LSTM model to generalize far beyond its training length threshold. Besides, we see that the models with the training length-window of $[50, 100]$ performed slightly better than the other two window ranges in the case of $a^n b^n c^",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is a confusion network or lattice?",
    "llm_answer": "A confusion network and a lattice are graph-like structures where arcs connected by nodes represent hypothesized words.  In a lattice, each node marks a timestamp in an utterance, and each arc represents a hypothesized word with its corresponding acoustic and language model scores.\n",
    "context": [
      {
        "id": "7d889172-e8d6-4a5c-b652-43caa3284c6c",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1810.13024",
          "start_idx": 8547,
          "end_idx": 9018
        },
        "page_content": "ices and confusion networks, these metrics are computed across all arcs in the network.  A number of important downstream and upstream applications rely on accurate confidence scores in graph-like structures, such as confusion networks (CN) in Fig. 2 and lattices in Fig. 2 , where arcs connected by nodes represent hypothesised words. This section describes an extension of BiRNNs to CNs and lattices. Fig. 2 shows that compared to 1-best sequences in Fig. 2 , each node",
        "type": "Document"
      },
      {
        "id": "75e8b0b7-e878-4ed1-9749-71394ad1b1a0",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1810.13024",
          "start_idx": 1643,
          "end_idx": 2202
        },
        "page_content": "-best hypotheses were reported, the impact of these approaches on all hypotheses available within confusion networks and lattices has not been investigated. Extending confidence estimation to confusion network and lattice structures can be straightforward for some approaches, such as decision trees, and challenging for others, such as recurrent forms of neural networks. The previous work on encoding graph structures into neural networks BIBREF13 has mostly focused on embedding lattices into a fixed dimensional vector representation BIBREF14 , BIBREF15 .",
        "type": "Document"
      },
      {
        "id": "bb83e49c-9576-4ac2-8ace-31f6d8a9bded",
        "metadata": {
          "chunk_id": 74,
          "document_id": "1902.09087",
          "start_idx": 27297,
          "end_idx": 27569
        },
        "page_content": "\u201d(founder), so it loses the most important information. While in lattice, although overlaps are limited, \u201cWangZhan\u201d(website, \u201cWang\u201d web, \u201cZhan\u201d station) can match \u201cWangZhi\u201d(Internet addresses, \u201cWang\u201d web, \u201cZhi\u201d addresses) and also relate to \u201cDaoHang\u201d(navigate), from which",
        "type": "Document"
      },
      {
        "id": "842d2998-eeb9-480b-80e0-a08f8021eb53",
        "metadata": {
          "chunk_id": 41,
          "document_id": "1810.13024",
          "start_idx": 11884,
          "end_idx": 12389
        },
        "page_content": " Fig. 2 , each node in a lattice marks a timestamp in an utterance and each arc represents a hypothesised word with its corresponding acoustic and language model scores. Although lattices do not normally obey a linear graph structure, if they are traversed in the topological order, no changes are required to compute confidences over lattice structures. The way the information is propagated in these graph structures is similar to the forward-backward algorithm BIBREF17 . There, the forward probability",
        "type": "Document"
      },
      {
        "id": "0b023f80-ce59-4708-b25a-019f54ea08dd",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1810.13024",
          "start_idx": 8202,
          "end_idx": 8547
        },
        "page_content": "all}(\\theta ) = \\dfrac{\\text{TP}(\\theta )}{\\text{TP}(\\theta ) + \\text{FN}(\\theta )}$$   (Eq. 10)  for a range of thresholds $\\theta $ , where TP are true positives, FP and FN are false positives and negatives. When evaluating performance on lattices and confusion networks, these metrics are computed across all arcs in the network.  A number of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How close do clusters match to ground truth tone categories?",
    "llm_answer": "In Mandarin, the 4 clusters correspond very well with the 4 phonemic tone categories.  In Cantonese, the 5 clusters correspond to low-falling, mid-level, high-level, mid-rising, and low-rising tones.\n",
    "context": [
      {
        "id": "2d7f6d6a-c3af-4623-b31e-261ce51133f0",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1910.08987",
          "start_idx": 10105,
          "end_idx": 10494
        },
        "page_content": " 5 clusters A-E correspond to low-falling, mid-level, high-level, mid-rising, and low-rising tones. Tone clustering in Cantonese is expected to be more difficult than in Mandarin because of 6 contrastive tones, rather than 4. The model is more effective at clustering the higher tones (1, 2, 3), and less effective at clustering the lower tones (4, 5, 6), particularly tone 4 (low-falling)",
        "type": "Document"
      },
      {
        "id": "26748f70-f448-4b70-b0ff-baa7f96d2e2e",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1710.10609",
          "start_idx": 18530,
          "end_idx": 19040
        },
        "page_content": " and compared with the K-means approach. We used the partitioning formed by the 8 categories (from which the utterance pairs were selected) as the ground truth clustering. Table TABREF20 summarizes the results. We observe that for K-means algorithm, the vectors generated from sequence to sequence model perform better than the vectors generated using paragraph to vector for both the domains. This is expected as the vectors generated from sequence to sequence model encode some adjacency information as well.",
        "type": "Document"
      },
      {
        "id": "19d82ce2-d922-4c34-ab74-39da7e52bd8a",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1910.08987",
          "start_idx": 9456,
          "end_idx": 9895
        },
        "page_content": " the latent space learned by the autoencoders and the clustering output. Our model found 4 tone clusters in Mandarin, matching the number of phonemic tones (Table TABREF12) and 5 in Cantonese, which is one fewer than the number of phonemic tones (Table TABREF13). In Mandarin, the 4 clusters correspond very well with the the 4 phonemic tone categories, and the generated contours closely match the ground truth in Figure FIGREF2. There is",
        "type": "Document"
      },
      {
        "id": "7b095159-8394-4124-b29f-6cbb88e59fec",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1910.08987",
          "start_idx": 9707,
          "end_idx": 10105
        },
        "page_content": " categories, and the generated contours closely match the ground truth in Figure FIGREF2. There is some overlap between tones 3 and 4; this is because tone 3 is sometimes realized a low-falling tone without the final rise, a process known as half T3 sandhi BIBREF19, thus, it may overlap with tone 4 (falling tone). In Cantonese, the 5 clusters A-E correspond to low-falling, mid-level, high-level,",
        "type": "Document"
      },
      {
        "id": "0490f139-ce1c-4e72-9d13-0e6d0a2e96fc",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1910.08987",
          "start_idx": 11511,
          "end_idx": 11972
        },
        "page_content": " phrase. To test this hypothesis, we evaluate the model on only the first syllable of every word, which eliminates carry-over and declination effects (Table TABREF14). In both Mandarin and Cantonese, the clustering is more accurate when using only the first syllables, compared to using all of the syllables. We propose a model for unsupervised clustering and discovery of phonemic tones in tonal languages, using spoken words as input. Our model extracts the F",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what are the evaluation metrics?",
    "llm_answer": "Task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, ROUGE, BLEU, METEOR, micro-F1, BPRA, F1-score, user satisfaction degree, dialogue fluency.\n",
    "context": [
      {
        "id": "6b1e72f3-8ee5-43ac-a062-f08c158e3d67",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1709.10217",
          "start_idx": 6394,
          "end_idx": 6850
        },
        "page_content": " evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the number of total tasks. User satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied, neutral, satisfied and very satisfied, respectively. Response fluency: There are three scores -1, 0, 1, which indicate nonfluency, neutral, fluency. Number of dialogue turns: The number of utterances in a task",
        "type": "Document"
      },
      {
        "id": "08e94578-b4df-43f1-b96f-4db730b4d02a",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1607.06025",
          "start_idx": 10088,
          "end_idx": 10607
        },
        "page_content": "REF37 , the evaluation metrics of text-generating models fall into three categories: manual evaluation, automatic evaluation metrics, task-based evaluation. In evaluation based on human judgment each generated textual example is inspected manually. The automatic evaluation metrics, like ROUGE, BLEU and METEOR, compare human texts and generated texts. BIBREF38 shows METEOR has the strongest correlation with human judgments in image description evaluation. The last category is task-based evaluation, where the impact",
        "type": "Document"
      },
      {
        "id": "447cbf78-0bad-4440-920c-4bdb449fa2bb",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1909.07734",
          "start_idx": 6722,
          "end_idx": 7207
        },
        "page_content": ", sadness, anger, and neutral. Evaluation of submissions was done using only utterances with these four labels. Utterances with labels other than the above four (i.e., surprise, disgust, fear or non-neutral) were discarded and not used in the evaluation. Scripts for verifying and evaluating the submissions were made available online. We used micro-F1 as the comparison metric. A total of eleven teams submitted their evaluations, and are presented in the online leaderboard. Seven of",
        "type": "Document"
      },
      {
        "id": "72b50da8-6c06-4809-91ff-95e6cb664a0d",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1909.09484",
          "start_idx": 15874,
          "end_idx": 16378
        },
        "page_content": " of the dialogue state tracker, it aims to show the impact of the dialogue state tracker on the dialogue policy maker; (2) the accuracy of dialogue policy maker, it aims to explain the performance of different methods of constructing dialogue policy; (3) the quality of the final response, it aims to explain the impact of the dialogue policy on the final dialogue response. The evaluation metrics are listed as follows: BPRA: Belief Per-Response Accuracy (BPRA) tests the ability to generate the correct",
        "type": "Document"
      },
      {
        "id": "b3036530-5228-47f4-b1f0-9a1687f209da",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1709.10217",
          "start_idx": 4141,
          "end_idx": 4657
        },
        "page_content": ", in which only the released data can be used for training and developing. The other is an open evaluation that allow to collect external data for training and developing. For task 1, we use F1-score as evaluation metric. For the task-oriented dialogue systems, the best way for evaluation is to use the online human-computer dialogue. After finishing an online human-computer dialogue with a dialogue system, the human then manually evaluate the system by using the metrics of user satisfaction degree, dialogue flu",
        "type": "Document"
      }
    ]
  },
  {
    "query": "which datasets were used in evaluation?",
    "llm_answer": "An open dataset for Polish QA systems, published by BIBREF56, gathered from Did you know... column of Polish Wikipedia.\n",
    "context": [
      {
        "id": "62bfaadf-d388-4a07-b13b-c24fa0bf8a92",
        "metadata": {
          "chunk_id": 116,
          "document_id": "1605.08675",
          "start_idx": 42686,
          "end_idx": 43142
        },
        "page_content": " was involved in previous experiments BIBREF4 , BIBREF36 . Evaluation set bases on an open dataset for Polish QA systems, published by BIBREF56 . It has been gathered from Did you know... column, appearing in the main page of the Polish Wikipedia. It contains 4721 questions, from which 1000 have been analysed, which resulted in 576 satisfying the task constrains, given in chapter SECREF2 . Table TABREF85 shows a distribution of different question types",
        "type": "Document"
      },
      {
        "id": "7bb4c95c-4811-4e15-ab8e-deca3c53de73",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1709.08299",
          "start_idx": 9435,
          "end_idx": 10003
        },
        "page_content": " questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models. The",
        "type": "Document"
      },
      {
        "id": "e3748966-4a55-4502-9800-788e84cbe8cb",
        "metadata": {
          "chunk_id": 134,
          "document_id": "1605.08675",
          "start_idx": 49919,
          "end_idx": 50491
        },
        "page_content": ", used at this stage, substantially differs from the one used during the development (see section SECREF80 ). A configuration for the final evaluation has been prepared based on results of the experiments. All of the tested versions share the following features: no question analysis corrections, question classification and query generation solutions which proved best in the previous experiments (see section SECREF18 ), a retrieved set of documents including 20 articles, no minimal confidence, singe sentence context with title. Tested solutions differ with respect to",
        "type": "Document"
      },
      {
        "id": "93a24c0d-f0fe-4956-bef2-abf5c9a5e603",
        "metadata": {
          "chunk_id": 122,
          "document_id": "1701.09123",
          "start_idx": 46004,
          "end_idx": 46555
        },
        "page_content": " test data differs. Annotation guidelines: The gold annotation of the test data follows different guidelines from the training data. This is usually reflected in different named entity spans. The datasets and languages chosen for these experiments are based on the availability of both previous results and publicly distributed NERC systems to facilitate direct comparison of our system with other approaches. Table TABREF83 specifies the datasets used for each out-of-domain setting and language. Details of each dataset can be found Table TABREF10 .",
        "type": "Document"
      },
      {
        "id": "c85b69e6-0bcc-433b-b0ae-ba235722e16f",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1909.00124",
          "start_idx": 12394,
          "end_idx": 12855
        },
        "page_content": " during training. Summary statistics of the training, validation, and test data are shown in Table TABREF9. Noisy-labeled Training Datasets. For the above three domains (movie, laptop, and restaurant), we collected 2,000 reviews for each domain from the same review source. We extracted sentences from each review and assigned review's label to its sentences. Like previous work, we treat 4 or 5 stars as positive and 1 or 2 stars as negative. The data is noisy",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what are the baselines?",
    "llm_answer": "ANRES_Sing_R, ANRES_Sing_Z, ANRES_Seq_R, ANRES_Seq_Z.\n",
    "context": [
      {
        "id": "3747343d-0517-442a-b7ba-66d01be55b6d",
        "metadata": {
          "chunk_id": 40,
          "document_id": "2004.01878",
          "start_idx": 14526,
          "end_idx": 14915
        },
        "page_content": " normal distribution $\\mathcal {N}(0, 1)$. In summary, the following four baselines are designed: ANRES_Sing_R: randomly initializing the states for each single trading day. ANRES_Sing_Z: initializing the states as zeros for each single trading day. ANRES_Seq_R: randomly initializing the first states for each trading sequence only. ANRES_Seq_Z: initializing the first states as zeros for",
        "type": "Document"
      },
      {
        "id": "f297d78b-0fdd-4eee-ae9e-9e75a2e66130",
        "metadata": {
          "chunk_id": 140,
          "document_id": "2001.02380",
          "start_idx": 51470,
          "end_idx": 51956
        },
        "page_content": " baselines. The left, middle and right panels in Figure FIGREF40 correspond to measurements when all signals are included, only cases contained entirely in the head EDUs shown to the model, and only DMs, respectively. The scenario on the left is rather unreasonable and is included only for completeness: here the model is also penalized for not detecting signals such as lexical chains, part of which is outside the units that the model is being shown. An example of such a case can be",
        "type": "Document"
      },
      {
        "id": "7bb4c95c-4811-4e15-ab8e-deca3c53de73",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1709.08299",
          "start_idx": 9435,
          "end_idx": 10003
        },
        "page_content": " questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models. The",
        "type": "Document"
      },
      {
        "id": "3ff0334d-c1b6-4522-b5b5-ee27747097d6",
        "metadata": {
          "chunk_id": 63,
          "document_id": "2004.01980",
          "start_idx": 22218,
          "end_idx": 22603
        },
        "page_content": " all baselines. We use the summarization-related evaluation metrics, i.e., BLEU, ROUGE, CIDEr, and METEOR, to measure how relevant the generated headlines are to the news articles, to some extent, by comparing them to the original human-written headlines. In Table TABREF59, the first row \u201cNHG\u201d shows the performance of the current state-of-the-art summarization model on our data, and",
        "type": "Document"
      },
      {
        "id": "b7568416-2511-4e7d-b42a-905e8dea8f55",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1909.09484",
          "start_idx": 19263,
          "end_idx": 19734
        },
        "page_content": " models and baselines, with a learning rate of 0.001 for supervised training and 0.0001 for reinforcement learning. In reinforcement learning, the decay parameter $\\lambda $ is set to 0.8. The weight decay is set to 0.001. And early stopping is performed on developing set. The experimental results of the proposed model and baselines will be analyzed from the following aspects. BPRA Results: As shown in Table TABREF35, most of the models have similar performance on BP",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What monolingual word representations are used?",
    "llm_answer": "Monolingual word embeddings.\n",
    "context": [
      {
        "id": "900970c2-ab45-448d-84ca-db5d0bf6e8ae",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1911.00069",
          "start_idx": 3095,
          "end_idx": 3624
        },
        "page_content": " across a variety of language families, without using aligned parallel corpora or machine translation systems. We organize the paper as follows. In Section 2 we provide an overview of our approach. In Section 3 we describe how to build monolingual word embeddings and learn a linear mapping between two languages. In Section 4 we present a neural network architecture for the source-language (English). In Section 5 we evaluate the performance of the proposed approach for a number of target languages. We discuss related work in",
        "type": "Document"
      },
      {
        "id": "178ea747-e40b-483f-bd4d-a42ac05a99db",
        "metadata": {
          "chunk_id": 20,
          "document_id": "2002.02427",
          "start_idx": 7113,
          "end_idx": 7613
        },
        "page_content": ".g., punctuation marks, positive and negative emoticons, quotations, personal pronouns, tweet's length, named entities) while others are language-dependent relying on dedicated lexicons (e.g., negation, opinion lexicons, opposition words). Several classical machine learning classifiers were tested with several feature combinations, among them Random Forest (RF) achieved the best result with all features. Neural model with monolingual embeddings. We used Convolutional Neural Network (CNN) network",
        "type": "Document"
      },
      {
        "id": "9a2be53b-5031-434f-a029-a70d5cec11f6",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1911.00069",
          "start_idx": 5108,
          "end_idx": 5566
        },
        "page_content": " vocabulary $V=|\\mathcal {V}|$ for efficient representation. It also aims to capture semantic similarities between the words based on their distributional properties in large samples of monolingual data. Cross-lingual word embedding models try to build word embeddings across multiple languages BIBREF15, BIBREF16. One approach builds monolingual word embeddings separately and then maps them to the same vector space using a bilingual dictionary BIBREF17, B",
        "type": "Document"
      },
      {
        "id": "d7ec6345-56c7-4697-b2e9-66a9e6a51509",
        "metadata": {
          "chunk_id": 18,
          "document_id": "2002.02427",
          "start_idx": 6346,
          "end_idx": 6847
        },
        "page_content": "-art models in monolingual ID but to investigate which of the monolingual architectures (neural or feature-based) can achieve comparable results with existing systems. The result can show which kind of features works better in the monolingual settings and can be employed to detect irony in a multilingual setting. In addition, it can show us to what extend ID is language dependent by comparing their results to multilingual results. Two models have been built, as explained below. Prior to learning,",
        "type": "Document"
      },
      {
        "id": "bc81cfe8-170a-47ff-96a1-46a89b7c7482",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1809.08510",
          "start_idx": 18865,
          "end_idx": 19389
        },
        "page_content": " optimization problem. We showed that we can use these representations to learn tasks in one language and automatically transfer them to others with no additional training. Furthermore we studied the importance of the Wasserstein constraint through the $\\lambda $ hyper-parameter. And lastly we explored the difference between a standard multi-lingual language model and UG-WGAN by studying the generated outputs of the respective language models as well as the perplexity gap growth with respect to the number of languages.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Do they build one model per topic or on all topics?",
    "llm_answer": "The context describes models that explicitly model different types of questions, but it does not specify whether a model is built per topic or across all topics.\n",
    "context": [
      {
        "id": "54830273-3c90-417d-8898-e4c92c7fa94a",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1703.04617",
          "start_idx": 14584,
          "end_idx": 15140
        },
        "page_content": "ity. The previous models are often trained for all questions without explicitly discriminating different question types; however, for a target question, both the common features shared by all questions and the specific features for a specific type of question are further considered in this paper, as they could potentially obey different distributions. In this paper we further explicitly model different types of questions in the end-to-end training. We start from a simple way to first analyze the word frequency of all questions, and obtain top-10 most",
        "type": "Document"
      },
      {
        "id": "3cfa11de-5614-4207-8be8-8c0c0438be5a",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1804.08139",
          "start_idx": 12806,
          "end_idx": 13320
        },
        "page_content": " four models because each task-specific classifier should consider the output of shared layer which is quite unstable during the beginning of training phrase. Moreover, benefit from the attention mechanism which is useful in feature extraction, SA-TML and DA-MTL are converged much more quickly than the rest of models. Since all the tasks share the same sentence encoding layer, the query vector INLINEFORM0 of each task determines which part of the sentence to attend. Thus, similar tasks should have the similar",
        "type": "Document"
      },
      {
        "id": "718f65f2-c5bb-476b-b90b-e4e23834089d",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1709.07916",
          "start_idx": 5355,
          "end_idx": 5826
        },
        "page_content": "cancer\", and \u201cinfluenza\" into a topic that has an overall \u201cdisease\" theme BIBREF44 , BIBREF45 . Topic modeling has a wide range of applications in health and medical domains such as predicting protein-protein relationships based on the literature knowledge BIBREF46 , discovering relevant clinical concepts and structures in patients' health records BIBREF47 , and identifying patterns of clinical events in a cohort of brain cancer patients BIBREF48 . Among topic models",
        "type": "Document"
      },
      {
        "id": "50cd9541-56c1-4a56-9e79-0a94f4b6b7f5",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1705.07368",
          "start_idx": 7527,
          "end_idx": 7989
        },
        "page_content": "0 belong to the same topic is relaxed, and replaced with a distribution over topics INLINEFORM1 . In the model's assumed generative process, for each word INLINEFORM2 in document INLINEFORM3 , a topic assignment INLINEFORM4 is drawn via INLINEFORM5 , then the word is drawn from the chosen topic INLINEFORM6 . The mixed membership formalism provides a useful compromise between model flexibility and statistical efficiency: the INLINEFORM7 topics INLINEFORM8 are",
        "type": "Document"
      },
      {
        "id": "485fd605-d98a-4cdf-9406-48bd72a65f91",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1604.05781",
          "start_idx": 12804,
          "end_idx": 13245
        },
        "page_content": "T$ and documents are considered as mixtures of topics via $P(T|d)$ , the probability of topic $T$ given document $d$ . These distributions are then found via statistical inference given the observed distributions of unigrams across documents. The total number of topics is a parameter chosen by the practitioner. For this study we used the MALLET v2.0.8RC3 topic modeling toolkit BIBREF40 for model inference. By inspecting the most probable",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Do they quantitavely or qualitatively evalute the output of their low-rank approximation to verify the grouping of lexical items?",
    "llm_answer": "Quantitatively.\n",
    "context": [
      {
        "id": "330a54e8-d3b6-431a-9c4e-4b4f53850d30",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1807.09671",
          "start_idx": 10469,
          "end_idx": 10903
        },
        "page_content": ", hence the help of automatic summarization. Our extensions of this work are along three dimensions: (i) we crack the \u201cblack-box\u201d of the low-rank approximation algorithm to understand if it indeed allows lexically-diverse but semantically-similar items to share co-occurrence statistics; (ii) we compare the ILP-based summarization framework with state-of-the-art baselines, including a popular neural encoder-decoder model for summar",
        "type": "Document"
      },
      {
        "id": "aaab9fbb-6d6f-48c2-b0f8-d39a33d6a4c4",
        "metadata": {
          "chunk_id": 136,
          "document_id": "1605.08675",
          "start_idx": 50790,
          "end_idx": 51265
        },
        "page_content": " results of the final evaluation, expressed by recall, precision, F1 measure and Mean Reciprocal Rank (MRR). Standard deviations of these values have been obtained by bootstrap resampling of the test set. Additionally, precision obtained by automatic evaluation has been added, where applicable. As we can see, only a small percentage of questions is handled by the quantitative entities recognition. NER-based solutions deal with slightly more (Nerf) or less (Liner2) than a",
        "type": "Document"
      },
      {
        "id": "14574cff-9143-4b61-a54e-d1eef9ceef88",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1807.09671",
          "start_idx": 4233,
          "end_idx": 4762
        },
        "page_content": " allows different sentences to share co-occurrence statistics. For example, \u201cThe activity with the bicycle parts\" will be allowed to partially contain \u201cbike elements\" although the latter phrase does not appear in the sentence. The low-rank matrix approximation provides an effective way to implicitly group lexically-diverse but semantically-similar expressions. It can handle out-of-vocabulary expressions and domain-specific terminologies well, hence being a more principled approach than heuristically calculating similarities",
        "type": "Document"
      },
      {
        "id": "c42a4ecf-2985-4370-9b02-907bf47dcd8c",
        "metadata": {
          "chunk_id": 54,
          "document_id": "1807.09671",
          "start_idx": 19986,
          "end_idx": 20421
        },
        "page_content": " an intrinsic evaluation to test whether the low-rank approximation does capture similar concepts or not. An example of the annotation is shown in Table TABREF15 , where phrases in the student responses that are semantically the same as the summary phrases are highlighted with the same color by human annotators. For example, \u201cerror bounding\" (S2), \u201cerror boundary\" (S4), \u201cfinding that error\" (S3), and \u201cdetermining the critical value",
        "type": "Document"
      },
      {
        "id": "bb1aabef-b76d-4b39-b7cf-8b9f94b6ded8",
        "metadata": {
          "chunk_id": 30,
          "document_id": "2004.01694",
          "start_idx": 12004,
          "end_idx": 12422
        },
        "page_content": " at WMT15, i. e., we run 1,000 iterations of the rankings recorded with Appraise followed by clustering (significance level $\\alpha =0.05$). Table TABREF17 shows the TrueSkill scores for each translation resulting from the evaluations by expert and non-expert translators. We find that translation expertise affects the judgement of MT$_1$ and H$_A$, where the rating gap is wider for the expert raters. This indicates",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How well does their system perform on the development set of SRE?",
    "llm_answer": "The context states that the system's performance on the SRE 2016 development set is described in a later section (SECREF79).\n",
    "context": [
      {
        "id": "9612e20e-4bb2-43b7-ac27-1494213608ba",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1707.03904",
          "start_idx": 4820,
          "end_idx": 5304
        },
        "page_content": "5\\%$ on Quasar-S and Quasar-T, while human performance is $50\\%$ and $60.6\\%$ respectively. For the automatic systems, we see an interesting tension between searching and reading accuracies \u2013 retrieving more documents in the search phase leads to a higher coverage of answers, but makes the comprehension task more difficult. We also collect annotations on a subset of the development set questions to allow researchers to analyze the categories in which their system performs well or",
        "type": "Document"
      },
      {
        "id": "01b4cdc4-3710-4dec-9296-e3d77fce77c2",
        "metadata": {
          "chunk_id": 99,
          "document_id": "1701.09123",
          "start_idx": 37380,
          "end_idx": 37854
        },
        "page_content": ", 1.72 and 1.85 respectively over the best single clustering model (CW600). It is worth mentioning that our models do not score best in the development data. As the development data is closer in style and genre to the training data BIBREF31 , this may suggest that our system generalizes better on test data that is not close to the training data; indeed, the results reported in Section SECREF79 seem to confirm this hypothesis. We also compared our results with respect to",
        "type": "Document"
      },
      {
        "id": "644859ec-212b-4705-9239-cf1b1521bff9",
        "metadata": {
          "chunk_id": 96,
          "document_id": "2001.02380",
          "start_idx": 37671,
          "end_idx": 38069
        },
        "page_content": ", we may note that the overall performance of the model, with an F-Score of 44.37, is not bad, though below the performance of state-of-the-art full discourse parsers (see BIBREF49) \u2013 this is to be expected, since the model is not aware of the entire RST tree, rather looking only at EDU pairs out of context, and given that standard scores on RST-DT come from a larger and more homogeneous corpus,",
        "type": "Document"
      },
      {
        "id": "9a4be966-1b18-4e35-8533-3c33ff3fbeba",
        "metadata": {
          "chunk_id": 97,
          "document_id": "1701.02877",
          "start_idx": 34796,
          "end_idx": 35231
        },
        "page_content": " Stanford NER and our CRFSuite baseline, the difference between its performance on seen NEs and unseen NEs is still noticeable. This is 21.77 for SENNA (macro average), whereas it is 29.41 for CRFSuite and 35.68 for Stanford NER. The fact that performance over unseen entities is significantly lower than on seen NEs partly explains what we observed in the previous section; i.e., that corpora with a high proportion of unseen entities",
        "type": "Document"
      },
      {
        "id": "0e135435-929e-416f-87f2-a23199b780a7",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1611.00514",
          "start_idx": 2202,
          "end_idx": 2692
        },
        "page_content": " , we describe experimental evaluation of the system on the SRE 2016 development set. Finally, we present a timing analysis of the system in Section SECREF6 . The fixed training condition is used to build our speaker recognition system. Only conversational telephone speech data from datasets released through the linguistic data consortium (LDC) have been used, including NIST SRE 2004-2010 and the Switchboard corpora (Switchboard Cellular Parts I and II, Switchboard2 Phase I,II and III)",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which of the classifiers showed the best performance?",
    "llm_answer": "Support Vector Machine (SVM)\n",
    "context": [
      {
        "id": "437c6727-97ee-41b6-8b5d-9e005e8f8062",
        "metadata": {
          "chunk_id": 55,
          "document_id": "2003.06279",
          "start_idx": 21474,
          "end_idx": 21889
        },
        "page_content": "ifier. Similarly to the results observed for Glove and Word2vec, the gain in performance obtained for kNN is low compared when compared to other methods. While Figures FIGREF14 \u2013 FIGREF16 show the relative behavior in the accuracy, it still interesting to observe the absolute accuracy rate obtained with the classifiers. In Table TABREF17, we show the best accuracy rate (i.e. $\\max \\Gamma _+ = \\max _p \\Gamma _+(p",
        "type": "Document"
      },
      {
        "id": "ad9d77ca-f26c-457e-b095-c4e2f1a59f48",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1707.03764",
          "start_idx": 16277,
          "end_idx": 16789
        },
        "page_content": " engineering. However, we believe that this is too strong a conclusion to draw from this limited study, since several factors specific to this setting need to be taken into account. For one, a support vector machine clearly outperforms other classifiers, but this does not mean that this is an inherently more powerful. Rather, we expect that an SVM is the best choice for the given amount of training data, but with more training data, a neural network-based approach would achieve better results. Regarding the",
        "type": "Document"
      },
      {
        "id": "e0b15f4e-9d1f-46af-afc3-aa83d84ccfd8",
        "metadata": {
          "chunk_id": 45,
          "document_id": "2001.00137",
          "start_idx": 17426,
          "end_idx": 17822
        },
        "page_content": "Net penalty; Nearest Centroid with Euclidian metric, where classification is done by representing each class with a centroid; Bernoulli Naive Bayes with smoothing parameter $alpha=10^{-2}$; K-means clustering with 2 clusters and L2 penalty; and Logistic Regression classifier with L2 penalty, tolerance of $10^{-4}$ and regularization term of $1.0$. Most often, the best performing classifier was",
        "type": "Document"
      },
      {
        "id": "e71da476-b3f2-46d1-b380-820aaeed1eb0",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1809.05752",
          "start_idx": 15241,
          "end_idx": 15722
        },
        "page_content": " zhang2011comparative found to improve classifier performance. Starting with the approach taken by McCoy et al. mccoy2015clinical, who used aggregate cosine similarity scores to compute domain similarity directly from their TF-IDF vector space model, we extend this method by training a suite of three-layer multilayer perceptron (MLP) and radial basis function (RBF) neural networks using a variety of parameters to compare performance. We employ the Keras deep learning library B",
        "type": "Document"
      },
      {
        "id": "4445c818-210b-4672-8e96-c4c50e1bca6a",
        "metadata": {
          "chunk_id": 133,
          "document_id": "1909.00183",
          "start_idx": 49280,
          "end_idx": 49763
        },
        "page_content": " with a 19% improvement with respect to TF-iDF text features and a 107% improvement with respect to categorical features. (For the PRC records, the corresponding improvements are 33% and 215%, respectively.) Note that similar improvements are also obtained for the other classifiers when using Doc2Vec and MS labels as features. It is also worth noting that the differences in the prediction of DoH between PRC and O-ranked records is reduced when using text tools and, specifically,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are the keywords associated with events such as protests selected?",
    "llm_answer": "Keywords associated with events are selected by searching for words that achieve the highest number of spikes matching the days of events, using Jaccard similarity to calculate scores.  The method looks for word-pairs that spike in frequency on the specific day of an event.\n",
    "context": [
      {
        "id": "fa34e0dd-5a69-42eb-a41e-3cc3fd2a11bd",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1901.00570",
          "start_idx": 4463,
          "end_idx": 4890
        },
        "page_content": " associated word-pairs with protest days. This feature selection method is built upon the assumption that people discuss an event on the day of that event more than on any day before or after the event. This implies that word-pairs related to the event will form a spike on this specific day. Some of the spiking word-pairs are related to the nature of the event itself, such as \u201ctaxi protest\u201d or \u201cfair education\u201d. These word-p",
        "type": "Document"
      },
      {
        "id": "ebaa8141-7c6b-4d3e-913a-636592654e6f",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1901.00570",
          "start_idx": 2522,
          "end_idx": 3002
        },
        "page_content": " using word-pairs, where we extract all the pairs of co-occurring words within each tweet. This allows us to recognize the context of the word ('Messi','strike' ) is different than ('labour','strike'). According to the distributional semantic hypothesis, event-related words are likely to be used on the day of an event more frequently than any normal day before or after the event. This will form a spike in the keyword count magnitude along the timeline as illustrated in Figure",
        "type": "Document"
      },
      {
        "id": "cd7e4b28-d849-41b5-8a11-b770eb9bcfe0",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1901.00570",
          "start_idx": 3002,
          "end_idx": 3504
        },
        "page_content": " the event. This will form a spike in the keyword count magnitude along the timeline as illustrated in Figure FIGREF6 . To find the words most associated with events, we search for the words that achieve the highest number of spikes matching the days of events. We use the Jaccard similarity metric as it values the spikes matching events and penalizes spikes with no event and penalizes events without spikes. Separate words can be noisy due to the misuse of the term by people, especially in big data",
        "type": "Document"
      },
      {
        "id": "b9376f23-b554-45a3-9ff2-b9b9a51ea9c7",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1901.00570",
          "start_idx": 4726,
          "end_idx": 5146
        },
        "page_content": " as \u201ctaxi protest\u201d or \u201cfair education\u201d. These word-pairs will appear once or twice along the time frame. Meanwhile, more generic word-pairs such as \u201chuman rights\u201d or \u201clabour strike\u201d will spike more frequently in the days of events regardless the protest nature. To test our method, we developed two experiments using all the tweets in Melbourne and Sydney over a period of 640 days. The total number of tweets exceeded 4",
        "type": "Document"
      },
      {
        "id": "f578e0b1-ab6f-44a9-8e0c-23092781b01a",
        "metadata": {
          "chunk_id": 74,
          "document_id": "1901.00570",
          "start_idx": 28155,
          "end_idx": 28667
        },
        "page_content": " paper, we proposed a framework to detect civil unrest events by tracking each word-pair volume in twitter. The main challenge with this model is to identify the word-pairs that are highly associated with the events with predictive power. We used temporal filtering to detect the spike within the time series vector and used Jaccard similarity to calculate the scores of each word-pair according to its similarity with the binary vector of event days. These scores are used to rank the word-pairs as features for",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many speeches are in the dataset?",
    "llm_answer": "1201\n",
    "context": [
      {
        "id": "44b8e5fd-d0db-4059-ba74-695d1461472d",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1808.06834",
          "start_idx": 9109,
          "end_idx": 9557
        },
        "page_content": "ongo ds235388.mlab.com:35388/synopsis -u public -p public We have annotated 1201 speeches with the four categories mentioned above, on the speeches. We also annotated stances of the speakers towards the bill/issue that is being debated on. There are two stances one is for and other is against. The statistics of the annotated data is shown in Table 4. Two humanities students were involved in the annotation of the four categories on 1201 speeches",
        "type": "Document"
      },
      {
        "id": "dbdf77a1-c0d3-4158-8ffc-8e4ffb4189f8",
        "metadata": {
          "chunk_id": 34,
          "document_id": "2002.01359",
          "start_idx": 14455,
          "end_idx": 14981
        },
        "page_content": "ased into a natural conversation by crowd workers. We ensure that the annotations for the dialogue state and slots generated by the simulator are preserved and hence need no other annotation. We omit details for brevity: please refer to BIBREF19 for more details. The entire dataset consists of over 16K dialogues spanning multiple domains. Overall statistics of the dataset and comparison with other datasets can be seen in Table TABREF9. Figure FIGREF8 shows the details of the distribution of dialogue lengths across single",
        "type": "Document"
      },
      {
        "id": "7676fff6-73ba-4a93-bd03-f1c1aa3b35a9",
        "metadata": {
          "chunk_id": 46,
          "document_id": "2003.06044",
          "start_idx": 16294,
          "end_idx": 16742
        },
        "page_content": " It covers various topics about our daily life. Each utterance in the conversation is manually labeled as one out of 4 dialogue act classes. Table TABREF18 presents the statistics for both datasets. In our preprocessing, the text was lowercased before tokenized, and then sentences were tokenized by WordPiece tokenizer BIBREF27 with a 30,000 token vocabulary to alleviate the Out-of-Vocabulary problem. [1]The author claimed that they achieved 78.",
        "type": "Document"
      },
      {
        "id": "f192cb41-d528-4016-aa73-f5426ddfa85c",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1808.06834",
          "start_idx": 4239,
          "end_idx": 4797
        },
        "page_content": " from a debate on NABARD bill in Lok Sabha. A speech can be labelled with multiple categories as members can appreciate and raise issues in the same speech. The following points are the contributions of this paper : Many linguists around the globe are concentrating on creation of parliamentary datasets. BIBREF1 gives an overview of the parliamentary records and corpora from countries with a focus on their availability through Clarin infrastructure. A dataset of Japanese Local Assembly minutes was created and analyzed for statistical data such as number",
        "type": "Document"
      },
      {
        "id": "3c82b205-5d09-49eb-b2f7-0d3d9bb50ffb",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1911.01799",
          "start_idx": 10361,
          "end_idx": 10700
        },
        "page_content": " two parts: VoxCeleb1 and VoxCeleb2. We used SITW BIBREF21, a subset of VoxCeleb1 as the evaluation set. The rest of VoxCeleb1 was merged with VoxCeleb2 to form the training set (simply denoted by VoxCeleb). The training set involves $1,236,567$ utterances from $7,185$ speakers, and the evaluation set involves $6,445$ utterances from 299",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is different in BERT-gen from standard BERT?",
    "llm_answer": "BERT-gen allows for the direct generation of text from out-of-the-box data.\n",
    "context": [
      {
        "id": "3a417c27-56d4-4884-9808-a1c551d116a4",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1812.06705",
          "start_idx": 10253,
          "end_idx": 10716
        },
        "page_content": " BERT shares the same model architecture with the original BERT. The differences are the input representation and training procedure. The input embeddings of BERT are the sum of the token embeddings, the segmentation embeddings and the position embeddings. For the segmentation embeddings in BERT, a learned sentence A embedding is added to every token of the first sentence, and if a second sentence exists, a sentence B embedding will be added to every token of",
        "type": "Document"
      },
      {
        "id": "4aa56808-357a-4899-b66f-2581ee39a385",
        "metadata": {
          "chunk_id": 85,
          "document_id": "2002.10832",
          "start_idx": 29327,
          "end_idx": 29767
        },
        "page_content": "ed, the model learns a \u201ccontortion\" allowing it to align text and vision. Finally, Image + Caption $>$ Image only can be attributed to BERT fine-tuning, contributing to an increase in the observed gap, and its emergence in earlier layers. We investigated whether the abstractions encoded in a pre-trained BERT model can generalize beyond text. We proposed BERT-gen, a novel methodology that allows to directly generate text from out-of-the-",
        "type": "Document"
      },
      {
        "id": "3dc17a8d-13b0-41b4-b7db-e6cf42f7f0a4",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1909.00578",
          "start_idx": 10519,
          "end_idx": 11017
        },
        "page_content": " have a larger receptive field as well. The BERT multi-task versions perform better with highly correlated qualities like $\\mathcal {Q}4$ and $\\mathcal {Q}5$ (as illustrated in Figures 2 to 4 in the supplementary material). However, there is not a clear winner among them. Mathematical equivalence does not lead to deterministic results, especially when random initialization and stochastic learning algorithms are involved. An in-depth exploration of this point would involve further investigation",
        "type": "Document"
      },
      {
        "id": "73d2f73d-e66a-4876-b17e-a74c675301a4",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1902.00821",
          "start_idx": 9236,
          "end_idx": 9525
        },
        "page_content": "P]}, x_{j+1:n}, \\texttt {[SEP]})$ , where [CLS] and [SEP] are special tokens and $x_{1:n}$ is a document splited into two sides of sentences $x_{1:j}$ and $x_{j+1:n}$ . The key performance gain of BERT comes from two novel pre-training objectives: masked language model (MLM) and next text",
        "type": "Document"
      },
      {
        "id": "76ebebdf-1afc-433f-b324-27fc4d2c7c11",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1902.00821",
          "start_idx": 8866,
          "end_idx": 9236
        },
        "page_content": "ERT itself. As such, BERT requires pre-training on large-scale data (Wikipedia articles) to fill intensive parameters in exchange for human structured architecture designs for specific end-tasks that carry human's understanding of data of those tasks. One training example of BERT is formulated as $(\\texttt {[CLS]}, x_{1:j}, \\texttt {[SEP]}, x_{j+1:n}, \\texttt {[SEP]})",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are multimodal representations combined?",
    "llm_answer": "Concatenated and then projected to a fixed size representation.\n",
    "context": [
      {
        "id": "3f575905-09b6-4ea2-aac2-6af230db202a",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1909.02764",
          "start_idx": 759,
          "end_idx": 1210
        },
        "page_content": "0. Note that the term \u201cmultimodal\u201d is also used beyond the combination of vision, audio, and text. For example, Soleymani2012 use it to refer to the combination of electroencephalogram, pupillary response and gaze distance. In this paper, we deal with the specific situation of car environments as a testbed for multimodal emotion recognition. This is an interesting environment since it is, to some degree, a controlled environment: Dialogue partners",
        "type": "Document"
      },
      {
        "id": "f2048211-c914-476d-ba11-f33dbc5a686c",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1909.08859",
          "start_idx": 15571,
          "end_idx": 15883
        },
        "page_content": " which results in another vector embedding $\\mathbf {f} \\in \\mathbb {R}^{2d_E \\times 1}$. Finally, these vector representations are concatenated and then projected to a fixed size representation using $\\mathbf {o}=\\varphi _o(\\left[\\mathbf {c}; \\mathbf {f}\\right]) \\in \\mathbb {R}^{2d \\times 1}$ where $\\varphi _o",
        "type": "Document"
      },
      {
        "id": "cc159715-c5f1-4f2c-bf96-4680342fe60f",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1603.00968",
          "start_idx": 7125,
          "end_idx": 7645
        },
        "page_content": "LINEFORM11 to feed into the softmax function, where INLINEFORM12 . This representation contains feature vectors generated from all sets of embeddings under consideration. We call this method multiple group CNN (MG-CNN). Here groups refer to the features generated from different embeddings. Note that this differs from `multi-channel' models because at the convolution layer we use different filters on each word embedding matrix independently, whereas in a standard multi-channel approach each filter would consider all",
        "type": "Document"
      },
      {
        "id": "0cc0ed83-2a4d-4567-a4d5-f221c3463f1d",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1910.02789",
          "start_idx": 7082,
          "end_idx": 7558
        },
        "page_content": " are combined using max-pooling followed by a ReLU activation. The result is then passed through multiple hidden linear layers with ReLU activation, eventually generating the final output. Contemporary methods for semantic representation of states currently follow one of three approaches: (1) raw visual inputs BIBREF2, BIBREF26, in which raw sensory values of pixels are used from one or multiple sources, (2) feature vectors BIBREF27, BIBREF28, in which general features of",
        "type": "Document"
      },
      {
        "id": "60b7818c-25c3-421f-860e-6909029e50f9",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1808.09920",
          "start_idx": 15254,
          "end_idx": 15527
        },
        "page_content": " representation is a gated combination of the previous representation and a non-linear transformation of the update message:  $$\\mathbf {h}^{(\\ell + 1)}_i = \\phi (\\mathbf {u}^{(\\ell )}_i) \\odot \\mathbf {a}^{(\\ell )}_i + \\mathbf {h}^{(\\ell )}_i \\odot (1 - \\mathbf {a}^{(\\ell",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the problem with existing metrics that they are trying to address?",
    "llm_answer": "Existing metrics penalize diversity and reward hypotheses similar to the reference hypothesis.  They also correlate weakly or moderately with human judgments, and a single reference doesn't perform well.\n",
    "context": [
      {
        "id": "c5dbb7af-19ec-4b95-9072-316b38078be0",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1607.06025",
          "start_idx": 3043,
          "end_idx": 3551
        },
        "page_content": " issue of these metrics is that they penalize the diversity since they penalize the generated hypotheses that are dissimilar to the reference hypothesis. An alternative metric is to use a NLI classifier to test the generated hypothesis if the input label is correct in respect to the premise. A perfect classifier would not penalize diverse hypotheses and would reward accurate and (arguably to some degree) comprehensible hypotheses. However, it would not reward non-trivial hypotheses. Non-trivial examples",
        "type": "Document"
      },
      {
        "id": "e6b1d2ab-b250-4d74-9b79-1c62f93eddd4",
        "metadata": {
          "chunk_id": 50,
          "document_id": "2002.01359",
          "start_idx": 20828,
          "end_idx": 21375
        },
        "page_content": " setting, we introduce changes in the definitions of evaluation metrics from prior work. These are listed below: [leftmargin=*] Joint goal accuracy calculation: Traditionally, joint goal accuracy has been defined as the accuracy of predicting the dialogue state for all domains correctly. This is not practical in our setup, as the large number of services would result in near zero joint goal accuracy if the traditional definition is used. Furthermore, an incorrect dialogue state prediction for a service in the beginning of a dialogue degrades",
        "type": "Document"
      },
      {
        "id": "51b1e3f8-9b61-465f-b0e8-a9b24dbb7303",
        "metadata": {
          "chunk_id": 93,
          "document_id": "2002.02492",
          "start_idx": 29289,
          "end_idx": 29596
        },
        "page_content": " perplexity. When $\\epsilon $ is too large ($\\epsilon =10^{-2}$), perplexity degrades. When $\\epsilon $ is too small ($\\epsilon =10^{-4}$), the lower-bound grows slowly, so $\\left<\\text{eos}\\right>$ is not guaranteed to be top-ranked within $L$ steps, and the metrics resemble the baseline's. An $\\epsilon $",
        "type": "Document"
      },
      {
        "id": "ca9dddcc-e3f4-4f29-8c33-fa0819f9fd44",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1912.01046",
          "start_idx": 14275,
          "end_idx": 14679
        },
        "page_content": " BIBREF16, which measures how far away the predicted span is from the gold standard span, as a metric. The rationale behind the metric is that, in practice, it suffices to recommend a rough span which contains the answer \u2013 a difference of a few seconds would not matter much to the user. Specifically, the predicted span is counted as correct if $|pred_{start} - gt_{start}| + |pred_{end} - gt_{end}| <=$",
        "type": "Document"
      },
      {
        "id": "307ef70b-df78-41d5-bd05-f50ee85e89a0",
        "metadata": {
          "chunk_id": 68,
          "document_id": "1809.08731",
          "start_idx": 23047,
          "end_idx": 23501
        },
        "page_content": " also found fully automatic metrics to correlate only weakly or moderately with human judgments. bojar2016ten further confirmed that automatic MT evaluation methods do not perform well with a single reference. The need of better metrics for MT has been addressed since 2008 in the WMT metrics shared task BIBREF31 , BIBREF32 . For unsupervised dialogue generation, liu-EtAl:2016:EMNLP20163 obtained close to no correlation with human judgements for BLEU,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are discourse embeddings analyzed?",
    "llm_answer": "t-SNE clustering is performed on them, using the best performing model.  Closest neighbors are examined to see if similar discourse relations tend to group together.\n",
    "context": [
      {
        "id": "d73dbdc0-2011-4358-ac2c-3138d57f37a7",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1709.02271",
          "start_idx": 21132,
          "end_idx": 21601
        },
        "page_content": " transitions/discourse relations through the INLINEFORM0 -sized filter sweeps. To further study the information encoded in the discourse embeddings, we perform t-SNE clustering BIBREF20 on them, using the best performing model CNN2-DE (global). We examine the closest neighbors of each embedding, and observe that similar discourse relations tend to go together (e.g., explanation and interpretation; consequence and result). Some examples are given in Table TABREF29 .",
        "type": "Document"
      },
      {
        "id": "f7d976f1-32be-49f1-984c-dfa7bbbfec90",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1912.01673",
          "start_idx": 14535,
          "end_idx": 15033
        },
        "page_content": " use this corpus to analyze a wide spectrum sentence embeddings methods to see to what extent the continuous space they induce reflects semantic relations between sentences in our corpus. The very first analysis using LASER embeddings indicates lack of \u201cmeaning relatability\u201d, i.e. the ability to move along a trajectory in the space in order to reach desired sentence transformations. Actually, not even paraphrases are found in close neighbourhoods of embedded sentences. More \u201csemantic\u201d sentence",
        "type": "Document"
      },
      {
        "id": "a853274d-caa9-4253-b09f-df4bc6843aa9",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1912.01673",
          "start_idx": 1159,
          "end_idx": 1704
        },
        "page_content": ", with novel representations appearing almost every other week. With an overwhelming number of methods to compute sentence vector representations, the study of their general properties becomes difficult. Furthermore, it is not so clear in which way the embeddings should be evaluated. In an attempt to bring together more traditional representations of sentence meanings and the emerging vector representations, bojar:etal:jnle:representations:2019 introduce a number of aspects or desirable properties of sentence embeddings. One of them is den",
        "type": "Document"
      },
      {
        "id": "ad9d7cff-01ed-4a66-9570-050a05a20da0",
        "metadata": {
          "chunk_id": 17,
          "document_id": "2001.00137",
          "start_idx": 7115,
          "end_idx": 7610
        },
        "page_content": " by an embedding layer used for input representation, with the final input embedding being a sum of token embedddings, segmentation embeddings and position embeddings. The first one, token embedding layer, uses a vocabulary dictionary to convert each token into a more representative embedding. The segmentation embedding layer indicates which tokens constitute a sentence by signaling either 1 or 0. In our case, since our data are formed of single sentences, the segment is 1 until the first `",
        "type": "Document"
      },
      {
        "id": "916b788e-5a01-4f29-9b23-50dd0fd61da2",
        "metadata": {
          "chunk_id": 53,
          "document_id": "2003.12738",
          "start_idx": 18607,
          "end_idx": 19018
        },
        "page_content": " that of a ground-truth response. In our experiments, we introduce two different ways to represent sentence embeddings. The first is $\\textbf {EMB}_\\textbf {FT}$ BIBREF28 that calculates the average of word embeddings in a sentence using FastText BIBREF29 which is trained with Common Crawl and Wikipedia data. We use FastText embeddings instead of other pre-trained word embeddings because it can handle out-of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are discourse features incorporated into the model?",
    "llm_answer": "Two extensions utilize discourse information: (i) CNN2 enhanced with relation probability vectors (CNN2-PV), and (ii) CNN2 enhanced with discourse embeddings (CNN2-DE).\n",
    "context": [
      {
        "id": "bd502256-925b-4a56-9025-bd79b4e5e74b",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1709.02271",
          "start_idx": 2992,
          "end_idx": 3522
        },
        "page_content": " entities: grammatical relations, and RST discourse relations. We apply these models to datasets of varying sizes and genres, and find that adding any discourse information improves AA consistently on longer documents, but has mixed results on shorter documents. Further, embedding the discourse features in a parallel CNN at the input end yields better performance than concatenating them to the output layer as a feature vector (Section SECREF3 ). The global featurization is more effective than the local one. We also show that",
        "type": "Document"
      },
      {
        "id": "26941448-69e2-432e-ad0e-0607b204d3d9",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1709.02271",
          "start_idx": 3522,
          "end_idx": 4048
        },
        "page_content": "REF3 ). The global featurization is more effective than the local one. We also show that SVMs, which can only use discourse probability vectors, neither produce a competitive performance (even with fine-tuning), nor generalize in using the discourse information effectively. Entity-grid model. Typical lexical features for AA are relatively superficial and restricted to within the same sentence. F&H14 hypothesize that discourse features beyond the sentence level also help authorship attribution. In particular, they propose",
        "type": "Document"
      },
      {
        "id": "7c994867-cbf0-4c43-a12a-9bd52c3de356",
        "metadata": {
          "chunk_id": 56,
          "document_id": "1709.02271",
          "start_idx": 19835,
          "end_idx": 20341
        },
        "page_content": " over CNN2-PV models. With an SVM, where the option is not available, we are only able to use relation probability vectors to obtain a very modest performance improvement. Further, we found an input-length threshold for the discourse features to help (Section SECREF26 ). Not surprisingly, discourse does not contribute on shorter texts. Many of the feature grids are empty for these shorter texts\u2013 either there are no coreference chains or they are not correctly resolved. Currently we only have empirical",
        "type": "Document"
      },
      {
        "id": "e2045473-f1d2-4f7d-8297-253602d4ee44",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1709.02271",
          "start_idx": 8132,
          "end_idx": 8544
        },
        "page_content": " CNN (CNN2), and propose two extensions which utilize discourse information: (i) CNN2 enhanced with relation probability vectors (CNN2-PV), and (ii) CNN2 enhanced with discourse embeddings (CNN2-DE). The CNN2-PV allows us to conduct a comparison with F&H14 and F15, which also use relation probability vectors. CNN2. CNN2 is the baseline model with no discourse features. Illustrated in Figure FIGREF10 (center),",
        "type": "Document"
      },
      {
        "id": "f3011e7a-76e8-463a-ac93-dd1a9edc4b12",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1709.05413",
          "start_idx": 10756,
          "end_idx": 11288
        },
        "page_content": " included as features in our model, to characterize the emotional intent within each act. The underlying goal of this work is to show how a well-defined taxonomy of dialogue acts can be used to summarize semantic information in real-time about the flow of a conversation to derive meaningful insights into the success/failure of the interaction, and then to develop actionable rules to be used in automating customer service interactions. We focus on the customer service domain on Twitter, which has not previously been explored in",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What discourse features are used?",
    "llm_answer": "Relation probability vectors, discourse embeddings.\n",
    "context": [
      {
        "id": "d94b7c50-a9ee-4ced-b6e5-cf5b91720558",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1709.05413",
          "start_idx": 23760,
          "end_idx": 24235
        },
        "page_content": " performed. The following list describes the set of features used for our dialogue act classification tasks: Word/Punctuation: binary bag-of-word unigrams, binary existence of a question mark, binary existence of an exclamation mark in a turn Temporal: response time of a turn (time in seconds elapsed between the posting time of the previous turn and that of the current turn) Second-Person Reference: existence of an explicit second-person reference in the turn (you, your,",
        "type": "Document"
      },
      {
        "id": "e2045473-f1d2-4f7d-8297-253602d4ee44",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1709.02271",
          "start_idx": 8132,
          "end_idx": 8544
        },
        "page_content": " CNN (CNN2), and propose two extensions which utilize discourse information: (i) CNN2 enhanced with relation probability vectors (CNN2-PV), and (ii) CNN2 enhanced with discourse embeddings (CNN2-DE). The CNN2-PV allows us to conduct a comparison with F&H14 and F15, which also use relation probability vectors. CNN2. CNN2 is the baseline model with no discourse features. Illustrated in Figure FIGREF10 (center),",
        "type": "Document"
      },
      {
        "id": "26941448-69e2-432e-ad0e-0607b204d3d9",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1709.02271",
          "start_idx": 3522,
          "end_idx": 4048
        },
        "page_content": "REF3 ). The global featurization is more effective than the local one. We also show that SVMs, which can only use discourse probability vectors, neither produce a competitive performance (even with fine-tuning), nor generalize in using the discourse information effectively. Entity-grid model. Typical lexical features for AA are relatively superficial and restricted to within the same sentence. F&H14 hypothesize that discourse features beyond the sentence level also help authorship attribution. In particular, they propose",
        "type": "Document"
      },
      {
        "id": "f3011e7a-76e8-463a-ac93-dd1a9edc4b12",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1709.05413",
          "start_idx": 10756,
          "end_idx": 11288
        },
        "page_content": " included as features in our model, to characterize the emotional intent within each act. The underlying goal of this work is to show how a well-defined taxonomy of dialogue acts can be used to summarize semantic information in real-time about the flow of a conversation to derive meaningful insights into the success/failure of the interaction, and then to develop actionable rules to be used in automating customer service interactions. We focus on the customer service domain on Twitter, which has not previously been explored in",
        "type": "Document"
      },
      {
        "id": "8bfdd672-b760-4fe8-aae8-8f685fbc40f5",
        "metadata": {
          "chunk_id": 13,
          "document_id": "2001.02380",
          "start_idx": 5234,
          "end_idx": 5751
        },
        "page_content": " the next section, and point out some open questions which we would like to address. In Section SECREF3 we present the discourse annotated data that we will be using, which covers a number of English text types from the Web annotated for 20 discourse relations in the framework of Rhetorical Structure Theory, and is enriched with human annotations of discourse relation signaling devices for a subset of the data. Moreover, we also propose a taxonomy of anchored signals based on the discourse annotated data used in",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are proof paths?",
    "llm_answer": "Paths from the root to leaf nodes in an argument tree.\n",
    "context": [
      {
        "id": "ac9ae573-9dbf-4c0d-970e-4d8d17380120",
        "metadata": {
          "chunk_id": 29,
          "document_id": "2004.03034",
          "start_idx": 11463,
          "end_idx": 11945
        },
        "page_content": " This argument path represents a particular line of reasoning for the given thesis. Similarly, for each claim, all the claims along the path from the thesis to the claim, represent the context for the claim. For example, in Figure FIGREF1, the context for O1 consists of only the thesis, whereas the context for S3 consists of both the thesis and O1 since S3 is provided to support the claim O1 which is an opposing claim for the thesis. The claims are not constructed independently",
        "type": "Document"
      },
      {
        "id": "c100f621-4957-4c23-8cf4-7486defdd2eb",
        "metadata": {
          "chunk_id": 4,
          "document_id": "2004.02393",
          "start_idx": 1612,
          "end_idx": 2030
        },
        "page_content": " reasoning and the importance of which has also been demonstrated in text-based QA BIBREF8. The other example is how the annotated evidence pieces are connected, which requires at least the definition of arguments, such as a linking entity, concept, or event. Such information has proved useful by the recently popular entity-centric methods BIBREF9, BIBREF10, BIBREF11, BIBREF12, BIBREF0, BIBREF2 and intuitively will",
        "type": "Document"
      },
      {
        "id": "9e0d31e9-b29c-47af-8265-2bbad2a8b668",
        "metadata": {
          "chunk_id": 73,
          "document_id": "1810.00663",
          "start_idx": 28430,
          "end_idx": 28785
        },
        "page_content": " blue route (shortest path) and the red route (alternative path) are described by: [leftmargin=*, labelsep=0.2em, itemsep=0em] \u201cGo out the office and make a left. Turn right at the corner and go down the hall. Make a right at the next corner and enter the kitchen in front of table.\u201d \u201cExit the room 0 and turn right, go to the end of the corridor and turn",
        "type": "Document"
      },
      {
        "id": "cdfe15c7-531a-48c8-b3ae-02e514c6828f",
        "metadata": {
          "chunk_id": 1,
          "document_id": "2004.02393",
          "start_idx": 445,
          "end_idx": 995
        },
        "page_content": "4, where the evidence sentences for the reasoning process were labeled by human annotators. Such evidence annotations are crucial for modern model training, since they provide finer-grained supervision for better guiding the model learning. Furthermore, they allow a pipeline fashion of model training, with each step, such as passage ranking and answer extraction, trained as a supervised learning sub-task. This is crucial from a practical perspective, in order to reduce the memory usage when handling a large amount of inputs with advanced, large",
        "type": "Document"
      },
      {
        "id": "439ad799-2c89-4178-9cd1-63ec9e3ac53f",
        "metadata": {
          "chunk_id": 19,
          "document_id": "2004.03034",
          "start_idx": 7706,
          "end_idx": 8236
        },
        "page_content": " impact labels depending on the context in which it is presented. Figure FIGREF1 shows a partial argument tree for the argument thesis \u201cPhysical torture of prisoners is an acceptable interrogation tool.\u201d. Each node in the argument tree corresponds to a claim, and these argument trees are constructed and edited collaboratively by the users of the platform. Except the thesis, every claim in the argument tree either opposes or supports its parent claim. Each path from the root to leaf nodes corresponds to an argument path which",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What external sources are used?",
    "llm_answer": "Press releases, academic journals, books, and transcripts of recorded speech.\n",
    "context": [
      {
        "id": "ec106c42-6192-4e56-aaf4-c2716f44583f",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1902.00821",
          "start_idx": 5993,
          "end_idx": 6277
        },
        "page_content": "REF15 ), fictional stories (MCTest BIBREF16 , CBT BIBREF17 , NarrativeQA BIBREF18 ), general Web documents (MS MARCO BIBREF19 , TriviaQA BIBREF20 , SearchQA BIBREF21 ) and news articles (NewsQA BIBREF22 , CNN/Daily Mail BIBREF23 , and RACE BIBREF24 ). Recently, CRC BIBREF6 , BIBREF25",
        "type": "Document"
      },
      {
        "id": "d806a906-3735-4703-9650-0f8a802513f7",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1705.07368",
          "start_idx": 2093,
          "end_idx": 2536
        },
        "page_content": " NLP applications, where the data of interest are often not obtained from large scale sources such as the internet and social media, but from sources such as press releases BIBREF11 , academic journals BIBREF10 , books BIBREF12 , and transcripts of recorded speech BIBREF13 , BIBREF14 , BIBREF15 . A standard practice in the literature is to train word embedding models on a generic large corpus such as Wikipedia, and use the embeddings for N",
        "type": "Document"
      },
      {
        "id": "c7765dd8-8490-4941-b3c4-5aa9daa2b8a1",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1710.01507",
          "start_idx": 0,
          "end_idx": 495
        },
        "page_content": "The Internet provides instant access to a wide variety of online content, news included. Formerly, users had static preferences, gravitating towards their trusted sources, incurring an unwavering sense of loyalty. The same cannot be said for current trends since users are likely to go with any source readily available to them. In order to stay in business, news agencies have switched, in part, to a digital front. Usually, they generate revenue by (1) advertisements on their websites, or (2)",
        "type": "Document"
      },
      {
        "id": "f2c36271-e280-46fd-acfb-707fb70dab8f",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1704.08960",
          "start_idx": 4193,
          "end_idx": 4760
        },
        "page_content": " belongs to recent neural word segmentation. To our knowledge, there has been no work in the literature systematically investigating rich external resources for neural word segmentation training. Closest in spirit to our work, BIBREF11 empirically studied the use of various external resources for enhancing a statistical segmentor, including character mutual information, access variety information, punctuation and other statistical information. Their baseline is similar to ours in the sense that both character and word contexts are considered. On the other hand,",
        "type": "Document"
      },
      {
        "id": "fb4f7b01-7328-425d-9060-5b0bfa587873",
        "metadata": {
          "chunk_id": 75,
          "document_id": "1910.00458",
          "start_idx": 26170,
          "end_idx": 26674
        },
        "page_content": "26 are human-generated and based on source documents or summaries in free text format. However, since annotators tend to copy spans as answers BIBREF27, the majority of answers are still extractive in these datasets. The multi-choice QA datasets are collected either via crowd sourcing, or collected from examinations designed by educational experts BIBREF7. In this type of QA datasets, besides token matching, a significant portion of questions require multi-sentence reasoning and external knowledge B",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much better peformance is achieved in human evaluation when model is trained considering proposed metric?",
    "llm_answer": "The context states the proposed metric correlates *far better* with human judgment than BLEU and perplexity, and consistently outperforms other metrics by a large margin.  It also correlates better with human evaluation on both datasets used in the experiments.\n",
    "context": [
      {
        "id": "084aa0d9-16fb-4e10-96ab-25c2b43f3e22",
        "metadata": {
          "chunk_id": 63,
          "document_id": "2002.05058",
          "start_idx": 24978,
          "end_idx": 25505
        },
        "page_content": " To ensure the computational budget to be roughly the same, we fix the number of plays in skill rating to 10,000. The experimental results are summarized in Table 1. We can see that the proposed comparative evaluator correlates far better with human judgment than BLEU and perplexity. When compared with recently proposed parameterized metrics including adversarial evaluator and ADEM, our model consistently outperforms them by a large margin, which demonstrates that our comparison-based evaluation metric is able to evaluate",
        "type": "Document"
      },
      {
        "id": "703d0adf-adb7-49bb-838f-4d067a096026",
        "metadata": {
          "chunk_id": 67,
          "document_id": "2002.05058",
          "start_idx": 26625,
          "end_idx": 27228
        },
        "page_content": " the proposed comparative evaluator with skill rating significantly outperforms all compared baselines, including comparative evaluator with averaged sample-level scores. This demonstrates the effectiveness of the skill rating system for performing model-level comparison with pairwise sample-level evaluation. In addition, the poor correlation between conventional evaluation metrics including BLEU and perplexity demonstrates the necessity of better automated evaluation metrics in open domain NLG evaluation. We further investigate the impact of imperfect metrics on training NLG models. As described",
        "type": "Document"
      },
      {
        "id": "b48f4a7c-a75f-4695-9d0c-5e86c7f6e6ca",
        "metadata": {
          "chunk_id": 72,
          "document_id": "2002.05058",
          "start_idx": 28671,
          "end_idx": 29267
        },
        "page_content": " should be considered as of worse quality. The second example suggests that our approach handles the diversity in possible responses well, as it regards both positive response and negative response as valid responses. Hopefully, these examples may provide us with some insights about why the proposed metric correlates better with human preference. To better understand the proposed comparative evaluator and analyze the relative importance of its different components, we conduct an ablation study with several variants of the proposed model: w/o comparison: Evaluating generated samples without",
        "type": "Document"
      },
      {
        "id": "987497c9-00ff-4d98-a1f3-a41678512657",
        "metadata": {
          "chunk_id": 79,
          "document_id": "2002.05058",
          "start_idx": 31816,
          "end_idx": 32364
        },
        "page_content": "level evaluation based on sample-level pairwise comparison. By transferring pretrained natural language understanding knowledge from BERT and fine-tuning with strong and weak supervision examples and human preference annotations, our model correlates better with human judgment than other compared metrics. In addition, we find that when used as evaluation metrics, conventional metrics such as BLEU and perplexity may affect the training stage of NLG models as they may lead to sub-optimal hyperparameter choice and checkpoint selection. Our model",
        "type": "Document"
      },
      {
        "id": "d90ada72-ed73-4bee-ae92-4d02ce124225",
        "metadata": {
          "chunk_id": 10,
          "document_id": "2002.05058",
          "start_idx": 4096,
          "end_idx": 4674
        },
        "page_content": " evaluation information provided by our pairwise comparison model. The skill rating system is more efficient and accurate than several baseline approaches. We conduct experiments on both story generation task and open domain dialogue response generation task. Experimental results show that our approach correlates better with human evaluation on both datasets. Moreover, we show that using automated metrics such as BLEU to perform hyperparameter tuning and early-stopping results in sub-optimal model and our approach helps alleviate this problem. Evaluation of NLG models has",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much transcribed data is available for for Ainu language?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "1f5d4af4-eeeb-4e15-abf5-a8b5e099c601",
        "metadata": {
          "chunk_id": 2,
          "document_id": "2002.06675",
          "start_idx": 844,
          "end_idx": 1319
        },
        "page_content": " recordings have been made. Nevertheless, a majority of the recordings have not been transcribed and utilized effectively. Since transcribing them requires expertise in the Ainu language, not so many people are able to work on this task. Hence, there is a strong demand for an ASR system for the Ainu language. We started a project of Ainu ASR and this article is the first report of this project. We have built an Ainu speech corpus based on data provided by the Ainu Museum",
        "type": "Document"
      },
      {
        "id": "d71d4827-5149-42da-ae23-1b76b6e84632",
        "metadata": {
          "chunk_id": 14,
          "document_id": "2002.06675",
          "start_idx": 4927,
          "end_idx": 5394
        },
        "page_content": "of-speech (POS) tagging and word segmentation. Ainu speech recognition was tried by ainutrans with 2.5 hours of Ainu folklore data even though the Ainu language was not their main target. Their phone error rare was about 40% which is not an accuracy level for practical use yet. It appears that there has not been a substantial Ainu speech recognition study yet that utilizes corpora of a reasonable size. Therefore, our first step was to build a speech corpus for AS",
        "type": "Document"
      },
      {
        "id": "bc11d483-c82c-4673-9cdd-3a82baca314e",
        "metadata": {
          "chunk_id": 10,
          "document_id": "2002.06675",
          "start_idx": 3742,
          "end_idx": 4099
        },
        "page_content": " writing system, the Ainu language is currently written following the examples in a reference book \u201cAkor itak\u201d BIBREF9. With this writing system, it is transcribed with sixteen Roman letters {a, c, e, h, i, k, m, n, o, p, r, s, t, u, w, y}. Since each of these letters correspond to a unique pronunciation, we call them \u201cphones\u201d for convenience. In addition",
        "type": "Document"
      },
      {
        "id": "b8fcb76e-a290-4169-8e25-5f56d0887f8d",
        "metadata": {
          "chunk_id": 7,
          "document_id": "2002.06675",
          "start_idx": 2788,
          "end_idx": 3285
        },
        "page_content": " critically endangered in 2009 BIBREF8. In response to this situation, Ainu folklore and songs have been actively recorded since the late 20th century in efforts initiated by the Government of Japan. For example, the Ainu Museum started audio recording of Ainu folklore in 1976 with the cooperation of a few Ainu elders which resulted in the collection of speech data with the total duration of roughly 700 hours. This kind of data should be a key to the understanding of Ainu culture, but most of",
        "type": "Document"
      },
      {
        "id": "577d7002-7452-470a-9e91-d3e9ae2ce04d",
        "metadata": {
          "chunk_id": 8,
          "document_id": "2002.06675",
          "start_idx": 3106,
          "end_idx": 3579
        },
        "page_content": ". This kind of data should be a key to the understanding of Ainu culture, but most of it is not transcribed and fully studied yet. The Ainu language is an agglutinative language and has some similarities to Japanese. However, its genealogical relationship with other languages has not been clearly understood yet. Among its features such as closed syllables and personal verbal affixes, one important feature is that there are many compound words. For example, a word atuyk",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What baseline approaches do they compare against?",
    "llm_answer": "K-means, SkipVec, RecNN, and Para2vec.\n",
    "context": [
      {
        "id": "5c6209f3-cef8-4a17-88a8-a660a1a73e1e",
        "metadata": {
          "chunk_id": 76,
          "document_id": "1701.00185",
          "start_idx": 27376,
          "end_idx": 27736
        },
        "page_content": " and NMI performance of our proposed approaches and four baseline methods, K-means, SkipVec, RecNN and Para2vec based clustering methods. Intuitively, we get a general observation that (1) BoW based approaches, including K-means (TF) and K-means (TF-IDF), and SkipVec based approaches perform not well; (2) RecNN based approaches, both RecNN (Ave.) and RecNN (",
        "type": "Document"
      },
      {
        "id": "5f5a488a-b689-46d4-b32b-50ea0883fe6f",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1703.09684",
          "start_idx": 2106,
          "end_idx": 2596
        },
        "page_content": " evaluate both baseline and state-of-the-art VQA algorithms. We found that our proposed approach enables more nuanced comparisons of VQA algorithms, and helps us understand the benefits of specific techniques better. In addition, it also allowed us to answer several key questions about VQA algorithms, such as, `Is the generalization capacity of the algorithms hindered by the bias in the dataset?', `Does the use of spatial attention help answer specific question-types?', `How successful",
        "type": "Document"
      },
      {
        "id": "42837a06-4572-4044-8126-3100084f04f6",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1703.09684",
          "start_idx": 24411,
          "end_idx": 24828
        },
        "page_content": " simple baseline methods routinely exceed more complex methods using simple accuracy BIBREF7 , BIBREF8 , BIBREF19 . On TDIUC, MLP surpasses MCB and NMN in terms of simple accuracy, but a closer inspection reveals that MLP's score is highly determined by performance on categories with a large number of examples, such as `absurd' and `object presence.' Using MPT, we find that both NMN and MCB outperform MLP. Inspect",
        "type": "Document"
      },
      {
        "id": "9747c688-beb0-423e-95ef-7dc653615c9e",
        "metadata": {
          "chunk_id": 83,
          "document_id": "1902.00821",
          "start_idx": 25453,
          "end_idx": 25621
        },
        "page_content": " that the pre-tuning approach is highly effective and outperforms existing baselines or highly competitive with supervised baselines trained from a large-scale dataset.",
        "type": "Document"
      },
      {
        "id": "b7568416-2511-4e7d-b42a-905e8dea8f55",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1909.09484",
          "start_idx": 19263,
          "end_idx": 19734
        },
        "page_content": " models and baselines, with a learning rate of 0.001 for supervised training and 0.0001 for reinforcement learning. In reinforcement learning, the decay parameter $\\lambda $ is set to 0.8. The weight decay is set to 0.001. And early stopping is performed on developing set. The experimental results of the proposed model and baselines will be analyzed from the following aspects. BPRA Results: As shown in Table TABREF35, most of the models have similar performance on BP",
        "type": "Document"
      }
    ]
  },
  {
    "query": "how many domains did they experiment with?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "e9ff0ea6-7735-4c7f-b973-c089ea86b236",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1701.03214",
          "start_idx": 4004,
          "end_idx": 4439
        },
        "page_content": "-of-domain data is used for the in-domain data; while for the \u201cmulti domain\u201d and \u201cmixed fine tuning\u201d methods, we use a vocabulary obtained from the mixed in-domain and out-of-domain data for all the training stages. We conducted NMT domain adaptation experiments in two different settings as follows: Chinese-to-English translation was the focus of the high quality in-domain corpus setting. We utilized the resource rich patent out-of",
        "type": "Document"
      },
      {
        "id": "1b77f6bf-fa46-4a95-a3a5-e1c39a211c21",
        "metadata": {
          "chunk_id": 36,
          "document_id": "2002.01359",
          "start_idx": 15270,
          "end_idx": 15785
        },
        "page_content": " a significant number of unseen domains/APIs in the dev and test sets. 77% of the dialogue turns in the test set and 45% of the turns in dev set contain at least one service not present in the training set. This facilitates the development of models which can generalize to new domains with very few labelled examples. The submissions from 25 teams included a variety of approaches and innovative solutions to specific problems posed by this dataset. For the workshop, we received submissions from 9 of these teams.",
        "type": "Document"
      },
      {
        "id": "f0bc28e3-a9b6-4b9f-ad9f-675d8d8c04c4",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1907.11499",
          "start_idx": 1311,
          "end_idx": 1706
        },
        "page_content": " debates BIBREF0 , BIBREF3 , BIBREF4 , BIBREF5 . The question of how to best deal with multiple domains when training data is available for one or few of them has met with much interest in the literature. The field of domain adaptation BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 aims at improving the learning of a predictive function in a target domain where there is little or no labeled",
        "type": "Document"
      },
      {
        "id": "631af310-7574-47af-955f-bac444cca632",
        "metadata": {
          "chunk_id": 54,
          "document_id": "2002.06053",
          "start_idx": 20155,
          "end_idx": 20652
        },
        "page_content": " Later, domains were described as the words, and domain architectures as sentences of the language BIBREF58, BIBREF59. Protein domains were treated as the words of multi-domain proteins in order to evaluate the semantic meaning behind the domains BIBREF60. The study supported prior work by BIBREF59 suggesting that domains displayed syntactic and semantic features, but there are only a few multi-domain proteins with more than six domains limiting the use of domains as words to build sentences.",
        "type": "Document"
      },
      {
        "id": "62bfcbfc-b724-4fa4-aa44-462ae5737962",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1910.00825",
          "start_idx": 11594,
          "end_idx": 12020
        },
        "page_content": "406 of them are single-domain (8.93 turns on average) and 7,302 are multi-domain (15.39 turns on average). During MultiWOZ data collection, instruction is provided for crowd workers to perform the task. We use the instructions as the dialog summary, and an example data is shown in Table TABREF25. Dialog domain label is extracted from existing MultiWOZ annotation. In the experiment, we split the dataset into 8,438 training,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How long is the dataset?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "6f04ea61-adc5-4abb-9261-abc554ebac1b",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1711.02013",
          "start_idx": 20658,
          "end_idx": 21116
        },
        "page_content": "ings, between recurrent layers, and on recurrent states were (0.7, 0.5, 0.5) respectively. dataset contains 17M training tokens and has a vocabulary size of 44k words. The dataset is partitioned into a training set (first 99M characters) and a development set (last 1M characters) that is used to report performance. As this dataset contains various articles from Wikipedia, the longer term information (such as current topic) plays a bigger role than in the",
        "type": "Document"
      },
      {
        "id": "9ccb7945-9a96-417a-b52e-68855c4910b1",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "2d3bdfec-4ed5-456d-bd82-023fad798858",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "b2a50f9d-7c05-4ecd-b65d-87cb9f0f75d3",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1906.03538",
          "start_idx": 12505,
          "end_idx": 12830
        },
        "page_content": ". We now provide a brief summary of [wave]390P[wave]415e[wave]440r[wave]465s[wave]485p[wave]525e[wave]535c[wave]595t[wave]610r[wave]635u[wave]660m. The dataset contains about INLINEFORM0 claims with a significant length diversity (Table TABREF19 ). Additionally, the dataset comes with INLINEFORM1 perspectives, most of which",
        "type": "Document"
      },
      {
        "id": "c074198c-0ba3-4756-8dd2-e14dd86518bc",
        "metadata": {
          "chunk_id": 39,
          "document_id": "2004.01980",
          "start_idx": 13028,
          "end_idx": 13433
        },
        "page_content": " romantic, and click-baity text. The average sentence length in the NYT, CNN, Humor, Romance, and Clickbait datasets are 8.8, 9.2, 12.6, 11.6 and 8.7 words, respectively. The source dataset contains news articles paired with corresponding headlines. To enrich the training corpus, we combine two datasets: the New York Times (56K) and CNN (90K). After combining these two datasets, we randomly selected 3,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is a string kernel?",
    "llm_answer": "String kernels measure the similarity of strings through character n-grams.\n",
    "context": [
      {
        "id": "72e1273f-a96c-4d9f-a88b-0a7dcc02c63c",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1811.01734",
          "start_idx": 8198,
          "end_idx": 8655
        },
        "page_content": " on string kernels have demonstrated remarkable performance in various text classification tasks BIBREF35 , BIBREF36 , BIBREF22 , BIBREF19 , BIBREF10 , BIBREF17 , BIBREF26 . String kernels represent a way of using information at the character level by measuring the similarity of strings through character n-grams. Lodhi et al. BIBREF35 used string kernels for document categorization, obtaining very good results. String kernels were also successfully used",
        "type": "Document"
      },
      {
        "id": "cef1d9cd-0097-4c2d-bd33-9a8ab26c7b64",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1811.01734",
          "start_idx": 8977,
          "end_idx": 9385
        },
        "page_content": ", obtaining impressive results. For instance, Ionescu et al. BIBREF19 have employed string kernels in a cross-corpus (and implicitly cross-domain) native language identification experiment, improving the state-of-the-art accuracy by a remarkable INLINEFORM0 . Gim\u00e9nez-P\u00e9rez et al. BIBREF10 have used string kernels for single-source and multi-source polarity classification. Remarkably, they obtain state-of-",
        "type": "Document"
      },
      {
        "id": "bf743586-d8db-4cd9-9015-3bb0977ef253",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1811.01734",
          "start_idx": 8655,
          "end_idx": 9183
        },
        "page_content": "35 used string kernels for document categorization, obtaining very good results. String kernels were also successfully used in authorship identification BIBREF22 . More recently, various combinations of string kernels reached state-of-the-art accuracy rates in native language identification BIBREF19 and Arabic dialect identification BIBREF17 . Interestingly, string kernels have been used in cross-domain settings without any domain adaptation, obtaining impressive results. For instance, Ionescu et al. BIBREF19 have employed",
        "type": "Document"
      },
      {
        "id": "31c6f37d-e4b5-4f34-9d6c-2da0d6b0c36f",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1811.01734",
          "start_idx": 10505,
          "end_idx": 10956
        },
        "page_content": " between text samples, simply based on character n-grams. Various string kernel functions have been proposed to date BIBREF35 , BIBREF38 , BIBREF19 . Perhaps one of the most recently introduced string kernels is the histogram intersection string kernel BIBREF19 . For two strings over an alphabet INLINEFORM0 , INLINEFORM1 , the intersection string kernel is formally defined as follows: DISPLAYFORM0  where INLINEFORM0 is the number of occurrences of",
        "type": "Document"
      },
      {
        "id": "3907c6e0-7d22-4e62-aad5-cefda905a708",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1811.01734",
          "start_idx": 10173,
          "end_idx": 10701
        },
        "page_content": " , BIBREF17 , we use unlabeled data from the target domain to significantly increase the performance of string kernels in cross-domain text classification, particularly in English polarity classification.   String kernels. Kernel functions BIBREF38 capture the intuitive notion of similarity between objects in a specific domain. For example, in text mining, string kernels can be used to measure the pairwise similarity between text samples, simply based on character n-grams. Various string kernel functions have been proposed",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they correlate NED with emotional bond levels?",
    "llm_answer": "The patient-to-therapist NED is negatively correlated with emotional bond, while therapist-to-patient NED does not have a significant correlation.  A t-SNE transformation of the difference of all patient-to-therapist turn embedding pairs is also used to visualize sessions with different emotional bond levels.\n",
    "context": [
      {
        "id": "0a606237-09d4-4fc8-a3ec-029768dd4323",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1804.08782",
          "start_idx": 15808,
          "end_idx": 16341
        },
        "page_content": " bond and the candidate measure. Results in Table TABREF26 show that the patient-to-therapist NED is negatively correlated with emotional bond with high statistical significance ( INLINEFORM0 ). This negative sign is consistent with previous studies as higher distance in acoustic features indicates lower entrainment. However, the therapist-to-patient NED does not have a significant correlation with emotional bond. A possible explanation for this finding is that the emotional bond is reported by the patient and influenced by the",
        "type": "Document"
      },
      {
        "id": "452a43c2-1a4c-43cb-b347-8030130f8426",
        "metadata": {
          "chunk_id": 40,
          "document_id": "1804.08782",
          "start_idx": 15019,
          "end_idx": 15513
        },
        "page_content": " domain theory BIBREF16 and from experimental validation BIBREF6 , a high emotional bond in patient-therapist interactions in the suicide therapy domain is associated with more entrainment. In this experiment, we compute the correlation of the proposed NED measure with the patient-perceived emotional bond ratings. Since the proposed measure is asymmetric in nature, we compute the measures for both patient-to-therapist and therapist-to-patient entrainment. We also compute the correlation of",
        "type": "Document"
      },
      {
        "id": "0d630600-fbef-431d-8838-b27baa8a6487",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1804.08782",
          "start_idx": 16341,
          "end_idx": 16857
        },
        "page_content": " A possible explanation for this finding is that the emotional bond is reported by the patient and influenced by the degree of their perceived therapist-entrainment. Thus, equipped with an asymmetric measure, we are also able to identify the latent directionality of the emotional bond metric. The complexity measure (Baseline 2) also shows statistically significant correlation, but the value of INLINEFORM1 is lower than that of the proposed measure. To analyze the embeddings encoded by our model, we also compute",
        "type": "Document"
      },
      {
        "id": "8bc63de8-e650-4262-a82b-ba84a201e99b",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1804.08782",
          "start_idx": 16647,
          "end_idx": 17078
        },
        "page_content": " that of the proposed measure. To analyze the embeddings encoded by our model, we also compute a t-SNE BIBREF20 transformation of the difference of all patient-to-therapist turn embedding pairs, denoted as INLINEFORM0 in Equation (3). Figure FIGREF27 shows the results of a session with high emotional bond and another one with low emotional bond (with values of 7 and 1 respectively) as a 2-dimensional scatter plot. Visibly there",
        "type": "Document"
      },
      {
        "id": "5b9a8247-7c57-4629-a9ee-2a4423b96ee7",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1804.08782",
          "start_idx": 17078,
          "end_idx": 17571
        },
        "page_content": " (with values of 7 and 1 respectively) as a 2-dimensional scatter plot. Visibly there is some separation between the sessions with low and high emotional bond. In this work, a novel deep neural network-based Neural Entrainment Distance (NED) measure is proposed for capturing entrainment in conversational speech. The neural network architecture consisting of an encoder and a decoder is trained on the Fisher corpus in an unsupervised training framework and then the measure is defined on the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was their F1 score on the Bengali NER corpus?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "3b78146b-5d9c-4042-a709-e3a6fa626287",
        "metadata": {
          "chunk_id": 107,
          "document_id": "1701.09123",
          "start_idx": 40331,
          "end_idx": 40729
        },
        "page_content": " in Table TABREF71 show that our nl-cluster model outperforms the best result published on CoNLL 2002 BIBREF45 by 3.83 points in F1 score. Adding the English Illinois NER gazetteers BIBREF31 and trigram and character n-gram features increases the score to 85.04 F1, 5.41 points better than previous published work on this dataset. We also compared our system with the more recently developed SONAR-",
        "type": "Document"
      },
      {
        "id": "2c310dfc-03e8-4750-877c-abc8ee3b9b02",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1911.03894",
          "start_idx": 13533,
          "end_idx": 14005
        },
        "page_content": ", and F1 score. Here precision measures the percentage of entities found by the system that are correctly tagged, recall measures the percentage of named entities present in the corpus that are found and the F1 score combines both precision and recall measures giving a general idea of a model's performance. Most of the advances in NER haven been achieved on English, particularly focusing on the CoNLL 2003 BIBREF40 and the Ontonotes v5 BIBREF41, BIBREF42 English corpor",
        "type": "Document"
      },
      {
        "id": "6e88f193-653f-4ba8-8d1d-4192af66023b",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1701.02877",
          "start_idx": 3620,
          "end_idx": 4066
        },
        "page_content": " primarily by F1 score), and also to study the relationship between OOV NEs and features and F1. See Section \"Experiments\" for details. To ensure representativeness and comprehensiveness, our experimental findings are based on key benchmark NER corpora spanning multiple genres, time periods, and corpus annotation methodologies and guidelines. As detailed in Section \"Datasets\" , the corpora studied are OntoNotes BIBREF29 , ACE BIBREF30 , MUC 7",
        "type": "Document"
      },
      {
        "id": "a8b23f92-6b45-4b0c-954f-cde7f32bef74",
        "metadata": {
          "chunk_id": 108,
          "document_id": "1701.09123",
          "start_idx": 40552,
          "end_idx": 40973
        },
        "page_content": " published work on this dataset. We also compared our system with the more recently developed SONAR-1 corpus and the companion NERD system distributed inside its release BIBREF33 . They report 84.91 F1 for the six main named entity types via 10-fold cross validation. For this comparison we chose the local, nl-cluster and nl-cluster-dict configurations from Table TABREF71 and run them on SONAR-1 using the same settings",
        "type": "Document"
      },
      {
        "id": "68c9fa82-7b71-4d3b-8d4e-70ded7bf95e4",
        "metadata": {
          "chunk_id": 21,
          "document_id": "1701.02877",
          "start_idx": 7791,
          "end_idx": 8297
        },
        "page_content": " studying NER performance on this genre gained momentum. So far, there have been no big evaluation efforts, such as ACE and OntoNotes, resulting in substantial amounts of gold standard data. Instead, benchmark corpora were created as part of smaller challenges or individual projects. The first such corpus is the UMBC corpus for Twitter NER BIBREF33 , where researchers used crowdsourcing to obtain annotations for persons, locations and organisations. A further Twitter NER corpus was created by BIBREF21",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the size of the dataset?",
    "llm_answer": "The CNNDM dataset has 287,226 training documents, 13,368 validation documents, and 11,490 test documents.  The NYT50 dataset is derived from the CNNDM dataset.  The SCI dataset contains 220 sentences.  The STS Corpus dataset contains 17M training tokens.\n",
    "context": [
      {
        "id": "2d3bdfec-4ed5-456d-bd82-023fad798858",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "9ccb7945-9a96-417a-b52e-68855c4910b1",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "6b7f6ca6-24d5-4144-9cfd-92be2de31f39",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "95bf2b84-1a56-4eed-b193-4a34d2c76c4c",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1706.08032",
          "start_idx": 14264,
          "end_idx": 14661
        },
        "page_content": " datasets in Table III. for all datasets, the filter window size ( INLINEFORM0 ) is 7 with 6 feature maps each for the first wide convolution layer, the second wide convolution layer has a filter window size of 5 with 14 feature maps each. Dropout rate ( INLINEFORM1 ) is 0.5, INLINEFORM2 constraint, learning rate is 0.1 and momentum of 0.9. Mini-batch size for STS Corpus is 100 and others are 4",
        "type": "Document"
      },
      {
        "id": "6f04ea61-adc5-4abb-9261-abc554ebac1b",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1711.02013",
          "start_idx": 20658,
          "end_idx": 21116
        },
        "page_content": "ings, between recurrent layers, and on recurrent states were (0.7, 0.5, 0.5) respectively. dataset contains 17M training tokens and has a vocabulary size of 44k words. The dataset is partitioned into a training set (first 99M characters) and a development set (last 1M characters) that is used to report performance. As this dataset contains various articles from Wikipedia, the longer term information (such as current topic) plays a bigger role than in the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many examples do they have in the target domain?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "1b77f6bf-fa46-4a95-a3a5-e1c39a211c21",
        "metadata": {
          "chunk_id": 36,
          "document_id": "2002.01359",
          "start_idx": 15270,
          "end_idx": 15785
        },
        "page_content": " a significant number of unseen domains/APIs in the dev and test sets. 77% of the dialogue turns in the test set and 45% of the turns in dev set contain at least one service not present in the training set. This facilitates the development of models which can generalize to new domains with very few labelled examples. The submissions from 25 teams included a variety of approaches and innovative solutions to specific problems posed by this dataset. For the workshop, we received submissions from 9 of these teams.",
        "type": "Document"
      },
      {
        "id": "d05c656a-a8c8-4f95-aa65-4b62374d4936",
        "metadata": {
          "chunk_id": 53,
          "document_id": "2002.11893",
          "start_idx": 19982,
          "end_idx": 20379
        },
        "page_content": "sim $5) in HAR domains with cross-domain informable slots. 572 dialogues have multiple sub-goals in HAR domains with cross-domain informable slots and at least one sub-goal in the metro or taxi domain (3$\\sim $5 sub-goals). The data statistics are shown in Table TABREF26. As mentioned in Section SECREF14, we generate independent multi-domain, cross multi-domain, and traffic domain sub-goals one",
        "type": "Document"
      },
      {
        "id": "6c41d32d-f3c4-455a-b3f7-a8d9e949b31b",
        "metadata": {
          "chunk_id": 51,
          "document_id": "2002.11893",
          "start_idx": 19391,
          "end_idx": 19796
        },
        "page_content": " (13.7). These statistics indicate that our dialogue data are more complex. According to the type of user goal, we group the dialogues in the training set into five categories: 417 dialogues have only one sub-goal in HAR domains. 1573 dialogues have multiple sub-goals (2$\\sim $3) in HAR domains. However, these sub-goals do not have cross-domain informable slots. 691 dialogues have multiple sub-goals in",
        "type": "Document"
      },
      {
        "id": "62bfcbfc-b724-4fa4-aa44-462ae5737962",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1910.00825",
          "start_idx": 11594,
          "end_idx": 12020
        },
        "page_content": "406 of them are single-domain (8.93 turns on average) and 7,302 are multi-domain (15.39 turns on average). During MultiWOZ data collection, instruction is provided for crowd workers to perform the task. We use the instructions as the dialog summary, and an example data is shown in Table TABREF25. Dialog domain label is extracted from existing MultiWOZ annotation. In the experiment, we split the dataset into 8,438 training,",
        "type": "Document"
      },
      {
        "id": "321cf16c-ea15-4275-9a89-2cd008eba8e1",
        "metadata": {
          "chunk_id": 85,
          "document_id": "1709.05413",
          "start_idx": 33585,
          "end_idx": 34151
        },
        "page_content": " insurance domain. Our findings show that our model is robust to different domains as our test set size increases, and that our more generic, company-independent experiment gives us better performance than any domain-specific experiments. Given our observation that Agent turns are more predictable, and that we achieve best performance in a company-independent setting, we question whether the training that agents receive is actually reliable in terms of resulting in overall \"satisfied customers\", regardless of company domain. Ultimately, our goal is to discover",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the baseline model?",
    "llm_answer": "Model 1\n",
    "context": [
      {
        "id": "4d522ea5-86b3-42df-acaa-37f028f2bec5",
        "metadata": {
          "chunk_id": 138,
          "document_id": "2001.11381",
          "start_idx": 31860,
          "end_idx": 32196
        },
        "page_content": "ados. Dicho lo anterior, el Modelo 1 podr\u00eda ser considerado como nuestro propio baseline. A continuaci\u00f3n presentamos un protocolo de evaluaci\u00f3n manual de los resultados obtenidos. El experimento consisti\u00f3 en la generaci\u00f3n de 15 frases por cada uno de los tres modelos propuestos. Para cada modelo, se consideraron tres queries: $Q=$ {AM",
        "type": "Document"
      },
      {
        "id": "94c43990-47e3-478c-a415-088eeac582b5",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1804.08782",
          "start_idx": 13990,
          "end_idx": 14427
        },
        "page_content": " number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 .  For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose",
        "type": "Document"
      },
      {
        "id": "b7568416-2511-4e7d-b42a-905e8dea8f55",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1909.09484",
          "start_idx": 19263,
          "end_idx": 19734
        },
        "page_content": " models and baselines, with a learning rate of 0.001 for supervised training and 0.0001 for reinforcement learning. In reinforcement learning, the decay parameter $\\lambda $ is set to 0.8. The weight decay is set to 0.001. And early stopping is performed on developing set. The experimental results of the proposed model and baselines will be analyzed from the following aspects. BPRA Results: As shown in Table TABREF35, most of the models have similar performance on BP",
        "type": "Document"
      },
      {
        "id": "7d4754c7-4ea8-4d20-9fb6-3d1e50bf0cf6",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1809.04960",
          "start_idx": 22497,
          "end_idx": 22960
        },
        "page_content": " the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types of comments as described in the above retrieval evaluation setting: Correct, Plausible, Popular, and Random. We rank the candidate comment set with four models (TF-IDF, S2S, IR, and Proposed+IR), and record the types of top-1 comments. Figure FIGREF40 shows the percentage of different types of top-1 comments generated by each model. It shows",
        "type": "Document"
      },
      {
        "id": "0d211bed-e5df-4233-8d02-e24d7ee9b65c",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1909.02635",
          "start_idx": 9050,
          "end_idx": 9548
        },
        "page_content": " (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Where did they get training data?",
    "llm_answer": "From a corpus of approximately 300,000 sentences (yielding 1.5 million single-quiz question training examples), and from the organizers (2,250 users, 225,000 tweets).  Also, from anonymized user interaction data, real quizzes generated for a collection of several input video sources.  And from commercially available data, artificially inflated data, and labeled/self-labeled anonymized recognition requests.\n",
    "context": [
      {
        "id": "9d6e4c30-d037-4a07-a951-0e7d1761e751",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1912.13109",
          "start_idx": 7657,
          "end_idx": 8136
        },
        "page_content": " above few data augmentation techniques were applied to boost the learning of the network. Before applying augmentation, a train-test split of 78%-22% was done from the original, cleansed data set. Thus, 700 tweets/messages were held out for testing. All model evaluation were done in on the test set that got generated by this process. The results presented in this report are based on the performance of the model on the test set. The training set of 2489 messages were however",
        "type": "Document"
      },
      {
        "id": "da4820e5-5f2a-4d72-959e-5a44268650fa",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1806.04524",
          "start_idx": 9624,
          "end_idx": 10136
        },
        "page_content": " the usage of other pre-trained vectors for future work. Using our platform, we extracted anonymized user interaction data in the manner of real quizzes generated for a collection of several input video sources. We obtained a corpus of approximately 300,000 sentences, from which roughly 1.5 million single-quiz question training examples were derived. We split this dataset using the regular 70/10/20 partition for training, validation and testing. As the system required the input sentences to be tokenized and",
        "type": "Document"
      },
      {
        "id": "1a6adc72-1767-4e4a-be77-138554d2b622",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1909.00542",
          "start_idx": 734,
          "end_idx": 1323
        },
        "page_content": " the training procedure was based on a regression set up, this year we experiment with various classification set ups. The main contributions of this paper are: We compare classification and regression approaches and show that classification produces better results than regression but the quality of the results depends on the approach followed to annotate the data labels. We conduct correlation analysis between various ROUGE evaluation metrics and the human evaluations conducted at BioASQ and show that Precision and F1 correlate better than Recall. Section SECREF2 briefly introduces",
        "type": "Document"
      },
      {
        "id": "84d0ea2b-fafd-4fc8-a8b6-ddaeac75b66c",
        "metadata": {
          "chunk_id": 82,
          "document_id": "1902.10525",
          "start_idx": 31058,
          "end_idx": 31608
        },
        "page_content": " commercially available data, artificially inflated data, and labeled/self-labeled anonymized recognition requests (see BIBREF14 for a more detailed description). The number of training samples varies from tens of thousands to several million per script, depending on the complexity and usage. The best configuration for our production systems were identified by running multiple experiments over a range of layer depths and widths on our Latin script datasets. For the Latin script experiments shown in Figure FIGREF63 , the training set we used was",
        "type": "Document"
      },
      {
        "id": "555f1347-5c14-4c7a-96bb-19cbae71d588",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1909.04181",
          "start_idx": 1596,
          "end_idx": 2096
        },
        "page_content": " were expected to submit their predictions on test. The shared task predictions are expected by organizers at the level of users. The distribution has 100 tweets for each user, and so each tweet is distributed with a corresponding user id. As such, in total, the distributed training data has 2,250 users, contributing a total of 225,000 tweets. The official task test set contains 720,00 tweets posted by 720 users. For our experiments, we split the training data released by organizers into 90% TRA",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What extraction model did they use?",
    "llm_answer": "Sequence-to-sequence model.\n",
    "context": [
      {
        "id": "39e72bb4-4c29-4ebe-9490-d2a85f66336c",
        "metadata": {
          "chunk_id": 90,
          "document_id": "1706.04815",
          "start_idx": 25770,
          "end_idx": 26320
        },
        "page_content": " The extraction model aims to match the question and passage and predict most important sub-spans in the passage related to the question as evidence. Then, the synthesis model synthesizes the question information and the evidence snippet to generate the final answer. We propose a multi-task learning framework to improve the evidence extraction model by passage ranking to extract the evidence snippet, and use the sequence-to-sequence model for answer synthesis. We conduct experiments on the MS-MARCO dataset. Results demonstrate that our approach",
        "type": "Document"
      },
      {
        "id": "417d15b2-39b6-476f-b647-e24df39a2453",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1706.04815",
          "start_idx": 12225,
          "end_idx": 12537
        },
        "page_content": "_i=0$ . The evident extraction part is trained by minimizing joint objective functions:  $$\\mathcal {L}_{E} = r \\mathcal {L}_{AP} + (1-r) \\mathcal {L}_{PR}$$   (Eq. 25)  where $r$ is the hyper-parameter for weights of two loss functions. As shown in Figure 3 , we use the sequence-to-sequence model to synthesize",
        "type": "Document"
      },
      {
        "id": "a0803eb9-04f9-4b4b-831e-91f69a9077ae",
        "metadata": {
          "chunk_id": 62,
          "document_id": "1706.04815",
          "start_idx": 15638,
          "end_idx": 16084
        },
        "page_content": "  (Eq. 32)  where $\\mathcal {D}$ is the set of data. $X$ represents the question and passage including evidence snippets, and $Y$ represents the answer. We conduct our experiments on the MS-MARCO dataset BIBREF1 . We compare our extraction-then-synthesis framework with pure extraction model and other baseline methods on the leaderboard of MS-MARCO. Experimental results show that our model achieves better results in official evaluation metrics",
        "type": "Document"
      },
      {
        "id": "496578d3-34f5-461b-997c-d9dd61b8a88f",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1908.06606",
          "start_idx": 5338,
          "end_idx": 5846
        },
        "page_content": "REF4 identified patients undergoing radical cystectomy for bladder cancer. Although they achieved good performance, none of their models could be used to another task due to output format difference. This makes building a new model for a new task a costly job. Pipeline methods BIBREF7, BIBREF8, BIBREF9 break down the entire task into several basic natural language processing tasks. Bill et al. BIBREF7 focused on attributes extraction which mainly relied on dependency parsing and named entity recognition",
        "type": "Document"
      },
      {
        "id": "8c67d4ea-119a-4d9e-a226-914122196f6b",
        "metadata": {
          "chunk_id": 49,
          "document_id": "2001.06888",
          "start_idx": 18183,
          "end_idx": 18669
        },
        "page_content": "ding. The result of this phase is extracted features for each word. Image Feature Extraction is shown in the right part of figure FIGREF11. For this part, we have used InceptionV3 pretrained on ImageNet BIBREF60. Many models were available as first part of image feature extraction, however the main reason we used InceptionV3 as feature extractor backbone is better performance of it on ImageNet and the results obtained by this particular model were slightly better compared to others",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which datasets did they experiment on?",
    "llm_answer": "Action Adult Adventure Animation Biography Comedy Crime Documentary Drama Family Fantasy Film-Noir Game-Show History Horror Music Musical Mystery Reality-TV Romance Sci-Fi Short Sport Talk-Show Thriller War Western, Biomedical, novel-9\n",
    "context": [
      {
        "id": "3b942450-5125-4b16-8163-c4e7540353a0",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1802.05322",
          "start_idx": 13905,
          "end_idx": 14341
        },
        "page_content": " explanation of the whole process needed to conduct an experiment like this. The process includes downloading a data set, web scraping for extra information, data preprocessing, model tuning and evaluation of the results. Action Adult Adventure Animation Biography Comedy Crime Documentary Drama Family Fantasy Film-Noir Game-Show History Horror Music Musical Mystery Reality-TV Romance Sci-Fi Short Sport Talk-Show Thriller War Western",
        "type": "Document"
      },
      {
        "id": "bb75bdfe-b8c1-4c6c-959c-d7910baa1d3e",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1607.06025",
          "start_idx": 24769,
          "end_idx": 25338
        },
        "page_content": " were constructed by filtering the generated datasets according to various thresholds. Thus, the generated datasets were constructed to contain enough examples, so that the filtered datasets had at least the number of examples as the original dataset. In the end, all the datasets were trimmed down to the size of the original dataset by selecting the samples sequentially from the beginning until the dataset had the right size. Also, the datasets were filtered so that each of the labels was represented equally. All the models, including classification and discrimin",
        "type": "Document"
      },
      {
        "id": "65333987-2358-483b-95cd-3ff028ff6a96",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      },
      {
        "id": "64a25737-469b-4e82-875c-ba81cb2e5018",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1701.00185",
          "start_idx": 3790,
          "end_idx": 4304
        },
        "page_content": " this paper we use a new short text dataset, Biomedical, in the experiment to verify the effectiveness of our approach. Third, we put much effort on studying the influence of various different semantic features integrated in our self-taught CNN framework, which is not involved in the conference paper. For the purpose of reproducibility, we make the datasets and software used in our experiments publicly available at the website. The remainder of this paper is organized as follows: In Section SECREF2 , we first",
        "type": "Document"
      },
      {
        "id": "1f18073b-4721-4651-900e-6b076387c7f1",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1709.02271",
          "start_idx": 11255,
          "end_idx": 11734
        },
        "page_content": " We begin by introducing the datasets (Section SECREF15 ), followed by detailing the featurization methods (Section SECREF17 ), the experiments (Section SECREF22 ), and finally reporting results (Section SECREF26 ). The statistics for the three datasets used in the experiments are summarized in Table TABREF16 . novel-9. This dataset was compiled by F&H14: a collection of 19 novels by 9 nineteenth century British and American authors in the Project Gutenberg. To compare to F&",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do slot binary classifiers improve performance?",
    "llm_answer": "Slot binary classifiers resolve issues of TSCP by performing binary classification for each slot, eliminating undesired dependencies among slots.\n",
    "context": [
      {
        "id": "0d8487f9-8540-44f6-b01a-35a231e31cd8",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1908.02402",
          "start_idx": 13555,
          "end_idx": 14034
        },
        "page_content": " to perform binary classification for each slot. This greatly resolves the issues of TSCP that uses a single decoder with each step having unconstrained vocabulary-size choices, which may potentially lead to generating non-slot words. Similar to the informable slots decoders, such a separate classifier also eliminates the undesired dependencies among slots. Let $\\lbrace k^R\\rbrace $ denote the set of requestable slots. A single GRU cell is used to perform the classification.",
        "type": "Document"
      },
      {
        "id": "b4ca793b-85a9-4830-9b92-e40e11cdd6bf",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1908.02402",
          "start_idx": 14849,
          "end_idx": 15064
        },
        "page_content": ". 12)  The loss function for all requestable slot binary classifiers is:  $$\\small \\begin{split}\n\\mathcal {L}^R =& - \\frac{1}{|\\lbrace k^R\\rbrace |} \\sum _{k^R} \\\\\n&z^{k^R} \\log (y^{k^R}) + (1-z^{k^R}) \\log (1-y^{k^",
        "type": "Document"
      },
      {
        "id": "f9a72084-04ab-44b8-874a-e2b7dbbe3f6a",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1909.11833",
          "start_idx": 5531,
          "end_idx": 6057
        },
        "page_content": "s\\in S$. This can be cumbersome when the ontology is large, and it suffers from the insufficient data problem: the labelled data for a single slot may not suffice to effectively train the parameters for the slot-specific neural networks structure. Therefore, we propose that in the classification process, the model needs to rely on the semantic similarity between the user's utterance and slot-value pair, with system action information. In other words, the model should have only a single global neural structure independent",
        "type": "Document"
      },
      {
        "id": "ef4b5e82-72ba-4865-9ce3-449e26bae7e1",
        "metadata": {
          "chunk_id": 56,
          "document_id": "2002.01359",
          "start_idx": 23358,
          "end_idx": 23874
        },
        "page_content": " slots is comparable to that on non-categorical slots. On the other hand, for seen services, the performance on categorical slots is better. This could be because there is less signal to differentiate between the different possible values for a categorical slot when they have not been observed in the training set. The winning team's performance on seen services is similar to that of the other top teams. However, the winning team has a considerable edge on unseen services, outperforming the second team by around",
        "type": "Document"
      },
      {
        "id": "b79fcb59-237a-4aeb-856b-fb59b7c5de36",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1908.02402",
          "start_idx": 11084,
          "end_idx": 11518
        },
        "page_content": " slot. The informable slot value decoder consists of GRU recurrent layers with a copy mechanism as shown in the yellow section of Figure 1 . It is composed of weight-tied GRU generators that take the same initial hidden state $h^{E}_{l}$ , but have different start-of-sentence symbols for each unique informable slot. This way, each informable slot value decoder is dependent on the encoder's output, but it is also independent of the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What baselines have been used in this work?",
    "llm_answer": "Baseline 1, Baseline 2, Baseline 3, ANRES_Sing_R, ANRES_Sing_Z, ANRES_Seq_R, ANRES_Seq_Z, and a LSTM baseline.\n",
    "context": [
      {
        "id": "94c43990-47e3-478c-a415-088eeac582b5",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1804.08782",
          "start_idx": 13990,
          "end_idx": 14427
        },
        "page_content": " number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 .  For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose",
        "type": "Document"
      },
      {
        "id": "f297d78b-0fdd-4eee-ae9e-9e75a2e66130",
        "metadata": {
          "chunk_id": 140,
          "document_id": "2001.02380",
          "start_idx": 51470,
          "end_idx": 51956
        },
        "page_content": " baselines. The left, middle and right panels in Figure FIGREF40 correspond to measurements when all signals are included, only cases contained entirely in the head EDUs shown to the model, and only DMs, respectively. The scenario on the left is rather unreasonable and is included only for completeness: here the model is also penalized for not detecting signals such as lexical chains, part of which is outside the units that the model is being shown. An example of such a case can be",
        "type": "Document"
      },
      {
        "id": "3747343d-0517-442a-b7ba-66d01be55b6d",
        "metadata": {
          "chunk_id": 40,
          "document_id": "2004.01878",
          "start_idx": 14526,
          "end_idx": 14915
        },
        "page_content": " normal distribution $\\mathcal {N}(0, 1)$. In summary, the following four baselines are designed: ANRES_Sing_R: randomly initializing the states for each single trading day. ANRES_Sing_Z: initializing the states as zeros for each single trading day. ANRES_Seq_R: randomly initializing the first states for each trading sequence only. ANRES_Seq_Z: initializing the first states as zeros for",
        "type": "Document"
      },
      {
        "id": "0d211bed-e5df-4233-8d02-e24d7ee9b65c",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1909.02635",
          "start_idx": 9050,
          "end_idx": 9548
        },
        "page_content": " (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo",
        "type": "Document"
      },
      {
        "id": "c9d62915-21d1-42b6-a16c-8d9d28de6e0c",
        "metadata": {
          "chunk_id": 91,
          "document_id": "1902.06843",
          "start_idx": 35139,
          "end_idx": 35416
        },
        "page_content": "s by -0.27. Baselines: To test the efficacy of our multi-modal framework for detecting depressed users, we compare it against existing content, content-network, and image-based models (based on the aforementioned general image feature, facial presence, and facial expressions.)",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what bottlenecks were identified?",
    "llm_answer": "Recognition bottlenecks in a menu-based speech solution.\n",
    "context": [
      {
        "id": "661a0fe4-2e81-4fac-b061-5c48957ec61a",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1601.02543",
          "start_idx": 12144,
          "end_idx": 12571
        },
        "page_content": " actually putting it to people test. There is a significant saving in terms of being able to identify recognition bottlenecks in a menu based speech solution through this analysis because it does not require actual people testing the system. This methodology was adopted to restructuring the set of active words at each node for better speech recognition in an actual menu based speech recognition system that caters to masses.",
        "type": "Document"
      },
      {
        "id": "fa81a08a-bacb-4828-b187-23561c888037",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1802.05574",
          "start_idx": 10995,
          "end_idx": 11490
        },
        "page_content": "12 . We used the time estimates from our internal annotation to price the task in order to be above US minimum wage. All workers were qualified before being issued tasks. Overall, we employed 10 crowd workers. On average it took 30 minutes for a worker to complete a HIT. In line with BIBREF13 , we monitored for potential non-performance or spam by looking for long response times and consecutive submitted results. We saw no indicators of low quality responses. In total, 11262 judgements were",
        "type": "Document"
      },
      {
        "id": "1e2f925a-9a0d-4318-af38-ee9ae9d2a426",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1912.13109",
          "start_idx": 7921,
          "end_idx": 8471
        },
        "page_content": " on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we",
        "type": "Document"
      },
      {
        "id": "02909708-446e-4390-9c4d-806003609f5c",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1611.00514",
          "start_idx": 17234,
          "end_idx": 17771
        },
        "page_content": " other portion to avoid any over-fitting, particularly important for the domain adaptation technique. This resulted in a relative improvement of 11% compared to the primary system in terms of the primary metric. However, the results can be misleading, since the recording condition may be the same for all speakers in the development set. This section reports on the CPU execution time (single threaded), and the amount of memory used to process a single trial, which includes the time for creating models from the enrolment data and the",
        "type": "Document"
      },
      {
        "id": "8005ecd6-6854-40ee-8c5b-c296aeb8fee3",
        "metadata": {
          "chunk_id": 141,
          "document_id": "1701.02877",
          "start_idx": 50827,
          "end_idx": 51373
        },
        "page_content": " The other genre that was particularly difficult for the systems was ACE Usenet. This is a form of user-generated content, not intended for publication but rather discussion among communities. In this sense, it is social media, and so it is not surprising that system performance on ACE UN resembles performance on social media more than other genres. Crucially, the computationally-cheap memorisation method actually acts as a reasonable predictor of the performance of other methods. This suggests that high entity diversity predicts difficulty",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much do they outperform BiLSTMs in Sentiment Analysis?",
    "llm_answer": "The context states the model outperforms *all* strong baselines, including BiLSTMs, but does not give a specific percentage improvement over BiLSTMs.  The answer is not found in the context.\n",
    "context": [
      {
        "id": "5c91c951-d82f-42ab-9267-8f27abf945b0",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1911.02711",
          "start_idx": 3209,
          "end_idx": 3722
        },
        "page_content": " results show that our model significantly outperforms all strong baselines, including joint modeling, separate encoder and joint encoder methods. In addition, our model achieves new state-of-the-art performance, attaining 2.1% (with generated summary) and 4.8% (with golden summary) absolutely improvements compared to the previous best method on SNAP Amazon review benchmark. The majority of recent sentiment analysis models are based on either convolutional or recurrent neural networks to encode sequences BIB",
        "type": "Document"
      },
      {
        "id": "590e33d8-9d72-45d5-be85-36f3e4f04c0b",
        "metadata": {
          "chunk_id": 62,
          "document_id": "1710.01492",
          "start_idx": 22676,
          "end_idx": 23087
        },
        "page_content": "REF30 , BIBREF26 , BIBREF29 , BIBREF27 . Using such large-scale lexicons was crucial for the performance of the top-ranked systems. Similar observations were made in the related Aspect-Based Sentiment Analysis task at SemEval 2014 BIBREF21 . In both tasks, the winning systems benefitted from building and using massive sentiment polarity lexicons BIBREF25 , BIBREF62 . The two most popular large-scale lexicons",
        "type": "Document"
      },
      {
        "id": "569eca1a-7926-4e99-bc77-ec52da6dbe0b",
        "metadata": {
          "chunk_id": 76,
          "document_id": "1710.01492",
          "start_idx": 27500,
          "end_idx": 27887
        },
        "page_content": " obtained by starting with mid-sized seeds, i.e., hundreds of words and phrases BIBREF66 . Sentiment analysis on Twitter has applications in a number of areas, including political science BIBREF39 , BIBREF40 , BIBREF41 , economics BIBREF42 , BIBREF7 , social science BIBREF43 , and market research BIBREF44 , BIBREF45 . It is used to study company reputation online BIBREF45 , to measure",
        "type": "Document"
      },
      {
        "id": "a2fe7773-6514-4b45-8035-03d9008cf92f",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1604.05781",
          "start_idx": 20364,
          "end_idx": 20856
        },
        "page_content": " scored text in the causal corpus and 77.2% of the control corpus. The difference in sentiment between corpora was significant for all distributions (t-test: $p < 0.01$ ). Fourth, to further confirm that the causal documents tend toward negative sentiment, we applied a separate, independent sentiment analysis using the Stanford NLP sentiment toolkit BIBREF38 to classify the sentiment of individual documents not unigrams (see Methods). Instead of a numeric sentiment score, this classifier",
        "type": "Document"
      },
      {
        "id": "9f3bc3df-9609-4c6d-b153-90450c4a5800",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1710.01492",
          "start_idx": 4787,
          "end_idx": 5328
        },
        "page_content": ", which is needed to train the high number of parameters of such complex models. Finally, as methods for sentiment analysis mature, more attention is also being paid to linguistic structure and to multi-linguality and cross-linguality. Sentiment analysis emerged as a popular research direction in the early 2000s. Initially, it was regarded as standard document classification into topics such as business, sport, and politics BIBREF10 . However, researchers soon realized that it was quite different from standard document classification B",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what state of the accuracy did they obtain?",
    "llm_answer": "95.4%, 95.0%, 91%, 99%, 82.9%, 88%, 14.6%\n",
    "context": [
      {
        "id": "08a05152-ac14-4303-8381-2d782949b627",
        "metadata": {
          "chunk_id": 96,
          "document_id": "1804.03396",
          "start_idx": 30860,
          "end_idx": 31374
        },
        "page_content": " Consistency. We randomly sample 25 articles respectively from the 6 datasets (in total of 1002 ground truth triples with 2691 labeled answer locations) and let two human annotators label the Triple Accuracy for each ground truth triple and the Contextual Consistency for each answer location. The two human annotators reached an agreement on 131 of 132 randomly selected Triple Accuracy cases and on 229 of 234 randomly selected Contextual Consistency cases. The human evaluation results are shown in Table 8 . We",
        "type": "Document"
      },
      {
        "id": "679e5943-aff3-4667-b5b6-229a58410174",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1911.03243",
          "start_idx": 10852,
          "end_idx": 11313
        },
        "page_content": "Bank. To examine this, we analyzed 100 false positive arguments. Only 32 of those were due to wrong or incomplete QA annotations in our gold, while most others were outside of PropBank's scope, capturing either implied arguments or roles not covered in PropBank. Extrapolating from this manual analysis estimates our true precision (on all roles) to be about 91%, which is consistent with the 88% precision figure in Table TABREF19. Compared with 2015, our QA-S",
        "type": "Document"
      },
      {
        "id": "6cf0a06f-34c6-44b2-a329-2285cf3a8bc8",
        "metadata": {
          "chunk_id": 57,
          "document_id": "1910.04269",
          "start_idx": 21961,
          "end_idx": 22224
        },
        "page_content": " which is a recently developed technique to prevent over\ufb01tting on test data.This approach achieved an accuracy of 95.4%. We also analysed how attention mechanism and recurrent layers impact the performance of networks. This approach achieved an accuracy of 95.0%.",
        "type": "Document"
      },
      {
        "id": "17d66dc1-e87f-4338-8219-d779e45c28d7",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1907.00758",
          "start_idx": 17058,
          "end_idx": 17541
        },
        "page_content": ".9% accuracy with a mean and standard deviation discrepancy of 32 INLINEFORM6 223ms. SyncNet reports INLINEFORM7 99% accuracy on lip video synchronisation using a manual evaluation where the lip error is not detectable to a human observer BIBREF4 . However, we argue that our data is more challenging (Section SECREF4 ). Analysis: We analyse the performance of our model across different conditions. Table shows the model accuracy broken down by utterance type. The model achieves 91",
        "type": "Document"
      },
      {
        "id": "36a77c1d-9383-4cb7-b9fb-bf21a7021719",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1907.00758",
          "start_idx": 16635,
          "end_idx": 17058
        },
        "page_content": " for each utterance. If the discrepancy falls within the minimum detectability range ( INLINEFORM0 125 INLINEFORM1 INLINEFORM2 INLINEFORM3 INLINEFORM4 45) then the prediction is correct. Random prediction (averaged over 1000 runs) yields 14.6% accuracy with a mean and standard deviation discrepancy of 328 INLINEFORM5 518ms. We achieve 82.9% accuracy with a mean and standard deviation discrepancy of 32 INLINEFORM6 223ms.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what models did they compare to?",
    "llm_answer": "NABERT+ baseline, Coref-GRU, MHPGM, Weaver, MHQA-GRN,  other models, Spacy, Stanford NER, Flair.\n",
    "context": [
      {
        "id": "38d0d5d3-4be2-4a08-bc2e-e9f401074e5d",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1701.06538",
          "start_idx": 17346,
          "end_idx": 17826
        },
        "page_content": ". Details can be found in Appendix UID77 . Results of these three models form the bottom line of Figure FIGREF32 -right. Table TABREF33 compares the results of these models to the best previously-published result on this dataset . Even the fastest of these models beats the best published result (when controlling for the number of training epochs), despite requiring only 6% of the computation. We trained our models using TensorFlow BIBREF30 on clusters containing 16-32 Tesla K",
        "type": "Document"
      },
      {
        "id": "60373253-fa83-4f44-bcbd-a972e3cb2654",
        "metadata": {
          "chunk_id": 24,
          "document_id": "2002.11402",
          "start_idx": 8819,
          "end_idx": 9288
        },
        "page_content": "es other models in all scores. Spacy results are good despite not being trained for case-less data. In terms of F1 and overall stability Spacy did better than Stanford NER, on our News Validation set. Similarly, Stanford did well in Precision but could not catch up with Spacy and our model in terms of Recall. Flair overall performed poorly, but as said before these open-source models are not trained for our particular use-case. Lets check some examples for detailed",
        "type": "Document"
      },
      {
        "id": "5e0f3225-070e-4250-8f70-93c083311ec4",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "50dd6a5b-8c24-470a-93be-d9e06f49afc2",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1808.09920",
          "start_idx": 16484,
          "end_idx": 16858
        },
        "page_content": " BIBREF3 and FastQA BIBREF6 . We also compare against Coref-GRU BIBREF12 , MHPGM BIBREF11 , and Weaver BIBREF10 . Additionally, we include results of MHQA-GRN BIBREF23 , from a recent arXiv preprint describing concurrent work. They jointly train graph neural networks and recurrent encoders. We report single runs of our two best single models and an ensemble one on the unm",
        "type": "Document"
      },
      {
        "id": "95031564-64b5-4309-9bee-9ae0ab6bef39",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1810.09774",
          "start_idx": 11025,
          "end_idx": 11459
        },
        "page_content": " , BIBREF2 and BIBREF0 respectively. For BERT we used the uncased 768-dimensional model (BERT-base). For ESIM + ELMo we used the AllenNLP BIBREF23 PyTorch implementation with the default settings and hyperparameter values. Table 4 contains all the experimental results. Our experiments show that, while all of the six models perform well when the test set is drawn from the same corpus as the training and development set, accuracy is",
        "type": "Document"
      }
    ]
  },
  {
    "query": "which benchmark tasks did they experiment on?",
    "llm_answer": "Molecule generation, NER (Named Entity Recognition).\n",
    "context": [
      {
        "id": "99824268-9771-44c0-ac96-a9bc0d4da913",
        "metadata": {
          "chunk_id": 123,
          "document_id": "2002.06053",
          "start_idx": 45532,
          "end_idx": 46097
        },
        "page_content": " datasets and different evaluation metrics makes the assessment of model performance challenging. Comprehensive benchmarking platforms that can assess the success of different tools are still lacking. A benchmarking environment rigorously brings together the suitable data sets and evaluation methodologies in order to provide a fair comparison between the available tools. Such environments are available for molecule generation task from MOSES BIBREF123 and GuacaMol BIBREF124. MoleculeNet is also a similar attempt to build a benchmarking platform for tasks such",
        "type": "Document"
      },
      {
        "id": "4fbc1f70-0043-4b32-98ef-39c7840fa73d",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1611.00514",
          "start_idx": 17771,
          "end_idx": 18207
        },
        "page_content": " to process a single trial, which includes the time for creating models from the enrolment data and the time needed for processing the test segments. The analysis was performed on an Intel(R) Xeon(R) CPU E5-2670 2.60GHz. The results are shown in Table TABREF27 . We used the time command in Unix to report these results. The user time is the actual CPU time used in executing the process (single thread). The real time is the wall clock",
        "type": "Document"
      },
      {
        "id": "1e2f925a-9a0d-4318-af38-ee9ae9d2a426",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1912.13109",
          "start_idx": 7921,
          "end_idx": 8471
        },
        "page_content": " on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we",
        "type": "Document"
      },
      {
        "id": "6197a8b7-7431-4ede-9bdd-77c8047938d2",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1910.11769",
          "start_idx": 7264,
          "end_idx": 7771
        },
        "page_content": "-processed the data by using the SpaCy pipeline. We masked out named entities with entity-type specific placeholders to reduce the chance of benchmark models utilizing named entities as a basis for classification. Benchmark results are shown in Table TABREF17. The dataset is approximately balanced after discarding the Surprise and Disgust classes. We report the average micro-F1 scores, with 5-fold cross validation for each technique. We provide a brief overview of each benchmark experiment below. Among",
        "type": "Document"
      },
      {
        "id": "637d4fe3-c7c1-4e7c-a855-261351957759",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1701.02877",
          "start_idx": 4315,
          "end_idx": 4755
        },
        "page_content": " NER approaches, namely Stanford NER BIBREF34 , SENNA BIBREF35 and CRFSuite BIBREF36 (see Section \"NER Models and Features\" for details). Since the goal of this study is to compare NER performance on corpora from diverse domains and genres, seven benchmark NER corpora are included, spanning newswire, broadcast conversation, Web content, and social media (see Table 1 for details). These datasets were chosen such that they have been annot",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the proficiency score calculated?",
    "llm_answer": "Each indicator is scored 0, 1, or 2 (bad, medium, good).  The total score is the sum of the scores for the 6 indicators.\n",
    "context": [
      {
        "id": "d75bd4b1-57f4-42b8-a8d4-40f640e5b157",
        "metadata": {
          "chunk_id": 1,
          "document_id": "2001.08051",
          "start_idx": 486,
          "end_idx": 928
        },
        "page_content": " experts in terms of some predefined \u201cindicators\u201d which, in turn, were used to assign the proficiency level to each student undertaking the assigned test. This level is expressed according to the well-known Common European Framework of Reference for Languages (Council of Europe, 2001) scale. The CEFR defines 6 levels of proficiency: A1 (beginner), A2, B1, B2, C1 and C2. The levels considered in the evaluation campaigns where the data have",
        "type": "Document"
      },
      {
        "id": "88dd1f0b-af21-49ce-a734-ea5c34d00a1d",
        "metadata": {
          "chunk_id": 22,
          "document_id": "2001.08051",
          "start_idx": 8701,
          "end_idx": 9229
        },
        "page_content": " medium, good, respectively. The list of the indicators used by the experts to score written sentences and spoken utterances in the evaluations, grouped by similarity, is reported in Table . Since every utterance was scored by only one expert, it was not possible to evaluate any kind of agreement among experts. For future evaluations, more experts are expected to provide independent scoring on same data sets, so this kind of evaluation will be possible. The speaking part of the proficiency tests in 2017/2018 consists of 47",
        "type": "Document"
      },
      {
        "id": "7cd77f54-92ef-4037-8bf0-c2242f9e85ae",
        "metadata": {
          "chunk_id": 62,
          "document_id": "2002.05058",
          "start_idx": 24483,
          "end_idx": 24978
        },
        "page_content": " machine-generated samples for each task and compare each sample with all references by the comparative evaluator. A sample gets 3 points when beats a reference, 1 point when draws with the reference, and get 0 point when loses, 2) we adopt skill rating system by regarding each sample as an NLG model which always outputs the same sample and use the skill rating for each sample as its score. To ensure the computational budget to be roughly the same, we fix the number of plays in skill rating",
        "type": "Document"
      },
      {
        "id": "dedcf8ca-189f-4e63-bf05-dde617ce16af",
        "metadata": {
          "chunk_id": 21,
          "document_id": "2001.08051",
          "start_idx": 8219,
          "end_idx": 8701
        },
        "page_content": " from both the written and spoken data collected so far in all the campaigns. Each written or spoken item received a total score by human experts, computed by summing up the scores related to 6 indicators in 2017/2018 (from 3 to 6 in the 2016 campaign, according to the proficiency levels and the type of test). Each indicator can assume a value 0, 1, 2, corresponding to bad, medium, good, respectively. The list of the indicators used by the experts to score written sentences and",
        "type": "Document"
      },
      {
        "id": "fb7eb17a-bd7e-48d0-be31-326e7ca5693e",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1705.03261",
          "start_idx": 14252,
          "end_idx": 14657
        },
        "page_content": " : the overall precision, recall, and F1 score on testing set. INLINEFORM0 denotes the set of {False, Mechanism, Effect, Advise, Int}. The precision and recall of each INLINEFORM1 are calculated by DISPLAYFORM0 DISPLAYFORM1  Then the overall precision, recall, and F1 score are calculated by DISPLAYFORM0  Besides, we evaluate the captured feature vectors with t-SNE BIBREF26 , a visualizing and intuitive",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What proficiency indicators are used to the score the utterances?",
    "llm_answer": "6 indicators.\n",
    "context": [
      {
        "id": "b5d65ce1-5c5f-402d-b247-d5472fb998d0",
        "metadata": {
          "chunk_id": 2,
          "document_id": "2001.08051",
          "start_idx": 768,
          "end_idx": 1203
        },
        "page_content": " B2, C1 and C2. The levels considered in the evaluation campaigns where the data have been collected are: A1, A2 and B1. The indicators measure the linguistic competence of test takers both in relation to the content (e.g. grammatical correctness, lexical richness, semantic coherence, etc.) and to the speaking capabilities (e.g. pronunciation, fluency, etc.). Refer to Section SECREF2 for a description of the adopted indicators. The",
        "type": "Document"
      },
      {
        "id": "88dd1f0b-af21-49ce-a734-ea5c34d00a1d",
        "metadata": {
          "chunk_id": 22,
          "document_id": "2001.08051",
          "start_idx": 8701,
          "end_idx": 9229
        },
        "page_content": " medium, good, respectively. The list of the indicators used by the experts to score written sentences and spoken utterances in the evaluations, grouped by similarity, is reported in Table . Since every utterance was scored by only one expert, it was not possible to evaluate any kind of agreement among experts. For future evaluations, more experts are expected to provide independent scoring on same data sets, so this kind of evaluation will be possible. The speaking part of the proficiency tests in 2017/2018 consists of 47",
        "type": "Document"
      },
      {
        "id": "dedcf8ca-189f-4e63-bf05-dde617ce16af",
        "metadata": {
          "chunk_id": 21,
          "document_id": "2001.08051",
          "start_idx": 8219,
          "end_idx": 8701
        },
        "page_content": " from both the written and spoken data collected so far in all the campaigns. Each written or spoken item received a total score by human experts, computed by summing up the scores related to 6 indicators in 2017/2018 (from 3 to 6 in the 2016 campaign, according to the proficiency levels and the type of test). Each indicator can assume a value 0, 1, 2, corresponding to bad, medium, good, respectively. The list of the indicators used by the experts to score written sentences and",
        "type": "Document"
      },
      {
        "id": "0d2a4e32-4e7a-4eb4-9834-9b346a75b599",
        "metadata": {
          "chunk_id": 58,
          "document_id": "2001.08051",
          "start_idx": 22646,
          "end_idx": 23174
        },
        "page_content": " to divide the second indicator (pronunciation and fluency) into two different indicators, since fluent students might not necessarily have good pronunciation skills and vice versa, drawing for example on the IELTS Speaking band descriptors. Also, next campaigns might consider an additional indicator specifically addressed to score prosody (in particular intonation and rhythm), especially for A2 and B1 level test-takers. Considering the scope of the evaluation campaign, it is important to be aware of the limitations of the",
        "type": "Document"
      },
      {
        "id": "ea8f7981-bccf-4b22-b942-8596955c764a",
        "metadata": {
          "chunk_id": 57,
          "document_id": "2001.08051",
          "start_idx": 22350,
          "end_idx": 22869
        },
        "page_content": " or encouraging code-mixing and negative transfer phenomena. The elicitation techniques in a broader sense will be object of revision (see BIBREF34 and specifically on children speech BIBREF35) in order to maximise the quality of the corpus. As for proficiency indicators, one first step that could be taken in order to increase accuracy in the evaluation phase both for human and automatic scoring would be to divide the second indicator (pronunciation and fluency) into two different indicators, since fluent students",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What accuracy is achieved by the speech recognition system?",
    "llm_answer": "91.2% accuracy on utterances containing words, sentences, and conversations.  82.9% accuracy with a discrepancy measure.  80% accuracy in speaker-closed setting, and 60% in speaker-open setting.\n",
    "context": [
      {
        "id": "391b62a7-0a9b-48eb-a258-07824f555a67",
        "metadata": {
          "chunk_id": 1,
          "document_id": "1601.02543",
          "start_idx": 478,
          "end_idx": 1101
        },
        "page_content": " of words for enabling a transaction. Before commercial deployment of a speech solution it is imperative to have a quantitative measure of the performance of the speech solution which is primarily based on the speech recognition accuracy of the speech engine used. Generally, the recognition performance of any speech recognition based solution is quantitatively evaluated by putting it to actual use by the people who are the intended users and then analyzing the logs to identify successful and unsuccessful transactions. This evaluation is then used to identifying any further improvement in the speech recognition based",
        "type": "Document"
      },
      {
        "id": "b54e5b94-84e8-4314-9e0d-1e3d9daa7055",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1907.00758",
          "start_idx": 17344,
          "end_idx": 17855
        },
        "page_content": " across different conditions. Table shows the model accuracy broken down by utterance type. The model achieves 91.2% accuracy on utterances containing words, sentences, and conversations, all of which exhibit natural variation in speech. The model is less successful with Articulatory utterances, which contain isolated phones occurring once or repeated (e.g., \u201csh sh sh\"). Such utterances contain subtle tongue movement, making it more challenging to correlate the visual signal with the audio. And indeed, the",
        "type": "Document"
      },
      {
        "id": "b5989ecc-0701-4c5b-8f12-b8a9de24adf4",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1611.00514",
          "start_idx": 4729,
          "end_idx": 5148
        },
        "page_content": " on an NVIDIA TITAN X GPU, using Kaldi software BIBREF12 . Evaluated on 50 hours of telephone speech data from the same database, our DNN-HMM SAD indicated a frame-level miss-classification (speech/non-speech) rate of 5.9% whereas an energy-based SAD did not perform better than 20%. For acoustic features we have experimented with different configurations of cepstral features. We have used 39-dimensional PLP features",
        "type": "Document"
      },
      {
        "id": "36a77c1d-9383-4cb7-b9fb-bf21a7021719",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1907.00758",
          "start_idx": 16635,
          "end_idx": 17058
        },
        "page_content": " for each utterance. If the discrepancy falls within the minimum detectability range ( INLINEFORM0 125 INLINEFORM1 INLINEFORM2 INLINEFORM3 INLINEFORM4 45) then the prediction is correct. Random prediction (averaged over 1000 runs) yields 14.6% accuracy with a mean and standard deviation discrepancy of 328 INLINEFORM5 518ms. We achieve 82.9% accuracy with a mean and standard deviation discrepancy of 32 INLINEFORM6 223ms.",
        "type": "Document"
      },
      {
        "id": "689ded63-1e36-4a94-9a29-148815b9c7a2",
        "metadata": {
          "chunk_id": 51,
          "document_id": "2002.06675",
          "start_idx": 17326,
          "end_idx": 17787
        },
        "page_content": " WSJ throughout the experiments. Table 4 shows the phone error rates (PERs) and word error rates (WERs) for the speaker-closed and speaker-open settings. The `average' is weighted by the numbers of tokens in the ground truth transcriptions for speaker-wise evaluation sets. The word recognition accuracy reached about 80% in the speaker-closed setting. In the speaker-open setting it was 60% on average and varied greatly from speaker to speaker (from 50% to 70",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the speech recognition system evaluated?",
    "llm_answer": "Generally, by putting it to actual use by intended users and analyzing logs of successful and unsuccessful transactions.  A method for evaluation without requiring human testing is also described.\n",
    "context": [
      {
        "id": "391b62a7-0a9b-48eb-a258-07824f555a67",
        "metadata": {
          "chunk_id": 1,
          "document_id": "1601.02543",
          "start_idx": 478,
          "end_idx": 1101
        },
        "page_content": " of words for enabling a transaction. Before commercial deployment of a speech solution it is imperative to have a quantitative measure of the performance of the speech solution which is primarily based on the speech recognition accuracy of the speech engine used. Generally, the recognition performance of any speech recognition based solution is quantitatively evaluated by putting it to actual use by the people who are the intended users and then analyzing the logs to identify successful and unsuccessful transactions. This evaluation is then used to identifying any further improvement in the speech recognition based",
        "type": "Document"
      },
      {
        "id": "5155558a-dfb3-4b8b-9585-41ac83603580",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1601.02543",
          "start_idx": 845,
          "end_idx": 1419
        },
        "page_content": " successful and unsuccessful transactions. This evaluation is then used to identifying any further improvement in the speech recognition based solution to better the overall transaction completion rates. This process of evaluation is both time consuming and expensive. For evaluation one needs to identify a set of users and also identify the set of actual usage situations and perform the test. It is also important that the set of users are able to use the system with ease meaning that even in the test conditions the performance of the system, should be good, while this",
        "type": "Document"
      },
      {
        "id": "e49d9a20-8c35-4d40-83ef-185fa45768f2",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1611.00514",
          "start_idx": 18797,
          "end_idx": 19312
        },
        "page_content": ", since they only include matrix operations. The time required for our SAD is also reported which increases linearly with the duration of segment. We have presented the Intelligent Voice speaker recognition system used for the NIST 2016 speaker recognition evaluation. Our system is based on a score fusion of MFCC- and PLP-based i-vector/PLDA systems. We have described the main components of the system including, acoustic feature extraction, speech activity detection, i-vector extraction as front-end processing",
        "type": "Document"
      },
      {
        "id": "64799545-42fd-4ee3-84fb-1905eded8dba",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1601.02543",
          "start_idx": 2270,
          "end_idx": 2754
        },
        "page_content": " method was adopted to evaluate a speech recognition based solution as a case study. This is the main contribution of the paper. The rest of the paper is organized as follows. The method for evaluation without testing is described in Section SECREF2 . In Section SECREF3 we present a case study and conclude in Section SECREF4 . Fig. FIGREF1 shows the schematic of a typical menu based speech solution having 3 nodes. At each node there are a set of words that the user is expected to",
        "type": "Document"
      },
      {
        "id": "661a0fe4-2e81-4fac-b061-5c48957ec61a",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1601.02543",
          "start_idx": 12144,
          "end_idx": 12571
        },
        "page_content": " actually putting it to people test. There is a significant saving in terms of being able to identify recognition bottlenecks in a menu based speech solution through this analysis because it does not require actual people testing the system. This methodology was adopted to restructuring the set of active words at each node for better speech recognition in an actual menu based speech recognition system that caters to masses.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many of the utterances are transcribed?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "9911ded7-ef2f-4437-a628-3871ad810abb",
        "metadata": {
          "chunk_id": 36,
          "document_id": "2001.08051",
          "start_idx": 13834,
          "end_idx": 14305
        },
        "page_content": " speech in a different language from the target language has to be reported by means of an explicit marker \u201cI am 10 years old @it(io ho gi\u00e0 risposto)\u201d. Next, we concatenated utterances to be transcribed into blocks of about 5 minutes each. We noticed that knowing the question and hearing several answers could be of great help for transcribing some poorly pronounced words or phrases. Therefore, each block contains only answers to the same question, explicitly reported",
        "type": "Document"
      },
      {
        "id": "5bbae2e3-9d03-4140-ac8e-cce537b1d4e1",
        "metadata": {
          "chunk_id": 34,
          "document_id": "2001.08051",
          "start_idx": 13177,
          "end_idx": 13680
        },
        "page_content": " the annotation, which were used by 5 researchers to manually transcribe about 20 minutes of audio data. This experience led to a discussion, from which a second set of guidelines originated, aiming at reaching a reasonable trade-off between transcription accuracy and speed. As a consequence, we decided to apply the following transcription rules: only the main speaker has to be transcribed; presence of other voices (schoolmates, teacher) should be reported only with the label \u201c@voices\u201d, presence of",
        "type": "Document"
      },
      {
        "id": "7676fff6-73ba-4a93-bd03-f1c1aa3b35a9",
        "metadata": {
          "chunk_id": 46,
          "document_id": "2003.06044",
          "start_idx": 16294,
          "end_idx": 16742
        },
        "page_content": " It covers various topics about our daily life. Each utterance in the conversation is manually labeled as one out of 4 dialogue act classes. Table TABREF18 presents the statistics for both datasets. In our preprocessing, the text was lowercased before tokenized, and then sentences were tokenized by WordPiece tokenizer BIBREF27 with a 30,000 token vocabulary to alleviate the Out-of-Vocabulary problem. [1]The author claimed that they achieved 78.",
        "type": "Document"
      },
      {
        "id": "31b43193-1ec7-463a-9389-9831b243a27c",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1808.08850",
          "start_idx": 13014,
          "end_idx": 13545
        },
        "page_content": " cover different topics like technology, human rights, terrorism and politics with a length variation between 2 and 10 minutes. To encourage the diversity of content format we included newscasts, interviews, reports and round tables. During the transcription phase we opted for a manual transcription process because we observed that using transcripts from an ASR system will difficult in a large degree the manual segmentation process. The number of words per transcript oscilate between 271 and 1,602 with a total number of 8,080",
        "type": "Document"
      },
      {
        "id": "773e3ef9-27e3-4146-bca7-e9fb955adc2f",
        "metadata": {
          "chunk_id": 37,
          "document_id": "2001.08051",
          "start_idx": 14305,
          "end_idx": 14798
        },
        "page_content": " poorly pronounced words or phrases. Therefore, each block contains only answers to the same question, explicitly reported at the beginning of the block. We engaged about 30 students from two Italian linguistic high schools (namely \u201cC\u201d and \u201cS\u201d) to perform manual transcriptions. After a joint training session, we paired students together. Each pair first transcribed, individually, the same block of 5 minutes. Then, they went through a comparison phase, where each pair of students discussed",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many utterances are in the corpus?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "6ca9c184-80d3-4aa2-b722-d0288073e955",
        "metadata": {
          "chunk_id": 40,
          "document_id": "2001.08051",
          "start_idx": 15428,
          "end_idx": 15968
        },
        "page_content": " assigned either to training or evaluation sets, with proportions of $\\frac{2}{3}$ and $\\frac{1}{3}$, respectively; then training and evaluation lists were built, accordingly. Table reports statistics from the spoken data set. The id All identifies the whole data set, while Clean defines the subset in which sentences containing background voices, incomprehensible speech and word fragments were excluded. From the above description it appears that the corpus can be effectively used in many research directions. The spoken corpus features",
        "type": "Document"
      },
      {
        "id": "5254b27e-1ebe-4049-8699-d0767f294567",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1909.00279",
          "start_idx": 13718,
          "end_idx": 14228
        },
        "page_content": " models, please refer to supplementary materials. Training and Validation Sets We collected a corpus of poems and a corpus of vernacular literature from online resources. The poem corpus contains 163K quatrain poems from Tang Poems and Song Poems, the vernacular literature corpus contains 337K short paragraphs from 281 famous books, the corpus covers various literary forms including prose, fiction and essay. Note that our poem corpus and a vernacular corpus are not aligned. We further split the two corpor",
        "type": "Document"
      },
      {
        "id": "3ace560c-0a12-4a96-973b-cec1d33a7583",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1808.09409",
          "start_idx": 5300,
          "end_idx": 5801
        },
        "page_content": "1) ruling out redundant content, (2) excluding sentences containing foreign words or Chinese phonetic alphabet by checking the Unicode values, (3) dropping overly simple sentences which may not be informative, and (4) utilizing a rule-based classifier to determine whether to include the sentence into the corpus. The final corpus consists of 717,241 learner sentences from writers of 61 different native languages, in which English and Japanese constitute the majority. As for completeness, 82.78% of",
        "type": "Document"
      },
      {
        "id": "3c82b205-5d09-49eb-b2f7-0d3d9bb50ffb",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1911.01799",
          "start_idx": 10361,
          "end_idx": 10700
        },
        "page_content": " two parts: VoxCeleb1 and VoxCeleb2. We used SITW BIBREF21, a subset of VoxCeleb1 as the evaluation set. The rest of VoxCeleb1 was merged with VoxCeleb2 to form the training set (simply denoted by VoxCeleb). The training set involves $1,236,567$ utterances from $7,185$ speakers, and the evaluation set involves $6,445$ utterances from 299",
        "type": "Document"
      },
      {
        "id": "446d0807-2556-4038-a63c-6cdaa6246630",
        "metadata": {
          "chunk_id": 15,
          "document_id": "2002.06675",
          "start_idx": 5394,
          "end_idx": 5852
        },
        "page_content": " corpora of a reasonable size. Therefore, our first step was to build a speech corpus for ASR based on the data sets provided by the Ainu Museum and the Nibutani Ainu Culture Museum. In this section we explain the content of the data sets and how we modified it for our ASR corpus. The corpus we have prepared for ASR in this study is composed of text and speech. Table 1 shows the number of episodes and the total speech duration for each speaker. Among the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does their model outperform both the state-of-the-art systems?",
    "llm_answer": "Our model outperforms the three baseline systems on all evaluation metrics.  Specifically, our model achieves improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford system, and 2.62% and 3.02% on CoNLL F1 score over the MIR model.\n",
    "context": [
      {
        "id": "38d0d5d3-4be2-4a08-bc2e-e9f401074e5d",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1701.06538",
          "start_idx": 17346,
          "end_idx": 17826
        },
        "page_content": ". Details can be found in Appendix UID77 . Results of these three models form the bottom line of Figure FIGREF32 -right. Table TABREF33 compares the results of these models to the best previously-published result on this dataset . Even the fastest of these models beats the best published result (when controlling for the number of training epochs), despite requiring only 6% of the computation. We trained our models using TensorFlow BIBREF30 on clusters containing 16-32 Tesla K",
        "type": "Document"
      },
      {
        "id": "1c47556f-19a3-4cd4-8f0a-25c9aaf40fbf",
        "metadata": {
          "chunk_id": 62,
          "document_id": "2001.06354",
          "start_idx": 21248,
          "end_idx": 21648
        },
        "page_content": " Specifically, our image-only model shows much higher NDCG score (60.16). On the other hand, our consensus dropout fusion model shows more balanced results over all metrics while still outperforming on most evaluation metrics (NDCG, MRR, R@1, and R@5). Compared to results of the Visual Dialog challenge 2019, our models also show strong results. Although ReDAN+ BIBREF26 and MReaL\u2013BDAI show higher N",
        "type": "Document"
      },
      {
        "id": "5db43982-28a9-4d63-8047-9594784d578b",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1603.04553",
          "start_idx": 11106,
          "end_idx": 11513
        },
        "page_content": " the MIR model, we obtain significant improvements of 2.62% and 3.02% on CoNLL F1 score. To make a thorough empirical comparison with previous studies, Table 3 (below the dashed line) also shows the results of some state-of-the-art supervised coreference resolution systems \u2014 IMS: the second best system in the CoNLL 2012 shared task BIBREF28 ; Latent-Tree: the latent tree model BIBREF29 obtaining the best",
        "type": "Document"
      },
      {
        "id": "c686ef15-dda8-4ff8-a15a-946f0174a62e",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1603.04553",
          "start_idx": 10414,
          "end_idx": 10861
        },
        "page_content": " baseline with two deterministic systems, namely Stanford: the Stanford system BIBREF10 and Multigraph: the unsupervised multigraph system BIBREF26 , and one unsupervised system, namely MIR: the unsupervised system using most informative relations BIBREF27 . Our model outperforms the three baseline systems on all the evaluation metrics. Specifically, our model achieves improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford system,",
        "type": "Document"
      },
      {
        "id": "d7a0b7aa-da14-4112-80c0-cc26c62fe83f",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the state-of-the art?",
    "llm_answer": "The best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks.\n",
    "context": [
      {
        "id": "a4a4c58b-d1f1-46a1-9912-24f7a070ad68",
        "metadata": {
          "chunk_id": 43,
          "document_id": "2001.00137",
          "start_idx": 16781,
          "end_idx": 17197
        },
        "page_content": " the online platforms. Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector",
        "type": "Document"
      },
      {
        "id": "76185514-2b57-42c0-acac-3fa0a8a69f7b",
        "metadata": {
          "chunk_id": 101,
          "document_id": "1908.02402",
          "start_idx": 29500,
          "end_idx": 29733
        },
        "page_content": "-the-art models, while at the same time providing a more practical solution for real-world applications. We would like to thank Alexandros Papangelis, Janice Lam, Stefan Douglas Webb and SIGDIAL reviewers for their valuable comments.",
        "type": "Document"
      },
      {
        "id": "e80993a9-7aed-4f4f-8185-08683755466a",
        "metadata": {
          "chunk_id": 91,
          "document_id": "1911.03562",
          "start_idx": 34938,
          "end_idx": 35474
        },
        "page_content": " since 2010. System demo papers have the third highest average citations; however, shared task papers have the third highest median citations. The popularity of shared tasks and the general importance given to beating the state of the art (SOTA) seems to have grown in recent years\u2014something that has come under criticism. It is interesting to note that in terms of citations, workshop papers are doing somewhat better than the conferences that are not top tier. Finally, the citation numbers for tutorials show that even though a small",
        "type": "Document"
      },
      {
        "id": "ed385b18-6171-42af-9672-281f1df69e11",
        "metadata": {
          "chunk_id": 53,
          "document_id": "1710.01492",
          "start_idx": 19472,
          "end_idx": 19941
        },
        "page_content": " particular, the best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks BIBREF53 , BIBREF54 . Conversely, kernel machines seem to be less frequently used than in the past, and the use of learning methods other than the ones mentioned above is at this point scarce. All these models are examples of supervised learning as they need labeled training data. Semi-supervised learning. We should note two things about the use of deep neural",
        "type": "Document"
      },
      {
        "id": "99aa59ef-b5ee-40f0-a832-bcb982d5ab1f",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1909.02764",
          "start_idx": 5772,
          "end_idx": 6281
        },
        "page_content": "2004automatic compare local and global features in support vector machines. Next to such discriminative approaches, hidden Markov models are well-studied, however, there is no agreement on which feature-based classifier is most suitable BIBREF13. Similar to the facial expression modality, recent efforts on applying deep learning have been increased for acoustic speech processing. For instance, lee2015high use a recurrent neural network and palaz2015analysis apply a convolutional neural network to the raw",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was the performance on the self-collected corpus?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "dd7e20d8-209b-4f1e-93fb-d6bb5599cc97",
        "metadata": {
          "chunk_id": 4,
          "document_id": "2001.08051",
          "start_idx": 1506,
          "end_idx": 2043
        },
        "page_content": " recorded during some of the above mentioned evaluation campaigns. We will release the written answers in future. Details and critical issues found during the acquisition of the answers of the test takers will be discussed in Section SECREF2. The tasks that can be addressed by using the corpus are very challenging and pose many problems, which have only partially been solved by the interested scientific community. From the ASR perspective, major difficulties are represented by: a) recognition of both child and non-native speech, i.",
        "type": "Document"
      },
      {
        "id": "cf348585-ca79-493f-be9d-ce678544cd42",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1909.00694",
          "start_idx": 8898,
          "end_idx": 9330
        },
        "page_content": " CO such that it was five times larger than AL. The results are shown in Table TABREF16. We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively: . \u4f5c\u696d\u304c\u697d\u3060\u3002 The work",
        "type": "Document"
      },
      {
        "id": "727bd5dd-0dd4-469a-85d1-e9618fc62c3a",
        "metadata": {
          "chunk_id": 55,
          "document_id": "2004.01980",
          "start_idx": 18999,
          "end_idx": 19503
        },
        "page_content": " style corpus, and the sampling strategy follows the uniform distribution with the probability being equal to $\\lambda $. The human evaluation is to have a comprehensive measurement of the performances. We conduct experiments on four criteria, relevance, attraction, fluency, and style strength. We summarize the human evaluation results on the first three criteria in Table TABREF51, and the last criteria in Table TABREF57. Note that through automatic evaluation, the baselines NST, Fine-tuned, and Gig",
        "type": "Document"
      },
      {
        "id": "48917cf3-6431-4f10-9372-7ebfe988ba3e",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1603.04553",
          "start_idx": 9763,
          "end_idx": 10169
        },
        "page_content": "oNotes corpus BIBREF21 . The corpora statistics are shown in Table 2 . Our system is evaluated with automatically extracted mentions on the version of the data with automatic preprocessing information (e.g., predicted parse trees). Evaluation Metrics. We evaluate our model on three measures widely used in the literature: MUC BIBREF22 , B $^{3}$ BIBREF23 , and Entity-based CEAF (CEAF $_e$ ) BIBREF24 . In",
        "type": "Document"
      },
      {
        "id": "68c9fa82-7b71-4d3b-8d4e-70ded7bf95e4",
        "metadata": {
          "chunk_id": 21,
          "document_id": "1701.02877",
          "start_idx": 7791,
          "end_idx": 8297
        },
        "page_content": " studying NER performance on this genre gained momentum. So far, there have been no big evaluation efforts, such as ACE and OntoNotes, resulting in substantial amounts of gold standard data. Instead, benchmark corpora were created as part of smaller challenges or individual projects. The first such corpus is the UMBC corpus for Twitter NER BIBREF33 , where researchers used crowdsourcing to obtain annotations for persons, locations and organisations. A further Twitter NER corpus was created by BIBREF21",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the size of their dataset?",
    "llm_answer": "17M training tokens, 44k vocabulary size.\n",
    "context": [
      {
        "id": "2d3bdfec-4ed5-456d-bd82-023fad798858",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "9ccb7945-9a96-417a-b52e-68855c4910b1",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "6f04ea61-adc5-4abb-9261-abc554ebac1b",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1711.02013",
          "start_idx": 20658,
          "end_idx": 21116
        },
        "page_content": "ings, between recurrent layers, and on recurrent states were (0.7, 0.5, 0.5) respectively. dataset contains 17M training tokens and has a vocabulary size of 44k words. The dataset is partitioned into a training set (first 99M characters) and a development set (last 1M characters) that is used to report performance. As this dataset contains various articles from Wikipedia, the longer term information (such as current topic) plays a bigger role than in the",
        "type": "Document"
      },
      {
        "id": "6b7f6ca6-24d5-4144-9cfd-92be2de31f39",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "001abd8b-7ba1-407b-8110-49e732f239a9",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1910.12618",
          "start_idx": 19089,
          "end_idx": 19565
        },
        "page_content": " maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014",
        "type": "Document"
      }
    ]
  },
  {
    "query": "how was the dataset built?",
    "llm_answer": "The methods for dataset creation vary.  Some datasets were constructed by filtering generated datasets, trimming to the original size, and ensuring equal representation of labels.  Other datasets were scraped from websites (PDF files), converted to text, and then parsed for relevant data using pattern matching.  Still other datasets were sourced from existing corpora.\n",
    "context": [
      {
        "id": "bb75bdfe-b8c1-4c6c-959c-d7910baa1d3e",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1607.06025",
          "start_idx": 24769,
          "end_idx": 25338
        },
        "page_content": " were constructed by filtering the generated datasets according to various thresholds. Thus, the generated datasets were constructed to contain enough examples, so that the filtered datasets had at least the number of examples as the original dataset. In the end, all the datasets were trimmed down to the size of the original dataset by selecting the samples sequentially from the beginning until the dataset had the right size. Also, the datasets were filtered so that each of the labels was represented equally. All the models, including classification and discrimin",
        "type": "Document"
      },
      {
        "id": "75c8f911-af20-49c1-8b12-e4aecd8370c1",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1707.03904",
          "start_idx": 5304,
          "end_idx": 5850
        },
        "page_content": " subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short. We plan to release these annotations along with the datasets, and our retrieved documents for each question. Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated",
        "type": "Document"
      },
      {
        "id": "3b942450-5125-4b16-8163-c4e7540353a0",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1802.05322",
          "start_idx": 13905,
          "end_idx": 14341
        },
        "page_content": " explanation of the whole process needed to conduct an experiment like this. The process includes downloading a data set, web scraping for extra information, data preprocessing, model tuning and evaluation of the results. Action Adult Adventure Animation Biography Comedy Crime Documentary Drama Family Fantasy Film-Noir Game-Show History Horror Music Musical Mystery Reality-TV Romance Sci-Fi Short Sport Talk-Show Thriller War Western",
        "type": "Document"
      },
      {
        "id": "6b7f6ca6-24d5-4144-9cfd-92be2de31f39",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "83bb0967-3871-4caa-ab53-4ebccd343589",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1808.06834",
          "start_idx": 7220,
          "end_idx": 7729
        },
        "page_content": " each speaker raises an issue of which he is concerned of but no discussion is done on the issues. The creation of the dataset involved 3 steps. The first step was to scrap the pdf files from the Lok Sabha website. Each pdf file is a session. The second step was to convert the pdf files into text files for easy parsing. The challenge here was to convert this unstructured information into a structured format. The third step is to extract relevant data using pattern matching. We developed a software parser",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what processing was done on the speeches before being parsed?",
    "llm_answer": "Punctuation, XML characters, hyperlinks, Twitter-specific \"at-mentions\" and \"hashtags\" were removed.  Speeches were broken into blocks of about 5 minutes each, and concatenated utterances were transcribed.\n",
    "context": [
      {
        "id": "2767fd26-ae63-497b-94d3-36c15b23d634",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1808.06834",
          "start_idx": 17785,
          "end_idx": 18358
        },
        "page_content": " parser for the conversion of unstructured pdfs into structured format i.e into a relational database using mongo database software. We analyzed the purpose of the speeches of the member of parliament and categorized them into 4 major categories and provided statistics of the categories. We also tried to identify them automatically using fastText algorithm and provided the results. The analysis is done for understanding the purpose of the speeches in the parliament. We also presented our results on binary stance classification of the speeches whether the member is in",
        "type": "Document"
      },
      {
        "id": "37220328-aeb4-44bc-aff6-f6376dfbc0eb",
        "metadata": {
          "chunk_id": 21,
          "document_id": "1904.00648",
          "start_idx": 8377,
          "end_idx": 8850
        },
        "page_content": " and the models considered. All the resources used are publicy available. We define a set of features for characterizing the text at the token level. We mix standard linguistic features, such as Part-Of-Speech (POS) and chunk tag, together with several gazetteers specifically built for classical music, and a series of features representing tokens' left and right context. For extracting the POS and the chunk tag we use the Python library twitter_nlp, presented in BIBREF",
        "type": "Document"
      },
      {
        "id": "b7fab197-3e3f-4ce3-ba4e-e345198516d4",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1708.04557",
          "start_idx": 24883,
          "end_idx": 25397
        },
        "page_content": " to estimate the policy positions of individual actors from these speeches. In relation to Irish political actors such estimation has been hindered by the structure of the available data. While all speeches made in D\u00e1il \u00c9ireann are dutifully recorded, the architecture of the data set, where digitized versions of speeches are stored, makes it impossible to apply any of the existing text analysis software. Speeches are currently stored by D\u00e1il \u00c9ireann in more than half a million separate HTML files with entries",
        "type": "Document"
      },
      {
        "id": "20df2585-9cf0-49d9-99da-00c30040240d",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1604.05781",
          "start_idx": 4432,
          "end_idx": 4838
        },
        "page_content": " BIBREF29 . A tweet will be referred to as a `document' for the rest of this work. All document text was processed the same way. Punctuation, XML characters, and hyperlinks were removed, as were Twitter-specific \u201cat-mentions\u201d and \u201chashtags\u201d (see also the Appendix). There is useful information here, but it is either not natural language text, or it is Twitter-specific, or both. Documents were broken into",
        "type": "Document"
      },
      {
        "id": "9911ded7-ef2f-4437-a628-3871ad810abb",
        "metadata": {
          "chunk_id": 36,
          "document_id": "2001.08051",
          "start_idx": 13834,
          "end_idx": 14305
        },
        "page_content": " speech in a different language from the target language has to be reported by means of an explicit marker \u201cI am 10 years old @it(io ho gi\u00e0 risposto)\u201d. Next, we concatenated utterances to be transcribed into blocks of about 5 minutes each. We noticed that knowing the question and hearing several answers could be of great help for transcribing some poorly pronounced words or phrases. Therefore, each block contains only answers to the same question, explicitly reported",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance change of the textual semantic similarity task when no error and maximum errors (noise) are present?",
    "llm_answer": "The context states that as the percentage of error increases, there is a significant drop in performance for the textual semantic similarity task.  However, it does not specify the exact performance levels for zero error or maximum error.\n",
    "context": [
      {
        "id": "101eb392-9907-4fbe-bfdd-548ffa9a993b",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1808.09409",
          "start_idx": 16355,
          "end_idx": 16884
        },
        "page_content": " this part. From the figure we can observe that, all the semantic roles achieve significant improvements in performances. To better understand the overall results, we further look deep into the output by addressing the questions: What types of error negatively impact both systems over learner texts? What types of error are more problematic for the neural syntax-agnostic one over the L2 data but can be solved by the syntax-based one to some extent? We first carry out a suite of empirical investigations by breaking down error",
        "type": "Document"
      },
      {
        "id": "d1c23c6a-0a9e-40ac-97b4-a65a177a1799",
        "metadata": {
          "chunk_id": 120,
          "document_id": "1705.03151",
          "start_idx": 42556,
          "end_idx": 43057
        },
        "page_content": " (SNR=10). This can be observed more clearly in Fig. 6 , where the performance degradation rates compared to the noise-free condition are shown. The figure shows that when the noise increases, the performance degradation with the PTN system is less significant compared to the degradation with the i-vector system. As the Babel speech data is much more noisy than the AP16-OLR speech, this noise robustness with the PTN approach partly explains why the relative performance is inconsistent between the",
        "type": "Document"
      },
      {
        "id": "5d39353d-a126-416e-afa7-c98ee150aa61",
        "metadata": {
          "chunk_id": 69,
          "document_id": "1909.09270",
          "start_idx": 24769,
          "end_idx": 25253
        },
        "page_content": "43. The Noise Adaptation method outperforms the Raw annotations Cogcomp baseline in most cases, but does not reach the performance of the Self-training method, despite using some fully labeled data. It is instructive to compare the neural and non-neural versions of each setup. The neural method is better overall, but is less able to learn from the knowledge-based initialization weights. In the non-neural method, the difference between Raw and Combined is nearly 20 points, but the",
        "type": "Document"
      },
      {
        "id": "6143561d-719a-47c3-b43d-6303373b9ef4",
        "metadata": {
          "chunk_id": 21,
          "document_id": "2003.12932",
          "start_idx": 7589,
          "end_idx": 8028
        },
        "page_content": " score vs % of error for Sentiment analysis on SST-2 dataset after fine tuning for 10 and 50 epochs respectively. Figure FIGREF12stsa and FIGREF12stsb shows Pearson-Spearman correlation vs % of error for textual semantic similarity on STS-B dataset after fine tuning for 10 and 50 epochs respectively. It is clear from the above plots that as we increase the percentage of error, for each of the three tasks, we see a significant drop in B",
        "type": "Document"
      },
      {
        "id": "e170a19b-e538-40d1-acf5-05f8aba0f2b5",
        "metadata": {
          "chunk_id": 6,
          "document_id": "2003.12932",
          "start_idx": 2146,
          "end_idx": 2647
        },
        "page_content": " evaluate the same. This evaluation is motivated from the business use case we are solving where we are building a dialogue system to screen candidates for blue collar jobs. Our candidate user base, coming from underprivileged backgrounds, are often high school graduates. This coupled with \u2018fat finger\u2019 problem over a mobile keypad leads to a lot of typos and spelling mistakes in the responses sent to the dialogue system. Hence, for this work we focus on spelling mistakes as the noise in the data.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which sentiment analysis data set has a larger performance drop when a 10% error is introduced?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "6143561d-719a-47c3-b43d-6303373b9ef4",
        "metadata": {
          "chunk_id": 21,
          "document_id": "2003.12932",
          "start_idx": 7589,
          "end_idx": 8028
        },
        "page_content": " score vs % of error for Sentiment analysis on SST-2 dataset after fine tuning for 10 and 50 epochs respectively. Figure FIGREF12stsa and FIGREF12stsb shows Pearson-Spearman correlation vs % of error for textual semantic similarity on STS-B dataset after fine tuning for 10 and 50 epochs respectively. It is clear from the above plots that as we increase the percentage of error, for each of the three tasks, we see a significant drop in B",
        "type": "Document"
      },
      {
        "id": "58cb9a50-bee8-46a8-84f0-1662d9ddbef9",
        "metadata": {
          "chunk_id": 20,
          "document_id": "2003.12932",
          "start_idx": 7196,
          "end_idx": 7589
        },
        "page_content": " the sentiment analysis task. We plot F1 score vs % of error, as shown in Figure FIGREF6. Figure FIGREF6imdba shows the performance after fine tuning for 10 epochs, while Figure FIGREF6imdbb shows the performance after fine tuning for 50 epochs. Similarly, Figure FIGREF9ssta and Figure FIGREF9sstb) shows F1 score vs % of error for Sentiment analysis on SST-2 dataset after fine tuning for 10",
        "type": "Document"
      },
      {
        "id": "50350010-7ff3-43e3-93e1-740c8c05cdb6",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1904.07342",
          "start_idx": 8927,
          "end_idx": 9438
        },
        "page_content": "-related data, since the low number of geo-tagged samples would produce an underpowered study. In Figure FIGREF8 , we see that overall sentiment averages rarely show movement post-event: that is, only Hurricane Florence shows a significant difference in average tweet sentiment pre- and post-event at the 1% level, corresponding to a 0.12 point decrease in positive climate change sentiment. However, controlling for the same group of users tells a different story: both Hurricane Florence and Hurricane Michael",
        "type": "Document"
      },
      {
        "id": "2fb117e9-d662-42e2-a3ec-08b2da6afef7",
        "metadata": {
          "chunk_id": 49,
          "document_id": "2001.00137",
          "start_idx": 18686,
          "end_idx": 19161
        },
        "page_content": " Chatbot Intent Classification Corpus). Experimental results for the Twitter Sentiment Classification task on Kaggle's Sentiment140 Corpus dataset, displayed in Table TABREF37, show that our model has better F1-micros scores, outperforming the baseline models by 6$\\%$ to 8$\\%$. We evaluate our model and baseline models on three versions of the dataset. The first one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80",
        "type": "Document"
      },
      {
        "id": "1dc6713a-d977-4cad-929e-1ba3bc8a2bfd",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1803.07771",
          "start_idx": 875,
          "end_idx": 1471
        },
        "page_content": " improved. The construction of a high-accuracy sentiment classification model usually entails the challenging compilation of training sets with numerous samples and sufficiently accurate labels. The reason behind this difficulty is two-fold. First, sentiment is somewhat subjective, and a sample may receive different labels from different users. Second, some texts contain complex sentiment representations, and a single label is difficult to provide. We conduct a statistical analysis of public Chinese sentiment text sets in GitHub. The results show that the average label error is larger than",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much is pre-training loss increased in Low/Medium/Hard level of pruning?",
    "llm_answer": "Low levels (30-40%): no increase.\nMedium levels: increases.\nHard levels: not specified.\n",
    "context": [
      {
        "id": "774be740-6475-47f0-9a53-c0437461d90e",
        "metadata": {
          "chunk_id": 34,
          "document_id": "2002.08307",
          "start_idx": 12593,
          "end_idx": 13063
        },
        "page_content": "training and pushes the rest of the weights to 0. We can then prune those weights without affecting the output of the network. Past 40% pruning, performance starts to degrade. Pre-training loss increases as we prune weights necessary for fitting the pre-training data (Table TABREF27). Feature activations of the hidden layers start to diverge from models with low levels of pruning (Figure FIGREF18). Downstream accuracy also begins to degrade at this point. We believe",
        "type": "Document"
      },
      {
        "id": "4cd5dcec-6fa6-4620-b011-3d5254c30e3d",
        "metadata": {
          "chunk_id": 32,
          "document_id": "2002.08307",
          "start_idx": 11904,
          "end_idx": 12310
        },
        "page_content": "2, 3, 4, 5] \\times 10^{-5}$ and show the results with the best development accuracy in Figure FIGREF15 / Table TABREF27. We also measure the difference in which weights are selected for pruning during pre-training vs. downstream fine-tuning and plot the results in Figure FIGREF25. Figure FIGREF15 shows that the first 30-40% of weights pruned by magnitude weight pruning do not impact pre-training loss or",
        "type": "Document"
      },
      {
        "id": "2ee08904-83db-4e4f-a5f0-4e9efda826aa",
        "metadata": {
          "chunk_id": 36,
          "document_id": "2002.08307",
          "start_idx": 13363,
          "end_idx": 13881
        },
        "page_content": " network without trying many at once. Why does pruning at these levels hurt downstream performance? On one hand, pruning deletes pre-training information by setting weights to 0, preventing the transfer of the useful inductive biases learned during pre-training. On the other hand, pruning regularizes the model by keeping certain weights at zero, which might prevent fitting downstream datasets. Figure FIGREF15 and Table TABREF27 show information deletion is the main cause of performance degradation between 40 - 60",
        "type": "Document"
      },
      {
        "id": "867e06ae-f157-46f8-8c7f-2e7dd1934fef",
        "metadata": {
          "chunk_id": 6,
          "document_id": "2002.08307",
          "start_idx": 2226,
          "end_idx": 2714
        },
        "page_content": " evaluation. We chose magnitude weight pruning, which compresses models by removing weights close to 0, because it is one of the most fine-grained and effective compression methods and because there are many interesting ways to view pruning, which we explore in the next section. Our findings are as follows: Low levels of pruning (30-40%) do not increase pre-training loss or affect transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent",
        "type": "Document"
      },
      {
        "id": "a628ff19-b257-4a81-8127-3f9562d4719b",
        "metadata": {
          "chunk_id": 35,
          "document_id": "2002.08307",
          "start_idx": 13063,
          "end_idx": 13562
        },
        "page_content": "uning (Figure FIGREF18). Downstream accuracy also begins to degrade at this point. We believe this observation may point towards a more principled stopping criterion for pruning. Currently, the only way to know how much to prune is by trial and (dev-set) error. Predictors of performance degradation while pruning might help us decide which level of sparsity is appropriate for a given trained network without trying many at once. Why does pruning at these levels hurt downstream performance? On one",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the average length of the recordings?",
    "llm_answer": "10.4 seconds (with a standard deviation of 2.3 seconds)\n",
    "context": [
      {
        "id": "7b0f113e-a167-45bc-a086-eb63d743b87d",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1910.04269",
          "start_idx": 8889,
          "end_idx": 9359
        },
        "page_content": "'s voice is around 300 Hz and according to Nyquist-Shannon sampling theorem all the useful frequencies (0-300 Hz) are preserved with sampling at 8 kHz, therefore, we sampled raw audio files from all six languages at 8 kHz The average length of audio files in this dataset was about 10.4 seconds and standard deviation was 2.3 seconds. For our experiments, the audio length was set to 10 seconds. If the audio files were shorter than 10 second, then the data was repeated",
        "type": "Document"
      },
      {
        "id": "6535ba40-2c1c-463b-b0c1-60cc1cac7bc3",
        "metadata": {
          "chunk_id": 71,
          "document_id": "1705.03151",
          "start_idx": 25730,
          "end_idx": 26199
        },
        "page_content": " phones, with a sampling rate of 16kHz and a sample size of 16 bits. Each dataset was split into a training set consisting of 18 speakers, and a test set consisting of 6 speakers. For Mandarin, Cantonese, Vietnamese and Indonesian, the recording was conducted in a quiet environment. For Russian, Korean and Japanese, there are 2 recording conditions for each speaker, quiet and noisy. The average duration (including silence) of all the $12,939$ test utterances of the",
        "type": "Document"
      },
      {
        "id": "41d905d3-bd50-45b8-8d2b-554d3a7f895f",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1909.06522",
          "start_idx": 8980,
          "end_idx": 9471
        },
        "page_content": "ated 10-second sound clips from YouTube videos BIBREF28. Note that in our video datasets, video lengths vary between 10 seconds and 5 minutes, with an average duration of about 2 minutes. Rather than constantly repeating the 10-second sound clip to match the original minute-long audio, we superpose each sound clip on the short utterances via audio segmentation. Specifically, we first use an initial bootstrap model to align each original long audio, and segment each audio into around 10-",
        "type": "Document"
      },
      {
        "id": "263505e0-cf0a-4942-bff5-9058099af378",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1907.05664",
          "start_idx": 3794,
          "end_idx": 4271
        },
        "page_content": " articles coupled with summaries of about three sentences. These summaries are in fact \u201chighlights\" of the articles provided by the media themselves. Articles have an average length of 780 words and the summaries of 50 words. We had 287 000 training pairs and 11 500 test pairs. Similarly to See et al. See2017, we limit during training and prediction the input text to 400 words and generate summaries of 200 words. We pad the shorter texts using an UNKNOWN token and truncate",
        "type": "Document"
      },
      {
        "id": "baebdcda-6a2e-453d-b187-3a74dffd3a8e",
        "metadata": {
          "chunk_id": 37,
          "document_id": "2002.11268",
          "start_idx": 10875,
          "end_idx": 11324
        },
        "page_content": "K, 1M and 21M utterance-level {audio, transcript} pairs taken from anonymized, transcribed Voice Search data. These fine-tuning sets roughly correspond to 10 hours, 100 hours, 1000 hours and 21,000 hours of audio, respectively. The following data sources were used to choose scaling factors and/or evaluate the final model performance. Source-domain Eval Set (YouTube). The in-domain performance of the YouTube-trained RNN-T baseline was measured on",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What were their results?",
    "llm_answer": "The method achieved good rankings in three international contests (TASS'15, SemEval'15-16, and SENTIPOLC'14).  Excellent results were obtained in five other languages.  On newstest2019, results were up to 0.85 BLEU points better than the Baseline;  newstest2017 saw an improvement of 0.18 BLEU points with statistical significance; newstest2018, an improvement of 0.15 BLEU points without statistical significance.  Emergency Resolved Dialogues scored significantly higher than Emergency Not Resolved Dialogues in a study.  Results for English-French translation are also positive (Document system obtained the...).\n",
    "context": [
      {
        "id": "fcfada0a-4a28-4ceb-b822-a1bb0d8ac606",
        "metadata": {
          "chunk_id": 118,
          "document_id": "1803.02839",
          "start_idx": 45558,
          "end_idx": 45772
        },
        "page_content": " mathematical consistency of the experiments, and provided useful insights into the results. Mark prompted the investigation into potential emergent monoid structures since they appear frequently in state machines.",
        "type": "Document"
      },
      {
        "id": "1f140aea-67fe-42b4-983e-2e4e083c362a",
        "metadata": {
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "10a35031-f90d-415f-aa3f-734db4bbcba6",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1612.05270",
          "start_idx": 1650,
          "end_idx": 2169
        },
        "page_content": " tested in eight different languages: Spanish, English, Italian, Arabic, German, Portuguese, Russian and Swedish. We compare our approach ranking in three international contests: TASS'15, SemEval'15-16 and SENTIPOLC'14, for Spanish, English and Italian respectively; the remaining languages are compared directly with the results reported in the literature. The experimental results locate our approach in good positions for all considered competitions; and excellent results in the other five languages tested. Finally",
        "type": "Document"
      },
      {
        "id": "1ac5777a-29eb-4f38-bb26-cc2f64bd9235",
        "metadata": {
          "chunk_id": 59,
          "document_id": "2003.05995",
          "start_idx": 23878,
          "end_idx": 24329
        },
        "page_content": " to the single lab-based WoZ study BIBREF4. Table TABREF33 gives the results from the post-task survey. We observe, that subjective and objective task success are similar in that the dialogues that resolved the emergency were rated consistently higher than the rest. Mann-Whitney-U one-tailed tests show that the scores of the Emergency Resolved Dialogues for Q1 and Q2 were significantly higher than the scores of the Emergency Not Resolved Dialogues",
        "type": "Document"
      },
      {
        "id": "b6aa7266-b83f-4fe9-ac46-0693324540a1",
        "metadata": {
          "chunk_id": 30,
          "document_id": "1910.07481",
          "start_idx": 11703,
          "end_idx": 12134
        },
        "page_content": " best results, up to 0.85 BLEU points over the Baseline on the newstest2019 corpus. It also surpassed the Baselinee by 0.18 points on the newstest2017 with strong statistical significance, and by 0.15 BLEU points on the newstest2018 but this time with no statistical evidence. These encouraging results prompted us to extend experiments to another language pair: English-French. En$\\rightarrow $Fr: The Document system obtained the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the prediction accuracy of the model?",
    "llm_answer": "Training: 69.9%\nValidation: 64.7%\nTest: 65.3% (binary classification)\n\n82.9% (synchronisation offset)\n",
    "context": [
      {
        "id": "cc9a8228-5773-41c6-8c72-86fa7382c744",
        "metadata": {
          "chunk_id": 40,
          "document_id": "1907.00758",
          "start_idx": 15137,
          "end_idx": 15586
        },
        "page_content": " plateaus for 2 epochs. Upon convergence, the model achieves 0.193 training loss, 0.215 validation loss, and 0.213 test loss. By placing a threshold of 0.5 on predicted distances, the model achieves 69.9% binary classification accuracy on training samples, 64.7% on validation samples, and 65.3% on test samples. Synchronisation offset prediction: Section SECREF3 described briefly how to use our model to predict the synchronisation offset for test",
        "type": "Document"
      },
      {
        "id": "36a77c1d-9383-4cb7-b9fb-bf21a7021719",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1907.00758",
          "start_idx": 16635,
          "end_idx": 17058
        },
        "page_content": " for each utterance. If the discrepancy falls within the minimum detectability range ( INLINEFORM0 125 INLINEFORM1 INLINEFORM2 INLINEFORM3 INLINEFORM4 45) then the prediction is correct. Random prediction (averaged over 1000 runs) yields 14.6% accuracy with a mean and standard deviation discrepancy of 328 INLINEFORM5 518ms. We achieve 82.9% accuracy with a mean and standard deviation discrepancy of 32 INLINEFORM6 223ms.",
        "type": "Document"
      },
      {
        "id": "b82913f7-578f-4442-a8d8-39b764908802",
        "metadata": {
          "chunk_id": 72,
          "document_id": "1808.09920",
          "start_idx": 24285,
          "end_idx": 24813
        },
        "page_content": " candidate answers or the number of nodes increases. In this section we provide an error analysis for our best single model predictions. First of all, we look at which type of questions our model performs well or poorly. There are more than 150 query types in the validation set but we filtered the three with the best and with the worst accuracy that have at least 50 supporting documents and at least 5 candidates. We show results in Table 4 . We observe that questions regarding places (birth and death) are considered harder",
        "type": "Document"
      },
      {
        "id": "81f6185f-e75d-4ae3-8771-7d5229769a9b",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1909.04181",
          "start_idx": 8006,
          "end_idx": 8418
        },
        "page_content": ", and (3) predictions from our user-level BERT model. These majority class models (i.e., our third submission) achieve best results on the official test data. We acquire 54.72% accuracy for age, 81.67% accuracy for gender, 93.75% accuracy for dialect, and 40.97% joint accuracy. In this work, we described our submitted models to the Arabic author profiling and deception detection shared task (APDA) BIBREF0. We",
        "type": "Document"
      },
      {
        "id": "b8da4891-6cb5-4057-b545-a69b3f7ef87f",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1802.05322",
          "start_idx": 5026,
          "end_idx": 5479
        },
        "page_content": "16 . Accuracy is a measurement of how correct a model's predictions are and is defined as DISPLAYFORM0  . Precision is a ratio of how often positive predictions actually are positve and is defined as DISPLAYFORM0  . Recall is a measurement of how good the model is to find all true positives and is defined as DISPLAYFORM0  . BIBREF5  It has been shown that when calculating precision and recall on multi-label classifiers, it can be advantageous to use",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the dataset used in the paper?",
    "llm_answer": "Biomedical, SCI, novel-9, CNNDM, NYT50\n",
    "context": [
      {
        "id": "6b7f6ca6-24d5-4144-9cfd-92be2de31f39",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "64a25737-469b-4e82-875c-ba81cb2e5018",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1701.00185",
          "start_idx": 3790,
          "end_idx": 4304
        },
        "page_content": " this paper we use a new short text dataset, Biomedical, in the experiment to verify the effectiveness of our approach. Third, we put much effort on studying the influence of various different semantic features integrated in our self-taught CNN framework, which is not involved in the conference paper. For the purpose of reproducibility, we make the datasets and software used in our experiments publicly available at the website. The remainder of this paper is organized as follows: In Section SECREF2 , we first",
        "type": "Document"
      },
      {
        "id": "1f18073b-4721-4651-900e-6b076387c7f1",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1709.02271",
          "start_idx": 11255,
          "end_idx": 11734
        },
        "page_content": " We begin by introducing the datasets (Section SECREF15 ), followed by detailing the featurization methods (Section SECREF17 ), the experiments (Section SECREF22 ), and finally reporting results (Section SECREF26 ). The statistics for the three datasets used in the experiments are summarized in Table TABREF16 . novel-9. This dataset was compiled by F&H14: a collection of 19 novels by 9 nineteenth century British and American authors in the Project Gutenberg. To compare to F&",
        "type": "Document"
      },
      {
        "id": "39c7530e-8090-4406-a7ea-bdb1339acbba",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1911.03562",
          "start_idx": 1524,
          "end_idx": 2018
        },
        "page_content": " are based on information about the papers taken directly from AA (as of June 2019) and citation information extracted from Google Scholar (as of June 2019). Thus, all subsequent papers and citations are not included in the analysis. A fresh data collection is planned for January 2020.  Interactive Visualizations: The visualizations we are developing for this work (using Tableau) are interactive\u2014so one can hover, click to select and filter, move sliders, etc. Since this work is high in the",
        "type": "Document"
      },
      {
        "id": "2d3bdfec-4ed5-456d-bd82-023fad798858",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does the SCAN dataset evaluate compositional generalization?",
    "llm_answer": "The SCAN dataset tests the ability to generalize known primitive verbs to valid unseen constructions, and  later augmented tests included compositional generalization based on known templates.  It uses splits with more than one example of the held-out primitive verb, where training sets include varying numbers of random samples of command sequences.\n",
    "context": [
      {
        "id": "e607bf38-b959-46bb-ab30-72ac4a47dde4",
        "metadata": {
          "chunk_id": 96,
          "document_id": "1904.09708",
          "start_idx": 35365,
          "end_idx": 35870
        },
        "page_content": " 8 and Table 4 ). Compositional generalization performance is already quite high at 1 example, and at 2 examples is almost perfect (99.997% correct). The compositional generalization splits of the SCAN dataset were originally designed to test for the ability to generalize known primitive verbs to valid unseen constructions BIBREF2 . Further work with SCAN augmented this set of tests to include compositional generalization based not on known verbs but on known templates BIBREF3 . These template splits",
        "type": "Document"
      },
      {
        "id": "ec54c758-94f9-447b-8fa8-393275f512e2",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1904.09708",
          "start_idx": 16275,
          "end_idx": 16769
        },
        "page_content": ". These results were highly skewed, with a mean accuracy of 78.4 % but a median of 91.0 % (see supplementary materials for detailed results). Overall, this represents an improvement over the best previously reported results on this task BIBREF4 , BIBREF15 , and does so without any hand-engineered features or additional supervision. To test our hypothesis that compositional generalization requires a separation between syntax (i.e. sequential information used for alignment), and semantics (i",
        "type": "Document"
      },
      {
        "id": "724a080a-9916-486e-9d41-47e2a8a63272",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1906.00180",
          "start_idx": 1632,
          "end_idx": 2202
        },
        "page_content": " tests for compositional generalization that address the concerns of deep learning skeptics. The paper makes three main contributions. First, we develop a protocol for automatically generating data that can be used in entailment recognition tasks. Second, we demonstrate that several deep learning architectures succeed at one such task. Third, we present and apply a number of experiments to test whether models are capable of compositional generalization. The data generation process is inspired by BIBREF13 : an artificial language is defined, sentences are generated",
        "type": "Document"
      },
      {
        "id": "4d5a6044-20b4-43d4-ba8c-25e81398d7fc",
        "metadata": {
          "chunk_id": 92,
          "document_id": "1904.09708",
          "start_idx": 33775,
          "end_idx": 34287
        },
        "page_content": " did not substantially degrade compositional generalization performance, suggesting that the success of the Syntactic Attention model does not depend on the parameterization of the semantic stream with a simple linear function. The original SCAN dataset was published with compositional generalization splits that have more than one example of the held-out primitive verb BIBREF2 . The training sets in these splits of the dataset include 1, 2, 4, 8, 16, or 32 random samples of command sequences with the \"jump\"",
        "type": "Document"
      },
      {
        "id": "07979c35-a4e1-4e6a-a82b-96bf6c53b003",
        "metadata": {
          "chunk_id": 1,
          "document_id": "1904.09708",
          "start_idx": 528,
          "end_idx": 970
        },
        "page_content": " when tested on unseen combinations of known elements, state-of-the-art models fail to generalize BIBREF2 , BIBREF3 , BIBREF4 . It has been suggested that this failure represents a major deficiency of current deep learning models, especially when they are compared to human learners BIBREF5 , BIBREF0 . A recently published dataset called SCAN BIBREF2 (Simplified version of the CommAI Navigation tasks), tests compositional generalization in",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much does this system outperform prior work?",
    "llm_answer": "The system outperforms the three baseline systems on all evaluation metrics, with improvements of 2.93% and 3.01% on CoNLL F1 score.  Additional improvements, relative to 300h setups, are also mentioned.\n",
    "context": [
      {
        "id": "c686ef15-dda8-4ff8-a15a-946f0174a62e",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1603.04553",
          "start_idx": 10414,
          "end_idx": 10861
        },
        "page_content": " baseline with two deterministic systems, namely Stanford: the Stanford system BIBREF10 and Multigraph: the unsupervised multigraph system BIBREF26 , and one unsupervised system, namely MIR: the unsupervised system using most informative relations BIBREF27 . Our model outperforms the three baseline systems on all the evaluation metrics. Specifically, our model achieves improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford system,",
        "type": "Document"
      },
      {
        "id": "75912fd2-5758-4243-ac71-578b8a922ab8",
        "metadata": {
          "chunk_id": 45,
          "document_id": "2001.07263",
          "start_idx": 17117,
          "end_idx": 17529
        },
        "page_content": "isher data. This model consists of 10 encoder layers, and is trained for only 50 epochs. Our overall results on the Hub5'00 and other evaluation sets are summarized in Table TABREF14. The results in Fig. FIGREF12 and Table TABREF14 show that adding more training data greatly improves the system, by around 30% relative in some cases. For comparison with others, the 2000-hour system reaches 8.7% and 7.4% WER on",
        "type": "Document"
      },
      {
        "id": "abc51b19-3db2-4cbf-ac46-ff97aa2fa1d1",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1603.07044",
          "start_idx": 14997,
          "end_idx": 15515
        },
        "page_content": " task A already had a substantial amount of training data. Table 4 gives the final comparison between different models (we only list the MAP score because it is the official score for the challenge). Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited",
        "type": "Document"
      },
      {
        "id": "5e0f3225-070e-4250-8f70-93c083311ec4",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "94725338-d50b-40f9-b9f8-72f74c10a7d7",
        "metadata": {
          "chunk_id": 46,
          "document_id": "2001.07263",
          "start_idx": 17384,
          "end_idx": 17775
        },
        "page_content": " others, the 2000-hour system reaches 8.7% and 7.4% WER on rt02 and rt04. We observe that the regularization techniques, which are extremely important on the 300h setup, are still beneficial but have a significantly smaller effect.  For comparison with results in the literature we refer to the Switchboard-300 results in BIBREF3, BIBREF7, BIBREF51, BIBREF52 and the Switchboard-2000 results",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the baseline systems that are compared against?",
    "llm_answer": "Baseline 1: smooth L1 distance; Baseline 2: PCA-based symmetric acoustic similarity; Baseline 3: Nonlinear dynamical systems-based complexity measure; Stanford system; Multigraph system; MIR system; Tactron 2.\n",
    "context": [
      {
        "id": "7bb4c95c-4811-4e15-ab8e-deca3c53de73",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1709.08299",
          "start_idx": 9435,
          "end_idx": 10003
        },
        "page_content": " questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models. The",
        "type": "Document"
      },
      {
        "id": "94c43990-47e3-478c-a415-088eeac582b5",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1804.08782",
          "start_idx": 13990,
          "end_idx": 14427
        },
        "page_content": " number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 .  For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose",
        "type": "Document"
      },
      {
        "id": "b7568416-2511-4e7d-b42a-905e8dea8f55",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1909.09484",
          "start_idx": 19263,
          "end_idx": 19734
        },
        "page_content": " models and baselines, with a learning rate of 0.001 for supervised training and 0.0001 for reinforcement learning. In reinforcement learning, the decay parameter $\\lambda $ is set to 0.8. The weight decay is set to 0.001. And early stopping is performed on developing set. The experimental results of the proposed model and baselines will be analyzed from the following aspects. BPRA Results: As shown in Table TABREF35, most of the models have similar performance on BP",
        "type": "Document"
      },
      {
        "id": "c686ef15-dda8-4ff8-a15a-946f0174a62e",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1603.04553",
          "start_idx": 10414,
          "end_idx": 10861
        },
        "page_content": " baseline with two deterministic systems, namely Stanford: the Stanford system BIBREF10 and Multigraph: the unsupervised multigraph system BIBREF26 , and one unsupervised system, namely MIR: the unsupervised system using most informative relations BIBREF27 . Our model outperforms the three baseline systems on all the evaluation metrics. Specifically, our model achieves improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford system,",
        "type": "Document"
      },
      {
        "id": "ba17340c-ad36-4ff1-9d10-81ab4d77db19",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1903.07398",
          "start_idx": 11099,
          "end_idx": 11528
        },
        "page_content": " of Tactron 2 (https://github.com/NVIDIA/tacotron2) as baseline comparison. Note this open-source version is trained for much longer (around 1000 epochs) however due to our limited compute we only trained our model up to 300 epochs We decide to evaluate our model against previous baselines on two fronts, Mean Opnion Score (MOS) and training speed. Typical TTS system evaluation is done with mean opinion score (MOS). To compute",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What systems are tested?",
    "llm_answer": "Support Vector Machines (SVMs), logistic regression, Na\u00efve Bayes, Perceptron, and decision trees;  neural network models.\n",
    "context": [
      {
        "id": "28d00479-9c63-4a20-9381-e9021c8c46ee",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1605.08675",
          "start_idx": 4738,
          "end_idx": 5272
        },
        "page_content": " requiring numerous evaluations may be performed swiftly; saving massive amount of time and human resources. As a test set, authentic questions from a popular Polish quiz TV show are used. Results of experiments, testing (among others) the optimal context length, a number of retrieved documents, a type of entity recognition solution, appear in section SECREF88 . To avoid overfitting, the final system evaluation is executed on a separate test set, previously unused in development, and is checked manually. The results are shown in",
        "type": "Document"
      },
      {
        "id": "65333987-2358-483b-95cd-3ff028ff6a96",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      },
      {
        "id": "7bb4c95c-4811-4e15-ab8e-deca3c53de73",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1709.08299",
          "start_idx": 9435,
          "end_idx": 10003
        },
        "page_content": " questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models. The",
        "type": "Document"
      },
      {
        "id": "b7a08b07-33f8-4756-8035-934c8ce8d913",
        "metadata": {
          "chunk_id": 40,
          "document_id": "1911.00133",
          "start_idx": 15746,
          "end_idx": 16159
        },
        "page_content": " first experiment with a suite of non-neural models, including Support Vector Machines (SVMs), logistic regression, Na\u00efve Bayes, Perceptron, and decision trees. We tune the parameters for these models using grid search and 10-fold cross-validation, and obtain results for different combinations of input and features. For input representation, we experiment with bag-of-n-grams (for $n \\in \\lbrace 1..3\\rbrace $),",
        "type": "Document"
      },
      {
        "id": "e3748966-4a55-4502-9800-788e84cbe8cb",
        "metadata": {
          "chunk_id": 134,
          "document_id": "1605.08675",
          "start_idx": 49919,
          "end_idx": 50491
        },
        "page_content": ", used at this stage, substantially differs from the one used during the development (see section SECREF80 ). A configuration for the final evaluation has been prepared based on results of the experiments. All of the tested versions share the following features: no question analysis corrections, question classification and query generation solutions which proved best in the previous experiments (see section SECREF18 ), a retrieved set of documents including 20 articles, no minimal confidence, singe sentence context with title. Tested solutions differ with respect to",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What benchmark datasets they use?",
    "llm_answer": "Stanford Sentiment Treebank (SST-2), Yelp dataset, Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), Quora Question Pairs (QQP), SQuAD 1.1, SQuAD 2.0, MRQA, IMDb, QA4IE-SPAN-S/M/L, QA4IE-SEQ-S/M/L.\n",
    "context": [
      {
        "id": "99824268-9771-44c0-ac96-a9bc0d4da913",
        "metadata": {
          "chunk_id": 123,
          "document_id": "2002.06053",
          "start_idx": 45532,
          "end_idx": 46097
        },
        "page_content": " datasets and different evaluation metrics makes the assessment of model performance challenging. Comprehensive benchmarking platforms that can assess the success of different tools are still lacking. A benchmarking environment rigorously brings together the suitable data sets and evaluation methodologies in order to provide a fair comparison between the available tools. Such environments are available for molecule generation task from MOSES BIBREF123 and GuacaMol BIBREF124. MoleculeNet is also a similar attempt to build a benchmarking platform for tasks such",
        "type": "Document"
      },
      {
        "id": "4a2bbc22-5a05-441f-92ce-90e5ab0a95c8",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1910.00458",
          "start_idx": 17496,
          "end_idx": 17916
        },
        "page_content": "500). For the sentiment analysis category, we used the Stanford Sentiment Treebank (SST-2) dataset from the GLUE benchmark BIBREF22 (around 60k train examples) and the Yelp dataset (around 430k train examples). For the paraphrase category, three paraphrasing datasets are used from the GLUE benchmark: Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), and Quora Question Pairs (",
        "type": "Document"
      },
      {
        "id": "a2d65394-c5cf-4214-8c60-25293b3c9800",
        "metadata": {
          "chunk_id": 67,
          "document_id": "1811.09786",
          "start_idx": 21716,
          "end_idx": 22144
        },
        "page_content": "of-the-art performance on SST, TREC question classification and 16 Amazon review datasets. This section aims to get a benchmark on model performance with respect to model efficiency. In order to do that, we benchmark RCRN along with BiLSTMs and 3 layered BiLSTMs (with and without cuDNN optimization) on different sequence lengths (i.e., INLINEFORM0 ). We use the IMDb sentiment task. We use the same standard hardware (a single",
        "type": "Document"
      },
      {
        "id": "6b985200-42eb-4d58-98e7-7246d8a67670",
        "metadata": {
          "chunk_id": 41,
          "document_id": "1804.03396",
          "start_idx": 14627,
          "end_idx": 14975
        },
        "page_content": " greater than 700 as L. Therefore we obtain 6 different datasets named QA4IE-SPAN-S/M/L and QA4IE-SEQ-S/M/L. A 5/1/5 splitting of train/dev/test sets is performed. The detailed statistics of QA4IE benchmark are provided in Table 1 . We further compare our QA4IE benchmark with some existing IE and QA benchmarks in Table 2 . One can observe that QA",
        "type": "Document"
      },
      {
        "id": "5c948b9c-fe5b-4140-b25b-fb8980d231a8",
        "metadata": {
          "chunk_id": 52,
          "document_id": "1910.00458",
          "start_idx": 17758,
          "end_idx": 18087
        },
        "page_content": "antic Textual Similarity Benchmark (STS-B), and Quora Question Pairs (QQP), which are denoted as \u201cGLUE-Para.\u201d. For the span-based QA, we use the SQuAD 1.1, SQuAD 2.0 , and MRQA which is a joint dataset including six popular span-based QA datasets. Table TABREF23 summarizes the results. We see that sentiment analysis datasets do",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they combine MonaLog with BERT?",
    "llm_answer": "MonaLog is followed by the re-trained BERT.\n",
    "context": [
      {
        "id": "f977192b-0d81-4790-ba6c-b3410aa1a66c",
        "metadata": {
          "chunk_id": 69,
          "document_id": "1910.08772",
          "start_idx": 24186,
          "end_idx": 24667
        },
        "page_content": ", i.e., MonaLog followed by the re-trained BERT, can also profit from the additional training data. Intuitively, we would expect smaller gains since MonaLog already handles a fair amount of the entailments and contradictions, i.e., those cases where BERT profits from more examples. However the experiments show that the hybrid system reaches an even higher accuracy of 87.16%, more than 2 percent points above the baseline, equivalent to roughly 100 more problems correctly solved",
        "type": "Document"
      },
      {
        "id": "4d16137d-dbed-4a14-890f-b85dc521da47",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1910.08772",
          "start_idx": 17205,
          "end_idx": 17639
        },
        "page_content": "ments and contradictions, as demonstrated by the high precision values, especially on the corrected SICK set (98.50 precision for E and 95.02 precision for C). The lower recall values are due to MonaLog's current inability to handle syntactic variation. Based on these results, we tested a hybrid model of MonaLog and BERT (see Table TABREF27) where we exploit MonaLog's strength: Since MonaLog has a very high precision on Entailment",
        "type": "Document"
      },
      {
        "id": "224a923e-f960-4d9d-ac07-f2dd2abe8559",
        "metadata": {
          "chunk_id": 52,
          "document_id": "1910.08772",
          "start_idx": 17845,
          "end_idx": 18281
        },
        "page_content": "ed SICK dataset, the hybrid system performs worse than BERT. Since MonaLog is optimized for the corrected SICK, it may mislabel many E and C judgments in the uncorrected dataset. The stand-alone BERT system performs better on the uncorrected data (86.74%) than the corrected set (85.00%). The corrected set may be too inconsistent since only a part has been checked. Overall, these hybird results show that it is possible to combine our",
        "type": "Document"
      },
      {
        "id": "2e58b02d-faff-4c55-b0dc-237d343b1428",
        "metadata": {
          "chunk_id": 68,
          "document_id": "1910.08772",
          "start_idx": 23899,
          "end_idx": 24347
        },
        "page_content": "ness of BERT in the current experiment and the usefulness of the generated data. The more data we add, the better the system performs. We also see that raising the threshold to relabel uncertain cases as neutral gives a small boost, from 86.51% to 86.71%. This translates into 10 cases where the relabeling corrected the answer. Finally, we also investigated whether the hybrid system, i.e., MonaLog followed by the re-trained BERT, can also profit",
        "type": "Document"
      },
      {
        "id": "73d2f73d-e66a-4876-b17e-a74c675301a4",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1902.00821",
          "start_idx": 9236,
          "end_idx": 9525
        },
        "page_content": "P]}, x_{j+1:n}, \\texttt {[SEP]})$ , where [CLS] and [SEP] are special tokens and $x_{1:n}$ is a document splited into two sides of sentences $x_{1:j}$ and $x_{j+1:n}$ . The key performance gain of BERT comes from two novel pre-training objectives: masked language model (MLM) and next text",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they select monotonicity facts?",
    "llm_answer": "They use automatically generated datasets (HELP), crowdsourced datasets, and examples from previous manually curated datasets and linguistics publications.  They also consider examples from previous linguistics publications and manually curated datasets like FraCaS and GLUE.\n",
    "context": [
      {
        "id": "73b04eb9-9a5e-47d0-8c74-00f04992b7b0",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1906.06448",
          "start_idx": 18651,
          "end_idx": 19120
        },
        "page_content": " of models on monotonicity reasoning depends on the training set or the model themselves, we conducted further analysis performed by data augmentation with the automatically generated monotonicity dataset HELP BIBREF11 . HELP contains 36K monotonicity inference examples (7,784 upward examples, 21,192 downward examples, and 1,105 non-monotone examples). The size of the HELP word vocabulary is 15K, and the overlap ratio of vocabulary between HELP and MED is 15.2%. We",
        "type": "Document"
      },
      {
        "id": "ed5947f7-d419-4755-9b76-407a381d0e11",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1906.06448",
          "start_idx": 13495,
          "end_idx": 14013
        },
        "page_content": " additional phrase and the gravity force is too low does not form constituents with atmosphere. Thus, such examples are not strict downward monotone inferences. In such cases as (a) and (b), we do not swap the premise and the hypothesis. In the end, we collected 4,068 examples from crowdsourced datasets. We also collect monotonicity inference problems from previous manually curated datasets and linguistics publications. The motivation is that previous linguistics publications related to monotonicity reasoning are",
        "type": "Document"
      },
      {
        "id": "ecd7a7e2-aa06-4095-93c4-41fc00de0b50",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1906.06448",
          "start_idx": 3623,
          "end_idx": 4118
        },
        "page_content": "otonicity with the training set, while they perform worse in the opposite direction. This indicates that the accuracy on monotonicity reasoning depends solely on the majority direction in the training set, and models might lack the ability to capture the structural relations between monotonicity operators and their arguments. As an example of a monotonicity inference, consider the example with the determiner every in ( \"Monotonicity\" ); here the premise $P$ entails the hypothesis $H$ .  $P$",
        "type": "Document"
      },
      {
        "id": "d2071fcf-2910-4e78-b376-92b735f0e143",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1906.06448",
          "start_idx": 2027,
          "end_idx": 2488
        },
        "page_content": " previous datasets containing monotonicity inference problems, FraCaS BIBREF8 and the GLUE diagnostic dataset BIBREF9 are manually-curated datasets for testing a wide range of linguistic phenomena. However, monotonicity problems are limited to very small sizes (FraCaS: 37/346 examples and GLUE: 93/1650 examples). The limited syntactic patterns and vocabularies in previous test sets are obstacles in accurately evaluating NLI models on monotonicity reasoning.",
        "type": "Document"
      },
      {
        "id": "b526eced-e4f6-4ea9-9586-bd1a8c7f8f25",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1906.06448",
          "start_idx": 6111,
          "end_idx": 6603
        },
        "page_content": "arity of each constituent is calculated based on a monotonicity operator of functional expressions (e.g., every, when) and their function-term relations. To create monotonicity inference problems, we should satisfy three requirements: (a) detect the monotonicity operators and their arguments; (b) based on the syntactic structure, induce the polarity of the argument positions; and (c) replace the phrase in the argument position with a more general or specific phrase in natural and various",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the 12 categories devised?",
    "llm_answer": "The provided text mentions 15 hand-coded categories, and a 44-community partition, but does not explicitly state 12 categories.  The answer is not found in the context.\n",
    "context": [
      {
        "id": "78208cbe-a716-4902-9b29-395f14ca91e1",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1703.09684",
          "start_idx": 11091,
          "end_idx": 11574
        },
        "page_content": " and the answer had to be a small countable integer (1-16). The other categories were determined using regular expressions. For example, a question of the form `Are feeling ?' was classified as sentiment understanding and `What is to the right of/left of/ behind the ?' was classified as positional reasoning. Similarly, `What <OBJECT CATEGORY> is in the image?' and similar templates were used to populate subordinate object recognition questions. This method was used for questions",
        "type": "Document"
      },
      {
        "id": "a4c2d929-5ee7-450b-a44b-2d849799a15e",
        "metadata": {
          "chunk_id": 62,
          "document_id": "2001.02380",
          "start_idx": 24346,
          "end_idx": 24863
        },
        "page_content": " these categories in our comparison of manually annotated and automatically predicted signals. To illustrate how the taxonomy works in practice, we can consider the example in Figure FIGREF23, which shows a signal whose associated tokens instantiate categories I and IV in a discourse tree \u2013 the words demographic variables appear both within a preparation satellite (unit [50], category I), which precedes and points to its nucleus [51\u201354], and within a satellite inside that block (unit [52], a dependent inside the",
        "type": "Document"
      },
      {
        "id": "bde5d0ab-663c-429a-8eb5-6697d780cdc4",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1703.09684",
          "start_idx": 10601,
          "end_idx": 11091
        },
        "page_content": " would be imported if it contained the word `color' in it and the answer was a commonly used color. Questions were classified as activity or sports recognition questions if the answer was one of nine common sports or one of fifteen common activities and the question contained common verbs describing actions or sports, e.g., playing, throwing, etc. For counting, the question had to begin with `How many' and the answer had to be a small countable integer (1-16). The other categories were",
        "type": "Document"
      },
      {
        "id": "77daaffd-c758-44e3-956d-3694212be5ed",
        "metadata": {
          "chunk_id": 93,
          "document_id": "1909.00183",
          "start_idx": 34610,
          "end_idx": 35088
        },
        "page_content": "22). Compared to the 15 hand-coded categories, this 44-community partition provides finer groupings corresponding to specific sub-themes within the generic hand-coded categories. This is apparent in the hand-coded classes `Accidents', `Medication', `Clinical assessment', `Documentation' and `Infrastructure', where a variety of meaningful subtopics are identified (see Fig. FIGREF23 for details). In other cases, however, the content clusters cut across the external categories",
        "type": "Document"
      },
      {
        "id": "61243fd2-2c6a-43aa-84f8-76e156e5bfc9",
        "metadata": {
          "chunk_id": 49,
          "document_id": "2003.07433",
          "start_idx": 19311,
          "end_idx": 19770
        },
        "page_content": " well as dimensions' text analysis scores. We chose three categories based on the three different surveys: 1) DOSPERT scale; 2) BSSS scale; and 3) VIAS scale. Dimension selection: We have to define the word sets (also called dimension) for each category. We chose one dimension for each of the questions under each category to reflect real survey system evaluation. Our chosen categories are state in Fig FIGREF20. Score calculation $\\alpha $-score: $\\alpha $",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what are the off-the-shelf systems discussed in the paper?",
    "llm_answer": "IBREF28, Latent-Tree, Berkeley, LaSO, Latent-Strc, Model-Stack, Non-Linear, and the first CRF.\n",
    "context": [
      {
        "id": "a4a4c58b-d1f1-46a1-9912-24f7a070ad68",
        "metadata": {
          "chunk_id": 43,
          "document_id": "2001.00137",
          "start_idx": 16781,
          "end_idx": 17197
        },
        "page_content": " the online platforms. Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector",
        "type": "Document"
      },
      {
        "id": "6bd45ba5-5818-4bf7-9706-f82d1b94eeae",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1603.04553",
          "start_idx": 11513,
          "end_idx": 11902
        },
        "page_content": "IBREF28 ; Latent-Tree: the latent tree model BIBREF29 obtaining the best results in the shared task; Berkeley: the Berkeley system with the final feature set BIBREF12 ; LaSO: the structured perceptron system with non-local features BIBREF30 ; Latent-Strc: the latent structure system BIBREF31 ; Model-Stack: the entity-centric system with model stacking BIBREF32 ; and Non-Linear: the non-",
        "type": "Document"
      },
      {
        "id": "23100207-2d90-4e39-a1c6-cd284ce1dcf6",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1701.09123",
          "start_idx": 20683,
          "end_idx": 21114
        },
        "page_content": " the first CRF. Their \u201cbaseline\u201d system uses a similar local featureset as Ratinov and Roth's (2009) but complemented with gazetteers. Their baseline system combined with their phrase embeddings trained with infused lexicons allow them to report the best CoNLL 2003 result so far. The best system of the GermEval 2014 task built an ensemble of classifiers and pattern extractors to find the most likely tag sequence BIBREF48 . They",
        "type": "Document"
      },
      {
        "id": "a6368e04-3287-4ba7-bbe3-740fe1258f61",
        "metadata": {
          "chunk_id": 82,
          "document_id": "1603.04513",
          "start_idx": 26830,
          "end_idx": 27230
        },
        "page_content": "ational aspect of the system. All results in the five blocks are with respect to row 34, \u201cMVCNN (overall)\u201d; e.g., row 19 shows what happens when HLBL is removed from row 34, row 28 shows what happens when mutual learning is removed from row 34 etc. The block \u201cbaselines\u201d (1\u201318) lists some systems representative of previous work on the corresponding datasets, including the state-of-the-art systems (",
        "type": "Document"
      },
      {
        "id": "f5627233-f9c6-4e48-bc1d-e708edccd379",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1907.00854",
          "start_idx": 2637,
          "end_idx": 3157
        },
        "page_content": " REST API call. We demonstrate the system using publicly available, pre-trained models and knowledge base articles extracted from Stack Exchange sites. However, users can extend the system to any number of topics, or domains, without the need to modify the model serving code. All components of the system are open source and publicly available under a permissive Apache 2 License. The rest of the paper is organized as follows. In the next section, we provide an overview of the system logic and its modules. In Section",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many rules had to be defined?",
    "llm_answer": "Ten\n",
    "context": [
      {
        "id": "317fbbcf-f2c0-44ed-87e8-3fd9e485a775",
        "metadata": {
          "chunk_id": 21,
          "document_id": "1706.08032",
          "start_idx": 8281,
          "end_idx": 8786
        },
        "page_content": " and devised ten semantic rules in their hybrid approach based on the presentation of BIBREF16 . We use five rules in the semantic rules set because other five rules are only used to compute polarity of words after POS tagging or Parsing steps. We follow the same naming convention for rules utilized by BIBREF11 to represent the rules utilized in our proposed method. The rules utilized in the proposed method are displayed in Table TABREF15 in which is included examples from STS Corpus and output after",
        "type": "Document"
      },
      {
        "id": "21a42ff1-5f16-427b-baea-4d2377ca9323",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1909.12140",
          "start_idx": 2277,
          "end_idx": 2783
        },
        "page_content": " These patterns were heuristically determined in a comprehensive linguistic analysis and encode syntactic and lexical features that can be derived from a sentence's parse tree. Each rule specifies (1) how to split up and rephrase the input into structurally simplified sentences and (2) how to set up a semantic hierarchy between them. They are recursively applied on a given source sentence in a top-down fashion. When no more rule matches, the algorithm stops and returns the generated discourse tree. In",
        "type": "Document"
      },
      {
        "id": "b73af1eb-24ab-4b54-a6ef-888726cee943",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1701.09123",
          "start_idx": 10485,
          "end_idx": 11015
        },
        "page_content": " NERC systems, algorithms and approaches. They provided manually annotated data for training and testing the systems as well as an objective evaluation methodology. Using such framework, research rapidly evolved from rule-based approaches (consisting of manually handcrafted rules) to language independent systems focused on learning supervised statistical models. Thus, while in the MUC 6 competition 5 out of 8 systems were rule-based, in CoNLL 2003 16 teams participated in the English task all using statistical-based NERC BIB",
        "type": "Document"
      },
      {
        "id": "74a38c4d-caeb-4c27-bf58-968d0752419b",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1909.05438",
          "start_idx": 5481,
          "end_idx": 5883
        },
        "page_content": " necessary. These rules are not perfect, with low coverage, and can even be incorrect for some situations. For instance, when predicting a SQL command in the first task, we have a prior knowledge that (1) WHERE values potentially have co-occurring words with table cells; (2) the words \u201cmore\u201d and \u201cgreater\u201d tend to be mapped to WHERE operator \u201c INLINEFORM2 \u201d; (3) within a WHERE clause, header and cell",
        "type": "Document"
      },
      {
        "id": "8d6f2085-3df8-43fe-a13a-bdea4ecba29d",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1905.07471",
          "start_idx": 6558,
          "end_idx": 7074
        },
        "page_content": " inputs. The tool ignores questions greater than 60 characters and complex questions it cannot parse, leaving a dataset smaller than the original (see Table 1). Each rule is on average forty lines of code that traverses a dependency parse tree according to its pre-specified pattern, extracting the matching spans at each step. A master function parse() determines which rule to apply based on the question type which is categorized by nsubj presence, and the type of question (who/what/etc.). Most questions contain",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was performance of classifiers before/after using distant supervision?",
    "llm_answer": "Before distant supervision, the performance is not specified.  After, the model's performance improves, often doubling a class's F1-score, and distant supervision sometimes improves a model's performance in the 1k and 2k data sets.  However, if the model trained on clean data performs better than one trained on clean data + distant supervision, then the distant supervision causes overfitting.\n",
    "context": [
      {
        "id": "d6388896-7a4c-4e45-99d2-ec18bc5c6026",
        "metadata": {
          "chunk_id": 41,
          "document_id": "2003.08370",
          "start_idx": 13632,
          "end_idx": 14133
        },
        "page_content": " investigate this further, but it shows already the benefits that are possible through noise-handling. An evaluation of the distant supervision for Yor\u00f9b\u00e1 is given in Table TABREF14. The quality of the automatically annotated labels differs between the classes. Locations perform better than person and organization names, probably due to locations being less diverse and better covered in Wikidata. With simple date rules, we obtain already a 48% F1-score. This shows the importance of leveraging the",
        "type": "Document"
      },
      {
        "id": "625140fe-aca1-42ab-a0a9-ccbadd0adecc",
        "metadata": {
          "chunk_id": 47,
          "document_id": "2003.08370",
          "start_idx": 15969,
          "end_idx": 16447
        },
        "page_content": " higher F1-score than the distant supervision technique, the model trained on clean and distantly-supervised data deteriorates. This suggests that the BERT model overfits too much on the noise in the distant supervision. In this study, we analysed distant supervision techniques and label-noise handling for NER in Hausa and Yor\u00f9b\u00e1, two languages from developing countries. We showed that they can be successfully leveraged in a realistic low-resource scenario to double a class",
        "type": "Document"
      },
      {
        "id": "fe9a9c42-7101-4a23-a3c7-be0a402c9bca",
        "metadata": {
          "chunk_id": 38,
          "document_id": "2004.02393",
          "start_idx": 12674,
          "end_idx": 13193
        },
        "page_content": " best performed model first selects the tail passage $p_t$ and then the head passage $p_h$, because the number of candidates of tail is smaller ($\\sim $2 per question). Table TABREF21 shows the results. First, training a ranker with distant supervision performs significantly better than the random baseline, showing that the training process itself has a certain degree of denoising ability to distinguish the more informative signals from distant supervision labels. By introducing additional inductive bias of orders",
        "type": "Document"
      },
      {
        "id": "7a07bbbf-b8ef-42a9-a4de-d4960d96ffe7",
        "metadata": {
          "chunk_id": 46,
          "document_id": "2003.08370",
          "start_idx": 15498,
          "end_idx": 15969
        },
        "page_content": " obtains consistently better results due to its more complex, contextual embeddings pretrained on more data. Still, the F1-score also drops nearly half for the BERT model in the 1k setting compared to the full dataset. For 1k and 2k labeled data, the distant supervision helps to improve the model's performance. However, once the model trained only on clean data reaches a higher F1-score than the distant supervision technique, the model trained on clean and distantly-",
        "type": "Document"
      },
      {
        "id": "e3d5ab2b-015c-47bd-9b79-b3b6cf118ee2",
        "metadata": {
          "chunk_id": 92,
          "document_id": "2001.02380",
          "start_idx": 36012,
          "end_idx": 36552
        },
        "page_content": " across partitions, since the splits are based on document boundaries. We note again that for the training and development data, we have no signaling annotations of any kind; this is possible since the network does not actually use the human signaling annotations we will be evaluating against: its distant supervision consists solely of the RST relation labels. Although only used as an auxiliary training task, we can look at the model's performance on predicting discourse relations, which is given in Table TABREF34. Unsurprisingly, the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big are the datasets used?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "ed0bc156-89c2-4b94-9e58-2194484d059d",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1709.05404",
          "start_idx": 17579,
          "end_idx": 18022
        },
        "page_content": "-validation setup) for each dataset while holding the number of posts per class constant. Figure FIGREF26 shows the performance of each of the Gen, RQ, and Hyp datasets at intervals of 100 posts per class (up to the maximum size of 582 posts per class for Hyp, and 851 posts per class for RQ). From the graph, we can see that as a general trend, the datasets benefit from larger dataset sizes. Interestingly, the results for the RQ dataset are",
        "type": "Document"
      },
      {
        "id": "2d3bdfec-4ed5-456d-bd82-023fad798858",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "4a2bbc22-5a05-441f-92ce-90e5ab0a95c8",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1910.00458",
          "start_idx": 17496,
          "end_idx": 17916
        },
        "page_content": "500). For the sentiment analysis category, we used the Stanford Sentiment Treebank (SST-2) dataset from the GLUE benchmark BIBREF22 (around 60k train examples) and the Yelp dataset (around 430k train examples). For the paraphrase category, three paraphrasing datasets are used from the GLUE benchmark: Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), and Quora Question Pairs (",
        "type": "Document"
      },
      {
        "id": "6b7f6ca6-24d5-4144-9cfd-92be2de31f39",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "dcf6bc99-8700-475c-ae97-53912475cb42",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1910.10408",
          "start_idx": 10206,
          "end_idx": 10669
        },
        "page_content": " we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De). While our main goal is to verify our hypotheses on a large data condition, thus the need to include proprietary data, for the sake of reproducibility in both languages we also provide results with systems only trained on TED Talks (small data condition). When training on large scale",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What datasets are used for training/testing models? ",
    "llm_answer": "The context mentions datasets like MSCOCO, Oxford-102, MNIST, CUB, Oxford, COCO, CIFAR-10, and Swissmetro, but does not specify which are used for training and testing in *all* cases.  It also references Tables TABREF12, TABREF19, TABREF83, and TABREF10, which likely contain the needed specifics.\n",
    "context": [
      {
        "id": "65333987-2358-483b-95cd-3ff028ff6a96",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      },
      {
        "id": "99824268-9771-44c0-ac96-a9bc0d4da913",
        "metadata": {
          "chunk_id": 123,
          "document_id": "2002.06053",
          "start_idx": 45532,
          "end_idx": 46097
        },
        "page_content": " datasets and different evaluation metrics makes the assessment of model performance challenging. Comprehensive benchmarking platforms that can assess the success of different tools are still lacking. A benchmarking environment rigorously brings together the suitable data sets and evaluation methodologies in order to provide a fair comparison between the available tools. Such environments are available for molecule generation task from MOSES BIBREF123 and GuacaMol BIBREF124. MoleculeNet is also a similar attempt to build a benchmarking platform for tasks such",
        "type": "Document"
      },
      {
        "id": "7d4af53c-0878-4ce8-8945-c54db6f33d53",
        "metadata": {
          "chunk_id": 147,
          "document_id": "1910.09399",
          "start_idx": 57115,
          "end_idx": 57512
        },
        "page_content": " MSCOCO, and Oxford-102 datasets, which facilitated the work on text-to-image synthesis for several papers released more recently. While most deep learning algorithms use MNIST BIBREF74 dataset as the benchmark, there are three main datasets that are commonly used for evaluation of proposed GAN models for text-to-image synthesis: CUB BIBREF75, Oxford BIBREF76, COCO BIBREF77, and CIFAR-10 BIBREF",
        "type": "Document"
      },
      {
        "id": "93a24c0d-f0fe-4956-bef2-abf5c9a5e603",
        "metadata": {
          "chunk_id": 122,
          "document_id": "1701.09123",
          "start_idx": 46004,
          "end_idx": 46555
        },
        "page_content": " test data differs. Annotation guidelines: The gold annotation of the test data follows different guidelines from the training data. This is usually reflected in different named entity spans. The datasets and languages chosen for these experiments are based on the availability of both previous results and publicly distributed NERC systems to facilitate direct comparison of our system with other approaches. Table TABREF83 specifies the datasets used for each out-of-domain setting and language. Details of each dataset can be found Table TABREF10 .",
        "type": "Document"
      },
      {
        "id": "cb26796d-fa6a-45cf-840c-0901678f1e04",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1909.00154",
          "start_idx": 24630,
          "end_idx": 25057
        },
        "page_content": " vectors) Development set: 20% of the dataset (2003 vectors) Test set: 20% of the dataset (2003 vectors) The PyLogit package BIBREF11 also uses Swissmetro as an example. Therefore, our model specifications will extend the default one from this package. We re-estimated this model with the train set and validated with testset. The results are shown in tables TABREF31 and TABREF32. Since we are comparing the models at the test",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How better is gCAS approach compared to other approaches?",
    "llm_answer": "gCAS outperforms all other methods, especially in predicting slots consistent with the act.  Its performance is particularly strong on Entity F$_1$ and Success F$_1$, though act-slot pair predictions aren't perfect.\n",
    "context": [
      {
        "id": "9b6891a1-21ab-41b7-a3c7-13c6a9c9a77a",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1908.11546",
          "start_idx": 11120,
          "end_idx": 11563
        },
        "page_content": " gCAS still outperforms all other methods. The performance difference between CAS and gCAS on frames becomes much more evident, suggesting that gCAS is more capable of predicting slots that are consistent with the act. This finding is also consistent with their Entity F$_1$ and Success F$_1$ performance. However, gCAS's act-slot pair performance is far from perfect. The most common failure case is on non-critical slots (like `genre' in the",
        "type": "Document"
      },
      {
        "id": "c950b800-0312-43ae-906a-c6bceeafa8be",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1908.11546",
          "start_idx": 9434,
          "end_idx": 9829
        },
        "page_content": " a beam search since their inference steps are much less than Seq2Seq methods. All models use Adam optimizer BIBREF24 with a learning rate of 0.001. As shown in Table TABREF13, gCAS outperforms all other methods on Entity F$_1$ in all three domains. Compared to Seq2Seq, the performance advantage of gCAS in the taxi and restaurant domains is small, while it is more evident in the movie domain.",
        "type": "Document"
      },
      {
        "id": "6e743678-b9a5-42a4-bbc0-7a0caebea002",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1908.11546",
          "start_idx": 11563,
          "end_idx": 11967
        },
        "page_content": " perfect. The most common failure case is on non-critical slots (like `genre' in the example in Table TABREF4): gCAS does not predict them, while it predicts the critical ones (like `moviename' in the example in Table TABREF4). Table TABREF15 shows predictions of all methods from two emblematic examples. Example 1 is a frequent single-act multi-slots agent act. Example 2 is a complex multi-act example",
        "type": "Document"
      },
      {
        "id": "d2f36825-32d0-4819-aed8-7f9e9ea5b8ba",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1908.11546",
          "start_idx": 10415,
          "end_idx": 10854
        },
        "page_content": " (see example 2 in Table TABREF15) and this makes classification and sequence generation harder, but gCAS multi-label slots decoder handles it easily. Table TABREF14 shows the turn-level acts and frame prediction performance. CAS and gCAS outperform all other models in acts prediction in terms of F$_1$ score. The main reason is that CAS and gCAS output a tuple at each recurrent step, which makes for shorter sequences that are easier to",
        "type": "Document"
      },
      {
        "id": "98346688-320a-454a-ba5b-d3c9fd4884e2",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1908.11546",
          "start_idx": 10106,
          "end_idx": 10608
        },
        "page_content": " by the classification model in the taxi domain. The reason is that in the taxi domain, the agent usually informs the user at the last turn, while in all previous turns the agent usually requests information from the user. It is easy for the classification model to overfit this pattern. The advantage of gCAS in the restaurant domain is much more evident: the agent's inform act usually has multiple slots (see example 2 in Table TABREF15) and this makes classification and sequence generation harder,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is specific to gCAS cell?",
    "llm_answer": "gCAS uses a recurrent cell with separate \"continue,\" \"act,\" and \"slot\" units connected sequentially.  It allows the decoder to output a tuple at each step.\n",
    "context": [
      {
        "id": "e70ad769-7d7f-4934-adfd-48f4f472bf94",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1908.11546",
          "start_idx": 8693,
          "end_idx": 9198
        },
        "page_content": " to continue, act and slots. For each step in the sequence of CAS tuples, given the output of the GRU, continue, act and slot predictions are obtained by separate heads, each with one fully connected layer. The hidden state of the GRU and the predictions at the previous step are passed to the cell at the next step connecting them sequentially. gCAS uses our proposed recurrent cell which contains separate continue, act and slots unit that are sequentially connected. The classification architecture has",
        "type": "Document"
      },
      {
        "id": "71edf5ba-2174-43a0-8399-9184f5a63aeb",
        "metadata": {
          "chunk_id": 41,
          "document_id": "1908.11546",
          "start_idx": 13833,
          "end_idx": 14277
        },
        "page_content": ", and proposed a novel recurrent cell, gated CAS, which allows the decoder to output a tuple at each step. Experimental results showed that gCAS is the best performing model for multi-act prediction. The CAS decoder and the gCAS cell can also be used in a user simulator and gCAS can be applied in the encoder. A few directions for improvement have also been identified: 1) improving the performance on non-critical slots, 2) tuning the decoder",
        "type": "Document"
      },
      {
        "id": "6e743678-b9a5-42a4-bbc0-7a0caebea002",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1908.11546",
          "start_idx": 11563,
          "end_idx": 11967
        },
        "page_content": " perfect. The most common failure case is on non-critical slots (like `genre' in the example in Table TABREF4): gCAS does not predict them, while it predicts the critical ones (like `moviename' in the example in Table TABREF4). Table TABREF15 shows predictions of all methods from two emblematic examples. Example 1 is a frequent single-act multi-slots agent act. Example 2 is a complex multi-act example",
        "type": "Document"
      },
      {
        "id": "1a85bea1-249f-46e7-a264-233f132af7ac",
        "metadata": {
          "chunk_id": 56,
          "document_id": "1908.02402",
          "start_idx": 16966,
          "end_idx": 17300
        },
        "page_content": " and requestable slots classifiers. Then, the concatenation of the context vector $c^{k^S}$ , the embedding vector of the response slot $e^{k^S}$ and the KB query vector $d_t$ are used as input to a single GRU cell. Finally, a sigmoid non-linearity is applied to the product of a weight vector $W_{y}^{S}$ and the output of the GRU $h",
        "type": "Document"
      },
      {
        "id": "548441e2-9457-4a5c-bb54-730043c54e85",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1811.09786",
          "start_idx": 6995,
          "end_idx": 7485
        },
        "page_content": " the controller cell is to learn gating functions in order to influence the target cell. In order to control the target cell, the controller cell constructs a forget gate and an output gate which are then used to influence the information flow of the listener cell. For each gate (output and forget), we use a separate RNN cell. As such, the controller cell comprises two cell states and an additional set of parameters. The equations of the controller cell are defined as follows: i1t = s(",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the source of external knowledge?",
    "llm_answer": "The context states that external knowledge comes from \"different knowledge sources.\"  It gives examples like parsing errors and incorrectly identified relations, but doesn't specify the *type* of sources.\n",
    "context": [
      {
        "id": "5ef17709-4fb7-4f63-b116-74d97027770d",
        "metadata": {
          "chunk_id": 73,
          "document_id": "1905.10238",
          "start_idx": 24474,
          "end_idx": 25054
        },
        "page_content": " BIBREF12 proposed an end-to-end model that learns contextual information with an LSTM module and proved that such knowledge is helpful for coreference resolution when the context is properly encoded. The aforementioned two types of knowledge have their own advantages: the contextual information covers diverse text expressions that are difficult to be predefined while the external knowledge is usually more precisely constructed and able to provide extra information beyond the training data. Different from previous work, we explore the possibility of joining the two types of",
        "type": "Document"
      },
      {
        "id": "2ef53658-12c3-427f-bdf4-a00681e0c1b5",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1905.10238",
          "start_idx": 18851,
          "end_idx": 19383
        },
        "page_content": " model. It proves that simply treating external knowledge as the feature, even though they are from the same sources, is not as effective as learning them in a joint framework. The reason behind this result is mainly from the noise in the knowledge source, e.g., parsing error, incorrectly identified relations, etc. For example, the plurality of 17% noun phrases are wrongly labeled in the test data. As a comparison, our knowledge attention might contribute to alleviate such noise when incorporating all knowledge sources. Effect",
        "type": "Document"
      },
      {
        "id": "048bcfff-0979-4eab-90b2-f41c6c9ed1e2",
        "metadata": {
          "chunk_id": 75,
          "document_id": "1905.10238",
          "start_idx": 25430,
          "end_idx": 26025
        },
        "page_content": ", where the first layer encodes contextual information and the second layer leverages external knowledge. Particularly, a knowledge attention mechanism is proposed to selectively leverage features from different knowledge sources. As an enhancement to existing methods, the proposed model combines the advantage of conventional feature-based models and deep learning models, so that context and external knowledge can be synchronously and effectively used for this task. Experimental results and case studies demonstrate the superiority of the proposed model to state-of-the-art baselines. Since",
        "type": "Document"
      },
      {
        "id": "9bfe99dd-4ce8-434d-b145-6db7330c32e0",
        "metadata": {
          "chunk_id": 57,
          "document_id": "1906.03538",
          "start_idx": 22310,
          "end_idx": 22852
        },
        "page_content": " life. In this work, we attempt to facilitate the development of systems that aid in better organization and access to information, with the hope that the access to more diverse information can address over-personalization too BIBREF41 . The dataset presented here is not intended to be exhaustive, nor does it attempt to reflect a true distribution of the important claims and perspectives in the world, or to associate any of the perspective and identified evidence with levels of expertise and trustworthiness. Moreover, it is important to",
        "type": "Document"
      },
      {
        "id": "8afbe501-cf0a-4b58-8475-47a25ab3cae4",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1606.08140",
          "start_idx": 1140,
          "end_idx": 1742
        },
        "page_content": " presented here. First, additional information sources might not be available, e.g., for knowledge bases for specialized domains. Second, models that don't exploit external resources are simpler and thus typically much faster to train than the more complex models using additional information. Third, the more complex models that exploit external information are typically extensions of these simpler models, and are often initialized with parameters estimated by such simpler models, so improvements to the simpler models should yield corresponding improvements to the more complex models as well. Emb",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What were the sizes of the test sets?",
    "llm_answer": "1585 IPUs for one test set, 1841 IPUs for another.  A test set of 100 dialogs with four turns is also mentioned.\n",
    "context": [
      {
        "id": "b99f9b51-787d-423c-8c49-bf72943f2462",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1701.02877",
          "start_idx": 9957,
          "end_idx": 10406
        },
        "page_content": " training and testing data sizes measured in number of NEs are listed in Table 2 . Separate models are then trained on the training parts of each corpus and evaluated on the development (if available) and test parts of the same corpus. If development parts are available, as they are for CoNLL (CoNLL Test A) and MUC (MUC 7 Dev), they are not merged with the training corpora for testing, as it was permitted to do in the context of those evaluation",
        "type": "Document"
      },
      {
        "id": "0f4da18f-2857-4f5e-94bf-e207e023f74d",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1801.09030",
          "start_idx": 17342,
          "end_idx": 17824
        },
        "page_content": " two test sets to do evaluation to take the advantages of both data magnitude and quality. In our experiments, we implement our models with the PyTorch toolkit . We set the embedding size of both Chinese characters in the symptoms and the herb tokens to 100. We set the hidden state size to 300, and the batch size to 20. We set the maximum length of the herb sequence to 20 because the length of nearly all the prescriptions are within this range (see Table 2 for the statistics of",
        "type": "Document"
      },
      {
        "id": "2b734770-7d2b-49a1-ba0d-f7fc9e5b5276",
        "metadata": {
          "chunk_id": 43,
          "document_id": "2002.06675",
          "start_idx": 14629,
          "end_idx": 15086
        },
        "page_content": " test sets. Thereafter, the total sizes of the development and test sets turns out to be 1585 IPUs spanning 2 hours 23 minutes and 1841 IPUs spanning 2 hours and 48 minutes respectively. The ASR model is trained with the rest data. In the speaker-open condition, all the data except for the test speaker's were used for training As it would be difficult to train the model if all of the data of speaker KM or UT were removed, experiments using their speaker",
        "type": "Document"
      },
      {
        "id": "739a687e-4917-4f98-b61c-1c2645d82d82",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1908.07816",
          "start_idx": 15613,
          "end_idx": 16096
        },
        "page_content": " pairs that have at least one utterance with length greater than 30. We also reduce the frequency of those pairs whose responses appear too many times (the threshold is set to 10 for Cornell, and 5 for DailyDialog), to prevent them from dominating the learning procedure. See Table TABREF25 for the sizes of the training and validation sets. The test set consists of 100 dialogs with four turns. We give more detailed description of how we create the test set in Section SECREF31. We",
        "type": "Document"
      },
      {
        "id": "bb75bdfe-b8c1-4c6c-959c-d7910baa1d3e",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1607.06025",
          "start_idx": 24769,
          "end_idx": 25338
        },
        "page_content": " were constructed by filtering the generated datasets according to various thresholds. Thus, the generated datasets were constructed to contain enough examples, so that the filtered datasets had at least the number of examples as the original dataset. In the end, all the datasets were trimmed down to the size of the original dataset by selecting the samples sequentially from the beginning until the dataset had the right size. Also, the datasets were filtered so that each of the labels was represented equally. All the models, including classification and discrimin",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which datasets are used?",
    "llm_answer": "Stanford Sentiment Treebank (SST-2), Yelp, Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), Quora Question Pairs, SCI, VQA Dataset, Visual7W, Visual Genome, and INLINEFORM0.\n",
    "context": [
      {
        "id": "4a2bbc22-5a05-441f-92ce-90e5ab0a95c8",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1910.00458",
          "start_idx": 17496,
          "end_idx": 17916
        },
        "page_content": "500). For the sentiment analysis category, we used the Stanford Sentiment Treebank (SST-2) dataset from the GLUE benchmark BIBREF22 (around 60k train examples) and the Yelp dataset (around 430k train examples). For the paraphrase category, three paraphrasing datasets are used from the GLUE benchmark: Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), and Quora Question Pairs (",
        "type": "Document"
      },
      {
        "id": "6b7f6ca6-24d5-4144-9cfd-92be2de31f39",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "91672d1f-40de-4361-a563-5175325a46aa",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1703.09684",
          "start_idx": 2802,
          "end_idx": 3121
        },
        "page_content": " BIBREF3 , FM-IQA BIBREF4 , The VQA Dataset BIBREF1 , Visual7W BIBREF5 , and Visual Genome BIBREF6 . FM-IQA needs human judges and has not been widely used, so we do not discuss it further. Table 1 shows statistics for the other datasets. Following others BIBREF7 , BIBREF8 , BIBREF9 , we refer to the portion of The VQ",
        "type": "Document"
      },
      {
        "id": "93a24c0d-f0fe-4956-bef2-abf5c9a5e603",
        "metadata": {
          "chunk_id": 122,
          "document_id": "1701.09123",
          "start_idx": 46004,
          "end_idx": 46555
        },
        "page_content": " test data differs. Annotation guidelines: The gold annotation of the test data follows different guidelines from the training data. This is usually reflected in different named entity spans. The datasets and languages chosen for these experiments are based on the availability of both previous results and publicly distributed NERC systems to facilitate direct comparison of our system with other approaches. Table TABREF83 specifies the datasets used for each out-of-domain setting and language. Details of each dataset can be found Table TABREF10 .",
        "type": "Document"
      },
      {
        "id": "a8398b5a-e257-4f5d-b703-dcb025251ed1",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1705.03261",
          "start_idx": 13854,
          "end_idx": 14252
        },
        "page_content": " five types. We give the definitions of these types and their example sentences, as shown in table TABREF4 . This standard dataset is made up of training set and testing set. We use the same metrics as in other drug-drug interaction extraction literature BIBREF11 , BIBREF10 , BIBREF25 , BIBREF9 , BIBREF8 , BIBREF12 : the overall precision, recall, and F1 score on testing set. INLINEFORM0 denotes",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much does it minimally cost to fine-tune some model according to benchmarking framework?",
    "llm_answer": "A few minutes to hours.\n",
    "context": [
      {
        "id": "e4a716d3-0925-45e2-98aa-97d63dc23172",
        "metadata": {
          "chunk_id": 12,
          "document_id": "2002.05829",
          "start_idx": 4883,
          "end_idx": 5371
        },
        "page_content": ". After every thousand of pretraining steps, we clone the model for fine-tuning and see if the final performance can reach our cut-off level. When the level is reached, time and cost for pretraining is used for comparison. Models faster or cheaper to pretrain are recommended. For fine-tuning phase, we consider the time and cost each model requires to reach certain multi-task performance fine-tuned from given pretrained models because for each single task with different difficulty and",
        "type": "Document"
      },
      {
        "id": "e6297a02-cfef-4a67-a989-301db1a0379d",
        "metadata": {
          "chunk_id": 11,
          "document_id": "2002.05829",
          "start_idx": 4581,
          "end_idx": 5107
        },
        "page_content": " relation between model parameter and fine-tuning efficiency and demonstrate consistency of energy efficiency between tasks for different pretrained models. For pretraining phase, the benchmark is designed to favor energy efficient models in terms of time and cost that each model takes to reach certain multi-task performance pretrained from scratch. For example, we keep track of the time and cost of a BERT model pretrained from scratch. After every thousand of pretraining steps, we clone the model for fine-tuning and see",
        "type": "Document"
      },
      {
        "id": "120c80cb-5ca3-4688-a23a-37064498ab22",
        "metadata": {
          "chunk_id": 7,
          "document_id": "2002.05829",
          "start_idx": 2969,
          "end_idx": 3417
        },
        "page_content": " If a model is trained from scratch, we consider such model has no pretraining phase but fine-tuned from scratch. Typically pretraining takes several days and hundreds of dollars, according to Table TABREF1. Fine-tuning takes a few minutes to hours, costing a lot less than pretraining phase. Inference takes several milli-seconds to seconds, costing much less than fine-tuning phase. Meanwhile, pretraining is done before fine-tuning once for all,",
        "type": "Document"
      },
      {
        "id": "f00d3185-b831-48a7-988d-e725cb3a35a7",
        "metadata": {
          "chunk_id": 29,
          "document_id": "2002.05829",
          "start_idx": 11715,
          "end_idx": 12170
        },
        "page_content": " For computation-heavy tasks, we adopt the reported resource requirements in the original papers as the pretraining phase baselines. For fine-tuning and inference phase, we conduct extensive experiments on given hardware (GTX 2080Ti GPU) with different model settings as shown in Table TABREF8 and Table TABREF9. We also collect the devlopment set performance with time in fine-tuning to investigate in how the model are fine-tuned for different tasks. In",
        "type": "Document"
      },
      {
        "id": "7174f731-a0c5-4560-b206-b8912989cf98",
        "metadata": {
          "chunk_id": 10,
          "document_id": "2002.05829",
          "start_idx": 4037,
          "end_idx": 4581
        },
        "page_content": " end-to-end energy efficiency in pretraining, fine-tuning and inference. With the Hulk benchmark, we quantify the energy efficiency of model pretraining, fine-tuning and inference phase by comparing the time and cost they require to reach certain overall task-specific performance level on selected datasets. The design principle and benchmarking process are detailed in section SECREF2. We also explore the relation between model parameter and fine-tuning efficiency and demonstrate consistency of energy efficiency between tasks for different",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What models are included in baseline benchmarking results?",
    "llm_answer": "Baseline 1, Baseline 2, Baseline 3.\n",
    "context": [
      {
        "id": "99824268-9771-44c0-ac96-a9bc0d4da913",
        "metadata": {
          "chunk_id": 123,
          "document_id": "2002.06053",
          "start_idx": 45532,
          "end_idx": 46097
        },
        "page_content": " datasets and different evaluation metrics makes the assessment of model performance challenging. Comprehensive benchmarking platforms that can assess the success of different tools are still lacking. A benchmarking environment rigorously brings together the suitable data sets and evaluation methodologies in order to provide a fair comparison between the available tools. Such environments are available for molecule generation task from MOSES BIBREF123 and GuacaMol BIBREF124. MoleculeNet is also a similar attempt to build a benchmarking platform for tasks such",
        "type": "Document"
      },
      {
        "id": "94c43990-47e3-478c-a415-088eeac582b5",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1804.08782",
          "start_idx": 13990,
          "end_idx": 14427
        },
        "page_content": " number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 .  For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose",
        "type": "Document"
      },
      {
        "id": "b7568416-2511-4e7d-b42a-905e8dea8f55",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1909.09484",
          "start_idx": 19263,
          "end_idx": 19734
        },
        "page_content": " models and baselines, with a learning rate of 0.001 for supervised training and 0.0001 for reinforcement learning. In reinforcement learning, the decay parameter $\\lambda $ is set to 0.8. The weight decay is set to 0.001. And early stopping is performed on developing set. The experimental results of the proposed model and baselines will be analyzed from the following aspects. BPRA Results: As shown in Table TABREF35, most of the models have similar performance on BP",
        "type": "Document"
      },
      {
        "id": "f00d3185-b831-48a7-988d-e725cb3a35a7",
        "metadata": {
          "chunk_id": 29,
          "document_id": "2002.05829",
          "start_idx": 11715,
          "end_idx": 12170
        },
        "page_content": " For computation-heavy tasks, we adopt the reported resource requirements in the original papers as the pretraining phase baselines. For fine-tuning and inference phase, we conduct extensive experiments on given hardware (GTX 2080Ti GPU) with different model settings as shown in Table TABREF8 and Table TABREF9. We also collect the devlopment set performance with time in fine-tuning to investigate in how the model are fine-tuned for different tasks. In",
        "type": "Document"
      },
      {
        "id": "7d4754c7-4ea8-4d20-9fb6-3d1e50bf0cf6",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1809.04960",
          "start_idx": 22497,
          "end_idx": 22960
        },
        "page_content": " the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types of comments as described in the above retrieval evaluation setting: Correct, Plausible, Popular, and Random. We rank the candidate comment set with four models (TF-IDF, S2S, IR, and Proposed+IR), and record the types of top-1 comments. Figure FIGREF40 shows the percentage of different types of top-1 comments generated by each model. It shows",
        "type": "Document"
      }
    ]
  },
  {
    "query": "It looks like learning to paraphrase questions, a neural scoring model and a answer selection model cannot be trained end-to-end. How are they trained?",
    "llm_answer": "The entire system, including the paraphrase generation, scoring model, and QA model, is trained end-to-end using question-answer pairs.\n",
    "context": [
      {
        "id": "a7303f97-53e3-4ef6-a80b-81091760e672",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1708.06022",
          "start_idx": 3723,
          "end_idx": 4216
        },
        "page_content": ". A neural scoring model predicts the quality of the generated paraphrases, while learning to assign higher weights to those which are more likely to yield correct answers. The paraphrases and the original question are fed into a QA model that predicts a distribution over answers given the question. The entire system is trained end-to-end using question-answer pairs as a supervision signal. The framework is flexible, it does not rely on specific paraphrase or QA models. In fact, this plug",
        "type": "Document"
      },
      {
        "id": "00c65782-38c7-4229-a350-95fda62b65a6",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1703.04617",
          "start_idx": 789,
          "end_idx": 1343
        },
        "page_content": " an end-to-end neural network framework, since we regard question understanding is of importance for such problems. We first introduced syntactic information to help encode questions. We then viewed and modelled different types of questions and the information shared among them as an adaptation problem and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results on our competitive baselines. Recent advance on reading comprehension and question answering has",
        "type": "Document"
      },
      {
        "id": "db3074db-c4f6-4dce-b8e7-a65b24982c09",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1708.06022",
          "start_idx": 5527,
          "end_idx": 6017
        },
        "page_content": " we first generate candidate paraphrases $H_q$ for question $q$ . Then, a neural scoring model predicts the quality of the generated paraphrases, and assigns higher weights to the paraphrases which are more likely to obtain the correct answers. These paraphrases and the original question simultaneously serve as input to a QA model that predicts a distribution over answers for a given question. Finally, the results of these two models are fused to predict the answer. In the following we",
        "type": "Document"
      },
      {
        "id": "5486b4f7-fda4-476b-8f6d-e12c4176deb8",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1708.06022",
          "start_idx": 3178,
          "end_idx": 3723
        },
        "page_content": " not informative enough to model the quality of question paraphrases, or cannot easily generalize to unseen rewrite rules. In this paper, we present a general framework for learning paraphrases for question answering tasks. Given a natural language question, our model estimates a probability distribution over candidate answers. We first generate paraphrases for the question, which can be obtained by one or several paraphrasing systems. A neural scoring model predicts the quality of the generated paraphrases, while learning to assign higher",
        "type": "Document"
      },
      {
        "id": "17e839cc-213e-41c5-b048-c6f0c9c6dfdc",
        "metadata": {
          "chunk_id": 1,
          "document_id": "1703.04617",
          "start_idx": 493,
          "end_idx": 995
        },
        "page_content": " of relatively large training datasets (see Section \"Related Work\" for more details) has made it more feasible to train and estimate rather complex models in an end-to-end fashion for these problems, in which a whole model is fit directly with given question-answer tuples and the resulting model has shown to be rather effective. In this paper, we take a closer look at modeling questions in such an end-to-end neural network framework, since we regard question understanding is of importance for such",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much more accurate is the model than the baseline?",
    "llm_answer": "The model outperforms the baseline by 35% in Exact Match and 25% in Goal Match in previously seen environments.  It also performs better on \"What\" questions, with 30 fewer incorrect outputs.  In new environments, the model significantly outperforms the second-best model.\n",
    "context": [
      {
        "id": "fc23cf7f-21ff-43cd-8d00-ef5914322400",
        "metadata": {
          "chunk_id": 68,
          "document_id": "1810.00663",
          "start_idx": 26280,
          "end_idx": 26763
        },
        "page_content": " room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM. Note that the average number of actions in the ground truth output sequences is 7.07 for the Test-New set. Our model's predictions are just INLINEFORM1 edits off on average from the correct navigation plans. This",
        "type": "Document"
      },
      {
        "id": "e907df3c-d8f6-452b-be07-b484a6382be6",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1912.10435",
          "start_idx": 18889,
          "end_idx": 19382
        },
        "page_content": " of \"Which\" questions, our model goes wrong 69 times where as the baseline model goes wrong 64 times, a very small numeric difference. However, for the \"What\" questions the baseline model produces incorrect outputs for 776 examples while our model produces 30 fewer incorrect outputs. The reason for this lapse appears to be related to data augmentation where we observed that many a times \"Which\" was backtranslated as \"What\" and vice versa. Thus, the questions in these two classes are mixed",
        "type": "Document"
      },
      {
        "id": "d7a0b7aa-da14-4112-80c0-cc26c62fe83f",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "8b374470-6f3b-46f3-b139-7dbb3f613b25",
        "metadata": {
          "chunk_id": 146,
          "document_id": "2001.02380",
          "start_idx": 53784,
          "end_idx": 54256
        },
        "page_content": ", and therefore prone to penalizing the baseline; many words mean more chances for a random guess to be wrong), model accuracy is substantially better in this scenario, reaching a 40% chance of hitting a signal with only one guess, exceeding 53% with two guesses, and capping at 64% for recall@3, over 20 points above baseline. Finally, the right panel in the figure shows recall when only DMs are considered. In this scenario, a random guess fares very poorly, since most",
        "type": "Document"
      },
      {
        "id": "4d522ea5-86b3-42df-acaa-37f028f2bec5",
        "metadata": {
          "chunk_id": 138,
          "document_id": "2001.11381",
          "start_idx": 31860,
          "end_idx": 32196
        },
        "page_content": "ados. Dicho lo anterior, el Modelo 1 podr\u00eda ser considerado como nuestro propio baseline. A continuaci\u00f3n presentamos un protocolo de evaluaci\u00f3n manual de los resultados obtenidos. El experimento consisti\u00f3 en la generaci\u00f3n de 15 frases por cada uno de los tres modelos propuestos. Para cada modelo, se consideraron tres queries: $Q=$ {AM",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is new state-of-the-art performance on CoNLL-2009 dataset?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "1818fc25-3d46-4906-9e69-a3ea11deddf8",
        "metadata": {
          "chunk_id": 64,
          "document_id": "1901.09755",
          "start_idx": 24362,
          "end_idx": 24809
        },
        "page_content": " 2014 training set is substantially larger, as detailed in Table TABREF7 . In fact, the smaller datasets seem to affect more the deep learning approaches (LSTM, WDEmb, RNCRF) where only the MIN and CMLA models obtain similar results to ours, albeit using manually added language-specific annotations. Finally, it would have been interesting to compare MIN, CNN-SenticNet and CMLA with our system on the three ABSA benchmarks, but their systems are",
        "type": "Document"
      },
      {
        "id": "a9246508-0626-4fda-8310-a33ad7fb46a4",
        "metadata": {
          "chunk_id": 117,
          "document_id": "1701.09123",
          "start_idx": 44092,
          "end_idx": 44535
        },
        "page_content": "31 and our best CoNLL 2003 model (en-91-18) for comparison. First, we can observe that for every portion of the training set, both our best cluster and en-91-18 model outperform the Illinois NER system. The best cluster results are noteworthy because, as opposed to Illinois NER, it does not use gazetteers or global features for extra performance. These results are mirrored by those obtained for the rest of the languages and datasets. Thus,",
        "type": "Document"
      },
      {
        "id": "3123489d-7210-44a3-83a2-d01e25666fd5",
        "metadata": {
          "chunk_id": 140,
          "document_id": "1701.09123",
          "start_idx": 52688,
          "end_idx": 53145
        },
        "page_content": " 2003 and for the three English out-of-domain evaluations. For German we improve the results of the best system in the GermEval 2014 task and obtain comparable results to previous work in the CoNLL 2003 dataset using publicly available data. In Spanish we provide results on CoNLL 2002 and in two out-of-domain evaluations clearly outperforming previous best results. For Dutch we improve over previous results in CoNLL 2002 and SONAR-1 data and two out-of-",
        "type": "Document"
      },
      {
        "id": "ce9a5da7-b644-459d-89dc-6f6485bbc871",
        "metadata": {
          "chunk_id": 62,
          "document_id": "1701.02877",
          "start_idx": 22362,
          "end_idx": 22802
        },
        "page_content": " data gives little to no improvement, whereas adding positive examples drastically improves performance. Table UID9 shows results with size normalised precision (P), recall (R), and F1-Score (F1). The five lowest P, R and F1 values per method (CRFSuite, Stanford NER, SENNA) are in bold to highlight underperformers. Results for all corpora are summed with macro average. Comparing the different methods, the highest F1 results are achieved",
        "type": "Document"
      },
      {
        "id": "5bb499ba-f0bd-4a1f-a76e-bcaeb2776d8b",
        "metadata": {
          "chunk_id": 105,
          "document_id": "1701.09123",
          "start_idx": 39639,
          "end_idx": 40079
        },
        "page_content": " art performance with respect to the best system published up to date BIBREF24 using public data.  BIBREF24 also report 78.20 F1 with a model trained with Clark clusters induced using the Huge German Corpus (HGC). Unfortunately, the corpus or the induced clusters were not available. The best system up to date on the CoNLL 2002 dataset, originally published by BIBREF47 , is distributed as part of the Freeling library BIBREF34 . Table TAB",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are two strong baseline methods authors refer to?",
    "llm_answer": "Baseline 1 and Baseline 2.\n",
    "context": [
      {
        "id": "94c43990-47e3-478c-a415-088eeac582b5",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1804.08782",
          "start_idx": 13990,
          "end_idx": 14427
        },
        "page_content": " number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 .  For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose",
        "type": "Document"
      },
      {
        "id": "0d211bed-e5df-4233-8d02-e24d7ee9b65c",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1909.02635",
          "start_idx": 9050,
          "end_idx": 9548
        },
        "page_content": " (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo",
        "type": "Document"
      },
      {
        "id": "9747c688-beb0-423e-95ef-7dc653615c9e",
        "metadata": {
          "chunk_id": 83,
          "document_id": "1902.00821",
          "start_idx": 25453,
          "end_idx": 25621
        },
        "page_content": " that the pre-tuning approach is highly effective and outperforms existing baselines or highly competitive with supervised baselines trained from a large-scale dataset.",
        "type": "Document"
      },
      {
        "id": "b7568416-2511-4e7d-b42a-905e8dea8f55",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1909.09484",
          "start_idx": 19263,
          "end_idx": 19734
        },
        "page_content": " models and baselines, with a learning rate of 0.001 for supervised training and 0.0001 for reinforcement learning. In reinforcement learning, the decay parameter $\\lambda $ is set to 0.8. The weight decay is set to 0.001. And early stopping is performed on developing set. The experimental results of the proposed model and baselines will be analyzed from the following aspects. BPRA Results: As shown in Table TABREF35, most of the models have similar performance on BP",
        "type": "Document"
      },
      {
        "id": "de359d02-162c-4112-8cc5-9d2fb3b54bb8",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1712.03547",
          "start_idx": 12435,
          "end_idx": 12963
        },
        "page_content": " the baseline method. In this work, we proposed a method for inducing interpretability in KG embeddings using a coherence regularization term. We evaluated the proposed and the baseline method on the interpretability of the learned embeddings. We also evaluated the methods on different KG tasks and compared their performance. We found that the proposed method achieves better interpretability while maintaining comparable performance on KG tasks. As next steps, we plan to evaluate the generalizability of the method with more",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many category tags are considered?",
    "llm_answer": "100\n",
    "context": [
      {
        "id": "ae12651f-2274-47c1-a71b-104297de9a4c",
        "metadata": {
          "chunk_id": 56,
          "document_id": "1707.02377",
          "start_idx": 20619,
          "end_idx": 21158
        },
        "page_content": " 300,000 wikipedia pages in 100 categories. The 100 categories includes categories under sports, entertainment, literature, and politics etc. Examples of categories include American drama films, Directorial debut films, Major League Baseball pitchers and Sydney Swans players. Body texts (the second paragraph) were extracted for each page as a document. For each category, we select 1,000 documents with unique category label, and 100 documents were used for training and 900 documents for testing. The remaining documents are used as unl",
        "type": "Document"
      },
      {
        "id": "be3d02d9-8369-42a0-9c16-c7d0d25ed47a",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1909.00154",
          "start_idx": 5332,
          "end_idx": 5844
        },
        "page_content": " beyond a few dozens. This means that, for example, it becomes clear later that the least number of original categories, the less the benefit of embeddings (in the limit, a binary variable like gender, is useless to do embeddings with), and also that if we do get a significantly large and statistically representative dataset, a dummy variables representation is sufficient. We will quickly see, however, that complexity can grow quick enough to justify an embeddings based method even if without the shockingly",
        "type": "Document"
      },
      {
        "id": "231182ec-4bc7-467b-b090-a1ee419284f3",
        "metadata": {
          "chunk_id": 43,
          "document_id": "2002.08899",
          "start_idx": 15442,
          "end_idx": 15949
        },
        "page_content": " and 6007 pairs, respectively. There are 9243 tokens in the input dictionary and 9486 tokens in the output dictionary. Figure FIGREF6 (c) shows some weights for four input words. They all highlight the relevant terminal, and syntactic categories that are usually associated with that word. The associated categories typically are either those of that word, the phrases headed by the category of that word, or those that select or are selected by that word. The relevant nonterminal terminology is as follows",
        "type": "Document"
      },
      {
        "id": "d925f445-4e2a-4f16-b244-98172bb0f2fc",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1909.02776",
          "start_idx": 25195,
          "end_idx": 25652
        },
        "page_content": " removal, and part of speech tagging is performed using the Hazm library BIBREF43. The majority of features have a range between zero and one. Other features are passed to a min-max scaler to transform into the same range. For the category feature which is nominal, the one-hot-encoding method applied and six flag features used instead. In assigning the target to a sentence, as mentioned in section (SECREF16), the goal is to assign a number between 0 and",
        "type": "Document"
      },
      {
        "id": "7eaf1e26-8770-4303-b824-58ffb2489ad7",
        "metadata": {
          "chunk_id": 40,
          "document_id": "1909.13375",
          "start_idx": 13084,
          "end_idx": 13439
        },
        "page_content": "B\\,O}$,$\\quad $ $\\mathtt {O\\,B\\,O\\,B}$. The number of correct tag sequences can be expressed by where $s$ is the number of spans in the answer and $\\#_i$ is the number of times the $i^\\text{th}$ span appears in the text. For questions with a reasonable amount of correct tag sequences, we generate all of them before the training starts. However, there is",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What domain does the dataset fall into?",
    "llm_answer": "The provided texts describe multiple domains, including Lifestyle, patent, scientific, and Wikipedia.  No single dataset is definitively identified.\n",
    "context": [
      {
        "id": "86a8b5b4-502a-47cd-be33-77fc57e26423",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1612.06897",
          "start_idx": 12548,
          "end_idx": 13016
        },
        "page_content": "domain data. Figure 2 shows the learning curve of the ensemble of the baseline and the continue model for different sizes of in-domain training data. The used in-domain data is a randomly selected subset of the entire pool of the in-domain data available to us. We also report the result when all of the in-domain data in the pool is used. As shown in Figure 2 the translation quality of the out-of-domain test set only degrades slightly for all the different sizes of",
        "type": "Document"
      },
      {
        "id": "8348e336-9079-412a-a078-faa11a754824",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1907.11499",
          "start_idx": 6351,
          "end_idx": 6867
        },
        "page_content": " at hand. For example, the definition of the \u201cLifestyle\u201d domain is \u201cthe interests, opinions, behaviors, and behavioral orientations of an individual, group, or culture\u201d. Figure FIGREF5 provides an overview of our Domain Detection Network, which we call DetNet. The model comprises two modules; an encoder learns representations for words and sentences whilst incorporating prior domain information; a detector generates domain scores for words, sentences, and documents by selectively attending to previously encoded",
        "type": "Document"
      },
      {
        "id": "e9ff0ea6-7735-4c7f-b973-c089ea86b236",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1701.03214",
          "start_idx": 4004,
          "end_idx": 4439
        },
        "page_content": "-of-domain data is used for the in-domain data; while for the \u201cmulti domain\u201d and \u201cmixed fine tuning\u201d methods, we use a vocabulary obtained from the mixed in-domain and out-of-domain data for all the training stages. We conducted NMT domain adaptation experiments in two different settings as follows: Chinese-to-English translation was the focus of the high quality in-domain corpus setting. We utilized the resource rich patent out-of",
        "type": "Document"
      },
      {
        "id": "2f68aa7d-78b4-43fd-8c08-5e5aee046062",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1811.01734",
          "start_idx": 0,
          "end_idx": 376
        },
        "page_content": " Domain shift is a fundamental problem in machine learning, that has attracted a lot of attention in the natural language processing and vision communities BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 . To understand and address this problem, generated by the lack of labeled data in a target domain, researchers",
        "type": "Document"
      },
      {
        "id": "319c9757-9bd8-4aed-9974-60604182d684",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1701.03214",
          "start_idx": 5368,
          "end_idx": 5787
        },
        "page_content": " (essentially open) in-domain data. The scientific domain MT was conducted on the Chinese-Japanese paper excerpt corpus (ASPEC-CJ) BIBREF11 , which is one subtask of the workshop on Asian translation (WAT) BIBREF15 . The ASPEC-CJ task uses 672315, 2090, and 2107 sentences for training, development, and testing, respectively. The Wikipedia domain task was conducted on a Chinese-Japanese corpus automatically extracted",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are EAC evaluated?",
    "llm_answer": "EAC performance is evaluated using qualitative and quantitative assessment techniques.  Qualitative assessment uses ISO 9241, while quantitative analysis uses unspecified techniques.\n",
    "context": [
      {
        "id": "0030d502-72b9-4749-b878-c54ba95f8eec",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1906.09774",
          "start_idx": 20313,
          "end_idx": 20820
        },
        "page_content": " and found several resources from the old one such as LIWC and Emolex to the new one, including DepecheMood and EmoWordNet. In the final part, we gather information about how to evaluate the performance of EAC, and we can classify the approach into two techniques, including qualitative and quantitative assessment. For qualitative assessment, most studies used ISO 9241, which covers several aspects such as efficiency, effectiveness, and satisfaction. While in quantitative analysis, two techniques can be",
        "type": "Document"
      },
      {
        "id": "966c6bbd-1522-4b70-b885-a8155d75cb5f",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1906.09774",
          "start_idx": 19203,
          "end_idx": 19656
        },
        "page_content": " how to evaluate EAC performance. The rise of EAC was started by Parry, which uses a simple rule-based approach. Now, most of EAC are built by using a neural-based approach, by exploiting emotion classifier to detect emotion contained in the text. In the modern era, the development of EAC gains more attention since Emotion Generation Challenge shared task on NLPCC 2017. In this era, most EAC is developed by adopting encoder-decoder architecture with",
        "type": "Document"
      },
      {
        "id": "d096b07d-50c2-4c04-a8c7-93b6297c9c1f",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1906.09774",
          "start_idx": 6366,
          "end_idx": 6844
        },
        "page_content": " section, we will try to review previous works which focus on EAC development. Table TABREF10 summarizes this information includes the objective and exploited approach of each work. In early development, EAC is designed by using a rule-based approach. However, in recent years mostly EAC exploit neural-based approach. Studies in EAC development become a hot topic start from 2017, noted by the first shared task in Emotion Generation Challenge on NLPCC 2017 BIBREF31 . Based on",
        "type": "Document"
      },
      {
        "id": "18d485b0-b1af-4609-847c-b7449df93c05",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1910.10408",
          "start_idx": 12581,
          "end_idx": 13061
        },
        "page_content": "Enc) models are evaluated in a length matching condition, i.e. output length has to match input length. We report the relative (Rel) and absolute (Abs) strategies of the approach as discussed in Section SECREF10. In the small data condition, we additionally evaluated how the fine-tuning strategy compares with a model trained from scratch. In the large data condition, we added a setting that combines both the length-token and length-encoding strategies. To evaluate all models'",
        "type": "Document"
      },
      {
        "id": "8f888341-3325-42c2-bd2f-c3e6c0767e9e",
        "metadata": {
          "chunk_id": 33,
          "document_id": "1906.09774",
          "start_idx": 13504,
          "end_idx": 14033
        },
        "page_content": "action aspect has three categories, including affect, ethics and behaviour, and accessibility. Affect is the most suitable assessment categories for EAC. This category asses several quality aspects such as, chatbots' ability to convey personality, give conversational cues, provide emotional information through tone, inflexion, and expressivity, entertain and/or enable the participant to enjoy the interaction and also read and respond to moods of human participant BIBREF59 . Ethic and behaviour category focuses on how a chat",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is triangulation?",
    "llm_answer": "A method to extend MUSE dictionaries to an additional 2352 lexicons covering 49 languages.\n",
    "context": [
      {
        "id": "2167ca55-cc12-4e74-a1f2-528ce01c767a",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1601.01705",
          "start_idx": 1745,
          "end_idx": 2146
        },
        "page_content": " is the bird? might be answered in two steps: first, \u201cwhere is the bird?\u201d (fig:examplesa), second, \u201cwhat color is that part of the image?\u201d (fig:examplesc). This first step, a generic module called find, can be expressed as a fragment of a neural network that maps from image features and a lexical item (here bird) to a distribution over pixels. This operation is commonly referred to as the attention",
        "type": "Document"
      },
      {
        "id": "d2076894-bb39-4702-8a7d-1720193fcc99",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1911.03058",
          "start_idx": 3793,
          "end_idx": 4334
        },
        "page_content": " target embedding spaces mildly correlates with performance, as does typological distance (a measure of language similarity based on language family membership trees). For distant languages, multilingual systems should in most cases be preferred over bilingual ones. We provide resources for training and evaluation on non-Anglocentric language pairs. We outline a simple triangulation method with which we extend the MUSE dictionaries to an additional 2352 lexicons covering 49 languages, and we present results on a subset of them. We also",
        "type": "Document"
      },
      {
        "id": "7dbd5a00-c099-41fe-8f5b-33989605f3fa",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1705.03151",
          "start_idx": 16622,
          "end_idx": 17163
        },
        "page_content": " language labels), leading to a strong DNN model for phonetic feature extraction. Importantly, phone discrimination and language identification are naturally correlated (from our phonetic perspective), which means that the phonetic features learned with the strong phone/word supervision involves rich information suitable for LID. This is an example of transfer learning, where a related task (i.e., phone discrimination) is used to learn features for another task (LID). The PTN approach also involves another two transfer learning schemes",
        "type": "Document"
      },
      {
        "id": "8bee0943-ac92-49c1-a597-8c341773116e",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1910.09399",
          "start_idx": 4013,
          "end_idx": 4538
        },
        "page_content": " computer vision field specifically BIBREF8, BIBREF12. Nowadays, researchers are attempting to solve a plethora of computer vision problems with the aid of deep convolutional networks, generative adversarial networks, and a combination of multiple methods, often called multimodal learning methods BIBREF8. For simplicity, multiple learning methods will be referred to as multimodal learning hereafter BIBREF13. Researchers often describe multimodal learning as a method that incorporates characteristics from several methods",
        "type": "Document"
      },
      {
        "id": "c2ebac5c-a31a-4b1f-9690-5a465b8abd84",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1807.08204",
          "start_idx": 7478,
          "end_idx": 7955
        },
        "page_content": " a nearest neighbour search problem. In this work, we use ANNS during the forward pass for considering only the most promising proof paths during the construction of the neural network. From ex:inference, we can see that the inference problem can be reduced to a nearest neighbour search problem. Given a query $g$ , the problem is finding the fact(s) in $\\mathcal {K}$ that maximise the unification score. This represents a computational bottleneck, since it is very costly to",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What languages do they use?",
    "llm_answer": "Amharic, Arabic, Hindi, Somali, English, Spanish, French, Italian, German, Russian, Basque, French, German, Hungarian, Italian, Navajo, Turkish, Quechua, English, Spanish, Zulu, Russian, Polish, Norwegian, Romanian, Dutch, Czech, Turkish, Thai, Urdu,  English, French, German.\n",
    "context": [
      {
        "id": "47156113-1a5a-4cd2-8111-284623be4586",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1909.09270",
          "start_idx": 13903,
          "end_idx": 14242
        },
        "page_content": "2003 shared tasks BIBREF21, BIBREF22. These are taken from newswire text, and have labelset of Person, Organization, Location, Miscellaneous. The remaining four languages come from the LORELEI project BIBREF23. These languages are: Amharic (amh: LDC2016E87), Arabic (ara: LDC2016E89), Hindi (hin: LDC2017E62), and Somali (som: LDC2016E91).",
        "type": "Document"
      },
      {
        "id": "e8fb7d0e-86e1-4332-bb4a-d8109e4146e4",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1910.04269",
          "start_idx": 19553,
          "end_idx": 20023
        },
        "page_content": ", the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Kat",
        "type": "Document"
      },
      {
        "id": "10b18f90-1375-4a41-9719-9a76bdd2287c",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1910.05456",
          "start_idx": 1974,
          "end_idx": 2423
        },
        "page_content": " on the L1, i.e., the \"native language\", in neural network models. To this goal, we select a diverse set of eight source languages from different language families \u2013 Basque, French, German, Hungarian, Italian, Navajo, Turkish, and Quechua \u2013 and three target languages \u2013 English, Spanish and Zulu. We pretrain a neural sequence-to-sequence architecture on each of the source languages and then fine-tune the resulting models on small datasets in each",
        "type": "Document"
      },
      {
        "id": "2aa9fa8e-cc32-4080-a0e8-1d95bb3f60c0",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1911.03562",
          "start_idx": 17539,
          "end_idx": 18028
        },
        "page_content": " as Russian, Polish, Norwegian, Romanian, Dutch, and Czech) and Asian languages (such as Turkish, Thai, and Urdu). Most of the well-represented languages are from the Indo-European language family. Yet, even in the limited landscape of the most common 122 languages, vast swathes are barren with inattention. Notable among these is the extremely low representation of languages from Africa, languages from non-Indo-European language families, and Indigenous languages from around the world",
        "type": "Document"
      },
      {
        "id": "7da029d3-0e6a-48d3-9dd2-3cfd37690294",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1910.04269",
          "start_idx": 6853,
          "end_idx": 7342
        },
        "page_content": " comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How they evaluate their approach?",
    "llm_answer": "Evaluation using cross-domain scenarios, comparing estimates with actual attack effects, studying adversarial attack effects on evaluation metrics, exploring application to KG representation interpretability, detecting incorrect triples, and leave-one evaluation.  Human evaluation for NLG, and examining the impact of imperfect metrics on training NLG models.  Also, evaluation of the quality of approximations and comparison with influence function (IF).\n",
    "context": [
      {
        "id": "1f140aea-67fe-42b4-983e-2e4e083c362a",
        "metadata": {
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "0b64308f-c420-4f0a-b1a3-2e2d3aee5598",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1905.00563",
          "start_idx": 13447,
          "end_idx": 13944
        },
        "page_content": " evaluate by ( \"Influence Function vs \" ) comparing estimate with the actual effect of the attacks, ( \"Robustness of Link Prediction Models\" ) studying the effect of adversarial attacks on evaluation metrics, ( \"Interpretability of Models\" ) exploring its application to the interpretability of KG representations, and ( \"Finding Errors in Knowledge Graphs\" ) detecting incorrect triples. To evaluate the quality of our approximations and compare with influence function (IF), we conduct leave one",
        "type": "Document"
      },
      {
        "id": "5155558a-dfb3-4b8b-9585-41ac83603580",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1601.02543",
          "start_idx": 845,
          "end_idx": 1419
        },
        "page_content": " successful and unsuccessful transactions. This evaluation is then used to identifying any further improvement in the speech recognition based solution to better the overall transaction completion rates. This process of evaluation is both time consuming and expensive. For evaluation one needs to identify a set of users and also identify the set of actual usage situations and perform the test. It is also important that the set of users are able to use the system with ease meaning that even in the test conditions the performance of the system, should be good, while this",
        "type": "Document"
      },
      {
        "id": "e12d7235-2654-4688-a6b5-218fe9724fb5",
        "metadata": {
          "chunk_id": 68,
          "document_id": "2002.05058",
          "start_idx": 26990,
          "end_idx": 27517
        },
        "page_content": " NLG evaluation. We further investigate the impact of imperfect metrics on training NLG models. As described in the human evaluation procedure, we perform 10 runs to test the reliability of each metric when used to perform hyperparameter tuning and early-stopping respectively. In each run, we select the best hyperparameter combination or early-stopping checkpoint based on each of the five compared metrics. Human evaluation is then employed to identify the best choice. We evaluate the performance of each metric by how many",
        "type": "Document"
      },
      {
        "id": "7ee3d1d0-5ed5-48f9-a7bc-3e689b448d34",
        "metadata": {
          "chunk_id": 275,
          "document_id": "1804.08186",
          "start_idx": 98870,
          "end_idx": 99381
        },
        "page_content": " evaluation in research to date. In this section, we examine the literature more closely, providing a broad overview of the evaluation metrics that have been used, as well as the experimental settings in which research has been evaluated. The most common approach is to treat the task as a document-level classification problem. Given a set of evaluation documents, each having a known correct label from a closed set of labels (often referred to as the \u201cgold-standard\u201d), and a predicted label for each document",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How large is the corpus?",
    "llm_answer": "13 million NRLS records.\n",
    "context": [
      {
        "id": "60fa7e17-1f20-4b48-a302-920cbf9c8b19",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1909.00183",
          "start_idx": 13387,
          "end_idx": 13902
        },
        "page_content": " of different sizes and content with different sets of hyper-parameters. . In particular, we checked the effect of corpus size by training Doc2Vec models on the full 13 million NRLS records and on randomly sampled subsets of 1 million and 2 million records. Since our target analysis has heavy medical content and specific use of words, we also tested the importance of the training corpus by generating an additional Doc2Vec model using a set of 5 million articles from the English Wikipedia representing standard,",
        "type": "Document"
      },
      {
        "id": "e5d603c3-2ea7-41fe-b1fe-3c2c52ebef23",
        "metadata": {
          "chunk_id": 40,
          "document_id": "1709.05404",
          "start_idx": 14931,
          "end_idx": 15431
        },
        "page_content": " consists of 582 posts per class (1,164 posts in total). To recap, Table TABREF10 summarizes the total number of posts for each subset of our final corpus. Our primary goal is not to optimize classification results, but to explore how results vary across different subcorpora and corpus properties. We also aim to demonstrate that the quality of our corpus makes it more straightforward to achieve high classification performance. We apply both supervised learning using SVM (from Scikit-Learn BIBREF",
        "type": "Document"
      },
      {
        "id": "02741ee7-8cb4-47ea-8819-33c8248da5db",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1911.12579",
          "start_idx": 13025,
          "end_idx": 13544
        },
        "page_content": " the corpus provides quantitative, reusable data, and an opportunity to examine intuitions and ideas about language. Therefore, the corpus has great importance for the study of written language to examine the text. In fact, realizing the necessity of large text corpus for Sindhi, we started this research by collecting raw corpus from multiple web resource using web-scrappy framwork for extraction of news columns of daily Kawish and Awami Awaz Sindhi newspapers, Wikipedia dumps, short stories and sports news from W",
        "type": "Document"
      },
      {
        "id": "6ca9c184-80d3-4aa2-b722-d0288073e955",
        "metadata": {
          "chunk_id": 40,
          "document_id": "2001.08051",
          "start_idx": 15428,
          "end_idx": 15968
        },
        "page_content": " assigned either to training or evaluation sets, with proportions of $\\frac{2}{3}$ and $\\frac{1}{3}$, respectively; then training and evaluation lists were built, accordingly. Table reports statistics from the spoken data set. The id All identifies the whole data set, while Clean defines the subset in which sentences containing background voices, incomprehensible speech and word fragments were excluded. From the above description it appears that the corpus can be effectively used in many research directions. The spoken corpus features",
        "type": "Document"
      },
      {
        "id": "b0e90a5f-0559-4f41-b2d2-1e48769396e5",
        "metadata": {
          "chunk_id": 76,
          "document_id": "1603.04513",
          "start_idx": 24922,
          "end_idx": 25308
        },
        "page_content": "NA. Size: 130,000 word embeddings; training corpus: Wikipedia. Note that we use their 50-dimensional embeddings. (v) Word2Vec. It has no 50-dimensional embeddings available online. We use released code to train skip-gram on English Gigaword Corpus BIBREF26 with setup: window size 5, negative sampling, sampling rate $10^{-3}$ , threads 12. It is worth emphasizing that above embeddings",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How large is the dataset?",
    "llm_answer": "The answer is not found in the context.  While various datasets and their sizes are mentioned, there is no single answer to \"how large is the dataset.\"\n",
    "context": [
      {
        "id": "9ccb7945-9a96-417a-b52e-68855c4910b1",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "2d3bdfec-4ed5-456d-bd82-023fad798858",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "6f04ea61-adc5-4abb-9261-abc554ebac1b",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1711.02013",
          "start_idx": 20658,
          "end_idx": 21116
        },
        "page_content": "ings, between recurrent layers, and on recurrent states were (0.7, 0.5, 0.5) respectively. dataset contains 17M training tokens and has a vocabulary size of 44k words. The dataset is partitioned into a training set (first 99M characters) and a development set (last 1M characters) that is used to report performance. As this dataset contains various articles from Wikipedia, the longer term information (such as current topic) plays a bigger role than in the",
        "type": "Document"
      },
      {
        "id": "6b7f6ca6-24d5-4144-9cfd-92be2de31f39",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "d6a5600d-c121-4e03-bad7-e6b2fee5b98f",
        "metadata": {
          "chunk_id": 294,
          "document_id": "1804.08186",
          "start_idx": 106599,
          "end_idx": 107101
        },
        "page_content": " whether there is a universal quantity of data that is \u201cenough\u201d for any language, rather this amount appears to be affected by the particular set of languages as well as the domain of the data. The breakdown presented by BIBREF32 shows that with less than 100KB per language, there are some languages where classification accuracy is near perfect, whereas there are others where it is very poor. Another aspect that is frequently reported on is how long a sample of text needs to be before its language",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was their perplexity score?",
    "llm_answer": "The answer is not found in the context.  While perplexity scores are discussed, no specific score is given.\n",
    "context": [
      {
        "id": "f4e21ae9-bea0-4fc9-84f7-861bf90ec947",
        "metadata": {
          "chunk_id": 58,
          "document_id": "1908.07816",
          "start_idx": 20270,
          "end_idx": 20712
        },
        "page_content": " the perplexity scores obtained by the three models on the two validation sets and the test set. As shown in the table, MEED achieves the lowest perplexity score on all three sets. We also conducted t-test on the perplexity obtained, and results show significant improvements (with $p$-value $<0.05$). Table TABREF34, TABREF35 and TABREF35 summarize the human evaluation results on the responses' grammatical correctness, contextual coherence",
        "type": "Document"
      },
      {
        "id": "97170c01-d120-4f5e-a3bf-3c01eb86996d",
        "metadata": {
          "chunk_id": 57,
          "document_id": "1908.07816",
          "start_idx": 20024,
          "end_idx": 20472
        },
        "page_content": " or not the response is context sensitive to the previous dialog history; (3) emotional appropriateness\u2014whether or not the response conveys the right emotion and feels as if it had been produced by a human. For each criterion, the raters gave scores of either 0, 1 or 2, where 0 means bad, 2 means good, and 1 indicates neutral. Table TABREF34 gives the perplexity scores obtained by the three models on the two validation sets and the test set. As",
        "type": "Document"
      },
      {
        "id": "d8bf1286-2623-48dd-8ccd-470f81f0add8",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1909.00578",
          "start_idx": 7983,
          "end_idx": 8378
        },
        "page_content": " the sentence-level perplexity of each summary: where $p(s_i|s_{i-1})$ is the probability that BERT assigns to the sequence of sentences $\\left< s_{i-1}, s \\right>$, and $n$ is the number of sentences in the peer summary. To evaluate our methods for a particular $\\mathcal {Q}$, we calculate the average of the predicted scores for the summaries of each particular contestant, and the average of",
        "type": "Document"
      },
      {
        "id": "6ecf5615-8852-471c-a5e6-9fd5b139ffe6",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1811.00942",
          "start_idx": 6450,
          "end_idx": 6778
        },
        "page_content": "/3$ for all $P(\\text{``choo''}) \\le 0.5$ ; thus, perplexity can drop as low as 2 without affecting recall. We conducted our experiments on Penn Treebank (PTB; BIBREF12 ) and WikiText-103 (WT103; BIBREF13 ). Preprocessed by BIBREF14 , PTB contains 887K tokens for training, 70K for validation, and 78K for test, with a vocabulary",
        "type": "Document"
      },
      {
        "id": "aeb9fac2-f26f-4f49-b66e-0a0e6db10912",
        "metadata": {
          "chunk_id": 61,
          "document_id": "2004.03744",
          "start_idx": 22301,
          "end_idx": 22694
        },
        "page_content": "55% on the test set. As reported in Table TABREF30, the overall PaE-BUTD-VE system achieves 69.40% balanced accuracy on the test set of e-SNLI-VE-2.0, which is a 3% decrease from the non-explanatory BUTD counterpart (72.52%). However, by setting $\\alpha $ to zero and selecting the model that gives the best perplexity per word at validation, the quality of explanation significantly increased",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What parallel corpus did they use?",
    "llm_answer": "The parallel corpora from the shared translation task of WMT'15 and WMT'16 were used.  Also, Fr*-De and Fr-De* corpora.\n",
    "context": [
      {
        "id": "f609810e-49b2-457f-b910-d22ee7b6b3de",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1704.00253",
          "start_idx": 6591,
          "end_idx": 7101
        },
        "page_content": " parallel corpus. In their work, the pseudo parallel corpus was employed in fine-tuning the target-specific attention mechanism of trained multi-way multilingual NMT BIBREF17 models, which enabled zero-resource NMT between the source and target languages. Lastly, synthetic sentence pairs have been utilized to enrich the training examples having rare or unknown translation lexicons BIBREF4 . As described in the previous section, synthetic parallel data have been widely used to boost the performance of NMT.",
        "type": "Document"
      },
      {
        "id": "5254b27e-1ebe-4049-8699-d0767f294567",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1909.00279",
          "start_idx": 13718,
          "end_idx": 14228
        },
        "page_content": " models, please refer to supplementary materials. Training and Validation Sets We collected a corpus of poems and a corpus of vernacular literature from online resources. The poem corpus contains 163K quatrain poems from Tang Poems and Song Poems, the vernacular literature corpus contains 337K short paragraphs from 281 famous books, the corpus covers various literary forms including prose, fiction and essay. Note that our poem corpus and a vernacular corpus are not aligned. We further split the two corpor",
        "type": "Document"
      },
      {
        "id": "ddc891f7-8efa-46f5-af8a-dd20a1375322",
        "metadata": {
          "chunk_id": 53,
          "document_id": "1709.01256",
          "start_idx": 19942,
          "end_idx": 20473
        },
        "page_content": "e.g. addition, deletion or replacement), and save them as separate documents. These documents become revisions of the original documents. We started from an initial corpus that did not have revisions, and kept adding new documents and revising existing documents. Similar to a file system, at any moment new documents could be added and/or some of the current documents could be revised. The revision operations we used were deletion, addition and replacement of words, sentences, paragraphs, section names and document titles. The",
        "type": "Document"
      },
      {
        "id": "6be09554-8641-4d7e-83a1-03b2bcaf1f27",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1704.00253",
          "start_idx": 7425,
          "end_idx": 7925
        },
        "page_content": " ground truth parallel corpus is established, the trained model can be improved by retraining or fine-tuning using the real parallel data. For a given translation task, we classify the existing pseudo parallel data into the following groups: Source-originated: The source sentences are from a real corpus, and the associated target sentences are synthetic. The corpus can be formed by automatically translating a source-side monolingual corpus into the target language BIBREF4 , BIBREF1 . It can also",
        "type": "Document"
      },
      {
        "id": "d9b75109-5ba8-4c94-a15c-10e7cae66914",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1704.00253",
          "start_idx": 12296,
          "end_idx": 12697
        },
        "page_content": "-Fr parallel corpus BIBREF19 . A beam of size 5 is used to generate synthetic sentences. Lastly, to match the size of the training data, PSEUDOmix is established by randomly sampling half of each Fr*-De and Fr-De* corpus and mixing them together. We use the parallel corpora from the shared translation task of WMT'15 and WMT'16 BIBREF27 . Using the same pivot-based technique as the previous task, Cs",
        "type": "Document"
      }
    ]
  },
  {
    "query": "In which setting they achieve the state of the art?",
    "llm_answer": "SemEval-2015 and SemEval-2016.\n",
    "context": [
      {
        "id": "76185514-2b57-42c0-acac-3fa0a8a69f7b",
        "metadata": {
          "chunk_id": 101,
          "document_id": "1908.02402",
          "start_idx": 29500,
          "end_idx": 29733
        },
        "page_content": "-the-art models, while at the same time providing a more practical solution for real-world applications. We would like to thank Alexandros Papangelis, Janice Lam, Stefan Douglas Webb and SIGDIAL reviewers for their valuable comments.",
        "type": "Document"
      },
      {
        "id": "1f140aea-67fe-42b4-983e-2e4e083c362a",
        "metadata": {
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "e80993a9-7aed-4f4f-8185-08683755466a",
        "metadata": {
          "chunk_id": 91,
          "document_id": "1911.03562",
          "start_idx": 34938,
          "end_idx": 35474
        },
        "page_content": " since 2010. System demo papers have the third highest average citations; however, shared task papers have the third highest median citations. The popularity of shared tasks and the general importance given to beating the state of the art (SOTA) seems to have grown in recent years\u2014something that has come under criticism. It is interesting to note that in terms of citations, workshop papers are doing somewhat better than the conferences that are not top tier. Finally, the citation numbers for tutorials show that even though a small",
        "type": "Document"
      },
      {
        "id": "ed385b18-6171-42af-9672-281f1df69e11",
        "metadata": {
          "chunk_id": 53,
          "document_id": "1710.01492",
          "start_idx": 19472,
          "end_idx": 19941
        },
        "page_content": " particular, the best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks BIBREF53 , BIBREF54 . Conversely, kernel machines seem to be less frequently used than in the past, and the use of learning methods other than the ones mentioned above is at this point scarce. All these models are examples of supervised learning as they need labeled training data. Semi-supervised learning. We should note two things about the use of deep neural",
        "type": "Document"
      },
      {
        "id": "a4a4c58b-d1f1-46a1-9912-24f7a070ad68",
        "metadata": {
          "chunk_id": 43,
          "document_id": "2001.00137",
          "start_idx": 16781,
          "end_idx": 17197
        },
        "page_content": " the online platforms. Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What they formulate the question generation as?",
    "llm_answer": "The context does not state what they formulate question generation *as*.\n",
    "context": [
      {
        "id": "8a7b8a56-bb48-415c-afba-38c02e4059a8",
        "metadata": {
          "chunk_id": 71,
          "document_id": "1905.08949",
          "start_idx": 26238,
          "end_idx": 26651
        },
        "page_content": "generated questions also overgeneralize \u2014 a model asks \u201cWho was born in New York?\" when given the triple (Donald_Trump, Place_of_birth, New_York). To solve this, BIBREF29 enrich the input with a sequence of keywords collected from its related triples. Visual Question Generation (VQG) is another emerging topic which aims to ask questions given an image. We categorize VQG into grounded- and open-ended VQG by the",
        "type": "Document"
      },
      {
        "id": "4c0ece65-4895-4377-9eaa-e70341f90ff2",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1905.08949",
          "start_idx": 0,
          "end_idx": 442
        },
        "page_content": "Question Generation (QG) concerns the task of \u201cautomatically generating questions from various inputs such as raw text, database, or semantic representation\" BIBREF0 . People have the ability to ask rich, creative, and revealing questions BIBREF1 ; e.g., asking Why did Gollum betray his master Frodo Baggins? after reading the fantasy novel The Lord of the Rings. How can machines be endowed with the ability to ask relevant and to-the-point",
        "type": "Document"
      },
      {
        "id": "3dd9b003-f969-416c-b5ec-6cd3444bb0d8",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1905.08949",
          "start_idx": 15932,
          "end_idx": 16357
        },
        "page_content": "-question should be triggered for answer \u201cthe end of the Mexican War\" while a why-question is generated by the model. A few works BIBREF49 , BIBREF53 considered question word generation separately in model design.  BIBREF49 proposed to first generate a question template that contains question word (e.g., \u201chow to #\", where # is the placeholder), before generating the rest of the question. To this end, they train two Seq2Se",
        "type": "Document"
      },
      {
        "id": "ed5a2c5e-5fd9-466d-ba41-60821a2f5407",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1905.08949",
          "start_idx": 3681,
          "end_idx": 4181
        },
        "page_content": " by applying syntactic or semantic parsing, respectively, to obtain intermediate symbolic representations. Question construction then converts intermediate representations to a natural language question, taking either a tranformation- or template-based approach. The former BIBREF17 , BIBREF18 , BIBREF13 rearranges the surface form of the input sentence to produce the question; the latter BIBREF19 , BIBREF20 , BIBREF21 generates questions from pre-defined question templates. Unfortunately, such Q",
        "type": "Document"
      },
      {
        "id": "2edd80e4-e48d-40e4-826a-f5de55878ab4",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1806.04524",
          "start_idx": 2824,
          "end_idx": 3312
        },
        "page_content": " adapt. Our goal is to effectively provide an adaptive learning experience in terms of style and difficulty, and thus better serve users' needs BIBREF9 . In this context, we study the ability of our proposed architectures in learning to generate quizzes based on data derived of the interaction of users with the platform. The problem of fill-in-the-blank question generation has been studied in the past by several authors. Perhaps the earlies approach is by BIBREF1 , who proposed a clo",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Was the degree of offensiveness taken as how generally offensive the text was, or how personally offensive it was to the annotator?",
    "llm_answer": "The offensiveness was rated on a 6-point Likert scale.\n",
    "context": [
      {
        "id": "b11abae3-ba62-44cb-9abd-13408991f38c",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1910.03814",
          "start_idx": 11041,
          "end_idx": 11575
        },
        "page_content": " workers. We received a lot of valuable feedback from the annotators. Most of them had understood the task correctly, but they were worried because of its subjectivity. This is indeed a subjective task, highly dependent on the annotator convictions and sensitivity. However, we expect to get cleaner annotations the more strong the attack is, which are the publications we are more interested on detecting. We also detected that several users annotate tweets for hate speech just by spotting slur. As already said previously, just the",
        "type": "Document"
      },
      {
        "id": "d44fcdaf-3d02-421b-9439-d7c4e401ee7b",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1911.03842",
          "start_idx": 3980,
          "end_idx": 4444
        },
        "page_content": " offensive or maybe offensive, they were asked to place it in one of four categories \u2013 racist, sexist, classist, other \u2013 and to provide a reason for their response. Just over 2% of personas were flagged by at least one annotator, and these personas are removed from the dataset. We further examined gender bias in personas. Annotators were asked to label the gender of each character based on their persona description (choosing \u201cneutral\" if it was not explicit in",
        "type": "Document"
      },
      {
        "id": "8ae81789-a720-495a-a763-083905db7c1a",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1701.08118",
          "start_idx": 1750,
          "end_idx": 2264
        },
        "page_content": " the question may also play a role. We decided to investigate the issue of reliability further by conducting a more comprehensive study across a large number of annotators, which we present in this paper. Our contribution in this paper is threefold: For the purpose of building a classifier, warner2012 define hate speech as \u201cabusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation\u201d. More recent approaches rely on lists of guidelines such as a tweet",
        "type": "Document"
      },
      {
        "id": "4bad7962-5765-4bae-b845-9a98332715df",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1701.08118",
          "start_idx": 5758,
          "end_idx": 6241
        },
        "page_content": " which are duplicates, contain links or pictures, or are retweets or replies. As a first measurement of the frequency of hate speech in our corpus, we personally annotated them based on our previous expertise. The 541 tweets were split into six parts and each part was annotated by two out of six annotators in order to determine if hate speech was present or not. The annotators were rotated so that each pair of annotators only evaluated one part. Additionally the offensiveness of",
        "type": "Document"
      },
      {
        "id": "e1728c48-437d-4d62-8108-3af2826d753a",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1701.08118",
          "start_idx": 6241,
          "end_idx": 6722
        },
        "page_content": " were rotated so that each pair of annotators only evaluated one part. Additionally the offensiveness of a tweet was rated on a 6-point Likert scale, the same scale used later in the study. Even among researchers familiar with the definitions outlined above, there was still a low level of agreement (Krippendorff's INLINEFORM0 ). This supports our claim that a clearer definition is necessary in order to be able to train a reliable classifier. The low reliability could of course",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which embeddings do they detect biases in?",
    "llm_answer": "Mainstream, striking examples from the literature on embedding bias.\n",
    "context": [
      {
        "id": "4f4d00a4-8ac1-4640-a27a-ab117dde0431",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1905.09866",
          "start_idx": 7284,
          "end_idx": 7764
        },
        "page_content": " as woman is to doctor (where INLINEFORM1 ). This means that the bias isn't necessarily (or at least not only) in the representations themselves, rather in the way we query them. So, what do the embedding spaces actually tell if you let them return any word in the vocabulary? We took a selection of mainstream, striking examples from the literature on embedding bias, and tested them fairly, without posing any constraint on the returned term, exactly as we did for all analogies",
        "type": "Document"
      },
      {
        "id": "7cbff857-9d54-4c9b-a97c-186fc74c60fc",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1905.09866",
          "start_idx": 14595,
          "end_idx": 15095
        },
        "page_content": "ing bias in word embeddings, since bias is anyway often encoded implicitly. As a side note, we would like to mention that in an earlier version of their paper, BIBREF18 accidentally searched for the inverse of the intended query, and still managed to find biased examples. This seems to be a further, strong, indication that strategies like this are not fully suitable to demonstrate the presence of bias in embeddings. If analogies might not be the most appropriate tool to capture certain relations",
        "type": "Document"
      },
      {
        "id": "a695a356-c575-4837-bd2e-4be114c8fff5",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1912.05238",
          "start_idx": 12172,
          "end_idx": 12620
        },
        "page_content": "'s bias, e.g. debias the model's gender bias. They describe that the first step for debiasing word embeddings is to identify a direction (or, more generally, a subspace) of the embedding that captures the bias. To identify the gender subspace, e.g., they proposed to take the difference vectors of given gender pairs and computed its principal components (PCs) and found a single direction that explains the majority of variance in these vectors, i",
        "type": "Document"
      },
      {
        "id": "fc00a6d5-3872-497b-8577-8051f4234f30",
        "metadata": {
          "chunk_id": 33,
          "document_id": "1912.05238",
          "start_idx": 12620,
          "end_idx": 13093
        },
        "page_content": " (PCs) and found a single direction that explains the majority of variance in these vectors, i.e. the first eigenvalue is significantly larger than the rest. Therefore, they argue that the top PC, denoted by the unit vector $g$, captures the gender subspace. Subsequently, they debias the embedding based on this subspace. Please note that the gender pairs are labelled beforehand. Using the above-mentioned methodology, we propose an alternative to identify the moral bias",
        "type": "Document"
      },
      {
        "id": "9394dae3-301c-45a2-9eeb-db875d756340",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1905.09866",
          "start_idx": 2953,
          "end_idx": 3477
        },
        "page_content": " and danger of human biases in word embeddings. On the contrary: embeddings do encode human biases, and we believe that this issue deserves the full attention of the field. However, we also believe that overemphasising and specifically seeking biases to achieve sensational results is not beneficial. It is also not necessary: what we observe naturally is worrying and sensational enough. Rather, we should aim at transparency and experimental clarity so as to ensure the fairest and most efficient treatment of the problem.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the two PharmaCoNER subtasks?",
    "llm_answer": "The context describes two PharmaCoNER subtasks, but does not name them.  It only mentions that one is dealing with the annotation of entities and the other is about evaluating dataset quality and the role of resource-based systems in validation.\n",
    "context": [
      {
        "id": "1aebdb79-7029-40cb-83bf-c73392faa244",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1705.03261",
          "start_idx": 6526,
          "end_idx": 6896
        },
        "page_content": " the interested two drugs with \u201cdrug1\u201d and \u201cdrug2\u201d while the other drugs are replaced by \u201cdurg0\u201d, as in BIBREF9 did. This step is called drug blinding. For example, the sentence in figure FIGREF5 generates 3 instances after drug blinding: \u201cdrug1: an increased risk of hepatitis has been reported to result from combined use of drug2 and drug0\u201d, \u201cdrug1: an increased risk",
        "type": "Document"
      },
      {
        "id": "e2ea85c6-73a6-4002-a7ca-02f04eb57781",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1905.07464",
          "start_idx": 3665,
          "end_idx": 4136
        },
        "page_content": " modeling approach, our deep learning architecture, and our training procedure. Each drug label is a collection of sections (e.g., DOSAGE & ADMINISTRATION, CONTRAINDICATIONS, and WARNINGS) where each section contains one or more sentences. Each sentence is annotated with a list of zero or more mentions and interactions. The training data released for this task contains 22 drug labels, referred to as Training-22, with gold standard annotations. Two test sets of 57 and",
        "type": "Document"
      },
      {
        "id": "649a53ad-5660-4dcc-a30f-1f7ac62f6f6a",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1905.07464",
          "start_idx": 6875,
          "end_idx": 7289
        },
        "page_content": " and D, K, and U for precipitants with pharmacodynamic, pharmacokinetic, and unspecified interactions respectively. As a preprocesssing step, we identify the label drug in the sentence, if it is mentioned, and bind it to a generic entity token (e.g. \u201cLABELDRUG\u201d). We additionally account for label drug aliases, such as the generic version of a brand-name drug, and bind them to the same entity token. Table TABREF",
        "type": "Document"
      },
      {
        "id": "9c34df1d-9c0c-4e2c-b322-da19601573fc",
        "metadata": {
          "chunk_id": 54,
          "document_id": "1905.07464",
          "start_idx": 19985,
          "end_idx": 20442
        },
        "page_content": " We also implemented several tweaks to the pre-processing and post-processing aspects of the model based on preliminary error analysis including (1) using drug class mentions (e.g., \u201cdiuretics\u201d) as proxies if the drug label is not mentioned directly; (2) removing modifiers such as moderate, strong, and potent so that output conforms to official annotation guidelines; and (3) purging predicted mentions with only stopwords or generic terms such as \u201cdrugs\ufffd",
        "type": "Document"
      },
      {
        "id": "84d2f0a9-1ba7-4626-a20f-e34f63b824f4",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1912.09152",
          "start_idx": 2881,
          "end_idx": 3371
        },
        "page_content": " the PharmaCoNER shared task, dealing with the annotation of some of the nes in health records (namely, pharmacological, chemical and biomedical entities) using a revisited version of rule- and dictionary-based approaches; and, on the other hand, to give pause for thought about the quality of datasets (and, thus, the fairness) with which systems of this type are evaluated, and to highlight the key role of resource-based systems in the validation and consolidation of both the annotation",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they perform data augmentation?",
    "llm_answer": "Mirroring, random cropping, and replacement-based methods (replacing words with synonyms) are used.  Frequency masking, speed perturbation, and volume perturbation are also employed.\n",
    "context": [
      {
        "id": "4e648957-1d5f-4b42-8059-3a4cdb3ad840",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1812.06705",
          "start_idx": 0,
          "end_idx": 467
        },
        "page_content": "Deep neural network-based models are easy to overfit and result in losing their generalization due to limited size of training data. In order to address the issue, data augmentation methods are often applied to generate more training samples. Recent years have witnessed great success in applying data augmentation in the field of speech area BIBREF0 , BIBREF1 and computer vision BIBREF2 , BIBREF3 , BIBREF4 . Data augmentation in these areas can be easily performed",
        "type": "Document"
      },
      {
        "id": "33d60d77-675b-4566-a44e-1e020bd5e195",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1812.06705",
          "start_idx": 789,
          "end_idx": 1246
        },
        "page_content": " by mirroring we get \u201cdoog si srotca ehT\", or by random cropping we get \u201cactors is\", both of which are meaningless. Existing data augmentation methods for text are often loss of generality, which are developed with handcrafted rules or pipelines for specific domains. A general approach for text data augmentation is replacement-based method, which generates new sentences by replacing the words in the sentences with relevant words (e.g. synonyms). However",
        "type": "Document"
      },
      {
        "id": "9d6e4c30-d037-4a07-a951-0e7d1761e751",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1912.13109",
          "start_idx": 7657,
          "end_idx": 8136
        },
        "page_content": " above few data augmentation techniques were applied to boost the learning of the network. Before applying augmentation, a train-test split of 78%-22% was done from the original, cleansed data set. Thus, 700 tweets/messages were held out for testing. All model evaluation were done in on the test set that got generated by this process. The results presented in this report are based on the performance of the model on the test set. The training set of 2489 messages were however",
        "type": "Document"
      },
      {
        "id": "4b0ab329-3d40-4931-922e-92d0afe294bf",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1909.06522",
          "start_idx": 16023,
          "end_idx": 16474
        },
        "page_content": " of the data augmentation techniques described in Section SECREF3. As in Table TABREF11, the performance gains of using frequency masking were substantial and comparable to those of using speed perturbation, where $m_F = 2$ and $F=15$ (Section SECREF12) worked best. In addition, combining both frequency masking and speed perturbation could provide further improvements. However, applying additional volume perturbation (Section SECREF4) or time mask",
        "type": "Document"
      },
      {
        "id": "54164de4-0990-49de-81c7-61ff8332ba3e",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1909.06522",
          "start_idx": 3503,
          "end_idx": 4007
        },
        "page_content": " proceed with our investigation on various data augmentation techniques to overcome the lack of training data in the above low-resource setting. Given the highly scalable neural network acoustic modeling, extensive alternatives to increasing the amount or diversity of existing training data have been explored in prior works, e.g., applying vocal tract length perturbation and speed perturbation BIBREF18, volume perturbation and normalization BIBREF19, additive noises BIBREF20, reverberation BIBREF19,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the characteristics of the city dialect?",
    "llm_answer": "The city descriptions are relatively short, contain rare words, complex syntactic structures, and figurative language.\n",
    "context": [
      {
        "id": "0b0698b7-072c-4671-ab2a-4782b07b124b",
        "metadata": {
          "chunk_id": 1,
          "document_id": "1702.06777",
          "start_idx": 508,
          "end_idx": 1087
        },
        "page_content": ". Typically, the dialect forms plotted on a map appear as a geographical continuum that gradually connects places with slightly different diatopic characteristics. A dialectometric analysis aims at a computational approach to dialect distribution, providing quantitative linguistic distances between locations BIBREF1 , BIBREF2 , BIBREF3 . Dialectometric data is based upon a corpus that contains the linguistic information needed for the statistical analysis. The traditional approach is to generate these data from surveys and questionnaires that address variable types used by",
        "type": "Document"
      },
      {
        "id": "b0ef006b-13b4-4388-a382-7e492c706138",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1702.06777",
          "start_idx": 13877,
          "end_idx": 14380
        },
        "page_content": "s (urban and rural) has been recently suggested BIBREF10 based on a machine learning approach. Here, we arrive at the same conclusion but with a totally distinct model and corpus. The advantage of our proposal is that it may serve as a useful tool for dialectometric purposes. To sum up, we have presented a dialectrometric analysis of lexical variation in social media posts employing information-theoretic measures of language distances. We have considered a grid of cells in Spain and have calculated",
        "type": "Document"
      },
      {
        "id": "d92f4106-0fa6-44b4-8cf3-693364c310d0",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1702.06777",
          "start_idx": 0,
          "end_idx": 508
        },
        "page_content": "Dialects are language varieties defined across space. These varieties can differ in distinct linguistic levels (phonetic, morphosyntactic, lexical), which determine a particular regional speech BIBREF0 . The extension and boundaries (always diffuse) of a dialect area are obtained from the variation of one or many features such as, e.g., the different word alternations for a given concept. Typically, the dialect forms plotted on a map appear as a geographical continuum that gradually connects places with",
        "type": "Document"
      },
      {
        "id": "305fb8f2-f03d-46c3-86bd-65af99191a0b",
        "metadata": {
          "chunk_id": 422,
          "document_id": "1804.08186",
          "start_idx": 157787,
          "end_idx": 158116
        },
        "page_content": "REF4 , BIBREF425 , Serbo-Croatian dialects BIBREF426 , English varieties BIBREF278 , BIBREF45 , Dutch\u2013Flemish BIBREF53 , Dutch dialects (including a temporal dimension) BIBREF427 , German Dialects BIBREF428 Mainland\u2013Singaporean\u2013Taiwanese Chinese BIBREF429 , Portuguese varieties BIBREF5 , BIBREF259 , Spanish varieties BIBREF70 ,",
        "type": "Document"
      },
      {
        "id": "532d0aff-768d-49d5-8137-a4fdc55c0bee",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1904.08386",
          "start_idx": 3323,
          "end_idx": 3851
        },
        "page_content": " the city descriptions is relatively short, Calvino's writing is filled with rare words, complex syntactic structures, and figurative language. Capturing the essential components of each city in a single vector is thus not as simple as it is with more standard forms of text. Nevertheless, we hope that representations from language models trained over billions of words of text can extract some meaningful semantics from these descriptions. We experiment with three different pretrained representations: ELMo BIBREF5 , BERT BIB",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the characteristics of the rural dialect?",
    "llm_answer": "Rural regions, represented by a purple background on the maps, have more specific words in their lexicon.\n",
    "context": [
      {
        "id": "6373ed52-6ebb-41f2-ad2c-e50d685ddd0a",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1702.06777",
          "start_idx": 12500,
          "end_idx": 13034
        },
        "page_content": " the other hand one can now investigate which regions show similar geolectal variation, yielding well defined linguistic varieties. Those cells that have similar colors in either figure FIGREF16 or figure FIGREF17 are expected to be ascribed to the same dialect zone. Thus, we can distinguish two main regions or clusters in the maps. The purple background covers most of the map and represents rural regions with small, scattered population. Our analysis shows that this group of cells possesses more specific words in their lexicon.",
        "type": "Document"
      },
      {
        "id": "b0ef006b-13b4-4388-a382-7e492c706138",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1702.06777",
          "start_idx": 13877,
          "end_idx": 14380
        },
        "page_content": "s (urban and rural) has been recently suggested BIBREF10 based on a machine learning approach. Here, we arrive at the same conclusion but with a totally distinct model and corpus. The advantage of our proposal is that it may serve as a useful tool for dialectometric purposes. To sum up, we have presented a dialectrometric analysis of lexical variation in social media posts employing information-theoretic measures of language distances. We have considered a grid of cells in Spain and have calculated",
        "type": "Document"
      },
      {
        "id": "d92f4106-0fa6-44b4-8cf3-693364c310d0",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1702.06777",
          "start_idx": 0,
          "end_idx": 508
        },
        "page_content": "Dialects are language varieties defined across space. These varieties can differ in distinct linguistic levels (phonetic, morphosyntactic, lexical), which determine a particular regional speech BIBREF0 . The extension and boundaries (always diffuse) of a dialect area are obtained from the variation of one or many features such as, e.g., the different word alternations for a given concept. Typically, the dialect forms plotted on a map appear as a geographical continuum that gradually connects places with",
        "type": "Document"
      },
      {
        "id": "0b0698b7-072c-4671-ab2a-4782b07b124b",
        "metadata": {
          "chunk_id": 1,
          "document_id": "1702.06777",
          "start_idx": 508,
          "end_idx": 1087
        },
        "page_content": ". Typically, the dialect forms plotted on a map appear as a geographical continuum that gradually connects places with slightly different diatopic characteristics. A dialectometric analysis aims at a computational approach to dialect distribution, providing quantitative linguistic distances between locations BIBREF1 , BIBREF2 , BIBREF3 . Dialectometric data is based upon a corpus that contains the linguistic information needed for the statistical analysis. The traditional approach is to generate these data from surveys and questionnaires that address variable types used by",
        "type": "Document"
      },
      {
        "id": "305fb8f2-f03d-46c3-86bd-65af99191a0b",
        "metadata": {
          "chunk_id": 422,
          "document_id": "1804.08186",
          "start_idx": 157787,
          "end_idx": 158116
        },
        "page_content": "REF4 , BIBREF425 , Serbo-Croatian dialects BIBREF426 , English varieties BIBREF278 , BIBREF45 , Dutch\u2013Flemish BIBREF53 , Dutch dialects (including a temporal dimension) BIBREF427 , German Dialects BIBREF428 Mainland\u2013Singaporean\u2013Taiwanese Chinese BIBREF429 , Portuguese varieties BIBREF5 , BIBREF259 , Spanish varieties BIBREF70 ,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance of the models on the tasks?",
    "llm_answer": "The best model obtained 42.48% accuracy for age, 37.50% for dialect, and 57.81% for gender.  Other results are mentioned, but not in a summarized form.\n",
    "context": [
      {
        "id": "f401fca1-b806-4ff6-9500-a007ae03af23",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1909.04181",
          "start_idx": 4044,
          "end_idx": 4423
        },
        "page_content": " of each epoch, choosing the model that performs highest accuracy on DEV as our best model. We present our best result on DEV in Table TABREF7. We report all our results using accuracy. Our best model obtains 42.48% for age, 37.50% for dialect, and 57.81% for gender. All models obtains best results with 2 epochs. For each task, we fine-tune on the BERT-Base Muultilingual Cased",
        "type": "Document"
      },
      {
        "id": "f99d9d00-0d4b-4434-8c55-23bf1c479e81",
        "metadata": {
          "chunk_id": 19,
          "document_id": "2003.12932",
          "start_idx": 6944,
          "end_idx": 7364
        },
        "page_content": " all the experiments, the learning rate is set to 4e-5, for optimization we use Adam optimizer with epsilon value 1e-8. We ran each of the experiments for 10 and 50 epochs. Let us discuss the results from the above mentioned experiments. We show the plots of accuracy vs noise for each of the tasks. For IMDB, we fine tune the model for the sentiment analysis task. We plot F1 score vs % of error, as shown in Figure FIG",
        "type": "Document"
      },
      {
        "id": "0ab3fdf3-8ea1-480c-8cac-92715fc9d565",
        "metadata": {
          "chunk_id": 52,
          "document_id": "1909.02764",
          "start_idx": 20420,
          "end_idx": 20841
        },
        "page_content": "=73 % on TEC. The model trained on our AMMER corpus achieves an F1 score of 57%. This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set. These results are mostly in line with Bostan2018. Now we analyze how well the models trained in Experiment 1 perform when applied to our data set. The results are shown in column \u201cSimple\u201d in Table TAB",
        "type": "Document"
      },
      {
        "id": "38d0d5d3-4be2-4a08-bc2e-e9f401074e5d",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1701.06538",
          "start_idx": 17346,
          "end_idx": 17826
        },
        "page_content": ". Details can be found in Appendix UID77 . Results of these three models form the bottom line of Figure FIGREF32 -right. Table TABREF33 compares the results of these models to the best previously-published result on this dataset . Even the fastest of these models beats the best published result (when controlling for the number of training epochs), despite requiring only 6% of the computation. We trained our models using TensorFlow BIBREF30 on clusters containing 16-32 Tesla K",
        "type": "Document"
      },
      {
        "id": "b43e47f1-8adf-489f-92c9-834052c4dc44",
        "metadata": {
          "chunk_id": 13,
          "document_id": "2002.05829",
          "start_idx": 5371,
          "end_idx": 5916
        },
        "page_content": "task performance fine-tuned from given pretrained models because for each single task with different difficulty and instance number, the fine-tuning characteristics may differ a lot. When pretrained models are used to deal with non-standard downstream task, especially ad hoc application in industry, the training set's difficulty cannot be accurately estimated. Therefore, it's important to compare the multi-task efficiency for model choice. For inference phase, the time and cost of each model making inference for single instance on multiple",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What other non-neural baselines do the authors compare to? ",
    "llm_answer": "PCA-based acoustic similarity measure (Lee et al. BIBREF10) and a nonlinear dynamical systems-based complexity measure (BIBREF6).\n",
    "context": [
      {
        "id": "94c43990-47e3-478c-a415-088eeac582b5",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1804.08782",
          "start_idx": 13990,
          "end_idx": 14427
        },
        "page_content": " number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 .  For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose",
        "type": "Document"
      },
      {
        "id": "5bfeb7ef-9b22-4425-9551-885be98e33be",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1804.08782",
          "start_idx": 14212,
          "end_idx": 14718
        },
        "page_content": " the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose the best performing one for reporting, thus providing an upper-bound performance. Also, for baseline 2 we choose the session with higher value of the measure as real, since it measures similarity.  As we can see in Table TABREF24 , our proposed NED measure achieves higher accuracy than all baselines on the Fisher corpus. The accuracy of our measure declines in the Suicide corpus as compared to",
        "type": "Document"
      },
      {
        "id": "26654c16-c460-4900-b78d-5ad48b826e37",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1909.00542",
          "start_idx": 15059,
          "end_idx": 15516
        },
        "page_content": " the results and details of the runs submitted to BioASQ. The table uses ROUGE-SU4 Recall since this is the metric available at the time of writing this paper. However, note that, as explained in Section SECREF6, these results might differ from the final human evaluation results. Therefore we do not comment on the results, other than observing that the \u201cfirst $n$\u201d baseline produces the same results as the neural regressor. As mentioned in Section SECREF",
        "type": "Document"
      },
      {
        "id": "7bb4c95c-4811-4e15-ab8e-deca3c53de73",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1709.08299",
          "start_idx": 9435,
          "end_idx": 10003
        },
        "page_content": " questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models. The",
        "type": "Document"
      },
      {
        "id": "3ff0334d-c1b6-4522-b5b5-ee27747097d6",
        "metadata": {
          "chunk_id": 63,
          "document_id": "2004.01980",
          "start_idx": 22218,
          "end_idx": 22603
        },
        "page_content": " all baselines. We use the summarization-related evaluation metrics, i.e., BLEU, ROUGE, CIDEr, and METEOR, to measure how relevant the generated headlines are to the news articles, to some extent, by comparing them to the original human-written headlines. In Table TABREF59, the first row \u201cNHG\u201d shows the performance of the current state-of-the-art summarization model on our data, and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "On what dataset is Aristo system trained?",
    "llm_answer": "Several datasets of independently-authored science questions taken from standardized tests.\n",
    "context": [
      {
        "id": "4dea9c85-f07c-40d5-986a-e73d76022fd8",
        "metadata": {
          "chunk_id": 69,
          "document_id": "1909.01958",
          "start_idx": 24838,
          "end_idx": 25300
        },
        "page_content": ") that includes the correct option, the system receives 1/N points (equivalent to the asymptote of random guessing between the N). The results are summarized in Table TABREF33, showing the performance of the solvers individually, and their combination in the full Aristo system. Note that Aristo is a single system run on the five datasets (not retuned for each dataset in turn). Most notably, Aristo's scores on the Regents Exams far exceed earlier performances",
        "type": "Document"
      },
      {
        "id": "fbc4bdf7-4818-4108-aff4-abe177b3f3db",
        "metadata": {
          "chunk_id": 89,
          "document_id": "1909.01958",
          "start_idx": 32148,
          "end_idx": 32664
        },
        "page_content": "). This paper reports on Aristo\u2014the first system to achieve a score of over 90% on the non-diagram, multiple choice part of the New York Regents 8th Grade Science Exam, demonstrating that modern NLP methods can result in mastery of this task. Although Aristo only answers multiple choice questions without diagrams, and operates only in the domain of science, it nevertheless represents an important milestone towards systems that can read and understand. The momentum on this task has been remarkable, with accuracy",
        "type": "Document"
      },
      {
        "id": "38351c44-175f-4f9f-94a4-b16a1d0d5d98",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1909.01958",
          "start_idx": 7153,
          "end_idx": 7578
        },
        "page_content": " dominated by information retrieval, statistical, and automated rule extraction and reasoning methods (BID24;BID25;BID26;BID27;BID28). Later work has harnessed state-of-the-art tools for large-scale language modeling and deep learning (BID29;BID30), which have come to dominate the performance of the overall system and reflects the stunning progress of the field of NLP as a whole. We now describe the architecture of Aristo",
        "type": "Document"
      },
      {
        "id": "bf82d7c7-05d0-4bc0-95e8-05c06c8a0b72",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1909.01958",
          "start_idx": 22619,
          "end_idx": 23113
        },
        "page_content": " Regents 8th Grade Science exams, only 38 ($<$8%) do not involve a diagram. Second, they are complex, often requiring explanation and synthesis. Both diagram and direct-answer questions are natural topics for future work. We evaluate Aristo using several datasets of independently-authored science questions taken from standardized tests. Each dataset is divided into train, development, and test partitions, the test partitions being \u201cblind\u201d, i.e., hidden to both the researchers and the Arist",
        "type": "Document"
      },
      {
        "id": "326f8729-60df-48ed-86a7-818068da8025",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1909.01958",
          "start_idx": 6693,
          "end_idx": 7153
        },
        "page_content": " (BID23). The metric progress of the Aristo system on the Regents 8th Grade exams (non-diagram, multiple choice part, for a hidden, held-out test set) is shown in Figure FIGREF6. The figure shows the variety of techniques attempted, and mirrors the rapidly changing trajectory of the Natural Language Processing (NLP) field in general. Early work was dominated by information retrieval, statistical, and automated rule extraction and reasoning methods (BID24;B",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many roles are proposed?",
    "llm_answer": "The context does not specify a precise number of proposed roles.\n",
    "context": [
      {
        "id": "86858e49-252a-4d09-b861-2cc059a8e042",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1911.03243",
          "start_idx": 4753,
          "end_idx": 5216
        },
        "page_content": ", while a third consolidates their annotations into a final set of roles and arguments. In this consolidation task, the worker validates questions, merges, splits or modifies answers for the same role according to guidelines, and removes redundant roles by picking the more naturally phrased questions. For example, in Table TABREF4 ex. 1, one worker could have chosen \u201c47 people\u201d, while another chose \u201cthe councillor\u201d; in this case the consolidator would include",
        "type": "Document"
      },
      {
        "id": "8c3fd7f3-9788-42f5-9a0f-53b6d718f992",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1911.03243",
          "start_idx": 1523,
          "end_idx": 1985
        },
        "page_content": " more roles, our coverage gain is demonstrated by evaluating against expertly annotated data and comparison with PropBank (Section SECREF4). To foster future research, we release an assessed high-quality gold dataset along with our reproducible protocol and evaluation scheme, and report the performance of the existing parser BIBREF5 as a baseline. In QA-SRL, a role question adheres to a 7-slot template, with slots corresponding to a WH-word, the verb, auxili",
        "type": "Document"
      },
      {
        "id": "f07ae244-943b-4c1d-9b22-d7d6e8c89a79",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1911.03243",
          "start_idx": 2267,
          "end_idx": 2736
        },
        "page_content": " is then considered as the set of arguments associated with that role. This broad question-based definition of roles captures traditional cases of syntactically-linked arguments, but also additional semantic arguments clearly implied by the sentence meaning (see example (2) in Table TABREF4). The original 2015 QA-SRL dataset BIBREF4 was annotated by non-expert workers after completing a brief training procedure. They annotated 7.8K verbs, reporting an average of 2.",
        "type": "Document"
      },
      {
        "id": "366bdec9-434c-44a7-81de-d43fcfea7e8d",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1910.14497",
          "start_idx": 12918,
          "end_idx": 13219
        },
        "page_content": "chancellor', 'chaplain', 'character', 'chef', 'chemist', 'choreographer', 'cinematographer', 'citizen', 'cleric', 'clerk', 'coach', 'collector', 'colonel', 'columnist', 'comedian', 'comic', 'commander', 'commentator', 'commissioner', 'composer', 'conductor', 'confesses', 'congressman', 'constable', '",
        "type": "Document"
      },
      {
        "id": "09de6815-fda7-4e40-94c1-670b85ed6297",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1806.07711",
          "start_idx": 24328,
          "end_idx": 24901
        },
        "page_content": " the main semantic roles and their compositions present on dictionary definitions. Moreover, we compared the identified semantic patterns with the definitions' syntactic structure, pointing out the features that can serve as input for automatic role labeling. The proposed semantic roles list is by no means definitive or exhaustive, but a first step at highlighting and formalizing the most relevant aspects of widely used intensional level definitions. As future work, we intend to implement a rule-based classifier, using the identified syntactic patterns to generate an",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What language technologies have been introduced in the past?",
    "llm_answer": "Statistical modeling approaches, linguistic theory based approaches, deep neural networks, and Transformer-based architectures.\n",
    "context": [
      {
        "id": "adf9e486-156b-4504-afa0-aef4b214a3dc",
        "metadata": {
          "chunk_id": 445,
          "document_id": "1804.08186",
          "start_idx": 166375,
          "end_idx": 166790
        },
        "page_content": " such as very short documents (e.g. search engine queries), non-linguistic \u201cnoise\u201d (e.g. HTML markup), non-standard use of language (e.g. as seen in social media data), and mixed-language documents (e.g. forum posts in multilingual web forums). Modern approaches to are generally data-driven and are based on comparing new documents with models of each target language learned from data. The types of models as well",
        "type": "Document"
      },
      {
        "id": "e7ec6188-0374-441b-9597-ad8194305ce2",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1909.00154",
          "start_idx": 1602,
          "end_idx": 2163
        },
        "page_content": " a while two general trends were competing: the statistical modeling approaches, and the linguistic theory based approaches. The former relied on simple representations, such as vector frequencies, or dummy variables, to become practical, while the latter used domain knowledge such as grammars or logic. Until recently, neither had considerable success in making machines able to understand or generate human language, but developments in deep neural networks together with overwhelmingly massive amounts of data (i.e. the World Wide Web) brought them to a new",
        "type": "Document"
      },
      {
        "id": "1679f9eb-4e98-4e5d-9e49-092f8d4cf482",
        "metadata": {
          "chunk_id": 334,
          "document_id": "1804.08186",
          "start_idx": 122863,
          "end_idx": 123368
        },
        "page_content": " developing the technique. To this end, a number of off-the-shelf language identifiers have been released over time. Many authors have evaluated these off-the-shelf identifiers, including a recent evaluation involving 13 language identifiers which was carried out by BIBREF400 . In this section, we provide a brief summary of open-source or otherwise free systems that are available, as well as the key characteristics of each system. We have also included dates of when the software has been last updated",
        "type": "Document"
      },
      {
        "id": "178ea747-e40b-483f-bd4d-a42ac05a99db",
        "metadata": {
          "chunk_id": 20,
          "document_id": "2002.02427",
          "start_idx": 7113,
          "end_idx": 7613
        },
        "page_content": ".g., punctuation marks, positive and negative emoticons, quotations, personal pronouns, tweet's length, named entities) while others are language-dependent relying on dedicated lexicons (e.g., negation, opinion lexicons, opposition words). Several classical machine learning classifiers were tested with several feature combinations, among them Random Forest (RF) achieved the best result with all features. Neural model with monolingual embeddings. We used Convolutional Neural Network (CNN) network",
        "type": "Document"
      },
      {
        "id": "bb19eb3a-1d46-4bdb-a1b7-857ce8a80a6f",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1912.00582",
          "start_idx": 3636,
          "end_idx": 4099
        },
        "page_content": " modeling. In the last decade, we have seen two major paradigm shifts in the state of the art for language modeling. The first major shift for language modeling was the movement from statistical methods based on $n$-grams BIBREF10 to neural methods such as LSTMs BIBREF11, which directly optimize on the task of predicting the next word. More recently, Transformer-based architectures employing self-attention BIBREF12 have outperformed LSTMs at language modeling",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they define local variance?",
    "llm_answer": "Local variance loss is defined as the reciprocal of the variance of the attention distribution, calculated using the median value instead of the mean, to encourage the model to focus on more salient parts of the input.\n",
    "context": [
      {
        "id": "cb2a9ada-dc0a-40cf-a70d-f3abeb23a754",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1910.11491",
          "start_idx": 5156,
          "end_idx": 5696
        },
        "page_content": " section SECREF1, the attention model putting most of attention weight on just a few parts of the input tends to achieve good performance. Mathematically, when only a small number of values are large, the shape of the distribution is sharp and the variance of the attention distribution is large. Drawing on the concept of variance in mathematics, local variance loss is defined as the reciprocal of its variance expecting the attention model to be able to focus on more salient parts. The standard variance calculation is based on the mean",
        "type": "Document"
      },
      {
        "id": "c12e35b1-1e97-4207-9eca-9ffb016208c5",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1910.11491",
          "start_idx": 5696,
          "end_idx": 6154
        },
        "page_content": " model to be able to focus on more salient parts. The standard variance calculation is based on the mean of the distribution. However, as previous work BIBREF15, BIBREF16 mentioned that the median value is more robust to outliers than the mean value, we use the median value to calculate the variance of the attention distribution. Thus, local variance loss can be calculated as: where $\\hat{\\cdot }$ is a median operator and $\\epsilon $ is utilized to avoid",
        "type": "Document"
      },
      {
        "id": "e96c4ce1-ef10-4d82-8c08-ed701d9eaab9",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1910.11491",
          "start_idx": 6756,
          "end_idx": 7263
        },
        "page_content": " difference between the sum of attention weight and maximum attention weight of $i$-th input state among all timesteps should be small. Moreover, the whole distribution of the difference over all input positions should have a flat shape. Similar to the definition of local variance loss, the global variance loss is formulated as: where $g_i$ represents the difference between the accumulated attention weight and maximum attention weight at $i$-th position. The model is firstly pre-trained to minimize the",
        "type": "Document"
      },
      {
        "id": "b0402055-876e-41a0-a17d-aa134eb2c8fc",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1910.11491",
          "start_idx": 9922,
          "end_idx": 10236
        },
        "page_content": " achieved a gain of 0.97 ROUGE-1, 0.35 ROUGE-2, and 0.64 ROUGE-L points; the local variance loss boosts the model by 3.01 ROUGE-1, 1.6 ROUGE-2, and 2.58 ROUGE-L. As shown in Figure FIGREF22, the global variance loss helps with eliminating n-gram repetitions, which verifies its effectiveness. We also conduct human",
        "type": "Document"
      },
      {
        "id": "7c42c433-39a7-4382-9616-23842ffa429c",
        "metadata": {
          "chunk_id": 65,
          "document_id": "2002.05058",
          "start_idx": 25823,
          "end_idx": 26327
        },
        "page_content": "G models). As this variance does not exist when we regard a sample as a model which always generates the same sample. As for model-level evaluation, we employ the average score of the evaluated 100 samples as each model's score and calculate their correlation with human scores. For comparative evaluator, we propose three different approaches to get an absolute score for each sample: 1) we calculate the average reference-based score (method 1 for sample-level comparison) of each sample as model-level",
        "type": "Document"
      }
    ]
  },
  {
    "query": "how do they measure discussion quality?",
    "llm_answer": "They measure discussion quality using students' argument moves (claims and textual evidence), argumentation, specificity, and knowledge domain.  Inter-rater reliability is assessed using Cohen's kappa.  Expert evaluations are also used for validity comparison (quadratic-weighted kappa).\n",
    "context": [
      {
        "id": "a4807de0-ac1a-4901-b471-4beca6040a5e",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1909.03023",
          "start_idx": 2138,
          "end_idx": 2728
        },
        "page_content": " to discussion quality and learning opportunities: argumentation (the process of systematically reasoning in support of an idea), specificity (the quality of belonging or relating uniquely to a particular subject), and knowledge domain (area of expertise represented in the content of the talk). We demonstrate the reliability and validity of our scheme via an annotation study of five transcripts of classroom discussion. One discourse feature used to assess the quality of discussions is students' argument moves: their claims about the text, their sharing of textual evidence for claims,",
        "type": "Document"
      },
      {
        "id": "67a7695f-67b2-4acb-98f7-3e39a52c72ef",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1910.00825",
          "start_idx": 16888,
          "end_idx": 17413
        },
        "page_content": " our method's increased performance on automatic evaluation metrics entails better human perceived quality. We randomly select 100 test samples from MultiWOZ test set for evaluation. We recruit 150 crowd workers from Amazon Mechanical Turk. For each sample, we show the conversation, reference summary, as well as summaries generated by Pointer-Generator and SPNet to three different participants. The participants are asked to score each summary on three indicators: relevance, conciseness and readability on a 1 to 5 scale,",
        "type": "Document"
      },
      {
        "id": "845c59e0-dd22-4adf-a86c-cdf32799acae",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1909.03023",
          "start_idx": 8992,
          "end_idx": 9437
        },
        "page_content": "-based discussions were used for testing reliability after training: pair P1 annotated discussions of The Bluest Eye, Death of a Salesman, and Macbeth, while pair P2 annotated two separate discussions of Ain't I a Woman. 250 argument moves (discussed by over 40 students and consisting of over 8200 words) were annotated. Inter-rater reliability was assessed using Cohen's kappa: unweighted for argumentation and knowledge domain, but quadratic-",
        "type": "Document"
      },
      {
        "id": "0b203124-4535-4d3e-b473-9446278ecd7e",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1909.03023",
          "start_idx": 1244,
          "end_idx": 1845
        },
        "page_content": "9 . This limitation is partly due to the time-intensive work required to analyze discourse data through qualitative methods such as ethnography and discourse analysis. Thus, qualitative case studies have generated compelling theories about the specific features of student talk that lead to high-quality discussions, but few findings can be generalized and leveraged to influence instructional improvements across ELA classrooms. As a first step towards developing an automated system for detecting the features of student talk that lead to high quality discussions, we propose a new annotation scheme",
        "type": "Document"
      },
      {
        "id": "1fc047cf-8143-4ddf-b3dd-ef8907214598",
        "metadata": {
          "chunk_id": 30,
          "document_id": "1909.03023",
          "start_idx": 12211,
          "end_idx": 12706
        },
        "page_content": ". We also BIBREF32 explored the validity of our coding scheme by comparing our annotations of student talk to English Education experts' evaluations (quadratic-weighted kappa of 0.544) of the discussion's quality. Using stepwise regressions, we found that the best model of discussion quality (R-squared of INLINEFORM0 ) included all three of our coding dimensions: argumentation, specificity, and knowledge domain. Our annotation scheme introduces opportunities for the educational community to",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what were the baselines?",
    "llm_answer": "Baseline 1: smooth L1 distance; Baseline 2: PCA-based acoustic similarity; Baseline 3: Nonlinear dynamical systems-based complexity; ANRES_Sing_R; ANRES_Sing_Z; ANRES_Seq_R; ANRES_Seq_Z\n",
    "context": [
      {
        "id": "94c43990-47e3-478c-a415-088eeac582b5",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1804.08782",
          "start_idx": 13990,
          "end_idx": 14427
        },
        "page_content": " number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 .  For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose",
        "type": "Document"
      },
      {
        "id": "3747343d-0517-442a-b7ba-66d01be55b6d",
        "metadata": {
          "chunk_id": 40,
          "document_id": "2004.01878",
          "start_idx": 14526,
          "end_idx": 14915
        },
        "page_content": " normal distribution $\\mathcal {N}(0, 1)$. In summary, the following four baselines are designed: ANRES_Sing_R: randomly initializing the states for each single trading day. ANRES_Sing_Z: initializing the states as zeros for each single trading day. ANRES_Seq_R: randomly initializing the first states for each trading sequence only. ANRES_Seq_Z: initializing the first states as zeros for",
        "type": "Document"
      },
      {
        "id": "7bb4c95c-4811-4e15-ab8e-deca3c53de73",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1709.08299",
          "start_idx": 9435,
          "end_idx": 10003
        },
        "page_content": " questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models. The",
        "type": "Document"
      },
      {
        "id": "f297d78b-0fdd-4eee-ae9e-9e75a2e66130",
        "metadata": {
          "chunk_id": 140,
          "document_id": "2001.02380",
          "start_idx": 51470,
          "end_idx": 51956
        },
        "page_content": " baselines. The left, middle and right panels in Figure FIGREF40 correspond to measurements when all signals are included, only cases contained entirely in the head EDUs shown to the model, and only DMs, respectively. The scenario on the left is rather unreasonable and is included only for completeness: here the model is also penalized for not detecting signals such as lexical chains, part of which is outside the units that the model is being shown. An example of such a case can be",
        "type": "Document"
      },
      {
        "id": "3ff0334d-c1b6-4522-b5b5-ee27747097d6",
        "metadata": {
          "chunk_id": 63,
          "document_id": "2004.01980",
          "start_idx": 22218,
          "end_idx": 22603
        },
        "page_content": " all baselines. We use the summarization-related evaluation metrics, i.e., BLEU, ROUGE, CIDEr, and METEOR, to measure how relevant the generated headlines are to the news articles, to some extent, by comparing them to the original human-written headlines. In Table TABREF59, the first row \u201cNHG\u201d shows the performance of the current state-of-the-art summarization model on our data, and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which soft-selection approaches are evaluated?",
    "llm_answer": "Traditional attention-based soft-selection methods.\n",
    "context": [
      {
        "id": "c5442c90-f53f-4ef4-a8ba-23d156aed5eb",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1905.10238",
          "start_idx": 3291,
          "end_idx": 3866
        },
        "page_content": " the given context. Moreover, a softmax pruning is placed in between the two layers to select high confident candidates. The architecture ensures the model being able to leverage both context and external knowledge. Especially, compared with conventional approaches that simply treat external knowledge as rules or features, our model is not only more flexible and effective but also interpretable as it reflects which knowledge source has the higher weight in order to make the decision. Experiments are conducted on a widely used evaluation dataset, where the results prove",
        "type": "Document"
      },
      {
        "id": "5728cdf9-4656-49c6-9f63-e6a4ba378a49",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1909.11297",
          "start_idx": 15224,
          "end_idx": 15794
        },
        "page_content": "-Soft. It is called a hard-selection approach since it employs reinforcement learning techniques to explicitly select the opinion snippet corresponding to a particular aspect for sentiment prediction. In this section, we evaluate the performance of our models by comparing them with various baseline models. Experimental results are illustrated in Table TABREF21, in which 3-way represents 3-class sentiment classification (positive, negative and neutral) and Binary denotes binary sentiment prediction (positive and negative). The best score of each column is marked in",
        "type": "Document"
      },
      {
        "id": "a3c02f51-24d4-4b05-b5ef-ba56fb37b838",
        "metadata": {
          "chunk_id": 55,
          "document_id": "2002.05058",
          "start_idx": 21738,
          "end_idx": 22296
        },
        "page_content": " three variants of the comparative evaluator where it is trained without strong supervision examples, without weak supervision examples, without fine-tuning with human preference annotations, and without transferring from BERT. As human evaluation is expensive, sample-level evaluation is performed jointly with model-level evaluation, which is also used for evaluating the ability of different metrics for performing hyperparameter search and early-stopping. Concretely, we perform 10 groups of evaluations for performing hyperparameter selecting and early-",
        "type": "Document"
      },
      {
        "id": "d37f173d-0055-4968-8334-faf092e9ae8b",
        "metadata": {
          "chunk_id": 67,
          "document_id": "1909.11297",
          "start_idx": 24704,
          "end_idx": 25168
        },
        "page_content": " snippet, our approach can alleviate the attention distraction problem of traditional attention-based soft-selection methods. Experimental results demonstrate the effectiveness of our method. Especially, our hard-selection approach outperforms soft-selection approaches significantly when handling multi-aspect sentences with different sentiment polarities. This work is supported by National Science and Technology Major Project, China (Grant No. 2018YFB0204304).",
        "type": "Document"
      },
      {
        "id": "84a31549-a044-41f7-a2ec-f9440338f2d2",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1905.10238",
          "start_idx": 19745,
          "end_idx": 20310
        },
        "page_content": " among all knowledge types, which indicates that potentially more cases in the evaluation dataset demand on the AG knowledge than others. More importantly, the results also prove the effectiveness of the knowledge attention module, which contributes to the performance gap between our model and the Feature Concatenation one. Effect of Different Pruning Thresholds We try different thresholds $t$ for the softmax pruning in selecting reliable candidates. The effects of different thresholds on reducing candidates and overall performance are shown in Figure 5 and 6",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is slot filing dataset?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "76fa3798-810a-4679-9503-5ff1ff1471c5",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1908.11546",
          "start_idx": 5797,
          "end_idx": 6239
        },
        "page_content": " of acts, slots, and act-slot pairs are also listed in Table TABREF11. Table TABREF12 shows the count of turns with multiple act annotations, which amounts to 23% of the dataset. We use MSR's dialogue management code and knowledge base to obtain the state at each turn and use it as input to every model. We evaluate the performance at the act, frame and task completion level. For a frame to be correct, both the act and all the slots should",
        "type": "Document"
      },
      {
        "id": "e4400fd8-4b91-4ab3-b33a-b5b007b697e1",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1911.01680",
          "start_idx": 16273,
          "end_idx": 16741
        },
        "page_content": " almost 2%, which corresponds to a 12.3% error rate reduction. This improvements proves the effectiveness of using contextual information for the task of slot filling. In order to analyze the contribution of the proposed sub-tasks we also evaluate the model when we remove one of the sub-task and retrain the model. The results are reported in Table TABREF23. This table shows that all sub-tasks are required for the model to have its best performance. Among all sub-t",
        "type": "Document"
      },
      {
        "id": "68b62186-fe40-4429-85da-cbb184305c57",
        "metadata": {
          "chunk_id": 52,
          "document_id": "1908.02402",
          "start_idx": 15534,
          "end_idx": 16050
        },
        "page_content": " indicates the number of records satisfying the constraints. $d_t$ is a five-dimensional one-hot vector, where the first four dimensions represent integers from 0 to 3 and the last dimension represents 4 or more matched records. It is later used to inform the response slot binary classifier and the agent response decoder. In order to incorporate all the relevant information about the retrieved entities into the response, FSDM introduces a new response slot binary classifier. Its inputs are requestable slots and",
        "type": "Document"
      },
      {
        "id": "278faace-fac8-447c-9e74-99e2b1188318",
        "metadata": {
          "chunk_id": 22,
          "document_id": "2002.01359",
          "start_idx": 9340,
          "end_idx": 9877
        },
        "page_content": "ents and slots which can be used for developing models which can condition their predictions on the schema. To reflect the constraints present in real-world services and APIs, we impose a few constraints on the data. Our dataset does not expose the set of all possible values for certain slots. Having such a list is impractical for slots like date or time because they have infinitely many possible values or for slots like movie or song names, for which new values are periodically added. Such slots are specifically identified as non-",
        "type": "Document"
      },
      {
        "id": "95bf2b84-1a56-4eed-b193-4a34d2c76c4c",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1706.08032",
          "start_idx": 14264,
          "end_idx": 14661
        },
        "page_content": " datasets in Table III. for all datasets, the filter window size ( INLINEFORM0 ) is 7 with 6 feature maps each for the first wide convolution layer, the second wide convolution layer has a filter window size of 5 with 14 feature maps each. Dropout rate ( INLINEFORM1 ) is 0.5, INLINEFORM2 constraint, learning rate is 0.1 and momentum of 0.9. Mini-batch size for STS Corpus is 100 and others are 4",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How large is the dataset they generate?",
    "llm_answer": "287,226 training documents, 13,368 validation documents, and 11,490 test documents.\n",
    "context": [
      {
        "id": "2d3bdfec-4ed5-456d-bd82-023fad798858",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "9ccb7945-9a96-417a-b52e-68855c4910b1",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "70c923ff-1ab7-4846-9d13-56c3cce615a4",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1705.10586",
          "start_idx": 14843,
          "end_idx": 15254
        },
        "page_content": "000 to 3,600,000 training samples, and of different text length ranging from average of 38 words for Ag News to 566 words in Sogou news as illustrated in Table 1 . The datasets are a good mix of polished (AG) and noisy (Yelp and Amazon reviews), long (Sogou) and short (DBP and AG), large (Amazon reviews) and small (AG) datasets. And thus the results over these datasets serve as good evaluation on the quality",
        "type": "Document"
      },
      {
        "id": "99853ffb-b108-4505-9252-bd9c2ff347c8",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1912.00864",
          "start_idx": 19724,
          "end_idx": 20165
        },
        "page_content": " Oshiete-goo dataset. On average, the answers to these questions consisted of about 3.5 conclusions and supplements selected by human experts. The questions, conclusions, and supplements had average lengths of 482, 41, and 46 characters, respectively. There were 9,779 word tokens in the questions and 6,317 tokens in answers; the overlap was 4,096. We also used the Yahoo nfL6 dataset, the largest publicly available English non-factoid CQA",
        "type": "Document"
      },
      {
        "id": "ed0bc156-89c2-4b94-9e58-2194484d059d",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1709.05404",
          "start_idx": 17579,
          "end_idx": 18022
        },
        "page_content": "-validation setup) for each dataset while holding the number of posts per class constant. Figure FIGREF26 shows the performance of each of the Gen, RQ, and Hyp datasets at intervals of 100 posts per class (up to the maximum size of 582 posts per class for Hyp, and 851 posts per class for RQ). From the graph, we can see that as a general trend, the datasets benefit from larger dataset sizes. Interestingly, the results for the RQ dataset are",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the weaknesses of their proposed interpretability quantification method?",
    "llm_answer": "The provided context does not describe a proposed interpretability quantification method.  Therefore, weaknesses cannot be determined.\n",
    "context": [
      {
        "id": "5a4e2d4b-2c4e-48b7-ad32-83b5cf353281",
        "metadata": {
          "chunk_id": 124,
          "document_id": "1711.00331",
          "start_idx": 46228,
          "end_idx": 46799
        },
        "page_content": " be a valuable tool in guiding future research on obtaining interpretable yet effective embedding spaces for many NLP tasks that critically rely on semantic information. For instance, performance evaluation of more interpretable word embeddings on higher level NLP tasks (i.e. sentiment analysis, named entity recognition, question answering) and the relation between interpretability and NLP performance can be worthwhile. We thank the anonymous reviewers for their constructive and helpful comments that have significantly improved our paper. This work was supported in",
        "type": "Document"
      },
      {
        "id": "b876e59f-17c9-4083-82ca-edb2b0580395",
        "metadata": {
          "chunk_id": 92,
          "document_id": "1804.03396",
          "start_idx": 29181,
          "end_idx": 29645
        },
        "page_content": " annotators lower the bar of accepting triples. However, the analysis on semantic roles and parsing trees cannot work very well on complicated input sentences like the 2nd and the 3rd cases. Besides, the baseline systems can hardly solve the last two cases which require inference on input sentences. Our framework works very well on this dataset with the QA measurements EM $= 91.87$ and F1 $= 93.53$ and the IE measurements can be found in Figure 3 . Most of the",
        "type": "Document"
      },
      {
        "id": "af5b725d-2602-4ec1-aa78-a1220c02b55c",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1811.01001",
          "start_idx": 1609,
          "end_idx": 2157
        },
        "page_content": "REF11 , BIBREF9 , BIBREF12 . These formulations follow narrow and bounded evaluation schemes though: They often define a length threshold in the test set and report the performance of the model on this fixed set. We acknowledge three unsettling issues with these formulations. First, the sequences in the training set are usually assumed to be uniformly or geometrically distributed, with little regard to the nature and complexity of the language. This assumption may undermine any conclusions drawn from empirical investigations, especially given",
        "type": "Document"
      },
      {
        "id": "ff453b67-2cea-4319-836d-7d47c4847292",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1611.03382",
          "start_idx": 21894,
          "end_idx": 22215
        },
        "page_content": " significantly speed up inference. We have demonstrated the effectiveness of our approach in the context of summarization and shown state-of-the-art performance. In the future, we plan to tackle summarization problems with large input text. We also plan to exploit our findings in other tasks such as machine translation.",
        "type": "Document"
      },
      {
        "id": "dd7e20d8-209b-4f1e-93fb-d6bb5599cc97",
        "metadata": {
          "chunk_id": 4,
          "document_id": "2001.08051",
          "start_idx": 1506,
          "end_idx": 2043
        },
        "page_content": " recorded during some of the above mentioned evaluation campaigns. We will release the written answers in future. Details and critical issues found during the acquisition of the answers of the test takers will be discussed in Section SECREF2. The tasks that can be addressed by using the corpus are very challenging and pose many problems, which have only partially been solved by the interested scientific community. From the ASR perspective, major difficulties are represented by: a) recognition of both child and non-native speech, i.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What advantages does their proposed method of quantifying interpretability have over the human-in-the-loop evaluation they compare to?",
    "llm_answer": "The provided text does not describe a method for quantifying interpretability.  It describes a comparison-based evaluation metric for machine translation that correlates better with human judgment than existing metrics.  There is no comparison between this method and a human-in-the-loop evaluation method in the context of interpretability.\n",
    "context": [
      {
        "id": "2f087fa2-1aed-415a-a42c-95e05656d14d",
        "metadata": {
          "chunk_id": 4,
          "document_id": "2004.01694",
          "start_idx": 1628,
          "end_idx": 2197
        },
        "page_content": " changes in the evaluation design affect the outcome of the human evaluation. We find that for all three aspects, human translations are judged more favourably, and significantly better than MT, when we make changes that we believe strengthen the evaluation design. Based on our empirical findings, we formulate a set of recommendations for human MT evaluation in general, and assessing human\u2013machine parity in particular. All of our data are made publicly available for external validation and further analysis. We first review current methods to assess the quality of",
        "type": "Document"
      },
      {
        "id": "748fb5f2-01c6-4839-9bc8-6212bf55e7d1",
        "metadata": {
          "chunk_id": 3,
          "document_id": "2004.01694",
          "start_idx": 1337,
          "end_idx": 1854
        },
        "page_content": " concur that these findings have demonstrated the need to critically re-evaluate the design of human MT evaluation. Our paper investigates three aspects of human MT evaluation, with a special focus on assessing human\u2013machine parity: the choice of raters, the use of linguistic context, and the creation of reference translations. We focus on the data shared by BIBREF3, and empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation. We find that for all three aspects",
        "type": "Document"
      },
      {
        "id": "722d736b-7199-4c7c-a04f-043ad5b9d60c",
        "metadata": {
          "chunk_id": 2,
          "document_id": "2002.05058",
          "start_idx": 718,
          "end_idx": 1314
        },
        "page_content": " perplexity and are useful in translation and summarization. However, they still correlate poorly with human evaluation BIBREF8 in open domain text generation tasks including story generation and dialogue response generation because two equally good generated texts may have no n-gram overlap. Human evaluation is generally considered to be the gold standard evaluation, however, it does not scale well as it is generally expensive and time-consuming to conduct human evaluation. Apart from measuring relative progress between different models, automated evaluation metrics also play an important",
        "type": "Document"
      },
      {
        "id": "5a4e2d4b-2c4e-48b7-ad32-83b5cf353281",
        "metadata": {
          "chunk_id": 124,
          "document_id": "1711.00331",
          "start_idx": 46228,
          "end_idx": 46799
        },
        "page_content": " be a valuable tool in guiding future research on obtaining interpretable yet effective embedding spaces for many NLP tasks that critically rely on semantic information. For instance, performance evaluation of more interpretable word embeddings on higher level NLP tasks (i.e. sentiment analysis, named entity recognition, question answering) and the relation between interpretability and NLP performance can be worthwhile. We thank the anonymous reviewers for their constructive and helpful comments that have significantly improved our paper. This work was supported in",
        "type": "Document"
      },
      {
        "id": "084aa0d9-16fb-4e10-96ab-25c2b43f3e22",
        "metadata": {
          "chunk_id": 63,
          "document_id": "2002.05058",
          "start_idx": 24978,
          "end_idx": 25505
        },
        "page_content": " To ensure the computational budget to be roughly the same, we fix the number of plays in skill rating to 10,000. The experimental results are summarized in Table 1. We can see that the proposed comparative evaluator correlates far better with human judgment than BLEU and perplexity. When compared with recently proposed parameterized metrics including adversarial evaluator and ADEM, our model consistently outperforms them by a large margin, which demonstrates that our comparison-based evaluation metric is able to evaluate",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How was lexical diversity measured?",
    "llm_answer": "Yule's I measure and the Measure of Textual Lexical Diversity (MTLD).\n",
    "context": [
      {
        "id": "fc0309ce-f5ee-4fb0-91f1-fcb9b8a0fb3f",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1911.00133",
          "start_idx": 11772,
          "end_idx": 12211
        },
        "page_content": " and the least in the financial domain). We also examine the overall lexical diversity of each domain by calculating Yule's I measure BIBREF13. fig:domain-yule shows the lexical diversity of our data, both for all words in the vocabulary and for only words in LIWC's \u201cnegemo\u201d word list. Yule's I measure reflects the repetitiveness of the data (as opposed to the broader coverage measured by our LIWC analysis). We notice exceptionally low",
        "type": "Document"
      },
      {
        "id": "0675b559-6a29-4588-9323-99f6e5315e75",
        "metadata": {
          "chunk_id": 76,
          "document_id": "1703.05260",
          "start_idx": 27528,
          "end_idx": 27991
        },
        "page_content": " to measure this variance. However, this measure is known to be sensitive to text length (see e.g. Tweedie1998), which would result in very small values for InScript and relatively large ones for DeScript, given the large average difference of text lengths between the corpora. Instead, we decided to use the Measure of Textual Lexical Diversity (MTLD) (McCarthy2010, McCarthy2005), which is familiar in corpus linguistics. This metric measures the average number",
        "type": "Document"
      },
      {
        "id": "3d2e6142-8716-47c4-91fa-3a8fbc310028",
        "metadata": {
          "chunk_id": 38,
          "document_id": "2001.05467",
          "start_idx": 14084,
          "end_idx": 14504
        },
        "page_content": " for the goal-oriented Ubuntu Dialogue Corpus\". We did not evaluate on BLEU score BIBREF8 because BIBREF17 showed that BLEU does not correlate well with dialogue quality. BIBREF18 (BIBREF18) also made similar observations on BLEU. To evaluate diversity, we employ two evaluation metrics from previous work, namely Distinct-1 and Distinct-2 BIBREF2. These are the ratios between the number of unique tokens and all tokens",
        "type": "Document"
      },
      {
        "id": "4f55cd4d-ac2c-4387-894c-3a38cf752821",
        "metadata": {
          "chunk_id": 56,
          "document_id": "2001.05467",
          "start_idx": 20825,
          "end_idx": 21301
        },
        "page_content": " coherent and informative responses. Multiple metrics and approaches have been proposed to measure dialogue diversity. Some focus more on how similar the responses are to the ground-truth sequences, such as Word Error Rate BIBREF3 and BLEU BIBREF20, while the others explicitly have diversity in mind when being created, such as Distinct-1 and -2 BIBREF2. The key difference between AvgOut and the previous work is that first, our metric is dynamic with no feature-engineering",
        "type": "Document"
      },
      {
        "id": "35457b66-2fb0-4bc8-97cc-91295b1865a5",
        "metadata": {
          "chunk_id": 27,
          "document_id": "2001.05467",
          "start_idx": 10382,
          "end_idx": 10856
        },
        "page_content": " $1.0$. Specifically, we add to the vocabulary a diversity label and scale its embedding vector with the intended diversity score of the target sequence. During training, this score is obtained by evaluating the diversity of the ground-truth target sequence (see Figure FIGREF8); during test time, we instead feed the model a diversity label scaled by a score of our choice (i.e., when we want the model to generate a more diverse response, we scale the label's embedding by",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which existing models does this approach outperform?",
    "llm_answer": "Regression-based approaches, manually defined knowledge and features, and several state-of-the-art NLI models (BiMPM, ESIM, Decomposable Attention Model, KIM).\n",
    "context": [
      {
        "id": "6aec65fa-03c3-4844-a3ac-097021959290",
        "metadata": {
          "chunk_id": 80,
          "document_id": "2002.05058",
          "start_idx": 32135,
          "end_idx": 32299
        },
        "page_content": " models as they may lead to sub-optimal hyperparameter choice and checkpoint selection. Our model, in contrast, is much more reliable when performing these choices.",
        "type": "Document"
      },
      {
        "id": "2d9e100c-8cb0-48b7-81d1-63d646c42530",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1909.00542",
          "start_idx": 8896,
          "end_idx": 9426
        },
        "page_content": " the results are compatible with the top part of the table and prior work suggesting that classification-based approaches improve over regression-based approaches. We also experiment with the use of reinforcement learning techniques. Again these experiments are based on BIBREF2, who uses REINFORCE to train a global policy. The policy predictor uses a simple feedforward network with a hidden layer. The results reported by BIBREF2 used ROUGE Recall and indicated no improvement with respect to deep learning architectures. Human",
        "type": "Document"
      },
      {
        "id": "673cbe85-5b35-4e82-9368-b763c0613707",
        "metadata": {
          "chunk_id": 57,
          "document_id": "1905.10238",
          "start_idx": 18003,
          "end_idx": 18545
        },
        "page_content": " the best one is selected based on its performance on the development set. Table 2 compares the performance of our model with all baselines. Overall, our model performs the best with respect to all evaluation metrics. Several findings are also observed from the results. First, manually defined knowledge and features are not enough to cover rich contextual information. Deep learning models (e.g., End2end and our proposed models), which leverage text representations for context, outperform other approaches by a great margin, especially on",
        "type": "Document"
      },
      {
        "id": "e218c2da-6400-4484-b946-c9f95cc23931",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1906.06448",
          "start_idx": 17189,
          "end_idx": 17518
        },
        "page_content": " five state-of-the-art NLI models adopting different approaches: BiMPM (Bilateral Multi-Perspective Matching Model; BIBREF31 , BIBREF31 ), ESIM (Enhanced Sequential Inference Model; BIBREF32 , BIBREF32 ), Decomposable Attention Model BIBREF33 , KIM (Knowledge-based Inference Model; BIBREF34 , BIBREF34 ), and BERT (Bidirectional",
        "type": "Document"
      },
      {
        "id": "76185514-2b57-42c0-acac-3fa0a8a69f7b",
        "metadata": {
          "chunk_id": 101,
          "document_id": "1908.02402",
          "start_idx": 29500,
          "end_idx": 29733
        },
        "page_content": "-the-art models, while at the same time providing a more practical solution for real-world applications. We would like to thank Alexandros Papangelis, Janice Lam, Stefan Douglas Webb and SIGDIAL reviewers for their valuable comments.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What human evaluation method is proposed?",
    "llm_answer": "Annotators evaluate the summary against the source content based on the goodness of the summary.\n",
    "context": [
      {
        "id": "7836676a-6d8c-47f0-9b59-ddbce72ee48e",
        "metadata": {
          "chunk_id": 74,
          "document_id": "1805.04033",
          "start_idx": 28680,
          "end_idx": 29253
        },
        "page_content": " eliminating the less semantically-related word correspondence. The proposed human evaluation method is effective and efficient in judging the semantic consistency, which is absent in previous work but is crucial in the accurate evaluation of the text summarization systems. The proposed metric is simple to conduct and easy to interpret. It also provides an insight on how practicable the existing systems are in the real-world scenario. For human evaluation, the annotators are asked to evaluate the summary against the source content based on the goodness of the summary",
        "type": "Document"
      },
      {
        "id": "2f087fa2-1aed-415a-a42c-95e05656d14d",
        "metadata": {
          "chunk_id": 4,
          "document_id": "2004.01694",
          "start_idx": 1628,
          "end_idx": 2197
        },
        "page_content": " changes in the evaluation design affect the outcome of the human evaluation. We find that for all three aspects, human translations are judged more favourably, and significantly better than MT, when we make changes that we believe strengthen the evaluation design. Based on our empirical findings, we formulate a set of recommendations for human MT evaluation in general, and assessing human\u2013machine parity in particular. All of our data are made publicly available for external validation and further analysis. We first review current methods to assess the quality of",
        "type": "Document"
      },
      {
        "id": "ed674135-c713-41cc-915c-42336218eb34",
        "metadata": {
          "chunk_id": 91,
          "document_id": "2004.01694",
          "start_idx": 35319,
          "end_idx": 35678
        },
        "page_content": " human evaluation of MT in general. Our recommendations have the aim of increasing the validity of MT evaluation, but we are aware of the high cost of having MT evaluation done by professional translators, and on the level of full documents. We welcome future research into alternative evaluation protocols that can demonstrate their validity at a lower cost.",
        "type": "Document"
      },
      {
        "id": "748fb5f2-01c6-4839-9bc8-6212bf55e7d1",
        "metadata": {
          "chunk_id": 3,
          "document_id": "2004.01694",
          "start_idx": 1337,
          "end_idx": 1854
        },
        "page_content": " concur that these findings have demonstrated the need to critically re-evaluate the design of human MT evaluation. Our paper investigates three aspects of human MT evaluation, with a special focus on assessing human\u2013machine parity: the choice of raters, the use of linguistic context, and the creation of reference translations. We focus on the data shared by BIBREF3, and empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation. We find that for all three aspects",
        "type": "Document"
      },
      {
        "id": "5155558a-dfb3-4b8b-9585-41ac83603580",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1601.02543",
          "start_idx": 845,
          "end_idx": 1419
        },
        "page_content": " successful and unsuccessful transactions. This evaluation is then used to identifying any further improvement in the speech recognition based solution to better the overall transaction completion rates. This process of evaluation is both time consuming and expensive. For evaluation one needs to identify a set of users and also identify the set of actual usage situations and perform the test. It is also important that the set of users are able to use the system with ease meaning that even in the test conditions the performance of the system, should be good, while this",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What languages are represented in the dataset?",
    "llm_answer": "English, French, German, Spanish, Russian, Italian.\n",
    "context": [
      {
        "id": "7da029d3-0e6a-48d3-9dd2-3cfd37690294",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1910.04269",
          "start_idx": 6853,
          "end_idx": 7342
        },
        "page_content": " comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (",
        "type": "Document"
      },
      {
        "id": "2304b6a3-a5d8-4009-af97-fffe36eae8f2",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1909.06522",
          "start_idx": 11616,
          "end_idx": 12081
        },
        "page_content": ": live streams from users. UserVOD (video on demand): was-live videos. For each language, the train and test set size are described in Table TABREF10, and most training data were Pages. On each language we also had a small validation set for model parameter tuning. Each monolingual ASR baseline was trained on language-specific data only. The character sets of these 7 languages have little overlap except that (i) they all include common basic Latin alphabet, and",
        "type": "Document"
      },
      {
        "id": "e8fb7d0e-86e1-4332-bb4a-d8109e4146e4",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1910.04269",
          "start_idx": 19553,
          "end_idx": 20023
        },
        "page_content": ", the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Kat",
        "type": "Document"
      },
      {
        "id": "e8ffc8fd-73a2-4096-87aa-87f5967c213f",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1910.11493",
          "start_idx": 5173,
          "end_idx": 5630
        },
        "page_content": " languages. Data for all but four languages (Basque, Kurmanji, Murrinhpatha, and Sorani) are extracted from English Wiktionary, a large multi-lingual crowd-sourced dictionary with morphological paradigms for many lemmata. 20 of the 100 language pairs are either distantly related or unrelated; this allows speculation into the relative importance of data quantity and linguistic relatedness. For each language, the basic data consists of triples of the form",
        "type": "Document"
      },
      {
        "id": "6bc2bb18-6a66-4628-bef7-a0e1a75c3767",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1909.13362",
          "start_idx": 15624,
          "end_idx": 16095
        },
        "page_content": " language groupings within families. We selected six evaluation languages: English, Dutch, Italian, French, Basque, and Manipuri. These represent two language families (Indo-European, Sino-Tibetan), a language isolate thought to be unrelated to any existing language (Basque), and two different subfamilies within the Indo-European family (West Germanic, Romance). The primary constraint was the availability of syllabified datasets for training and testing. Table TABREF",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How faster is training and decoding compared to former models?",
    "llm_answer": "The provided text states that the model results in faster decoding time, but does not compare it to other models in a way that quantifies the speed difference.  The answer is not found in the context.\n",
    "context": [
      {
        "id": "d6d73f8b-5fd0-4855-a602-ed4c92920687",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1707.00110",
          "start_idx": 14339,
          "end_idx": 14828
        },
        "page_content": ". Table 3 shows that our model results in faster decoding time even on a complex dataset with a large vocabulary of 16k. We measured decoding time over the full validation set, not including time used for model setup and data loading, averaged across 10 runs. The average sequence length for examples in this data was 35, and we expect more significant speedups for tasks with longer sequences, as suggested by our experiments on toy data. Note that in our NMT examples/experiments, $K\\app",
        "type": "Document"
      },
      {
        "id": "eeba3d27-5733-4b85-8229-26e51ff7e2cd",
        "metadata": {
          "chunk_id": 56,
          "document_id": "1912.06813",
          "start_idx": 18935,
          "end_idx": 19463
        },
        "page_content": ". First, when the network was trained from scratch without any pretraining, the performance was not satisfactory even with the full training set. With decoder pretraining, a performance boost in MCD was obtained, whereas the ASR results were similar. Nonetheless, as we reduced the training size, the performance dropped dramatically, a similar trend to that reported in BIBREF12. Finally, by incorporating encoder pretraining, the model exhibited a significant improvement in all objective measures, where the effectiveness was",
        "type": "Document"
      },
      {
        "id": "84ab4a95-87d0-4b41-8ae9-254ac961f4d4",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1810.03459",
          "start_idx": 8080,
          "end_idx": 8548
        },
        "page_content": "REF20 shows the performance of simply retraining the last layer using a single target language Assamese. Based on the observations from stage-1 model in section SECREF22 , we found that simply retraining the decoder towards a target language resulted in degrading %CER the performance from 45.6 to 61.3. This is mainly due to the difference in distribution across encoder and decoder. So, to alleviate this difference the encoder and decoder is once again retrained or",
        "type": "Document"
      },
      {
        "id": "89402f7f-66e9-450c-8d42-c30fc9bf8d45",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1811.00942",
          "start_idx": 12674,
          "end_idx": 13079
        },
        "page_content": " over 1.2 seconds per next-word prediction. Neural models achieve perplexity drops of 60\u201380% and R@3 increases of 22\u201334%, but these improvements come at a much higher cost in latency and energy usage. In Table 2 (last two columns), the desktop yields very different results: the neural models on PTB (rows 2\u20133) are 9 $\\times $ slower than KN-5, but the absolute latency is only 8 ms/q, which is still much",
        "type": "Document"
      },
      {
        "id": "4be6a969-03cc-432c-9085-17e4e4538592",
        "metadata": {
          "chunk_id": 74,
          "document_id": "1909.02480",
          "start_idx": 23944,
          "end_idx": 24420
        },
        "page_content": " iterative refinement to further improve the performance of FlowSeq has been left to future work. In this section, we compare the decoding speed (measured in average time in seconds required to decode one sentence) of FlowSeq at test time with that of the autoregressive Transformer model. We use the test set of WMT14 EN-DE for evaluation and all experiments are conducted on a single NVIDIA TITAN X GPU. First, we investigate how different decoding batch size can affect the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What datasets are used to evaluate the model?",
    "llm_answer": "Oshiete-goo dataset, nfL6 dataset, and corpora listed in Table TABREF12.\n",
    "context": [
      {
        "id": "cb26796d-fa6a-45cf-840c-0901678f1e04",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1909.00154",
          "start_idx": 24630,
          "end_idx": 25057
        },
        "page_content": " vectors) Development set: 20% of the dataset (2003 vectors) Test set: 20% of the dataset (2003 vectors) The PyLogit package BIBREF11 also uses Swissmetro as an example. Therefore, our model specifications will extend the default one from this package. We re-estimated this model with the train set and validated with testset. The results are shown in tables TABREF31 and TABREF32. Since we are comparing the models at the test",
        "type": "Document"
      },
      {
        "id": "6df49c9f-d8f5-4d56-a86b-73d9ec27c8c3",
        "metadata": {
          "chunk_id": 70,
          "document_id": "1912.00864",
          "start_idx": 22055,
          "end_idx": 22516
        },
        "page_content": " three evaluations using the Oshiete-goo dataset; we selected three different sets of 500 human-annotated test pairs from the full dataset. In each set, we trained the model by using training pairs and input questions in test pairs to the model. We repeated the experiments three times by randomly shuffling the train/test sets. For the evaluations using the nfL6 dataset, we prepared three different sets of 500 human-annotated test q-c-s triples from the full",
        "type": "Document"
      },
      {
        "id": "11bed98c-eee8-466c-b192-b4d7bee2693b",
        "metadata": {
          "chunk_id": 56,
          "document_id": "1909.00154",
          "start_idx": 21118,
          "end_idx": 21619
        },
        "page_content": " different experiments in the paper (and report the respective mean and standard deviation statistics). Whenever we want to analyse a particular model (e.g. to check the coefficients of a choice model), we select the one with the highest likelihood at the development set (i.e. in practice, its out-of-sample generalization performance), and report its performance on the test set. Since a choice model will typically involve other variables than the categorical ones that we learn the embeddings for,",
        "type": "Document"
      },
      {
        "id": "68b224ce-6d67-46b7-a019-e8988c49c3ab",
        "metadata": {
          "chunk_id": 57,
          "document_id": "1909.05438",
          "start_idx": 22119,
          "end_idx": 22656
        },
        "page_content": " starts from examples covered by rules, which are used to initialize a semantic parser and a question generator in a back-translation paradigm. Generated examples are measured and filtered based on statistic analysis, and then used with model-agnostic meta-learning, which guarantees model's accuracy and stability on rule-covered examples, and acquires the versatility to generalize well on rule-uncovered examples. We conduct experiments on three datasets for table-based and knowledge-based question answering tasks. Results show that",
        "type": "Document"
      },
      {
        "id": "65333987-2358-483b-95cd-3ff028ff6a96",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the source of the dataset?",
    "llm_answer": "The dataset sources vary.  Some come from online debate websites, the OA-STM corpus, the testing images of MS COCO dataset, and the testing questions of US DARPA.\n",
    "context": [
      {
        "id": "6b7f6ca6-24d5-4144-9cfd-92be2de31f39",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "75c8f911-af20-49c1-8b12-e4aecd8370c1",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1707.03904",
          "start_idx": 5304,
          "end_idx": 5850
        },
        "page_content": " subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short. We plan to release these annotations along with the datasets, and our retrieved documents for each question. Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated",
        "type": "Document"
      },
      {
        "id": "5aae2354-e125-4179-9260-e8eac191d87c",
        "metadata": {
          "chunk_id": 99,
          "document_id": "1701.00185",
          "start_idx": 35418,
          "end_idx": 35678
        },
        "page_content": "ASQ for making the datasets available. This work is supported by the National Natural Science Foundation of China (No. 61602479, No. 61303172, No. 61403385) and the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDB02070005).",
        "type": "Document"
      },
      {
        "id": "8410bd17-7c9f-4f27-98e4-c5a96b13e1b5",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1703.06492",
          "start_idx": 9989,
          "end_idx": 10376
        },
        "page_content": " dataset, called Basic Question Dataset (BQD), generated by our basic question generation algorithm. BQD is the first basic question dataset. Regarding the BQD, the dataset format is $\\lbrace Image,~MQ,~3~(BQ + corresponding~similarity~score)\\rbrace $ . All of our images are from the testing images of MS COCO dataset BIBREF30 , the MQ, main questions, are from the testing questions of",
        "type": "Document"
      },
      {
        "id": "00ea7799-d261-4f01-b6cc-6b0dec5e75e6",
        "metadata": {
          "chunk_id": 64,
          "document_id": "1906.03538",
          "start_idx": 25144,
          "end_idx": 25628
        },
        "page_content": " US Defense Advanced Research Projects Agency (DARPA). The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. We provide brief statistics on the sources of different content in our dataset in Table TABREF46 . In particular, this table shows: the size of the data collected from online debate websites (step 1). the size of the data filtered out (step 2a). the size of the perspectives added",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What were the baselines?",
    "llm_answer": "Baseline 1: smooth L1 distance; Baseline 2: PCA-based acoustic similarity; Baseline 3: Nonlinear dynamical systems-based complexity; ANRES_Sing_R; ANRES_Sing_Z; ANRES_Seq_R; ANRES_Seq_Z\n",
    "context": [
      {
        "id": "94c43990-47e3-478c-a415-088eeac582b5",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1804.08782",
          "start_idx": 13990,
          "end_idx": 14427
        },
        "page_content": " number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 .  For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose",
        "type": "Document"
      },
      {
        "id": "3747343d-0517-442a-b7ba-66d01be55b6d",
        "metadata": {
          "chunk_id": 40,
          "document_id": "2004.01878",
          "start_idx": 14526,
          "end_idx": 14915
        },
        "page_content": " normal distribution $\\mathcal {N}(0, 1)$. In summary, the following four baselines are designed: ANRES_Sing_R: randomly initializing the states for each single trading day. ANRES_Sing_Z: initializing the states as zeros for each single trading day. ANRES_Seq_R: randomly initializing the first states for each trading sequence only. ANRES_Seq_Z: initializing the first states as zeros for",
        "type": "Document"
      },
      {
        "id": "7bb4c95c-4811-4e15-ab8e-deca3c53de73",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1709.08299",
          "start_idx": 9435,
          "end_idx": 10003
        },
        "page_content": " questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models. The",
        "type": "Document"
      },
      {
        "id": "f297d78b-0fdd-4eee-ae9e-9e75a2e66130",
        "metadata": {
          "chunk_id": 140,
          "document_id": "2001.02380",
          "start_idx": 51470,
          "end_idx": 51956
        },
        "page_content": " baselines. The left, middle and right panels in Figure FIGREF40 correspond to measurements when all signals are included, only cases contained entirely in the head EDUs shown to the model, and only DMs, respectively. The scenario on the left is rather unreasonable and is included only for completeness: here the model is also penalized for not detecting signals such as lexical chains, part of which is outside the units that the model is being shown. An example of such a case can be",
        "type": "Document"
      },
      {
        "id": "3ff0334d-c1b6-4522-b5b5-ee27747097d6",
        "metadata": {
          "chunk_id": 63,
          "document_id": "2004.01980",
          "start_idx": 22218,
          "end_idx": 22603
        },
        "page_content": " all baselines. We use the summarization-related evaluation metrics, i.e., BLEU, ROUGE, CIDEr, and METEOR, to measure how relevant the generated headlines are to the news articles, to some extent, by comparing them to the original human-written headlines. In Table TABREF59, the first row \u201cNHG\u201d shows the performance of the current state-of-the-art summarization model on our data, and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What metadata is included?",
    "llm_answer": "Normalized representation of the service interface (name, tasks, entities, descriptions).\n",
    "context": [
      {
        "id": "b7f721d5-444e-412e-b2cb-d119dfdb5898",
        "metadata": {
          "chunk_id": 21,
          "document_id": "2002.01359",
          "start_idx": 9023,
          "end_idx": 9552
        },
        "page_content": " optional parameters. In addition to the dialogues, for each service used in the dataset, a normalized representation of the interface exposed is provided as the schema. The schema contains details like the name of the service, the list of tasks supported by the service (intents) and the attributes of the entities used by the service (slots). The schema also contains natural language descriptions of the service, intents and slots which can be used for developing models which can condition their predictions on the schema. To",
        "type": "Document"
      },
      {
        "id": "e2213de9-c25d-4817-80ce-de1477d66a0b",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1909.03242",
          "start_idx": 23711,
          "end_idx": 24250
        },
        "page_content": " no metadata at all. To further investigate this, we split the test set into those instances for which no metadata is available vs. those for which metadata is available. We find that encoding metadata within the model hurts performance for domains where no metadata is available, but improves performance where it is. In practice, an ensemble of both types of models would be sensible, as well as exploring more involved methods of encoding metadata. An analysis of labels frequently confused with one another, for the largest domain `pom",
        "type": "Document"
      },
      {
        "id": "a7dd4b83-a423-4210-8cf5-a2555b3488d8",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1911.07228",
          "start_idx": 6150,
          "end_idx": 6611
        },
        "page_content": " corpus which are provided by VLSP 2016 - Named Entity Recognition. The dataset contains four different types of label: Location (LOC), Person (PER), Organization (ORG) and Miscellaneous - Name of an entity that do not belong to 3 types above (Table TABREF15). Although the corpus has more information about the POS and chunks, but we do not use them as features in our model. There are two folders with 267 text files of training data and 45 text files of test",
        "type": "Document"
      },
      {
        "id": "82c8dd79-f34e-4f24-a750-66441024005b",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1610.00879",
          "start_idx": 4217,
          "end_idx": 4654
        },
        "page_content": "damn', along with profane words. Heuristics other than these hashtags could have been used for dataset creation. For example, timestamps were a good option to account for time at which a tweet was posted. However, this could not be used because user's local times was not available, since very few users had geolocation enabled. The complete set of features is shown in Table TABREF7 . There are two sets of features: (a) N-gram features",
        "type": "Document"
      },
      {
        "id": "a6b29bb9-376b-44f0-98a0-79c305b4f0ce",
        "metadata": {
          "chunk_id": 39,
          "document_id": "2004.03034",
          "start_idx": 15301,
          "end_idx": 15680
        },
        "page_content": ", ratio of quotation marks, exclamation marks, modal verbs, stop words, type-token ratio, hedging BIBREF29, named entity types, POS n-grams, sentiment BIBREF30 and subjectivity scores BIBREF31, spell-checking, readibility features such as Coleman-Liau BIBREF32, Flesch BIBREF33, argument lexicon features BIBREF34 and surface features such as word lengths, sentence lengths, word",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Do the images have multilingual annotations or monolingual ones?",
    "llm_answer": "Monolingual.\n",
    "context": [
      {
        "id": "d2a1b3d8-44d6-454e-ade7-2096ee5fc4f9",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1909.09270",
          "start_idx": 14123,
          "end_idx": 14565
        },
        "page_content": "hin: LDC2017E62), and Somali (som: LDC2016E91). These come from a variety of sources including discussion forums, newswire, and social media. The labelset is Person, Organization, Location, Geo-political entity. We define train/development/test splits, taking care to keep a similar distribution of genres in each split. Data statistics for all languages are shown in Table TABREF25. We create partial annotations by perturbing gold annotated",
        "type": "Document"
      },
      {
        "id": "d7ec6345-56c7-4697-b2e9-66a9e6a51509",
        "metadata": {
          "chunk_id": 18,
          "document_id": "2002.02427",
          "start_idx": 6346,
          "end_idx": 6847
        },
        "page_content": "-art models in monolingual ID but to investigate which of the monolingual architectures (neural or feature-based) can achieve comparable results with existing systems. The result can show which kind of features works better in the monolingual settings and can be employed to detect irony in a multilingual setting. In addition, it can show us to what extend ID is language dependent by comparing their results to multilingual results. Two models have been built, as explained below. Prior to learning,",
        "type": "Document"
      },
      {
        "id": "27843844-28c4-4a97-a733-909c6a489bfc",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1905.12260",
          "start_idx": 20273,
          "end_idx": 20780
        },
        "page_content": " by BIBREF21 . Results for crosslingual semantic similarity are presented in Table 1 , and results for multilingual document classification are presented in Table 2 . Our experiments corresponding to Section \"Leveraging Image Understanding\" are titled ImageVec 100-Dim and ImageVec 300-Dim in Tables 1 and 2 . Both experiments significantly outperform the multiSkip experiments in all crosslingual semantic similarity subtasks, and the 300-dimensional experiment slightly outperforms MUSE as well. Note that",
        "type": "Document"
      },
      {
        "id": "60ccceb6-cceb-416a-a4d9-79fe06e62c09",
        "metadata": {
          "chunk_id": 24,
          "document_id": "2002.10361",
          "start_idx": 9889,
          "end_idx": 10441
        },
        "page_content": " while acknowledging that our annotations are not ground truth. We obtained the annotations from the crowdsourcing website, Figure Eight (https://figure-eight.com/). We randomly sampled 50 users whose attributes came from Face++ in each language. We anonymize the user profiles and feed the information to the crowdsourcing website. Three annotators annotated each user photo with the binary demographic categories. To select qualified annotators and ensure quality of the evaluations, we set up 5 golden standard annotation questions for each language",
        "type": "Document"
      },
      {
        "id": "de7e83e8-1232-44f6-af34-dd6be6505c53",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1912.07025",
          "start_idx": 7473,
          "end_idx": 8015
        },
        "page_content": " annotation. Our curated selection aims to maximize the diversity of the dataset in terms of various attributes such as the extent of document degradation, script language, presence of non-textual elements (e.g. pictures, tables) and number of lines. Some images contain multiple manuscript pages stacked vertically or horizontally (see bottom-left image in Figure FIGREF1). The second source for manuscript images in our dataset is Bhoomi, an assorted collection of 315 images sourced from multiple Oriental Research Institutes and libraries",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much important is the visual grounding in the learning of the multilingual representations?",
    "llm_answer": "The provided text states that image representations leverage generic image features, but does not explicitly state the importance of visual grounding in learning multilingual representations.  It is implied that visual grounding is a factor but not the primary focus of the investigation.\n",
    "context": [
      {
        "id": "7fd20be5-6971-48d6-96a8-cfc4101fd2bf",
        "metadata": {
          "chunk_id": 94,
          "document_id": "1807.03367",
          "start_idx": 35549,
          "end_idx": 36052
        },
        "page_content": " shown that grounding can lead to practical improvements on various natural language understanding tasks BIBREF14 , BIBREF31 . In robotics, researchers dissatisfied with purely symbolic accounts of meaning attempted to build robotic systems with the aim of grounding meaning in physical experience of the world BIBREF44 , BIBREF46 . Recently, grounding has also been applied to the learning of sentence representations BIBREF32 , image captioning BIBREF37 , BIBREF49 , visual question answering BIBREF12",
        "type": "Document"
      },
      {
        "id": "d620392d-3970-4912-a5ad-2704a73f5d70",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1809.08510",
          "start_idx": 13149,
          "end_idx": 13685
        },
        "page_content": "G representations are useful in situations when there is a lack of data in an specific language. The language agnostics properties of UG embeddings allows us to do successful zero-shot learning without needing any parallel corpus, furthermore the ability to generalize from language modeling to sentiment attests for the universal properties of these representations. Although we aren't able to improve over the state of the art in a single language we are able to learn a model that does surprisingly well on a set of languages without",
        "type": "Document"
      },
      {
        "id": "6e5638a3-edbb-4487-ba09-d38beade88ce",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1905.12260",
          "start_idx": 17707,
          "end_idx": 18119
        },
        "page_content": " \u201cpain\u201d) have a shared embedding. In this sense, shared surface forms are used as a bridge between languages. This is illustrated in Figure 3 . This may be helpful in certain cases, as for English \u201cactor\u201d and Spanish \u201cactor\u201d. The image representations leverage generic image features, exactly as in Section \"Leveraging Image Understanding\" . In our language-unaware experiment, we use $m = 200$ and $n = 100$ and",
        "type": "Document"
      },
      {
        "id": "8f8dd64e-f425-4343-a1d6-d03aa3cd20f3",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1807.03367",
          "start_idx": 12465,
          "end_idx": 12974
        },
        "page_content": "7 or text recognition features BIBREF8 barely outperform a random baseline\u2014see Appendix SECREF13 for full details. This finding implies that localization models from image input are limited by their ability to recognize landmarks, and, as a result, would not generalize to unseen environments. To ensure that perception is not the limiting factor when investigating the landmark-grounding and action-grounding capabilities of localization models, we assume \u201cperfect perception\u201d: in lieu of the 360 image view,",
        "type": "Document"
      },
      {
        "id": "8dc9de54-972a-49e5-a49b-e65a88626cff",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1809.04686",
          "start_idx": 1578,
          "end_idx": 2066
        },
        "page_content": " or multilingual NLP, the task of transferring knowledge from one language to another, serves as a good test bed for evaluating various transfer learning approaches. For cross-lingual NLP, the most widely studied approach is to use multilingual embeddings as features in neural network models. However, research has shown that representations learned in context are more effective BIBREF5 , BIBREF6 ; therefore, we aim at doing better than just using multilingual embeddings in the cross-",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the generative model evaluated?",
    "llm_answer": "A classifier is trained on the generated dataset, and its accuracy on the original test set is compared to the accuracy of a classifier trained on the original training set and tested on the original test set.\n",
    "context": [
      {
        "id": "a7cbc298-f7d0-44fd-a766-fb8583fa594c",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1607.06025",
          "start_idx": 4370,
          "end_idx": 4877
        },
        "page_content": " for every example in the original dataset to construct a new dataset. Next, a classifier is trained on the new dataset. Finally, the classifier is evaluated on the original test set. The accuracy of the classifier is the proposed quality metric for the generative model. It can be compared to the accuracy of the classifier trained on the original training set and tested on the original test set. The generative models learn solely from the original training set to regenerate the dataset. Thus, the model",
        "type": "Document"
      },
      {
        "id": "22f1a8af-99ab-45a3-9847-c0b8cac31678",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1607.06025",
          "start_idx": 6054,
          "end_idx": 6581
        },
        "page_content": " evaluation metric for NLI generative models \u2013 the accuracy of the classifier trained on the generated dataset. In Section SECREF2 we present the related work. In Section SECREF3 the considered neural networks are presented. Besides the main generative networks, we also present classification and discriminative networks, which are used for evaluation. The results are presented in Section SECREF5 , where the generative models are evaluated and compared. From the experiments we can see that the best dataset was generated by",
        "type": "Document"
      },
      {
        "id": "8eff2d21-9d0b-4d67-ae31-707c53c7f7e4",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1607.06025",
          "start_idx": 4668,
          "end_idx": 5227
        },
        "page_content": " The generative models learn solely from the original training set to regenerate the dataset. Thus, the model learns the distribution of the original dataset. Furthermore, the generated dataset is just a random sample from the estimated distribution. To determine how well did the generative model learn the distribution, we observe how close does the accuracy of the classifier trained on the generated dataset approach the accuracy of classifier trained on the original dataset. Our flagship generative network EmbedDecoder works in a similar fashion as the",
        "type": "Document"
      },
      {
        "id": "93948151-2308-46c3-aaa9-814a1e670c30",
        "metadata": {
          "chunk_id": 53,
          "document_id": "1809.04960",
          "start_idx": 20749,
          "end_idx": 21194
        },
        "page_content": "world settings. Generative-based models directly generate comments without a candidate set. We compare the generated comments of either the retrieval-based models or the generative models with the five reference comments. We select four popular metrics in text generation to compare the model outputs with the references: BLEU BIBREF14 , METEOR BIBREF15 , ROUGE BIBREF16 , CIDEr BIBREF17 . Table TABREF32 shows the performance for our models and",
        "type": "Document"
      },
      {
        "id": "6e67c754-c63d-4bc8-97ef-32f3f5df38d9",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1607.06025",
          "start_idx": 3868,
          "end_idx": 4370
        },
        "page_content": " of generative models, which is also presented in Figure FIGREF2 . First, the generative model is trained on the original training dataset. Then, the premise and label from an example in the original dataset are taken as the input to the generative model to generate a new random hypothesis. The generated hypothesis is combined with the premise and the label to form a new unseen example. This is done for every example in the original dataset to construct a new dataset. Next, a classifier is trained",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is an example of a health-related tweet?",
    "llm_answer": "The context provides \"diabetes, diet, exercise, and obesity\" as examples of health-related topics discussed in tweets.\n",
    "context": [
      {
        "id": "db50406f-15d4-4502-ad04-2aeb21db592b",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1805.09959",
          "start_idx": 19021,
          "end_idx": 19599
        },
        "page_content": "23) without appropriate healthcare insurance coverage. The vote was later postponed and eventually failed, BIBREF29 . Public outcry likely influenced this legal outcome, demonstrating Twitter's innovative potential as a support tool for public lobbying of health benefits. Twitter can further be used to remind, motivate and change individual and population health behavior using messages of encouragement (translated to happiness) or dissatisfaction (translated to diminished happiness), for example, with memes that can have knock on social consequences when they are re-tweet",
        "type": "Document"
      },
      {
        "id": "6639e681-d7ea-464c-90df-f59954908efa",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1901.00439",
          "start_idx": 0,
          "end_idx": 485
        },
        "page_content": "Social media plays an important role in health informatics and Twitter has been one of the most influential social media channel for mining population-level health insights BIBREF0 , BIBREF1 , BIBREF2 . These insights range from forecasting of influenza epidemics BIBREF3 to predicting adverse drug reactions BIBREF4 . A notable challenge due to the short length of Twitter messages is categorization of tweets into topics in a supervised manner, i.e., topic classification, as well as",
        "type": "Document"
      },
      {
        "id": "897de222-a484-4028-9586-6c9c0e988441",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1709.07916",
          "start_idx": 4288,
          "end_idx": 4806
        },
        "page_content": " uses semantic and linguistics analyses for disclosing health characteristics of opinions in tweets containing DDEO words. The present study included three phases: data collection, topic discovery, and topic-content analysis. This phase collected tweets using Twitter's Application Programming Interfaces (API) BIBREF43 . Within the Twitter API, diabetes, diet, exercise, and obesity were selected as the related words BIBREF4 and the related health areas BIBREF19 . Twitter's APIs provides both historic and real-time",
        "type": "Document"
      },
      {
        "id": "e5d184df-68c5-45fb-8cee-953a942c8f36",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1709.07916",
          "start_idx": 2788,
          "end_idx": 3144
        },
        "page_content": " BIBREF20 , BIBREF21 , BIBREF22 . Twitter data has been used for a wide range of health and non-health related applications, such as stock market BIBREF23 and election analysis BIBREF24 . Some examples of Twitter data analysis for health-related topics include: flu BIBREF25 , BIBREF26 , BIBREF27 , BIBREF28 , BIBREF29 , BIBREF30 , mental health BIBREF31 ,",
        "type": "Document"
      },
      {
        "id": "c048b25e-f409-4c28-b325-41daae3eb26a",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1709.07916",
          "start_idx": 3349,
          "end_idx": 3907
        },
        "page_content": " BIBREF42 , BIBREF21 . The previous Twitter studies have dealt with extracting common topics of one health issue discussed by the users to better understand common themes; however, this study utilizes an innovative approach to computationally analyze unstructured health related text data exchanged via Twitter to characterize health opinions regarding four common health issues, including diabetes, diet, exercise and obesity (DDEO) on a population level. This study identifies the characteristics of the most common health opinions with respect to DDEO and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the challenge for other language except English",
    "llm_answer": "The provided text describes challenges for Spanish, Italian, and Zulu, as well as comparisons with results from other languages in international contests.  It also details a challenge focused on non-native children's speech recognition, with an English subset.  No single, overarching \"challenge for other languages\" is described.\n",
    "context": [
      {
        "id": "910e2fe4-f2a4-42fb-afd4-880f58ce8885",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1910.05456",
          "start_idx": 9884,
          "end_idx": 10340
        },
        "page_content": " additionally makes use of internal stem changes (e.g., o $\\rightarrow $ ue). Since English and Spanish are both Indo-European languages, and, thus, relatively similar, we further add a third, unrelated target language. We choose Zulu (ZUL), a Bantoid language. In contrast to the first two, it is strongly prefixing. For pretraining, we choose languages with different degrees of relatedness and varying morphological similarity to English, Spanish, and Z",
        "type": "Document"
      },
      {
        "id": "10a35031-f90d-415f-aa3f-734db4bbcba6",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1612.05270",
          "start_idx": 1650,
          "end_idx": 2169
        },
        "page_content": " tested in eight different languages: Spanish, English, Italian, Arabic, German, Portuguese, Russian and Swedish. We compare our approach ranking in three international contests: TASS'15, SemEval'15-16 and SENTIPOLC'14, for Spanish, English and Italian respectively; the remaining languages are compared directly with the results reported in the literature. The experimental results locate our approach in good positions for all considered competitions; and excellent results in the other five languages tested. Finally",
        "type": "Document"
      },
      {
        "id": "de1d89c2-dce0-44b5-8664-923e9eea97f2",
        "metadata": {
          "chunk_id": 13,
          "document_id": "2001.08051",
          "start_idx": 5133,
          "end_idx": 5580
        },
        "page_content": " Swiss students learning English had to answer to both written and spoken prompts. The goal of this challenge is to label students' spoken responses as \u201caccept\u201d or \u201creject\u201d. Refer to BIBREF5 for details of the challenge and of the associated data sets. Many non-native speech corpora (mostly in English as target language) have been collected during the years. A list, though not recent, as well as a brief description of most of them can be found",
        "type": "Document"
      },
      {
        "id": "f51f0d19-9076-4f5a-94a7-354440dbc939",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1911.03562",
          "start_idx": 15671,
          "end_idx": 16139
        },
        "page_content": " world for the speakers of that language. We know that much of the research in NLP is on English or uses English datasets. Many reasons have been proffered, and we will not go into that here. Instead, we will focus on estimating how much research pertains to non-English languages. We will make use of the idea that often when work is done focusing on a non-English language, then the language is mentioned in the title. We collected a list of 122 languages indexed by",
        "type": "Document"
      },
      {
        "id": "31be8618-b8d0-43af-878e-6b6a37dfc5b0",
        "metadata": {
          "chunk_id": 53,
          "document_id": "2001.08051",
          "start_idx": 20668,
          "end_idx": 21204
        },
        "page_content": " check the available human scores and - if possible - merge or map the scores according to more general performance categories (e.g. delivery, language use, topic development) and an acknowledged scale (e.g. from 0 to 4). In particular, the proposal for an international challenge focused on non-native children speech recognition is being submitted where an English subset will be released and the perspective participants are invited to propose and evaluate state-of-art techniques for dealing with the multiple issues related to this",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many categories of offensive language were there?",
    "llm_answer": "Three\n",
    "context": [
      {
        "id": "8897ad3d-dd7c-4bbe-9719-11b234fb2906",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1908.04531",
          "start_idx": 1702,
          "end_idx": 2198
        },
        "page_content": " usually done on the basis of some characteristics such as race, color, ethnicity, gender, sexual orientation, nationality or religion BIBREF3 . In section \"Background\" , hate speech is defined in more detail. Offensive language, on the other hand, is a more general category containing any type of profanity or insult. Hate speech can, therefore, be classified as a subset of offensive language. BIBREF0 propose guidelines for classifying offensive language as well as the type and the target of",
        "type": "Document"
      },
      {
        "id": "dea760f2-bdaf-471d-ab5d-b794524eb125",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1908.04531",
          "start_idx": 7347,
          "end_idx": 7867
        },
        "page_content": " the two, by classifying posts as either Hate speech, Offensive or Neither. This lack of consensus led BIBREF14 to propose annotation guidelines and introduce a typology. BIBREF17 argue that these proposed guidelines do not effectively capture both the type and target of the offensive language. In this section we give a comprehensive overview of the structure of the task and describe the dataset provided in BIBREF0 . Our work adopts this framing of the offensive language phenomenon. Offensive content is broken into",
        "type": "Document"
      },
      {
        "id": "2e9c1ae1-1d19-40ed-9190-d74c46afa2f2",
        "metadata": {
          "chunk_id": 33,
          "document_id": "1908.04531",
          "start_idx": 13119,
          "end_idx": 13652
        },
        "page_content": " comments contained within each post. We published a survey on Reddit asking Danish speaking users to suggest offensive, sexist, and racist terms for a lexicon. Language and user behaviour varies between platforms, so the goal is to capture platform-specific terms. This gave 113 offensive and hateful terms which were used to find offensive comments. The remainder of comments in the corpus were shuffled and a subset of this corpus was then used to fill the remainder of the final dataset. The resulting dataset contains 3600 user-",
        "type": "Document"
      },
      {
        "id": "79d69601-5cba-44f5-a565-4b6a50181f50",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1909.12642",
          "start_idx": 0,
          "end_idx": 540
        },
        "page_content": "In social media, abusive language denotes a text which contains any form of unacceptable language in a post or a comment. Abusive language can be divided into hate speech, offensive language and profanity. Hate speech is a derogatory comment that hurts an entire group in terms of ethnicity, race or gender. Offensive language is similar to derogatory comment, but it is targeted towards an individual. Profanity refers to any use of unacceptable language without a specific target. While profanity is the least threatening, hate speech has",
        "type": "Document"
      },
      {
        "id": "d926b72f-817c-4a1a-ab7d-63fec59e180c",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1908.04531",
          "start_idx": 8157,
          "end_idx": 8586
        },
        "page_content": "-task A the goal is to classify posts as either offensive or not. Offensive posts include insults and threats as well as any form of untargeted profanity BIBREF17 . Each sample is annotated with one of the following labels: In English this could be a post such as #TheNunMovie was just as scary as I thought it would be. Clearly the critics don't think she is terrifyingly creepy. I like how it ties in with #TheConjuring series.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which matching features do they employ?",
    "llm_answer": "INLINEFORM6, INLINEFORM7, INLINEFORM8, INLINEFORM9, INLINEFORM10, Brown BIBREF32 clusters, Clark BIBREF33 clusters, Word2vec BIBREF34 clusters, orthographic, word shape, n-gram features.\n",
    "context": [
      {
        "id": "30c4abd6-642c-49df-b699-0919af0a8c8e",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1901.02222",
          "start_idx": 10251,
          "end_idx": 10759
        },
        "page_content": " , which preserves the different parts and removes the similar parts. And the INLINEFORM6 can be regarded as a sim-feature of INLINEFORM7 and INLINEFORM8 , which emphasizes on the similar parts and neglects the different parts between INLINEFORM9 and INLINEFORM10 . Each feature helps us focus on particular parts between the context vectors and the aligned vectors. These matching features are vector representations with low dimension, but containing high-order semantic information. To make further use of",
        "type": "Document"
      },
      {
        "id": "8e261bc8-95e2-43f7-8d68-79ba642f89db",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1901.09755",
          "start_idx": 17261,
          "end_idx": 17633
        },
        "page_content": ", shallow features based mostly on orthographic, word shape and n-gram features plus their context; and (ii) three types of simple clustering features, based on unigram matching: (i) Brown BIBREF32 clusters, taking the 4th, 8th, 12th and 20th node in the path; (ii) Clark BIBREF33 clusters and, (iii) Word2vec BIBREF34 clusters, based on K-means applied over the extracted",
        "type": "Document"
      },
      {
        "id": "115e7637-41f0-48d2-ad46-7d760711ef7c",
        "metadata": {
          "chunk_id": 30,
          "document_id": "1901.02222",
          "start_idx": 10545,
          "end_idx": 11058
        },
        "page_content": " are vector representations with low dimension, but containing high-order semantic information. To make further use of these matching features, we collect them to generate a matching sequence INLINEFORM11 . DISPLAYFORM0   where INLINEFORM0 . The output of this layer is the matching sequence INLINEFORM0 , which stores three kinds of matching features. The order of the matching features in INLINEFORM1 is inspired by the attention trajectory of human beings making inference on premise and hypothesis. We process",
        "type": "Document"
      },
      {
        "id": "dd5af701-fd2b-4a9d-8596-ffceb157ba08",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1902.09087",
          "start_idx": 5594,
          "end_idx": 6057
        },
        "page_content": " features to get the sentence level representations, which are merged via element-wise multiplication. The matching score is produced by a multi-layer perceptron (MLP) with one hidden layer based on the merged vector. The fusing and matching procedure is formulated as follows: DISPLAYFORM0  where INLINEFORM0 and INLINEFORM1 are feature vectors of question and candidate (sentence or predicate) separately encoded by CNNs, INLINEFORM2 is the sigmoid function, IN",
        "type": "Document"
      },
      {
        "id": "7438d13d-b41d-409c-ae1b-468573508e08",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1901.02222",
          "start_idx": 11363,
          "end_idx": 11897
        },
        "page_content": " move the attention to the different parts. Finally, we pay attention to the similar parts. In this layer, we aim to acquire inference outputs by aggregating the information in the matching sequence by multi-turn inference mechanism. We regard the inference on the matching sequence as the multi-turn interaction among various matching features. In each turn, we process one matching feature instead of all the matching features BIBREF9 , BIBREF26 . To enhance the information interaction between matching features, a memory component",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does their method outperform the multi-head attention model?",
    "llm_answer": "The context does not contain the performance figures of a *single* multi-head attention model for comparison.  It discusses the *impact* of multi-head attention, but not a direct comparison to another method.\n",
    "context": [
      {
        "id": "9ce4b5da-67b6-49de-8a9c-32f7ec8d2aea",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1910.09295",
          "start_idx": 17498,
          "end_idx": 17963
        },
        "page_content": " considerable amount of performance, constituting 44.32% of the overall performance (a boost of 42.67% in accuracy) in the multitasking setup, and constituting 43.93% of the overall performance (a boost of 39.97%) in the standard finetuning setup. This provides evidence that the pretraining step is necessary in achieving state-of-the-art performance. An ablation study was done to establish causality between the multiheaded nature of the attention mechanisms and",
        "type": "Document"
      },
      {
        "id": "8f0fb654-3989-4626-84f8-48a38c2db541",
        "metadata": {
          "chunk_id": 57,
          "document_id": "1908.06606",
          "start_idx": 20051,
          "end_idx": 20502
        },
        "page_content": " on both periods achieved the best performance on both EM-score and F$_1$-score. Unfortunately, applying multi-head attention on both period one and period two can not reach convergence in our experiments. This probably because it makes the model too complex to train. The difference on other two methods are the order of concatenation and multi-head attention. Applying multi-head attention on two named entity information $I_{nt}$ and $I_{nq}$ first",
        "type": "Document"
      },
      {
        "id": "4e0b723f-345d-4019-9fe8-19f4d4da4bb7",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1910.09295",
          "start_idx": 17963,
          "end_idx": 18444
        },
        "page_content": " An ablation study was done to establish causality between the multiheaded nature of the attention mechanisms and state-of-the-art performance. We posit that since the model can refer to multiple context points at once, it improves in performance. For this experiment, we performed several pretraining-finetuning setups with varied numbers of attention heads using the multitask-based finetuning scheme. Using a pretrained GPT-2 model, attention heads were masked with zero-tensors",
        "type": "Document"
      },
      {
        "id": "a2d5160a-8656-4de8-aa4b-56ccb1c000af",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1908.06606",
          "start_idx": 21038,
          "end_idx": 21574
        },
        "page_content": " multi-head attention method on both periods can not converge. Although Table TABREF27 shows the best integration method is concatenation, multi-head attention still has great potential. Due to the lack of computational resources, our experiment fixed the head number and hidden vector size. However, tuning these hyper parameters may have impact on the result. Tuning integration method and try to utilize larger datasets may give help to improving the performance. To investigate how shared task and shared model can benefit, we split",
        "type": "Document"
      },
      {
        "id": "a8986993-a926-4ec0-a641-759852220150",
        "metadata": {
          "chunk_id": 53,
          "document_id": "1910.09295",
          "start_idx": 18749,
          "end_idx": 19277
        },
        "page_content": " the performance of 10 heads using the standard finetuning scheme. This shows that more attention heads, thereby attending to multiple different contexts at once, is important to boosting performance to state-of-the-art results. While increasing the number of attention heads improves performance, keeping on adding extra heads will not result to an equivalent boost as the performance plateaus after a number of heads. As shown in Figure FIGREF35, the performance boost of the model plateaus after 10 attention heads, which was",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How large is the corpus they use?",
    "llm_answer": "13 million NRLS records; 1 million and 2 million records; 5 million articles from English Wikipedia.\n",
    "context": [
      {
        "id": "60fa7e17-1f20-4b48-a302-920cbf9c8b19",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1909.00183",
          "start_idx": 13387,
          "end_idx": 13902
        },
        "page_content": " of different sizes and content with different sets of hyper-parameters. . In particular, we checked the effect of corpus size by training Doc2Vec models on the full 13 million NRLS records and on randomly sampled subsets of 1 million and 2 million records. Since our target analysis has heavy medical content and specific use of words, we also tested the importance of the training corpus by generating an additional Doc2Vec model using a set of 5 million articles from the English Wikipedia representing standard,",
        "type": "Document"
      },
      {
        "id": "9c7f218a-80a4-486c-815b-422ad44b40e9",
        "metadata": {
          "chunk_id": 121,
          "document_id": "1701.02877",
          "start_idx": 43692,
          "end_idx": 44145
        },
        "page_content": " the CoNLL 2003 corpus. The seed domain for the experiments here is newswire, where we use the classifier trained on the biggest NW corpus investigated in this study, i.e. OntoNotes NW. That classifier is then applied to all other corpora. The rationale is to test how suitable such a big corpus would be for improving Twitter NER, for which only small training corpora are available. Results for out-of-domain performance are reported in Table 11 . The",
        "type": "Document"
      },
      {
        "id": "6ca9c184-80d3-4aa2-b722-d0288073e955",
        "metadata": {
          "chunk_id": 40,
          "document_id": "2001.08051",
          "start_idx": 15428,
          "end_idx": 15968
        },
        "page_content": " assigned either to training or evaluation sets, with proportions of $\\frac{2}{3}$ and $\\frac{1}{3}$, respectively; then training and evaluation lists were built, accordingly. Table reports statistics from the spoken data set. The id All identifies the whole data set, while Clean defines the subset in which sentences containing background voices, incomprehensible speech and word fragments were excluded. From the above description it appears that the corpus can be effectively used in many research directions. The spoken corpus features",
        "type": "Document"
      },
      {
        "id": "2d3bdfec-4ed5-456d-bd82-023fad798858",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "234869e5-ff45-40e2-92dc-2fddc8eec67f",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1909.02560",
          "start_idx": 18101,
          "end_idx": 18537
        },
        "page_content": " questions are paraphrases. We use the same dataset partition as BIBREF5, with 384,348/10,000/10,000 pairs in the training/development/test set respectively. MRPC BIBREF34: The Microsoft Research Paraphrase Corpus consists of sentence pairs collected from online news. Each pair is annotated with a label indicating whether the two sentences are semantically equivalent. There are 4,076/1,725 pairs in the training/test set respectively",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many shared layers are in the system?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "858cc803-6ff8-4649-aa3f-9b044b136fb0",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1902.10525",
          "start_idx": 23016,
          "end_idx": 23477
        },
        "page_content": " 5 layers outperforms more shallow networks, and using more layers gives hardly any improvement. Furthermore, using 64 nodes per layer is sufficient, as wider networks give only small improvements, if at all. Finally, we show a comparison of our old and new systems with the literature on the IAM-OnDB dataset in Table TABREF49 . Our method establishes a new state of the art result when relying on closed data using IAM-OnDB, as well as when relying on our in-",
        "type": "Document"
      },
      {
        "id": "415aa967-c234-4a69-b6c2-9dbc77f6c838",
        "metadata": {
          "chunk_id": 71,
          "document_id": "1910.02789",
          "start_idx": 28472,
          "end_idx": 28930
        },
        "page_content": " has 8 6X6 filters with stride 3 and ReLU activation. The second convolutional layer has 16 3X3 filters with stride 2 and ReLU activation. The fully connected layers has 32 and 16 units, both of them are followed by ReLU activation. The output layer's size is the amount of actions the agent has available in the trained scenario. Used in the feature vector based agent. Naturally, some discretization is needed in order to build a feature vector, so some of",
        "type": "Document"
      },
      {
        "id": "c13bef1c-15fb-4e6c-911e-216962da72d6",
        "metadata": {
          "chunk_id": 41,
          "document_id": "1701.06538",
          "start_idx": 15826,
          "end_idx": 16312
        },
        "page_content": " of these models vary from 2 million to 151 million. Quality increases greatly with parameter count, as do computational costs. Results for these models form the top line of Figure FIGREF32 -right. Our models consist of two stacked LSTM layers with a MoE layer between them (see Figure FIGREF8 ). We vary the sizes of the layers and the number of experts. For full details on model architecture, training regimen, additional baselines and results, see Appendix SECREF65 . To investigate",
        "type": "Document"
      },
      {
        "id": "081c4ff6-6c95-4f47-9823-a8f18537a421",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1908.11546",
          "start_idx": 8988,
          "end_idx": 9434
        },
        "page_content": " cell which contains separate continue, act and slots unit that are sequentially connected. The classification architecture has two fully connected layers of size 128, and the remaining models have a hidden size of 64 and a teacher-forcing rate of 0.5. Seq2Seq and Copy Seq2Seq use a beam search with beam size 10 during inference. CAS and gCAS do not adopt a beam search since their inference steps are much less than Seq2Seq methods. All models",
        "type": "Document"
      },
      {
        "id": "771051a7-e058-4364-9106-812923bfdb3c",
        "metadata": {
          "chunk_id": 34,
          "document_id": "2001.11268",
          "start_idx": 13733,
          "end_idx": 14201
        },
        "page_content": " vectors can be obtained for each of the network's layers, and multiple layers can be represented together by concatenation and pooling. We used the t-distributed Stochastic Neighbour Embedding (t-SNE) algorithm to reduce each layer-embedding into two-dimensional space, and plotted the resulting values. Additionally, we computed adjusted rand scores in order to evaluate how well each layer (or concatenation thereof, always using reduce_mean pooling) represents our",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many additional task-specific layers are introduced?",
    "llm_answer": "Two sets of task-specific BiRNN layers.\n",
    "context": [
      {
        "id": "b735b8fa-87c1-4b5b-aeea-62dbb1cb8a35",
        "metadata": {
          "chunk_id": 10,
          "document_id": "2002.06424",
          "start_idx": 3858,
          "end_idx": 4313
        },
        "page_content": " RE tasks is not constant across all textual domains, we take the number of shared and task-specific layers to be an explicit hyperparameter of the model that can be tuned separately for different datasets. We evaluate the proposed architecture on two publicly available datasets: the Adverse Drug Events (ADE) dataset BIBREF6 and the CoNLL04 dataset BIBREF7. We show that our architecture is able to outperform the current state-of-the-art (SOTA) results",
        "type": "Document"
      },
      {
        "id": "f09a91b1-4644-464b-928f-3323865329ed",
        "metadata": {
          "chunk_id": 31,
          "document_id": "2002.06424",
          "start_idx": 11350,
          "end_idx": 11786
        },
        "page_content": " tasks. We take the number of shared and task-specific layers to be a hyperparameter of the model. Both sets of task-specific BiRNN layers are followed by task-specific scoring and output layers. Figure FIGREF4 illustrates this architecture. Below, we use superscript $e$ for NER-specific variables and layers and superscript $r$ for RE-specific variables and layers. We obtain contextual token embeddings using the pre-trained ELMo 5.5",
        "type": "Document"
      },
      {
        "id": "df8ca8cd-9168-4524-b92b-f4432887c8aa",
        "metadata": {
          "chunk_id": 76,
          "document_id": "2002.06424",
          "start_idx": 25311,
          "end_idx": 25794
        },
        "page_content": " work may explore ways to select this aspect of the model architecture in a more principled way. For example, Vandenhende et al. vandenhende2019branched propose using a measure of affinity between tasks to determine how many layers to share in MTL networks. Task affinity scores of NER and RE could be computed for different textual domains or datasets, which could then guide the decision regarding the number of shared and task-specific layers to employ for joint NER and RE models",
        "type": "Document"
      },
      {
        "id": "023fac94-480a-4e01-8bf8-ceff8cbad620",
        "metadata": {
          "chunk_id": 75,
          "document_id": "2002.06424",
          "start_idx": 25019,
          "end_idx": 25527
        },
        "page_content": " across domains with little domain-specific hyperparameter tuning. On the other hand, the similarity between the NER and RE tasks varies across domains, and improved performance can be achieved on these tasks by tuning the number of shared and task-specific parameters. In our work, we treated the number of shared and task-specific layers as a hyperparameter to be tuned for each dataset, but future work may explore ways to select this aspect of the model architecture in a more principled way. For example",
        "type": "Document"
      },
      {
        "id": "0ae82ead-66c2-4749-acde-070077fa0103",
        "metadata": {
          "chunk_id": 28,
          "document_id": "2002.06424",
          "start_idx": 10121,
          "end_idx": 10621
        },
        "page_content": " to joint NER and RE have experimented little with deep task-specificity, with the exception of those models that include additional layers for the RE task. To our knowledge, no work has considered including additional NER-specific layers beyond scoring and/or output layers. This may reflect a residual influence of the pipeline approach in which the NER task must be solved first before additional layers are used to solve the RE task. However, there is no a priori reason to think that the RE task",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many layers of self-attention does the model have?",
    "llm_answer": "10\n",
    "context": [
      {
        "id": "03ec3ca7-b715-4096-af17-73d2e13173f8",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1907.02636",
          "start_idx": 15243,
          "end_idx": 15664
        },
        "page_content": " dimensions of character embedding, hidden states of character-based token embedding LSTM, hidden states of Bi-LSTM, and sentence attention are set to 25, 25, 100 and 100, respectively. For multi-head self-attention module, we employ a stack of 6 multi-head self attention layer, each of which has 4 head and dimension of each head is set to 64. (b) All of the ANN\u2019s parameters are initialized with a uniform distribution",
        "type": "Document"
      },
      {
        "id": "9d073833-947f-4e73-8c62-2edf797b8afd",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1901.02262",
          "start_idx": 10964,
          "end_idx": 11316
        },
        "page_content": " . As in BIBREF12 , the self-attention sub-layer uses a sub-sequent mask to prevent positions from attending to subsequent positions. The second and third sub-layers perform the multi-head attention over $M^q$ and $M^{p_\\mathrm {all}}$ , respectively. The $M^{p_\\mathrm {all}}$ is the concatenated outputs of the encoder stack for the passages,  $$M^{p",
        "type": "Document"
      },
      {
        "id": "7d2a4c18-100a-4ee2-966d-181ba478a17e",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1910.11204",
          "start_idx": 12494,
          "end_idx": 12912
        },
        "page_content": " argument and predicate in one sentence. Compared to the standard attention, in this paper, we add the dependency information into $Q$ and $V$ in each attention head, like equation (DISPLAY_FORM15) shows: where $E_D$ and $E_R$ mean the syntactic dependency head and relation information respectively. For our multi-layer multi-head self-attention model, we make this change to each head of the first $N$ self-attention",
        "type": "Document"
      },
      {
        "id": "b26be19c-b4d7-4cda-adc9-386689c9bb37",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1901.02262",
          "start_idx": 5146,
          "end_idx": 5553
        },
        "page_content": " It consists of two sub-layers: a self-attention layer and a position-wise feed-forward network. For the self-attention layer, we adopt the multi-head attention mechanism defined in BIBREF12 . The feed-forward network consists of two linear transformations with a GELU BIBREF18 activation in between, following OpenAI GPT BIBREF19 . Each sub-layer is placed inside a residual block BIBREF20 . For an input $",
        "type": "Document"
      },
      {
        "id": "e19aa9b0-4379-48c1-a000-2135969b26dc",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1910.11204",
          "start_idx": 14972,
          "end_idx": 15357
        },
        "page_content": "}=d_f+d_p$. The hidden dimension $d_{ff}=800$ is applied for all the experiments. We set the number of shared self-attention blocks $N=10$. The number of heads varies with $d_{model}$, but dimension of each head is 25. Besides, LISA incorporates syntax knowledge in the 5-th self-attention layer while RelAwe incorporates in the first 5 layers. We apply the similar dropout strategy as",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what are the state of the art methods?",
    "llm_answer": "DNN methods,  Maximum Entropy Markov Model (MEMM), Support Vector Machine (SVM), and Conditional Random Field (CRF).\n",
    "context": [
      {
        "id": "1f140aea-67fe-42b4-983e-2e4e083c362a",
        "metadata": {
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "4794b743-1694-4c35-9149-86a3a13dcc8d",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1911.07228",
          "start_idx": 2445,
          "end_idx": 2848
        },
        "page_content": " used traditional machine learning methods such as Maximum Entropy Markov Model (MEMM), Support Vector Machine (SVM) and Conditional Random Field (CRF). In particular, most of the toolkits for NER task attempted to use MEMM BIBREF4, and CRF BIBREF5 to solve this problem. Nowadays, because of the increase in data, DNN methods are used a lot. They have archived great results when it comes to NER tasks,",
        "type": "Document"
      },
      {
        "id": "82f55fdc-e3a1-40ec-88ba-b2dd4453e358",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1910.06748",
          "start_idx": 21361,
          "end_idx": 21867
        },
        "page_content": "10. We obtained publicly-available implementations of each of these algorithms, and test them all against our three datasets. In Table TABREF33, we report each algorithm's accuracy and F1 score, the two metrics usually reported in the LID literature. We also included precision and recall values, which are necessary for computing F1 score. And finally we included the speed in number of messages handled per second. This metric is not often discussed in the LID literature, but is of particular importance",
        "type": "Document"
      },
      {
        "id": "a4a4c58b-d1f1-46a1-9912-24f7a070ad68",
        "metadata": {
          "chunk_id": 43,
          "document_id": "2001.00137",
          "start_idx": 16781,
          "end_idx": 17197
        },
        "page_content": " the online platforms. Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector",
        "type": "Document"
      },
      {
        "id": "5f2b0d28-39e5-4788-ad3c-8ff924aaf332",
        "metadata": {
          "chunk_id": 39,
          "document_id": "2003.06279",
          "start_idx": 15274,
          "end_idx": 15751
        },
        "page_content": " (DT), nearest neighbors (kNN), Naive Bayes (NB) and Support Vector Machines (SVM). We used some heuristics to optimize classifier parameters. Such techniques are described in the literature BIBREF49. The accuracy of the pattern recognition methods were evaluated using cross-validation BIBREF50. In summary, the methodology used in this paper encompasses the following steps: Network construction: here texts are mapped into a co-occurrence networks. Some variations exists in",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which four languages do they experiment with?",
    "llm_answer": "Spanish, English, Italian, and Russian.\n",
    "context": [
      {
        "id": "e8fb7d0e-86e1-4332-bb4a-d8109e4146e4",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1910.04269",
          "start_idx": 19553,
          "end_idx": 20023
        },
        "page_content": ", the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Kat",
        "type": "Document"
      },
      {
        "id": "10a35031-f90d-415f-aa3f-734db4bbcba6",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1612.05270",
          "start_idx": 1650,
          "end_idx": 2169
        },
        "page_content": " tested in eight different languages: Spanish, English, Italian, Arabic, German, Portuguese, Russian and Swedish. We compare our approach ranking in three international contests: TASS'15, SemEval'15-16 and SENTIPOLC'14, for Spanish, English and Italian respectively; the remaining languages are compared directly with the results reported in the literature. The experimental results locate our approach in good positions for all considered competitions; and excellent results in the other five languages tested. Finally",
        "type": "Document"
      },
      {
        "id": "78dd82c6-c6af-4015-b4e0-f73037eb863b",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1904.05862",
          "start_idx": 15343,
          "end_idx": 15454
        },
        "page_content": "letter++ experiments, and Tatiana Likhomanenko for providing convolutional language models for our experiments.",
        "type": "Document"
      },
      {
        "id": "10b18f90-1375-4a41-9719-9a76bdd2287c",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1910.05456",
          "start_idx": 1974,
          "end_idx": 2423
        },
        "page_content": " on the L1, i.e., the \"native language\", in neural network models. To this goal, we select a diverse set of eight source languages from different language families \u2013 Basque, French, German, Hungarian, Italian, Navajo, Turkish, and Quechua \u2013 and three target languages \u2013 English, Spanish and Zulu. We pretrain a neural sequence-to-sequence architecture on each of the source languages and then fine-tune the resulting models on small datasets in each",
        "type": "Document"
      },
      {
        "id": "6bc2bb18-6a66-4628-bef7-a0e1a75c3767",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1909.13362",
          "start_idx": 15624,
          "end_idx": 16095
        },
        "page_content": " language groupings within families. We selected six evaluation languages: English, Dutch, Italian, French, Basque, and Manipuri. These represent two language families (Indo-European, Sino-Tibetan), a language isolate thought to be unrelated to any existing language (Basque), and two different subfamilies within the Indo-European family (West Germanic, Romance). The primary constraint was the availability of syllabified datasets for training and testing. Table TABREF",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Does DCA or GMM-based attention perform better in experiments?",
    "llm_answer": "DCA performs better than GMMv2b in some experiments.\n",
    "context": [
      {
        "id": "7f5cb0a5-8d07-445a-a73a-e0488eea3bfe",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1910.10288",
          "start_idx": 17211,
          "end_idx": 17754
        },
        "page_content": " slower alignment can sometimes lead to worse quality models, probably because the other model components are being optimized in an unaligned state for longer. Compared to GMMv2b, DCA can more easily bound its receptive field (because its prior filter numerically disallows excessive forward movement), which makes it easier to incorporate hard windowing optimizations in production. Another advantage of DCA over GMM attention is that its attention weights are normalized, which helps to stabilize the alignment, especially for coarse-grained",
        "type": "Document"
      },
      {
        "id": "ed375874-783f-4cd6-bb80-df89b4f206c7",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1803.09230",
          "start_idx": 13315,
          "end_idx": 13814
        },
        "page_content": "izations are also shown in Figure 3 . Visualizations demonstrate that both hybrid and DCA models perform better than vanilla Co-Attention and BiDAF attention mechanisms and reduce the losses faster and increase the dev F1/EM scores faster as well. We made a brief attempt to do a bit of hyperparameter tuning on our proposed DCA model and we report the results in Table 3 . Ideally, hyperparameter tuning for neural network architectures should be done using bayesian hyperparameter optimization but",
        "type": "Document"
      },
      {
        "id": "91968d08-2519-4245-81de-8dae423cc92c",
        "metadata": {
          "chunk_id": 41,
          "document_id": "1803.09230",
          "start_idx": 14099,
          "end_idx": 14601
        },
        "page_content": " probably because it reduces the system complexity which makes the model easier to train. In Table 4 , we briefly provide error analysis on a small sample of results for hybrid and DCA models and try to explain the model behavior. In this paper, we studied and implemented two well known attention mechanisms namely, BiDAF and Co-Attention. We also introduced a simple combination of these two schemes called Hybrid attention mechanism that outperforms both BiDAF and Co-Attention. In addition to this,",
        "type": "Document"
      },
      {
        "id": "1d1210ca-400d-44cf-be2b-4c2dffb31ce8",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1910.10288",
          "start_idx": 7614,
          "end_idx": 8092
        },
        "page_content": " normalization isn't a significant problem in fine-grained output-to-text alignment, but becomes more of an issue for coarser-grained alignment tasks where the attention window needs to gradually move to the next index (for example in variable-length prosody transfer applications BIBREF12). Because DCA is in the energy-based attention family, it is normalized by default and should work well for a variety of monotonic alignment tasks. Another issue with GMM attention is that",
        "type": "Document"
      },
      {
        "id": "b7c93a95-4f94-4bbe-af94-95f046197b2b",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1910.10288",
          "start_idx": 15889,
          "end_idx": 16316
        },
        "page_content": " the test length exceeds the max training length. LSA shoots up soon after at around 3x the max training length, while the two location-relative mechanisms, DCA and GMMv2b, are both able to generalize to the whole range of utterance lengths tested. We have shown that Dynamic Convolution Attention (DCA) and our V2 GMM attention with initial bias (GMMv2b) are able to generalize to utterances much longer than those seen during",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What evaluation metric is used?",
    "llm_answer": "Manual evaluation, automatic evaluation metrics (ROUGE, BLEU, METEOR), and task-based evaluation.\n",
    "context": [
      {
        "id": "7ee3d1d0-5ed5-48f9-a7bc-3e689b448d34",
        "metadata": {
          "chunk_id": 275,
          "document_id": "1804.08186",
          "start_idx": 98870,
          "end_idx": 99381
        },
        "page_content": " evaluation in research to date. In this section, we examine the literature more closely, providing a broad overview of the evaluation metrics that have been used, as well as the experimental settings in which research has been evaluated. The most common approach is to treat the task as a document-level classification problem. Given a set of evaluation documents, each having a known correct label from a closed set of labels (often referred to as the \u201cgold-standard\u201d), and a predicted label for each document",
        "type": "Document"
      },
      {
        "id": "6a5777b6-5e74-4edf-9018-a1ed47830e3f",
        "metadata": {
          "chunk_id": 69,
          "document_id": "2002.05058",
          "start_idx": 27517,
          "end_idx": 28041
        },
        "page_content": " evaluation is then employed to identify the best choice. We evaluate the performance of each metric by how many times (out of 10) they succeeded in selecting the best hyperparameter combination or early-stopping checkpoint (out of 4) and the average human-annotated score for their selected models. The results are shown in Table 3. We can see that conventional automated metrics perform poorly and result in sub-optimal result when performing hyperparameter search and selecting the best performing checkpoints. Converting",
        "type": "Document"
      },
      {
        "id": "08e94578-b4df-43f1-b96f-4db730b4d02a",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1607.06025",
          "start_idx": 10088,
          "end_idx": 10607
        },
        "page_content": "REF37 , the evaluation metrics of text-generating models fall into three categories: manual evaluation, automatic evaluation metrics, task-based evaluation. In evaluation based on human judgment each generated textual example is inspected manually. The automatic evaluation metrics, like ROUGE, BLEU and METEOR, compare human texts and generated texts. BIBREF38 shows METEOR has the strongest correlation with human judgments in image description evaluation. The last category is task-based evaluation, where the impact",
        "type": "Document"
      },
      {
        "id": "e12d7235-2654-4688-a6b5-218fe9724fb5",
        "metadata": {
          "chunk_id": 68,
          "document_id": "2002.05058",
          "start_idx": 26990,
          "end_idx": 27517
        },
        "page_content": " NLG evaluation. We further investigate the impact of imperfect metrics on training NLG models. As described in the human evaluation procedure, we perform 10 runs to test the reliability of each metric when used to perform hyperparameter tuning and early-stopping respectively. In each run, we select the best hyperparameter combination or early-stopping checkpoint based on each of the five compared metrics. Human evaluation is then employed to identify the best choice. We evaluate the performance of each metric by how many",
        "type": "Document"
      },
      {
        "id": "a669a759-4988-4843-98f8-c12f54d6dcaa",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1906.03538",
          "start_idx": 17710,
          "end_idx": 18208
        },
        "page_content": "6 and INLINEFORM7 , respectively. The metrics are averaged across all the perspectives INLINEFORM8 participating in the test set. The goal is to get estimates of the overall performance of the systems. Instead of creating a complex measure that would take all the aspects into account, we approximate the overall performance by multiplying the disjoint measures in INLINEFORM0 , INLINEFORM1 and INLINEFORM2 . While this gives an estimate on the overall quality, it ignores the pipeline structure of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is any data-to-text generation model trained on this new corpus, what are the results?",
    "llm_answer": "No data-to-text generation model is mentioned as being trained on the new corpus.\n",
    "context": [
      {
        "id": "9d659b7a-98f5-4acc-8ef1-1dbdd7448d4b",
        "metadata": {
          "chunk_id": 37,
          "document_id": "2002.10361",
          "start_idx": 15263,
          "end_idx": 15732
        },
        "page_content": " (URL, USER, HASHTAG). Documents are lowercased and tokenized using NLTK BIBREF38. The corpus is randomly split into training (70%), development (15%), and test (15%) sets. We train the models on the training set and find the optimal hyperparameters on the development set before final evaluations on the test set. We randomly shuffle the training data at the beginning of each training epoch. We implement and experiment four baseline classification models. To compare",
        "type": "Document"
      },
      {
        "id": "60f7e7e2-3b0c-44b7-88d0-8ccd367fefb0",
        "metadata": {
          "chunk_id": 62,
          "document_id": "1905.10044",
          "start_idx": 23339,
          "end_idx": 23791
        },
        "page_content": " a language model on the Books corpus BIBREF36 . Our BERTL model fine-tunes the 24 layer 1024 dimensional transformer from BIBREF8 , which has been trained on next-sentence-selection and masked language modelling on the Book Corpus and Wikipedia. We fine-tune the BERTL and the OpenAI GPT models using the optimizers recommended by the authors, but found it important to tune the optimization parameters to achieve the best results. We use a batch size",
        "type": "Document"
      },
      {
        "id": "9c7f218a-80a4-486c-815b-422ad44b40e9",
        "metadata": {
          "chunk_id": 121,
          "document_id": "1701.02877",
          "start_idx": 43692,
          "end_idx": 44145
        },
        "page_content": " the CoNLL 2003 corpus. The seed domain for the experiments here is newswire, where we use the classifier trained on the biggest NW corpus investigated in this study, i.e. OntoNotes NW. That classifier is then applied to all other corpora. The rationale is to test how suitable such a big corpus would be for improving Twitter NER, for which only small training corpora are available. Results for out-of-domain performance are reported in Table 11 . The",
        "type": "Document"
      },
      {
        "id": "cf348585-ca79-493f-be9d-ce678544cd42",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1909.00694",
          "start_idx": 8898,
          "end_idx": 9330
        },
        "page_content": " CO such that it was five times larger than AL. The results are shown in Table TABREF16. We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively: . \u4f5c\u696d\u304c\u697d\u3060\u3002 The work",
        "type": "Document"
      },
      {
        "id": "f678afa6-44c6-4189-813a-3e8d2262f0bd",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1809.10644",
          "start_idx": 8845,
          "end_idx": 9310
        },
        "page_content": " classification and a hybrid of word and character CNNs, using approximately twice the parameters we use excluding the word embeddings) and even recent work by BIBREF8 , (whose best model relies on GRUs, metadata including popularity, network reciprocity, and subscribed lists). On the SR dataset, we outperform BIBREF8 's text based model by 3 F1 points, while just falling short of the Text + Metadata Interleaved Training model. While we appreciate the potential",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are the potentially relevant text fragments identified?",
    "llm_answer": "Entities are extracted, contexts are generated, and a distance between a question and the entity context is computed.  Candidate chunks come from lemmata, surface forms, and base forms.  A PATRICIA trie is used to search for matching strings.\n",
    "context": [
      {
        "id": "31dd0252-7e67-44b8-914c-0cb67096c7f8",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1605.08675",
          "start_idx": 15409,
          "end_idx": 15878
        },
        "page_content": " extracted (ENTITY RECOGNITION). For each of them, a context is generated (CONTEXT GENERATION). Finally, a distance between a question content and the entity context is computed to asses its relevance (DISTANCE MEASURE). All the mentions and their distance scores are stored and, after no more documents are left, used to select the best match (BEST ENTITY SELECTION). The system returns the entity, supplied with information about a supporting sentence and a document,",
        "type": "Document"
      },
      {
        "id": "5a46e2a2-482c-4460-be6d-bacc9d28616e",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1706.04815",
          "start_idx": 2890,
          "end_idx": 3417
        },
        "page_content": " learning framework. We use the bidirectional recurrent neural networks (RNN) for the word-level representation, and then apply the attention mechanism BIBREF2 to incorporate matching information from question to passage at the word level. Next, we predict start and end positions of the evidence snippet by pointer networks BIBREF3 . Moreover, we aggregate the word-level matching information of each passage using the attention pooling, and use the passage-level representation to rank all candidate passages as an additional",
        "type": "Document"
      },
      {
        "id": "4724cd50-1e9e-482b-93b6-53ea4d190e90",
        "metadata": {
          "chunk_id": 45,
          "document_id": "2001.08051",
          "start_idx": 17431,
          "end_idx": 17972
        },
        "page_content": " words fragments and spontaneous speech phenomena requires specific investigations to reduce their impact on the final performance. We believe that the particular domain and set of data pave the way to investigate into various ASR topics, such as: non-native speech, children speech, spontaneous speech, code-switching, multiple pronunciation, etc. The corpus has been (partly) annotated using the guidelines presented in Section SECREF3 on the basis of a preliminary analysis of the most common acoustic phenomena appearing in the data sets",
        "type": "Document"
      },
      {
        "id": "08020147-a0e0-44bf-8358-5f7a01beecb7",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1605.08675",
          "start_idx": 25418,
          "end_idx": 25879
        },
        "page_content": " into a PATRICIA trie, a very efficient prefix tree. In this structure, every entity name becomes a key for storing a corresponding list of entities. When a document is ready for analysis, it is searched for strings that match any of the keys in the trie. The candidate chunks (sequences of segments) come from three sources: lemmata of words and syntactic groups, sequences of words in surface forms (as they appear in text), sequences of words in base forms (",
        "type": "Document"
      },
      {
        "id": "1b103906-78eb-4713-8748-0fc2af9f6d4b",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1907.05664",
          "start_idx": 5236,
          "end_idx": 5692
        },
        "page_content": " words with more common alternatives or repeating non-sense after the third sentence. We can see in Figure 1 an example of summary obtained compared to the target one. The \u201csummaries\" we generate are far from being valid summaries of the information in the texts but are sufficient to look at the attribution that LRP will give us. They pick up the general subject of the original text. We present in this section the Layer-Wise Relevance Propagation (LRP)",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What dataset did they use?",
    "llm_answer": "A dataset of 3 billion (query, image, weight) triples, with 900 million unique images and 220 million unique queries; and a dataset (SCI) of 220 sentences from the scientific literature.\n",
    "context": [
      {
        "id": "91df3232-acdc-442e-89f1-92d8ec6a1b84",
        "metadata": {
          "chunk_id": 75,
          "document_id": "1912.09152",
          "start_idx": 26669,
          "end_idx": 26983
        },
        "page_content": " a true Gold Standard Dataset. We thank three anonymous reviewers of our manuscript for their careful reading and their many insightful comments and suggestions. We have made our best in providing a revised version of the manuscript that reflects their suggestions. Any remaining errors are our own responsability.",
        "type": "Document"
      },
      {
        "id": "6b7f6ca6-24d5-4144-9cfd-92be2de31f39",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "5240d67c-b588-4df6-a1bb-56be351fb42b",
        "metadata": {
          "chunk_id": 21,
          "document_id": "1905.12260",
          "start_idx": 8180,
          "end_idx": 8639
        },
        "page_content": " results. For example, one (query, image) pair might be \u201ccat with big ears\u201d and an image of a cat. Each (query, image) pair also has a weight corresponding to a relevance score of the image for the query. The dataset includes 3 billion (query, image, weight) triples, with 900 million unique images and 220 million unique queries. The data was prepared by first taking the query-image set, filtering to remove any personally identifiable information and adult",
        "type": "Document"
      },
      {
        "id": "5aae2354-e125-4179-9260-e8eac191d87c",
        "metadata": {
          "chunk_id": 99,
          "document_id": "1701.00185",
          "start_idx": 35418,
          "end_idx": 35678
        },
        "page_content": "ASQ for making the datasets available. This work is supported by the National Natural Science Foundation of China (No. 61602479, No. 61303172, No. 61403385) and the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDB02070005).",
        "type": "Document"
      },
      {
        "id": "3b942450-5125-4b16-8163-c4e7540353a0",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1802.05322",
          "start_idx": 13905,
          "end_idx": 14341
        },
        "page_content": " explanation of the whole process needed to conduct an experiment like this. The process includes downloading a data set, web scraping for extra information, data preprocessing, model tuning and evaluation of the results. Action Adult Adventure Animation Biography Comedy Crime Documentary Drama Family Fantasy Film-Noir Game-Show History Horror Music Musical Mystery Reality-TV Romance Sci-Fi Short Sport Talk-Show Thriller War Western",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the size of the dataset?",
    "llm_answer": "The answer is not found in the context.  While sizes of various *parts* of different datasets are given, there is no single \"size of the dataset\" figure.\n",
    "context": [
      {
        "id": "2d3bdfec-4ed5-456d-bd82-023fad798858",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "9ccb7945-9a96-417a-b52e-68855c4910b1",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "6b7f6ca6-24d5-4144-9cfd-92be2de31f39",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "95bf2b84-1a56-4eed-b193-4a34d2c76c4c",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1706.08032",
          "start_idx": 14264,
          "end_idx": 14661
        },
        "page_content": " datasets in Table III. for all datasets, the filter window size ( INLINEFORM0 ) is 7 with 6 feature maps each for the first wide convolution layer, the second wide convolution layer has a filter window size of 5 with 14 feature maps each. Dropout rate ( INLINEFORM1 ) is 0.5, INLINEFORM2 constraint, learning rate is 0.1 and momentum of 0.9. Mini-batch size for STS Corpus is 100 and others are 4",
        "type": "Document"
      },
      {
        "id": "6f04ea61-adc5-4abb-9261-abc554ebac1b",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1711.02013",
          "start_idx": 20658,
          "end_idx": 21116
        },
        "page_content": "ings, between recurrent layers, and on recurrent states were (0.7, 0.5, 0.5) respectively. dataset contains 17M training tokens and has a vocabulary size of 44k words. The dataset is partitioned into a training set (first 99M characters) and a development set (last 1M characters) that is used to report performance. As this dataset contains various articles from Wikipedia, the longer term information (such as current topic) plays a bigger role than in the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the 12 AV approaches which are examined?",
    "llm_answer": "The provided text lists 12 existing AV approaches, but does not name them.\n",
    "context": [
      {
        "id": "da89a1ff-630a-4fab-bc29-f802ee1f4c08",
        "metadata": {
          "chunk_id": 77,
          "document_id": "1906.10551",
          "start_idx": 29331,
          "end_idx": 29815
        },
        "page_content": " 12 existing AV approaches, which have shown their potentials in the previous PAN-AV competitions BIBREF11 , BIBREF12 as well as in a number of AV studies. The methods are listed in Table TABREF33 together with their classifications regarding the AV characteristics, which we proposed in Section SECREF3 . All (optimizable) AV methods were tuned regarding their hyperparameters, according to the original procedure mentioned in the respective paper. However, in the case of the binary",
        "type": "Document"
      },
      {
        "id": "a6ca2b58-48e5-4fe9-8824-04823ca78dc3",
        "metadata": {
          "chunk_id": 116,
          "document_id": "1906.10551",
          "start_idx": 43724,
          "end_idx": 44245
        },
        "page_content": " rated as reliable. Moreover, we clarified a number of misunderstandings in previous research works and proposed three clear criteria that allow to classify the model category of an AV method, which in turn influences its design and the way how it should be evaluated. In regard to binary-extrinsic AV approaches, we explained which challenges exist and how they affect their applicability. In an experimental setup, we applied 12 existing AV methods on three self-compiled corpora, where the intention behind each corpus",
        "type": "Document"
      },
      {
        "id": "4b370dd2-9124-488d-9427-1f79430612f4",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1906.10551",
          "start_idx": 4902,
          "end_idx": 5362
        },
        "page_content": " approaches which, according to Gr\u00f6ndahl and Asokan BIBREF9 can be divided into manual, computer-assisted and automatic types, perform a variety of modifications on the texts. These include simple synonym replacements, rule-based substitutions or word order permutations. In 2016, Potthast et al. BIBREF10 presented the first large-scale evaluation of three AO approaches that aim to attack 44 AV methods, which were submitted to the PAN-AV competitions during",
        "type": "Document"
      },
      {
        "id": "d3c674d0-6609-4993-8841-31cdccf45c3a",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1906.10551",
          "start_idx": 2469,
          "end_idx": 3019
        },
        "page_content": " contribute to the further development of this young research field. Based on the proposed properties, we investigate the applicability of 12 existing AV approaches on three self-compiled corpora, where each corpus involves a specific challenge. The rest of this paper is structured as follows. Section SECREF2 discusses the related work that served as an inspiration for our analysis. Section SECREF3 comprises the proposed criteria and properties to characterize AV methods. Section SECREF4 describes the methodology, consisting of the used corpora",
        "type": "Document"
      },
      {
        "id": "ddb1f13e-e9c3-44d1-8db6-07aef9fa020f",
        "metadata": {
          "chunk_id": 117,
          "document_id": "1906.10551",
          "start_idx": 44245,
          "end_idx": 44729
        },
        "page_content": " applied 12 existing AV methods on three self-compiled corpora, where the intention behind each corpus was to focus on a different aspect of the methods applicability. Our findings regarding the examined approaches can be summarized as follows: Despite of the good performance of the five AV methods GenIM, ImpGI, Unmasking, Caravel and SPATIUM, none of them can be truly considered as reliable and therefore applicable in real forensic cases. The reason for this is not only the non-",
        "type": "Document"
      }
    ]
  },
  {
    "query": "how was annotation done?",
    "llm_answer": "Annotations were done by undergraduate students of computational linguistics, in single annotation mode.  A small sample was annotated by all four annotators to assess inter-annotator agreement.  Annotators discussed emerging issues with the authors throughout the process.\n",
    "context": [
      {
        "id": "7ad84a74-dfae-4c36-99a3-0c3aa9c1ab59",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1703.05260",
          "start_idx": 17551,
          "end_idx": 18087
        },
        "page_content": " results were discussed with the authors. Throughout the whole annotation phase, annotators could discuss any emerging issues with the authors. All annotations were done by undergraduate students of computational linguistics. The annotation was rather time-consuming due to the complexity of the task, and thus we decided for single annotation mode. To assess annotation quality, a small sample of texts was annotated by all four annotators and their inter-annotator agreement was measured (see Section \"Inter-Annotator Agreement\" ). It",
        "type": "Document"
      },
      {
        "id": "df260bde-2da4-422a-8d79-9d3441514e3a",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1710.00341",
          "start_idx": 15082,
          "end_idx": 15541
        },
        "page_content": ".g., \u201cIt is Friday tomorrow.\u201d, \u201cIt was raining last week.\u201d), or for which the annotators were unsure. We targeted very high quality, and thus we did not use crowdsourcing for the annotation, as pilot annotations showed that the task was very difficult and that it was not possible to guarantee that Turkers would do all the necessary verification, e.g., gathering evidence from trusted sources. Instead, all examples were first annotated independently by four",
        "type": "Document"
      },
      {
        "id": "953d651f-8d20-4b6f-b126-f5473fb50683",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1906.03538",
          "start_idx": 6916,
          "end_idx": 7478
        },
        "page_content": " more than 1000 finished HITs and at least a 95% acceptance rate. To ensure the diversity of responses, we do not require additional qualifications or demographic information from our annotators. For any of the annotations steps described below, the users are guided to an external platform where they first read the instructions and try a verification step to make sure they have understood the instructions. Only after successful completion are they allowed to start the annotation tasks. Throughout our annotations, it is our aim to make sure that the workers",
        "type": "Document"
      },
      {
        "id": "346e7867-4bb7-412c-b15e-a3148bed2618",
        "metadata": {
          "chunk_id": 33,
          "document_id": "1912.09152",
          "start_idx": 12001,
          "end_idx": 12474
        },
        "page_content": ", the annotation process is very simple. The program reads the input byte stream trying to identify known entities by means of a huge regexp built through the pre-processing of the available resources. If the candidate entity is ambiguous and (at least) one contextual rule exists for it, it is applied. For the rest of the nes, the system assigns them the class and sctid found in our ranked in-memory lexicon. As already mentioned in passing, the system does not tokenize",
        "type": "Document"
      },
      {
        "id": "03dfc8c2-ef9d-47f3-bbef-d9ea1b6be331",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1905.10044",
          "start_idx": 9361,
          "end_idx": 9867
        },
        "page_content": " is made before the annotator sees the Wikipedia page. Next, for good questions, annotators find a passage within the document that contains enough information to answer the question. Annotators can mark questions as \u201cnot answerable\" if the Wikipedia article does not contain the requested information. Finally, annotators mark whether the question's answer is \u201cyes\" or \u201cno\". Annotating data in this manner is quite expensive since annotators need to search entire Wikipedia documents for relevant evidence",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they measure correlation between the prediction and explanation quality?",
    "llm_answer": "The provided text describes experiments to investigate how prediction accuracy is affected by jointly training the network to produce explanations and how well the model learns to generate explanations, but does not detail a method for measuring correlation between prediction and explanation quality.\n",
    "context": [
      {
        "id": "29ccf86b-2793-44eb-a59e-fefa62a4cd25",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1708.01776",
          "start_idx": 10757,
          "end_idx": 11314
        },
        "page_content": " the average of the output vector's values), but found that a fixed threshold of .5 works best. To evaluate the model's ability to jointly learn to predict and explain its predictions we performed two experiments. First, we investigate how the prediction accuracy is affected by jointly training the network to produce explanations. Second, we evaluate how well the model learns to generate explanations. To understand the role of the explanation content in the learning process we perform both of these experiments for each of the two types of explanation:",
        "type": "Document"
      },
      {
        "id": "6b2cf4a6-408b-4f7c-83a0-61522744abee",
        "metadata": {
          "chunk_id": 30,
          "document_id": "1708.01776",
          "start_idx": 12047,
          "end_idx": 12622
        },
        "page_content": "REF53 . Here again the performance on possible-answer explanations is significantly higher than for relevant-variable explanations. Possible-answer explanations reach an F-Score of .9, while relevant-variable explanations one of .09 only, with precision and recall only slightly deviating from the F-Score in all experiments. We would expect that explanation performance should correlate with prediction performance. Since Possible-answer knowledge is primarily needed to decide if the net has enough information to answer the challenge question without guessing and relevant",
        "type": "Document"
      },
      {
        "id": "a2fbb1c1-188d-4ff2-aad3-32f529099387",
        "metadata": {
          "chunk_id": 55,
          "document_id": "2004.03744",
          "start_idx": 20038,
          "end_idx": 20622
        },
        "page_content": " of explanations and suggested human evaluation instead. We therefore manually scored the relevance of 100 explanations that were generated when the model predicted correct labels. We found that only 20% of explanations were relevant. We highlight that the relevance of explanations is in terms of whether the explanation reflects ground-truth reasons supporting the correct label. This is not to be confused with whether an explanation is correctly illustrating the inner working of the model, which is left as future work. It is also important to note that on a similar experimental",
        "type": "Document"
      },
      {
        "id": "056fcbd8-555a-4c40-9502-a9a7e36fb07e",
        "metadata": {
          "chunk_id": 33,
          "document_id": "1909.00578",
          "start_idx": 11017,
          "end_idx": 11604
        },
        "page_content": " stochastic learning algorithms are involved. An in-depth exploration of this point would involve further investigation, which will be part of future work. We propose a novel Quality Estimation model for summarization which does not require human references to estimate the quality of automatically produced summaries. Sum-QE successfully predicts qualitative aspects of summaries that recall-oriented evaluation metrics fail to approximate. Leveraging powerful BERT representations, it achieves high correlations with human scores for most linguistic qualities rated, on three different",
        "type": "Document"
      },
      {
        "id": "3aea14a4-657f-4bf2-8836-c47475ae5824",
        "metadata": {
          "chunk_id": 16,
          "document_id": "2001.02380",
          "start_idx": 6369,
          "end_idx": 6911
        },
        "page_content": ", we combine the anchoring annotation data from Section SECREF3 with the model's predictions to evaluate how `human-like' its performance is, using an information retrieval approach measuring recall@k and assessing the stability of different signal types based on how the model scores them. We develop a visualization for tokenwise signaling strength and perform error analysis for some signals found by the model which were not flagged by humans and vice versa, and point out the strengths and weaknesses of the architecture. Section SECREF6",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What datasets are used to evaluate the introduced method?",
    "llm_answer": "Swissmetro, Oshiete-goo, and nfL6 datasets.\n",
    "context": [
      {
        "id": "1f140aea-67fe-42b4-983e-2e4e083c362a",
        "metadata": {
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "e23dc951-77e9-483d-84ce-d8a31bf62b89",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1910.12618",
          "start_idx": 19565,
          "end_idx": 20023
        },
        "page_content": " the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014 and 2015 serve as validation set (730 days). It is used to tune the hyperparameters of the different approaches. All the data from January the 1st 2016 (974 days for France and 1,096 for the UK) is used as test set, on which the final results are presented. Grid search is applied to find the best combination of values: for each hyperparameter,",
        "type": "Document"
      },
      {
        "id": "e3748966-4a55-4502-9800-788e84cbe8cb",
        "metadata": {
          "chunk_id": 134,
          "document_id": "1605.08675",
          "start_idx": 49919,
          "end_idx": 50491
        },
        "page_content": ", used at this stage, substantially differs from the one used during the development (see section SECREF80 ). A configuration for the final evaluation has been prepared based on results of the experiments. All of the tested versions share the following features: no question analysis corrections, question classification and query generation solutions which proved best in the previous experiments (see section SECREF18 ), a retrieved set of documents including 20 articles, no minimal confidence, singe sentence context with title. Tested solutions differ with respect to",
        "type": "Document"
      },
      {
        "id": "9638118b-fae1-4140-a7d1-11bd33d21e74",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1909.00154",
          "start_idx": 23166,
          "end_idx": 23592
        },
        "page_content": "CCM), for example. We will apply the methodology to the well-known \u201cSwissmetro\" dataset. We will compare it with a dummy variables and PCA baselines. We will follow the 3-way experimental design mentioned before: split the dataset into train, development and test sets, so that the embeddings, PCA eigenvectors and the choice model are estimated from the same train and development sets, and validate it out-of-sample. For the",
        "type": "Document"
      },
      {
        "id": "6df49c9f-d8f5-4d56-a86b-73d9ec27c8c3",
        "metadata": {
          "chunk_id": 70,
          "document_id": "1912.00864",
          "start_idx": 22055,
          "end_idx": 22516
        },
        "page_content": " three evaluations using the Oshiete-goo dataset; we selected three different sets of 500 human-annotated test pairs from the full dataset. In each set, we trained the model by using training pairs and input questions in test pairs to the model. We repeated the experiments three times by randomly shuffling the train/test sets. For the evaluations using the nfL6 dataset, we prepared three different sets of 500 human-annotated test q-c-s triples from the full",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the results achieved from the introduced method?",
    "llm_answer": "The method achieves significant improvement in ROUGE-L compared to the baseline.  It also outperforms the baseline on the development set in terms of ROUGE-L.  All three methods (GLAD, Caravel, and COAV) perform similarly in terms of AUC, but show different results regarding c@1.\n",
    "context": [
      {
        "id": "1f140aea-67fe-42b4-983e-2e4e083c362a",
        "metadata": {
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "70e1ea66-1171-4546-a976-9b33995d48d1",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1908.07816",
          "start_idx": 4017,
          "end_idx": 4509
        },
        "page_content": " relate. It is the first time such an approach is designed with consideration for the human judges. Our main goal is to increase the objectivity of the results and reduce judges' mistakes due to out-of-context dialogs they have to evaluate. The rest of the paper unfolds as follows. Section SECREF2 discusses some related work. In Section SECREF3, we give detailed description of the methodology. We present experimental results and some analysis in Section SECREF4. The paper is concluded in",
        "type": "Document"
      },
      {
        "id": "57783df1-30f6-441d-a14b-3cda08a935e0",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1905.07464",
          "start_idx": 18517,
          "end_idx": 19008
        },
        "page_content": "REF15 optimization method. These hyper-parameters were tuned during initial experiments. In this section, we present and discuss the results of our cross-validation experiments. We then describe the \u201cruns\u201d that were submitted as challenge entries and present our official challenge results. We discuss these results in Section SECREF28 . We present the results of our initial experiments in Table TABREF20 . Evaluations were produced as as result of 11-fold cross-validation over Training-22",
        "type": "Document"
      },
      {
        "id": "abd17386-ea53-4c93-8a50-50ec32967825",
        "metadata": {
          "chunk_id": 78,
          "document_id": "1706.04815",
          "start_idx": 21188,
          "end_idx": 21662
        },
        "page_content": " observe that our method only achieves significant improvement in terms of ROUGE-L compared with our baseline. The reason is that our synthesis model works better when the answer is short, which almost has no effect on BLEU as it is normalized with all questions. Since answers on the test set are not published, we analyze our model on the development set. Table 3 shows results on the development set in terms of ROUGE-L. As we can see, our method outperforms the baseline",
        "type": "Document"
      },
      {
        "id": "3edb024f-13f7-4dc7-9020-9641baa9c5ef",
        "metadata": {
          "chunk_id": 111,
          "document_id": "1906.10551",
          "start_idx": 41826,
          "end_idx": 42275
        },
        "page_content": "1 of the methods, we decided to take a closer look on the ROC curves (cf. Figure FIGREF49 ) generated by GLAD, Caravel and COAV for the four corpora versions, where a number of interesting observations can be made. When focusing on AUC, it turns out that all three methods perform very similar to each other, whereas big discrepancies between GLAD and COAV can be observed regarding c@1. When we consider the current and maximum achievable results (",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they incorporate human advice?",
    "llm_answer": "They explicitly represent advice in calculating gradients, allowing the system to trade off between data and advice throughout the learning phase.  Advice is converted to first-order logic and given as input to the algorithm.\n",
    "context": [
      {
        "id": "f51b5e4d-3244-48c6-bf1f-9e5782bb772d",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1607.00424",
          "start_idx": 14590,
          "end_idx": 15135
        },
        "page_content": " in the negative, although future work could lead to improvements. Table TABREF26 shows the results of experiments that test the use of advice within the joint learning setting. The use of advice improves or matches the performance of using only joint learning. The key impact of advice can be mostly seen in the improvement of recall in several relations. This clearly shows that using human advice patterns allows us to extract more relations effectively making up for noisy or less number of training examples. This is in-line with previously",
        "type": "Document"
      },
      {
        "id": "1ecf4181-a407-4ab7-bfbd-0e2270658c88",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1607.00424",
          "start_idx": 11408,
          "end_idx": 11891
        },
        "page_content": " calculating gradients. This allows the system to trade-off between data and advice throughout the learning phase, rather than only consider advice in initial iterations. Advice, in particular, become influential in the presence of noisy or less amout of data. A few sample advice rules in English (these are converted to first-order logic format and given as input to our algorithm) are presented in Table TABREF11 . Note that some of the rules are \u201csoft\" rules in that they are not",
        "type": "Document"
      },
      {
        "id": "96570551-2f8f-4c15-a861-7d3ba298a4f4",
        "metadata": {
          "chunk_id": 101,
          "document_id": "1709.05413",
          "start_idx": 40354,
          "end_idx": 40892
        },
        "page_content": ", with ratios of at least 4:1. By using the feature weights we derive from using our predicted dialogue acts in our outcome classification experiments, we can thus derive data-driven patterns that offer useful insight into good/bad practices. Our goal is to then use these rules as guidelines, serving as a basis for automated response planning in the customer service domain. For example, these rules can be used to recommend certain dialogue act responses given the position in a conversation, and based previous turns. This information",
        "type": "Document"
      },
      {
        "id": "92141f70-72ef-48bc-b4a2-9921dc848ede",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1607.00424",
          "start_idx": 10947,
          "end_idx": 11408
        },
        "page_content": " and use them to deduce the nature of the relation between two entities present in the text. The goal of our work is to capture such mental patterns of the humans as advice to the learning algorithm. We modified the work of Odom et al. odomAIME15,odomAAAI15 to learn RDNs in the presence of advice. The key idea is to explicitly represent advice in calculating gradients. This allows the system to trade-off between data and advice throughout the learning phase",
        "type": "Document"
      },
      {
        "id": "e637b22b-75da-4993-99bf-72e5925459b6",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1910.08210",
          "start_idx": 21291,
          "end_idx": 21907
        },
        "page_content": " performs well on complex tasks that require several reasoning and coreference steps with natural language templated goals and descriptions of the dynamics. Our work suggests that language understanding via reading is a promising way to learn policies that generalise to new environments. Despite curriculum learning, our best models trail performance of human players, suggesting that there is ample room for improvement in grounded policy learning on complex problems. In addition to jointly learning policies based on external documentation and language goals, we are interested in exploring how to use supporting",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What affective-based features are used?",
    "llm_answer": "DAL Activation, ANEW Dominance, Emolex Negative, Emolex Fear, LIWC Assent, LIWC Cause, LIWC Certain, LIWC.  Also, features based on positive/negative sentiment polarity lexicons (number of positive terms, number of negative terms, etc.).\n",
    "context": [
      {
        "id": "1f73e085-d38e-47f1-9c74-c0dcefca36eb",
        "metadata": {
          "chunk_id": 33,
          "document_id": "1901.01911",
          "start_idx": 12612,
          "end_idx": 13054
        },
        "page_content": " of features. It is worth to be noted that in our best configuration system, not all of affective and dialogue-act features were used in our feature vector. After several optimization steps, we found that some features were not improving the system's performance. Our final list of affective and dialogue-act based features includes: DAL Activation, ANEW Dominance, Emolex Negative, Emolex Fear, LIWC Assent, LIWC Cause, LIWC Certain and LIWC",
        "type": "Document"
      },
      {
        "id": "9c1a9ce8-6ccb-48cb-a153-070b30399240",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1901.01911",
          "start_idx": 2907,
          "end_idx": 3386
        },
        "page_content": " a relationship exists between the use of an affective lexicon and the communicative intention of an utterance which includes AGREE-ACCEPT (support), REJECT (deny), INFO-REQUEST (question), and OPINION (comment). They exploited several LIWC categories to analyse the role of affective content. Our results show that our model outperforms the state of the art on the Semeval-2017 benchmark dataset. Feature analysis highlights the contribution of the different feature groups, and",
        "type": "Document"
      },
      {
        "id": "762e1087-120b-4250-baa7-86a67f3de6de",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1901.01911",
          "start_idx": 6007,
          "end_idx": 6594
        },
        "page_content": " Twitter language. In addition, we propose to utilize conversational-based features by exploiting the peculiar tree structure of the dataset. We also explored the use of affective based feature by extracting information from several affective resources including dialogue-act inspired features. They were designed taking into account several Twitter data characteristics, and then selecting the most relevant features to improve the classification performance. The set of structural features that we used is listed below. Retweet Count: The number of retweet of each tweet. Question Mark",
        "type": "Document"
      },
      {
        "id": "f00fa0ac-ed70-4469-b6cb-4724433f5085",
        "metadata": {
          "chunk_id": 51,
          "document_id": "1710.01492",
          "start_idx": 18698,
          "end_idx": 19207
        },
        "page_content": " the most important features are those based on the presence of words and phrases in sentiment polarity lexicons with positive/negative scores; examples of such features include number of positive terms, number of negative terms, ratio of the number of positive terms to the number of positive+negative terms, ratio of the number of negative terms to the number of positive+negative terms, sum of all positive scores, sum of all negative scores, sum of all scores, etc. Supervised learning. Traditionally, the",
        "type": "Document"
      },
      {
        "id": "aaefc345-d3fe-4d3f-8a3e-62ce012adb08",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1901.01911",
          "start_idx": 8949,
          "end_idx": 9418
        },
        "page_content": " BIBREF6 . Basically, these features are part of the affective feature group, but we present them separately because we are interested in exploring the contribution of such feature set separately. This feature set was obtained by selecting 4 communicative goals related to our classes in the stance task: agree-accept (support), reject (deny), info-request (question), and opinion (comment). The 11 LIWC categories include: Agree-accept: Assent, Certain, Affect; Reject",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is performance improvement proposed methods are used?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "1f140aea-67fe-42b4-983e-2e4e083c362a",
        "metadata": {
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "5f60f35c-49af-411d-9dd5-a573069248ea",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1707.03764",
          "start_idx": 6168,
          "end_idx": 6712
        },
        "page_content": " out to be detrimental to performance, when compared to the simpler system that we have described in Section SECREF2 . We divide our attempts according to the different ways we attempted to enhance performance: manipulating the data itself (adding more, and changing preprocessing), using a large variety of features, and changing strategies in modelling the problem by using different algorithms and paradigms. All reported results are on the PAN 2017 training data using five-fold cross-validation, unless otherwise specified. We extended the",
        "type": "Document"
      },
      {
        "id": "15069f29-5992-4f8d-8d8e-e8965bd22fab",
        "metadata": {
          "chunk_id": 74,
          "document_id": "1912.09152",
          "start_idx": 26186,
          "end_idx": 26669
        },
        "page_content": " system performance. With these methods, system development can go hand in hand with dataset refinement in a virtuous circle that let us think that maybe next time we are planning to add a new gazetteer or word embedding to our system in order to try to improve system performance, we should first look at our data and, like King Midas, turn our Human Annotated Dataset into a true Gold Standard Dataset. We thank three anonymous reviewers of our manuscript for their careful reading",
        "type": "Document"
      },
      {
        "id": "97f5a7ab-6607-4500-903d-1a898035b8b3",
        "metadata": {
          "chunk_id": 73,
          "document_id": "1912.09152",
          "start_idx": 25875,
          "end_idx": 26391
        },
        "page_content": "design system can outperform a ner tagger for biomedical entities based on state-of-the-art artificial neural network technology. Thus, knowledge-based methods stand on their own merits in task resolution. But, perhaps most importantly, the other key point brought to light in this contribution is that a resource-based approach also favours a more critical stance on the dataset(s) used to evaluate system performance. With these methods, system development can go hand in hand with dataset refinement in a virtuous",
        "type": "Document"
      },
      {
        "id": "7174f731-a0c5-4560-b206-b8912989cf98",
        "metadata": {
          "chunk_id": 10,
          "document_id": "2002.05829",
          "start_idx": 4037,
          "end_idx": 4581
        },
        "page_content": " end-to-end energy efficiency in pretraining, fine-tuning and inference. With the Hulk benchmark, we quantify the energy efficiency of model pretraining, fine-tuning and inference phase by comparing the time and cost they require to reach certain overall task-specific performance level on selected datasets. The design principle and benchmarking process are detailed in section SECREF2. We also explore the relation between model parameter and fine-tuning efficiency and demonstrate consistency of energy efficiency between tasks for different",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does transfer learning improve performance on this task?",
    "llm_answer": "Transfer learning achieves an average F\u2081 = 75%, better than the in-domain Experiment 1 results.  The best performance is F\u2081 = 76% in transfer learning models.\n",
    "context": [
      {
        "id": "eed78302-566d-4b22-9309-f9df33986926",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1909.02764",
          "start_idx": 21414,
          "end_idx": 21819
        },
        "page_content": " column \u201cTransfer L.\u201d in Table TABREF19. The confusion matrix is also depicted in Table TABREF16. With this procedure we achieve an average performance of F$_1$=75 %, being better than the results from the in-domain Experiment 1. The best performance of F$_1$=76 % is achieved with the model pre-trained on each data set, except for ISEAR. All transfer learning models clearly outperform their simple out-",
        "type": "Document"
      },
      {
        "id": "a3ac2dfd-4141-4495-9a1f-33d286abedad",
        "metadata": {
          "chunk_id": 56,
          "document_id": "1909.02764",
          "start_idx": 21819,
          "end_idx": 22276
        },
        "page_content": " each data set, except for ISEAR. All transfer learning models clearly outperform their simple out-of-domain counterpart. To ensure that this performance increase is not only due to the larger data set, we compare these results to training the model without transfer on a corpus consisting of each corpus together with AMMER (again, in leave-one-out crossvalidation). These results are depicted in column \u201cJoint C.\u201d. Thus, both settings, \u201ctransfer learning\ufffd",
        "type": "Document"
      },
      {
        "id": "9b2cc4bc-6dd9-49c7-9aec-cb34279e8b76",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1811.02906",
          "start_idx": 19010,
          "end_idx": 19381
        },
        "page_content": " performed best suggests that this approach will also contribute beneficially to other text classification tasks such as sentiment analysis. Thus, in future work, we plan to evaluate our approach with regard to such other tasks. We also plan to evaluate more task-agnostic approaches for transfer learning, for instance employing language modeling as a pre-training task.",
        "type": "Document"
      },
      {
        "id": "1c2578f1-bceb-404b-9def-b62f95759b46",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1910.09295",
          "start_idx": 7227,
          "end_idx": 7713
        },
        "page_content": "ifier, instead of setting it as a separate step. This produces two losses to be optimized together during training, and ensures that no task (stylometric adaptation or classification) will be prioritized over the other. This concept has been proposed and explored to improve the performance of transfer learning in multiple language tasks BIBREF9, BIBREF10. We show that this method improves performance on both BERT and GPT-2, given that it learns to adapt to the idiosyncracies of its",
        "type": "Document"
      },
      {
        "id": "b4b2f0aa-fdce-449d-9290-d8c4969f36ab",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1809.04686",
          "start_idx": 0,
          "end_idx": 531
        },
        "page_content": "Transfer learning has been shown to work well in Computer Vision where pre-trained components from a model trained on ImageNet BIBREF0 are used to initialize models for other tasks BIBREF1 . In most cases, the other tasks are related to and share architectural components with the ImageNet task, enabling the use of such pre-trained models for feature extraction. With this transfer capability, improvements have been obtained on other image classification datasets, and on other tasks such as object detection, action recognition,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What baseline is used?",
    "llm_answer": "Baseline 1, Baseline 2, Baseline 3, ANRES_Sing_R, ANRES_Sing_Z, ANRES_Seq_R, ANRES_Seq_Z, LSTM.\n",
    "context": [
      {
        "id": "94c43990-47e3-478c-a415-088eeac582b5",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1804.08782",
          "start_idx": 13990,
          "end_idx": 14427
        },
        "page_content": " number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 .  For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose",
        "type": "Document"
      },
      {
        "id": "f297d78b-0fdd-4eee-ae9e-9e75a2e66130",
        "metadata": {
          "chunk_id": 140,
          "document_id": "2001.02380",
          "start_idx": 51470,
          "end_idx": 51956
        },
        "page_content": " baselines. The left, middle and right panels in Figure FIGREF40 correspond to measurements when all signals are included, only cases contained entirely in the head EDUs shown to the model, and only DMs, respectively. The scenario on the left is rather unreasonable and is included only for completeness: here the model is also penalized for not detecting signals such as lexical chains, part of which is outside the units that the model is being shown. An example of such a case can be",
        "type": "Document"
      },
      {
        "id": "3747343d-0517-442a-b7ba-66d01be55b6d",
        "metadata": {
          "chunk_id": 40,
          "document_id": "2004.01878",
          "start_idx": 14526,
          "end_idx": 14915
        },
        "page_content": " normal distribution $\\mathcal {N}(0, 1)$. In summary, the following four baselines are designed: ANRES_Sing_R: randomly initializing the states for each single trading day. ANRES_Sing_Z: initializing the states as zeros for each single trading day. ANRES_Seq_R: randomly initializing the first states for each trading sequence only. ANRES_Seq_Z: initializing the first states as zeros for",
        "type": "Document"
      },
      {
        "id": "0d211bed-e5df-4233-8d02-e24d7ee9b65c",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1909.02635",
          "start_idx": 9050,
          "end_idx": 9548
        },
        "page_content": " (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo",
        "type": "Document"
      },
      {
        "id": "4d522ea5-86b3-42df-acaa-37f028f2bec5",
        "metadata": {
          "chunk_id": 138,
          "document_id": "2001.11381",
          "start_idx": 31860,
          "end_idx": 32196
        },
        "page_content": "ados. Dicho lo anterior, el Modelo 1 podr\u00eda ser considerado como nuestro propio baseline. A continuaci\u00f3n presentamos un protocolo de evaluaci\u00f3n manual de los resultados obtenidos. El experimento consisti\u00f3 en la generaci\u00f3n de 15 frases por cada uno de los tres modelos propuestos. Para cada modelo, se consideraron tres queries: $Q=$ {AM",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What topic clusters are identified by LDA?",
    "llm_answer": "Substance use, suicidality, and anxiety disorders.  Also, topics in a causal corpus tend to fall into three main categories, but those categories are not specified.\n",
    "context": [
      {
        "id": "7b757cb6-a62f-4611-be92-7d6ecceb8b70",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1809.05752",
          "start_idx": 7163,
          "end_idx": 7672
        },
        "page_content": " of .784 compared to a baseline of .618. To perform clinical validation of the topics derived from the LDA model, they manually evaluated and annotated the topics, identifying the most informative vocabulary for the top ten topics. With their training data, they found the strongest coherence occurred in topics involving substance use, suicidality, and anxiety disorders. But given the unsupervised nature of the LDA clustering algorithm, the topic coherence they observed is not guaranteed across data sets.",
        "type": "Document"
      },
      {
        "id": "c52c4a6c-f82d-4371-854d-0f59a0f4ccc6",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1604.05781",
          "start_idx": 21504,
          "end_idx": 21944
        },
        "page_content": " terms co-occur across a set of documents. We used the popular topic modeling method Latent Dirichlet Allocation (LDA) BIBREF39 . We ranked unigrams by how strongly associated they were with the topic. Inspecting these unigrams we found that a 10-topic model discovered meaningful topics. See Methods for full details. The top unigrams for each topic are shown in Tab. 1 . Topics in the causal corpus tend to fall into three main categories",
        "type": "Document"
      },
      {
        "id": "1e0c6548-cdc2-4496-84bf-779d678afbcc",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1709.07916",
          "start_idx": 5826,
          "end_idx": 6267
        },
        "page_content": " identifying patterns of clinical events in a cohort of brain cancer patients BIBREF48 . Among topic models, Latent Dirichlet Allocation (LDA) BIBREF49 is the most popular effective model BIBREF50 , BIBREF19 as studies have shown that LDA is an effective computational linguistics model for discovering topics in a corpus BIBREF51 , BIBREF52 . LDA assumes that a corpus contains topics such that each word in each document can be assigned to",
        "type": "Document"
      },
      {
        "id": "b7b675a5-b0c9-45a8-b040-e8fda561eac6",
        "metadata": {
          "chunk_id": 114,
          "document_id": "1909.00183",
          "start_idx": 42250,
          "end_idx": 42770
        },
        "page_content": " LDA topic modelling. While LDA computes topics at a pre-determined level of resolution, our method obtains partitions at all resolutions in one sweep of the Markov time, from which relevant partitions are chosen based on their robustness. The MS partitions at all resolutions are available for further investigation if so needed. Comparison of MS to other partitioning and community detection algorithms: We have partitioned the same kNN-MST graph using several well-known algorithms readily available in code libraries",
        "type": "Document"
      },
      {
        "id": "808e3a0e-48f4-4bda-b6d8-50656526fb4c",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1709.07916",
          "start_idx": 6060,
          "end_idx": 6564
        },
        "page_content": " . LDA assumes that a corpus contains topics such that each word in each document can be assigned to the topics with different degrees of membership BIBREF53 , BIBREF54 , BIBREF55 . Twitter users can post their opinions or share information about a subject to the public. Identifying the main topics of users' tweets provides an interesting point of reference, but conceptualizing larger subtopics of millions of tweets can reveal valuable insight to users' opinions. The topic discovery component of the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much do they outperform previous state-of-the-art?",
    "llm_answer": "Even the fastest of the new models beats the best published result, requiring only 6% of the computation.\n",
    "context": [
      {
        "id": "38d0d5d3-4be2-4a08-bc2e-e9f401074e5d",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1701.06538",
          "start_idx": 17346,
          "end_idx": 17826
        },
        "page_content": ". Details can be found in Appendix UID77 . Results of these three models form the bottom line of Figure FIGREF32 -right. Table TABREF33 compares the results of these models to the best previously-published result on this dataset . Even the fastest of these models beats the best published result (when controlling for the number of training epochs), despite requiring only 6% of the computation. We trained our models using TensorFlow BIBREF30 on clusters containing 16-32 Tesla K",
        "type": "Document"
      },
      {
        "id": "5e0f3225-070e-4250-8f70-93c083311ec4",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "e60e0d3d-b592-4a3f-9ebd-b12edba8a33b",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1909.13375",
          "start_idx": 20788,
          "end_idx": 21299
        },
        "page_content": " For the other types, most of the improvement came from better preprocessing. A more detailed discussion could be found in Section (SECREF36). Notice that different BERTlarge models were used, so the comparison is less direct. Overall, our large models exhibits similar results to those of MTMSNlarge. For multi-span questions we achieve a significantly better performance. While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still",
        "type": "Document"
      },
      {
        "id": "a54407ed-5048-4a8f-be06-c930fc9b5b4e",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1910.09295",
          "start_idx": 14533,
          "end_idx": 14924
        },
        "page_content": " 87.47% accuracy, a 10.05% improvement over the siamese network's performance. GPT-2 finetuned to a final accuracy of 90.99%, a 13.57% improvement from the baseline performance. ULMFiT finetuning gave a final accuracy of 91.59%, an improvement of 14.17% over the baseline Siamese Network. We could see that TL techniques outperformed the siamese network baseline, which we hypothesize is due",
        "type": "Document"
      },
      {
        "id": "cdeb1ecc-94ce-4e50-81fe-1ec41e0037bf",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1710.00341",
          "start_idx": 12930,
          "end_idx": 13373
        },
        "page_content": " vs. false claims extracted from snopes.com, by using information extracted from the Web. Their formulation of the task is the same as ours, but our corpora and label distributions are not the same, which makes a direct comparison impossible. Still, we can see that regarding overall classification accuracy they improve a baseline from 73.7% to 84.02% with their best model, i.e., a 39.2% relative error reduction. In our case, we go from 66.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is the provided treebank?",
    "llm_answer": "The context mentions treebanks with sizes ranging from more than 3000 sentences to 2,557 and 2,922 days.  It does not provide a single, definitive size for a single treebank.\n",
    "context": [
      {
        "id": "bc9cb008-5e81-4bd0-8258-466413ca72e1",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1711.02013",
          "start_idx": 22459,
          "end_idx": 22972
        },
        "page_content": "bank Wall Street Journal section which contained 10 words or less after the removal of punctuation and null elements. Evaluation was done by seeing whether proposed constituent spans are also in the Treebank parse, measuring unlabeled F1 ( INLINEFORM0 ) of unlabeled constituent precision and recall. Constituents which could not be gotten wrong (those of span one and those spanning entire sentences) were discarded. Given the mechanism discussed in Section SECREF14 , our model generates a binary tree. Although",
        "type": "Document"
      },
      {
        "id": "a8782194-ae44-4cad-8916-604ae2b4bd8f",
        "metadata": {
          "chunk_id": 68,
          "document_id": "1911.08673",
          "start_idx": 23969,
          "end_idx": 24082
        },
        "page_content": " effectiveness of our parser by achieving good performance on 27 treebanks, including the PTB and CTB benchmarks.",
        "type": "Document"
      },
      {
        "id": "54e831c9-30f8-4511-8e13-ccf29568f8fd",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1911.03894",
          "start_idx": 10104,
          "end_idx": 10550
        },
        "page_content": "IBREF30 is the second-largest treebank available for French after the FTB (described in subsection SECREF25), it contains data from blogs, news articles, reviews, and Wikipedia. The Sequoia treebank BIBREF31, BIBREF32 comprises more than 3000 sentences, from the French Europarl, the regional newspaper L\u2019Est R\u00e9publicain, the French Wikipedia and documents from the European Medicines Agency. Spoken is a corpus converted automatically from the R",
        "type": "Document"
      },
      {
        "id": "001abd8b-7ba1-407b-8110-49e732f239a9",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1910.12618",
          "start_idx": 19089,
          "end_idx": 19565
        },
        "page_content": " maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014",
        "type": "Document"
      },
      {
        "id": "efee0228-286f-4512-ab11-68aff6bdb428",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1911.08673",
          "start_idx": 17050,
          "end_idx": 17393
        },
        "page_content": " the overall time complexity is $O$($n$log$n$) $+$ $O(n^2)$, simplifying to $O(n^2)$. We evaluate our parsing model on the English Penn Treebank (PTB), the Chinese Penn Treebank (CTB), treebanks from two CoNLL shared tasks and the Universal Dependency (UD) Treebanks, using unlabeled attachment scores (UAS) and labeled attachment scores (LAS)",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what dataset was used?",
    "llm_answer": "Basic Question Dataset (BQD), MS COCO dataset, SCI dataset.\n",
    "context": [
      {
        "id": "6b7f6ca6-24d5-4144-9cfd-92be2de31f39",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "8410bd17-7c9f-4f27-98e4-c5a96b13e1b5",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1703.06492",
          "start_idx": 9989,
          "end_idx": 10376
        },
        "page_content": " dataset, called Basic Question Dataset (BQD), generated by our basic question generation algorithm. BQD is the first basic question dataset. Regarding the BQD, the dataset format is $\\lbrace Image,~MQ,~3~(BQ + corresponding~similarity~score)\\rbrace $ . All of our images are from the testing images of MS COCO dataset BIBREF30 , the MQ, main questions, are from the testing questions of",
        "type": "Document"
      },
      {
        "id": "91df3232-acdc-442e-89f1-92d8ec6a1b84",
        "metadata": {
          "chunk_id": 75,
          "document_id": "1912.09152",
          "start_idx": 26669,
          "end_idx": 26983
        },
        "page_content": " a true Gold Standard Dataset. We thank three anonymous reviewers of our manuscript for their careful reading and their many insightful comments and suggestions. We have made our best in providing a revised version of the manuscript that reflects their suggestions. Any remaining errors are our own responsability.",
        "type": "Document"
      },
      {
        "id": "5240d67c-b588-4df6-a1bb-56be351fb42b",
        "metadata": {
          "chunk_id": 21,
          "document_id": "1905.12260",
          "start_idx": 8180,
          "end_idx": 8639
        },
        "page_content": " results. For example, one (query, image) pair might be \u201ccat with big ears\u201d and an image of a cat. Each (query, image) pair also has a weight corresponding to a relevance score of the image for the query. The dataset includes 3 billion (query, image, weight) triples, with 900 million unique images and 220 million unique queries. The data was prepared by first taking the query-image set, filtering to remove any personally identifiable information and adult",
        "type": "Document"
      },
      {
        "id": "ce53a13e-c986-4e03-9bcf-c6148fa88586",
        "metadata": {
          "chunk_id": 69,
          "document_id": "1803.07771",
          "start_idx": 24768,
          "end_idx": 25193
        },
        "page_content": " 250, 300]. The parameter INLINEFORM0 in INLINEFORM1 -hot encoding is searched in INLINEFORM2 . In this subsubsection, each of the three raw data sets (associated with their labels) shown in Table 1 is used. The clause data are not used. In other words, the training data used in this subsubsection are the same as those used in previous studies. For each data corpus, 1000 raw data samples are used as the test data, and the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the citation intent labels in the datasets?",
    "llm_answer": "Background, Method, ResultComparison.  (Table 1) and six intent categories outlined in Table 2.\n",
    "context": [
      {
        "id": "ea181a7e-6d08-4859-80e0-535b4a5df191",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1904.01608",
          "start_idx": 11894,
          "end_idx": 12502
        },
        "page_content": " as background information providing more context for the current research topic. More interesting intent categories are a direct use of a method or comparison of results. Therefore, our dataset provides a concise annotation scheme that is useful for navigating research topics and machine reading of scientific papers. We consider three intent categories outlined in Table 1 : Background, Method and ResultComparison. Below we describe data collection and annotation details. Citation intent of sentence extractions was labeled through the crowdsourcing platform Figure Eight. We selected a sample of papers",
        "type": "Document"
      },
      {
        "id": "50b346b3-d11d-468f-acbb-d540e7b53d56",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1904.01608",
          "start_idx": 10479,
          "end_idx": 11015
        },
        "page_content": " annotated by domain experts in the NLP field. The data was split into three standard stratified sets of train, validation, and test with 85% of data used for training and remaining 15% divided equally for validation and test. Each citation unit includes information about the immediate citation context, surrounding context, as well as information about the citing and cited paper. The data includes six intent categories outlined in Table 2 . Most existing datasets contain citation categories that are too fine-grained. Some of these",
        "type": "Document"
      },
      {
        "id": "20fad0bd-f2d6-463f-9d42-44ce228cb689",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1904.01608",
          "start_idx": 9710,
          "end_idx": 10218
        },
        "page_content": " of studying citation intents, there are only a few existing publicly available datasets on the task of citation intent classification. We use the most recent and comprehensive (ACL-ARC citations dataset) by BIBREF7 as a benchmark dataset to compare the performance of our model to previous work. In addition, to address the limited scope and size of this dataset, we introduce SciCite, a new dataset of citation intents that addresses multiple scientific domains and is more than five times larger than ACL-",
        "type": "Document"
      },
      {
        "id": "afa3588c-927e-483d-aeab-b965c06e143a",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1904.01608",
          "start_idx": 831,
          "end_idx": 1348
        },
        "page_content": " the problem of citation intent classification by modeling the language expressed in the citation context. A citation context includes text spans in a citing paper describing a referenced work and has been shown to be the primary signal in intent classification BIBREF8 , BIBREF9 , BIBREF7 . Existing models for this problem are feature-based, modeling the citation context with respect to a set of predefined hand-engineered features (such as linguistic patterns or cue phrases) and ignoring other signals that could",
        "type": "Document"
      },
      {
        "id": "6aeec519-8fa8-499c-8e8f-8554e79b05fb",
        "metadata": {
          "chunk_id": 33,
          "document_id": "1904.01608",
          "start_idx": 11347,
          "end_idx": 11894
        },
        "page_content": " are relatively small (less than 2,000 annotated citations). To address these limitations, we introduce SciCite, a new dataset of citation intents that is significantly larger, more coarse-grained and general-domain compared with existing datasets. Through examination of citation intents, we found out many of the categories defined in previous work such as motivation, extension or future work, can be considered as background information providing more context for the current research topic. More interesting intent categories are a direct use",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is quality of annotation measured?",
    "llm_answer": "By comparing independent annotations, adjudicating disagreements, measuring accuracy against AMT annotations,  and/or measuring inter-annotator agreement.  Gold-standard labels are also created for comparison.\n",
    "context": [
      {
        "id": "4944b688-3739-49f0-b85d-b949ac935857",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1906.03538",
          "start_idx": 9308,
          "end_idx": 9838
        },
        "page_content": " the five labels into three labels, by collapsing mildly-support and mildly-oppose into support and oppose, respectively. To assess the quality of these annotations, two of the authors independently annotate a random subset of instances in the previous step (328 perspectives for 10 claims). Afterwards, the differences were adjudicated. We measure the accuracy adjudicated results with AMT annotations to estimate the quality of our annotation. This results in an accuracy of 94%, which shows high-agreement with the crowdsourced",
        "type": "Document"
      },
      {
        "id": "71942aaf-f5f1-4290-810c-1e49eac7dc8f",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1905.10044",
          "start_idx": 10909,
          "end_idx": 11439
        },
        "page_content": " and the kinds of reasoning abilities required to answer them. First, in order to assess annotation quality, three of the authors labelled 110 randomly chosen examples. If there was a disagreement, the authors conferred and selected a single answer by mutual agreement. We call the resulting labels \u201cgold-standard\" labels. On the 110 selected examples, the answer annotations reached 90% accuracy compared to the gold-standard labels. Of the cases where the answer annotation differed from the gold-standard, six were ambiguous or",
        "type": "Document"
      },
      {
        "id": "7eec9b39-3ce7-4587-99e7-65b9168adb3f",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1906.09774",
          "start_idx": 17090,
          "end_idx": 17533
        },
        "page_content": "., (1: not at all, 3: somewhat, 5: very much) from around 100 different annotators. After getting all of the human judgement with different criteria, some of these studies used a t-test to get the statistical significance BIBREF28 , BIBREF39 , while some other used inter-annotator agreement measurement such as Fleiss Kappa BIBREF15 , BIBREF14 . Based on these evaluations, they can compare their system performance with baseline or any other",
        "type": "Document"
      },
      {
        "id": "7ad84a74-dfae-4c36-99a3-0c3aa9c1ab59",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1703.05260",
          "start_idx": 17551,
          "end_idx": 18087
        },
        "page_content": " results were discussed with the authors. Throughout the whole annotation phase, annotators could discuss any emerging issues with the authors. All annotations were done by undergraduate students of computational linguistics. The annotation was rather time-consuming due to the complexity of the task, and thus we decided for single annotation mode. To assess annotation quality, a small sample of texts was annotated by all four annotators and their inter-annotator agreement was measured (see Section \"Inter-Annotator Agreement\" ). It",
        "type": "Document"
      },
      {
        "id": "c6935c4b-93d9-4e80-a744-3cd0b64ea955",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1802.05574",
          "start_idx": 2859,
          "end_idx": 3379
        },
        "page_content": ". Human annotators analyze sentences and determine correct relations to be extracted. Systems are then evaluated with respect to the overlap or similarity of their extractions to the ground truth annotations, allowing the standard metrics of precision and recall to be reported. This seems sensible but is actually problematic because of different but equivalent representations of the information in an article. For example, consider the sentence \u201cThe patient was treated with Emtricitabine, Etravirine, and Darunavir\u201d.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What accuracy score do they obtain?",
    "llm_answer": "Team 9 achieved a joint goal accuracy of 86.53%.\n",
    "context": [
      {
        "id": "a5ccc168-6ed7-4b5a-9a9b-1aaaf5f12202",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1909.05246",
          "start_idx": 16417,
          "end_idx": 16962
        },
        "page_content": " accuracy measures the similarity of the system generated response versus the target response. Eric and Manning eric2017copy used this metric to evaluate their systems in which they considered their response to be correct if all tokens in the system generated response matched the corresponding token in the target response. This metric is a little bit harsh, and the results may be low since all the tokens in the generated response have to be exactly in the same position as in the target response. Per-Dialogue Accuracy: We calculate per-dial",
        "type": "Document"
      },
      {
        "id": "ad156fd3-1eaf-4e70-8f56-1f3024106d99",
        "metadata": {
          "chunk_id": 58,
          "document_id": "2001.11268",
          "start_idx": 22809,
          "end_idx": 23352
        },
        "page_content": " precision scores in ranges between 0.92 and 0.97 are possible with F1 scores not dropping below 0.84 for the main classes of interest. In practice, the detachment between model predictions and assignment of labels means that a reviewer who wishes to switch between high recall and high precision results can do so very quickly, without obtaining new predictions from the model itself. More visualizations can be found in this project's GitHub repository , including true class labels and a detailed breakdown of true and false predictions for",
        "type": "Document"
      },
      {
        "id": "db1a912a-f080-47ed-9383-89ff56523759",
        "metadata": {
          "chunk_id": 276,
          "document_id": "1804.08186",
          "start_idx": 99381,
          "end_idx": 99888
        },
        "page_content": "often referred to as the \u201cgold-standard\u201d), and a predicted label for each document from the same set, the document-level accuracy is the proportion of documents that are correctly labeled over the entire evaluation collection. This is the most frequently reported metric and conveys the same information as the error rate, which is simply the proportion of documents that are incorrectly labeled (i.e. INLINEFORM0 ). Authors sometimes provide a per-language breakdown of results. There are two distinct ways",
        "type": "Document"
      },
      {
        "id": "db5e1481-51b8-45de-83a5-8564d1b31d8a",
        "metadata": {
          "chunk_id": 55,
          "document_id": "2002.01359",
          "start_idx": 23040,
          "end_idx": 23557
        },
        "page_content": "11 shows the evaluation metrics for the different submissions obtained on the test set. It also lists the performance of different submissions on seen and unseen services, helping evaluate the effectiveness in zero-shot settings. Team 9 achieved a very high joint goal accuracy of 86.53%, around 9% higher than the second-placed team. We observed the following trends across submissions: For unseen services, performance on categorical slots is comparable to that on non-categorical slots. On the other hand, for seen",
        "type": "Document"
      },
      {
        "id": "ce9a5da7-b644-459d-89dc-6f6485bbc871",
        "metadata": {
          "chunk_id": 62,
          "document_id": "1701.02877",
          "start_idx": 22362,
          "end_idx": 22802
        },
        "page_content": " data gives little to no improvement, whereas adding positive examples drastically improves performance. Table UID9 shows results with size normalised precision (P), recall (R), and F1-Score (F1). The five lowest P, R and F1 values per method (CRFSuite, Stanford NER, SENNA) are in bold to highlight underperformers. Results for all corpora are summed with macro average. Comparing the different methods, the highest F1 results are achieved",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the 12 class bilingual text?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "ff9b71f6-09d4-4944-bd7c-9651f1041a00",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1911.13066",
          "start_idx": 1723,
          "end_idx": 2230
        },
        "page_content": " many NLP tasks involving multilingual text. A Convolutional Neural Network (CNN) based model for sentiment classification of a multilingual dataset was proposed in BIBREF3. However, a particular record in the dataset belonged to one language only. In our case, a record can have either one or two languages. There is very little published work on this specific setting. One way to classify bilingual text is to normalize the different variations of a word to a standard spelling before training the model B",
        "type": "Document"
      },
      {
        "id": "02448219-6e5d-4da3-9af7-87673197483a",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1711.11221",
          "start_idx": 12389,
          "end_idx": 12768
        },
        "page_content": " We selected corpora LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and a portion of data from the corpus LDC2004T08 (Hong Kong Hansards/Laws/News) as our bilingual training data, where document boundaries are explicitly kept. In total, our training data contain 103,236 documents and 2.80M sentences. On average, each document consists of 28.4 sentences. We chose NIST05 dataset",
        "type": "Document"
      },
      {
        "id": "89d9c1df-9290-4eba-a064-19d884c6b60c",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1905.12260",
          "start_idx": 5539,
          "end_idx": 5963
        },
        "page_content": " bilingual lexicons BIBREF13 , BIBREF4 , BIBREF6 , BIBREF14 or parallel/aligned corpora BIBREF15 , BIBREF4 , BIBREF16 , BIBREF17 . These works are also largely bilingual due to either limitations of methods or the requirement for data that exists only for a few language pairs. Bilingual embeddings are less desirable because they do not leverage the relevant resources of other languages. For example, in learning bilingual",
        "type": "Document"
      },
      {
        "id": "47156113-1a5a-4cd2-8111-284623be4586",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1909.09270",
          "start_idx": 13903,
          "end_idx": 14242
        },
        "page_content": "2003 shared tasks BIBREF21, BIBREF22. These are taken from newswire text, and have labelset of Person, Organization, Location, Miscellaneous. The remaining four languages come from the LORELEI project BIBREF23. These languages are: Amharic (amh: LDC2016E87), Arabic (ara: LDC2016E89), Hindi (hin: LDC2017E62), and Somali (som: LDC2016E91).",
        "type": "Document"
      },
      {
        "id": "9762ec6e-f352-40c6-922d-85a08024eee3",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1911.13066",
          "start_idx": 2508,
          "end_idx": 3032
        },
        "page_content": " is not generalizable in the case of Roman Urdu text as it is an informal language with no proper grammatical rules and dictionary. More recent approach utilizes code-switching annotations to improve the predictive performance of the model, where each word is annotated with its respective language label. Such an approach is not scalable for large data as annotation task becomes tedious. In this paper, we propose a multi-cascaded deep learning network, called as McM for multi-class classification of bilingual short text",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which are the sequence model architectures this method can be transferred across?",
    "llm_answer": "LSTM seq2seq, Convolutional seq2seq, and transformer.\n",
    "context": [
      {
        "id": "6404e975-165d-42fe-a1e9-21c76b763acb",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1707.00110",
          "start_idx": 0,
          "end_idx": 365
        },
        "page_content": "Sequence-to-sequence models BIBREF0 , BIBREF1 have achieved state of the art results across a wide variety of tasks, including Neural Machine Translation (NMT) BIBREF2 , BIBREF3 , text summarization BIBREF4 , BIBREF5 , speech recognition BIBREF6 , BIBREF7 , image captioning BIBREF8 , and conversational modeling BIBREF9 , BIBREF10 . The most popular approaches are",
        "type": "Document"
      },
      {
        "id": "b893d32c-e2d2-4f08-955c-60acf2248991",
        "metadata": {
          "chunk_id": 51,
          "document_id": "2002.05058",
          "start_idx": 20175,
          "end_idx": 20645
        },
        "page_content": " representative sequence-to-sequence architectures: LSTM BIBREF27 seq2seq, Convolutional seq2seq BIBREF28, and transformer BIBREF1 model. We compare models with different architectures, hyperparameter choices, and early-stopping criteria with different automated metrics, as well as human evaluation. Regarding the evaluation metric (and criteria for choosing hyperparameter choice and early-stopping), we compare the proposed approach with the discriminative evaluator,",
        "type": "Document"
      },
      {
        "id": "74b9bf60-1bb5-4699-a3d3-d7de910eba23",
        "metadata": {
          "chunk_id": 73,
          "document_id": "1811.09786",
          "start_idx": 23896,
          "end_idx": 24139
        },
        "page_content": " are also prevalent for sequence modeling. The source code of our model can be found at https://github.com/vanzytay/NIPS2018_RCRN. We thank the anonymous reviewers and area chair from NIPS 2018 for their constructive and high quality feedback.",
        "type": "Document"
      },
      {
        "id": "f0fa5aa9-4b2c-42d9-a3a4-c6a2ce550e7e",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1909.05246",
          "start_idx": 1277,
          "end_idx": 1753
        },
        "page_content": " neural networks based BIBREF2, BIBREF3, BIBREF4, BIBREF5 and thus can be adapted to new domains by training on relevant dialogue datasets for that specific domain. Furthermore, all sequence modelling methods can also be used in training end-to-end task-oriented chatbots. A sequence modelling method receives a sequence as input and predicts another sequence as output. For example in the case of machine translation the input could be a sequence of words in a given language",
        "type": "Document"
      },
      {
        "id": "4d051c35-61fb-4f15-983c-a4ace5a7177f",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1907.02636",
          "start_idx": 2332,
          "end_idx": 2725
        },
        "page_content": ", and tokens assigned with IOC labels are then collected as IOCs. Recently, sequence labelling models have been utilized in many NLP tasks. Huang et al. BIBREF6 proposed using a sequence labelling model based on the bidirectional long short-term memory (LSTM) BIBREF7 for the task of named entity recognition (NER). Chiu et al. BIBREF8 and Lample et al. BIBREF9 proposed integrating LSTM encod",
        "type": "Document"
      }
    ]
  },
  {
    "query": " What percentage of improvement in inference speed is obtained by the proposed method over the newest state-of-the-art methods?",
    "llm_answer": "The answer is not found in the context.  While the text discusses faster inference speed than some previous methods, it does not quantify the improvement relative to the *newest* state-of-the-art methods.\n",
    "context": [
      {
        "id": "076036f2-15be-4e07-9b27-ce4ec96f6399",
        "metadata": {
          "chunk_id": 89,
          "document_id": "1908.05969",
          "start_idx": 26445,
          "end_idx": 26883
        },
        "page_content": " inference speed of our method when implementing the sequnece modeling layer with the LSTM-based, CNN-based, and Transformer-based architecture, respectively. The speed was evaluated by average sentences per second using a GPU (NVIDIA TITAN X). For a fair comparison with Lattice-LSTM and LR-CNN, we set the batch size of our method to 1 at inference time. From the table, we can see that our method has a much faster inference speed than",
        "type": "Document"
      },
      {
        "id": "3ce2ee6b-f6a8-44ea-97ea-673aa3cb80a5",
        "metadata": {
          "chunk_id": 90,
          "document_id": "1908.05969",
          "start_idx": 26883,
          "end_idx": 27346
        },
        "page_content": " inference time. From the table, we can see that our method has a much faster inference speed than Lattice-LSTM when using the LSTM-based sequence modeling layer, and it was also much faster than LR-CNN, which used an CNN architecture to implement the sequence modeling layer. And as expected, our method with the CNN-based sequence modeling layer showed some advantage in inference speed than those with the LSTM-based and Transformer-based sequence model layer.",
        "type": "Document"
      },
      {
        "id": "499d6543-14ac-46bc-9927-462bdc9d16c4",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1710.00341",
          "start_idx": 13213,
          "end_idx": 13653
        },
        "page_content": "e., a 39.2% relative error reduction. In our case, we go from 66.7% to 80.0%, i.e., an almost identical 39.9% error reduction. These results are very encouraging, especially given the fact that our model is much simpler than theirs regarding the sources of information used (they model the stance of the text, the reliability of the sources, the language style of the articles, and the temporal footprint). Next, we tested the generality of",
        "type": "Document"
      },
      {
        "id": "ff453b67-2cea-4319-836d-7d47c4847292",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1611.03382",
          "start_idx": 21894,
          "end_idx": 22215
        },
        "page_content": " significantly speed up inference. We have demonstrated the effectiveness of our approach in the context of summarization and shown state-of-the-art performance. In the future, we plan to tackle summarization problems with large input text. We also plan to exploit our findings in other tasks such as machine translation.",
        "type": "Document"
      },
      {
        "id": "f8949217-0f8b-4344-91b0-74cb9adf0516",
        "metadata": {
          "chunk_id": 3,
          "document_id": "2002.05829",
          "start_idx": 1320,
          "end_idx": 1837
        },
        "page_content": " million inferences. The energy efficiency of inference cost could be critical to both business planning and environment impact. Previous work BIBREF10, BIBREF11 on this topic proposed new metrics like FPO (floating point operations) and new practice to report experimental results based on computing budget. Other benchmarks like BIBREF12 and BIBREF13 compares the efficiency of models on the classic reading comprehension task SQuAD and machine translation tasks. However, there has not been a concrete or practical",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the metric that is measures in this paper?",
    "llm_answer": "Premise-Hypothesis Distance, METEOR, tIoU, BIBREF16, WMD, VSM, PV-DTW, and WordSLOR/WPSLOR.\n",
    "context": [
      {
        "id": "78baabfc-a3d8-4532-ab82-565a8f653ade",
        "metadata": {
          "chunk_id": 75,
          "document_id": "1607.06025",
          "start_idx": 28598,
          "end_idx": 29144
        },
        "page_content": " figure is not show absolute values for each of the metrics, but to compare the metrics' curves to the curve of our main metric, the accuracy of the classifier. The first metric \u2013 Premise-Hypothesis Distance \u2013 represents the average Jaccard distance between the premise and the generated hypothesis. Datasets generated with low latent dimensions have hypotheses more similar to premises, which indicates that the generated hypotheses are more trivial and less diverse than hypothesis generated with higher latent dimensions. We also evaluated the",
        "type": "Document"
      },
      {
        "id": "6542b67b-e627-4256-b99a-5efa12753e85",
        "metadata": {
          "chunk_id": 70,
          "document_id": "2003.07758",
          "start_idx": 23045,
          "end_idx": 23519
        },
        "page_content": "REF52. We regard the METEOR as our primary metric as it has been shown to be highly correlated with human judgement in a situation with a limited number of references (only one, in our case). We employ the official evaluation script provided in BIBREF53. Thus, the metrics are calculated if a proposed event and a ground truth location of a caption overlaps more than a specified temporal Intersection over Union (tIoU) and zero otherwise. All metric values are averaged for",
        "type": "Document"
      },
      {
        "id": "ca9dddcc-e3f4-4f29-8c33-fa0819f9fd44",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1912.01046",
          "start_idx": 14275,
          "end_idx": 14679
        },
        "page_content": " BIBREF16, which measures how far away the predicted span is from the gold standard span, as a metric. The rationale behind the metric is that, in practice, it suffices to recommend a rough span which contains the answer \u2013 a difference of a few seconds would not matter much to the user. Specifically, the predicted span is counted as correct if $|pred_{start} - gt_{start}| + |pred_{end} - gt_{end}| <=$",
        "type": "Document"
      },
      {
        "id": "362ad168-3327-44d9-8851-5fafdaa75e0f",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1709.01256",
          "start_idx": 17745,
          "end_idx": 18165
        },
        "page_content": " distance measure explained in Section SECREF23 . WMD: The Word Mover's Distance introduced in Section SECREF1 . WMD adapts the earth mover's distance to the space of documents. VSM: The similarity measure introduced in Section UID12 . PV-DTW: PV-DTW is the same as Algorithm SECREF21 except that the distance between two paragraphs is not based on Algorithm SECREF20 but rather computed as INLINEFORM0 where INLINEFORM1",
        "type": "Document"
      },
      {
        "id": "85a66ada-4302-4eda-8b84-478df80153a6",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1809.08731",
          "start_idx": 12336,
          "end_idx": 12724
        },
        "page_content": "$ , it is a non-symmetric quasi-metric. We apply it with $Y$ being the human ratings. An additional advantage as compared to Pearson is that it has an interpretable meaning: the expected error made by a given metric as compared to the human rating. As shown in Table 3 , WordSLOR and WPSLOR correlate best with human judgments: WordSLOR (respectively WPSLOR) has a $0.025$ (respectively $",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Was evaluation metrics and criteria were used to evaluate the output of the cascaded multimodal speech translation?",
    "llm_answer": "Automatic metrics, subjective human evaluation (with a native German/fluent English speaker), and sentence-level ranking of outputs (head-to-head) were used.\n",
    "context": [
      {
        "id": "082910e0-ed77-488d-adf8-43eae65c7fe2",
        "metadata": {
          "chunk_id": 41,
          "document_id": "1612.06897",
          "start_idx": 13297,
          "end_idx": 13817
        },
        "page_content": " the score on the in-domain test set without seeing any significant degradation on the out-of-domain test set. In addition to evaluating on automatic metrics, we also performed a subjective human evaluation where a human annotator assigns a score based on the quality of the translation. The judgments are done by an experienced annotator (a native speaker of German and a fluent speaker of English). We ask our annotator to judge the translation output of different systems on a randomly selected in-domain sample of 50",
        "type": "Document"
      },
      {
        "id": "67747abf-6bd0-4403-ae57-fb90db5a6572",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1910.10408",
          "start_idx": 21469,
          "end_idx": 21983
        },
        "page_content": " goal is to be able to generate shorter translations and as close as possible to the length of the source sentences, we focused the manual evaluation on the Short output class and aimed to verify possible losses in quality with respect to the baseline system. We ran a head-to-head evaluation on the first 10 sentences of each test talk, for a total of 270 sentences, by asking annotators to blindly rank the two system outputs (ties were also permitted) in terms of quality with respect to a reference translation",
        "type": "Document"
      },
      {
        "id": "893e95ee-7e19-4559-b803-e94ab522f5ec",
        "metadata": {
          "chunk_id": 33,
          "document_id": "2004.01694",
          "start_idx": 13179,
          "end_idx": 13705
        },
        "page_content": " is the unit of evaluation. Historically, machine translation has primarily operated on the level of sentences, and so has machine translation evaluation. However, it has been remarked that human raters do not necessarily understand the intended meaning of a sentence shown out-of-context BIBREF25, which limits their ability to spot some mistranslations. Also, a sentence-level evaluation will be blind to errors related to textual cohesion and coherence. While sentence-level evaluation may be good enough when evaluating MT",
        "type": "Document"
      },
      {
        "id": "a9b19769-4bc4-4353-8cc0-85ab0f9ddc2a",
        "metadata": {
          "chunk_id": 1,
          "document_id": "2004.01694",
          "start_idx": 541,
          "end_idx": 1049
        },
        "page_content": " human evaluation studies in which some neural machine translation systems reportedly performed at (or above) the level of human translators for news translation from Chinese to English BIBREF3 and English to Czech BIBREF4, BIBREF5. Both evaluations were based on current best practices in the field: they used a source-based direct assessment with non-expert annotators, using data sets and the evaluation protocol of the Conference on Machine Translation (WMT). While the results are intriguing, especially",
        "type": "Document"
      },
      {
        "id": "58e15f56-c72b-4674-af7a-104b8db45293",
        "metadata": {
          "chunk_id": 29,
          "document_id": "2004.01694",
          "start_idx": 11778,
          "end_idx": 12167
        },
        "page_content": "-experts were NLP researchers native in Chinese, working in an English-speaking country. The ratings are elicited with Appraise BIBREF21. We derive an overall score for each translation (H$_A$, MT$_1$, and MT$_2$) based on the rankings. We use the TrueSkill method adapted to MT evaluation BIBREF22 following its usage at WMT15, i. e., we run 1,000 iterations of the rankings recorded with",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the two decoding functions?",
    "llm_answer": "Linear projection and a family of bijective transformations with non-linear functions.\n",
    "context": [
      {
        "id": "9280259f-992e-4eb4-a2b5-77e81d1e92df",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1809.02731",
          "start_idx": 7153,
          "end_idx": 7523
        },
        "page_content": " noted as $f^{-1}_{\\text{de}}()$ . In order to reduce the complexity and the running time during both training and testing, the decoding function $f_{\\text{de}}()$ needs to be easily invertible. Here, two types of decoding functions are considered and explored. In this case, the decoding function is a linear projection, which is $= f_{\\text{de}}()=+ $ , where $\\in ^{d",
        "type": "Document"
      },
      {
        "id": "9b5177b2-7f6d-4b95-9900-85f338904637",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1809.02731",
          "start_idx": 8563,
          "end_idx": 8966
        },
        "page_content": "top (- ) \\nonumber $$   (Eq. 13)   Therefore, the decoder is also utilised after learning to serve as a linear encoder in addition to the RNN encoder. A general case is to use a bijective function as the decoder, as the bijective functions are naturally invertible. However, the inverse of a bijective function could be hard to find and its calculation could also be computationally intense. A family of",
        "type": "Document"
      },
      {
        "id": "31e5530a-fed2-4491-878d-15321be8c1e2",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1912.06813",
          "start_idx": 6852,
          "end_idx": 7230
        },
        "page_content": " encoder\u2014decoder structure BIBREF4, where advanced ones are equipped with an attention mechanism BIBREF5, BIBREF6. First, an encoder ($\\text{Enc}$) maps $\\vec{x}_{1:n}$ into a sequence of hidden representations ${1:n}=(1, \\cdots , n)$. The decoding of the output sequence is autoregressive, which means that the previously generated symbols are considered an additional input at",
        "type": "Document"
      },
      {
        "id": "780cbde5-c016-485c-89be-c791f3b6b24e",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1809.02731",
          "start_idx": 8154,
          "end_idx": 8495
        },
        "page_content": " function is simply $$1 , which is easily computed. The regularisation formula is $$2 , where $$3 is the Frobenius norm. Specifically, the update rule BIBREF23 for the regularisation is:  $$:=(1+\\beta )-\\beta (^\\top )\\nonumber $$   (Eq. 12)   The usage of the decoder during training and testing is defined as follows:  $$\\text{Training:} \\h",
        "type": "Document"
      },
      {
        "id": "9b4bdcf4-abf0-4e27-bbec-8617eefe70fc",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1809.02731",
          "start_idx": 2839,
          "end_idx": 3294
        },
        "page_content": " Our approach is to invert the decoding function in order to use it as another encoder to assist the original encoder. In order to make computation of the inverse function well-posed and tractable, careful design of the decoder is needed. A simple instance of an invertible decoder is a linear projection with an orthonormal square matrix, whose transpose is its inverse. A family of bijective transformations with non-linear functions BIBREF17 , BIBREF18",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the domains covered in the dataset?",
    "llm_answer": "17 domains (excluding the \"Alarm\" domain), and 20 domains across the training, development, and test datasets.\n",
    "context": [
      {
        "id": "86f1ea1a-ccd6-490e-82a2-7c29633090b9",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1909.03242",
          "start_idx": 7530,
          "end_idx": 7943
        },
        "page_content": " of instances, as well as labels per domain, are shown in Table TABREF34 and label names in Table TABREF43 in the appendix. The dataset is split into a training part (80%) and a development and testing part (10% each) in a label-stratified manner. Note that the domains vary in the number of labels, ranging from 2 to 27. Labels include both straight-forward ratings of veracity (`correct', `incorrect'), but also",
        "type": "Document"
      },
      {
        "id": "86a8b5b4-502a-47cd-be33-77fc57e26423",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1612.06897",
          "start_idx": 12548,
          "end_idx": 13016
        },
        "page_content": "domain data. Figure 2 shows the learning curve of the ensemble of the baseline and the continue model for different sizes of in-domain training data. The used in-domain data is a randomly selected subset of the entire pool of the in-domain data available to us. We also report the result when all of the in-domain data in the pool is used. As shown in Figure 2 the translation quality of the out-of-domain test set only degrades slightly for all the different sizes of",
        "type": "Document"
      },
      {
        "id": "3035ded2-a530-4e6f-a74e-7931d8085645",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1909.05855",
          "start_idx": 5622,
          "end_idx": 6170
        },
        "page_content": " of multiple services per domain, are representative of these scale-related challenges. Furthermore, our evaluation sets contain many services, and consequently slots, which are not present in the training set, to help evaluate model performance on unseen services. The 17 domains (`Alarm' domain not included in training) present in our dataset are listed in Table TABREF5. We create synthetic implementations of a total of 34 services or APIs over these domains. Our simulator framework interacts with these services to generate dialogue outlines",
        "type": "Document"
      },
      {
        "id": "8348e336-9079-412a-a078-faa11a754824",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1907.11499",
          "start_idx": 6351,
          "end_idx": 6867
        },
        "page_content": " at hand. For example, the definition of the \u201cLifestyle\u201d domain is \u201cthe interests, opinions, behaviors, and behavioral orientations of an individual, group, or culture\u201d. Figure FIGREF5 provides an overview of our Domain Detection Network, which we call DetNet. The model comprises two modules; an encoder learns representations for words and sentences whilst incorporating prior domain information; a detector generates domain scores for words, sentences, and documents by selectively attending to previously encoded",
        "type": "Document"
      },
      {
        "id": "40f64673-65b3-488c-bf5e-698652b3ef8a",
        "metadata": {
          "chunk_id": 29,
          "document_id": "2002.01359",
          "start_idx": 12314,
          "end_idx": 12843
        },
        "page_content": " common approach utilized in other datasets BIBREF0. It has been shown that such datasets suffer from substantial annotation errors BIBREF18. Thirdly, using a simulator greatly simplifies the data collection task and instructions as only paraphrasing is needed to achieve a natural dialogue. This is particularly important for creating a large dataset spanning multiple domains. The 20 domains present across the train, dev and test datasets are listed in Table TABREF10, as are the details regarding which domains are present in",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are the two different models trained?",
    "llm_answer": "One model is trained first for coarse-grained features, then the other is unfrozen and trained in a low learning rate for fine-grained features.  Another method fine-tunes a BERT model with a new prediction layer first, then deploys the model and loads the fine-tuned model.  Both models described use Stochastic Gradient Descent with ADAM optimization and a learning rate of 1e-4.  Dropout regularization is also used.\n",
    "context": [
      {
        "id": "9f6a36c2-5bd1-45d6-9bfd-bc386cca7e27",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1908.06606",
          "start_idx": 14712,
          "end_idx": 15209
        },
        "page_content": " One is trained at first for coarse-graind features while freezing the parameter of the other. Then unfreeze the other one and train the entire model in a low learning rate for fetching fine-grained features. Inspired by this and due to the large amount of parameters in BERT model, to speed up the training process, we fine tune the BERT model with new prediction layer first to achieve a better contextualized representation performance. Then we deploy the proposed model and load the fine tuned",
        "type": "Document"
      },
      {
        "id": "b47dbf1b-101c-472a-b08d-0c3028da0961",
        "metadata": {
          "chunk_id": 88,
          "document_id": "1902.10525",
          "start_idx": 33593,
          "end_idx": 34116
        },
        "page_content": " ). The new architecture performs between 20%\u201340% (relative) better over almost all languages. To understand how the different datasets relate to each other, we performed a set of experiments and evaluations with the goal of better characterizing the differences between the datasets. We trained a recognizer on each of the three training sets separately, then evaluated each system on all three test sets (Table TABREF65 ). The neural network architecture is the same as the one we determined earlier (5 layers bidirection",
        "type": "Document"
      },
      {
        "id": "e6945aae-4eed-4ce7-8711-9722b62d448f",
        "metadata": {
          "chunk_id": 68,
          "document_id": "1910.08210",
          "start_idx": 25978,
          "end_idx": 26510
        },
        "page_content": " TABREF50 shows the three different settings. For each, we study the transfer behaviour of models on new environments. Figure FIGREF52 shows the learning curve when training a model on the held-out environments directly and when transferring the model trained on train environments to held-out environments. We observe that all models are significantly more sample-efficient when transferring from training environments, despite the introduction of new edges and new nodes. In Figure FIGREF51, we see that the FiLM model outperforms",
        "type": "Document"
      },
      {
        "id": "168510e3-3ce5-4e16-9a4c-c74d4558b648",
        "metadata": {
          "chunk_id": 31,
          "document_id": "2002.05829",
          "start_idx": 12450,
          "end_idx": 12894
        },
        "page_content": " model converges and can reach expected performance as soon as possible with parameter searching. As shown in Figure FIGREF15, the fine-tuning performance curve differs a lot among pretrained models. The x-axis denoting time consumed is shown in log-scale for better comparison of different models. None of the models acutally take the lead in all tasks. However, if two pretrained models are in the same family, such as BERTBASE and BERTLARGE,",
        "type": "Document"
      },
      {
        "id": "e6cc3ce3-3133-404f-b83d-ca93be6d260e",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1910.11949",
          "start_idx": 10675,
          "end_idx": 11085
        },
        "page_content": " Both models are trained using Stochastic Gradient Descent with ADAM optimization BIBREF28 and a learning rate of 1e-4. Besides, we use dropout regularization BIBREF29 which prevents from over-fitting by dropping some units of the network. The VQG encoder is composed of 2048 neuron cells, while the VQG decoder has an attention layer of 512 followed by an embedding layer of 512 and a LSTM with the same size.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How long is the dataset?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "6f04ea61-adc5-4abb-9261-abc554ebac1b",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1711.02013",
          "start_idx": 20658,
          "end_idx": 21116
        },
        "page_content": "ings, between recurrent layers, and on recurrent states were (0.7, 0.5, 0.5) respectively. dataset contains 17M training tokens and has a vocabulary size of 44k words. The dataset is partitioned into a training set (first 99M characters) and a development set (last 1M characters) that is used to report performance. As this dataset contains various articles from Wikipedia, the longer term information (such as current topic) plays a bigger role than in the",
        "type": "Document"
      },
      {
        "id": "9ccb7945-9a96-417a-b52e-68855c4910b1",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "2d3bdfec-4ed5-456d-bd82-023fad798858",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "b2a50f9d-7c05-4ecd-b65d-87cb9f0f75d3",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1906.03538",
          "start_idx": 12505,
          "end_idx": 12830
        },
        "page_content": ". We now provide a brief summary of [wave]390P[wave]415e[wave]440r[wave]465s[wave]485p[wave]525e[wave]535c[wave]595t[wave]610r[wave]635u[wave]660m. The dataset contains about INLINEFORM0 claims with a significant length diversity (Table TABREF19 ). Additionally, the dataset comes with INLINEFORM1 perspectives, most of which",
        "type": "Document"
      },
      {
        "id": "c074198c-0ba3-4756-8dd2-e14dd86518bc",
        "metadata": {
          "chunk_id": 39,
          "document_id": "2004.01980",
          "start_idx": 13028,
          "end_idx": 13433
        },
        "page_content": " romantic, and click-baity text. The average sentence length in the NYT, CNN, Humor, Romance, and Clickbait datasets are 8.8, 9.2, 12.6, 11.6 and 8.7 words, respectively. The source dataset contains news articles paired with corresponding headlines. To enrich the training corpus, we combine two datasets: the New York Times (56K) and CNN (90K). After combining these two datasets, we randomly selected 3,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big are negative effects of proposed techniques on high-resource tasks?",
    "llm_answer": "Comparable on high-resource En-Fr task.\n",
    "context": [
      {
        "id": "33fbda7b-17cb-4dd8-8bf2-33c036055236",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1909.06434",
          "start_idx": 11976,
          "end_idx": 12470
        },
        "page_content": " the optimization process. Compared to single-task baselines, performance improved on the low-resource En-De task and was comparable on high-resource En-Fr task. For future work, in order to increase the utility of adaptive schedulers, it would be beneficial to explore their use on a much larger number of simultaneous tasks. In this scenario, they may prove more useful as hyper-parameter search over fixed schedules would become cumbersome. In this appendix, we present the impact of various",
        "type": "Document"
      },
      {
        "id": "97f5a7ab-6607-4500-903d-1a898035b8b3",
        "metadata": {
          "chunk_id": 73,
          "document_id": "1912.09152",
          "start_idx": 25875,
          "end_idx": 26391
        },
        "page_content": "design system can outperform a ner tagger for biomedical entities based on state-of-the-art artificial neural network technology. Thus, knowledge-based methods stand on their own merits in task resolution. But, perhaps most importantly, the other key point brought to light in this contribution is that a resource-based approach also favours a more critical stance on the dataset(s) used to evaluate system performance. With these methods, system development can go hand in hand with dataset refinement in a virtuous",
        "type": "Document"
      },
      {
        "id": "5b141f36-24a0-4aa4-b890-a31a720631f1",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1707.06939",
          "start_idx": 2233,
          "end_idx": 2805
        },
        "page_content": " population of workers. Our results indicate that the AUI treatment did not affect quality, and did not help workers perform more quickly or achieve greater response consensus. Instead, workers with the AUI were significantly slower and their responses were more diverse than workers in the non-AUI control group. An important goal of crowdsourcing research is achieving efficient scalability of the crowd to very large sets of tasks. Efficiency in crowdsourcing manifests both in receiving more effective information per worker and in making individual workers faster and",
        "type": "Document"
      },
      {
        "id": "161876d1-e935-43d3-bb57-cfe9d2566dea",
        "metadata": {
          "chunk_id": 4,
          "document_id": "2002.05829",
          "start_idx": 1602,
          "end_idx": 2139
        },
        "page_content": " comprehension task SQuAD and machine translation tasks. However, there has not been a concrete or practical reference for accurate estimation on NLP model pretraining, fine-tunning and inference considering multi-task energy efficiency. Energy efficiency can be reflected in many metrics including carbon emission, electricity usage, time consumption, number of parameters and FPO as shown in BIBREF10. Carbon emission and electricity are intuitive measures yet either hard to track or hardware-dependent. Number of parameteres does not",
        "type": "Document"
      },
      {
        "id": "e30bfd22-016a-4dd9-a066-e4be3608e241",
        "metadata": {
          "chunk_id": 41,
          "document_id": "1707.03764",
          "start_idx": 15735,
          "end_idx": 16277
        },
        "page_content": "performing out of the 22 systems submitted in this shared task. Using additional training data, `smart' features, and hand-crafted resources hurts rather than helps performance. A possible lesson to take from this would be that manually crafting features serves only to hinder a machine learning algorithm's ability to find patterns in a dataset, and perhaps it is better to focus one's efforts on parameter optimisation instead of feature engineering. However, we believe that this is too strong a conclusion to draw from this limited study,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Are this techniques used in training multilingual models, on what languages?",
    "llm_answer": "English, Russian, Arabic, Chinese, German, Spanish, and French.  Also, Asturian (target) and Spanish (source).\n",
    "context": [
      {
        "id": "8dc9de54-972a-49e5-a49b-e65a88626cff",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1809.04686",
          "start_idx": 1578,
          "end_idx": 2066
        },
        "page_content": " or multilingual NLP, the task of transferring knowledge from one language to another, serves as a good test bed for evaluating various transfer learning approaches. For cross-lingual NLP, the most widely studied approach is to use multilingual embeddings as features in neural network models. However, research has shown that representations learned in context are more effective BIBREF5 , BIBREF6 ; therefore, we aim at doing better than just using multilingual embeddings in the cross-",
        "type": "Document"
      },
      {
        "id": "b63f6629-8862-4f50-bd94-e80c3317d585",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1809.08510",
          "start_idx": 17158,
          "end_idx": 17644
        },
        "page_content": ", Russian, Arabic, Chinese, German, Spanish, French. We maintain the same procedure as described above. The hidden size of the language model was increased to 1024 with 16K BPE tokens being used. The first model was trained on English Russian, second was trained on English Russian Arabic and so on. For arabic we still trained from left to right even though naturally the language is read from right to left. We report the results in Figure 5 . As the number of languages increases the",
        "type": "Document"
      },
      {
        "id": "7aa6ccc6-f057-4e09-89d2-dfe2a18df83a",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1710.09589",
          "start_idx": 2405,
          "end_idx": 2905
        },
        "page_content": ". Early approaches focus mainly on bilingual approaches, while recent research aims at mapping several languages into a single space. The body of literature is huge, but an excellent recent overview is given in xlingsurvey. We chose a very simple and recently proposed method that does not rely on any parallel data BIBREF4 and extend it to the multilingual case. In particular, the method falls under the broad umbrella of monolingual mappings. These approaches first train monolingual embeddings on",
        "type": "Document"
      },
      {
        "id": "1d46cce7-14e7-442d-bb5e-4d7eaa7afbe1",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1911.03894",
          "start_idx": 4523,
          "end_idx": 4978
        },
        "page_content": " have been released: ELMo models for Japanese, Portuguese, German and Basque and BERT for Simplified and Traditional Chinese and German. However, to the best of our knowledge, no particular effort has been made toward training models for languages other than English, at a scale similar to the latest English models (e.g. RoBERTa trained on more than 100GB of data). Our approach is based on RoBERTa BIBREF9, which replicates and improves the initial BERT",
        "type": "Document"
      },
      {
        "id": "fe471e06-da09-4982-aa8a-6a14463f900c",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1910.11493",
          "start_idx": 3314,
          "end_idx": 3804
        },
        "page_content": " that models be trained by fine-tuning. Joint modeling or any number of methods may be explored instead. The model will have access to type-level data in a low-resource target language, plus a high-resource source language. We give an example here of Asturian as the target language with Spanish as the source language.  We score the output of each system in terms of its predictions' exact-match accuracy and the average Levenshtein distance between the predictions and their corresponding",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What metric is used to measure performance?",
    "llm_answer": "Accuracy, precision, recall, F1-score, BIBREF16, ROUGE-LM, WPSLOR, ROUGE.\n",
    "context": [
      {
        "id": "6a5777b6-5e74-4edf-9018-a1ed47830e3f",
        "metadata": {
          "chunk_id": 69,
          "document_id": "2002.05058",
          "start_idx": 27517,
          "end_idx": 28041
        },
        "page_content": " evaluation is then employed to identify the best choice. We evaluate the performance of each metric by how many times (out of 10) they succeeded in selecting the best hyperparameter combination or early-stopping checkpoint (out of 4) and the average human-annotated score for their selected models. The results are shown in Table 3. We can see that conventional automated metrics perform poorly and result in sub-optimal result when performing hyperparameter search and selecting the best performing checkpoints. Converting",
        "type": "Document"
      },
      {
        "id": "1d812102-98f4-45bc-b355-c1653ccfaf7e",
        "metadata": {
          "chunk_id": 72,
          "document_id": "1809.08731",
          "start_idx": 24617,
          "end_idx": 25067
        },
        "page_content": " metrics on a benchmark dataset. Additionally, they can be applied even in settings where no references are available, or would be costly to obtain. Finally, for given references, we proposed the reference-based metric ROUGE-LM, which consists of a combination of WPSLOR and ROUGE. Thus, we were able to obtain an even more accurate fluency evaluation. We would like to thank Sebastian Ebert and Samuel Bowman for their detailed and helpful feedback.",
        "type": "Document"
      },
      {
        "id": "ca9dddcc-e3f4-4f29-8c33-fa0819f9fd44",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1912.01046",
          "start_idx": 14275,
          "end_idx": 14679
        },
        "page_content": " BIBREF16, which measures how far away the predicted span is from the gold standard span, as a metric. The rationale behind the metric is that, in practice, it suffices to recommend a rough span which contains the answer \u2013 a difference of a few seconds would not matter much to the user. Specifically, the predicted span is counted as correct if $|pred_{start} - gt_{start}| + |pred_{end} - gt_{end}| <=$",
        "type": "Document"
      },
      {
        "id": "0635fc14-2596-4869-9db7-f52d9463feb3",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1911.13066",
          "start_idx": 14008,
          "end_idx": 14543
        },
        "page_content": " the standard metrics that are widely adapted in the literature for measuring multi-class classification performance. These metrics are accuracy, precision, recall, and F1-score, where latter three can be computed using micro-average or macro-average strategies BIBREF15. In micro-average strategy, each instance holds equal weight and outcomes are aggregated across all classes to compute a particular metric. This essentially means that the outcome would be influenced by the frequent class, if class distribution is skewed. In macro",
        "type": "Document"
      },
      {
        "id": "a669a759-4988-4843-98f8-c12f54d6dcaa",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1906.03538",
          "start_idx": 17710,
          "end_idx": 18208
        },
        "page_content": "6 and INLINEFORM7 , respectively. The metrics are averaged across all the perspectives INLINEFORM8 participating in the test set. The goal is to get estimates of the overall performance of the systems. Instead of creating a complex measure that would take all the aspects into account, we approximate the overall performance by multiplying the disjoint measures in INLINEFORM0 , INLINEFORM1 and INLINEFORM2 . While this gives an estimate on the overall quality, it ignores the pipeline structure of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do Zipf and Herdan-Heap's laws differ?",
    "llm_answer": "The context states that Heaps-Herdan law and the degree distribution of the adjacency network are consequences of Zipf's law.  It further states that Zipf's law reflects only word frequency, not word order.  The provided text does not describe a difference between Zipf's law and Heaps-Herdan's law beyond the fact that the latter is a consequence of the former.\n",
    "context": [
      {
        "id": "b9b56dbc-a17b-43a3-8ba1-1154baafeefa",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1911.08915",
          "start_idx": 637,
          "end_idx": 1041
        },
        "page_content": " as linguistsBIBREF12, BIBREF13, BIBREF14. Indeed, a large amount of effort has been devoted to try to understand the origin of Zipf's law, in some cases arguing that it arises from the fact that texts carry information BIBREF15, all the way to arguing that it is the result of mere chance BIBREF16, BIBREF17. Another interesting characterization of texts is the Heaps-Herdan law, which describes how the",
        "type": "Document"
      },
      {
        "id": "990d651b-f7d3-4273-9678-6f542ffe589d",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1911.08915",
          "start_idx": 16743,
          "end_idx": 17225
        },
        "page_content": " accurately as it describes texts written in real languages. Further, Heaps-Herdan law and the degree distribution of the adjacency network, appear to be consequences of Zipf's law, and are, thus, as universal. In this work we studied 91 texts in seven different languages, as well as random texts constructed by randomizing the spacings between words without altering the order of the letters in the text. We find that they are all well described by the universal laws. However, we",
        "type": "Document"
      },
      {
        "id": "ec395906-23d0-4746-a10d-a46c279871f9",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1911.08915",
          "start_idx": 16228,
          "end_idx": 16743
        },
        "page_content": " Spanish and Turkish, similar for Spanish and French, and very different for all languages and random. Zipf's law is one of the most universal statistics of natural languages. However, it may be too universal. While it may not strictly apply to sequences of independent random symbols with random spacings BIBREF32, it appears to describe random texts that conserve most of the correlations between successive symbols, as accurately as it describes texts written in real languages. Further, Heaps-Herdan law and the",
        "type": "Document"
      },
      {
        "id": "ed4d0e0e-b463-48a2-bed4-855330f2b0fc",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1911.08915",
          "start_idx": 7331,
          "end_idx": 7689
        },
        "page_content": " plots for some of the texts, including the random texts constructed as described previously. It is clear that all the texts reproduce convincingly Zipf's law: $f(n)\\sim 1/n^\\alpha $ where $n=1,2,...N_{tot}$ is the word rank, $N_{tot}$ is the size of the vocabulary and $f(n)$ is its frequency. This is in contrast to previous work in which it is argued that",
        "type": "Document"
      },
      {
        "id": "b31dac37-b808-4ae2-917b-d9bf02baab93",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1911.08915",
          "start_idx": 10312,
          "end_idx": 10824
        },
        "page_content": " again in close agreement with what we observe. Thus far, our results confirm that the all our texts exhibit the expected universal statistics observed in natural languages. Actually, it could be argued that these laws may be \"too universal\", not being able to clearly distinguish texts written in real languages from our random texts. Further, all these laws appear to be consequence of Zipf's law, and this law reflects only the frequency of words, not their order. Thus, all three laws would still hold if the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are the synthetic examples generated?",
    "llm_answer": "The generative model is trained on the original training dataset.  Then, the premise and label from an example in the original dataset are taken as input to generate a new hypothesis.  The generated hypothesis is combined with the premise and the label to form a new unseen example.  This is done for every example in the original dataset.  Also, synthetic data is generated using simple rules such as dropping non-initial vowels and replacing consonants.\n",
    "context": [
      {
        "id": "7535771e-f179-48ba-9611-5aca6d4b2cb7",
        "metadata": {
          "chunk_id": 53,
          "document_id": "1909.13668",
          "start_idx": 16998,
          "end_idx": 17465
        },
        "page_content": " sentences are generated by sampling $z\\sim p(z)$ and decoding the samples into sentences. The synthetic sequences are then used to train a language model (an LSTM with the parametrisation of our decoder). The FCE score is estimated by reporting the negative log likelihood (NLL) of the trained LM on the set of human generated sentences. We generated synthetic corpora using trained models from Table TABREF12 with different C and decoding schemes and using the same",
        "type": "Document"
      },
      {
        "id": "6e67c754-c63d-4bc8-97ef-32f3f5df38d9",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1607.06025",
          "start_idx": 3868,
          "end_idx": 4370
        },
        "page_content": " of generative models, which is also presented in Figure FIGREF2 . First, the generative model is trained on the original training dataset. Then, the premise and label from an example in the original dataset are taken as the input to the generative model to generate a new random hypothesis. The generated hypothesis is combined with the premise and the label to form a new unseen example. This is done for every example in the original dataset to construct a new dataset. Next, a classifier is trained",
        "type": "Document"
      },
      {
        "id": "632bec8f-89e6-4aa9-930c-9586f9d4cec7",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1804.05868",
          "start_idx": 6795,
          "end_idx": 7281
        },
        "page_content": " (4,29,715) synthetically generated from the English vocabulary. We use simple rules such as dropping non-initial vowels and replacing consonants based on their phonological proximity to generate synthetic data for normalization. Figure \"Supplemental Material\" shows some of the noisy forms generated from standard word forms using simple and finite rules which include vowel elision (please $\\rightarrow $ pls), interchanging similar consonants and vowels (cousin $\\rightarrow $ couzin",
        "type": "Document"
      },
      {
        "id": "22778d18-9044-4a6d-a7cb-1794092ee9e6",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1607.06025",
          "start_idx": 1312,
          "end_idx": 1803
        },
        "page_content": " examples are presented in Table TABREF1 . The proposed generative networks are trained to generate a hypothesis given a premise and a label, which allow us to construct new, unseen examples. Some generative models are build to generate a single optimal response given the input. Such models have been applied to machine translation BIBREF5 , image caption generation BIBREF6 , or dialogue systems BIBREF7 . Another type of generative models are autoencoders that generate a stream of random",
        "type": "Document"
      },
      {
        "id": "6141d49a-0046-4b80-83de-234a4b1da876",
        "metadata": {
          "chunk_id": 29,
          "document_id": "2004.04696",
          "start_idx": 10320,
          "end_idx": 10666
        },
        "page_content": " synthetic data. The following section presents our 9 pre-training tasks, summarized in Table TABREF3. Additional implementation details are in the Appendix. We create three signals ${\\tau _{\\text{BLEU}}}$, ${\\tau _{\\text{ROUGE}}}$, and ${\\tau _{\\text{BERTscore}}}$ with sentence BLEU BIBREF13, ROUGE BIBREF14, and BERTscore BIBREF28 respectively",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does the new parser outperform the current state-of-the-art?",
    "llm_answer": "The context states the new parser is \"competitive with the state-of-the-art on PTB when considering the UAS metric,\" and \"gives better or comparable results\" to BIAF and STACKPTR, but does *not* specify by how much.\n",
    "context": [
      {
        "id": "65e384bd-e499-4ffc-bad6-21c1448c8d71",
        "metadata": {
          "chunk_id": 67,
          "document_id": "1911.08673",
          "start_idx": 23641,
          "end_idx": 24173
        },
        "page_content": "building actions, and it also supports non-projective parsing when introducing two extra non-projective arc-building actions. Compared to graph-based and transition-based parsers, our parser achieves a better tradeoff between parsing accuracy and efficiency by taking advantages of both graph-based models' training methods and transition-based models' linear time decoding strategies. Experimental results on 28 treebanks show the effectiveness of our parser by achieving good performance on 27 treebanks, including the PTB and CTB",
        "type": "Document"
      },
      {
        "id": "d3f59df0-c5cd-4c5a-9866-2534ef515406",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1911.08673",
          "start_idx": 3681,
          "end_idx": 4273
        },
        "page_content": " From the evaluation results on the benchmark treebanks, our proposed model gives significant improvements when compared to the baseline parser. In summary, our contributions are thus: $\\bullet $ We integrate the arc scoring mechanism of graph-based parsers and the linear time complexity inference approach of transition parsing models, which, by replacing stepwise local feature scoring, significantly alleviates the drawbacks of these models, improving their moderate performance caused by error propagation and increasing their training speeds resulting from their lack of parallelism. $\\",
        "type": "Document"
      },
      {
        "id": "a05980a9-94ad-431e-b1a5-bdae11896724",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1911.08673",
          "start_idx": 20643,
          "end_idx": 21073
        },
        "page_content": "based parsers, our model is also competitive with the state-of-the-art on PTB when considering the UAS metric. Compared to state-of-the-art parsers in transition and graph types, BIAF and STACKPTR, respectively, our model gives better or comparable results but with much faster training and decoding. Additionally, with the help of pre-trained language models, ELMo or BERT, our model can achieve even greater results. In order to",
        "type": "Document"
      },
      {
        "id": "49142455-cff0-4f6a-8a9a-fd78cb8bbdb6",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1710.09340",
          "start_idx": 10044,
          "end_idx": 10515
        },
        "page_content": " a comparison between the Covington parser and the novel variant developed here. The NL-Covington parser outperforms the original version in all datasets tested, with all improvements statistically significant ( INLINEFORM0 ). Table TABREF12 compares our novel system with other state-of-the-art transition-based dependency parsers on the PT-SD. Greedy parsers are in the first block, beam-search and dynamic programming parsers in the second block. The third block shows",
        "type": "Document"
      },
      {
        "id": "a8782194-ae44-4cad-8916-604ae2b4bd8f",
        "metadata": {
          "chunk_id": 68,
          "document_id": "1911.08673",
          "start_idx": 23969,
          "end_idx": 24082
        },
        "page_content": " effectiveness of our parser by achieving good performance on 27 treebanks, including the PTB and CTB benchmarks.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What experimental evaluation is used?",
    "llm_answer": "Cross-domain evaluation, human evaluation (relevance, attraction, fluency, style strength), qualitative and quantitative assessment (using ISO 9241 and other techniques).\n",
    "context": [
      {
        "id": "1f140aea-67fe-42b4-983e-2e4e083c362a",
        "metadata": {
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "5155558a-dfb3-4b8b-9585-41ac83603580",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1601.02543",
          "start_idx": 845,
          "end_idx": 1419
        },
        "page_content": " successful and unsuccessful transactions. This evaluation is then used to identifying any further improvement in the speech recognition based solution to better the overall transaction completion rates. This process of evaluation is both time consuming and expensive. For evaluation one needs to identify a set of users and also identify the set of actual usage situations and perform the test. It is also important that the set of users are able to use the system with ease meaning that even in the test conditions the performance of the system, should be good, while this",
        "type": "Document"
      },
      {
        "id": "7ee3d1d0-5ed5-48f9-a7bc-3e689b448d34",
        "metadata": {
          "chunk_id": 275,
          "document_id": "1804.08186",
          "start_idx": 98870,
          "end_idx": 99381
        },
        "page_content": " evaluation in research to date. In this section, we examine the literature more closely, providing a broad overview of the evaluation metrics that have been used, as well as the experimental settings in which research has been evaluated. The most common approach is to treat the task as a document-level classification problem. Given a set of evaluation documents, each having a known correct label from a closed set of labels (often referred to as the \u201cgold-standard\u201d), and a predicted label for each document",
        "type": "Document"
      },
      {
        "id": "727bd5dd-0dd4-469a-85d1-e9618fc62c3a",
        "metadata": {
          "chunk_id": 55,
          "document_id": "2004.01980",
          "start_idx": 18999,
          "end_idx": 19503
        },
        "page_content": " style corpus, and the sampling strategy follows the uniform distribution with the probability being equal to $\\lambda $. The human evaluation is to have a comprehensive measurement of the performances. We conduct experiments on four criteria, relevance, attraction, fluency, and style strength. We summarize the human evaluation results on the first three criteria in Table TABREF51, and the last criteria in Table TABREF57. Note that through automatic evaluation, the baselines NST, Fine-tuned, and Gig",
        "type": "Document"
      },
      {
        "id": "0030d502-72b9-4749-b878-c54ba95f8eec",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1906.09774",
          "start_idx": 20313,
          "end_idx": 20820
        },
        "page_content": " and found several resources from the old one such as LIWC and Emolex to the new one, including DepecheMood and EmoWordNet. In the final part, we gather information about how to evaluate the performance of EAC, and we can classify the approach into two techniques, including qualitative and quantitative assessment. For qualitative assessment, most studies used ISO 9241, which covers several aspects such as efficiency, effectiveness, and satisfaction. While in quantitative analysis, two techniques can be",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the architecture fault-tolerant?",
    "llm_answer": "The architecture uses Apache Spark, which acts as an in-memory data store and allows for fault tolerant processing of large volumes of data, and is persistent in real time.\n",
    "context": [
      {
        "id": "3aea14a4-657f-4bf2-8836-c47475ae5824",
        "metadata": {
          "chunk_id": 16,
          "document_id": "2001.02380",
          "start_idx": 6369,
          "end_idx": 6911
        },
        "page_content": ", we combine the anchoring annotation data from Section SECREF3 with the model's predictions to evaluate how `human-like' its performance is, using an information retrieval approach measuring recall@k and assessing the stability of different signal types based on how the model scores them. We develop a visualization for tokenwise signaling strength and perform error analysis for some signals found by the model which were not flagged by humans and vice versa, and point out the strengths and weaknesses of the architecture. Section SECREF6",
        "type": "Document"
      },
      {
        "id": "0a523fb5-d1ac-4f2c-884a-f2329f3bb64e",
        "metadata": {
          "chunk_id": 27,
          "document_id": "2003.04967",
          "start_idx": 11675,
          "end_idx": 12186
        },
        "page_content": " scalable, being 100x faster than Hadoop on large datasets, and provides out of the box libraries for both streaming and machine learning. The growth of the volume of data inspired us to opt for a big data architecture which can not only handle the prediction algorithms but also the streaming and increasing volume of data in a fault tolerant way. Figure FIGREF2 gives an overview of the architecture design. Central to this design is Apache Spark which acts as an in-memory data store and allows us to perform",
        "type": "Document"
      },
      {
        "id": "481e6fb1-761d-4d7d-b1e4-0740e24d8262",
        "metadata": {
          "chunk_id": 5,
          "document_id": "2003.04967",
          "start_idx": 2255,
          "end_idx": 2850
        },
        "page_content": " learning algorithms, which introduces an additional difficulty. These challenges call for learning platforms based on big data architectures that can not only handle heterogeneous volumes of data but also be fault tolerant and persistent in real time. In this paper we provide a novel real-time and adaptive cryptocurrency price prediction platform based on Twitter sentiments. The integrative and modular platform copes with the three aforementioned challenges in several ways. Firstly, it provides a Spark-based architecture which handles the large volume of incoming data in a persistent and",
        "type": "Document"
      },
      {
        "id": "5b38b1b2-72c1-4bfa-b023-9f8d5f20d97b",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1808.09409",
          "start_idx": 17134,
          "end_idx": 17611
        },
        "page_content": "-based one. Finally, a concrete study on the instances in the output is conducted, as to validate conclusions in the previous step. We employ 6 oracle transformations designed by he2017deep to fix various prediction errors sequentially (see details in Table TABREF19 ), and observe the relative improvements after each operation, as to obtain fine-grained error types. Figure FIGREF21 compares two systems in terms of different mistakes on ENG-L2 and JPN-L2 respectively. After",
        "type": "Document"
      },
      {
        "id": "b82913f7-578f-4442-a8d8-39b764908802",
        "metadata": {
          "chunk_id": 72,
          "document_id": "1808.09920",
          "start_idx": 24285,
          "end_idx": 24813
        },
        "page_content": " candidate answers or the number of nodes increases. In this section we provide an error analysis for our best single model predictions. First of all, we look at which type of questions our model performs well or poorly. There are more than 150 query types in the validation set but we filtered the three with the best and with the worst accuracy that have at least 50 supporting documents and at least 5 candidates. We show results in Table 4 . We observe that questions regarding places (birth and death) are considered harder",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which elements of the platform are modular?",
    "llm_answer": "The question answering platform, a bitcoin price prediction platform, and a modular architecture for a spoken dialog system are all described as modular.\n",
    "context": [
      {
        "id": "1cfaa2d5-e6a9-454a-b7c2-47234007a3b9",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1907.00854",
          "start_idx": 14715,
          "end_idx": 15274
        },
        "page_content": " portable because it is built on cloud native technologies (i.e., Docker and Kubernetes) and can be deployed to any cloud or on-premise environment. It is modular because it is composed of four configurable modules that collectively enable identification of questions, classification of those questions into topics, a search of knowledge base articles, and reading comprehension. Initial usage of the system indicates that it provides a flexible and developer friendly way to enable question answering functionality for multiple topics or domains via REST API",
        "type": "Document"
      },
      {
        "id": "dd66b681-e8de-4761-bfdf-3f4a713e84ad",
        "metadata": {
          "chunk_id": 7,
          "document_id": "2003.04967",
          "start_idx": 3154,
          "end_idx": 3699
        },
        "page_content": " cope with new prices and sentiments. Finally, the platform is modular and integrative in the sense that it combines these different solutions to provide novel real-time tool support for bitcoin price prediction that is more scalable, data-rich, and proactive, and can help accelerate decision-making, uncover new opportunities and provide more timely insights based on the available and ever-larger financial data volume and variety. The rest of the paper is organized as follows. Section 2 discusses the related work proposed in the literature",
        "type": "Document"
      },
      {
        "id": "b97eaf7c-ed2f-4466-a9d0-d5486f5d028e",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1707.05853",
          "start_idx": 719,
          "end_idx": 1211
        },
        "page_content": " a modular architecture: First, an ASR system converts the spoken user utterance into text. Then, a spoken language understanding (SLU) module extracts the user's intent and coarse-grained semantic information. Next, a dialog state tracking (DST) component maintains a distribution over the state of the dialog, updating it in every turn. Given this information, the dialog policy manager decides on the next action of the system. Finally, a natural language generation (NLG) module forms the",
        "type": "Document"
      },
      {
        "id": "6bd45ba5-5818-4bf7-9706-f82d1b94eeae",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1603.04553",
          "start_idx": 11513,
          "end_idx": 11902
        },
        "page_content": "IBREF28 ; Latent-Tree: the latent tree model BIBREF29 obtaining the best results in the shared task; Berkeley: the Berkeley system with the final feature set BIBREF12 ; LaSO: the structured perceptron system with non-local features BIBREF30 ; Latent-Strc: the latent structure system BIBREF31 ; Model-Stack: the entity-centric system with model stacking BIBREF32 ; and Non-Linear: the non-",
        "type": "Document"
      },
      {
        "id": "393a685b-45cf-4c73-af70-86d31dcf22a6",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1911.11744",
          "start_idx": 13205,
          "end_idx": 13620
        },
        "page_content": " stochastic network outputs to get a distribution over the belief. The modularity of our architecture allows us to easily exchange parts of the network. This can be utilized for transfer learning between different tasks in the semantic network or transfer between different robots by transferring the policy translation network to different robots in simulation, or to bridge the gap between simulation and reality.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Could you tell me more about the metrics used for performance evaluation?",
    "llm_answer": "The context describes various evaluation metrics, including:\n\n* **Hyperparameter search and early stopping:** Success rate in selecting the best combination (out of 10 runs) and average human-annotated score for selected models.\n* **NLG evaluation:** Recall rate, F1 score, and overall accuracy, averaged across perspectives.  Overall performance is approximated by multiplying disjoint measures.\n* **Task 2:** Task completion ratio, user satisfaction degree (scores from -2 to 2), response fluency (scores from -1 to 1), and number of dialogue turns.\n",
    "context": [
      {
        "id": "6a5777b6-5e74-4edf-9018-a1ed47830e3f",
        "metadata": {
          "chunk_id": 69,
          "document_id": "2002.05058",
          "start_idx": 27517,
          "end_idx": 28041
        },
        "page_content": " evaluation is then employed to identify the best choice. We evaluate the performance of each metric by how many times (out of 10) they succeeded in selecting the best hyperparameter combination or early-stopping checkpoint (out of 4) and the average human-annotated score for their selected models. The results are shown in Table 3. We can see that conventional automated metrics perform poorly and result in sub-optimal result when performing hyperparameter search and selecting the best performing checkpoints. Converting",
        "type": "Document"
      },
      {
        "id": "e12d7235-2654-4688-a6b5-218fe9724fb5",
        "metadata": {
          "chunk_id": 68,
          "document_id": "2002.05058",
          "start_idx": 26990,
          "end_idx": 27517
        },
        "page_content": " NLG evaluation. We further investigate the impact of imperfect metrics on training NLG models. As described in the human evaluation procedure, we perform 10 runs to test the reliability of each metric when used to perform hyperparameter tuning and early-stopping respectively. In each run, we select the best hyperparameter combination or early-stopping checkpoint based on each of the five compared metrics. Human evaluation is then employed to identify the best choice. We evaluate the performance of each metric by how many",
        "type": "Document"
      },
      {
        "id": "a669a759-4988-4843-98f8-c12f54d6dcaa",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1906.03538",
          "start_idx": 17710,
          "end_idx": 18208
        },
        "page_content": "6 and INLINEFORM7 , respectively. The metrics are averaged across all the perspectives INLINEFORM8 participating in the test set. The goal is to get estimates of the overall performance of the systems. Instead of creating a complex measure that would take all the aspects into account, we approximate the overall performance by multiplying the disjoint measures in INLINEFORM0 , INLINEFORM1 and INLINEFORM2 . While this gives an estimate on the overall quality, it ignores the pipeline structure of",
        "type": "Document"
      },
      {
        "id": "96ecbdb9-17ee-46e0-9f1c-9eef560fb526",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1912.13109",
          "start_idx": 10203,
          "end_idx": 10681
        },
        "page_content": " metrics - Recall rate, F1 score and Overall accuracy. Choice of model parameters were in the above models were inspired from previous work done but then were tuned to the best performance of the Test dataset. Following parameters were considered for tuning. Learning rate: Based on grid search the best performance was achieved when learning rate was set to 0.01. This value was arrived by a grid search on lr parameter. Number of Bidirectional LSTM units: A set of 32, 64, 128",
        "type": "Document"
      },
      {
        "id": "6b1e72f3-8ee5-43ac-a062-f08c158e3d67",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1709.10217",
          "start_idx": 6394,
          "end_idx": 6850
        },
        "page_content": " evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the number of total tasks. User satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied, neutral, satisfied and very satisfied, respectively. Response fluency: There are three scores -1, 0, 1, which indicate nonfluency, neutral, fluency. Number of dialogue turns: The number of utterances in a task",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the tasks that this method has shown improvements?",
    "llm_answer": "The context states that the method shows improvement in WER (word error rate) on Hub5'00.  It also mentions improvements when supervised data is reduced by half.\n",
    "context": [
      {
        "id": "1f140aea-67fe-42b4-983e-2e4e083c362a",
        "metadata": {
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "5f60f35c-49af-411d-9dd5-a573069248ea",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1707.03764",
          "start_idx": 6168,
          "end_idx": 6712
        },
        "page_content": " out to be detrimental to performance, when compared to the simpler system that we have described in Section SECREF2 . We divide our attempts according to the different ways we attempted to enhance performance: manipulating the data itself (adding more, and changing preprocessing), using a large variety of features, and changing strategies in modelling the problem by using different algorithms and paradigms. All reported results are on the PAN 2017 training data using five-fold cross-validation, unless otherwise specified. We extended the",
        "type": "Document"
      },
      {
        "id": "b066e5d0-1a4b-4826-a974-1dc9fddc6d6f",
        "metadata": {
          "chunk_id": 35,
          "document_id": "2001.07263",
          "start_idx": 13259,
          "end_idx": 13753
        },
        "page_content": " Sorting the results by decreasing number of absolute errors on Hub5'00, Table TABREF7 indicates that each regularization method contributes to the improved WER. SpecAugment is by far the most important method, while using $\\Delta $ and $\\Delta \\Delta $ features or switching off the curriculum learning in the later stage of training have marginal but positive effects. Other direct input level perturbation steps (speed/tempo perturbation and sequence noise injection) are also key techniques",
        "type": "Document"
      },
      {
        "id": "1c5d5d84-1141-4e97-92d9-8992d472df42",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1701.09123",
          "start_idx": 5380,
          "end_idx": 5887
        },
        "page_content": " a clear methodology for choosing and combining them. Third, we demonstrate that our system still obtains very competitive results even when the supervised data is reduced by half (even less in some cases), alleviating the dependency on costly hand annotated data. These three main contributions are based on: A simple and shallow robust set of features across languages and datasets, even in out-of-domain evaluations. The lack of linguistic motivated features, even for languages with agglutinative (e.g.,",
        "type": "Document"
      },
      {
        "id": "15069f29-5992-4f8d-8d8e-e8965bd22fab",
        "metadata": {
          "chunk_id": 74,
          "document_id": "1912.09152",
          "start_idx": 26186,
          "end_idx": 26669
        },
        "page_content": " system performance. With these methods, system development can go hand in hand with dataset refinement in a virtuous circle that let us think that maybe next time we are planning to add a new gazetteer or word embedding to our system in order to try to improve system performance, we should first look at our data and, like King Midas, turn our Human Annotated Dataset into a true Gold Standard Dataset. We thank three anonymous reviewers of our manuscript for their careful reading",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Why does the model improve in monolingual spaces as well? ",
    "llm_answer": "The context states that multilingual models, when trained on a combination of languages, improve recognition performance in languages with limited data.  This, in turn, benefits monolingual performance as well.\n",
    "context": [
      {
        "id": "95d984cf-c801-4361-b0f5-6dbd06e18d40",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1810.03459",
          "start_idx": 6376,
          "end_idx": 6908
        },
        "page_content": "ization across languages. Languages with limited data when trained with other languages allows them to be robust and helps in improving the recognition performance. In spite of being simple, the model has limitations in keeping the target language data unseen during training. Table TABREF16 shows the recognition performance of naive multilingual approach using BLSTMP and VGG model against a monolingual model trained with BLSTMP. The results clearly indicate that having a better architecture such as VGG-BLSTM helps in improving",
        "type": "Document"
      },
      {
        "id": "a3c864eb-a3b9-4d5d-ae7a-3a2ff62768e1",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1710.09589",
          "start_idx": 11047,
          "end_idx": 11545
        },
        "page_content": " multilingual model shows promising, overall (macro average) outperforming the single language-specific models. However, the multilingual model does not consistently fare better than single models, for example on French a monolingual model would be more beneficial. Adding POS tags did not help (cf. Table TABREF31 ), actually dropped performance. We disregard this feature for the final official runs. We trained the final models on the concatenation of Train and Dev data. The results on the test",
        "type": "Document"
      },
      {
        "id": "bc6f78ff-c84b-423e-9147-2b80024fd66b",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1710.09589",
          "start_idx": 11811,
          "end_idx": 12338
        },
        "page_content": " models on the two languages (Spanish and Japanese) which have the least amount of training data. Overall results between the monolingual and multilingual model are close, but the advantage of our multilingual All-in-1 approach is that it is a single model that can be applied to all four languages. Second, automatic translation harms, the performance of the EN model on the translated data is substantially lower than the respective in-language model. We could investigate this as the organizers provided us with translations",
        "type": "Document"
      },
      {
        "id": "f4db6e36-782f-4e4b-a217-1af7e72a37de",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1909.06522",
          "start_idx": 18220,
          "end_idx": 18769
        },
        "page_content": " performance, but also serves as an alternative to the standard production deployment that typically includes extensive monolingual ASR systems and a separate language ID model. Future work will expand the language coverage to include both geographically proximal and distant languages. Additionally, given the identity of a target test language, we will consider the hidden layers of such multilingual acoustic model as a pre-trained model, and thus perform subsequent monolingual fine-tuning, as compared to the multitask learning procedure in BIB",
        "type": "Document"
      },
      {
        "id": "2d57954a-13f5-43c4-8d73-06f77deadb75",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1710.09589",
          "start_idx": 10557,
          "end_idx": 11047
        },
        "page_content": " performance of the character+embedding model. However, the bilingual models are still only able to handle pairs of languages. We therefore mapped all embeddings to a common space and train a single multilingual All-in-1 model on the union of all training data. This is the second model that we submitted to the shared task. As we can see from the development data, on average the multilingual model shows promising, overall (macro average) outperforming the single language-specific models",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is annotation projection done when languages have different word order?",
    "llm_answer": "The context states that the approach projects target-language word embeddings to the source-language embedding space preserving word order.  It is expected to work better for languages with similar word order to the source language (English).\n",
    "context": [
      {
        "id": "e72a717a-7cc7-4ecf-a6e2-86a27126da6c",
        "metadata": {
          "chunk_id": 76,
          "document_id": "1911.00069",
          "start_idx": 24530,
          "end_idx": 24998
        },
        "page_content": ".9 $F_1$ points, except for Arabic. Since our approach projects the target-language word embeddings to the source-language embedding space preserving the word order, it is expected to work better for a target language that has more similar word order as the source language. This has been verified by our experiments. The source language, English, belongs to the SVO (Subject, Verb, Object) language family where in a sentence the subject comes first, the verb second,",
        "type": "Document"
      },
      {
        "id": "2b64acd1-2c30-4a02-b1d4-b0b8002af4e1",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1704.04539",
          "start_idx": 1353,
          "end_idx": 1809
        },
        "page_content": " annotation projection, where existing annotations are projected from a source language (English) to a target language through a parallel corpus BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 . By evaluating the parsers and manually analyzing their output, we show that the parsers are able to recover the AMR structures even when there exist structural differences between the languages, i.e., although AMR is not an interlingua it can act as one. This method also",
        "type": "Document"
      },
      {
        "id": "a6ad2d34-e931-4a10-93c9-afb341405728",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1704.04539",
          "start_idx": 13333,
          "end_idx": 13834
        },
        "page_content": " noisy word alignments in annotation projection: the parser learns what words are likely to trigger a node (or a set of nodes) in the AMR by looking at their AMR alignments (which are induced by the word alignments). If an important word consistently remains unaligned, the parser will erroneously learn to discard it. More accurate alignments are therefore crucial in order to achieve better parsing results. We computed the percentage of words in the training data that are learned to be non-content",
        "type": "Document"
      },
      {
        "id": "51ef2901-5d11-4c60-b3d3-cc17456e7748",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1704.04539",
          "start_idx": 808,
          "end_idx": 1353
        },
        "page_content": " that refined annotation guidelines suffice to resolve some of these cases. We extend this line of research by exploring whether divergences among languages can be overcome, i.e., we investigate whether it is possible to maintain the AMR annotated for English as a semantic representation for sentences written in other languages, as in Figure 1 . We implement AMR parsers for Italian, Spanish, German and Chinese using annotation projection, where existing annotations are projected from a source language (English) to a target language through",
        "type": "Document"
      },
      {
        "id": "22e9e67c-c7a7-40b4-9745-58cd662f5205",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1704.04539",
          "start_idx": 18855,
          "end_idx": 19311
        },
        "page_content": " make use of literal translation. Previous work has also focused on assessing the stability across languages of semantic frameworks such as AMR BIBREF7 , BIBREF6 , UCCA BIBREF27 and Propbank BIBREF28 . Cross-lingual techniques can cope with the lack of labeled data on languages when this data is available in at least one language, usually English. The annotation projection method, which we follow in this work, is one way to address this problem. It was",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What's the precision of the system?",
    "llm_answer": "The provided context shows precision scores ranging from 0.92 to 0.97, and an average precision of 88% (and 91% estimated).  No single \"system\" precision is definitively stated.\n",
    "context": [
      {
        "id": "a5ccc168-6ed7-4b5a-9a9b-1aaaf5f12202",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1909.05246",
          "start_idx": 16417,
          "end_idx": 16962
        },
        "page_content": " accuracy measures the similarity of the system generated response versus the target response. Eric and Manning eric2017copy used this metric to evaluate their systems in which they considered their response to be correct if all tokens in the system generated response matched the corresponding token in the target response. This metric is a little bit harsh, and the results may be low since all the tokens in the generated response have to be exactly in the same position as in the target response. Per-Dialogue Accuracy: We calculate per-dial",
        "type": "Document"
      },
      {
        "id": "11f28d99-74a1-4662-a7a7-6e48494eae1d",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1802.05574",
          "start_idx": 13420,
          "end_idx": 13921
        },
        "page_content": " if we have 100% inter-annotator agreement that a triple was correctly extracted we label it as correct. Table 2 show the results for the combinations of systems and data sources. The Correct Triples column contains the number of triples that are labelled as being correct by all annotators. Total Triples are the total number of triples extracted by the given systems over the specified data. Precision is calculated as typical where Correct Triples are treated as true positives. On average, 3.1 tri",
        "type": "Document"
      },
      {
        "id": "7b9b8e01-be94-4e94-a444-d4d18a476d6b",
        "metadata": {
          "chunk_id": 81,
          "document_id": "1804.03396",
          "start_idx": 25766,
          "end_idx": 26030
        },
        "page_content": " to calculate the precision and recall under the threshold $\\delta $ . Specifically, we try two confidence scores calculated as follows: $$\\begin{split}\n{\\rm Score_{mul}} = \\prod _{t=1}^L{\\rm P}(a^t_{i_t}),~~~{\\rm Score_{avg}} = \\sum _{t=1}^L{\\rm P}(a^t_{i_t}) / L",
        "type": "Document"
      },
      {
        "id": "679e5943-aff3-4667-b5b6-229a58410174",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1911.03243",
          "start_idx": 10852,
          "end_idx": 11313
        },
        "page_content": "Bank. To examine this, we analyzed 100 false positive arguments. Only 32 of those were due to wrong or incomplete QA annotations in our gold, while most others were outside of PropBank's scope, capturing either implied arguments or roles not covered in PropBank. Extrapolating from this manual analysis estimates our true precision (on all roles) to be about 91%, which is consistent with the 88% precision figure in Table TABREF19. Compared with 2015, our QA-S",
        "type": "Document"
      },
      {
        "id": "ad156fd3-1eaf-4e70-8f56-1f3024106d99",
        "metadata": {
          "chunk_id": 58,
          "document_id": "2001.11268",
          "start_idx": 22809,
          "end_idx": 23352
        },
        "page_content": " precision scores in ranges between 0.92 and 0.97 are possible with F1 scores not dropping below 0.84 for the main classes of interest. In practice, the detachment between model predictions and assignment of labels means that a reviewer who wishes to switch between high recall and high precision results can do so very quickly, without obtaining new predictions from the model itself. More visualizations can be found in this project's GitHub repository , including true class labels and a detailed breakdown of true and false predictions for",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which of the two ensembles yields the best performance?",
    "llm_answer": "Mean probability ensemble.\n",
    "context": [
      {
        "id": "4e04ab32-f047-4b22-8b03-4e6977b298f9",
        "metadata": {
          "chunk_id": 60,
          "document_id": "1901.02257",
          "start_idx": 18849,
          "end_idx": 19405
        },
        "page_content": " Our ensemble model surpasses the current state-of-the-art model with an accuracy of 84.84%. We got the final ensemble result by voting on 4 single models. Every single model uses the same architecture but different parameters. To study the effectiveness of each perspective, we conduct several experiments on the three single perspectives and their combination perspective. Table 3 presents their comparison results. The first group of models are based on the three single perspectives, and we can observe that the union perspective performs best compared",
        "type": "Document"
      },
      {
        "id": "0c4f0564-6497-4c6c-8f58-0f8e3d8afc77",
        "metadata": {
          "chunk_id": 65,
          "document_id": "1905.07464",
          "start_idx": 24260,
          "end_idx": 24836
        },
        "page_content": " available training data. While precision was similar between the three systems (with exceptions), we observed that our ensemble-based system benefited mostly from improved recall. This aligns with our initial expectation (based on prior experience with deep learning models) that an ensemble-based approach would improve stability and accuracy with deep neural models. Although including NLM-180 as training data resulted in significant performance gains during 11-fold cross validation, we find that the same improvements were not as dramatic on either test sets despite the",
        "type": "Document"
      },
      {
        "id": "727dee20-3e7f-455a-9cc4-9e131e6c8aa2",
        "metadata": {
          "chunk_id": 60,
          "document_id": "2001.06354",
          "start_idx": 20528,
          "end_idx": 20984
        },
        "page_content": ". As also shown in Table TABREF46, the ensemble model seems to take the best results from each model. Specifically, the NDCG score of the ensemble model is comparable to that of the image-only model and the scores of other metrics are comparable to those of the image-history joint model. From this experiment, we can confirm that the two models are in complementary relation. For the evaluation on the test-standard dataset of VisDial v1.0, we try 6 image",
        "type": "Document"
      },
      {
        "id": "54fcaba0-378f-43df-8f35-44366f26216e",
        "metadata": {
          "chunk_id": 259,
          "document_id": "1804.08186",
          "start_idx": 92868,
          "end_idx": 93327
        },
        "page_content": "-based classifiers with different features at the 2017 DSL shared task. A voting ensemble gave better results on the closed track than a singular word-based perplexity classifier (0.9025 weighted F1-score over 0.9013), but worse results on the open track (0.9016 with ensemble and 0.9065 without). In a highest probability ensemble, the winner is simply the language which is given the highest probability by any of the individual classifiers in the ensemble.",
        "type": "Document"
      },
      {
        "id": "9419365e-1680-4096-a036-01e115f92d0d",
        "metadata": {
          "chunk_id": 263,
          "document_id": "1804.08186",
          "start_idx": 94355,
          "end_idx": 94863
        },
        "page_content": " several ensemble methods and found that the mean probability ensemble attained better results than plurality voting, median probability, product, highest confidence, or Borda count ensembles. In a median probability ensemble, the medians over the probabilities given by the individual classifiers are calculated for each language. BIBREF270 and BIBREF171 used a median probability rule ensemble over SVM classifiers. Consistent with the results of BIBREF270 , BIBREF171 found that a mean ensemble was better",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the new initialization method proposed in this paper?",
    "llm_answer": "Multichannel initialization.\n",
    "context": [
      {
        "id": "9782c633-1b03-4298-9ea0-65c93f8bcd0c",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1603.04513",
          "start_idx": 20319,
          "end_idx": 20747
        },
        "page_content": " the \u201cNCE\u201d layer (\u201con\u201d in the figure). In this work, we use multichannel initialization for case (i) and random initialization for cases (ii) and (iii). Only fine-tuned multichannel representations (case (i)) are kept for subsequent supervised training. The rationale for this pretraining is similar to auto-encoder: for an object composed of smaller-granular elements, the representations of the whole object and its components",
        "type": "Document"
      },
      {
        "id": "b94ae015-76a4-4168-8589-84e57c3ef069",
        "metadata": {
          "chunk_id": 67,
          "document_id": "2002.10832",
          "start_idx": 22843,
          "end_idx": 23334
        },
        "page_content": ". Again, we additionally report the results obtained bypassing the intermediate steps 1 and 2: for the model denoted as Step 3 (from scratch) (last row of Table ), $\\Theta $ parameters are initialized with the original weights from pre-trained BERT, while the $W$ matrix is randomly initialized. Under this experimental condition, we observe lower performances, a finding that consolidates the importance of the multi-step training procedure we adopted. In Table , we report quantitative VQG",
        "type": "Document"
      },
      {
        "id": "af4926c8-63fe-4683-ac63-0a297f06ac5f",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1910.10288",
          "start_idx": 4037,
          "end_idx": 4421
        },
        "page_content": "y}_i$, from the decoder RNN state, $\\mathbf {d}_i$. An early sequence-to-sequence attention mechanism was proposed by Graves in BIBREF0. This approach is a purely location-based mechanism that uses an unnormalized mixture of $K$ Gaussians to produce the attention weights, $\\mathbf {\\alpha }_i$, for each encoder state. The general form of this type of attention is shown in (DISPLAY_",
        "type": "Document"
      },
      {
        "id": "9ea2a036-ab17-4fe3-8490-423b403d521f",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1910.11491",
          "start_idx": 7263,
          "end_idx": 7654
        },
        "page_content": " weight at $i$-th position. The model is firstly pre-trained to minimize the maximum-likelihood loss, which is widely used in sequence generation tasks. We define $y^* = \\lbrace y^*_1, \\cdots , y_T^*\\rbrace $ as the ground-truth output sequence for a given input sequence $x$, then the loss function is formulated as: After converging, the model is further optimized with local variance loss",
        "type": "Document"
      },
      {
        "id": "4c50ea6b-6324-4c7a-bebc-3a230d71347e",
        "metadata": {
          "chunk_id": 39,
          "document_id": "2004.01878",
          "start_idx": 14189,
          "end_idx": 14714
        },
        "page_content": " As the first set of development experiments, we try different ways to initialize the noisy recurrent states of our ANRES model to find a suitable approach. For each trading day, we compare the results whether states transitions are modeled or not. Besides, we also compare the methods of random initialization and zero initialization. Note that the random initialization method we use here returns a tensor filled with random numbers from the standard normal distribution $\\mathcal {N}(0, 1)$. In summary, the following four",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How was a quality control performed so that the text is noisy but the annotations are accurate?",
    "llm_answer": "A small sample of texts was annotated by all annotators, and inter-annotator agreement was measured.  For each annotator and question, the probability of agreement with the majority response was computed; annotators with probabilities more than two standard deviations from the mean were discarded.\n",
    "context": [
      {
        "id": "7ad84a74-dfae-4c36-99a3-0c3aa9c1ab59",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1703.05260",
          "start_idx": 17551,
          "end_idx": 18087
        },
        "page_content": " results were discussed with the authors. Throughout the whole annotation phase, annotators could discuss any emerging issues with the authors. All annotations were done by undergraduate students of computational linguistics. The annotation was rather time-consuming due to the complexity of the task, and thus we decided for single annotation mode. To assess annotation quality, a small sample of texts was annotated by all four annotators and their inter-annotator agreement was measured (see Section \"Inter-Annotator Agreement\" ). It",
        "type": "Document"
      },
      {
        "id": "7476ba72-3284-4645-8bfc-80654aedd342",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1909.00124",
          "start_idx": 1276,
          "end_idx": 1646
        },
        "page_content": ", the crowdsourced annotations can be quite noisy. Research on learning with noisy labels dates back to 1980s BIBREF4. It is still vibrant today BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11, BIBREF12 as it is highly challenging. We will discuss the related work in the next section. This paper studies the problem of learning with noisy labels for SSC",
        "type": "Document"
      },
      {
        "id": "a6ad2d34-e931-4a10-93c9-afb341405728",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1704.04539",
          "start_idx": 13333,
          "end_idx": 13834
        },
        "page_content": " noisy word alignments in annotation projection: the parser learns what words are likely to trigger a node (or a set of nodes) in the AMR by looking at their AMR alignments (which are induced by the word alignments). If an important word consistently remains unaligned, the parser will erroneously learn to discard it. More accurate alignments are therefore crucial in order to achieve better parsing results. We computed the percentage of words in the training data that are learned to be non-content",
        "type": "Document"
      },
      {
        "id": "5bbae2e3-9d03-4140-ac8e-cce537b1d4e1",
        "metadata": {
          "chunk_id": 34,
          "document_id": "2001.08051",
          "start_idx": 13177,
          "end_idx": 13680
        },
        "page_content": " the annotation, which were used by 5 researchers to manually transcribe about 20 minutes of audio data. This experience led to a discussion, from which a second set of guidelines originated, aiming at reaching a reasonable trade-off between transcription accuracy and speed. As a consequence, we decided to apply the following transcription rules: only the main speaker has to be transcribed; presence of other voices (schoolmates, teacher) should be reported only with the label \u201c@voices\u201d, presence of",
        "type": "Document"
      },
      {
        "id": "c20ead49-475b-4689-9e00-8f87a036a667",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1912.03184",
          "start_idx": 17696,
          "end_idx": 18237
        },
        "page_content": " annotations that were done in less than 10 seconds and Phase 2 annotations that were done in less than 70 seconds. After we collected all annotations, we found unreliable annotators for both phases in the following way: for each annotator and for each question, we compute the probability with which the annotator agrees with the response chosen by the majority. If the computed probability is more than two standard deviations away from the mean we discard all annotations done by that annotator. On average, 310 distinct annotators needed",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is it a neural model? How is it trained?",
    "llm_answer": "Yes.  Trained using cross-entropy as the loss function, with variations including neural regressors and classifiers.\n",
    "context": [
      {
        "id": "a63db46a-a278-4a94-955b-22100018819f",
        "metadata": {
          "chunk_id": 15,
          "document_id": "2002.11402",
          "start_idx": 5576,
          "end_idx": 5982
        },
        "page_content": " as explained above before passing to the network. Architecture is built using tensorflow/keras. Coding inspiration taken from BERT-keras and for CRF layer keras-contrib. If one is more comfortable in pytorch there are many examples available on github, but pytorch-bert-crf-ner is better for an easy start. We used BERT-Multilingual model so that we can train and fine-tune the same model for other Indian",
        "type": "Document"
      },
      {
        "id": "50ca5e64-17e0-4da9-ba41-eef90d680b74",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1710.00341",
          "start_idx": 857,
          "end_idx": 1406
        },
        "page_content": " modeling. Indeed, the system makes very few assumptions about the task, and looks for supportive information directly on the Web. Our system works fully automatically. It does not use any heavy feature engineering and can be easily used in combination with task-specific approaches as well, as a core subsystem. Finally, it combines the representational strength of recurrent neural networks with kernel-based classification. The system starts with a claim to verify. First, we automatically convert the claim into a query, which we execute against",
        "type": "Document"
      },
      {
        "id": "9ca1fb9b-e265-4afe-9acb-0725b5869de4",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1910.11949",
          "start_idx": 7091,
          "end_idx": 7577
        },
        "page_content": " its features. The core of our chatbot model is a sequence-to-sequence BIBREF24. This architecture uses a Recurrent Neural Network (RNN) to encode a variable-length sequence to obtain a large fixed dimensional vector representation and another RNN to decode the vector into a variable-length sequence. The encoder iterates through the input sentence one word at each time step producing an output vector and a hidden state vector. The hidden state vector is passed to the next time step",
        "type": "Document"
      },
      {
        "id": "cc19f0cb-3472-43b2-b27a-65551596a757",
        "metadata": {
          "chunk_id": 20,
          "document_id": "1909.00542",
          "start_idx": 7743,
          "end_idx": 8155
        },
        "page_content": " layer, and use cross-entropy as the loss function. The complete architecture is shown in Fig. FIGREF28. The bottom section of Table TABREF26 shows the results of several variants of the neural architecture. The table includes a neural regressor (NNR) and a neural classifier (NNC). The neural classifier is trained in two set ups: \u201cNNC top 5\u201d uses classification labels as described in Section SECREF3, and \u201cNNC",
        "type": "Document"
      },
      {
        "id": "7188814a-9cc0-45ad-afbe-3ffcaa35234f",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1706.07179",
          "start_idx": 2809,
          "end_idx": 3302
        },
        "page_content": " in this section. Figure 1 provides a high-level view of the model. The model is sequential in nature, consisting of the following steps: read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer. We model the dynamic memory in a fashion similar to Recurrent Entity Networks BIBREF17 and then equip it with an additional relational memory. There are three main components to the model: 1) input encoder 2) dynamic memory, and 3)",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How they evaluate quality of generated output?",
    "llm_answer": "They evaluate quality through summarization quality (BLEU, METEOR, ROUGE) and language fluency, along with a human evaluation of style strength.\n",
    "context": [
      {
        "id": "c1de1582-f607-4bb9-89c5-254ee1cabc39",
        "metadata": {
          "chunk_id": 51,
          "document_id": "2004.01980",
          "start_idx": 17590,
          "end_idx": 18113
        },
        "page_content": " measure the generation quality through two conventional aspects: summarization quality and language fluency. Note that the purpose of this two-way automatic evaluation is to confirm that the performance of our model is in an acceptable range. Good automatic evaluation performances are necessary proofs to compliment human evaluations on the model effectiveness. We use the standard automatic evaluation metrics for summarization with the original headlines as the reference: BLEU BIBREF47, METEOR BIBREF48, ROUGE BIBREF49",
        "type": "Document"
      },
      {
        "id": "1b0e8d2b-87fd-463c-a243-90439487cbd1",
        "metadata": {
          "chunk_id": 70,
          "document_id": "1909.02776",
          "start_idx": 27191,
          "end_idx": 27684
        },
        "page_content": " was produced, and compared by ROUGE. Averaging these ROUGE scores over each document and then over the dataset, the overall quality of summaries produced by the model can be obtained. The same process was repeated with a random regressor that needed no training, and which simply assigns a random number between zero and one to any given sample. Apart from measuring the performance of this regressor on the test set, the quality of summaries produced is evaluated and reported as a baseline.",
        "type": "Document"
      },
      {
        "id": "0ac41b45-9407-43aa-8ef0-39c616dbbddc",
        "metadata": {
          "chunk_id": 50,
          "document_id": "2004.01980",
          "start_idx": 17033,
          "end_idx": 17590
        },
        "page_content": " scores as the final score. In addition, we have another independent human evaluation task about the style strength \u2013 we present the generated headlines from TitleStylist and baselines to the human judges and let them choose the one that most conforms to the target style such as humor. Then we define the style strength score as the proportion of choices. Apart from the comprehensive human evaluation, we use automatic evaluation to measure the generation quality through two conventional aspects: summarization quality and language fluency. Note that the",
        "type": "Document"
      },
      {
        "id": "141ee518-37b9-4a42-b1e6-537f48b4d170",
        "metadata": {
          "chunk_id": 64,
          "document_id": "2002.05058",
          "start_idx": 25291,
          "end_idx": 25823
        },
        "page_content": "forms them by a large margin, which demonstrates that our comparison-based evaluation metric is able to evaluate sample quality more accurately. In addition, we find that evaluating generated samples by comparing it with a set of randomly selected samples or using sample-level skill rating performs almost equally well. This is not surprising as the employed skill rating is able to handle the inherent variance of players (i.e. NLG models). As this variance does not exist when we regard a sample as a model which always generates",
        "type": "Document"
      },
      {
        "id": "13c025c5-9268-4604-ac9e-858cbe896c39",
        "metadata": {
          "chunk_id": 62,
          "document_id": "2004.01980",
          "start_idx": 21664,
          "end_idx": 22218
        },
        "page_content": " human evaluation of the overall generation quality on four criteria, we also conducted a conventional automatic assessment to gauge only the summarization quality. This evaluation does not take other measures such as the style strength into consideration, but it serves as important complimentary proof to ensure that the model has an acceptable level of summarization ability. Table TABREF59 summarizes the automatic evaluation results of our proposed TitleStylist model and all baselines. We use the summarization-related evaluation metrics, i.e., BLE",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the four forums the data comes from?",
    "llm_answer": "BIBREF, Reddit, Twitter, Facebook\n",
    "context": [
      {
        "id": "82086b55-7cb8-4aad-b653-09ba12ed77c3",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1708.09609",
          "start_idx": 1347,
          "end_idx": 1894
        },
        "page_content": " of a forum in terms of what products it deals with, identify trends over time, associate users with particular activity profiles, and connect to price information to better understand the marketplace. Some of these analyses only require post-level information (what is the product being bought or sold in this post?) whereas other analyses might require token-level references; we annotate at the token level to make our annotation as general as possible. Our dataset has already proven enabling for case studies on these particular forums BIBREF",
        "type": "Document"
      },
      {
        "id": "33cf796d-2dd4-47e2-b6ae-76404909ffb9",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1710.00341",
          "start_idx": 3035,
          "end_idx": 3556
        },
        "page_content": " a different dataset and a slightly different task: fact checking in community question answering forums. Section SECREF5 presents related work. Finally, Section SECREF6 concludes and suggests some possible directions for future work. Given a claim, our system searches for support information on the Web in order to verify whether the claim is likely to be true. The three steps in this process are (i) external support retrieval, (ii) text representation, and (iii) veracity prediction. This step consists of generating",
        "type": "Document"
      },
      {
        "id": "08afb2b3-2d1c-4554-b106-96e35446dd5a",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1812.07023",
          "start_idx": 2828,
          "end_idx": 3309
        },
        "page_content": " , we discuss existing literature on end-to-end dialogue systems with a special focus on multi-modal dialogue systems. Section SECREF3 describes the avsd dataset. In Section SECREF4 , we present the architecture of our modelname model. We describe our evaluation and experimental setup in Section SECREF5 and then conclude in Section SECREF6 . With the availability of large conversational corpora from sources like Reddit and Twitter, there has been a lot of recent work on end-to",
        "type": "Document"
      },
      {
        "id": "411f8f41-9feb-44b8-ab8d-367e25b511d7",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1708.09609",
          "start_idx": 3003,
          "end_idx": 3433
        },
        "page_content": " a different forum: in this sense, even two different cybercrime forums seem to represent different \u201cfine-grained domains.\u201d Since we want to avoid having to annotate data for every new forum that might need to be analyzed, we explore several methods for adaptation, mixing type-level annotation BIBREF12 , BIBREF13 , token-level annotation BIBREF14 , and semi-supervised approaches BIBREF15 , BIBREF16 . We find little improvement",
        "type": "Document"
      },
      {
        "id": "93da99b6-d14d-4cc6-9499-5ca6c85778d5",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1908.04531",
          "start_idx": 11374,
          "end_idx": 11919
        },
        "page_content": ". Twitter. Twitter has been used extensively as a source of user-generated content and it was the first source considered in our initial data collection phase. The platform provides excellent interface for developers making it easy to gather substantial amounts of data with limited efforts. However, Twitter was not a suitable source of data for our task. This is due to the fact that Twitter has limited usage in Denmark, resulting in low quality data with many classes of interest unrepresented. Facebook. We next considered Facebook, and the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are sentence embeddings incorporated into the speech recognition system?",
    "llm_answer": "Pre-trained sentence embeddings, such as those from BERT, are encoded to learn conversational-context embeddings.  These embeddings are used within the end-to-end speech recognition framework.\n",
    "context": [
      {
        "id": "f58ac915-9a6a-45ed-9117-43d8095d0b69",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1906.11604",
          "start_idx": 2151,
          "end_idx": 2635
        },
        "page_content": " text resources, (i.e. fastText, BERT) so that the model can learn better conversational-context representations. So far, the use of such pre-trained embeddings have found limited success in the speech recognition task. We also add a gating mechanism to the decoder network that can integrate all the available embeddings (word, speech, conversational-context) efficiently with increase representational power using multiplicative interactions. Additionally, we explore a way to train",
        "type": "Document"
      },
      {
        "id": "d10a4e80-8d8d-46b7-aaa1-13dd4ee3b6b2",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1906.11604",
          "start_idx": 11497,
          "end_idx": 11992
        },
        "page_content": " sentence embeddings, such as deep contextualized word representations BIBREF20 , BERT BIBREF22 , encode the complex characteristics and meanings of words in various context by jointly training a bidirectional language model. The BERT model proposed a masked language model training approach enabling them to also learn good \u201csentence\u201d representation in order to predict the masked word. In this work, we explore both types of embeddings to learn conversational-context embeddings as illustrated",
        "type": "Document"
      },
      {
        "id": "eec3989b-0d46-443b-a164-ab1d03d57a4a",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1906.11604",
          "start_idx": 4184,
          "end_idx": 4714
        },
        "page_content": " network that incorporate external word/sentence embeddings from multiple preceding sentence history, their methods are limited to learn conversational-context representation from one preceding sentence in annotated speech training set. Gating-based approaches have been used for fusing word embeddings with visual representations in genre classification task or image search task BIBREF28 , BIBREF29 and for learning different languages in speech recognition task BIBREF30 . We perform end-to-end speech recognition using a joint",
        "type": "Document"
      },
      {
        "id": "ad9d7cff-01ed-4a66-9570-050a05a20da0",
        "metadata": {
          "chunk_id": 17,
          "document_id": "2001.00137",
          "start_idx": 7115,
          "end_idx": 7610
        },
        "page_content": " by an embedding layer used for input representation, with the final input embedding being a sum of token embedddings, segmentation embeddings and position embeddings. The first one, token embedding layer, uses a vocabulary dictionary to convert each token into a more representative embedding. The segmentation embedding layer indicates which tokens constitute a sentence by signaling either 1 or 0. In our case, since our data are formed of single sentences, the segment is 1 until the first `",
        "type": "Document"
      },
      {
        "id": "2f7b2991-6b00-42c5-85be-e394a5c30480",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1906.11604",
          "start_idx": 10315,
          "end_idx": 10823
        },
        "page_content": " other conversations while passing context to the next batch. Learning better representation of conversational-context is the key to achieve better processing of long conversations. To do so, we propose to encode the general word/sentence embeddings pre-trained on large textual corpora within our end-to-end speech recognition framework. Another advantage of using pre-trained embedding models is that we do not need to back-propagate the gradients across contexts, making it easier and faster to update the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How different is the dataset size of source and target?",
    "llm_answer": "The source dataset is significantly larger than the target dataset.  For example, 25% of MovieQA (2462 examples) is larger than the MC160 training set (280 examples).\n",
    "context": [
      {
        "id": "adcc4e32-cc26-4714-8eeb-f5dce7202e05",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1711.05345",
          "start_idx": 16243,
          "end_idx": 16669
        },
        "page_content": " how large the source dataset should be to make transfer learning feasible. The results are shown in Table 4 . We find that even a small amount of source data can help. For example, by using only 25% of MovieQA for pre-training, the accuracy increases 6.3% on MC160. This is because 25% of MovieQA training set (2,462 examples) is still much larger than the MC160 training set (280 examples). As the size of the source dataset",
        "type": "Document"
      },
      {
        "id": "582ff71b-a576-4460-b230-412e6f42a962",
        "metadata": {
          "chunk_id": 71,
          "document_id": "1910.06748",
          "start_idx": 25247,
          "end_idx": 25756
        },
        "page_content": " in accuracy and F1 score. Looking at the choice of datasets, it can be seen that training with the large-scale dataset leads to a nearly 1% improvement compared to the medium-sized dataset, which also gives a 1% improvement compared to the small-scale dataset. While it is expected that using more training data will lead to a better system and better results, the small improvement indicates that even our small-scale dataset has sufficient messages to allow the network training to converge. We can further",
        "type": "Document"
      },
      {
        "id": "d0aeb0cb-a60f-4f06-b3a0-ef596a743f65",
        "metadata": {
          "chunk_id": 310,
          "document_id": "1804.08186",
          "start_idx": 113568,
          "end_idx": 114102
        },
        "page_content": " datasets are critical for replicable and comparable research in . Where a subset of data is used from a larger collection, researchers should include details of the specific subset, including any breakdown into training and test data, or partitions for cross-validation. Where data from a new source is used, justification should be given for its inclusion, as well as some means for other researchers to replicate experiments on the same dataset. To address specific sub-problems in , a number of shared tasks have been organized on",
        "type": "Document"
      },
      {
        "id": "048d4cb1-5805-4471-a6cc-0357ca9a5973",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1711.05345",
          "start_idx": 2670,
          "end_idx": 3189
        },
        "page_content": " the same model on the other MCQA dataset, which is referred to as the target task, that we actually care about, but that usually contains much less training data. The effectiveness of transfer learning is evaluated by the model's performance on the target task. In supervised transfer learning, both the source and target datasets provide the correct answer to each question during pre-training and fine-tuning, and the QA model is guided by the correct answer to optimize its objective function in a supervised manner",
        "type": "Document"
      },
      {
        "id": "ed0bc156-89c2-4b94-9e58-2194484d059d",
        "metadata": {
          "chunk_id": 48,
          "document_id": "1709.05404",
          "start_idx": 17579,
          "end_idx": 18022
        },
        "page_content": "-validation setup) for each dataset while holding the number of posts per class constant. Figure FIGREF26 shows the performance of each of the Gen, RQ, and Hyp datasets at intervals of 100 posts per class (up to the maximum size of 582 posts per class for Hyp, and 851 posts per class for RQ). From the graph, we can see that as a general trend, the datasets benefit from larger dataset sizes. Interestingly, the results for the RQ dataset are",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What type of documents are supported by the annotation platform?",
    "llm_answer": "PDFs and Microsoft Word\n",
    "context": [
      {
        "id": "92872244-1788-4e55-9e1c-3c096e4986a2",
        "metadata": {
          "chunk_id": 33,
          "document_id": "2002.01861",
          "start_idx": 12971,
          "end_idx": 13505
        },
        "page_content": " features: First, it provides an annotation interface that allows users to define content elements, upload documents, and annotate documents; a screenshot is shown in Figure FIGREF12. We have invested substantial effort in making the interface as easy to use as possible; for example, annotating content elements is as easy as selecting text from the document. Our platform is able to ingest documents in a variety of formats, including PDFs and Microsoft Word, and converts these formats into plain text before presenting them to the",
        "type": "Document"
      },
      {
        "id": "37083b46-334a-4c82-b5c2-3becc33cb7c3",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1912.07025",
          "start_idx": 23832,
          "end_idx": 24368
        },
        "page_content": "aged thereby improving performance over time. Our proposed web-based annotator system, although designed for Indic manuscripts, is flexible, and could be reused for similar manuscripts from Asian subcontinent. We intend to expand the capabilities of our annotator system in many useful ways. For instance, the layout estimated by our deep-network could be provided to annotators for correction, thus reducing annotation efforts. Finally, we plan to have our dataset, instance segmentation system and annotator system publicly available.",
        "type": "Document"
      },
      {
        "id": "b5872c77-1ceb-4a02-be26-9b171873c766",
        "metadata": {
          "chunk_id": 41,
          "document_id": "2001.02380",
          "start_idx": 16317,
          "end_idx": 16722
        },
        "page_content": " annotated with a large number of layers, including document layout (headings, paragraphs, figures, etc.); multiple POS tags (Penn tags, CLAWS5, Universal POS); lemmas; sentence types (e.g. imperative, wh-question etc., BIBREF35); Universal Dependencies BIBREF36; (non-)named entity types; coreference and bridging resolution; and discourse parses using Rhetorical Structure Theory BIBREF11. In particular",
        "type": "Document"
      },
      {
        "id": "85efa7b3-d981-483d-8710-7763c1c249c7",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1912.07025",
          "start_idx": 11135,
          "end_idx": 11635
        },
        "page_content": "forations in the document interior. It may be important to identify such degradations before attempting lexically-focused tasks such as OCR or word-spotting. Keeping the aforementioned challenges in mind, we introduce a new browser-based annotation tool (see Figure FIGREF10). The tool is designed to operate both stand-alone and as a web-service. The web-service mode enables features such as distributed parallel sessions by registered annotators, dashboard-based live session monitoring and a wide",
        "type": "Document"
      },
      {
        "id": "c979da2a-ce8c-42d4-b588-765caa2099e2",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1912.07025",
          "start_idx": 6677,
          "end_idx": 7221
        },
        "page_content": " system addresses these aspects and provides additional capabilities. Many of the additional features in our system are tailored for annotation and examining annotation analytics for documents with dense and irregular layout elements, especially those found in Indic manuscripts. In this respect, our annotation system is closer to the recent trend of collaborative, cloud/web-based annotation systems and services BIBREF36, BIBREF37, BIBREF38. The Indic manuscript document images in our dataset are obtained from two sources. The first source",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the strong baselines you have?",
    "llm_answer": "The context does not specify which baselines are \"strong\".  It describes multiple baselines, but does not evaluate their relative strengths.\n",
    "context": [
      {
        "id": "94c43990-47e3-478c-a415-088eeac582b5",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1804.08782",
          "start_idx": 13990,
          "end_idx": 14427
        },
        "page_content": " number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 .  For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose",
        "type": "Document"
      },
      {
        "id": "555b8b94-8d95-496e-ac65-47922482cfe0",
        "metadata": {
          "chunk_id": 40,
          "document_id": "1911.13066",
          "start_idx": 15067,
          "end_idx": 15591
        },
        "page_content": " dataset. Table TABREF23 presents results of baseline and all variations of our experiments. We focus our discussion on F1-score as accuracy is often misleading for dataset with unbalanced class distribution. However, for completeness sake, all measures are reported. It is observed from the results that baseline model performs worst among all the experiments. The reason behind this degradation in performance can be traced back to the nature of the texts in the datasets (i.e., datasets used in original paper of baseline",
        "type": "Document"
      },
      {
        "id": "c9d62915-21d1-42b6-a16c-8d9d28de6e0c",
        "metadata": {
          "chunk_id": 91,
          "document_id": "1902.06843",
          "start_idx": 35139,
          "end_idx": 35416
        },
        "page_content": "s by -0.27. Baselines: To test the efficacy of our multi-modal framework for detecting depressed users, we compare it against existing content, content-network, and image-based models (based on the aforementioned general image feature, facial presence, and facial expressions.)",
        "type": "Document"
      },
      {
        "id": "3f956a4e-f9f6-4339-b75c-d529dbc23a27",
        "metadata": {
          "chunk_id": 53,
          "document_id": "1707.03904",
          "start_idx": 20086,
          "end_idx": 20524
        },
        "page_content": " \u2013 finding the few most relevant ones will allow the reader to work best. In Tables 2 and 3 we compare all baselines when the context size is tuned to maximize the overall accuracy on the validation set. For Quasar-S the best performing baseline is the BiRNN language model, which achieves $33.6\\%$ accuracy. The GA model achieves $48.3\\%$ accuracy on the set of instances for which the answer is in context, however, a search accuracy of",
        "type": "Document"
      },
      {
        "id": "3ff0334d-c1b6-4522-b5b5-ee27747097d6",
        "metadata": {
          "chunk_id": 63,
          "document_id": "2004.01980",
          "start_idx": 22218,
          "end_idx": 22603
        },
        "page_content": " all baselines. We use the summarization-related evaluation metrics, i.e., BLEU, ROUGE, CIDEr, and METEOR, to measure how relevant the generated headlines are to the news articles, to some extent, by comparing them to the original human-written headlines. In Table TABREF59, the first row \u201cNHG\u201d shows the performance of the current state-of-the-art summarization model on our data, and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are causal attribution networks?",
    "llm_answer": "A causal attribution dataset is a collection of text pairs reflecting cause-effect relationships proposed by humans.  These networks map out the underlying or latent causal attribution network held collectively by humans.  Nodes are defined by written descriptions, and edges represent cause-effect relationships.\n",
    "context": [
      {
        "id": "bcd84ed5-1f61-4e87-ac51-af990280ee08",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1812.06038",
          "start_idx": 0,
          "end_idx": 494
        },
        "page_content": "In this work we compare causal attribution networks derived from three datasets. A causal attribution dataset is a collection of text pairs that reflect cause-effect relationships proposed by humans (for example, \u201cvirus causes sickness\u201d). These written statements identify the nodes of the network (see also our graph fusion algorithm for dealing with semantically equivalent statements) while cause-effect relationships form the directed edges (\u201cvirus\u201d $\\rightarrow $ \u201csickness\u201d) of the causal",
        "type": "Document"
      },
      {
        "id": "73022c3d-fe5a-4e73-8b7d-2d31b32d9152",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1812.06038",
          "start_idx": 18228,
          "end_idx": 18801
        },
        "page_content": " adjectives than ConceptNet and IPRnet. Overall, shorter, noun-heavy sentences may either help or harm the ability to combine causal attribution networks, depending on their ambiguity relative to longer, typical written statements. These causal attributions networks are separate efforts to map out the underlying or latent causal attribution network held collectively by humans. It is natural to then ask if these different efforts can be combined in an effective way. Fusing these networks together can provide a single causal attribution network for researchers to study",
        "type": "Document"
      },
      {
        "id": "1e70fa2d-0ccb-4650-9902-7b2fd93aa11d",
        "metadata": {
          "chunk_id": 1,
          "document_id": "1812.06038",
          "start_idx": 494,
          "end_idx": 921
        },
        "page_content": "\ufffdvirus\u201d $\\rightarrow $ \u201csickness\u201d) of the causal attribution network. We collected causal attribution networks from three sources of data: English Wikidata BIBREF11 , English ConceptNet BIBREF10 , and IPRnet BIBREF12 . Wikidata and ConceptNet, are large knowledge graphs that contain semantic links denoting many types of interactions, one of which is causal attribution, while IPRnet comes from an Amazon Mechanical Turk study",
        "type": "Document"
      },
      {
        "id": "d0e127be-4dd3-4534-8528-d80fce7361db",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1812.06038",
          "start_idx": 12945,
          "end_idx": 13533
        },
        "page_content": " cause-effect relationships held by humans. We perform a descriptive analysis of the three datasets, comparing and contrasting their features and properties. We focus on two aspects, the network structure and the text information (the written descriptions associated with each node in the network). Understanding these data at these levels can inform efforts to combine different causal attribution networks (Sec. \"Fusing causal networks\" ). Table 1 and Fig. 2 summarize network characteristics for the three causal attribution networks. We focus on standard measures of network structure",
        "type": "Document"
      },
      {
        "id": "eafb91bc-4a81-48e2-9d83-44f1007d6ecb",
        "metadata": {
          "chunk_id": 40,
          "document_id": "1812.06038",
          "start_idx": 12063,
          "end_idx": 12629
        },
        "page_content": " . Here we use network and text analysis tools to compare causal attribution networks (Sec. \"Comparing causal networks\" ). Crucially, nodes in these networks are defined only by their written descriptions, and multiple written descriptions can represent the same conceptual entity. Thus, to understand how causal attribution networks can be combined, we introduce and analyze a method for fusing networks (Sec. \"Fusing causal networks\" ) that builds off both the network structure and associated text information and explicitly incorporates conceptual equivalencies.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "how did they ask if a tweet was racist?",
    "llm_answer": "The context does not describe how participants were asked if a tweet was racist.  It only mentions assessing offensiveness on a scale and asking about hate speech in general.\n",
    "context": [
      {
        "id": "835df265-7275-4555-939f-7e96e5531620",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1701.08118",
          "start_idx": 7852,
          "end_idx": 8358
        },
        "page_content": ". Participants were finally asked (3) to answer how offensive they thought the tweet was on a 6-point Likert scale from 1 (Not offensive at all) to 6 (Very offensive). If they answered 4 or higher, the participants had the option to state which particular words they found offensive. After the annotation of the 20 tweets, participants were asked to voluntarily answer an open question regarding the definition of hate speech. In the survey with the definition, they were asked if the definition of Twitter",
        "type": "Document"
      },
      {
        "id": "eaeb1b2f-9990-40c3-b749-b2247ad2bc00",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1701.08118",
          "start_idx": 7381,
          "end_idx": 7852
        },
        "page_content": " of 20 tweets presented in a random order. For each tweet, each participant was asked three questions. Depending on the survey, participants were asked (1) to answer (yes/no) if they considered the tweet hate speech, either based on the definition or based on their personal opinion. Afterwards they were asked (2) to answer (yes/no) if the tweet should be banned from Twitter. Participants were finally asked (3) to answer how offensive they thought the tweet was on a 6",
        "type": "Document"
      },
      {
        "id": "15647f11-9c3c-4453-b26a-c5edf39dcdb4",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1901.01911",
          "start_idx": 16609,
          "end_idx": 17070
        },
        "page_content": " people in #Ferguson were shouting \u201ckill the police\", local reporting has refuted your ugly racism  Here the misclassified tweet is a reply including an explicit expression of agreement with the author of the source tweet (\u201cI join you\u201d). Tweet (s2) is one of the rare cases of source tweets denying the rumor (source tweets in the RumourEval17 dataset are mostly supporting the rumor at issue). Our hypothesis is that it is difficult for a system to detect such",
        "type": "Document"
      },
      {
        "id": "906342ec-8b52-4e29-bace-fa5a341da2eb",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1811.02906",
          "start_idx": 5683,
          "end_idx": 6214
        },
        "page_content": "izing the same tweets into either `insult', `profanity', `abuse', or `other'. The training data contains 5,008 manually labeled tweets sampled from Twitter from selected accounts that are suspected to contain a high share of offensive language. Manual inspection reveals a high share of political tweets among those labeled as offensive. These tweets range from offending single Twitter users, politicians and parties to degradation of whole social groups such as Muslims, migrants or refugees. The test data contains 3,532 tweets.",
        "type": "Document"
      },
      {
        "id": "35dfc885-704d-4264-a5d4-086128d099c1",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1910.03814",
          "start_idx": 9411,
          "end_idx": 9950
        },
        "page_content": ", tweets containing less than three words and tweets containing porn related terms. From that selection, we kept the ones that included images and downloaded them. Twitter applies hate speech filters and other kinds of content control based on its policy, although the supervision is based on users' reports. Therefore, as we are gathering tweets from real-time posting, the content we get has not yet passed any filter. We aim to create a multimodal hate speech database where all the instances contain visual and textual information that",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does the model compute the likelihood of executing to the correction semantic denotation?",
    "llm_answer": "The parser generates candidate logical forms via beam search, executes them, and marks those yielding the correct denotation as consistent.  The parser is trained to maximize the total likelihood of these consistent logical forms.\n",
    "context": [
      {
        "id": "a2c115ec-bda0-4a13-bbd6-8f31c766e3b7",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1808.07625",
          "start_idx": 13753,
          "end_idx": 14137
        },
        "page_content": " term computes the reconstruction likelihood $p(x|y)$ ; and the second term is the entropy of the approximated posterior $q(y|x) $ for regularization. Specifically, we use the semantic parser to compute the approximated posterior $q(y|x)$ . The reconstruction likelihood $p(x|y)$ is computed with an inverse parser which recovers utterance $x$ from its logical form $y$ . We use $p(x|",
        "type": "Document"
      },
      {
        "id": "0d6e6243-ea99-4048-a98b-03805b90a57e",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1809.02731",
          "start_idx": 4791,
          "end_idx": 5057
        },
        "page_content": "+1}}\\log P(w_j|S_i)  \\nonumber $$   (Eq. 5)   where $\\theta $ and $\\phi $ contain the parameters in the encoder $f_\\text{en}(S_i;\\theta )$ and the decoder $f_\\text{de}(_i;\\phi )$ respectively. The forward computation of our model for a given sentence pair $\\lbrace S",
        "type": "Document"
      },
      {
        "id": "df847e1d-eb8e-47ac-9f6f-0f03f662b663",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1808.07625",
          "start_idx": 9486,
          "end_idx": 9719
        },
        "page_content": ". Then, the parser is trained to maximize the total log likelihood of these consistent logical forms:  $$\\begin{split}\n& \\sum _{y \\in Y_c(x)} \\log p(y|x) = \\\\\n& \\sum _{y \\in Y_c(x)} \\log p(f_1,\\cdots , f_k, l_1, \\cdots , l_o|x)\n\\end{",
        "type": "Document"
      },
      {
        "id": "8a23745f-524a-4e93-b3f9-189e06a27606",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1808.07625",
          "start_idx": 11252,
          "end_idx": 11757
        },
        "page_content": " equates to 1 in this case. Training of the neural parser-ranker system involves the following steps. Given an input utterance, the parser first generates a list of candidate logical forms via beam search. The logical forms are then executed and those which yield the correct denotation are marked as consistent. The parser is trained to optimize the total likelihood of consistent logical forms (Equation ( 21 )), while the ranker is trained to optimize the marginal likelihood of denotations (Equation (",
        "type": "Document"
      },
      {
        "id": "ddc8df66-b990-4dc0-86d1-8df5722de789",
        "metadata": {
          "chunk_id": 12,
          "document_id": "2001.05467",
          "start_idx": 4836,
          "end_idx": 5327
        },
        "page_content": " this, during training, we average out all the output probability distributions for each time step of the decoder for the whole mini-batch. The resulting vector $D^{\\prime }$ will reflect each token's probability of being generated from the model's perspective. Note that we do not use discrete ground-truth tokens to evaluate the model's bias, because there is a fine distinction between the two: a statistics of frequency on ground-truth tokens is an evaluation of the corpus's bias, while",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are state of the art methods authors compare their work with? ",
    "llm_answer": "Potthast et al. BIBREF10 evaluated three AO approaches against 44 AV methods.\n",
    "context": [
      {
        "id": "4b370dd2-9124-488d-9427-1f79430612f4",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1906.10551",
          "start_idx": 4902,
          "end_idx": 5362
        },
        "page_content": " approaches which, according to Gr\u00f6ndahl and Asokan BIBREF9 can be divided into manual, computer-assisted and automatic types, perform a variety of modifications on the texts. These include simple synonym replacements, rule-based substitutions or word order permutations. In 2016, Potthast et al. BIBREF10 presented the first large-scale evaluation of three AO approaches that aim to attack 44 AV methods, which were submitted to the PAN-AV competitions during",
        "type": "Document"
      },
      {
        "id": "34f9b031-2144-4a20-9811-5b17301e609a",
        "metadata": {
          "chunk_id": 446,
          "document_id": "1804.08186",
          "start_idx": 166790,
          "end_idx": 167290
        },
        "page_content": " on comparing new documents with models of each target language learned from data. The types of models as well as the sources of training data used in the literature are diverse, and work to date has not compared and evaluated these in a systematic manner, making it difficult to draw broader conclusions about what the \u201cbest\u201d method for actually is. We have attempted to synthesize results to date to identify a set of \u201cbest practices\u201d, but these should be treated as guidelines and should always be",
        "type": "Document"
      },
      {
        "id": "c8175824-116a-471e-8ed4-47f180d3720b",
        "metadata": {
          "chunk_id": 301,
          "document_id": "1804.08186",
          "start_idx": 109612,
          "end_idx": 110179
        },
        "page_content": " different authors have used to evaluate methods. BIBREF32 emphasize this by demonstrating how the performance of a system can vary according to the data used for evaluation. This implies that comparisons of results reported by different authors may not be meaningful, as a strong result in one paper may not translate into a strong result on the dataset used in a different paper. In other areas of research, authors have proposed standardized corpora to allow for the objective comparison of different methods. Some authors have released datasets to accompany their",
        "type": "Document"
      },
      {
        "id": "1f140aea-67fe-42b4-983e-2e4e083c362a",
        "metadata": {
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "9521dfd0-aa9f-4645-b1d4-9f0936084cc5",
        "metadata": {
          "chunk_id": 7,
          "document_id": "1906.10551",
          "start_idx": 3019,
          "end_idx": 3554
        },
        "page_content": " properties to characterize AV methods. Section SECREF4 describes the methodology, consisting of the used corpora, examined AV methods, selected performance measures and experiments. Finally, Section SECREF5 concludes the work and outlines future work. Over the years, researchers in the field of authorship analysis identified a number of challenges and limitations regarding existing studies and approaches. Azarbonyad et al. BIBREF8 , for example, focused on the questions if the writing styles of authors of short texts change over",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much improvement do they get?",
    "llm_answer": "Various improvements are mentioned, ranging from negligible (e.g., 2% increase in accuracy) to significant (e.g., 5.4 EM improvement in multi-span questions).  A 2.1 point gain is also noted.\n",
    "context": [
      {
        "id": "d63c1e3c-bd64-40b5-a740-854f75384d1b",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1803.02839",
          "start_idx": 1481,
          "end_idx": 1849
        },
        "page_content": " , BIBREF7 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF10 , BIBREF11 , BIBREF8 , BIBREF9 . Often, the improvement simply mitigates the exploding/vanishing gradient problem BIBREF18 , BIBREF19 , but, for many tasks, the improvement is more capable of generalizing the network's training for that task. Understanding better how and why certain networks excel at certain NLP",
        "type": "Document"
      },
      {
        "id": "8d8c69e7-bb1a-4f05-96ab-2b33b61d748d",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1610.00879",
          "start_idx": 7038,
          "end_idx": 7487
        },
        "page_content": "2%. This degradation implies that our features capture a subset of drunk tweets and that there are properties of drunk tweets that may be more subtle. When both N-gram and stylistic features are used, there is negligible improvement. The accuracy for Dataset 2 increases from 77.9% to 78.1%. Precision/Recall metrics do not change significantly either. The best accuracy of our classifier is 78.1% for all features, and 75.6% for stylistic features.",
        "type": "Document"
      },
      {
        "id": "b744e65c-657e-4027-a1ce-6f063f2f2d46",
        "metadata": {
          "chunk_id": 67,
          "document_id": "1909.13375",
          "start_idx": 22947,
          "end_idx": 23384
        },
        "page_content": " 3.5 points. Given that number questions make up about 61% of the dataset, we can deduce that our improved number handling is responsible for about a 2.1 point gain, while the rest could be be attributed to the improved Wikipedia parsing. Although NER span cleaning (Section SECREF23) affected only 3% of the multi-span questions, it provided a solid improvement of 5.4 EM in multi-span questions and 1.5 EM in single-span questions. The",
        "type": "Document"
      },
      {
        "id": "764be4b4-f71f-4f78-a71f-e574f158ab6d",
        "metadata": {
          "chunk_id": 56,
          "document_id": "2003.06279",
          "start_idx": 21752,
          "end_idx": 22053
        },
        "page_content": "e. $\\max \\Gamma _+ = \\max _p \\Gamma _+(p)$) for GloVe. We also show the average difference in performance ($\\langle \\Gamma _+ - \\Gamma _0 \\rangle $) and the total number of cases in which an improvement in performance was observed ($N_+$). $N_+$ ranges in the interval $0 \\le N_+ \\le 20$. Table TABREF",
        "type": "Document"
      },
      {
        "id": "ca225e91-eb6b-43c4-b4af-22d8c937c0ed",
        "metadata": {
          "chunk_id": 129,
          "document_id": "1909.00183",
          "start_idx": 47837,
          "end_idx": 48317
        },
        "page_content": "). The improvement is larger for finer clusterings into 70 and 45 communities, which contain enough detail that can be associated with levels of risk (e.g., type of accident). This supports the value of the multi-resolution groupings we have extracted through our analysis. We also studied the impact of using Doc2Vec vectors as features. Interestingly, the comparison between columns 2 and 11 shows that there is only a slight improvement of 2% when using Doc2Vec instead of TF-i",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which languages do they test on?",
    "llm_answer": "Dutch, Spanish, English, German, Basque, French, Italian, Finnish, Hindi, Portuguese, Japanese, Thai, Vietnamese, Arabic, Russian.\n",
    "context": [
      {
        "id": "5b455659-78c3-4d09-ae6a-bfc475e72c40",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1701.09123",
          "start_idx": 6692,
          "end_idx": 7180
        },
        "page_content": " our approach is much simpler than most, we obtain the best results for Dutch, Spanish and English and comparable results in German (on CoNLL 2002 and 2003). We also report best results for German using the GermEval 2014 shared task data and for Basque using the Egunkaria testset BIBREF23 . We report out-of-domain evaluations in three languages (Dutch, English and Spanish) using four different datasets to compare our system with the best publicly available systems for those languages",
        "type": "Document"
      },
      {
        "id": "56abf65f-90a8-44a4-8d27-e52d3543d1dd",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1808.08780",
          "start_idx": 13409,
          "end_idx": 13926
        },
        "page_content": ". We test our bilingual embedding refinement approach on both intrinsic and extrinsic tasks. In Section \"Cross-lingual embeddings training\" we describe the common training setup for all experiments and language pairs. The languages we considered are English, Spanish, Italian, German and Finnish. Throughout all the experiments we use publicly available resources in order to make comparisons and reproducibility of our experiments easier. Corpora. In our experiments we make use of web-extracted corpora. For English",
        "type": "Document"
      },
      {
        "id": "e56f0098-5a47-4cd9-941c-b820adb7a34c",
        "metadata": {
          "chunk_id": 33,
          "document_id": "1909.13695",
          "start_idx": 12139,
          "end_idx": 12554
        },
        "page_content": " came from the Linguaskill test, of which there are 6 major L1s each with more than 100 candidates: Hindi, Portuguese, Japanese, Spanish, Thai and Vietnamese. Each of the training set and two test sets was fairly gender balanced, with approximately one third of candidates graded as B1, one third graded as B2, and the rest graded as A1, A2, C1, or C2, according to CEFR ability levels. For each test set candidate,",
        "type": "Document"
      },
      {
        "id": "74d41b8d-63d7-479f-a82f-76142d721807",
        "metadata": {
          "chunk_id": 44,
          "document_id": "1911.03894",
          "start_idx": 15623,
          "end_idx": 16121
        },
        "page_content": " is also machine translated for all languages. The dataset is composed of 122k train, 2490 valid and 5010 test examples. As usual, NLI performance is evaluated using accuracy. To evaluate a model on a language other than English (such as French), we consider the two following settings: TRANSLATE-TEST: The French test set is machine translated into English, and then used with an English classification model. This setting provides a reasonable, although imperfect, way to circumvent the fact that",
        "type": "Document"
      },
      {
        "id": "7367d0f1-0e76-4843-a7a9-1611742d90a2",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1809.04686",
          "start_idx": 9144,
          "end_idx": 9613
        },
        "page_content": "40 chose a subset of the SNLI test set (1332 sentences) and professionally translated it into four major languages - Arabic, French, Russian, and Spanish. We use the French test set for evaluation in Section \"Zero-Shot Classification Results\" and \"Analyses\" . Here, we first describe the model and training details of the base multilingual NMT model whose encoder is reused in all other tasks. Then we provide details about the task-specific classifiers. For each task,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What limitations are mentioned?",
    "llm_answer": "The limitations mentioned are:  sequences in the training set are usually assumed to be uniformly or geometrically distributed, with little regard to the nature and complexity of the language.\n",
    "context": [
      {
        "id": "64bb3935-6ffb-4c24-b545-79b24c3fc3e9",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1709.08299",
          "start_idx": 7668,
          "end_idx": 8137
        },
        "page_content": "OO, SBV, VOB, HED, FOB, IOB, POB, and only preserve the parts that have dependencies. Further Filtering: Only preserve SBV, VOB and restrict the related words not to be pronouns and verbs. Frequency Restriction: After calculating word frequencies, only word frequency that greater than 2 is valid for generating question. Question Restriction: Only five questions can be extracted within one passage. Apart from the automatically generated large-scale training data, we",
        "type": "Document"
      },
      {
        "id": "fc0eb602-d561-487a-972c-8fe5544cdae4",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1606.04631",
          "start_idx": 18364,
          "end_idx": 18507
        },
        "page_content": " . In the future we will make more effort to fix these limitations and exploit the linguistic domain knowledge in visual content understanding.",
        "type": "Document"
      },
      {
        "id": "06b4e9b7-0023-467d-8d00-0b10fcea3e96",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1606.04631",
          "start_idx": 11739,
          "end_idx": 12163
        },
        "page_content": "ively when the sum of the number of frames and words exceed the limit. If the number of words is within 40, we arbitrarily truncate the frames to satisfy the maximum length. When the length of sentence is more than 40, we discard the words that beyond the length and take video frames with a maximum number of 40. Bidirectional S2VT: Similar to BIBREF19 , we implement several S2VT-based models: S2VT, bidirectional S2VT and",
        "type": "Document"
      },
      {
        "id": "a6b29bb9-376b-44f0-98a0-79c305b4f0ce",
        "metadata": {
          "chunk_id": 39,
          "document_id": "2004.03034",
          "start_idx": 15301,
          "end_idx": 15680
        },
        "page_content": ", ratio of quotation marks, exclamation marks, modal verbs, stop words, type-token ratio, hedging BIBREF29, named entity types, POS n-grams, sentiment BIBREF30 and subjectivity scores BIBREF31, spell-checking, readibility features such as Coleman-Liau BIBREF32, Flesch BIBREF33, argument lexicon features BIBREF34 and surface features such as word lengths, sentence lengths, word",
        "type": "Document"
      },
      {
        "id": "af5b725d-2602-4ec1-aa78-a1220c02b55c",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1811.01001",
          "start_idx": 1609,
          "end_idx": 2157
        },
        "page_content": "REF11 , BIBREF9 , BIBREF12 . These formulations follow narrow and bounded evaluation schemes though: They often define a length threshold in the test set and report the performance of the model on this fixed set. We acknowledge three unsettling issues with these formulations. First, the sequences in the training set are usually assumed to be uniformly or geometrically distributed, with little regard to the nature and complexity of the language. This assumption may undermine any conclusions drawn from empirical investigations, especially given",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the baselines?",
    "llm_answer": "Baseline 1: smooth L1 distance; Baseline 2: PCA-based acoustic similarity; Baseline 3: Nonlinear dynamical systems-based complexity; ANRES_Sing_R; ANRES_Sing_Z; ANRES_Seq_R; ANRES_Seq_Z.\n",
    "context": [
      {
        "id": "94c43990-47e3-478c-a415-088eeac582b5",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1804.08782",
          "start_idx": 13990,
          "end_idx": 14427
        },
        "page_content": " number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 .  For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose",
        "type": "Document"
      },
      {
        "id": "555b8b94-8d95-496e-ac65-47922482cfe0",
        "metadata": {
          "chunk_id": 40,
          "document_id": "1911.13066",
          "start_idx": 15067,
          "end_idx": 15591
        },
        "page_content": " dataset. Table TABREF23 presents results of baseline and all variations of our experiments. We focus our discussion on F1-score as accuracy is often misleading for dataset with unbalanced class distribution. However, for completeness sake, all measures are reported. It is observed from the results that baseline model performs worst among all the experiments. The reason behind this degradation in performance can be traced back to the nature of the texts in the datasets (i.e., datasets used in original paper of baseline",
        "type": "Document"
      },
      {
        "id": "3747343d-0517-442a-b7ba-66d01be55b6d",
        "metadata": {
          "chunk_id": 40,
          "document_id": "2004.01878",
          "start_idx": 14526,
          "end_idx": 14915
        },
        "page_content": " normal distribution $\\mathcal {N}(0, 1)$. In summary, the following four baselines are designed: ANRES_Sing_R: randomly initializing the states for each single trading day. ANRES_Sing_Z: initializing the states as zeros for each single trading day. ANRES_Seq_R: randomly initializing the first states for each trading sequence only. ANRES_Seq_Z: initializing the first states as zeros for",
        "type": "Document"
      },
      {
        "id": "f297d78b-0fdd-4eee-ae9e-9e75a2e66130",
        "metadata": {
          "chunk_id": 140,
          "document_id": "2001.02380",
          "start_idx": 51470,
          "end_idx": 51956
        },
        "page_content": " baselines. The left, middle and right panels in Figure FIGREF40 correspond to measurements when all signals are included, only cases contained entirely in the head EDUs shown to the model, and only DMs, respectively. The scenario on the left is rather unreasonable and is included only for completeness: here the model is also penalized for not detecting signals such as lexical chains, part of which is outside the units that the model is being shown. An example of such a case can be",
        "type": "Document"
      },
      {
        "id": "7bb4c95c-4811-4e15-ab8e-deca3c53de73",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1709.08299",
          "start_idx": 9435,
          "end_idx": 10003
        },
        "page_content": " questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models. The",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What semantic features help in detecting whether a piece of text is genuine or generated? of ",
    "llm_answer": "Dense representations of the claim, snippets, and related sentences, automatically trained using Long Short-Term Memory networks.\n",
    "context": [
      {
        "id": "700c3220-4b15-4dd2-85c8-76f895379e96",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1710.00341",
          "start_idx": 1406,
          "end_idx": 1923
        },
        "page_content": " a claim to verify. First, we automatically convert the claim into a query, which we execute against a search engine in order to obtain a list of potentially relevant documents. Then, we take both the snippets and the most relevant sentences in the full text of these Web documents, and we compare them to the claim. The features we use are dense representations of the claim, of the snippets and of related sentences from the Web pages, which we automatically train for the task using Long Short-Term Memory networks",
        "type": "Document"
      },
      {
        "id": "708bad92-51a3-472d-ad78-0c579fec9214",
        "metadata": {
          "chunk_id": 14,
          "document_id": "2004.03788",
          "start_idx": 5596,
          "end_idx": 6110
        },
        "page_content": "of-the-art part-of-speech tagging and named entity recognition model, we observe that semantic features are more important than word level statistical features to model performance. Thus, we decompose the syntactic tree and use word vectors to more precisely capture the semantic inconsistencies in different structural parts of a satirical news tweet. Recently, with the success of deep learning in NLP, many researchers attempted to detect fake news with end-to-end neural nets based approaches. Ruchansky et al.",
        "type": "Document"
      },
      {
        "id": "7496b5c4-d1c2-4c3d-be07-e336a95e8c12",
        "metadata": {
          "chunk_id": 67,
          "document_id": "1910.09399",
          "start_idx": 26295,
          "end_idx": 26820
        },
        "page_content": " methods of each category. black Semantic relevance is one the of most important criteria of the text-to-image synthesis. For most GNAs discussed in this survey, they are required to generate images semantically related to the text descriptions. However, the semantic relevance is a rather subjective measure, and images are inherently rich in terms of its semantics and interpretations. Therefore, many GANs are further proposed to enhance the text-to-image synthesis from different perspectives. In this subsection, we will",
        "type": "Document"
      },
      {
        "id": "a40fe310-fe2c-46a4-b56c-7d0fa638fbe3",
        "metadata": {
          "chunk_id": 114,
          "document_id": "1906.10551",
          "start_idx": 42880,
          "end_idx": 43427
        },
        "page_content": " loud) or \u201ck\u201d (ok), smileys and specific obscene words. Therefore, we assume that the verification results of the examined methods are mainly caused by the similar vocabularies between the texts. We highlighted the problem that underlying characteristics of authorship verification approaches have not been paid much attention in the past research and that these affect the applicability of the methods in real forensic settings. Then, we proposed several properties that enable a better characterization and by this a better comparison between AV",
        "type": "Document"
      },
      {
        "id": "85e2eb76-6624-45fc-8d7d-925a863dcfd7",
        "metadata": {
          "chunk_id": 442,
          "document_id": "1804.08186",
          "start_idx": 165159,
          "end_idx": 165656
        },
        "page_content": " only the text itself as evidence on which to base the prediction of the language. Within a text, there can sometimes be domain-specific peculiarities that can be used for . For example, BIBREF399 investigates of user-to-user messages in the eBay e-commerce portal. He finds that using only the first two and last two words of a message is sufficient for identifying the language of a message. This article has presented a comprehensive survey on language identification of digitally-encoded text.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is the assumption that natural language is stationary and ergodic valid?",
    "llm_answer": "Manning and Sch\u00fctze argue that, even though not quite correct, language text can be modeled as stationary, ergodic random processes.\n",
    "context": [
      {
        "id": "9bd20ef0-f80e-40cb-9853-f112ebea6839",
        "metadata": {
          "chunk_id": 26,
          "document_id": "2002.03438",
          "start_idx": 9108,
          "end_idx": 9486
        },
        "page_content": " we assume this stationary ergodic random process with finite alphabet $\\mathcal {A}$ denoted $X = \\lbrace X_i, -\\infty < i < \\infty \\rbrace $ is non-null in the sense that always $P(x_{-m}^{-1}) > 0$ and This is sometimes called the smoothing requirement. We further introduce an additional property of random processes that we assume for language $L$. We define the continuity",
        "type": "Document"
      },
      {
        "id": "5d835bf3-93b5-49c4-939c-538ef4757b48",
        "metadata": {
          "chunk_id": 25,
          "document_id": "2002.03438",
          "start_idx": 8831,
          "end_idx": 9278
        },
        "page_content": " among the class of models that are $k$-order Markov approximations of language $L$, which is simply the empirical plug-in estimate. Manning and Sch\u00fctze argue that, even though not quite correct, language text can be modeled as stationary, ergodic random processes BIBREF29, an assumption that we follow. Moreover, given the diversity of language production, we assume this stationary ergodic random process with finite alphabet $\\mathcal {A}$ den",
        "type": "Document"
      },
      {
        "id": "a1da15b4-65d4-483a-8f10-ca7f8910178a",
        "metadata": {
          "chunk_id": 28,
          "document_id": "2002.03438",
          "start_idx": 9681,
          "end_idx": 10158
        },
        "page_content": " of smoothing and continuity are taken from the literature on estimation of stationary, ergodic random processes BIBREF30. As such, the hypothesis test we aim to consider here is between a non-null, stationary, ergodic process with summable continuity rate (genuine language) and its empirical $k$-order Markov approximation based on training data (language model output). We think of the setting where the language model is trained on data with many tokens, a sequence of very",
        "type": "Document"
      },
      {
        "id": "00bc072f-401c-48f4-a318-ec4068b947f0",
        "metadata": {
          "chunk_id": 41,
          "document_id": "2002.03438",
          "start_idx": 13686,
          "end_idx": 14144
        },
        "page_content": " for any other random process $Y$ defined on the same alphabet $\\mathcal {A}$, for some constant $\\tilde{K}$. If this generalized reverse Pinsker inequality holds, it implies the following further bound on the Kullback-Leibler divergence and therefore the error exponent of the detection problem for the empirical maximum likelihood Markov language model. Conjecture 2 Let $X$ be a non-null stationary ergodic process with summable continuity rate defined on",
        "type": "Document"
      },
      {
        "id": "23ff98fc-a7cc-466e-846a-d594adb779b5",
        "metadata": {
          "chunk_id": 27,
          "document_id": "2002.03438",
          "start_idx": 9486,
          "end_idx": 9912
        },
        "page_content": " further introduce an additional property of random processes that we assume for language $L$. We define the continuity rate of the process $X$ as: We further let $\\gamma = \\sum _{k=1}^{\\infty } \\gamma (k)$, and If $\\gamma < \\infty $, then the process has summable continuity rate. These specific technical notions of smoothing and continuity are taken from the literature on estimation of stationary, ergodic random processes",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which models do they try out?",
    "llm_answer": "Support Vector Machines (SVMs), logistic regression, Na\u00efve Bayes, Perceptron, and decision trees.\n",
    "context": [
      {
        "id": "11bed98c-eee8-466c-b192-b4d7bee2693b",
        "metadata": {
          "chunk_id": 56,
          "document_id": "1909.00154",
          "start_idx": 21118,
          "end_idx": 21619
        },
        "page_content": " different experiments in the paper (and report the respective mean and standard deviation statistics). Whenever we want to analyse a particular model (e.g. to check the coefficients of a choice model), we select the one with the highest likelihood at the development set (i.e. in practice, its out-of-sample generalization performance), and report its performance on the test set. Since a choice model will typically involve other variables than the categorical ones that we learn the embeddings for,",
        "type": "Document"
      },
      {
        "id": "65333987-2358-483b-95cd-3ff028ff6a96",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      },
      {
        "id": "5e5c6963-88aa-4a86-96ff-323aeb271bfb",
        "metadata": {
          "chunk_id": 66,
          "document_id": "1810.00663",
          "start_idx": 25415,
          "end_idx": 25983
        },
        "page_content": " In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models. The previous section evaluated model performance on new instructions (and corresponding navigation plans) for environments that were previously seen at training time. Here, we examine whether the trained models succeed on environments that are completely new. The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as",
        "type": "Document"
      },
      {
        "id": "de3be0ac-632a-4d98-8906-a9861db55f1c",
        "metadata": {
          "chunk_id": 102,
          "document_id": "1904.09708",
          "start_idx": 37855,
          "end_idx": 38357
        },
        "page_content": " is shown for each of the model's outputs (in the rows of the image) for some illustrative examples. Darker blue indicates a higher weight. The examples are shown in pairs for a model trained and tested on the add-jump split, with one example drawn from the training set and a corresponding example drawn from the test set. Examples are shown in increasing complexity, with a failure mode depicted in Figure 15 . In general, it can be seen that although the attention distributions on the test examples",
        "type": "Document"
      },
      {
        "id": "b7a08b07-33f8-4756-8035-934c8ce8d913",
        "metadata": {
          "chunk_id": 40,
          "document_id": "1911.00133",
          "start_idx": 15746,
          "end_idx": 16159
        },
        "page_content": " first experiment with a suite of non-neural models, including Support Vector Machines (SVMs), logistic regression, Na\u00efve Bayes, Perceptron, and decision trees. We tune the parameters for these models using grid search and 10-fold cross-validation, and obtain results for different combinations of input and features. For input representation, we experiment with bag-of-n-grams (for $n \\in \\lbrace 1..3\\rbrace $),",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the competing models?",
    "llm_answer": "BiMPM, ESIM, Decomposable Attention Model, KIM, BERT, Latent-Tree, Berkeley, LaSO, Latent-Strc, Model-Stack, Non-Linear, Coref-GRU, MHPGM, Weaver, MHQA-GRN.  Also, RF (with and without reduced features), Aggregated forecaster, MLP, Random Forest, and Linear Support Vector Machine.\n",
    "context": [
      {
        "id": "e218c2da-6400-4484-b946-c9f95cc23931",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1906.06448",
          "start_idx": 17189,
          "end_idx": 17518
        },
        "page_content": " five state-of-the-art NLI models adopting different approaches: BiMPM (Bilateral Multi-Perspective Matching Model; BIBREF31 , BIBREF31 ), ESIM (Enhanced Sequential Inference Model; BIBREF32 , BIBREF32 ), Decomposable Attention Model BIBREF33 , KIM (Knowledge-based Inference Model; BIBREF34 , BIBREF34 ), and BERT (Bidirectional",
        "type": "Document"
      },
      {
        "id": "6bd45ba5-5818-4bf7-9706-f82d1b94eeae",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1603.04553",
          "start_idx": 11513,
          "end_idx": 11902
        },
        "page_content": "IBREF28 ; Latent-Tree: the latent tree model BIBREF29 obtaining the best results in the shared task; Berkeley: the Berkeley system with the final feature set BIBREF12 ; LaSO: the structured perceptron system with non-local features BIBREF30 ; Latent-Strc: the latent structure system BIBREF31 ; Model-Stack: the entity-centric system with model stacking BIBREF32 ; and Non-Linear: the non-",
        "type": "Document"
      },
      {
        "id": "50dd6a5b-8c24-470a-93be-d9e06f49afc2",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1808.09920",
          "start_idx": 16484,
          "end_idx": 16858
        },
        "page_content": " BIBREF3 and FastQA BIBREF6 . We also compare against Coref-GRU BIBREF12 , MHPGM BIBREF11 , and Weaver BIBREF10 . Additionally, we include results of MHQA-GRN BIBREF23 , from a recent arXiv preprint describing concurrent work. They jointly train graph neural networks and recurrent encoders. We report single runs of our two best single models and an ensemble one on the unm",
        "type": "Document"
      },
      {
        "id": "47401a98-3774-4cb1-816d-255b6b9652b2",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1910.12618",
          "start_idx": 24043,
          "end_idx": 24512
        },
        "page_content": " the gradient descent in the NNs for instance. In order to tone it down and to increase the consistency of our results, the different models are run $B=10$ times. The results presented hereafter correspond to the average and standard-deviation on those runs. The RF model denoted as \"sel\" is the one with the reduced number of features, whereas the other RF uses the full vocabulary. We also considered an aggregated forecaster (abridged Agg), consisting of the average",
        "type": "Document"
      },
      {
        "id": "a4a4c58b-d1f1-46a1-9912-24f7a070ad68",
        "metadata": {
          "chunk_id": 43,
          "document_id": "2001.00137",
          "start_idx": 16781,
          "end_idx": 17197
        },
        "page_content": " the online platforms. Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the input triple translated to a slot-filling task?",
    "llm_answer": "The context describes a slot-filling task as a sequence labeling problem where a parameterized mapping function $f_{\\theta}$ maps input words to slot tags.  No specific input triple translation is described.\n",
    "context": [
      {
        "id": "d0098e3e-50ea-454d-8622-82aa97bbf76c",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1909.06937",
          "start_idx": 4332,
          "end_idx": 4674
        },
        "page_content": " $N$ words and its corresponding slot tags $Y^{slot} = \\lbrace y_1, y_2, \\cdots , y_N \\rbrace $, the slot filling task aims to learn a parameterized mapping function $f_{\\theta } : X \\rightarrow Y $ from input words to slot tags. For the intent detection, it is designed to predict the intent label $\\hat{y}^{int}$ for the entire utterance $X",
        "type": "Document"
      },
      {
        "id": "ce426570-0e07-4dfd-b772-cdcf4dfc02cb",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1911.01680",
          "start_idx": 2291,
          "end_idx": 2654
        },
        "page_content": "% error rate reduction. The task of Slot Filling is formulated as a sequence labeling problem. Deep learning has been extensively employed for this task (BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11). The prior work has mainly utilized the recurrent neural network as the encoder to extract features per word and Cond",
        "type": "Document"
      },
      {
        "id": "acb1a6f5-0f2c-4143-9716-4d8453395661",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1909.06937",
          "start_idx": 736,
          "end_idx": 1244
        },
        "page_content": " the intent prediction, without interactions between representations of slots and intents. Intuitively, slots and intents from similar fields tend to occur simultaneously, which can be observed from Figure FIGREF2 and Table TABREF3. Therefore, it is beneficial to generate the representations of slots and intents with the guidance from each other. Some works explore enhancing the slot filling task unidirectionally with the guidance from intent representations via gating mechanisms BIBREF7, BIBREF8, while",
        "type": "Document"
      },
      {
        "id": "3ccb166a-d523-4101-925c-c232749bbe6a",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1911.01680",
          "start_idx": 0,
          "end_idx": 403
        },
        "page_content": "Slot Filling (SF) is the task of identifying the semantic concept expressed in natural language utterance. For instance, consider a request to edit an image expressed in natural language: \u201cRemove the blue ball on the table and change the color of the wall to brown\u201d. Here, the user asks for an \"Action\" (i.e., removing) on one \u201cObject\u201d (blue ball on the table) in the image and changing an \u201cAttribute\u201d (",
        "type": "Document"
      },
      {
        "id": "96c2f697-17ef-4a6b-b05c-9198df2a9047",
        "metadata": {
          "chunk_id": 47,
          "document_id": "1911.01680",
          "start_idx": 17090,
          "end_idx": 17449
        },
        "page_content": " of Slot Filling. In a multi-task setting, our model increase the mutual information between word representations and its context, improve the label information in the context and predict which concepts are expressed in the given sentence. Our experiments on an image edit request corpus shows that our model achieves state-of-the-art results on this dataset.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is module that analyzes behavioral state trained?",
    "llm_answer": "The behavior model is pre-trained to identify the presence of each behavior from a sequence of words using a multi-label classification scheme.  The pre-trained portion was implemented using a single layer RNN with LSTM units with dimension size 50.\n",
    "context": [
      {
        "id": "81667463-55cc-4c68-9227-ad0c7a4d773d",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1909.00107",
          "start_idx": 12029,
          "end_idx": 12563
        },
        "page_content": " model. The behavior states are used as gating mechanism for a typical RNN based language model. We show through our experiments that the proposed technique improves language modeling perplexity specifically in the case of behavior-rich scenarios. Finally, we show improvements on the previous state-of-the-art benchmark model with Penn Tree Bank Corpus to underline the affect of behavior states in language modeling. In future, we plan to incorporate the behavior-gated language model into the task of automatic speech recognition (",
        "type": "Document"
      },
      {
        "id": "36d72b99-b9ee-4948-9a90-70d743ce43e7",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1909.00107",
          "start_idx": 8441,
          "end_idx": 8945
        },
        "page_content": "ivity, and Sadness to train our models. This is motivated from previous works which showed good separability in these behaviors as well as being easy to interpret. The behavior model is pre-trained to identify the presence of each behavior from a sequence of words using a multi-label classification scheme. The pre-trained portion of the behavior model was implemented using a single layer RNN with LSTM units with dimension size 50. We augmented previous RNN language model architectures by BIBREF1 and",
        "type": "Document"
      },
      {
        "id": "9dce1bde-f5ef-4313-bc8b-c083048fc112",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1810.00663",
          "start_idx": 3977,
          "end_idx": 4584
        },
        "page_content": " collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation. The dataset opens up opportunities to explore data-driven methods for grounding navigation commands into high-level motion plans. We conduct extensive experiments to study the generalization capabilities of the proposed model for following natural language instructions. We investigate both generalization to new instructions in known and in new environments. We conclude this paper by discussing the benefits of",
        "type": "Document"
      },
      {
        "id": "2fb47fd4-1060-4958-9c5c-455c954df04f",
        "metadata": {
          "chunk_id": 76,
          "document_id": "1810.00663",
          "start_idx": 29565,
          "end_idx": 30130
        },
        "page_content": " applications, including information retrieval. We proposed an end-to-end system to translate user instructions to a high-level navigation plan. Our model utilized an attention mechanism to merge relevant information from the navigation instructions with a behavioral graph of the environment. The model then used a decoder to predict a sequence of navigation behaviors that matched the input commands. As part of this effort, we contributed a new dataset of 11,051 pairs of user instructions and navigation plans from 100 different environments. Our model achieved",
        "type": "Document"
      },
      {
        "id": "24d5a116-d59b-42d1-8d90-ac261365e468",
        "metadata": {
          "chunk_id": 77,
          "document_id": "1810.00663",
          "start_idx": 29906,
          "end_idx": 30510
        },
        "page_content": " of 11,051 pairs of user instructions and navigation plans from 100 different environments. Our model achieved the best performance in this dataset in comparison to a two-step baseline approach for interpreting navigation instructions, and a sequence-to-sequence model that does not consider the behavioral graph. Our quantitative and qualitative results suggest that attention mechanisms can help leverage the behavioral graph as a relevant knowledge base to facilitate the translation of free-form navigation instructions. Overall, our approach demonstrated practical form of learning for a complex and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Can the model add new relations to the knowledge graph, or just new entities?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "4fa70e40-b499-4b91-92d9-ca41dd8c8d6d",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1910.03891",
          "start_idx": 4084,
          "end_idx": 4666
        },
        "page_content": " knowledge, most existing knowledge graph embedding methods just leverage relation triples of KGs while ignoring a large number of attribute triples. Therefore, these methods easily suffer from sparseness and incompleteness of knowledge graph. Even worse, structure information usually cannot distinguish the different meanings of relations and entities in different triples. We believe that these rich information encoded in attribute triples can help explore rich semantic information and further improve the performance of knowledge graph. For example, we can learn date of birth",
        "type": "Document"
      },
      {
        "id": "a3dca3f7-1e32-4ec5-bbfe-f689fc0cefe9",
        "metadata": {
          "chunk_id": 98,
          "document_id": "1711.03438",
          "start_idx": 31110,
          "end_idx": 31633
        },
        "page_content": " It may be easy to apply a filter to modify the list of predicted target entities so that entities that are same as the relationship will be rearranged. We leave this task as a matter for future work. In the present work we introduced a new open-world Knowledge Graph Completion model ConMask that uses relationship-dependent content masking, fully convolutional neural networks, and semantic averaging to extract relationship-dependent embeddings from the textual features of entities and relationships in KGs. Experiments",
        "type": "Document"
      },
      {
        "id": "4368222b-c8eb-4da8-b675-7000e0df43ec",
        "metadata": {
          "chunk_id": 1,
          "document_id": "1910.03891",
          "start_idx": 363,
          "end_idx": 743
        },
        "page_content": "}, relation, \\textit {tail entity})$ (denoted $(h, r, t)$ in this study) through the Resource Description Framework, e.g.,$(\\textit {Donald Trump}, Born In, \\textit {New York City})$. Figure FIGREF2 shows the subgraph of knowledge graph about the family of Donald Trump. In many KGs, we can observe that some relations indicate attributes of entities, such as the $\\textit {Born}$",
        "type": "Document"
      },
      {
        "id": "9524fb5d-2129-4933-8904-509fd4b2d704",
        "metadata": {
          "chunk_id": 0,
          "document_id": "1706.07179",
          "start_idx": 0,
          "end_idx": 428
        },
        "page_content": "Reasoning about entities and their relations is an important problem for achieving general artificial intelligence. Often such problems are formulated as reasoning over graph-structured representation of knowledge. Knowledge graphs, for example, consist of entities and relations between them BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . Representation learning BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 and reasoning BIBREF8 , BIBREF9 ,",
        "type": "Document"
      },
      {
        "id": "795b6d86-3680-4291-9088-00985b96b200",
        "metadata": {
          "chunk_id": 79,
          "document_id": "1910.03891",
          "start_idx": 27755,
          "end_idx": 28300
        },
        "page_content": " behavior. The results of entity and relation predication on FB24K are shown in the Table TABREF33. This results indicates that KANE still outperforms other baselines significantly and consistently. This also verifies the necessity of modeling high-order structural and attribute information of KGs in Knowledge graph embedding models. Many recent works have demonstrated the benefits of knowledge graph embedding in knowledge graph completion, such as relation extraction. However, We argue that knowledge graph embedding method still have room",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Do twitter users tend to tweet about the DOS attack when it occurs? How much data supports this assumption?",
    "llm_answer": "Yes, the context states that legitimate users are willing to reveal information about service outages on social media platforms, suggesting that Twitter users do tweet about DoS attacks when they occur.  The text references a dataset of annotated tweets extracted from Twitter during DoS attacks, but doesn't specify the volume of data.\n",
    "context": [
      {
        "id": "370438d3-c3d8-47b1-8ca0-470c59324c80",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1909.05890",
          "start_idx": 5331,
          "end_idx": 5840
        },
        "page_content": " Twitter during DoS attacks on a variety organizations from differing domains such as banking (like Bank Of America) and technology. A weakly-supervised approach to identifying detect likely DoS service related events on twitter in real-time. A score to measure impact of the DoS attack based on the frequency of user complaints about the event. The rest of this paper is organized as follows: In section 2, previous work regarding DDoS attack detection and new event detection will be discussed. In section 3",
        "type": "Document"
      },
      {
        "id": "0f8a3eef-1fb3-4ee1-8652-ca0a88608499",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1909.05890",
          "start_idx": 1577,
          "end_idx": 2133
        },
        "page_content": " of service attack always results in legitimate users being unable to access the network system or slowing down their access and they are usually willing to reveal this information on social media plaforms. Thus legitimate user feedback can be a reliable indicator about the severity level of the service outage. Thus we split this problem into two parts namely by first isolating the tweet stream that is likely related to a DoS attack and then measuring the impact of attack by analyzing the extracted tweets. A central challenge to measure the impact is",
        "type": "Document"
      },
      {
        "id": "126308d6-0db2-4aad-bda1-d49923dfa78c",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1909.05890",
          "start_idx": 4781,
          "end_idx": 5331
        },
        "page_content": " tweet in the time window is computed using the distribution of topics for the tweet and the score of the topics. We're looking for tweets on new topics through time. While the experiments show promising results, precision can be further increased by adding a layer of a supervised classifier trained with attack data at the expense of recall. Following are the contributions in this paper: A dataset of annotated tweets extracted from Twitter during DoS attacks on a variety organizations from differing domains such as banking (like Bank Of America",
        "type": "Document"
      },
      {
        "id": "1f31d261-61cb-48cd-b00c-27d69bc18f01",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1909.05890",
          "start_idx": 5620,
          "end_idx": 6130
        },
        "page_content": " 2, previous work regarding DDoS attack detection and new event detection will be discussed. In section 3, we describe the how the data was collected. We also present the model we created to estimate the impact of DDoS attacks from Twitter feeds. In section 4, the experiments are described and the results are provided. In section 5 we discuss some additional questions. Finally, section 6 concludes our paper and describes future work. Denial of Service (DoS) attacks are a major threat to Internet security,",
        "type": "Document"
      },
      {
        "id": "2810ff6d-1abc-48ea-ad97-d6a78c72545d",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1909.05890",
          "start_idx": 18412,
          "end_idx": 18750
        },
        "page_content": " attack days ($D_a$) are manually annotated and only 50 percent of those tweets are actually about a DDoS attack. The 5 tweets that have the highest relative score in the dataset are: jiwa mines and miner u.s. bancorp, pnc latest bank websites to face access issues: (reuters) - some u.s. bancorp... http://bit.ly/p5xpmz u.s. bancorp, pnc",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the training and test data used?",
    "llm_answer": "The training data consists of all previous rounds of data, Wikipedia Toxic Comments data, and the newly collected adversarial data from the \"break it\" round.  Training-22 (with gold standard annotations) and an external dataset (NLM-180 BIBREF5) are also used.  Test sets include Test Set 1 (57 drug labels), Test Set 2 (66 drug labels), and the test sets from the APDA shared task (without labels).\n",
    "context": [
      {
        "id": "be946cc9-7e3f-4c04-8aea-2232b0448e27",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1908.06083",
          "start_idx": 12013,
          "end_idx": 12455
        },
        "page_content": " the newly collected adversarial data from the \u201cbreak it\" round. The training data consists of all previous rounds of data, so that model $A_i$ is trained on all rounds $n$ for $n \\le i$, as well as the Wikipedia Toxic Comments data. We split each round of data into train, validation, and test partitions. The validation set is used for hyperparameter selection. The test sets are used to measure how robust we are to new adversarial attacks",
        "type": "Document"
      },
      {
        "id": "7c07de9a-2536-4a2c-9fe3-ac95785f8273",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1911.00133",
          "start_idx": 14430,
          "end_idx": 14888
        },
        "page_content": " content. This results in a total of 2,838 train data points (51.6% labeled stressful) and 715 test data points (52.4% labeled stressful). Because our data is relatively small, we train our traditional supervised models with 10-fold cross-validation; for our neural models, we break off a further random 10% of the training data for validation and average the predictions of 10 randomly-initialized trained models. In addition to the words of the posts (both",
        "type": "Document"
      },
      {
        "id": "029e181a-b1c7-4323-aace-9d81ed6cdf66",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1905.07464",
          "start_idx": 3930,
          "end_idx": 4425
        },
        "page_content": " labels, referred to as Training-22, with gold standard annotations. Two test sets of 57 and 66 drug labels, referred to as Test Set 1 and 2 respectively, with gold standard annotations are used to evaluate participating systems. As Training-22 is a relatively small dataset, we additionally utilize an external dataset with 180 annotated drug labels dubbed NLM-180 BIBREF5 (more later). We provide summary statistics about these datasets in Table TABREF3 . Test Set 1 closely resembles Training",
        "type": "Document"
      },
      {
        "id": "65333987-2358-483b-95cd-3ff028ff6a96",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      },
      {
        "id": "711b2025-041c-48a4-8d44-3589de380bef",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1909.04181",
          "start_idx": 1283,
          "end_idx": 1837
        },
        "page_content": " experimental conditions and results. We then provide a literature review and conclude. For the purpose of our experiments, we use data released by the APDA shared task organizers. The dataset is divided into train and test by organizers. The training set is distributed with labels for the three tasks of age, dialect, and gender. Following the standard shared tasks set up, the test set is distributed without labels and participants were expected to submit their predictions on test. The shared task predictions are expected by organizers at the level",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What writing styles are present in the corpus?",
    "llm_answer": "Humor, newswire, letters, journal, and non-fiction.\n",
    "context": [
      {
        "id": "5254b27e-1ebe-4049-8699-d0767f294567",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1909.00279",
          "start_idx": 13718,
          "end_idx": 14228
        },
        "page_content": " models, please refer to supplementary materials. Training and Validation Sets We collected a corpus of poems and a corpus of vernacular literature from online resources. The poem corpus contains 163K quatrain poems from Tang Poems and Song Poems, the vernacular literature corpus contains 337K short paragraphs from 281 famous books, the corpus covers various literary forms including prose, fiction and essay. Note that our poem corpus and a vernacular corpus are not aligned. We further split the two corpor",
        "type": "Document"
      },
      {
        "id": "db89f3ce-c1fe-4167-9517-7b4240ee56e7",
        "metadata": {
          "chunk_id": 18,
          "document_id": "2004.01980",
          "start_idx": 6486,
          "end_idx": 6805
        },
        "page_content": " _{i=1}^N$. The target corpus $T=\\lbrace \\mathbf {t^{(i)}}\\rbrace _{i=1}^{M}$ comprises of sentences $\\mathbf {t}$ written in a specific style (e.g., humor). We assume that it conforms to the distribution $P(T)$. Note that the target corpus $T$ only contains style-carrying sentences, not necessarily headlines \u2014 it can",
        "type": "Document"
      },
      {
        "id": "70454308-97df-447a-b850-6bb4ec7847f8",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1809.08731",
          "start_idx": 8424,
          "end_idx": 8817
        },
        "page_content": " two-sentence paragraphs from the Open American National Corpus (OANC), which belong to 4 genres: newswire, letters, journal, and non-fiction. Gold references are manually created and the outputs of 4 compression systems (ILP (extractive), NAMAS (abstractive), SEQ2SEQ (extractive), and T3 (abstractive); cf. toutanova2016dataset for details) for the test data are provided. Each example has 3",
        "type": "Document"
      },
      {
        "id": "0846a8a5-f18e-4b84-bde9-7c7fa98e9e6a",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1909.00279",
          "start_idx": 6941,
          "end_idx": 7337
        },
        "page_content": " model relies on a vernacular texts corpus $\\textbf {\\emph {S}}$ and a poem corpus $\\textbf {\\emph {T}}$. We denote $S$ and $T$ as instances in $\\textbf {\\emph {S}}$ and $\\textbf {\\emph {T}}$ respectively. The training of our model relies on three procedures, namely parameter initialization, language modeling and back-translation. We will give detailed introduction to each procedure. Parameter",
        "type": "Document"
      },
      {
        "id": "be8d2216-9502-437a-8a15-b053f4aeb9e1",
        "metadata": {
          "chunk_id": 74,
          "document_id": "1906.10551",
          "start_idx": 28096,
          "end_idx": 28581
        },
        "page_content": " regarding the individual's writing style. As a third corpus, we compiled INLINEFORM0 , which is a collection of 200 aggregated postings crawled from the Reddit platform. Overall, the postings were written by 100 Reddit users and stem from a variety of subreddits. In order to construct the Y-cases, we selected exactly two postings from disjoint subreddits for each user such that both the known and unknown document INLINEFORM1 and INLINEFORM2 differ in their topic. Regarding the N-",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What meta-information is being transferred?",
    "llm_answer": "Relation-specific meta information, specifically relation meta and gradient meta.\n",
    "context": [
      {
        "id": "e245b78c-4f3f-4039-a2dc-9099207c5dd6",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1909.01515",
          "start_idx": 2143,
          "end_idx": 2735
        },
        "page_content": "-specific meta information related to the relation CEOof or CountryCapital will be extracted and transferred by MetaR from a few existing instances to incomplete triples. The relation-specific meta information is helpful in the following two perspectives: 1) transferring common relation information from observed triples to incomplete triples, 2) accelerating the learning process within one task by observing only a few instances. Thus we propose two kinds of relation-specific meta information: relation meta and gradient meta corresponding to afore mentioned two perspectives respectively",
        "type": "Document"
      },
      {
        "id": "fd365a34-02c7-4ac2-bf8a-f16e5a4124cd",
        "metadata": {
          "chunk_id": 30,
          "document_id": "1909.01515",
          "start_idx": 10253,
          "end_idx": 10790
        },
        "page_content": " loss function via gradient descending, thus gradients reveal how the model's parameters should be changed. Intuitively, we believe that gradients are valuable source to accelerate learning process. Based on these thoughts, we propose two kinds of meta information which are shared between support set and query set to deal with above problems: In order to extract relation meta and gradient mate and incorporate them with knowledge graph embedding to solve few-shot link prediction, our proposal, MetaR, mainly contains two modules: The",
        "type": "Document"
      },
      {
        "id": "68f9f39a-3ef5-4a3b-9499-5892414d64bd",
        "metadata": {
          "chunk_id": 13,
          "document_id": "2002.01030",
          "start_idx": 5577,
          "end_idx": 6081
        },
        "page_content": " dataset are chosen from more natural contexts such as Facebook posts, tweets, political debates, etc. They proposed neural network architecture for taking advantage of text and meta-data together. The model consists of a Convolutional Neural Network (CNN) for feature extraction from the text and a Bi-directional Long Short Term Memory (BiLSTM) network for feature extraction from the meta-data and feeds the concatenation of these two features into a fully connected softmax layer for making the final",
        "type": "Document"
      },
      {
        "id": "1575108d-9499-4a59-affb-2d111d2a724f",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1909.01515",
          "start_idx": 1827,
          "end_idx": 2386
        },
        "page_content": " try to accomplish few-shot link prediction from another perspective based on the intuition that the most important information to be transferred from a few existing instances to incomplete triples should be the common and shared knowledge within one task. We call such information relation-specific meta information and propose a new framework Meta Relational Learning (MetaR) for few-shot link prediction. For example, in Figure 1 , relation-specific meta information related to the relation CEOof or CountryCapital will be extracted and transferred by Meta",
        "type": "Document"
      },
      {
        "id": "1debdb5d-a592-427f-a72f-33dc381487b0",
        "metadata": {
          "chunk_id": 9,
          "document_id": "1909.01515",
          "start_idx": 3561,
          "end_idx": 4034
        },
        "page_content": "-shot link prediction tasks. In summary, main contributions of our work are three-folds: One target of MetaR is to learn the representation of entities fitting the few-shot link prediction task and the learning framework is inspired by knowledge graph embedding methods. Furthermore, using loss gradient as one kind of meta information is inspired by MetaNet BIBREF12 and MAML BIBREF13 which explore methods for few-shot learning by meta-learning. From these two points, we",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What datasets are used to evaluate the approach?",
    "llm_answer": "Swissmetro dataset.\n",
    "context": [
      {
        "id": "1f140aea-67fe-42b4-983e-2e4e083c362a",
        "metadata": {
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "269153e0-38d8-4dd6-90ab-d4840d64c949",
        "metadata": {
          "chunk_id": 62,
          "document_id": "1711.06351",
          "start_idx": 21596,
          "end_idx": 22086
        },
        "page_content": " which we evaluate quantitatively. Second, it was tasked with generating genuinely novel questions that were not present in the data set, which we evaluate qualitatively. To make predictions, the different candidate models were fit to 15 contexts and asked to predict the remaining one (i.e., leave one out cross-validation). This results in 64 different model fits (i.e., 4 models $\\times $ 16 fits). First, we verify that compositionality is an essential ingredient in an account of human",
        "type": "Document"
      },
      {
        "id": "9638118b-fae1-4140-a7d1-11bd33d21e74",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1909.00154",
          "start_idx": 23166,
          "end_idx": 23592
        },
        "page_content": "CCM), for example. We will apply the methodology to the well-known \u201cSwissmetro\" dataset. We will compare it with a dummy variables and PCA baselines. We will follow the 3-way experimental design mentioned before: split the dataset into train, development and test sets, so that the embeddings, PCA eigenvectors and the choice model are estimated from the same train and development sets, and validate it out-of-sample. For the",
        "type": "Document"
      },
      {
        "id": "3b942450-5125-4b16-8163-c4e7540353a0",
        "metadata": {
          "chunk_id": 38,
          "document_id": "1802.05322",
          "start_idx": 13905,
          "end_idx": 14341
        },
        "page_content": " explanation of the whole process needed to conduct an experiment like this. The process includes downloading a data set, web scraping for extra information, data preprocessing, model tuning and evaluation of the results. Action Adult Adventure Animation Biography Comedy Crime Documentary Drama Family Fantasy Film-Noir Game-Show History Horror Music Musical Mystery Reality-TV Romance Sci-Fi Short Sport Talk-Show Thriller War Western",
        "type": "Document"
      },
      {
        "id": "e23dc951-77e9-483d-84ce-d8a31bf62b89",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1910.12618",
          "start_idx": 19565,
          "end_idx": 20023
        },
        "page_content": " the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014 and 2015 serve as validation set (730 days). It is used to tune the hyperparameters of the different approaches. All the data from January the 1st 2016 (974 days for France and 1,096 for the UK) is used as test set, on which the final results are presented. Grid search is applied to find the best combination of values: for each hyperparameter,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much is performance hurt when using too small amount of layers in encoder?",
    "llm_answer": "Reducing the $enc_{src \\rightarrow mt}$ encoder block's depth reduces performance for all four scores.\n",
    "context": [
      {
        "id": "f426f509-b739-49a9-877d-15ab1141634e",
        "metadata": {
          "chunk_id": 86,
          "document_id": "1908.06151",
          "start_idx": 25676,
          "end_idx": 26094
        },
        "page_content": " Exp. 5.1 shows that decreasing the number of layers on the decoder side does not hurt the performance. In fact, in the case of test2016, we got some improvement, while for test2017, the scores got slightly worse. In contrast, reducing the $enc_{src \\rightarrow mt}$ encoder block's depth (Exp. 5.2) does indeed reduce the performance for all four scores, showing the importance of this second encoder. In Table TABREF",
        "type": "Document"
      },
      {
        "id": "bc71c1f9-0541-48f7-a4b8-c5507fe5ec61",
        "metadata": {
          "chunk_id": 46,
          "document_id": "1911.02711",
          "start_idx": 16199,
          "end_idx": 16671
        },
        "page_content": " increasing layer numbers from 1 to 2. More layers do not increase the accuracy on development set. We thus set 2 as the number of review encoder layers in the experiments. The best performing model size is comparable to that of the BiLSTM self-attention, demonstrating that the number of parameters is not the key factor to models' performance. Table TABREF34 and Table TABREF35 show the final results. Our model outperforms all the baseline models and the top-performing",
        "type": "Document"
      },
      {
        "id": "75912fd2-5758-4243-ac71-578b8a922ab8",
        "metadata": {
          "chunk_id": 45,
          "document_id": "2001.07263",
          "start_idx": 17117,
          "end_idx": 17529
        },
        "page_content": "isher data. This model consists of 10 encoder layers, and is trained for only 50 epochs. Our overall results on the Hub5'00 and other evaluation sets are summarized in Table TABREF14. The results in Fig. FIGREF12 and Table TABREF14 show that adding more training data greatly improves the system, by around 30% relative in some cases. For comparison with others, the 2000-hour system reaches 8.7% and 7.4% WER on",
        "type": "Document"
      },
      {
        "id": "55478aa3-0b6c-4305-b23b-cb22a2c59864",
        "metadata": {
          "chunk_id": 49,
          "document_id": "1908.06151",
          "start_idx": 15443,
          "end_idx": 15749
        },
        "page_content": " ($enc_{src \\rightarrow mt}$), compared to the source encoder ($enc_{src}$) and the decoder ($dec_{pe}$), by reducing and expanding the amount of layers in the encoders and the decoder. Our standard setup, which we use for fine-tuning, ensembling etc., is fixed to 6-6-6 for $N_{src}$-$N_{mt}$-$N_{pe}$ (cf",
        "type": "Document"
      },
      {
        "id": "696af99c-225f-4921-af8e-5576c281fe0c",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1910.10408",
          "start_idx": 10454,
          "end_idx": 10921
        },
        "page_content": " also provide results with systems only trained on TED Talks (small data condition). When training on large scale data we use Transformer with layer size of 1024, hidden size of 4096 on feed forward layers, 16 heads in the multi-head attention, and 6 layers in both encoder and decoder. When training only on TED talks, we set layer size of 512, hidden size of 2048 for the feed forward layers, multi-head attention with 8 heads and again 6 layers in both encoder and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What neural machine translation models can learn in terms of transfer learning?",
    "llm_answer": "Neural machine translation (NMT) models can learn from a corpus of examples to build a statistical model that aims to find the most probable target sequence for an input sequence.\n",
    "context": [
      {
        "id": "2d08441b-6954-4466-9e8a-22ce0319fa8f",
        "metadata": {
          "chunk_id": 42,
          "document_id": "1707.06945",
          "start_idx": 15780,
          "end_idx": 16358
        },
        "page_content": " of the specialisation and clustering algorithms may also result in improved verb class induction. We have presented a novel cross-lingual transfer model which enables the automatic induction of VerbNet-style verb classifications across multiple languages. The transfer is based on a word vector space specialisation framework, utilised to directly model the assumption of cross-linguistic validity of VerbNet-style classifications. Our results indicate strong improvements in verb classification accuracy across all six target languages explored. All automatically induced Verb",
        "type": "Document"
      },
      {
        "id": "8dc9de54-972a-49e5-a49b-e65a88626cff",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1809.04686",
          "start_idx": 1578,
          "end_idx": 2066
        },
        "page_content": " or multilingual NLP, the task of transferring knowledge from one language to another, serves as a good test bed for evaluating various transfer learning approaches. For cross-lingual NLP, the most widely studied approach is to use multilingual embeddings as features in neural network models. However, research has shown that representations learned in context are more effective BIBREF5 , BIBREF6 ; therefore, we aim at doing better than just using multilingual embeddings in the cross-",
        "type": "Document"
      },
      {
        "id": "87453ac6-79d4-4c87-99f1-413125d1a52a",
        "metadata": {
          "chunk_id": 107,
          "document_id": "2002.06053",
          "start_idx": 39210,
          "end_idx": 39720
        },
        "page_content": " products). Machine translation is a challenging task because the syntactic and semantic dependencies of each language differ from one another and this may give rise to ambiguities. Neural Machine Translation (NMT) models benefit from the potential of deep learning architectures to build a statistical model that aims to find the most probable target sequence for an input sequence by learning from a corpus of examples BIBREF110, BIBREF111. The main advantage of NMT models is that they provide an end-to-end",
        "type": "Document"
      },
      {
        "id": "6352ed20-91f6-402c-bcc4-c1d1c8104fbe",
        "metadata": {
          "chunk_id": 13,
          "document_id": "1802.00273",
          "start_idx": 5454,
          "end_idx": 6050
        },
        "page_content": " languages from a collection of Bible translations and focus on the ability of the model to pick up genetic relations between languages when being forced to cover many languages in one single model. In the following, we will first present the basic architecture of the neural translation model together with the setup for training multilingual models. After that we will discuss our experimental results before concluding the paper with some final comments and prospects for future work. Neural machine translation typically applies an end-to-end network architecture that includes one or several",
        "type": "Document"
      },
      {
        "id": "3660f48e-f004-40fe-89ab-8281c169d292",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1811.02906",
          "start_idx": 4294,
          "end_idx": 4859
        },
        "page_content": " Transfer learning recently has gained a lot of attention since it can be easily applied to neural network learning architectures. For instance, Howard.2018 propose a generic transfer learning setup for text classification based on language modeling for pre-training neural models with large background corpora. To improve offensive language detection for English social media texts, a transfer learning approach was recently introduced by Felbo.2017. Their `deepmoji' approach relies on the idea to pre-train a neural network model for an actual offensive language",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does the semi-automatic construction process work?",
    "llm_answer": "Researchers use crowdworkers to create sentiment and semantic role-labeling systems, then construct examples to find their faults.  Models are retrained to improve them, and the build, break, and fix sequence is repeated.\n",
    "context": [
      {
        "id": "1aebfcab-d794-45c7-b7c1-de7739be1938",
        "metadata": {
          "chunk_id": 22,
          "document_id": "1601.01705",
          "start_idx": 8846,
          "end_idx": 9269
        },
        "page_content": " models in reverse order. We first imagine that $z$ is always observed, and in sec:model:modules describe how to evaluate and learn modules parameterized by $\\theta _e$ within fixed structures. In sec:model:assemblingNetworks, we move to the real scenario, where $z$ is unknown. We describe how to predict layouts from questions and learn $\\theta _e$ and $\\theta _\\ell $ jointly without layout supervision. Given a layout $",
        "type": "Document"
      },
      {
        "id": "6df095b3-d765-4d2e-b5be-02f0aa7173f9",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1908.06083",
          "start_idx": 1550,
          "end_idx": 2021
        },
        "page_content": " researchers, \u201cbuilders\u201d and \u201cbreakers\u201d were used to first create sentiment and semantic role-labeling systems and then construct examples that find their faults. In this work we instead fully automate such an approach using crowdworkers as the humans-in-the-loop, and also apply a fixing stage where models are retrained to improve them. Finally, we repeat the whole build, break, and fix sequence over a number of iterations. We show that such an approach provides more",
        "type": "Document"
      },
      {
        "id": "9ea2a036-ab17-4fe3-8490-423b403d521f",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1910.11491",
          "start_idx": 7263,
          "end_idx": 7654
        },
        "page_content": " weight at $i$-th position. The model is firstly pre-trained to minimize the maximum-likelihood loss, which is widely used in sequence generation tasks. We define $y^* = \\lbrace y^*_1, \\cdots , y_T^*\\rbrace $ as the ground-truth output sequence for a given input sequence $x$, then the loss function is formulated as: After converging, the model is further optimized with local variance loss",
        "type": "Document"
      },
      {
        "id": "50ca5e64-17e0-4da9-ba41-eef90d680b74",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1710.00341",
          "start_idx": 857,
          "end_idx": 1406
        },
        "page_content": " modeling. Indeed, the system makes very few assumptions about the task, and looks for supportive information directly on the Web. Our system works fully automatically. It does not use any heavy feature engineering and can be easily used in combination with task-specific approaches as well, as a core subsystem. Finally, it combines the representational strength of recurrent neural networks with kernel-based classification. The system starts with a claim to verify. First, we automatically convert the claim into a query, which we execute against",
        "type": "Document"
      },
      {
        "id": "9042ae6f-675c-4781-ba64-2a625d243c68",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1911.11744",
          "start_idx": 10434,
          "end_idx": 10948
        },
        "page_content": " language generator, we conducted an human-subject study to collect sentence templates of a placement task as well as common words and synonyms for each of the used features. By utilising these data, we are able to generate over 180,000 unique sentences, depending on the generated scenario. The generated parameters of the low-level DMP controller \u2013 the weights and goal position \u2013 must be sufficiently accurate in order to successfully deliver the object to the specified bin. On the right side of Figure FIGREF4",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What does \"explicitly leverages their probabilistic correlation to guide the training process of both models\" mean?",
    "llm_answer": "The framework uses the probabilistic relationship between QA and QG as a regularizer to influence how both models are trained.\n",
    "context": [
      {
        "id": "418d303e-f3b5-410c-b4e2-3cd71d8a15e9",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1706.02027",
          "start_idx": 2013,
          "end_idx": 2458
        },
        "page_content": " framework that exploits the duality of QA and QG to improve both tasks. There might be different ways of exploiting the duality of QA and QG. In this work, we leverage the probabilistic correlation between QA and QG as the regularization term to influence the training process of both tasks. Specifically, the training objective of our framework is to jointly learn the QA model parameterized by $\\theta _{qa}$ and the QG model parameterized by",
        "type": "Document"
      },
      {
        "id": "2cd4638a-c96c-4403-8d95-2f88f45760dd",
        "metadata": {
          "chunk_id": 30,
          "document_id": "2004.02393",
          "start_idx": 9949,
          "end_idx": 10347
        },
        "page_content": " example, we train the Ranker and Reasoner alternatively as a cooperative game: Reasoner Step: Given the first passage $p_t$ selected by the trained Ranker, the Reasoner predicts the probability of each entity $e$ appearing in $p_t$. The Reasoner is trained with the cross-entropy loss:  Ranker Step: Given the Reasoner's top-1 predicted linking entity $e$, the reward for Ranker at the $2^{\\textrm",
        "type": "Document"
      },
      {
        "id": "74ec3dfb-523a-43d2-abf0-bbb0c88867d6",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1910.09399",
          "start_idx": 12524,
          "end_idx": 12961
        },
        "page_content": " to become increasingly adept at its specific task until a threshold is reached. This is analogous to a min-max game between the two models, according to the following equation: In Eq. (DISPLAY_FORM10), $x$ denotes a multi-dimensional sample, e.g., an image, and $z$ denotes a multi-dimensional latent space vector, e.g., a multidimensional data point following a predefined distribution function such as that of normal distributions. $D",
        "type": "Document"
      },
      {
        "id": "dceb0639-d50d-4469-96c4-6bf39ddcb987",
        "metadata": {
          "chunk_id": 98,
          "document_id": "2001.02380",
          "start_idx": 38384,
          "end_idx": 38880
        },
        "page_content": " see in Section SECREF5 that model signal predictions overlap considerably with human judgments, even when the predicted relation is incorrect. Intuitively, for similar relations, such as concession or contrast, both of which are adversative, the model may notice a relevant cue (e.g. `but', or contrasting lexical items) despite choosing the wrong one. Second, as we will see below, we will be analyzing the model's behavior with respect to the probability of the correct relation, regardless of",
        "type": "Document"
      },
      {
        "id": "2ef53658-12c3-427f-bdf4-a00681e0c1b5",
        "metadata": {
          "chunk_id": 59,
          "document_id": "1905.10238",
          "start_idx": 18851,
          "end_idx": 19383
        },
        "page_content": " model. It proves that simply treating external knowledge as the feature, even though they are from the same sources, is not as effective as learning them in a joint framework. The reason behind this result is mainly from the noise in the knowledge source, e.g., parsing error, incorrectly identified relations, etc. For example, the plurality of 17% noun phrases are wrongly labeled in the test data. As a comparison, our knowledge attention might contribute to alleviate such noise when incorporating all knowledge sources. Effect",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is WNGT 2019 shared task?",
    "llm_answer": "NDNLP's submission to the 2019 WNGT Efficiency shared task uses a standard, recommended baseline Transformer network.\n",
    "context": [
      {
        "id": "161d47d3-a5fc-4fd4-8b08-ffc93a09893c",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1910.07134",
          "start_idx": 1305,
          "end_idx": 1774
        },
        "page_content": ". NDNLP's submission to the 2019 WNGT Efficiency shared task uses a standard, recommended baseline Transformer network. Following BIBREF2, we investigate the application of auto-sizing to various portions of the network. Differing from their work, the shared task used a significantly larger training dataset from WMT 2014 BIBREF4, as well as the goal of reducing model size even if it impacted translation performance. Our best system was able to prune over 25% of the",
        "type": "Document"
      },
      {
        "id": "b1ebcba5-a4e9-425f-8635-0f225619db42",
        "metadata": {
          "chunk_id": 312,
          "document_id": "1804.08186",
          "start_idx": 114392,
          "end_idx": 114938
        },
        "page_content": " . Shared tasks are important for because they provide datasets and standardized evaluation methods that serve as benchmarks for the community. We summarize all shared tasks organized to date in sharedtasks. Generally, datasets for shared tasks have been made publicly available after the conclusion of the task, and are a good source of standardized evaluation data. However, the shared tasks to date have tended to target specific sub-problems in , and no general, broad-coverage datasets have been compiled. Widespread interest in over closely",
        "type": "Document"
      },
      {
        "id": "e9d12717-25b7-417d-8f79-3e20ae6a5008",
        "metadata": {
          "chunk_id": 331,
          "document_id": "1804.08186",
          "start_idx": 121838,
          "end_idx": 122267
        },
        "page_content": ". A number of shared tasks discussed in detail in evaluation:sharedtasks included data from social media. Examples are the TweetLID shared task on tweet held at SEPLN 2014 BIBREF388 , BIBREF389 , the data sets used in the first and second shared tasks on in code-switched data which were partially taken from Twitter BIBREF383 , BIBREF398 , and the third edition of the DSL shared task which contained two out-of-domain test sets",
        "type": "Document"
      },
      {
        "id": "555f1347-5c14-4c7a-96bb-19cbae71d588",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1909.04181",
          "start_idx": 1596,
          "end_idx": 2096
        },
        "page_content": " were expected to submit their predictions on test. The shared task predictions are expected by organizers at the level of users. The distribution has 100 tweets for each user, and so each tweet is distributed with a corresponding user id. As such, in total, the distributed training data has 2,250 users, contributing a total of 225,000 tweets. The official task test set contains 720,00 tweets posted by 720 users. For our experiments, we split the training data released by organizers into 90% TRA",
        "type": "Document"
      },
      {
        "id": "22e847ea-6806-4836-9f6b-ac659bb5cfe7",
        "metadata": {
          "chunk_id": 30,
          "document_id": "1910.11493",
          "start_idx": 11324,
          "end_idx": 11824
        },
        "page_content": "ger as an additional attribute. The model uses hard monotonic attention instead of standard soft attention, along with a dynamic programming based training scheme. The SIGMORPHON 2019 shared task received 30 submissions\u201414 for task 1 and 16 for task 2\u2014from 23 teams. In addition, the organizers' baseline systems were evaluated. Five teams participated in the first Task, with a variety of methods aimed at leveraging the cross-lingual data to improve system performance. The University of Alberta (U",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Was any variation in results observed based on language typology?",
    "llm_answer": "Yes.  Different languages showed varying results in clustering coefficient distributions, with notable differences between Spanish and Turkish, and similarities between Spanish and French.\n",
    "context": [
      {
        "id": "48489649-3b68-46a4-8b39-bf331b7883a0",
        "metadata": {
          "chunk_id": 53,
          "document_id": "1808.08780",
          "start_idx": 19773,
          "end_idx": 20259
        },
        "page_content": "ual and cross-lingual settings, we can notice that our models generally outperform the corresponding baselines. Moreover, in cases where no improvement is obtained, the differences tend to be minimal, with the exception of RG-65, but this is a very small test set for which larger variations can thus be expected. In contrast, there are a few cases where substantial gains were obtained by using our model. This is most notable for English WordSim and SimLex in the monolingual setting.",
        "type": "Document"
      },
      {
        "id": "b091c6db-75a8-4581-a92f-f676e480955b",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1702.06777",
          "start_idx": 14710,
          "end_idx": 15305
        },
        "page_content": ", i.e., mostly villages and less industrialized regions. Furthermore, we have checked that the different metrics used here lead to similar results in the analysis of the lexical variation for a representative concept and provide a reasonable description to language variation in Twitter. We remark that the small amount of tweets generated after matching the lexical variations of concepts within our automatic corpus puts a limit to the quantitative analysis, making the differences between regions small. Our work might be improved by similarly examining Spanish tweets worldwide, specially in",
        "type": "Document"
      },
      {
        "id": "7b1c0836-da82-4465-ac87-556abf447377",
        "metadata": {
          "chunk_id": 61,
          "document_id": "1909.13362",
          "start_idx": 22056,
          "end_idx": 22556
        },
        "page_content": " differences in the French results can be considered negligible because the accuracies are all near $100\\%$. The Small model performed best on Manipuri, which may suggest that reducing the number of parameters of the Base model leads to better accuracy on smaller datasets. When comparing our model with previous syllabifiers, we consider the Base model exclusively. In Table TABREF26, a side-by-side comparison of our Base model to a selection of published syllabifiers shows that Base is near state",
        "type": "Document"
      },
      {
        "id": "7fb4fdf5-fb5a-4fe7-a961-dcefe77194d4",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1911.08915",
          "start_idx": 4478,
          "end_idx": 5047
        },
        "page_content": " less similar for all the texts of the same language, provided the networks are of the same size. However, it is apparent that the distributions are different between different languages. We also compare the clustering coefficient distributions with those of the null hypothesis. The data show that the strongest differences between languages occur for the fractions of nodes with clustering coefficients 0 and 1. We build a scatter plot for these fractions for all the texts in each language. Though there is overlap between some languages, other languages are clearly",
        "type": "Document"
      },
      {
        "id": "5ab017b3-cacc-4d7d-b15d-5842c103248e",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1911.08915",
          "start_idx": 15970,
          "end_idx": 16417
        },
        "page_content": " local structure around those words, i.e, the way that they relate in the adjacency network, is particular to each language, and seems to be a key for language differentiation. In the Table TABREF14 we see the average of $\\nu ^{\\prime }_H(C)$ for each of the languages studied here. Note that for example the values are clearly different for Spanish and Turkish, similar for Spanish and French, and very different for all languages and random. Zip",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Can the approach be generalized to other technical domains as well? ",
    "llm_answer": "Models should generalize well for all domains.\n",
    "context": [
      {
        "id": "e92130c0-4293-43e2-9117-25a4adda542b",
        "metadata": {
          "chunk_id": 448,
          "document_id": "1804.08186",
          "start_idx": 167598,
          "end_idx": 168133
        },
        "page_content": " solved problem, aspects of make it an archetypal learning task with subtleties that could be tackled by future work on supervised learning, representation learning, multi-task learning, domain adaptation, multi-label classification and other subfields of machine learning. We hope that this paper can serve as a reference point for future work in the area, both for providing insight into work to date, as well as pointing towards the key aspects that merit further investigation. This research was supported in part by the Australian",
        "type": "Document"
      },
      {
        "id": "e8befb1c-2993-4885-aec6-3f5ec012da80",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1905.06906",
          "start_idx": 2449,
          "end_idx": 2912
        },
        "page_content": " in $\\textit {Electronics}$ domain. Hence, models should generalize well for all domains. Several methods have been introduced for performing Domain Adaptation. Blitzer BIBREF12 proposed Structural Correspondence Learning (SCL) which relies on pivot features between source and target domains. Pan BIBREF13 performed Domain Adaptation using Spectral Feature Alignment (SFA) that aligns features across different domains. Glorot BIBREF14 proposed Stacked Denoising",
        "type": "Document"
      },
      {
        "id": "a182fcfd-8153-41ca-b040-98606e16a2bf",
        "metadata": {
          "chunk_id": 447,
          "document_id": "1804.08186",
          "start_idx": 167092,
          "end_idx": 167598
        },
        "page_content": " set of \u201cbest practices\u201d, but these should be treated as guidelines and should always be considered in the broader context of a target application. Existing work on serves to illustrate that the scope and depth of the problem are much greater than they may first appear. In openissues, we discussed open issues in , identifying the key challenges, and outlining opportunities for future research. Far from being a solved problem, aspects of make it an archetypal learning task with subtleties that could be",
        "type": "Document"
      },
      {
        "id": "13b841cb-a2f9-4123-85d9-4271851c369a",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1909.02265",
          "start_idx": 4138,
          "end_idx": 4670
        },
        "page_content": " domains. Most approaches have difficulty scaling up with multiple domains due to the dependency of their model parameters on the underlying knowledge bases. Recently, Ramadan et al. BIBREF9 has introduced a novel approach which utilizes semantic similarity between dialogue utterances and knowledge base terms, allowing the information to be shared across domains. This method has been shown not only to scale well to multi-domain dialogues, but also outperform existing state-of-the-art models in single-domain tracking tasks. The",
        "type": "Document"
      },
      {
        "id": "97f5a7ab-6607-4500-903d-1a898035b8b3",
        "metadata": {
          "chunk_id": 73,
          "document_id": "1912.09152",
          "start_idx": 25875,
          "end_idx": 26391
        },
        "page_content": "design system can outperform a ner tagger for biomedical entities based on state-of-the-art artificial neural network technology. Thus, knowledge-based methods stand on their own merits in task resolution. But, perhaps most importantly, the other key point brought to light in this contribution is that a resource-based approach also favours a more critical stance on the dataset(s) used to evaluate system performance. With these methods, system development can go hand in hand with dataset refinement in a virtuous",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What dataset they use for evaluation?",
    "llm_answer": "16 Amazon review datasets, Stanford Sentiment TreeBank (SST-5/SST-2), and the IMDb Sentiment dataset.\n",
    "context": [
      {
        "id": "7ee3d1d0-5ed5-48f9-a7bc-3e689b448d34",
        "metadata": {
          "chunk_id": 275,
          "document_id": "1804.08186",
          "start_idx": 98870,
          "end_idx": 99381
        },
        "page_content": " evaluation in research to date. In this section, we examine the literature more closely, providing a broad overview of the evaluation metrics that have been used, as well as the experimental settings in which research has been evaluated. The most common approach is to treat the task as a document-level classification problem. Given a set of evaluation documents, each having a known correct label from a closed set of labels (often referred to as the \u201cgold-standard\u201d), and a predicted label for each document",
        "type": "Document"
      },
      {
        "id": "cd570fcf-f513-4195-a32c-953a4bcc2f4f",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1811.09786",
          "start_idx": 12018,
          "end_idx": 12427
        },
        "page_content": " concretely, we use 16 Amazon review datasets from BIBREF32 , the well-established Stanford Sentiment TreeBank (SST-5/SST-2) BIBREF33 and the IMDb Sentiment dataset BIBREF34 . All tasks are binary classification tasks with the exception of SST-5. The metric is the accuracy score. The goal of this task is to classify questions into fine-grained categories such as number or location. We use the TREC question",
        "type": "Document"
      },
      {
        "id": "c8175824-116a-471e-8ed4-47f180d3720b",
        "metadata": {
          "chunk_id": 301,
          "document_id": "1804.08186",
          "start_idx": 109612,
          "end_idx": 110179
        },
        "page_content": " different authors have used to evaluate methods. BIBREF32 emphasize this by demonstrating how the performance of a system can vary according to the data used for evaluation. This implies that comparisons of results reported by different authors may not be meaningful, as a strong result in one paper may not translate into a strong result on the dataset used in a different paper. In other areas of research, authors have proposed standardized corpora to allow for the objective comparison of different methods. Some authors have released datasets to accompany their",
        "type": "Document"
      },
      {
        "id": "1f140aea-67fe-42b4-983e-2e4e083c362a",
        "metadata": {
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "c85b69e6-0bcc-433b-b0ae-ba235722e16f",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1909.00124",
          "start_idx": 12394,
          "end_idx": 12855
        },
        "page_content": " during training. Summary statistics of the training, validation, and test data are shown in Table TABREF9. Noisy-labeled Training Datasets. For the above three domains (movie, laptop, and restaurant), we collected 2,000 reviews for each domain from the same review source. We extracted sentences from each review and assigned review's label to its sentences. Like previous work, we treat 4 or 5 stars as positive and 1 or 2 stars as negative. The data is noisy",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which regions of the United States do they consider?",
    "llm_answer": "Northeast, Southeast, Midwest, and West Coast.\n",
    "context": [
      {
        "id": "6db039d4-5797-4496-864b-4014c7c76a94",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1708.05873",
          "start_idx": 17963,
          "end_idx": 18387
        },
        "page_content": " of regions: Latin America and the Caribbean (LCN), South Asia (SAS), Sub-Saharan Africa (SSA), Europe and Central Asia (ECS), Middle East and North Africa (MEA), East Asia and the Pacific (EAS), North America (NAC). The figure shows that states in South Asia, and Latin America and the Caribbean are likely to discuss Topic 2 the most. States in South Asia and East Asia and the Pacific discuss Topic 7 the most. The figure",
        "type": "Document"
      },
      {
        "id": "f7353b21-a471-4d52-9274-e3fdfb44ac90",
        "metadata": {
          "chunk_id": 18,
          "document_id": "1904.07342",
          "start_idx": 6890,
          "end_idx": 7375
        },
        "page_content": " on a subset with geographic user information and compare results to the prior literature. In Figure FIGREF3 , we map 4-clustering results on three dimensions: predicted sentiments, latitude, and longitude. The clusters correspond to four major regions of the U.S.: the Northeast (green), Southeast (yellow), Midwest (blue), and West Coast (purple); centroids are designated by crosses. Average sentiments within each cluster confirm prior knowledge BIBREF1 : the Southeast and Midwest",
        "type": "Document"
      },
      {
        "id": "70cac370-0116-4382-b8ab-840a4f4d9549",
        "metadata": {
          "chunk_id": 33,
          "document_id": "1909.01247",
          "start_idx": 11077,
          "end_idx": 11441
        },
        "page_content": ".  U.S.A. ramane indiferent\u0103 amenin\u021b\u0103rilor Coreei de Nord. green!55!blueU.S.A. remains indifferent to North Korea's threats.  Non-geo-political locations: mountains, seas, lakes, streets, neighbourhoods, addresses, continents, regions that are not GPEs. We include regions such as Middle East, \"continents\" like Central America or East Europe. Such regions include",
        "type": "Document"
      },
      {
        "id": "75339e0c-cf77-412a-9bc4-ba5062f2b7cc",
        "metadata": {
          "chunk_id": 21,
          "document_id": "1809.02494",
          "start_idx": 8157,
          "end_idx": 8743
        },
        "page_content": " highlight the creation of models of geographical descriptors, comparing models between both subject groups, studying combinations of models of cardinal directions, and researching on geographical referring expression generation. Furthermore, insights about the semantics of geographical concepts could be inferred under a more thorough analysis. One of the inconveniences that our data sets present is the appearance of the issues described in Sec. SECREF10 . It could be necessary to filter some of the answers according to different criteria (e.g., deviation of the centroid location",
        "type": "Document"
      },
      {
        "id": "ef4f69b1-c0cd-447a-acb5-0a0449b8aad3",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1803.09745",
          "start_idx": 10068,
          "end_idx": 10527
        },
        "page_content": " both Ngrams and Twitter. The demographic data for U.S. counties comes from the 2015 American Community Survey 5-year estimates, tables DP02--Selected Social Characteristics, DP03--Selected Economic Characteristics, DP04--Selected Housing Characteristics, and DP05--Demographic and Housing Estimates, which can be found by searching online at https://factfinder.census.gov/. These tables comprise a total of 513 usable socio-demographic and economic variables",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is performance measured?",
    "llm_answer": "Performance is measured by the number of times a metric selects the best hyperparameter combination or checkpoint (out of 10 trials), and the average human-annotated score for the selected models.  For speech recognition, performance is evaluated by analyzing successful and unsuccessful transactions in logs.  Classifier performance depends on the competition, using metrics like average score INLINEFORM0, accuracy, or a custom metric.\n",
    "context": [
      {
        "id": "6a5777b6-5e74-4edf-9018-a1ed47830e3f",
        "metadata": {
          "chunk_id": 69,
          "document_id": "2002.05058",
          "start_idx": 27517,
          "end_idx": 28041
        },
        "page_content": " evaluation is then employed to identify the best choice. We evaluate the performance of each metric by how many times (out of 10) they succeeded in selecting the best hyperparameter combination or early-stopping checkpoint (out of 4) and the average human-annotated score for their selected models. The results are shown in Table 3. We can see that conventional automated metrics perform poorly and result in sub-optimal result when performing hyperparameter search and selecting the best performing checkpoints. Converting",
        "type": "Document"
      },
      {
        "id": "86e79b76-4608-4217-9d4c-7a4227430d56",
        "metadata": {
          "chunk_id": 89,
          "document_id": "2004.04696",
          "start_idx": 28641,
          "end_idx": 29104
        },
        "page_content": "ing. We run training and evaluation in parallel: we run the evaluation every 1,500 steps and store the checkpoint that performs best on a held-out validation set (more details on the data splits and our choice of metrics in the following sections). We use Google Cloud TPUs v2 for learning, and Nvidia Tesla V100 accelerators for evaluation and test. Our code uses Tensorflow 1.15 and Python 2.7. The metrics used to compare the evaluation systems vary across the",
        "type": "Document"
      },
      {
        "id": "391b62a7-0a9b-48eb-a258-07824f555a67",
        "metadata": {
          "chunk_id": 1,
          "document_id": "1601.02543",
          "start_idx": 478,
          "end_idx": 1101
        },
        "page_content": " of words for enabling a transaction. Before commercial deployment of a speech solution it is imperative to have a quantitative measure of the performance of the speech solution which is primarily based on the speech recognition accuracy of the speech engine used. Generally, the recognition performance of any speech recognition based solution is quantitatively evaluated by putting it to actual use by the people who are the intended users and then analyzing the logs to identify successful and unsuccessful transactions. This evaluation is then used to identifying any further improvement in the speech recognition based",
        "type": "Document"
      },
      {
        "id": "6f344b9c-8b75-4337-83df-5ca74a4b3538",
        "metadata": {
          "chunk_id": 39,
          "document_id": "1612.05270",
          "start_idx": 15129,
          "end_idx": 15565
        },
        "page_content": " to test the classifier. The performance of the classifier is presented using different metrics depending the competition. SemEval uses the average of score INLINEFORM0 of positive and negative labels, TASS uses the accuracy and SENTIPOLC uses a custom metric (see BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 ). We tested our framework on two kinds of datasets. On one hand, we compare our performance on three languages having well known",
        "type": "Document"
      },
      {
        "id": "5155558a-dfb3-4b8b-9585-41ac83603580",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1601.02543",
          "start_idx": 845,
          "end_idx": 1419
        },
        "page_content": " successful and unsuccessful transactions. This evaluation is then used to identifying any further improvement in the speech recognition based solution to better the overall transaction completion rates. This process of evaluation is both time consuming and expensive. For evaluation one needs to identify a set of users and also identify the set of actual usage situations and perform the test. It is also important that the set of users are able to use the system with ease meaning that even in the test conditions the performance of the system, should be good, while this",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is novel in author's approach?",
    "llm_answer": "The authors propose an AA approach based on time-aware language models that incorporate temporal changes in writing style.  Their experiments investigate whether author writing style can be recognized despite large time spans between documents.\n",
    "context": [
      {
        "id": "f83f81b0-81a4-4786-b9c1-ea4761dfbc8a",
        "metadata": {
          "chunk_id": 84,
          "document_id": "1901.02262",
          "start_idx": 23149,
          "end_idx": 23457
        },
        "page_content": " in the answer sentence; our model will be promising for controlling other styles such as length and speaking styles. Future work will involve exploring the potential of hybrid models combining extractive and abstractive approaches and improving the passage re-ranking and answerable question identification.",
        "type": "Document"
      },
      {
        "id": "fe9d1807-bfc3-4342-a4fd-ddc3f3e43b0e",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1906.10551",
          "start_idx": 3340,
          "end_idx": 3874
        },
        "page_content": "8 , for example, focused on the questions if the writing styles of authors of short texts change over time and how this affects AA. To answer these questions, the authors proposed an AA approach based on time-aware language models that incorporate the temporal changes of the writing style of authors. In one of our experiments, we focus on a similar question, namely, whether it is possible to recognize the writing style of authors, despite of large time spans between their documents. However, there are several differences between",
        "type": "Document"
      },
      {
        "id": "cb6e24d2-8294-4c70-ad6b-948a76a3531c",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1904.08386",
          "start_idx": 9489,
          "end_idx": 10035
        },
        "page_content": " we demonstrate that NLP techniques can be used to support literary analyses and obtain new insights, they also have clear limitations (e.g., in understanding abstract themes). As text representation methods become more powerful, we hope that (1) computational tools will become useful for analyzing novels with more conventional structures, and (2) literary criticism will be used as a testbed for evaluating representations. We thank the anonymous reviewers for their insightful comments. Additionally, we thank Nader Akoury, Garrett Bernstein,",
        "type": "Document"
      },
      {
        "id": "ee36d48e-34d4-4092-bb39-deba17da1cf7",
        "metadata": {
          "chunk_id": 87,
          "document_id": "1705.09665",
          "start_idx": 35882,
          "end_idx": 36460
        },
        "page_content": " the release of a new novel, while Seahawks may foster different identities during and between football seasons. Conversely, a community's type may also mediate the impact of new events. Moving beyond a static view of community identity could enable us to better understand how temporal phenomena such as linguistic change manifest across different communities, and also provide a more nuanced view of user engagement\u2014for instance, are communities more welcoming to newcomers at certain points in their lifecycle? Another important avenue of future work is to explore other ways",
        "type": "Document"
      },
      {
        "id": "bc2455c3-54fc-4ad2-be45-eedafb050fb1",
        "metadata": {
          "chunk_id": 21,
          "document_id": "1904.08386",
          "start_idx": 8660,
          "end_idx": 9186
        },
        "page_content": " specific literary criticism about a single narrative. There has been other computational work that focuses on just a single book or a small number of books, much of it focused on network analysis: BIBREF35 extract character social networks from Alice in Wonderland, while BIBREF36 recover social networks from 19th century British novels. BIBREF37 disentangles multiple narrative threads within the novel Infinite Jest, while BIBREF7 provides several automated statistical methods for close reading and test them on the award",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How large is the Dialog State Tracking Dataset?",
    "llm_answer": "Over 16,000 dialogues.\n",
    "context": [
      {
        "id": "283aa65d-7a8d-4558-a098-e74f04906b59",
        "metadata": {
          "chunk_id": 8,
          "document_id": "1909.11833",
          "start_idx": 3127,
          "end_idx": 3573
        },
        "page_content": " than the previously best model GLAD, with only 22% of the number of parameters. On DSTC2 dataset, SIM achieves comparable performance with previous best models with only 19% of the model size. As outlined in BIBREF9, the dialogue state tracking task is formulated as follows: at each turn of dialogue, the user's utterance is semantically decoded into a set of slot-value pairs. There are two types of slots. Goal slots indicate the category, e.",
        "type": "Document"
      },
      {
        "id": "5be978d8-80e9-4008-af14-95236c164f41",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1707.05853",
          "start_idx": 6842,
          "end_idx": 7275
        },
        "page_content": " provided for the second Dialog State Tracking Challenge (DSTC2) BIBREF22 that consists of user interactions with an SDS in the restaurant domain. It encompasses 1612, 506, 1117 dialogs for training, development and testing, respectively. Every dialog turn is annotated with its dialog state encompassing the three goals for area (7 values), food (93 values) and price range (5 values) and 8 requestable slots, e.g. phone and address",
        "type": "Document"
      },
      {
        "id": "dbdf77a1-c0d3-4158-8ffc-8e4ffb4189f8",
        "metadata": {
          "chunk_id": 34,
          "document_id": "2002.01359",
          "start_idx": 14455,
          "end_idx": 14981
        },
        "page_content": "ased into a natural conversation by crowd workers. We ensure that the annotations for the dialogue state and slots generated by the simulator are preserved and hence need no other annotation. We omit details for brevity: please refer to BIBREF19 for more details. The entire dataset consists of over 16K dialogues spanning multiple domains. Overall statistics of the dataset and comparison with other datasets can be seen in Table TABREF9. Figure FIGREF8 shows the details of the distribution of dialogue lengths across single",
        "type": "Document"
      },
      {
        "id": "91599b4e-ac74-44cb-b6df-0a21ee4fe652",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1909.05855",
          "start_idx": 13044,
          "end_idx": 13583
        },
        "page_content": " more data with the same resources. Furthermore, it is important to note that this entire procedure preserves all other annotations obtained from the simulator including the dialogue state. Hence, no further annotation is needed. With over 16000 dialogues in the training set, the Schema-Guided Dialogue dataset is the largest publicly available annotated task-oriented dialogue dataset. The annotations include the active intents and dialogue states for each user utterance and the system actions for every system utterance. We have a few",
        "type": "Document"
      },
      {
        "id": "480f6084-285d-4431-b27c-fffcf1464cbb",
        "metadata": {
          "chunk_id": 86,
          "document_id": "2002.11893",
          "start_idx": 32022,
          "end_idx": 32535
        },
        "page_content": " filled, and all cross-domain informable slots are filled by real values. Result Analysis: During the evaluation, we initialized the user state of the simulator using the previous gold user state. The input to the simulator is the gold system dialogue acts. We used joint state accuracy (percentage of exact matching) to evaluate user state prediction and F1 score to evaluate the prediction of user dialogue acts. The results are presented in Table TABREF31. We can observe that the performance on complex dialog",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What dataset is used for train/test of this method?",
    "llm_answer": "The training data consists of all previous rounds of data, plus the Wikipedia Toxic Comments data.  Test sets are used to measure robustness.\n",
    "context": [
      {
        "id": "65333987-2358-483b-95cd-3ff028ff6a96",
        "metadata": {
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      },
      {
        "id": "1f140aea-67fe-42b4-983e-2e4e083c362a",
        "metadata": {
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "be946cc9-7e3f-4c04-8aea-2232b0448e27",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1908.06083",
          "start_idx": 12013,
          "end_idx": 12455
        },
        "page_content": " the newly collected adversarial data from the \u201cbreak it\" round. The training data consists of all previous rounds of data, so that model $A_i$ is trained on all rounds $n$ for $n \\le i$, as well as the Wikipedia Toxic Comments data. We split each round of data into train, validation, and test partitions. The validation set is used for hyperparameter selection. The test sets are used to measure how robust we are to new adversarial attacks",
        "type": "Document"
      },
      {
        "id": "5e2d6809-3d96-4de0-86eb-68fbb8e0ff01",
        "metadata": {
          "chunk_id": 72,
          "document_id": "1909.02635",
          "start_idx": 27869,
          "end_idx": 27993
        },
        "page_content": " testbed supported by the National Science Foundation. Thanks as well to the anonymous reviewers for their helpful comments.",
        "type": "Document"
      },
      {
        "id": "029e181a-b1c7-4323-aace-9d81ed6cdf66",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1905.07464",
          "start_idx": 3930,
          "end_idx": 4425
        },
        "page_content": " labels, referred to as Training-22, with gold standard annotations. Two test sets of 57 and 66 drug labels, referred to as Test Set 1 and 2 respectively, with gold standard annotations are used to evaluate participating systems. As Training-22 is a relatively small dataset, we additionally utilize an external dataset with 180 annotated drug labels dubbed NLM-180 BIBREF5 (more later). We provide summary statistics about these datasets in Table TABREF3 . Test Set 1 closely resembles Training",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much is the gap between using the proposed objective and using only cross-entropy objective?",
    "llm_answer": "The context states that the proposed mixed objective converges quickly and outperforms the purely cross entropy model in practice.  However, it does not quantify the gap.\n",
    "context": [
      {
        "id": "43a61273-a978-4472-bc1a-17211c4bb4a0",
        "metadata": {
          "chunk_id": 29,
          "document_id": "1711.00106",
          "start_idx": 9499,
          "end_idx": 9980
        },
        "page_content": " span produced by the model and the ground truth answer span. We note that there is a disconnect between the cross entropy optimization objective and the evaluation metrics. For example, suppose we are given the answer estimates $A$ and $B$ , neither of which match the ground truth positions. However, $A$ has an exact string match with the ground truth answer whereas $B$ does not. The cross entropy objective penalizes $A$ and $B$ equally, despite the former being correct under",
        "type": "Document"
      },
      {
        "id": "f9b7b346-0478-42b1-beb4-d4fe5bb06c7f",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1711.00106",
          "start_idx": 16878,
          "end_idx": 17429
        },
        "page_content": " without mixing in the cross entropy loss, it is extremely difficult to learn the policy. When we combine the cross entropy loss with the reinforcement learning objective, we find that the model initially performs worse early on as it begins policy learning from scratch (shown in Figure 4 ). However, with the addition of cross entropy loss, the model quickly learns a reasonable policy and subsequently outperforms the purely cross entropy model (shown in Figure 4 ). Figure 5 compares predictions by and by the baseline on the development set. Both",
        "type": "Document"
      },
      {
        "id": "9f56f933-2052-46c6-96e2-9bbf7f17222b",
        "metadata": {
          "chunk_id": 30,
          "document_id": "1711.00106",
          "start_idx": 9782,
          "end_idx": 10226
        },
        "page_content": " cross entropy objective penalizes $A$ and $B$ equally, despite the former being correct under both evaluation metrics. In the less extreme case where $A$ does not have exact match but has some degree of word overlap with the ground truth, the F1 metric still prefers $A$ over $B$ despite its wrongly predicted positions. We encode this preference using reinforcement learning, using the F1 score as the reward function. Let $\\hat{s_t} \\sim _t$",
        "type": "Document"
      },
      {
        "id": "4ad42d7a-ccd0-4508-a379-0921f0f8a734",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1711.00106",
          "start_idx": 1258,
          "end_idx": 1826
        },
        "page_content": " traditional cross entropy based training it is no better than the span \u201chistory\u201d. To address this problem, we propose a mixed objective that combines traditional cross entropy loss over positions with a measure of word overlap trained with reinforcement learning. We obtain the latter objective using self-critical policy learning in which the reward is based on word overlap between the proposed answer and the ground truth answer. Our mixed objective brings two benefits: (i) the reinforcement learning objective encourages answers that are textually similar to the",
        "type": "Document"
      },
      {
        "id": "30bb2ad9-c0d7-4437-8d80-2b2003658065",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1711.00106",
          "start_idx": 1618,
          "end_idx": 2146
        },
        "page_content": " brings two benefits: (i) the reinforcement learning objective encourages answers that are textually similar to the ground truth answer and discourages those that are not; (ii) the cross entropy objective significantly facilitates policy learning by encouraging trajectories that are known to be correct. The resulting objective is one that is both faithful to the evaluation metric and converges quickly in practice. In addition to our mixed training objective, we extend the Dynamic Coattention Network (DCN) by BIBREF0 with a",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many domains of ontologies do they gather data from?",
    "llm_answer": "17\n",
    "context": [
      {
        "id": "86f1ea1a-ccd6-490e-82a2-7c29633090b9",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1909.03242",
          "start_idx": 7530,
          "end_idx": 7943
        },
        "page_content": " of instances, as well as labels per domain, are shown in Table TABREF34 and label names in Table TABREF43 in the appendix. The dataset is split into a training part (80%) and a development and testing part (10% each) in a label-stratified manner. Note that the domains vary in the number of labels, ranging from 2 to 27. Labels include both straight-forward ratings of veracity (`correct', `incorrect'), but also",
        "type": "Document"
      },
      {
        "id": "f5a9cd70-b791-4dc7-862f-0231955c6f73",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1811.09529",
          "start_idx": 6119,
          "end_idx": 6486
        },
        "page_content": "T-Box). As a result we selected 5 ontologies with 234 competency questions in total. Table TABREF8 summarizes our dataset size and source of each ontology. The Software Ontology (SWO) BIBREF5 is included because its set of CQs is of substantial size and it was part of Ren et al.'s set of analysed CQs. The CQ sets of Dem@Care BIBREF8 and OntoDT BIBREF9 were included",
        "type": "Document"
      },
      {
        "id": "d2d9878e-9861-49f7-956e-0f6e90355fd2",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1909.00754",
          "start_idx": 15273,
          "end_idx": 15680
        },
        "page_content": ", WoZ2.0 BIBREF19 . It consists of 1,200 dialogues from the restaurant reservation domain with three pre-defined slots: food, price range, and area. Since the name slot rarely occurs in the dataset, it is not included in our experiments, following previous literature BIBREF3 , BIBREF20 . Our model is also tested on the multi-domain dataset, MultiWoZ BIBREF9 . It has a more complex ontology with 7 domains",
        "type": "Document"
      },
      {
        "id": "b40273cc-66d5-4c5d-ba0c-2f86b0cdd780",
        "metadata": {
          "chunk_id": 64,
          "document_id": "1903.00172",
          "start_idx": 22362,
          "end_idx": 22720
        },
        "page_content": " Kumar. 2018. Enriching domain ontologies using question-answer datasets. In Proc. CoDS-COMAD '18, pages 329\u2013332. Swarnadeep Saha, Harinder Pal, et al. 2017. Bootstrapping for numerical open ie. In Proc. ACL '17, pages 317\u2013323. Denis Savenkov, Wei-Lwun Lu, Jeff Dalton, and Eugene Agichtein. 2015. Relation extraction from community generated question-answer",
        "type": "Document"
      },
      {
        "id": "3035ded2-a530-4e6f-a74e-7931d8085645",
        "metadata": {
          "chunk_id": 14,
          "document_id": "1909.05855",
          "start_idx": 5622,
          "end_idx": 6170
        },
        "page_content": " of multiple services per domain, are representative of these scale-related challenges. Furthermore, our evaluation sets contain many services, and consequently slots, which are not present in the training set, to help evaluate model performance on unseen services. The 17 domains (`Alarm' domain not included in training) present in our dataset are listed in Table TABREF5. We create synthetic implementations of a total of 34 services or APIs over these domains. Our simulator framework interacts with these services to generate dialogue outlines",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what is the practical application for this paper?",
    "llm_answer": "The paper demonstrates how a corpus can be used for social science research by analyzing budget speeches, spoken corpus features, and language representations for decision making.  It also discusses applications in image synthesis, image and video editing, and text-to-image synthesis.\n",
    "context": [
      {
        "id": "bdd954e9-5c12-4bf7-a0b0-01024f085697",
        "metadata": {
          "chunk_id": 11,
          "document_id": "2003.06279",
          "start_idx": 4620,
          "end_idx": 5004
        },
        "page_content": " present Zipf's Law BIBREF21 and the analysis of linguistic properties of natural and unknown texts BIBREF22, BIBREF23. Applications of network science in text mining and text classification encompasses applications in semantic analysis BIBREF24, BIBREF25, BIBREF26, BIBREF27, authorship attribution BIBREF28, BIBREF29 and stylometry BIBREF28, BIBREF30, BIBREF31. Here we focus in the",
        "type": "Document"
      },
      {
        "id": "a184125b-3ed9-4081-82ad-3db56e275dd6",
        "metadata": {
          "chunk_id": 24,
          "document_id": "1910.09399",
          "start_idx": 9710,
          "end_idx": 10171
        },
        "page_content": " the challenges it presents followed by a section dedicated to core concepts of GANs and enhancements to the original GAN model. In addition, the paper covers the review of two types of applications: (1) unconstrained applications of image synthesis such as super-resolution, image inpainting, etc., and (2) constrained image synthesis applications, namely image-to-image, text-to-image, and sketch-to image, and also discusses image and video editing using GAN",
        "type": "Document"
      },
      {
        "id": "7c67a5f4-f3c9-4e85-9b34-5a70447475d9",
        "metadata": {
          "chunk_id": 15,
          "document_id": "1708.04557",
          "start_idx": 6608,
          "end_idx": 7186
        },
        "page_content": ". The data is organized in a way that facilitates analysis for substantive questions of interest to social scientists. Every row in the data set is one contribution with columns containing information on the following variables: In the previous section, we have explained the structure of the database. In the following three sections we demonstrate how the data can be used for social science research. We do this by demonstrating three different applications. In the first application, we analyze the budget speeches of all finance ministers from 1922 to 2008. Budget speeches",
        "type": "Document"
      },
      {
        "id": "b4e947f8-edf6-4ed3-b5f7-15fab14ca732",
        "metadata": {
          "chunk_id": 41,
          "document_id": "2001.08051",
          "start_idx": 15968,
          "end_idx": 16545
        },
        "page_content": " above description it appears that the corpus can be effectively used in many research directions. The spoken corpus features non-native speech recordings in real classrooms and, consequently, peculiar phenomena appear and can be investigated. Phonological and cross-language interference requires specific approaches for accurate acoustic modelling. Moreover, for coping with cross-language interference it is important to consider alternative ways to represent specific words (e.g. words of two languages with the same graphemic representation). Table , extracted from BIBREF",
        "type": "Document"
      },
      {
        "id": "53153648-9bf5-4510-b134-2e6324e09ad5",
        "metadata": {
          "chunk_id": 54,
          "document_id": "1910.02789",
          "start_idx": 21493,
          "end_idx": 22038
        },
        "page_content": " language representations are beneficial: Natural use-case: Information contained in both generic and task-specific textual corpora may be highly valuable for decision making. This case assumes the state can either be easily described using natural language or is already in a natural language state. This includes examples such as user-based domains, in which user profiles and comments are part of the state, or the stock market, in which stocks are described by analysts and other readily available text. 3D physical environments such as VizD",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What's the method used here?",
    "llm_answer": "A method using Bayes' rule for end-to-end training, with a shared ratio of posteriors.\n",
    "context": [
      {
        "id": "1f140aea-67fe-42b4-983e-2e4e083c362a",
        "metadata": {
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "bb20c9d7-8f67-4d62-908b-cc9d3f8ede8d",
        "metadata": {
          "chunk_id": 20,
          "document_id": "2002.11268",
          "start_idx": 6009,
          "end_idx": 6300
        },
        "page_content": " target domain posterior as: with $k(X) = p_{\\psi }(X) / p_{\\tau }(X)$ shared by all hypotheses $W$, and the ratio $P_{\\tau }(W) / {P_{\\psi }(W)}$ (really a probablity mass ratio) giving the proposed method its name. In essence, this model is just an application of Bayes' rule to end-to-end",
        "type": "Document"
      },
      {
        "id": "98b66a9a-dc24-4ad4-a4cc-871fc6622b9a",
        "metadata": {
          "chunk_id": 147,
          "document_id": "1910.02339",
          "start_idx": 42553,
          "end_idx": 42777
        },
        "page_content": " self #19 ) ( + 1 #20 ) ( if #17 0 #21 ) ( lambda1 #22 ) ( sort a ) ( len a ) ( / #25 2 ) ( deref #24 #26 ) ( invoke1 #23 #27 ) ( - #28 c ) ( * #14 #29 ) ( - b d ) ( - b d ) ( * #31 #32 ) ( + #30 #33 ) ( sqrt #34 ) ( floor #",
        "type": "Document"
      },
      {
        "id": "f6ef3246-a143-4199-8db8-244ec1ff2780",
        "metadata": {
          "chunk_id": 45,
          "document_id": "1706.04815",
          "start_idx": 12072,
          "end_idx": 12345
        },
        "page_content": " (1-y_i)\\log (1-\\hat{g}_i)]$$   (Eq. 23)  where $k$ is the number of passages. $y_i \\in \\lbrace 0,1\\rbrace $ denotes a label. $y_i=1$ means $P_i$ is the correct passage, otherwise $y_i=0$ . The evident extraction part is trained by minimizing joint objective functions:  $$",
        "type": "Document"
      },
      {
        "id": "ed732085-2297-4071-a20e-de82f8d08cb1",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1912.01046",
          "start_idx": 11421,
          "end_idx": 11905
        },
        "page_content": " tool'. Here we observe different types of questions, such as \"why\", \"how\", \"what\", and \"where\", and can see why the answers may involve multiple steps. Some questions that the worked paraphrased were in the \"yes/no\" style, however our answer segments then provide an explanation to these questions. Each answer segment was extracted from an image editing tutorial video that involved multiple steps and procedures to produce a final image, which can partially be seen in FIGREF1. The",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does their method outperform state-of-the-art OOD detection?",
    "llm_answer": "More than 17 points in Precision@K on average.\n",
    "context": [
      {
        "id": "f32f5342-8738-4933-bc60-b82d0b4e581d",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1905.10247",
          "start_idx": 12345,
          "end_idx": 12839
        },
        "page_content": " turns to foster further research. In the presence of OOD utterances, our method outperforms state-of-the-art dialog models equipped with an OOD detection mechanism by a large margin \u2014 more than 17 points in Precision@K on average \u2014 while minimizing performance trade-off on in-domain test data. The detailed analysis sheds light on the difficulty of optimizing context-independent OOD detection and justifies the necessity of context-aware OOD handling models. We plan to explore other ways of",
        "type": "Document"
      },
      {
        "id": "8cb1ede2-34ca-4260-9698-5914a802abb5",
        "metadata": {
          "chunk_id": 33,
          "document_id": "1905.10247",
          "start_idx": 12036,
          "end_idx": 12546
        },
        "page_content": " superior stability nicely contrasts with the high sensitivity of AE-HCN-Indep with regard to threshold values as shown in Table TABREF25 . We proposed a novel OOD detection method that does not require OOD data without any restrictions by utilizing counterfeit OOD turns in the context of a dialog. We also release new dialog datasets which are three publicly available dialog corpora augmented with natural OOD turns to foster further research. In the presence of OOD utterances, our method outperforms state",
        "type": "Document"
      },
      {
        "id": "2c9ddbdf-a686-47ec-95f8-2183a0dc2bb5",
        "metadata": {
          "chunk_id": 28,
          "document_id": "1905.10247",
          "start_idx": 10154,
          "end_idx": 10567
        },
        "page_content": " on in-domain training data. AE-HCN(-CNN) outperforms HCN on Test-OOD by a large margin about 17(20) points on average while keeping the minimum performance trade-off compared to Test. Interestingly, AE-HCN-CNN has even better performance than HCN on Test, indicating that, with the CNN encoder, counterfeit OOD augmentation acts as an effective regularization. In contrast, AE-HCN-Indep failed to robustly detect",
        "type": "Document"
      },
      {
        "id": "f2a9d62b-a0be-4105-bc70-7db139645a69",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1905.10247",
          "start_idx": 11300,
          "end_idx": 11764
        },
        "page_content": " Table TABREF25 shows the search for the best threshold value for AE-HCN-Indep on the bAbI6 task when given actual OOD utterances (which is highly unrealistic for the real-world scenario). Note that the best performance achieved at 9 is still not as good as that of AE-HCN(-CNN). This implies that we can perform better OOD detection by jointly considering other context features. Finally, we conduct a sensitivity analysis by varying counterfeit OOD probabilities",
        "type": "Document"
      },
      {
        "id": "534cfd9c-cd60-412d-8aac-75bb347bd99e",
        "metadata": {
          "chunk_id": 27,
          "document_id": "1905.10247",
          "start_idx": 9866,
          "end_idx": 10321
        },
        "page_content": " are multiple actions that are appropriate for a given dialog context, we use per-utterance Precision@K as performance metric. We also report f1-score for OOD detection to measure the balance between precision and recall. The performances of HCN on Test-OOD are about 15 points down on average from those on Test, showing the detrimental impact of OOD utterances to such models only trained on in-domain training data. AE-HCN(-CNN) outperforms HCN on Test",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are dilated convolutions?",
    "llm_answer": "Dilated convolutions skip some input values so that the convolution kernel is applied over a larger area than its own, increasing the receptive field without increasing the number of parameters.\n",
    "context": [
      {
        "id": "f94650ef-53dc-4e11-b629-fcfdb46fc50e",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1811.07684",
          "start_idx": 4383,
          "end_idx": 4825
        },
        "page_content": " increase in computational cost yielded by larger receptive fields. Dilated convolutions skip some input values so that the convolution kernel is applied over a larger area than its own. The network therefore operates on a larger scale, without the downside of increasing the number of parameters. The receptive field $r$ of a network made of stacked convolutions indeed reads: $r = \\sum _i d_i (s_i - 1),$  where $d_i$ refers to the dilation",
        "type": "Document"
      },
      {
        "id": "bf2b6c3d-a9ff-402a-b850-dc60b65063d5",
        "metadata": {
          "chunk_id": 30,
          "document_id": "1910.08210",
          "start_idx": 11739,
          "end_idx": 12233
        },
        "page_content": " visual inputs with height $H$, width $W$, and $_$ channels. Let $$ denote a convolution layer. Let + and * symbols denote element-wise addition and multiplication operations that broadcast over spatial dimensions. We first modulate visual features using text features: Unlike FiLM, we additionally modulate text features using visual features: The output of the layer consists of the sum of the modulated features $$, as well as a max-pooled summary $$ over this sum across spatial dimensions.",
        "type": "Document"
      },
      {
        "id": "c154e6d7-fb05-48e8-8511-176297de6058",
        "metadata": {
          "chunk_id": 12,
          "document_id": "1811.07684",
          "start_idx": 4658,
          "end_idx": 5030
        },
        "page_content": " (s_i - 1),$  where $d_i$ refers to the dilation rate ( $d_i=1$ for normal convolutions) and $s_i$ the filter size of the $i^{th}$ layer. Additionally, causal convolutions kernels ensure a causal ordering of input frames: the prediction emitted at time $t$ only depends on previous time stamps. It allows to reduce the latency at inference time. As mentioned in BIBREF10 ,",
        "type": "Document"
      },
      {
        "id": "bcec2287-2b96-4e24-8913-9319aa89fbb6",
        "metadata": {
          "chunk_id": 42,
          "document_id": "2001.06888",
          "start_idx": 15719,
          "end_idx": 16083
        },
        "page_content": ",k)}], \\dots , [c_{(n,0)}, c_{(n,1)}, \\dots , c_{(n,k)}] \\rangle $ and in order to apply one dimensional convolution, it is required to be in a fixed length. $k$ shows the fixed length of the character sequence representing each word. Rather than using the one-hot representation of characters, a randomly initialized (uniform distribution) embedding layer is used",
        "type": "Document"
      },
      {
        "id": "f56c33a6-09f3-4dc1-ba73-df925e22ddea",
        "metadata": {
          "chunk_id": 25,
          "document_id": "1603.04513",
          "start_idx": 9584,
          "end_idx": 9897
        },
        "page_content": " {R}^{d}$ denotes the embedding of word $w_i$ ; a convolution layer uses sliding filters to extract local features of that sentence. The filter width $l$ is a parameter. We first concatenate the initialized embeddings of $l$ consecutive words ( $\\mathbf {w}_{i-l+1},\n\\ldots , \\mathbf {w}_i$ ) as $\\mathbf {c}_i\\in",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what are the three methods presented in the paper?",
    "llm_answer": "The provided texts describe multiple methods, but not three *specific* methods are presented in all the contexts.  The answer is not found in the context.\n",
    "context": [
      {
        "id": "1f140aea-67fe-42b4-983e-2e4e083c362a",
        "metadata": {
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "154cf29a-d954-4c7f-b9fa-d4ae87469e7c",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1908.08419",
          "start_idx": 3629,
          "end_idx": 4094
        },
        "page_content": " summarized as follows: The rest of this paper is organized as follows. Section SECREF2 briefly reviews the related work on CWS and active learning. Section SECREF3 presents an active learning method for CWS. We experimentally evaluate our proposed method in Section SECREF4 . Finally, Section SECREF5 concludes the paper and envisions on future work. In past decades, researches on CWS have a long history and various methods have been proposed BIBREF13 , BIBREF14",
        "type": "Document"
      },
      {
        "id": "d3c674d0-6609-4993-8841-31cdccf45c3a",
        "metadata": {
          "chunk_id": 6,
          "document_id": "1906.10551",
          "start_idx": 2469,
          "end_idx": 3019
        },
        "page_content": " contribute to the further development of this young research field. Based on the proposed properties, we investigate the applicability of 12 existing AV approaches on three self-compiled corpora, where each corpus involves a specific challenge. The rest of this paper is structured as follows. Section SECREF2 discusses the related work that served as an inspiration for our analysis. Section SECREF3 comprises the proposed criteria and properties to characterize AV methods. Section SECREF4 describes the methodology, consisting of the used corpora",
        "type": "Document"
      },
      {
        "id": "fdbc37b7-f710-4304-9174-eb1a34a2aa14",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1909.02265",
          "start_idx": 6691,
          "end_idx": 7196
        },
        "page_content": " on. Next, Section SECREF3 presents experimental settings and results. Finally, Section SECREF4 concludes the paper and gives some directions for future work. In this section, we present briefly two methods that we use in our experiments which have been mentioned in the previous section. The first method is the Sequicity framework and the second one is the state-of-the-art multi-domain dialogue state tracking approach. Figure FIGREF1 shows the architecture of the Sequicity framework as described in B",
        "type": "Document"
      },
      {
        "id": "70e1ea66-1171-4546-a976-9b33995d48d1",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1908.07816",
          "start_idx": 4017,
          "end_idx": 4509
        },
        "page_content": " relate. It is the first time such an approach is designed with consideration for the human judges. Our main goal is to increase the objectivity of the results and reduce judges' mistakes due to out-of-context dialogs they have to evaluate. The rest of the paper unfolds as follows. Section SECREF2 discusses some related work. In Section SECREF3, we give detailed description of the methodology. We present experimental results and some analysis in Section SECREF4. The paper is concluded in",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what datasets did the authors use?",
    "llm_answer": "The authors used two datasets: one of 220 sentences from the scientific literature (SCI) and a dataset of 19 novels (novel-9).\n",
    "context": [
      {
        "id": "6b7f6ca6-24d5-4144-9cfd-92be2de31f39",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "1f18073b-4721-4651-900e-6b076387c7f1",
        "metadata": {
          "chunk_id": 31,
          "document_id": "1709.02271",
          "start_idx": 11255,
          "end_idx": 11734
        },
        "page_content": " We begin by introducing the datasets (Section SECREF15 ), followed by detailing the featurization methods (Section SECREF17 ), the experiments (Section SECREF22 ), and finally reporting results (Section SECREF26 ). The statistics for the three datasets used in the experiments are summarized in Table TABREF16 . novel-9. This dataset was compiled by F&H14: a collection of 19 novels by 9 nineteenth century British and American authors in the Project Gutenberg. To compare to F&",
        "type": "Document"
      },
      {
        "id": "91df3232-acdc-442e-89f1-92d8ec6a1b84",
        "metadata": {
          "chunk_id": 75,
          "document_id": "1912.09152",
          "start_idx": 26669,
          "end_idx": 26983
        },
        "page_content": " a true Gold Standard Dataset. We thank three anonymous reviewers of our manuscript for their careful reading and their many insightful comments and suggestions. We have made our best in providing a revised version of the manuscript that reflects their suggestions. Any remaining errors are our own responsability.",
        "type": "Document"
      },
      {
        "id": "42c11738-fcba-4949-8188-2176465fd847",
        "metadata": {
          "chunk_id": 69,
          "document_id": "1909.05855",
          "start_idx": 26527,
          "end_idx": 26688
        },
        "page_content": "-of-the-art results. The authors thank Guan-Lin Chao for help with model design and implementation, and Amir Fayazi and Maria Wang for help with data collection.",
        "type": "Document"
      },
      {
        "id": "032b9bed-6a38-456d-b02a-dea6816fa084",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1910.12618",
          "start_idx": 7204,
          "end_idx": 7727
        },
        "page_content": " section 5 concludes this paper and gives insight on future possible work. In order to prove the consistency of our work, experiments have been conducted on two data sets, one for France and the other for the UK. In this section details about the text and time series data are given, as well as the major preprocessing steps. Three types of time series are considered in our work: national net electricity consumption (also referred as load or demand), national temperature and wind speed. The load data sets were retrieved",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much performance improvements they achieve on SQuAD?",
    "llm_answer": "F1 score improved by 11.8 points in Japanese SQuAD.\n",
    "context": [
      {
        "id": "1776b1fe-f186-4ae6-bf04-55423160b124",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1912.10435",
          "start_idx": 1471,
          "end_idx": 1945
        },
        "page_content": " augmented the SQUAD2.0 dataset with additional backtranslated examples. This augmented dataset will be publicly available on our github BIBREF4 on the completion of this course. After performing hyperparameter tuning, we ensembled our two best networks to get F1 and EM scores of 82.317 and 79.442 respectively. The experiments took around 300 GPU hours to train. The SQUAD2.0 creators proposed this dataset as a means for networks to actually understand the text they were",
        "type": "Document"
      },
      {
        "id": "e70ded19-51b8-409d-9291-9ca407115473",
        "metadata": {
          "chunk_id": 74,
          "document_id": "2001.11268",
          "start_idx": 29425,
          "end_idx": 29920
        },
        "page_content": " use of original SQuAD domains to enrich our data. This was done in order to save computing resources, as an addition of 100 SQuAD domains resulted in training time increases of two hours, depending on various other parameter settings. Adjusted parameters include increased batch size, and decreased maximal context length in order to reduce training time. With this paper we aimed to explore state-of-the-art NLP methods to advance systematic review (semi)automation. Both of the presented fine",
        "type": "Document"
      },
      {
        "id": "c5fe73d6-e999-475d-b197-a08b7335b420",
        "metadata": {
          "chunk_id": 78,
          "document_id": "1902.00821",
          "start_idx": 23708,
          "end_idx": 24133
        },
        "page_content": " evaluation of SQuAD 2.0, CoQA script reports turn-level Exact Match (EM) and F1 scores for all turns in all dialogues. EM requires the answers to have exact string match with human annotated answer spans. F1 score is the averaged F1 scores of individual answers, which is typically higher than EM and is the major metric. As shown in Table 3 , BERT+Pre-tuning has significant performance gains over many baselines. To answer",
        "type": "Document"
      },
      {
        "id": "e33aa25a-7549-4017-8437-c59c54f4f99c",
        "metadata": {
          "chunk_id": 77,
          "document_id": "1902.00821",
          "start_idx": 23287,
          "end_idx": 23708
        },
        "page_content": " better results. We fine-tune 6 epochs, though most runs converged just within 3 epochs due to the pre-trained/tuned weights of BERT. Results are reported as averages of 3 runs of fine-tuning (3 different random seeds for tuning batch generation). To be consistent with existing research, we leverage the same evaluation script from CoQA. Similar to the evaluation of SQuAD 2.0, CoQA script reports turn-level Exact Match",
        "type": "Document"
      },
      {
        "id": "7aebd943-9de3-496d-ade9-691c986446f9",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1909.00361",
          "start_idx": 20160,
          "end_idx": 20623
        },
        "page_content": "en}}$) is higher than previous work (Back-Translation BIBREF10), when using SimpleMatch to extract original passage span could obtain competitive of even larger improvements. In Japanese SQuAD, the F1 score improved by 9.6 in BIBREF10 using NMT attention, while we obtain larger improvement with 11.8 points demonstrating the effectiveness of the proposed method. BERT with pre-trained SQuAD weights yields the best performance among these systems, as it does not",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the baseline?",
    "llm_answer": "There are multiple baselines described.  Baseline 1 uses smooth L1 distance, Baseline 2 uses a PCA-based acoustic similarity measure, Baseline 3 uses a nonlinear dynamical systems-based complexity measure, and others.  More baselines are also mentioned but not fully described.\n",
    "context": [
      {
        "id": "94c43990-47e3-478c-a415-088eeac582b5",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1804.08782",
          "start_idx": 13990,
          "end_idx": 14427
        },
        "page_content": " number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 .  For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose",
        "type": "Document"
      },
      {
        "id": "555b8b94-8d95-496e-ac65-47922482cfe0",
        "metadata": {
          "chunk_id": 40,
          "document_id": "1911.13066",
          "start_idx": 15067,
          "end_idx": 15591
        },
        "page_content": " dataset. Table TABREF23 presents results of baseline and all variations of our experiments. We focus our discussion on F1-score as accuracy is often misleading for dataset with unbalanced class distribution. However, for completeness sake, all measures are reported. It is observed from the results that baseline model performs worst among all the experiments. The reason behind this degradation in performance can be traced back to the nature of the texts in the datasets (i.e., datasets used in original paper of baseline",
        "type": "Document"
      },
      {
        "id": "f297d78b-0fdd-4eee-ae9e-9e75a2e66130",
        "metadata": {
          "chunk_id": 140,
          "document_id": "2001.02380",
          "start_idx": 51470,
          "end_idx": 51956
        },
        "page_content": " baselines. The left, middle and right panels in Figure FIGREF40 correspond to measurements when all signals are included, only cases contained entirely in the head EDUs shown to the model, and only DMs, respectively. The scenario on the left is rather unreasonable and is included only for completeness: here the model is also penalized for not detecting signals such as lexical chains, part of which is outside the units that the model is being shown. An example of such a case can be",
        "type": "Document"
      },
      {
        "id": "3747343d-0517-442a-b7ba-66d01be55b6d",
        "metadata": {
          "chunk_id": 40,
          "document_id": "2004.01878",
          "start_idx": 14526,
          "end_idx": 14915
        },
        "page_content": " normal distribution $\\mathcal {N}(0, 1)$. In summary, the following four baselines are designed: ANRES_Sing_R: randomly initializing the states for each single trading day. ANRES_Sing_Z: initializing the states as zeros for each single trading day. ANRES_Seq_R: randomly initializing the first states for each trading sequence only. ANRES_Seq_Z: initializing the first states as zeros for",
        "type": "Document"
      },
      {
        "id": "c9d62915-21d1-42b6-a16c-8d9d28de6e0c",
        "metadata": {
          "chunk_id": 91,
          "document_id": "1902.06843",
          "start_idx": 35139,
          "end_idx": 35416
        },
        "page_content": "s by -0.27. Baselines: To test the efficacy of our multi-modal framework for detecting depressed users, we compare it against existing content, content-network, and image-based models (based on the aforementioned general image feature, facial presence, and facial expressions.)",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the network architecture?",
    "llm_answer": "The network architecture consists of five main modules: an input module, an attention module, a reasoning module, a modeling module, and an output module.\n",
    "context": [
      {
        "id": "768e8e84-3e84-4d1a-a7d5-fe2929dc50c3",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1909.08859",
          "start_idx": 4603,
          "end_idx": 5140
        },
        "page_content": " with an explicit reasoning module that acts on entity-specific relational memory units. Fig. FIGREF4 shows an overview of the network architecture. It consists of five main modules: An input module, an attention module, a reasoning module, a modeling module, and an output module. Note that the question answering tasks we consider here are multimodal in that while the context is a procedural text, the question and the multiple choice answers are composed of images. Input Module extracts vector representations of inputs at different",
        "type": "Document"
      },
      {
        "id": "a63db46a-a278-4a94-955b-22100018819f",
        "metadata": {
          "chunk_id": 15,
          "document_id": "2002.11402",
          "start_idx": 5576,
          "end_idx": 5982
        },
        "page_content": " as explained above before passing to the network. Architecture is built using tensorflow/keras. Coding inspiration taken from BERT-keras and for CRF layer keras-contrib. If one is more comfortable in pytorch there are many examples available on github, but pytorch-bert-crf-ner is better for an easy start. We used BERT-Multilingual model so that we can train and fine-tune the same model for other Indian",
        "type": "Document"
      },
      {
        "id": "99db0986-6552-4425-8d58-52b78652feef",
        "metadata": {
          "chunk_id": 23,
          "document_id": "1912.13109",
          "start_idx": 9357,
          "end_idx": 9891
        },
        "page_content": " Vo et al and Mathur et al. This was also used as a baseline model but we did not get appreciable performance on such architecture due to FC networks not being able to capture local and long term dependencies. Convolution based architectures: Architecture and hyperparameter choices were chosen from the past study Deon on the subject. We were able to boost the performance as compared to only FC based network but we noticed better performance from architectures that are suitable to sequences such as text messages or any timeseries",
        "type": "Document"
      },
      {
        "id": "f8aa35a5-33ed-44a7-9431-e80aad4db4c7",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1901.00570",
          "start_idx": 6305,
          "end_idx": 6842
        },
        "page_content": " networks are (1) content analysis, where the textual content of each post is analyzed using natural language processing to identify the topic or the sentiment of the authors. (2) Network structure analysis, where the relation between the users are described in a tree structure for the follower-followee patterns, or in a graph structure for friendship and interaction patterns. These patterns can be used to know the political preference of people prior to elections. (3) Behavioural analysis of each user including sentiment, response",
        "type": "Document"
      },
      {
        "id": "e375aa62-3579-4b11-b617-a18e536baf90",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1710.09340",
          "start_idx": 13393,
          "end_idx": 13706
        },
        "page_content": "ES-UDC (FFI2014-51978-C2-2-R) and ANSWER-ASAP (TIN2017-85160-C2-1-R) projects from MINECO, and from Xunta de Galicia (ED431B 2017/01). We provide more details of the neural network architecture used in this paper, which is taken from Qi2017. The model consists of two blocks of 2-layered bidirectional long short-",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What does recurrent deep stacking network do?",
    "llm_answer": "RDSN stacks and concatenates the outputs of previous frames into the input features of the current frame, utilizing phoneme information in previous frames as additional inputs to the raw features.\n",
    "context": [
      {
        "id": "2f7972fa-1a4d-4d6b-98b4-84409bf1ccf2",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1612.04675",
          "start_idx": 1527,
          "end_idx": 2023
        },
        "page_content": " Model into a hybrid model consisted of an Acoustic Model and a simple N-gram Language Model on phoneme level. In the next section, we will explain the framework of RDSN and tricks to compress the outputs. Then we will show the experimental results and make a conclusion. As indicated in its name, Recurrent Deep Stacking Network stacks and concatenates the outputs of previous frames into the input features of the current frame. If we view acoustic models in ASR systems as functions projecting",
        "type": "Document"
      },
      {
        "id": "fb6346f5-839b-468f-ad9c-627cda0e0d7b",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1612.04675",
          "start_idx": 3695,
          "end_idx": 4220
        },
        "page_content": " other output dimensions that have the same corresponding monophone. This process compresses the dimension from thousands to about forty. Compressing the output dimensions enables us to add multiple recurrent outputs and keep the input vector dense. Originated from a similar idea as of RDSN, a BiPass Stacking Network (BPSN) takes as input both conventional features and the outputs of previous frames. But the way BiPass Stacking Network generates the representations of previous frames is through a two-pass scheme similar",
        "type": "Document"
      },
      {
        "id": "fc110dd1-6154-4f41-b3b6-5927cf43e6a5",
        "metadata": {
          "chunk_id": 3,
          "document_id": "1612.04675",
          "start_idx": 1221,
          "end_idx": 1729
        },
        "page_content": " The models in this paper belong to the back-end methods. Inspired by recent progresses in Natural Language Processing BIBREF5 , we proposed the Recurrent Deep Stacking Network (RDSN) and successfully applied it to Speech Enhancement tasks. RDSN utilizes the phoneme information in previous frames as additional inputs to the raw features. From another perspective of view, this framework transformed the Acoustic Model into a hybrid model consisted of an Acoustic Model and a simple N-gram Language Model on",
        "type": "Document"
      },
      {
        "id": "0f37b767-0b69-4a55-8d22-a0bbd10e7fcb",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1612.09113",
          "start_idx": 815,
          "end_idx": 1357
        },
        "page_content": " Recurrent Neural Network (RNN), where junior tasks in the hierarchy are supervised on inner layers and the parameters are jointly optimised during training. Joint optimisation within a hierarchical network acts as a form of regularisation in two ways: first, it forces the network to learn general representations within the parameters of the shared hidden layers BIBREF0 ; second, there is a penalty on the supervised junior layers for forming a representation and making predictions that are inconsistent with senior tasks. Intuitively, we",
        "type": "Document"
      },
      {
        "id": "e867bd06-b1e5-4c57-886d-0cdae68690fd",
        "metadata": {
          "chunk_id": 71,
          "document_id": "1811.09786",
          "start_idx": 23088,
          "end_idx": 23555
        },
        "page_content": " Recurrent Networks (RCRN), a new recurrent architecture and encoder for a myriad of NLP tasks. RCRN operates in a novel controller-listener architecture which uses RNNs to learn the gating functions of another RNN. We apply RCRN to a potpourri of NLP tasks and achieve promising/highly competitive results on all tasks and 26 benchmark datasets. Overall findings suggest that our controller-listener architecture is more effective than stacking RNN layers. Moreover,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the reward model for the reinforcement learning appraoch?",
    "llm_answer": "A balanced reward maximizing the probability of finding an answer to the generated question within the input paragraph but not inside the source sentence.  Also, a QA-based metric reward, computing the number of overlapping words between the ground truth answer and the predicted answer.\n",
    "context": [
      {
        "id": "183bb656-289a-48cd-b6b5-d4742728a3d9",
        "metadata": {
          "chunk_id": 55,
          "document_id": "1702.03274",
          "start_idx": 20589,
          "end_idx": 21098
        },
        "page_content": " for the system to continue to learn autonomously using reinforcement learning (RL). With RL, each turn receives a measurement of goodness called a reward; the agent explores different sequences of actions in different situations, and makes adjustments so as to maximize the expected discounted sum of rewards, which is called the return, denoted $G$ . For optimization, we selected a policy gradient approach BIBREF34 , which has been successfully applied to dialog systems BIBREF35 , robotics BIBREF36 , and",
        "type": "Document"
      },
      {
        "id": "03aa9283-b1df-468b-9d20-25c705e046f1",
        "metadata": {
          "chunk_id": 52,
          "document_id": "1911.03350",
          "start_idx": 19179,
          "end_idx": 19669
        },
        "page_content": " the models appear to learn to exploit the weaknesses of the reward source. To overcome this issue, we propose to use a balanced reward: thus maximizing the probability of finding an answer to the generated question within the input paragraph but not inside the source sentence. In our experiments, we follow the approach proposed by BIBREF18, BIBREF29, considering a mixed loss $L_{ml+rl}$ which combines supervised and reinforcement learning schemes: where the maximum likelihood $L_{ml}$",
        "type": "Document"
      },
      {
        "id": "bd9e9b93-3c0e-4e65-b722-95063c9d5e85",
        "metadata": {
          "chunk_id": 43,
          "document_id": "1909.00542",
          "start_idx": 16697,
          "end_idx": 17106
        },
        "page_content": " for the reward function used by a reinforcement learning system. Reinforcement learning gives promising results, especially in human evaluations made on the runs submitted to BioASQ 6b. This year we introduced very small changes to the runs using reinforcement learning, and will aim to explore more complex reinforcement learning strategies and more complex neural models in the policy and value estimators.",
        "type": "Document"
      },
      {
        "id": "c4150e82-068a-4001-8f6f-84912b4f92e1",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1911.03350",
          "start_idx": 3876,
          "end_idx": 4372
        },
        "page_content": " datasets BIBREF15, BIBREF16. For instance, BIBREF15 proposed a reinforcement learning setup trained using a QA-based metric reward: given a paragraph and an answer, the model first generates questions; then, the paragraph and the corresponding generated questions are given to a pre-trained QA model which predicts an answer; finally, the reward is computed as the number of overlapping words between the ground truth answer and the predicted answer. For an extensive evalution of models trained",
        "type": "Document"
      },
      {
        "id": "413bd7c3-9f2b-4bd5-8c48-fea8f8176948",
        "metadata": {
          "chunk_id": 101,
          "document_id": "1902.00330",
          "start_idx": 34293,
          "end_idx": 34753
        },
        "page_content": " state to help disambiguation. In the last few years, reinforcement learning has emerged as a powerful tool for solving complex sequential decision-making problems. It is well known for its great success in the game field, such as Go BIBREF37 and Atari games BIBREF38 . Recently, reinforcement learning has also been successfully applied to many natural language processing tasks and achieved good performance BIBREF12 , BIBREF39 , BIBREF5 . Feng et al. BIBREF",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Does this paper propose a new task that others can try to improve performance on?",
    "llm_answer": "Yes.\n",
    "context": [
      {
        "id": "5f60f35c-49af-411d-9dd5-a573069248ea",
        "metadata": {
          "chunk_id": 16,
          "document_id": "1707.03764",
          "start_idx": 6168,
          "end_idx": 6712
        },
        "page_content": " out to be detrimental to performance, when compared to the simpler system that we have described in Section SECREF2 . We divide our attempts according to the different ways we attempted to enhance performance: manipulating the data itself (adding more, and changing preprocessing), using a large variety of features, and changing strategies in modelling the problem by using different algorithms and paradigms. All reported results are on the PAN 2017 training data using five-fold cross-validation, unless otherwise specified. We extended the",
        "type": "Document"
      },
      {
        "id": "8872bd93-adad-4031-938f-20000b3a274d",
        "metadata": {
          "chunk_id": 4,
          "document_id": "1703.09684",
          "start_idx": 1620,
          "end_idx": 2106
        },
        "page_content": " task they solve; 2) We propose two new evaluation metrics that compensate for forms of dataset bias; 3) We balance the number of yes/no object presence detection questions to assess whether a balanced distribution can help algorithms learn better; and 4) We introduce absurd questions that force an algorithm to determine if a question is valid for a given image. We then use the new dataset to re-train and evaluate both baseline and state-of-the-art VQA algorithms. We found that our",
        "type": "Document"
      },
      {
        "id": "050846c9-f0a0-4415-9bbd-5f88dc3fbd5c",
        "metadata": {
          "chunk_id": 10,
          "document_id": "1910.06748",
          "start_idx": 3889,
          "end_idx": 4391
        },
        "page_content": " benchmarks on our Twitter datasets, our proposed model consistently achieves new state-of-the-art results with an improvement of 5% in accuracy and F1 score and a competitive inference time. The rest of this paper is structured as follows. After a background review in the next section, we will present our Twitter dataset in Section SECREF3. Our novel LID algorithm will be the topic of Section SECREF4. We will then present and analyze some experiments we conducted with our algorithm in Section SEC",
        "type": "Document"
      },
      {
        "id": "e70ded19-51b8-409d-9291-9ca407115473",
        "metadata": {
          "chunk_id": 74,
          "document_id": "2001.11268",
          "start_idx": 29425,
          "end_idx": 29920
        },
        "page_content": " use of original SQuAD domains to enrich our data. This was done in order to save computing resources, as an addition of 100 SQuAD domains resulted in training time increases of two hours, depending on various other parameter settings. Adjusted parameters include increased batch size, and decreased maximal context length in order to reduce training time. With this paper we aimed to explore state-of-the-art NLP methods to advance systematic review (semi)automation. Both of the presented fine",
        "type": "Document"
      },
      {
        "id": "7dc51bb5-3f7c-4d38-9e9b-f23eb7183c20",
        "metadata": {
          "chunk_id": 5,
          "document_id": "1810.12885",
          "start_idx": 1760,
          "end_idx": 2277
        },
        "page_content": " something complicated when they read'. If a system performs as well as a human on a task, the conclusion shouldn't immediately be that the task is too easy, it should more subtly be that new datasets are then needed to see if the inference mechanisms hold up, where the creation of the datasets can be based based on an explicitly stated intuition that humans may rely on more than pattern matching. It is a hypothesis at this point in the Introduction, that systems doing well on earlier datasets won't also do well",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is their dataset?",
    "llm_answer": "Not found in the context.  There are several dataset sizes mentioned, but no single \"their\" dataset size is given.\n",
    "context": [
      {
        "id": "863ccf9b-daae-4116-a5c7-6f288a192c35",
        "metadata": {
          "chunk_id": 40,
          "document_id": "2002.01030",
          "start_idx": 16339,
          "end_idx": 16786
        },
        "page_content": ". Every instance in the dataset is longer than 200 characters. For each article, the following metadata is available: article type, article text, article title, article date, and article label (fake or real). Table TABREF12 shows the type and size of the articles for the real and fake categories. As mentioned in Section SECREF2, one of the recent well-known datasets, is provided by BIBREF15. BIBREF15 introduced a new large dataset called LIAR,",
        "type": "Document"
      },
      {
        "id": "16ce7b0e-ac71-4bb7-86e3-3c6cd0c325a4",
        "metadata": {
          "chunk_id": 37,
          "document_id": "1910.06748",
          "start_idx": 14072,
          "end_idx": 14512
        },
        "page_content": "860,000 tweets. Large-scale dataset: Once again, we increased tenfold the number of tweets per language, and kept the 14 languages that had sufficient tweets in our initial 900 million tweet corpus. This gives us a dataset where each language has 700,000 tweets in its training set, 300,000 tweets in its validation set, and 300,000 tweets in its testing set, for a total 18,200,000 tweets. Referring to Table TABREF6, this dataset includes",
        "type": "Document"
      },
      {
        "id": "a6d83161-2349-4cf5-9fe8-5d09bab08be0",
        "metadata": {
          "chunk_id": 19,
          "document_id": "1909.00107",
          "start_idx": 7348,
          "end_idx": 7859
        },
        "page_content": ". The dataset comprises of approximately 0.83 million words with 10,000 unique entries of which 0.5 million is used for training (0.24m for dev and 88k for test). Cancer Couples Interaction Dataset: This dataset was gathered as part of a observational study of couples coping with advanced cancer. Advanced cancer patients and their spouse caregivers were recruited from clinics and asked to interact with each other in two structured discussions: neutral discussion and cancer related. Interactions were audio-",
        "type": "Document"
      },
      {
        "id": "2d3bdfec-4ed5-456d-bd82-023fad798858",
        "metadata": {
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "e4f83679-3b36-4a2a-a8aa-3fb723baa0d2",
        "metadata": {
          "chunk_id": 67,
          "document_id": "1909.03242",
          "start_idx": 25291,
          "end_idx": 25806
        },
        "page_content": " for bigrams. This means that the model has an easy time succeeding for instances where the claims are short, where specific topics tend to co-occur with certain veracities, and where evidence documents are highly informative. Instances with longer, more complex claims where evidence is ambiguous remain challenging. We present a new, real-world fact checking dataset, currently the largest of its kind. It consists of 34,918 claims collected from 26 fact checking websites, rich metadata and 10 retrieved evidence",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What task do they evaluate on?",
    "llm_answer": "Task 1 and Task 2.\n",
    "context": [
      {
        "id": "b3036530-5228-47f4-b1f0-9a1687f209da",
        "metadata": {
          "chunk_id": 11,
          "document_id": "1709.10217",
          "start_idx": 4141,
          "end_idx": 4657
        },
        "page_content": ", in which only the released data can be used for training and developing. The other is an open evaluation that allow to collect external data for training and developing. For task 1, we use F1-score as evaluation metric. For the task-oriented dialogue systems, the best way for evaluation is to use the online human-computer dialogue. After finishing an online human-computer dialogue with a dialogue system, the human then manually evaluate the system by using the metrics of user satisfaction degree, dialogue flu",
        "type": "Document"
      },
      {
        "id": "5155558a-dfb3-4b8b-9585-41ac83603580",
        "metadata": {
          "chunk_id": 2,
          "document_id": "1601.02543",
          "start_idx": 845,
          "end_idx": 1419
        },
        "page_content": " successful and unsuccessful transactions. This evaluation is then used to identifying any further improvement in the speech recognition based solution to better the overall transaction completion rates. This process of evaluation is both time consuming and expensive. For evaluation one needs to identify a set of users and also identify the set of actual usage situations and perform the test. It is also important that the set of users are able to use the system with ease meaning that even in the test conditions the performance of the system, should be good, while this",
        "type": "Document"
      },
      {
        "id": "192051cd-ed1a-43c8-bddb-135c16fd249a",
        "metadata": {
          "chunk_id": 40,
          "document_id": "1906.03538",
          "start_idx": 15736,
          "end_idx": 16245
        },
        "page_content": " range of natural language understanding tasks. Human performance provides us with an estimate of the best achievable results on datasets. We use human annotators to measure human performance for each task. We randomly sample 10 claims from the test set, and instruct two expert annotators to solve each of T1 to T4. We perform evaluations on four different subtasks in our dataset. In all of the following evaluations, the systems are given the two pools of perspectives INLINEFORM0 and evidences INLINEFORM1",
        "type": "Document"
      },
      {
        "id": "394d2284-fca8-49dc-9618-30140de06765",
        "metadata": {
          "chunk_id": 17,
          "document_id": "1709.10217",
          "start_idx": 5875,
          "end_idx": 6394
        },
        "page_content": " the same. The tester then begin to converse to each system. A dialogue is finished if the system successfully returns the information which the user inquires or the number of dialogue turns is larger than 30 for a single task. For building the dialogue systems of participants, we release an example set of complete user intent and three data files of flight, train and hotel in JSON format. There are five evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the",
        "type": "Document"
      },
      {
        "id": "08e94578-b4df-43f1-b96f-4db730b4d02a",
        "metadata": {
          "chunk_id": 26,
          "document_id": "1607.06025",
          "start_idx": 10088,
          "end_idx": 10607
        },
        "page_content": "REF37 , the evaluation metrics of text-generating models fall into three categories: manual evaluation, automatic evaluation metrics, task-based evaluation. In evaluation based on human judgment each generated textual example is inspected manually. The automatic evaluation metrics, like ROUGE, BLEU and METEOR, compare human texts and generated texts. BIBREF38 shows METEOR has the strongest correlation with human judgments in image description evaluation. The last category is task-based evaluation, where the impact",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many feature maps are generated for a given triple?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "bc65b431-b023-4cd6-abc2-c5c3b113f76a",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1804.03396",
          "start_idx": 12619,
          "end_idx": 13113
        },
        "page_content": " we prefer the articles with more golden triples involved by assuming that Wikipedia articles with more annotated triples are more informative and better annotated. Therefore, we figure out the distribution of the number of golden triples in articles and decide to discard the articles with less than 6 golden triples (account for about 80%). After this step, we obtain about 200K articles and 1.4M triples with 636 relation types. Query and Answer Assignment. For each golden triple $\\lbrace e",
        "type": "Document"
      },
      {
        "id": "744fb749-c97c-40ae-a9e8-74678eb7f692",
        "metadata": {
          "chunk_id": 34,
          "document_id": "1804.03396",
          "start_idx": 12119,
          "end_idx": 12619
        },
        "page_content": "64 DBpedia relations. Then we gather all the DBpedia triples with the first entity is corresponding to one of the above 3.5M articles and the relation is one of the projected 148 relations. After the same clipping process as above and removing the repetitive triples, we obtain 394K additional triples in 302K existing Wikipedia articles. Distillation. Since our benchmark is for IE, we prefer the articles with more golden triples involved by assuming that Wikipedia articles with more annotated tri",
        "type": "Document"
      },
      {
        "id": "fc6c703c-9353-4961-9131-2157054fc4e9",
        "metadata": {
          "chunk_id": 32,
          "document_id": "1804.03396",
          "start_idx": 11405,
          "end_idx": 11801
        },
        "page_content": ". We discard the triples with multiple entities (or values) for $e_j$ (account for about 6%, e.g., a book may have multiple authors). Besides, we discard the triples with any word in $e_j$ out of the corresponding article (account for about 50%). After this step, we obtain about 3.5M articles and 9M triples with 636 relation types. Incorporating DBpedia. Unlike WikiData, DBpedia is constructed",
        "type": "Document"
      },
      {
        "id": "bfb96416-e35d-4cd9-869d-86e84ed2d839",
        "metadata": {
          "chunk_id": 36,
          "document_id": "1802.05574",
          "start_idx": 14230,
          "end_idx": 14702
        },
        "page_content": " 5 judgements this excludes 9 triples. We indicate not only the pair-wise inter-annotator agreement but also the number of annotators who have judged a triple to be correct. For example, at the 40% agreement level at least 3 annotators have agreed that a triple is true. The figure separates the results by extractor and by data source. We see that as expected the amount of triples agreed to as correct grows larger as we relax the requirement for agreement. For example,",
        "type": "Document"
      },
      {
        "id": "6d1369fc-48a0-487c-a56e-42406e2f6030",
        "metadata": {
          "chunk_id": 35,
          "document_id": "1802.05574",
          "start_idx": 13921,
          "end_idx": 14410
        },
        "page_content": " calculated as typical where Correct Triples are treated as true positives. On average, 3.1 triples were extracted per sentence. Figure 1 shows the performance of extractors in terms of precision as inter-annotator agreement decreases. In this figure, we look only at agreement on triples where the majority agree that the triple is correct. Furthermore, to ease comparison, we only consider triples with 5 judgements this excludes 9 triples. We indicate not only the pair-wise inter-annot",
        "type": "Document"
      }
    ]
  }
]