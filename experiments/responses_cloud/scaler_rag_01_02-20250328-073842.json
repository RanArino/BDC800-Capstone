[
  {
    "query": "What is the seed lexicon?",
    "llm_answer": "A seed lexicon consists of 15 positive words and 15 negative words, as shown in Section SECREF27.  It also contains positive and negative predicates.\n",
    "context": [
      {
        "id": "4f29e322-79a3-46ce-ae67-a6e4db8acd6c",
        "metadata": {
          "vector_store_key": "1909.00694-3",
          "chunk_id": 9,
          "document_id": "1909.00694",
          "start_idx": 5556,
          "end_idx": 6191
        },
        "page_content": "We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types. The seed lexicon matches (1) the latter event but (2) not the former event, and (3) their discourse relation type is Cause or Concession. If the discourse relation type is Cause, the former event is given the same score as the latter. Likewise, if the discourse relation type is Concession, the former event is given the opposite of the latter's score. They are used as reference scores during training.",
        "type": "Document"
      },
      {
        "id": "70fb0c12-b950-414d-8251-a26ab029fea3",
        "metadata": {
          "vector_store_key": "1909.00694-3",
          "chunk_id": 14,
          "document_id": "1909.00694",
          "start_idx": 8485,
          "end_idx": 9093
        },
        "page_content": "From this sentence, we extracted the event pair of \u201c\u91cd\u5927\u306a\u5931\u6557\u3092\u72af\u3059\u201d ([I] make a serious mistake) and \u201c\u4ed5\u4e8b\u3092\u30af\u30d3\u306b\u306a\u308b\u201d ([I] get fired), and tagged it with Cause. We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL.",
        "type": "Document"
      },
      {
        "id": "676eaae5-9b96-464f-85a2-e2c3e36a8bf6",
        "metadata": {
          "vector_store_key": "1909.00694-3",
          "chunk_id": 8,
          "document_id": "1909.00694",
          "start_idx": 4872,
          "end_idx": 5556
        },
        "page_content": "We assume that we can automatically extract discourse-tagged event pairs, $(x_{i1}, x_{i2})$ ($i=1, \\cdots $) from the raw corpus. We refer to $x_{i1}$ and $x_{i2}$ as former and latter events, respectively. As shown in Figure FIGREF1, we limit our scope to two discourse relations: Cause and Concession. The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation.",
        "type": "Document"
      },
      {
        "id": "3e132b47-5829-432c-8f11-490e2e039631",
        "metadata": {
          "vector_store_key": "1909.00694-3",
          "chunk_id": 10,
          "document_id": "1909.00694",
          "start_idx": 5556,
          "end_idx": 6188
        },
        "page_content": "They are used as reference scores during training. The seed lexicon matches neither the former nor the latter event, and their discourse relation type is Cause. We assume the two events have the same polarities. The seed lexicon matches neither the former nor the latter event, and their discourse relation type is Concession. We assume the two events have the reversed polarities. Using AL, CA, and CO data, we optimize the parameters of the polarity function $p(x)$. We define a loss function for each of the three types of event pairs and sum up the multiple loss functions. We use mean squared error to construct loss functions.",
        "type": "Document"
      },
      {
        "id": "b250212f-1fdd-46c8-a02b-a25329f07e80",
        "metadata": {
          "vector_store_key": "1909.00694-2",
          "chunk_id": 25,
          "document_id": "1909.00694",
          "start_idx": 14808,
          "end_idx": 15407
        },
        "page_content": "\u6012\u308b (get angry), \u60b2\u3057\u3044 (be sad), \u5bc2\u3057\u3044 (be lonely), \u6016\u3044 (be scared), \u4e0d\u5b89 (feel anxious), \u6065\u305a\u304b\u3057\u3044 (be embarrassed), \u5acc (hate), \u843d\u3061\u8fbc\u3080 (feel down), \u9000\u5c48 (be bored), \u7d76\u671b (feel hopeless), \u8f9b\u3044 (have a hard time), \u56f0\u308b (have trouble), \u6182\u9b31 (be depressed), \u5fc3\u914d (be worried), and \u60c5\u3051\u306a\u3044 (be sorry). The dimension of the embedding layer was 256. The embedding layer was initialized with the word embeddings pretrained using the Web corpus. The input sentences were segmented into words by the morphological analyzer Juman++. The vocabulary size was 100,000. The number of hidden layers was 2. The dimension of hidden units was 256.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the results?",
    "llm_answer": "The models performed considerably better than random baselines.  Performance gaps with semi-supervised counterparts were less than 7%.  BiGRU encoder trained with AL+CA+CO dataset achieved the highest score.\n",
    "context": [
      {
        "id": "8af794fa-6366-4443-bd4b-33d94a832648",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 17,
          "document_id": "1909.00694",
          "start_idx": 9543,
          "end_idx": 10209
        },
        "page_content": "For the evaluation, we used the test set of the ACP Corpus. The model output was classified as positive if $p(x) > 0$ and negative if $p(x) \\le 0$. As for ${\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. GRU BIBREF16 is a recurrent neural network sequence encoder. BiGRU reads an input sequence forward and backward and the output is the concatenation of the final forward and backward hidden states. BERT BIBREF17 is a pre-trained multi-layer bidirectional Transformer BIBREF18 encoder. Its output is the final hidden state corresponding to the special classification tag ([CLS]). For the details of ${\\rm Encoder}$, see Sections SECREF30.",
        "type": "Document"
      },
      {
        "id": "9d58f368-1832-4cfc-bd63-3a1added7a13",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 18,
          "document_id": "1909.00694",
          "start_idx": 9543,
          "end_idx": 10328
        },
        "page_content": "For the details of ${\\rm Encoder}$, see Sections SECREF30. We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\mathcal {L}_{\\rm AL}$, $\\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$, $\\mathcal {L}_{\\rm ACP}$, and $\\mathcal {L}_{\\rm ACP} + \\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$. Table TABREF23 shows accuracy. As the Random baseline suggests, positive and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon and output the corresponding label (or the reverse of it for negation) if the event's predicate is in the seed lexicon.",
        "type": "Document"
      },
      {
        "id": "bb96f53b-98f0-4811-a691-0975f908cb31",
        "metadata": {
          "vector_store_key": "1909.00694-2",
          "chunk_id": 26,
          "document_id": "1909.00694",
          "start_idx": 15102,
          "end_idx": 15722
        },
        "page_content": "The dimension of hidden units was 256. The optimizer was Momentum SGD BIBREF21. The mini-batch size was 1024. We ran 100 epochs and selected the snapshot that achieved the highest score for the dev set. We used a Japanese BERT model pretrained with Japanese Wikipedia. The input sentences were segmented into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam BIBREF19.",
        "type": "Document"
      },
      {
        "id": "7be35e82-24d5-49fc-b11f-3511ee8e7a66",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 21,
          "document_id": "1909.00694",
          "start_idx": 12285,
          "end_idx": 12964
        },
        "page_content": "As the results shown in Table TABREF24 demonstrate, our method is effective when labeled data are small. The result of hyperparameter optimization for the BiGRU encoder was as follows: As the CA and CO pairs were equal in size (Table TABREF16), $\\lambda _{\\rm CA}$ and $\\lambda _{\\rm CO}$ were comparable values. $\\lambda _{\\rm CA}$ was about one-third of $\\lambda _{\\rm CO}$, and this indicated that the CA pairs were noisier than the CO pairs. A major type of CA pairs that violates our assumption was in the form of \u201c$\\textit {problem}_{\\text{negative}}$ causes $\\textit {solution}_{\\text{positive}}$\u201d: . (\u60aa\u3044\u3068\u3053\u308d\u304c\u3042\u308b, \u3088\u304f\u306a\u308b\u3088\u3046\u306b\u52aa\u529b\u3059\u308b) (there is a bad point, [I] try to improve [it])",
        "type": "Document"
      },
      {
        "id": "439bc21a-fce8-4632-bebc-fabe777f5a61",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 19,
          "document_id": "1909.00694",
          "start_idx": 11154,
          "end_idx": 11804
        },
        "page_content": "The Random+Seed baseline made use of the seed lexicon and output the corresponding label (or the reverse of it for negation) if the event's predicate is in the seed lexicon. We can see that the seed lexicon itself had practically no impact on prediction. The models in the top block performed considerably better than the random baselines. The performance gaps with their (semi-)supervised counterparts, shown in the middle block, were less than 7%. This demonstrates the effectiveness of discourse relation-based label propagation. Comparing the model variants, we obtained the highest score with the BiGRU encoder trained with the AL+CA+CO dataset.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are relations used to propagate polarity?",
    "llm_answer": "Discourse relations like Cause (same polarity propagated) and Concession (reverse polarity propagated) are used to propagate polarity from seed predicates to other events.\n",
    "context": [
      {
        "id": "98208804-0c2a-4c11-a428-8db83c52e8df",
        "metadata": {
          "vector_store_key": "1909.00694-3",
          "chunk_id": 3,
          "document_id": "1909.00694",
          "start_idx": 1831,
          "end_idx": 2473
        },
        "page_content": "Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event. We trained the models using a Japanese web corpus. Given the minimum amount of supervision, they performed well.",
        "type": "Document"
      },
      {
        "id": "3343ac98-c8a4-4e4d-8dad-6e9b67f9076f",
        "metadata": {
          "vector_store_key": "1909.00694-0",
          "chunk_id": 2,
          "document_id": "1909.00694",
          "start_idx": 1127,
          "end_idx": 1831
        },
        "page_content": "As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., \u201cto be glad\u201d is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$.",
        "type": "Document"
      },
      {
        "id": "676eaae5-9b96-464f-85a2-e2c3e36a8bf6",
        "metadata": {
          "vector_store_key": "1909.00694-3",
          "chunk_id": 8,
          "document_id": "1909.00694",
          "start_idx": 4872,
          "end_idx": 5556
        },
        "page_content": "We assume that we can automatically extract discourse-tagged event pairs, $(x_{i1}, x_{i2})$ ($i=1, \\cdots $) from the raw corpus. We refer to $x_{i1}$ and $x_{i2}$ as former and latter events, respectively. As shown in Figure FIGREF1, we limit our scope to two discourse relations: Cause and Concession. The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation.",
        "type": "Document"
      },
      {
        "id": "3e132b47-5829-432c-8f11-490e2e039631",
        "metadata": {
          "vector_store_key": "1909.00694-3",
          "chunk_id": 10,
          "document_id": "1909.00694",
          "start_idx": 5556,
          "end_idx": 6188
        },
        "page_content": "They are used as reference scores during training. The seed lexicon matches neither the former nor the latter event, and their discourse relation type is Cause. We assume the two events have the same polarities. The seed lexicon matches neither the former nor the latter event, and their discourse relation type is Concession. We assume the two events have the reversed polarities. Using AL, CA, and CO data, we optimize the parameters of the polarity function $p(x)$. We define a loss function for each of the three types of event pairs and sum up the multiple loss functions. We use mean squared error to construct loss functions.",
        "type": "Document"
      },
      {
        "id": "d2152400-c335-44b5-a40d-6a46163f3024",
        "metadata": {
          "vector_store_key": "1909.00694-0",
          "chunk_id": 1,
          "document_id": "1909.00694",
          "start_idx": 752,
          "end_idx": 1513
        },
        "page_content": "Learning affective events is challenging because, as the examples above suggest, the polarity of an event is not necessarily predictable from its constituent words. Combined with the unbounded combinatorial nature of language, the non-compositionality of affective polarity entails the need for large amounts of world knowledge, which can hardly be learned from small annotated data. In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., \u201cto be glad\u201d is positive).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is the Japanese data?",
    "llm_answer": "The context mentions a training set of 0.6 million events.\n",
    "context": [
      {
        "id": "bb96f53b-98f0-4811-a691-0975f908cb31",
        "metadata": {
          "vector_store_key": "1909.00694-2",
          "chunk_id": 26,
          "document_id": "1909.00694",
          "start_idx": 15102,
          "end_idx": 15722
        },
        "page_content": "The dimension of hidden units was 256. The optimizer was Momentum SGD BIBREF21. The mini-batch size was 1024. We ran 100 epochs and selected the snapshot that achieved the highest score for the dev set. We used a Japanese BERT model pretrained with Japanese Wikipedia. The input sentences were segmented into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam BIBREF19.",
        "type": "Document"
      },
      {
        "id": "3c294228-6d28-4acb-a236-44c26fd622ef",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 20,
          "document_id": "1909.00694",
          "start_idx": 11804,
          "end_idx": 12506
        },
        "page_content": "Comparing the model variants, we obtained the highest score with the BiGRU encoder trained with the AL+CA+CO dataset. BERT was competitive but its performance went down if CA and CO were used in addition to AL. We conjecture that BERT was more sensitive to noises found more frequently in CA and CO. Contrary to our expectations, supervised models (ACP) outperformed semi-supervised models (ACP+AL+CA+CO). This suggests that the training set of 0.6 million events is sufficiently large for training the models. For comparison, we trained the models with a subset (6,000 events) of the ACP dataset. As the results shown in Table TABREF24 demonstrate, our method is effective when labeled data are small.",
        "type": "Document"
      },
      {
        "id": "b250212f-1fdd-46c8-a02b-a25329f07e80",
        "metadata": {
          "vector_store_key": "1909.00694-2",
          "chunk_id": 25,
          "document_id": "1909.00694",
          "start_idx": 14808,
          "end_idx": 15407
        },
        "page_content": "\u6012\u308b (get angry), \u60b2\u3057\u3044 (be sad), \u5bc2\u3057\u3044 (be lonely), \u6016\u3044 (be scared), \u4e0d\u5b89 (feel anxious), \u6065\u305a\u304b\u3057\u3044 (be embarrassed), \u5acc (hate), \u843d\u3061\u8fbc\u3080 (feel down), \u9000\u5c48 (be bored), \u7d76\u671b (feel hopeless), \u8f9b\u3044 (have a hard time), \u56f0\u308b (have trouble), \u6182\u9b31 (be depressed), \u5fc3\u914d (be worried), and \u60c5\u3051\u306a\u3044 (be sorry). The dimension of the embedding layer was 256. The embedding layer was initialized with the word embeddings pretrained using the Web corpus. The input sentences were segmented into words by the morphological analyzer Juman++. The vocabulary size was 100,000. The number of hidden layers was 2. The dimension of hidden units was 256.",
        "type": "Document"
      },
      {
        "id": "8af794fa-6366-4443-bd4b-33d94a832648",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 17,
          "document_id": "1909.00694",
          "start_idx": 9543,
          "end_idx": 10209
        },
        "page_content": "For the evaluation, we used the test set of the ACP Corpus. The model output was classified as positive if $p(x) > 0$ and negative if $p(x) \\le 0$. As for ${\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. GRU BIBREF16 is a recurrent neural network sequence encoder. BiGRU reads an input sequence forward and backward and the output is the concatenation of the final forward and backward hidden states. BERT BIBREF17 is a pre-trained multi-layer bidirectional Transformer BIBREF18 encoder. Its output is the final hidden state corresponding to the special classification tag ([CLS]). For the details of ${\\rm Encoder}$, see Sections SECREF30.",
        "type": "Document"
      },
      {
        "id": "7be35e82-24d5-49fc-b11f-3511ee8e7a66",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 21,
          "document_id": "1909.00694",
          "start_idx": 12285,
          "end_idx": 12964
        },
        "page_content": "As the results shown in Table TABREF24 demonstrate, our method is effective when labeled data are small. The result of hyperparameter optimization for the BiGRU encoder was as follows: As the CA and CO pairs were equal in size (Table TABREF16), $\\lambda _{\\rm CA}$ and $\\lambda _{\\rm CO}$ were comparable values. $\\lambda _{\\rm CA}$ was about one-third of $\\lambda _{\\rm CO}$, and this indicated that the CA pairs were noisier than the CO pairs. A major type of CA pairs that violates our assumption was in the form of \u201c$\\textit {problem}_{\\text{negative}}$ causes $\\textit {solution}_{\\text{positive}}$\u201d: . (\u60aa\u3044\u3068\u3053\u308d\u304c\u3042\u308b, \u3088\u304f\u306a\u308b\u3088\u3046\u306b\u52aa\u529b\u3059\u308b) (there is a bad point, [I] try to improve [it])",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?",
    "llm_answer": "The context states that the method is effective when labeled data are small, but does not provide a specific comparison of improvements between supervised learning on small labeled data with and without the proposed approach.\n",
    "context": [
      {
        "id": "3c294228-6d28-4acb-a236-44c26fd622ef",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 20,
          "document_id": "1909.00694",
          "start_idx": 11804,
          "end_idx": 12506
        },
        "page_content": "Comparing the model variants, we obtained the highest score with the BiGRU encoder trained with the AL+CA+CO dataset. BERT was competitive but its performance went down if CA and CO were used in addition to AL. We conjecture that BERT was more sensitive to noises found more frequently in CA and CO. Contrary to our expectations, supervised models (ACP) outperformed semi-supervised models (ACP+AL+CA+CO). This suggests that the training set of 0.6 million events is sufficiently large for training the models. For comparison, we trained the models with a subset (6,000 events) of the ACP dataset. As the results shown in Table TABREF24 demonstrate, our method is effective when labeled data are small.",
        "type": "Document"
      },
      {
        "id": "439bc21a-fce8-4632-bebc-fabe777f5a61",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 19,
          "document_id": "1909.00694",
          "start_idx": 11154,
          "end_idx": 11804
        },
        "page_content": "The Random+Seed baseline made use of the seed lexicon and output the corresponding label (or the reverse of it for negation) if the event's predicate is in the seed lexicon. We can see that the seed lexicon itself had practically no impact on prediction. The models in the top block performed considerably better than the random baselines. The performance gaps with their (semi-)supervised counterparts, shown in the middle block, were less than 7%. This demonstrates the effectiveness of discourse relation-based label propagation. Comparing the model variants, we obtained the highest score with the BiGRU encoder trained with the AL+CA+CO dataset.",
        "type": "Document"
      },
      {
        "id": "7be35e82-24d5-49fc-b11f-3511ee8e7a66",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 21,
          "document_id": "1909.00694",
          "start_idx": 12285,
          "end_idx": 12964
        },
        "page_content": "As the results shown in Table TABREF24 demonstrate, our method is effective when labeled data are small. The result of hyperparameter optimization for the BiGRU encoder was as follows: As the CA and CO pairs were equal in size (Table TABREF16), $\\lambda _{\\rm CA}$ and $\\lambda _{\\rm CO}$ were comparable values. $\\lambda _{\\rm CA}$ was about one-third of $\\lambda _{\\rm CO}$, and this indicated that the CA pairs were noisier than the CO pairs. A major type of CA pairs that violates our assumption was in the form of \u201c$\\textit {problem}_{\\text{negative}}$ causes $\\textit {solution}_{\\text{positive}}$\u201d: . (\u60aa\u3044\u3068\u3053\u308d\u304c\u3042\u308b, \u3088\u304f\u306a\u308b\u3088\u3046\u306b\u52aa\u529b\u3059\u308b) (there is a bad point, [I] try to improve [it])",
        "type": "Document"
      },
      {
        "id": "bb96f53b-98f0-4811-a691-0975f908cb31",
        "metadata": {
          "vector_store_key": "1909.00694-2",
          "chunk_id": 26,
          "document_id": "1909.00694",
          "start_idx": 15102,
          "end_idx": 15722
        },
        "page_content": "The dimension of hidden units was 256. The optimizer was Momentum SGD BIBREF21. The mini-batch size was 1024. We ran 100 epochs and selected the snapshot that achieved the highest score for the dev set. We used a Japanese BERT model pretrained with Japanese Wikipedia. The input sentences were segmented into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam BIBREF19.",
        "type": "Document"
      },
      {
        "id": "9d58f368-1832-4cfc-bd63-3a1added7a13",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 18,
          "document_id": "1909.00694",
          "start_idx": 9543,
          "end_idx": 10328
        },
        "page_content": "For the details of ${\\rm Encoder}$, see Sections SECREF30. We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\mathcal {L}_{\\rm AL}$, $\\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$, $\\mathcal {L}_{\\rm ACP}$, and $\\mathcal {L}_{\\rm ACP} + \\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$. Table TABREF23 shows accuracy. As the Random baseline suggests, positive and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon and output the corresponding label (or the reverse of it for negation) if the event's predicate is in the seed lexicon.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does their model learn using mostly raw data?",
    "llm_answer": "The model learns complex phenomena through label propagation.  Seed lexicon matches are used to assign scores to event pairs based on discourse relations (Cause or Concession).  These scores are used as reference scores during training.\n",
    "context": [
      {
        "id": "4f29e322-79a3-46ce-ae67-a6e4db8acd6c",
        "metadata": {
          "vector_store_key": "1909.00694-3",
          "chunk_id": 9,
          "document_id": "1909.00694",
          "start_idx": 5556,
          "end_idx": 6191
        },
        "page_content": "We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types. The seed lexicon matches (1) the latter event but (2) not the former event, and (3) their discourse relation type is Cause or Concession. If the discourse relation type is Cause, the former event is given the same score as the latter. Likewise, if the discourse relation type is Concession, the former event is given the opposite of the latter's score. They are used as reference scores during training.",
        "type": "Document"
      },
      {
        "id": "9d58f368-1832-4cfc-bd63-3a1added7a13",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 18,
          "document_id": "1909.00694",
          "start_idx": 9543,
          "end_idx": 10328
        },
        "page_content": "For the details of ${\\rm Encoder}$, see Sections SECREF30. We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\mathcal {L}_{\\rm AL}$, $\\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$, $\\mathcal {L}_{\\rm ACP}$, and $\\mathcal {L}_{\\rm ACP} + \\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$. Table TABREF23 shows accuracy. As the Random baseline suggests, positive and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon and output the corresponding label (or the reverse of it for negation) if the event's predicate is in the seed lexicon.",
        "type": "Document"
      },
      {
        "id": "439bc21a-fce8-4632-bebc-fabe777f5a61",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 19,
          "document_id": "1909.00694",
          "start_idx": 11154,
          "end_idx": 11804
        },
        "page_content": "The Random+Seed baseline made use of the seed lexicon and output the corresponding label (or the reverse of it for negation) if the event's predicate is in the seed lexicon. We can see that the seed lexicon itself had practically no impact on prediction. The models in the top block performed considerably better than the random baselines. The performance gaps with their (semi-)supervised counterparts, shown in the middle block, were less than 7%. This demonstrates the effectiveness of discourse relation-based label propagation. Comparing the model variants, we obtained the highest score with the BiGRU encoder trained with the AL+CA+CO dataset.",
        "type": "Document"
      },
      {
        "id": "676eaae5-9b96-464f-85a2-e2c3e36a8bf6",
        "metadata": {
          "vector_store_key": "1909.00694-3",
          "chunk_id": 8,
          "document_id": "1909.00694",
          "start_idx": 4872,
          "end_idx": 5556
        },
        "page_content": "We assume that we can automatically extract discourse-tagged event pairs, $(x_{i1}, x_{i2})$ ($i=1, \\cdots $) from the raw corpus. We refer to $x_{i1}$ and $x_{i2}$ as former and latter events, respectively. As shown in Figure FIGREF1, we limit our scope to two discourse relations: Cause and Concession. The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation.",
        "type": "Document"
      },
      {
        "id": "3c294228-6d28-4acb-a236-44c26fd622ef",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 20,
          "document_id": "1909.00694",
          "start_idx": 11804,
          "end_idx": 12506
        },
        "page_content": "Comparing the model variants, we obtained the highest score with the BiGRU encoder trained with the AL+CA+CO dataset. BERT was competitive but its performance went down if CA and CO were used in addition to AL. We conjecture that BERT was more sensitive to noises found more frequently in CA and CO. Contrary to our expectations, supervised models (ACP) outperformed semi-supervised models (ACP+AL+CA+CO). This suggests that the training set of 0.6 million events is sufficiently large for training the models. For comparison, we trained the models with a subset (6,000 events) of the ACP dataset. As the results shown in Table TABREF24 demonstrate, our method is effective when labeled data are small.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is seed lexicon used for training?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "439bc21a-fce8-4632-bebc-fabe777f5a61",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 19,
          "document_id": "1909.00694",
          "start_idx": 11154,
          "end_idx": 11804
        },
        "page_content": "The Random+Seed baseline made use of the seed lexicon and output the corresponding label (or the reverse of it for negation) if the event's predicate is in the seed lexicon. We can see that the seed lexicon itself had practically no impact on prediction. The models in the top block performed considerably better than the random baselines. The performance gaps with their (semi-)supervised counterparts, shown in the middle block, were less than 7%. This demonstrates the effectiveness of discourse relation-based label propagation. Comparing the model variants, we obtained the highest score with the BiGRU encoder trained with the AL+CA+CO dataset.",
        "type": "Document"
      },
      {
        "id": "bb96f53b-98f0-4811-a691-0975f908cb31",
        "metadata": {
          "vector_store_key": "1909.00694-2",
          "chunk_id": 26,
          "document_id": "1909.00694",
          "start_idx": 15102,
          "end_idx": 15722
        },
        "page_content": "The dimension of hidden units was 256. The optimizer was Momentum SGD BIBREF21. The mini-batch size was 1024. We ran 100 epochs and selected the snapshot that achieved the highest score for the dev set. We used a Japanese BERT model pretrained with Japanese Wikipedia. The input sentences were segmented into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam BIBREF19.",
        "type": "Document"
      },
      {
        "id": "3c294228-6d28-4acb-a236-44c26fd622ef",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 20,
          "document_id": "1909.00694",
          "start_idx": 11804,
          "end_idx": 12506
        },
        "page_content": "Comparing the model variants, we obtained the highest score with the BiGRU encoder trained with the AL+CA+CO dataset. BERT was competitive but its performance went down if CA and CO were used in addition to AL. We conjecture that BERT was more sensitive to noises found more frequently in CA and CO. Contrary to our expectations, supervised models (ACP) outperformed semi-supervised models (ACP+AL+CA+CO). This suggests that the training set of 0.6 million events is sufficiently large for training the models. For comparison, we trained the models with a subset (6,000 events) of the ACP dataset. As the results shown in Table TABREF24 demonstrate, our method is effective when labeled data are small.",
        "type": "Document"
      },
      {
        "id": "665b82fa-27a0-42a2-83ea-99d5b5cc678a",
        "metadata": {
          "vector_store_key": "1909.00694-2",
          "chunk_id": 27,
          "document_id": "1909.00694",
          "start_idx": 15722,
          "end_idx": 15798
        },
        "page_content": "The optimizer was Adam BIBREF19. The mini-batch size was 32. We ran 1 epoch.",
        "type": "Document"
      },
      {
        "id": "9d58f368-1832-4cfc-bd63-3a1added7a13",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 18,
          "document_id": "1909.00694",
          "start_idx": 9543,
          "end_idx": 10328
        },
        "page_content": "For the details of ${\\rm Encoder}$, see Sections SECREF30. We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\mathcal {L}_{\\rm AL}$, $\\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$, $\\mathcal {L}_{\\rm ACP}$, and $\\mathcal {L}_{\\rm ACP} + \\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$. Table TABREF23 shows accuracy. As the Random baseline suggests, positive and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon and output the corresponding label (or the reverse of it for negation) if the event's predicate is in the seed lexicon.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do the various social phenomena examined manifest in different types of communities?",
    "llm_answer": "Radically contrasting engagement patterns emerge in different parts of a multi-community space, and this variation is partly explained by the type of identity each community fosters.  For example, established members of distinctive communities engage more with volatile content than outsiders, while the opposite is true in generic communities.\n",
    "context": [
      {
        "id": "f0eb2efa-74d7-452c-8a49-9933803c2e88",
        "metadata": {
          "vector_store_key": "1705.09665-4",
          "chunk_id": 61,
          "document_id": "1705.09665",
          "start_idx": 35175,
          "end_idx": 35866
        },
        "page_content": "For example, we find that in niche communities established members are more likely to engage with volatile content than outsiders, while the opposite is true in generic communities. Such insights can be useful for community maintainers seeking to understand engagement patterns in their own communities. One main area of future research is to examine the temporal dynamics in the multi-community landscape. By averaging our measures of distinctiveness and dynamicity across time, our present study treated community identity as a static property. However, as communities experience internal changes and respond to external events, we can expect the nature of their identity to shift as well.",
        "type": "Document"
      },
      {
        "id": "43b4c5c4-7611-4fa6-a018-2069c44e661d",
        "metadata": {
          "vector_store_key": "1705.09665-4",
          "chunk_id": 59,
          "document_id": "1705.09665",
          "start_idx": 34009,
          "end_idx": 34868
        },
        "page_content": "Typologies of this kind are critical to these broader, social-psychological studies of collective identity: they allow researchers to systematically analyze how the psychological manifestations and implications of collective identity vary across diverse sets of communities. Our current understanding of engagement patterns in online communities is patched up from glimpses offered by several disparate studies focusing on a few individual communities. This work calls into attention the need for a method to systematically reason about similarities and differences across communities. By proposing a way to structure the multi-community space, we find not only that radically contrasting engagement patterns emerge in different parts of this space, but also that this variation can be at least partly explained by the type of identity each community fosters.",
        "type": "Document"
      },
      {
        "id": "31bbfc96-19bb-49ad-b765-79d5676aced6",
        "metadata": {
          "vector_store_key": "1705.09665-4",
          "chunk_id": 8,
          "document_id": "1705.09665",
          "start_idx": 4723,
          "end_idx": 5582
        },
        "page_content": "(Section SECREF5 ). Interestingly, while established members of distinctive communities more avidly respond to temporal updates than newcomers, in more generic communities it is the outsiders who engage more with volatile content, perhaps suggesting that such content may serve as an entry-point to the community (but not necessarily a reason to stay). Such insights into the relation between collective identity and user engagement can be informative to community maintainers seeking to better understand growth patterns within their online communities. More generally, our methodology stands as an example of how sociological questions can be addressed in a multi-community setting. In performing our analyses across a rich variety of communities, we reveal both the diversity of phenomena that can occur, as well as the systematic nature of this diversity.",
        "type": "Document"
      },
      {
        "id": "b41fa3fb-24e3-4787-bfd7-e04c85e11a2b",
        "metadata": {
          "vector_store_key": "1705.09665-4",
          "chunk_id": 62,
          "document_id": "1705.09665",
          "start_idx": 35541,
          "end_idx": 36272
        },
        "page_content": "However, as communities experience internal changes and respond to external events, we can expect the nature of their identity to shift as well. For instance, the relative consistency of harrypotter may be disrupted by the release of a new novel, while Seahawks may foster different identities during and between football seasons. Conversely, a community's type may also mediate the impact of new events. Moving beyond a static view of community identity could enable us to better understand how temporal phenomena such as linguistic change manifest across different communities, and also provide a more nuanced view of user engagement\u2014for instance, are communities more welcoming to newcomers at certain points in their lifecycle?",
        "type": "Document"
      },
      {
        "id": "f9abd7fc-544f-4709-b7f1-a18aaeadadbd",
        "metadata": {
          "vector_store_key": "1705.09665-4",
          "chunk_id": 1,
          "document_id": "1705.09665",
          "start_idx": 595,
          "end_idx": 1475
        },
        "page_content": "However, the sheer variety of online platforms complicates the task of generalizing insights beyond these isolated, single-community glimpses. A new way to reason about the variation across multiple communities is needed in order to systematically characterize the relationship between properties of a community and the dynamics taking place within. One especially important component of community dynamics is user engagement. We can aim to understand why users join certain communities BIBREF6 , what factors influence user retention BIBREF7 , and how users react to innovation BIBREF5 . While striking patterns of user engagement have been uncovered in prior case studies of individual communities BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , we do not know whether these observations hold beyond these cases, or when we can draw analogies between different communities.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How did the select the 300 Reddit communities for comparison?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "6ae8f19a-e2be-4246-ad65-cdd576d0c4a2",
        "metadata": {
          "vector_store_key": "1705.09665-8",
          "chunk_id": 10,
          "document_id": "1705.09665",
          "start_idx": 5726,
          "end_idx": 6466
        },
        "page_content": "We start by providing an intuition through inspecting a few example communities. We then introduce a generalizable language-based methodology and use it to map a large set of Reddit communities onto the landscape defined by our typology of community identity. In order to illustrate the diversity within the multi-community space, and to provide an intuition for the underlying structure captured by the proposed typology, we first examine a few example communities and draw attention to some key social dynamics that occur within them. We consider four communities from Reddit: in Seahawks, fans of the Seahawks football team gather to discuss games and players; in BabyBumps, expecting mothers trade advice and updates on their pregnancy;",
        "type": "Document"
      },
      {
        "id": "8cef4a74-0a18-4a85-b97a-6db4a50e00fb",
        "metadata": {
          "vector_store_key": "1705.09665-8",
          "chunk_id": 5,
          "document_id": "1705.09665",
          "start_idx": 2851,
          "end_idx": 3584
        },
        "page_content": "Basing our typology on language is also convenient since it renders our framework immediately applicable to a wide variety of online communities, where communication is primarily recorded in a textual format. Using our framework, we map almost 300 Reddit communities onto the landscape defined by the two axes of our typology (Section SECREF2 ). We find that this mapping induces conceptually sound categorizations that effectively capture key aspects of community-level social dynamics. In particular, we quantitatively validate the effectiveness of our mapping by showing that our two-dimensional typology encodes signals that are predictive of community-level rates of user retention, complementing strong activity-based features.",
        "type": "Document"
      },
      {
        "id": "5c5795c7-f80a-418d-93d1-beb882a7e689",
        "metadata": {
          "vector_store_key": "1705.09665-8",
          "chunk_id": 21,
          "document_id": "1705.09665",
          "start_idx": 12032,
          "end_idx": 12909
        },
        "page_content": "We now explain how our typology can be applied to the particular setting of Reddit, and describe the overall behaviour of our linguistic axes in this context. Dataset description. Reddit is a popular website where users form and participate in discussion-based communities called subreddits. Within these communities, users post content\u2014such as images, URLs, or questions\u2014which often spark vibrant lengthy discussions in thread-based comment sections. The website contains many highly active subreddits with thousands of active subscribers. These communities span an extremely rich variety of topical interests, as represented by the examples described earlier. They also vary along a rich multitude of structural dimensions, such as the number of users, the amount of conversation and social interaction, and the social norms determining which types of content become popular.",
        "type": "Document"
      },
      {
        "id": "afc70624-6fa9-47eb-9c54-9caae01d2c5b",
        "metadata": {
          "vector_store_key": "1705.09665-8",
          "chunk_id": 28,
          "document_id": "1705.09665",
          "start_idx": 15776,
          "end_idx": 16543
        },
        "page_content": "Typology output on Reddit. The distribution of INLINEFORM0 and INLINEFORM1 across Reddit communities is shown in Figure FIGREF3 .B, along with examples of communities at the extremes of our typology. We find that interpretable groupings of communities emerge at various points within our axes. For instance, highly distinctive and dynamic communities tend to focus on rapidly-updating interests like sports teams and games, while generic and consistent communities tend to be large \u201clink-sharing\u201d hubs where users generally post content with no clear dominating themes. More examples of communities at the extremes of our typology are shown in Table TABREF9 . We note that these groupings capture abstract properties of a community's content that go beyond its topic.",
        "type": "Document"
      },
      {
        "id": "4b1db6f6-18eb-403e-a524-9e20fe3a9eca",
        "metadata": {
          "vector_store_key": "1705.09665-8",
          "chunk_id": 14,
          "document_id": "1705.09665",
          "start_idx": 8063,
          "end_idx": 9008
        },
        "page_content": "In the remainder of this section we formally introduce a methodology for mapping communities in this space defined by their distinctiveness and dynamicity (examples in Figure FIGREF3 .B). Our approach follows the intuition that a distinctive community will use language that is particularly specific, or unique, to that community. Similarly, a dynamic community will use volatile language that rapidly changes across successive windows of time. To capture this intuition automatically, we start by defining word-level measures of specificity and volatility. We then extend these word-level primitives to characterize entire comments, and the community itself. Our characterizations of words in a community are motivated by methodology from prior literature that compares the frequency of a word in a particular setting to its frequency in some background distribution, in order to identify instances of linguistic variation BIBREF21 , BIBREF19 .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the clinical text structuring task defined?",
    "llm_answer": "Clinical text structuring (CTS) is extracting or generating a key-value pair where the key is a query term and the value is a result of that query term according to the paragraph text.\n",
    "context": [
      {
        "id": "33c1dc1c-0a6b-4589-a140-8e6587312c04",
        "metadata": {
          "vector_store_key": "1908.06606-4",
          "chunk_id": 5,
          "document_id": "1908.06606",
          "start_idx": 3347,
          "end_idx": 3999
        },
        "page_content": "In addition, we also show that two-stage training mechanism has a great improvement on QA-CTS task. The rest of the paper is organized as follows. We briefly review the related work on clinical text structuring in Section SECREF2. Then, we present question answer based clinical text structuring task in Section SECREF3. In Section SECREF4, we present an effective model for this task. Section SECREF5 is devoted to computational studies and several investigations on the key issues of our proposed model. Finally, conclusions are given in Section SECREF6. Clinical text structuring is a final problem which is highly related to practical applications.",
        "type": "Document"
      },
      {
        "id": "c337fd5e-d0a7-4693-a4cf-bc50e47df7e4",
        "metadata": {
          "vector_store_key": "1908.06606-2",
          "chunk_id": 0,
          "document_id": "1908.06606",
          "start_idx": 0,
          "end_idx": 734
        },
        "page_content": "Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained. It is important to extract structured data from clinical text because bio-medical systems or bio-medical researches greatly rely on structured data but they cannot obtain them directly. In addition, clinical text often contains abundant healthcare information. CTS is able to provide large-scale extracted structured data for enormous down-stream clinical researches.",
        "type": "Document"
      },
      {
        "id": "1729cc24-a274-47a1-9bd1-8cfe06f76c71",
        "metadata": {
          "vector_store_key": "1908.06606-2",
          "chunk_id": 13,
          "document_id": "1908.06606",
          "start_idx": 8375,
          "end_idx": 9068
        },
        "page_content": "Given a sequence of paragraph text $X=<x_1, x_2, ..., x_n>$, clinical text structuring (CTS) can be regarded to extract or generate a key-value pair where key $Q$ is typically a query term such as proximal resection margin and value $V$ is a result of query term $Q$ according to the paragraph text $X$. Generally, researchers solve CTS problem in two steps. Firstly, the answer-related text is pick out. And then several steps such as entity names conversion and negative words recognition are deployed to generate the final answer. While final answer varies from task to task, which truly causes non-uniform output formats, finding the answer-related text is a common action among all tasks.",
        "type": "Document"
      },
      {
        "id": "89091e82-f1f3-4e57-bd35-ea13918f845d",
        "metadata": {
          "vector_store_key": "1908.06606-2",
          "chunk_id": 6,
          "document_id": "1908.06606",
          "start_idx": 3805,
          "end_idx": 4512
        },
        "page_content": "Clinical text structuring is a final problem which is highly related to practical applications. Most of existing studies are case-by-case. Few of them are developed for the general purpose structuring task. These studies can be roughly divided into three categories: rule and dictionary based methods, task-specific end-to-end methods and pipeline methods. Rule and dictionary based methods BIBREF0, BIBREF1, BIBREF2 rely extremely on heuristics and handcrafted extraction rules which is more of an art than a science and incurring extensive trial-and-error experiments. Fukuda et al. BIBREF0 identified protein names from biological papers by dictionaries and several features of protein names. Wang et al.",
        "type": "Document"
      },
      {
        "id": "c40b40bb-33b9-45ac-82af-a56f334033de",
        "metadata": {
          "vector_store_key": "1908.06606-4",
          "chunk_id": 3,
          "document_id": "1908.06606",
          "start_idx": 2041,
          "end_idx": 2819
        },
        "page_content": "However, when the pipeline depth grows, error propagation will have a greater impact on the performance. To reduce the pipeline depth and break the barrier of non-uniform output formats, we present a question answering based clinical text structuring (QA-CTS) task (see Fig. FIGREF1). Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. For some cases, it is already the final answer in deed (e.g., extracting sub-string). While for other cases, it needs several steps to obtain the final answer, such as entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is all text in this dataset a question, or are there unrelated sentences in between questions?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "d7c1f42d-1bcb-4a33-bcc1-9a938c802756",
        "metadata": {
          "vector_store_key": "1908.06606-2",
          "chunk_id": 17,
          "document_id": "1908.06606",
          "start_idx": 10326,
          "end_idx": 11109
        },
        "page_content": "As shown in Fig. FIGREF8, paragraph text $X$ is first passed to a clinical named entity recognition (CNER) model BIBREF12 to capture named entity information and obtain one-hot CNER output tagging sequence for query text $I_{nq}$ and paragraph text $I_{nt}$ with BIEOS (Begin, Inside, End, Outside, Single) tag scheme. $I_{nq}$ and $I_{nt}$ are then integrated together into $I_n$. Meanwhile, the paragraph text $X$ and query text $Q$ are organized and passed to contextualized representation model which is pre-trained language model BERT BIBREF26 here to obtain the contextualized representation vector $V_s$ of both text and query. Afterwards, $V_s$ and $I_n$ are integrated together and fed into a feed forward network to calculate the start and end index of answer-related text.",
        "type": "Document"
      },
      {
        "id": "9f2d7d04-954a-4fe8-9075-28cd6ba7044c",
        "metadata": {
          "vector_store_key": "1908.06606-2",
          "chunk_id": 18,
          "document_id": "1908.06606",
          "start_idx": 10643,
          "end_idx": 11287
        },
        "page_content": "Afterwards, $V_s$ and $I_n$ are integrated together and fed into a feed forward network to calculate the start and end index of answer-related text. Here we define this calculation problem as a classification for each word to be the start or end word. For any clinical free-text paragraph $X$ and query $Q$, contextualized representation is to generate the encoded vector of both of them. Here we use pre-trained language model BERT-base BIBREF26 model to capture contextual information. The text input is constructed as `[CLS] $Q$ [SEP] $X$ [SEP]'. For Chinese sentence, each word in this input will be mapped to a pre-trained embedding $e_i$.",
        "type": "Document"
      },
      {
        "id": "c40b40bb-33b9-45ac-82af-a56f334033de",
        "metadata": {
          "vector_store_key": "1908.06606-4",
          "chunk_id": 3,
          "document_id": "1908.06606",
          "start_idx": 2041,
          "end_idx": 2819
        },
        "page_content": "However, when the pipeline depth grows, error propagation will have a greater impact on the performance. To reduce the pipeline depth and break the barrier of non-uniform output formats, we present a question answering based clinical text structuring (QA-CTS) task (see Fig. FIGREF1). Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. For some cases, it is already the final answer in deed (e.g., extracting sub-string). While for other cases, it needs several steps to obtain the final answer, such as entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data.",
        "type": "Document"
      },
      {
        "id": "cbbb7aae-59ab-4986-8d05-0cff1a90a5fb",
        "metadata": {
          "vector_store_key": "1908.06606-4",
          "chunk_id": 16,
          "document_id": "1908.06606",
          "start_idx": 9572,
          "end_idx": 10326
        },
        "page_content": "Since BERT BIBREF26 has already demonstrated the usefulness of shared model, we suppose extracting commonality of this problem and unifying the output format will make the model more powerful than dedicated model and meanwhile, for a specific clinical task, use the data for other tasks to supplement the training data. In this section, we present an effective model for the question answering based clinical text structuring (QA-CTS). As shown in Fig. FIGREF8, paragraph text $X$ is first passed to a clinical named entity recognition (CNER) model BIBREF12 to capture named entity information and obtain one-hot CNER output tagging sequence for query text $I_{nq}$ and paragraph text $I_{nt}$ with BIEOS (Begin, Inside, End, Outside, Single) tag scheme.",
        "type": "Document"
      },
      {
        "id": "1729cc24-a274-47a1-9bd1-8cfe06f76c71",
        "metadata": {
          "vector_store_key": "1908.06606-2",
          "chunk_id": 13,
          "document_id": "1908.06606",
          "start_idx": 8375,
          "end_idx": 9068
        },
        "page_content": "Given a sequence of paragraph text $X=<x_1, x_2, ..., x_n>$, clinical text structuring (CTS) can be regarded to extract or generate a key-value pair where key $Q$ is typically a query term such as proximal resection margin and value $V$ is a result of query term $Q$ according to the paragraph text $X$. Generally, researchers solve CTS problem in two steps. Firstly, the answer-related text is pick out. And then several steps such as entity names conversion and negative words recognition are deployed to generate the final answer. While final answer varies from task to task, which truly causes non-uniform output formats, finding the answer-related text is a common action among all tasks.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many questions are in the dataset?",
    "llm_answer": "2,714\n",
    "context": [
      {
        "id": "088347bc-6391-466f-81eb-e37d14e108ba",
        "metadata": {
          "vector_store_key": "1705.09665-6",
          "chunk_id": 23,
          "document_id": "1705.09665",
          "start_idx": 13165,
          "end_idx": 13951
        },
        "page_content": "Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities ( INLINEFORM0 ), for a total of 4,872 community-months ( INLINEFORM1 ). Estimating linguistic measures. We estimate word frequencies INLINEFORM0 , and by extension each downstream measure, in a carefully controlled manner in order to ensure we capture robust and meaningful linguistic behaviour.",
        "type": "Document"
      },
      {
        "id": "e0a05c05-3392-47fa-b698-6f263198f907",
        "metadata": {
          "vector_store_key": "1908.06606-2",
          "chunk_id": 26,
          "document_id": "1908.06606",
          "start_idx": 15152,
          "end_idx": 15944
        },
        "page_content": "Then we deploy the proposed model and load the fine tuned BERT weights, attach named entity information layers and retrain the model. In this section, we devote to experimentally evaluating our proposed task and approach. The best results in tables are in bold. Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences).",
        "type": "Document"
      },
      {
        "id": "d50bd955-993d-4140-b06f-46c6425afab9",
        "metadata": {
          "vector_store_key": "1908.06606-0",
          "chunk_id": 27,
          "document_id": "1908.06606",
          "start_idx": 15944,
          "end_idx": 16737
        },
        "page_content": "These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage of predictions that match any one of the ground truth answers exactly. The F$_1$-score metric is a looser metric measures the average overlap between the prediction and ground truth answer. To implement deep neural network models, we utilize the Keras library BIBREF36 with TensorFlow BIBREF37 backend.",
        "type": "Document"
      },
      {
        "id": "aa94fb66-c910-42df-b61d-4a5241934b5b",
        "metadata": {
          "vector_store_key": "1908.06606-0",
          "chunk_id": 29,
          "document_id": "1908.06606",
          "start_idx": 17099,
          "end_idx": 17817
        },
        "page_content": "Due to the high cost of pre-training BERT language model, we directly adopt parameters pre-trained by Google in Chinese general corpus. The named entity recognition is applied on both pathology report texts and query texts. Since BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large. Prediction layer is attached at the end of the original BERT-Base model and we fine tune it on our dataset.",
        "type": "Document"
      },
      {
        "id": "5c5795c7-f80a-418d-93d1-beb882a7e689",
        "metadata": {
          "vector_store_key": "1705.09665-8",
          "chunk_id": 21,
          "document_id": "1705.09665",
          "start_idx": 12032,
          "end_idx": 12909
        },
        "page_content": "We now explain how our typology can be applied to the particular setting of Reddit, and describe the overall behaviour of our linguistic axes in this context. Dataset description. Reddit is a popular website where users form and participate in discussion-based communities called subreddits. Within these communities, users post content\u2014such as images, URLs, or questions\u2014which often spark vibrant lengthy discussions in thread-based comment sections. The website contains many highly active subreddits with thousands of active subscribers. These communities span an extremely rich variety of topical interests, as represented by the examples described earlier. They also vary along a rich multitude of structural dimensions, such as the number of users, the amount of conversation and social interaction, and the social norms determining which types of content become popular.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What aspects have been compared between various language models?",
    "llm_answer": "Perplexity, computational complexity (measured in FLOPs), latency, and energy usage.\n",
    "context": [
      {
        "id": "ba7d3438-b48c-4e2a-a7fe-1f6b7432cfce",
        "metadata": {
          "vector_store_key": "1811.00942-2",
          "chunk_id": 0,
          "document_id": "1811.00942",
          "start_idx": 0,
          "end_idx": 956
        },
        "page_content": "Deep learning has unquestionably advanced the state of the art in many natural language processing tasks, from syntactic dependency parsing BIBREF0 to named-entity recognition BIBREF1 to machine translation BIBREF2 . The same certainly applies to language modeling, where recent advances in neural language models (NLMs) have led to dramatically better approaches as measured using standard metrics such as perplexity BIBREF3 , BIBREF4 . Specifically focused on language modeling, this paper examines an issue that to our knowledge has not been explored: advances in neural language models have come at a significant cost in terms of increased computational complexity. Computing the probability of a token sequence using non-neural techniques requires a number of phrase lookups and perhaps a few arithmetic operations, whereas model inference with NLMs require large matrix multiplications consuming perhaps millions of floating point operations (FLOPs).",
        "type": "Document"
      },
      {
        "id": "974245d0-aa86-48ae-8171-9ed89aa0a7a1",
        "metadata": {
          "vector_store_key": "1811.00942-2",
          "chunk_id": 1,
          "document_id": "1811.00942",
          "start_idx": 956,
          "end_idx": 1768
        },
        "page_content": "Computing the probability of a token sequence using non-neural techniques requires a number of phrase lookups and perhaps a few arithmetic operations, whereas model inference with NLMs require large matrix multiplications consuming perhaps millions of floating point operations (FLOPs). These performance tradeoffs are worth discussing. In truth, language models exist in a quality\u2013performance tradeoff space. As model quality increases (e.g., lower perplexity), performance as measured in terms of energy consumption, query latency, etc. tends to decrease. For applications primarily running in the cloud\u2014say, machine translation\u2014practitioners often solely optimize for the lowest perplexity. This is because such applications are embarrassingly parallel and hence trivial to scale in a data center environment.",
        "type": "Document"
      },
      {
        "id": "98948535-0e36-49a4-a3ca-aabea669a302",
        "metadata": {
          "vector_store_key": "1811.00942-1",
          "chunk_id": 11,
          "document_id": "1811.00942",
          "start_idx": 6755,
          "end_idx": 7459
        },
        "page_content": "BIBREF13 ). Preprocessed by BIBREF14 , PTB contains 887K tokens for training, 70K for validation, and 78K for test, with a vocabulary size of 10,000. On the other hand, WT103 comprises 103 million tokens for training, 217K for validation, and 245K for test, spanning a vocabulary of 267K unique tokens. For the neural language model, we used a four-layer QRNN BIBREF10 , which achieves state-of-the-art results on a variety of datasets, such as WT103 BIBREF11 and PTB. To compare against more common LSTM architectures, we also evaluated AWD-LSTM BIBREF4 on PTB. For the non-neural approach, we used a standard five-gram model with modified Kneser-Ney smoothing BIBREF15 , as explored in BIBREF16 on PTB.",
        "type": "Document"
      },
      {
        "id": "59d56e0c-18b9-4c46-b2b3-cd1283b93660",
        "metadata": {
          "vector_store_key": "1811.00942-1",
          "chunk_id": 6,
          "document_id": "1811.00942",
          "start_idx": 3834,
          "end_idx": 4717
        },
        "page_content": "BIBREF7 explore LSTMs BIBREF8 with binary weights for language modeling; BIBREF9 examine shallow feedforward neural networks for natural language processing. AWD-LSTM. BIBREF4 show that a simple three-layer LSTM, with proper regularization and optimization techniques, can achieve state of the art on various language modeling datasets, surpassing more complex models. Specifically, BIBREF4 apply randomized backpropagation through time, variational dropout, activation regularization, embedding dropout, and temporal activation regularization. A novel scheduler for optimization, non-monotonically triggered ASGD (NT-ASGD) is also introduced. BIBREF4 name their three-layer LSTM model trained with such tricks, \u201cAWD-LSTM.\u201d Quasi-Recurrent Neural Networks. Quasi-recurrent neural networks (QRNNs; BIBREF10 ) achieve current state of the art in word-level language modeling BIBREF11 .",
        "type": "Document"
      },
      {
        "id": "64948bc6-7ce4-4bd6-b412-d4279babea2e",
        "metadata": {
          "vector_store_key": "1811.00942-2",
          "chunk_id": 24,
          "document_id": "1811.00942",
          "start_idx": 13509,
          "end_idx": 14044
        },
        "page_content": "In the present work, we describe and examine the tradeoff space between quality and performance for the task of language modeling. Specifically, we explore the quality\u2013performance tradeoffs between KN-5, a non-neural approach, and AWD-LSTM and QRNN, two neural language models. We find that with decreased perplexity comes vastly increased computational requirements: In one of the NLMs, a perplexity reduction by 2.5 $\\times $ results in a 49 $\\times $ rise in latency and 32 $\\times $ increase in energy usage, when compared to KN-5.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many attention layers are there in their model?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "7db5c4fa-81b5-4233-bfa3-c618af780291",
        "metadata": {
          "vector_store_key": "1907.05664-0",
          "chunk_id": 20,
          "document_id": "1907.05664",
          "start_idx": 10218,
          "end_idx": 10834
        },
        "page_content": "It is not the only information received by the decoder and the importance it \u201callocates\" to this attention state might be very low. What seems to happen in this application is that most of the information used is transmitted from the encoder to the decoder and the attention mechanism at each decoding step just changes marginally how it is used. Quantifying the difference between attention distribution and saliency map across multiple tasks is a possible future work. The second observation we can make is that the saliency map doesn't seem to highlight the right things in the input for the summary it generates.",
        "type": "Document"
      },
      {
        "id": "dd0a96ab-7240-4b8f-af4d-03b89a4757b1",
        "metadata": {
          "vector_store_key": "1907.05664-3",
          "chunk_id": 9,
          "document_id": "1907.05664",
          "start_idx": 4730,
          "end_idx": 5411
        },
        "page_content": "The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254. We train the 21 350 992 parameters of the network for about 60 epochs until we achieve results that are qualitatively equivalent to the results of See et al. See2017. We obtain summaries that are broadly relevant to the text but do not match the target summaries very well. We observe the same problems such as wrong reproduction of factual details, replacing rare words with more common alternatives or repeating non-sense after the third sentence.",
        "type": "Document"
      },
      {
        "id": "d100c0fd-db45-40e5-bead-e64f6f5e189e",
        "metadata": {
          "vector_store_key": "1908.06606-3",
          "chunk_id": 34,
          "document_id": "1908.06606",
          "start_idx": 20053,
          "end_idx": 20795
        },
        "page_content": "Unfortunately, applying multi-head attention on both period one and period two can not reach convergence in our experiments. This probably because it makes the model too complex to train. The difference on other two methods are the order of concatenation and multi-head attention. Applying multi-head attention on two named entity information $I_{nt}$ and $I_{nq}$ first achieved a better performance with 89.87% in EM-score and 92.88% in F$_1$-score. Applying Concatenation first can only achieve 80.74% in EM-score and 84.42% in F$_1$-score. This is probably due to the processing depth of hidden vectors and dataset size. BERT's output has been modified after many layers but named entity information representation is very close to input.",
        "type": "Document"
      },
      {
        "id": "bb96f53b-98f0-4811-a691-0975f908cb31",
        "metadata": {
          "vector_store_key": "1909.00694-2",
          "chunk_id": 26,
          "document_id": "1909.00694",
          "start_idx": 15102,
          "end_idx": 15722
        },
        "page_content": "The dimension of hidden units was 256. The optimizer was Momentum SGD BIBREF21. The mini-batch size was 1024. We ran 100 epochs and selected the snapshot that achieved the highest score for the dev set. We used a Japanese BERT model pretrained with Japanese Wikipedia. The input sentences were segmented into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam BIBREF19.",
        "type": "Document"
      },
      {
        "id": "85e22236-8452-423b-88ce-7311f15f9b63",
        "metadata": {
          "vector_store_key": "1811.00942-3",
          "chunk_id": 9,
          "document_id": "1811.00942",
          "start_idx": 5855,
          "end_idx": 6683
        },
        "page_content": "In the recurrent pooling layer, the convolution outputs are combined sequentially: $\n\\mathbf {c}_t &= \\mathbf {f}_t \\odot \\mathbf {c}_{t-1} + (1 -\n\\mathbf {f}_t) \\odot \\mathbf {z}_t\\\\\n\\mathbf {h}_t &= \\mathbf {o}_t \\odot \\mathbf {c}_t\n$  Multiple QRNN layers can be stacked for deeper hierarchical representation, with the output $\\mathbf {h}_{1:t}$ being fed as the input into the subsequent layer: In language modeling, a four-layer QRNN is a standard architecture BIBREF11 . Perplexity\u2013Recall Scale. Word-level perplexity does not have a strictly monotonic relationship with recall-at- $k$ , the fraction of top $k$ predictions that contain the correct word. A given R@ $k$ imposes a weak minimum perplexity constraint\u2014there are many free parameters that allow for large variability in the perplexity given a certain R@ $k$ .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the three measures of bias which are reduced in experiments?",
    "llm_answer": "Geometric bias, WEAT, and neighborhood bias.\n",
    "context": [
      {
        "id": "06d39a3a-7db6-4e55-af27-0beaa4d2514b",
        "metadata": {
          "vector_store_key": "1910.14497-0",
          "chunk_id": 5,
          "document_id": "1910.14497",
          "start_idx": 2952,
          "end_idx": 3868
        },
        "page_content": "Geometric bias mitigation uses the cosine distances between words to both measure and remove gender bias BIBREF0. This method implicitly defines bias as a geometric asymmetry between words when projected onto a subspace, such as the gender subspace constructed from a set of gender pairs such as $\\mathcal {P} = \\lbrace (he,she),(man,woman),(king,queen)...\\rbrace $. The projection of a vector $v$ onto $B$ (the subspace) is defined by $v_B = \\sum _{j=1}^{k} (v \\cdot b_j) b_j$ where a subspace $B$ is defined by k orthogonal unit vectors $B = {b_1,...,b_k}$. The WEAT statistic BIBREF1 demonstrates the presence of biases in word embeddings with an effect size defined as the mean test statistic across the two word sets: Where $s$, the test statistic, is defined as: $s(w,A,B) = mean_{a \\in A} cos(w,a) - mean_{b \\in B} cos(w,a)$, and $X$,$Y$,$A$, and $B$ are groups of words for which the association is measured.",
        "type": "Document"
      },
      {
        "id": "3a5c344a-93fc-4bc2-aab5-b8721e8f3962",
        "metadata": {
          "vector_store_key": "1910.14497-0",
          "chunk_id": 1,
          "document_id": "1910.14497",
          "start_idx": 882,
          "end_idx": 1583
        },
        "page_content": "The most well-established method thus far for mitigating bias relies on projecting target words onto a bias subspace (such as a gender subspace) and subtracting out the difference between the resulting distances BIBREF0. On the other hand, the most popular metric for measuring bias is the WEAT statistic BIBREF1, which compares the cosine similarities between groups of words. However, WEAT has been recently shown to overestimate bias as a result of implicitly relying on similar frequencies for the target words BIBREF4, and BIBREF5 demonstrated that evidence of bias can still be recovered after geometric bias mitigation by examining the neighborhood of a target word among socially-biased words.",
        "type": "Document"
      },
      {
        "id": "38a703f6-669d-4fba-93bc-8a108e581caf",
        "metadata": {
          "vector_store_key": "1910.14497-1",
          "chunk_id": 6,
          "document_id": "1910.14497",
          "start_idx": 3399,
          "end_idx": 4156
        },
        "page_content": "The WEAT statistic BIBREF1 demonstrates the presence of biases in word embeddings with an effect size defined as the mean test statistic across the two word sets: Where $s$, the test statistic, is defined as: $s(w,A,B) = mean_{a \\in A} cos(w,a) - mean_{b \\in B} cos(w,a)$, and $X$,$Y$,$A$, and $B$ are groups of words for which the association is measured. Possible values range from $-2$ to 2 depending on the association of the words groups, and a value of zero indicates $X$ and $Y$ are equally associated with $A$ and $B$. See BIBREF4 for further details on WEAT. The RIPA (relational inner product association) metric was developed as an alternative to WEAT, with the critique that WEAT is likely to overestimate the bias of a target attribute BIBREF4.",
        "type": "Document"
      },
      {
        "id": "f94cb776-65b7-432d-8c39-81add14c31c3",
        "metadata": {
          "vector_store_key": "1910.14497-1",
          "chunk_id": 8,
          "document_id": "1910.14497",
          "start_idx": 4447,
          "end_idx": 5312
        },
        "page_content": "We report the absolute value of the RIPA metric as the value can be positive or negative according to the direction of the bias. A value of zero indicates a lack of bias, and the value is bound by $[-||w||,||w||]$. The neighborhood bias metric proposed by BIBREF5 quantifies bias as the proportion of male socially-biased words among the $k$ nearest socially-biased male and female neighboring words, whereby biased words are obtained by projecting neutral words onto a gender relation vector. As we only examine the target word among the 1000 most socially-biased words in the vocabulary (500 male and 500 female), a word\u2019s bias is measured as the ratio of its neighborhood of socially-biased male and socially-biased female words, so that a value of 0.5 in this metric would indicate a perfectly unbiased word, and values closer to 0 and 1 indicate stronger bias.",
        "type": "Document"
      },
      {
        "id": "c18ef50d-4a9b-493a-a86e-4e005f70aabb",
        "metadata": {
          "vector_store_key": "1910.14497-0",
          "chunk_id": 19,
          "document_id": "1910.14497",
          "start_idx": 10599,
          "end_idx": 11432
        },
        "page_content": "Words in this list should be gender neutral, and are related to the topics of career, arts, science, math, family and professions (see appendix). We note that this list can easily be expanded to include a greater proportion of words in the corpus. For example, BIBREF4 suggested a method for identifying inappropriately gendered words using unsupervised learning. We compare this method of bias mitigation with the no bias mitigation (\"Orig\"), geometric bias mitigation (\"Geo\"), the two pieces of our method alone (\"Prob\" and \"KNN\") and the composite method (\"KNN+Prob\"). We note that the composite method performs reasonably well according the the RIPA metric, and much better than traditional geometric bias mitigation according to the neighborhood metric, without significant performance loss according to the accepted benchmarks.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is the dataset?",
    "llm_answer": "The answer is not found in the context.  While various dataset sizes are mentioned, there is no single answer to \"How big is the dataset?\".\n",
    "context": [
      {
        "id": "da74a9a9-0751-4d68-9ca9-313bf450f848",
        "metadata": {
          "vector_store_key": "1907.05664-3",
          "chunk_id": 7,
          "document_id": "1907.05664",
          "start_idx": 3615,
          "end_idx": 4242
        },
        "page_content": "See2017 to then apply LRP on it. The CNN/Daily mail dataset BIBREF12 is a text summarization dataset adapted from the Deepmind question-answering dataset BIBREF13 . It contains around three hundred thousand news articles coupled with summaries of about three sentences. These summaries are in fact \u201chighlights\" of the articles provided by the media themselves. Articles have an average length of 780 words and the summaries of 50 words. We had 287 000 training pairs and 11 500 test pairs. Similarly to See et al. See2017, we limit during training and prediction the input text to 400 words and generate summaries of 200 words.",
        "type": "Document"
      },
      {
        "id": "088347bc-6391-466f-81eb-e37d14e108ba",
        "metadata": {
          "vector_store_key": "1705.09665-6",
          "chunk_id": 23,
          "document_id": "1705.09665",
          "start_idx": 13165,
          "end_idx": 13951
        },
        "page_content": "Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities ( INLINEFORM0 ), for a total of 4,872 community-months ( INLINEFORM1 ). Estimating linguistic measures. We estimate word frequencies INLINEFORM0 , and by extension each downstream measure, in a carefully controlled manner in order to ensure we capture robust and meaningful linguistic behaviour.",
        "type": "Document"
      },
      {
        "id": "34df9a0f-974b-460c-9178-c3de80bac6ac",
        "metadata": {
          "vector_store_key": "2002.02224-3",
          "chunk_id": 25,
          "document_id": "2002.02224",
          "start_idx": 14910,
          "end_idx": 15763
        },
        "page_content": "These datasets consist of the individual pairs containing the identification of the decision from which the reference was retrieved, and the identification of the referred documents. As we only extracted references to other judicial decisions, we obtained 471,319 references from Supreme Court decisions, 167,237 references from Supreme Administrative Court decisions and 264,463 references from Constitutional Court Decisions. These are numbers of text spans identified as references prior the further processing described in Section SECREF3. These references include all identifiers extracted from the court decisions contained in the CzCDC 1.0. Therefore, this number includes all other court decisions, including lower courts, the Court of Justice of the European Union, the European Court of Human Rights, decisions of other public authorities etc.",
        "type": "Document"
      },
      {
        "id": "bb96f53b-98f0-4811-a691-0975f908cb31",
        "metadata": {
          "vector_store_key": "1909.00694-2",
          "chunk_id": 26,
          "document_id": "1909.00694",
          "start_idx": 15102,
          "end_idx": 15722
        },
        "page_content": "The dimension of hidden units was 256. The optimizer was Momentum SGD BIBREF21. The mini-batch size was 1024. We ran 100 epochs and selected the snapshot that achieved the highest score for the dev set. We used a Japanese BERT model pretrained with Japanese Wikipedia. The input sentences were segmented into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam BIBREF19.",
        "type": "Document"
      },
      {
        "id": "e0a05c05-3392-47fa-b698-6f263198f907",
        "metadata": {
          "vector_store_key": "1908.06606-2",
          "chunk_id": 26,
          "document_id": "1908.06606",
          "start_idx": 15152,
          "end_idx": 15944
        },
        "page_content": "Then we deploy the proposed model and load the fine tuned BERT weights, attach named entity information layers and retrain the model. In this section, we devote to experimentally evaluating our proposed task and approach. The best results in tables are in bold. Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the intensity of the PTSD established?",
    "llm_answer": "The intensity of PTSD is established by weekly survey results from three clinical assessment tools (DOSPERT, BSSS, and VIAS).  High risk is determined by scores exceeding thresholds on all three tools; moderate risk by scores exceeding thresholds on any two; low risk by exceeding thresholds on one; and no PTSD by not exceeding thresholds on any.\n",
    "context": [
      {
        "id": "3a3a5e02-b04e-43a2-a64c-50552dbd3ade",
        "metadata": {
          "vector_store_key": "2003.07433-3",
          "chunk_id": 18,
          "document_id": "2003.07433",
          "start_idx": 9798,
          "end_idx": 10546
        },
        "page_content": "Dryhootch defines the intensity of PTSD into four categories based on the weekly survey results of all three clinical survey tools (DOSPERT, BSSS and VIAS ) High risk PTSD: If one individual veteran's weekly PTSD assessment scores go above the threshold for all three PTSD assessment tools i.e. DOSPERT, BSSS and VIAS, then he/she is in high risk situation which needs immediate mental support to avoid catastrophic effect of individual's health or surrounding people's life. Moderate risk PTSD: If one individual veteran's weekly PTSD assessment scores go above the threshold for any two of the three PTSD assessment tools, then he/she is in moderate risk situation which needs close observation and peer mentoring to avoid their risk progression.",
        "type": "Document"
      },
      {
        "id": "7068029a-fdc8-4032-a892-3cc19be1e328",
        "metadata": {
          "vector_store_key": "2003.07433-3",
          "chunk_id": 1,
          "document_id": "2003.07433",
          "start_idx": 806,
          "end_idx": 1746
        },
        "page_content": "As a result, the Veteran Administration's National Center for PTSD (NCPTSD) suggests to reconceptualize PTSD not just in terms of a psychiatric symptom cluster, but focusing instead on the specific high risk behaviors associated with it, as these may be directly addressed though behavioral change efforts BIBREF0. Consensus prevalence estimates suggest that PTSD impacts between 15-20% of the veteran population which is typically chronic and treatment resistant BIBREF0. The PTSD patients support programs organized by different veterans peer support organization use a set of surveys for local weekly assessment to detect the intensity of PTSD among the returning veterans. However, recent advanced evidence-based care for PTSD sufferers surveys have showed that veterans, suffered with chronic PTSD are reluctant in participating assessments to the professionals which is another significant symptom of war returning veterans with PTSD.",
        "type": "Document"
      },
      {
        "id": "56099834-1ec3-4313-bf17-9537276d2950",
        "metadata": {
          "vector_store_key": "2003.07433-4",
          "chunk_id": 46,
          "document_id": "2003.07433",
          "start_idx": 24953,
          "end_idx": 25723
        },
        "page_content": "The distribution of this training-test dataset segmentation followed a 50% distribution of PTSD and No PTSD from the original dataset. Our final survey based classification results showed an accuracy of 96% in detecting PTSD and mean squared error of 1.2 in estimating its intensity given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively. Table TABREF29 shows the classification details of our experiment which provide the very good accuracy of our classification. To compare the outperformance of our method, we also implemented Coppersmith et. al. proposed method and achieved an 86% overall accuracy of detecting PTSD users BIBREF11 following the same training-test dataset distribution.",
        "type": "Document"
      },
      {
        "id": "69d88da2-01a5-4726-91b3-159b1fbd3bef",
        "metadata": {
          "vector_store_key": "2003.07433-3",
          "chunk_id": 19,
          "document_id": "2003.07433",
          "start_idx": 10546,
          "end_idx": 11252
        },
        "page_content": "Moderate risk PTSD: If one individual veteran's weekly PTSD assessment scores go above the threshold for any two of the three PTSD assessment tools, then he/she is in moderate risk situation which needs close observation and peer mentoring to avoid their risk progression. Low risk PTSD: If one individual veteran's weekly PTSD assessment scores go above the threshold for any one of the three PTSD assessment tools, then he/she has light symptoms of PTSD. No PTSD: If one individual veteran's weekly PTSD assessment scores go below the threshold for all three PTSD assessment tools, then he/she has no PTSD. To develop an explainable model, we first need to develop twitter-based PTSD detection algorithm.",
        "type": "Document"
      },
      {
        "id": "8730ef0a-9b58-4bc1-b5c4-f5b9ad417698",
        "metadata": {
          "vector_store_key": "2003.07433-3",
          "chunk_id": 0,
          "document_id": "2003.07433",
          "start_idx": 0,
          "end_idx": 806
        },
        "page_content": "Combat veterans diagnosed with PTSD are substantially more likely to engage in a number of high risk activities including engaging in interpersonal violence, attempting suicide, committing suicide, binge drinking, and drug abuse BIBREF0. Despite improved diagnostic screening, outpatient mental health and inpatient treatment for PTSD, the syndrome remains treatment resistant, is typically chronic, and is associated with numerous negative health effects and higher treatment costs BIBREF1. As a result, the Veteran Administration's National Center for PTSD (NCPTSD) suggests to reconceptualize PTSD not just in terms of a psychiatric symptom cluster, but focusing instead on the specific high risk behaviors associated with it, as these may be directly addressed though behavioral change efforts BIBREF0.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "how is quality measured?",
    "llm_answer": "By validating saliency maps by incrementally deleting \"important\" words from input text and observing changes in generated summaries;  using Cronbach's alpha to assess internal consistency; and using EM-score and F$_1$-score to evaluate deep neural network models.\n",
    "context": [
      {
        "id": "bfa6dafe-f3a5-4f12-b123-2c9efbd588b0",
        "metadata": {
          "vector_store_key": "1907.05664-0",
          "chunk_id": 22,
          "document_id": "1907.05664",
          "start_idx": 11178,
          "end_idx": 11841
        },
        "page_content": "We will call that truthfulness of the attribution in regard to the computation, meaning that an attribution is truthful in regard to the computation if it actually highlights the important input features that the network attended to during prediction. We proceed to measure the truthfulness of the attributions by validating them quantitatively. We propose to validate the saliency maps in a similar way as Arras et al. Arras2017 by incrementally deleting \u201cimportant\" words from the input text and observe the change in the resulting generated summaries. We first define what \u201cimportant\" (and \u201cunimportant\") input words mean across the 50 saliency maps per texts.",
        "type": "Document"
      },
      {
        "id": "b278d524-a8ae-4324-95f9-606c8bb32cd8",
        "metadata": {
          "vector_store_key": "2003.07433-0",
          "chunk_id": 37,
          "document_id": "2003.07433",
          "start_idx": 20219,
          "end_idx": 20914
        },
        "page_content": "After the PTSD Linguistic Dictionary has been created, we empirically evaluate its psychometric properties such as reliability and validity as per American Standards for educational and psychological testing guideline BIBREF24. In psychometrics, reliability is most commonly evaluated by Cronbach's alpha, which assesses internal consistency based on inter-correlations and the number of measured items. In the text analysis scenario, each word in our PTSD Linguistic dictionary is considered an item, and reliability is calculated based on each text file's response to each word item, which forms an $N$(number of text files) $\\times $ $J$(number of words or stems in a dictionary) data matrix.",
        "type": "Document"
      },
      {
        "id": "c7a5f4b3-a8d8-4418-b4ed-2106461df1d1",
        "metadata": {
          "vector_store_key": "2003.07433-0",
          "chunk_id": 36,
          "document_id": "2003.07433",
          "start_idx": 19458,
          "end_idx": 20219
        },
        "page_content": "We chose one dimension for each of the questions under each category to reflect real survey system evaluation. Our chosen categories are state in Fig FIGREF20. Score calculation $\\alpha $-score: $\\alpha $-scores refer to the Cronbach's alphas for the internal reliability of the specific words within each category. The binary alphas are computed on the ratio of occurrence and non-occurrence of each dictionary word whereas the raw or uncorrected alphas are based on the percentage of use of each of the category words within texts. After the PTSD Linguistic Dictionary has been created, we empirically evaluate its psychometric properties such as reliability and validity as per American Standards for educational and psychological testing guideline BIBREF24.",
        "type": "Document"
      },
      {
        "id": "880d9baa-cdc2-49f6-8447-136508276df4",
        "metadata": {
          "vector_store_key": "1907.05664-4",
          "chunk_id": 24,
          "document_id": "1907.05664",
          "start_idx": 12244,
          "end_idx": 12886
        },
        "page_content": "We did however also try with different methods, like averaging the raw relevance or averaging a scaled absolute value where negative relevance is scaled down by a constant factor. The absolute value average seemed to deliver the best results. We delete incrementally the important words (words with the highest average) in the input and compared it to the control experiment that consists of deleting the least important word and compare the degradation of the resulting summaries. We obtain mitigated results: for some texts, we observe a quick degradation when deleting important words which are not observed when deleting unimportant words",
        "type": "Document"
      },
      {
        "id": "d50bd955-993d-4140-b06f-46c6425afab9",
        "metadata": {
          "vector_store_key": "1908.06606-0",
          "chunk_id": 27,
          "document_id": "1908.06606",
          "start_idx": 15944,
          "end_idx": 16737
        },
        "page_content": "These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage of predictions that match any one of the ground truth answers exactly. The F$_1$-score metric is a looser metric measures the average overlap between the prediction and ground truth answer. To implement deep neural network models, we utilize the Keras library BIBREF36 with TensorFlow BIBREF37 backend.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the accuracy reported by state-of-the-art methods?",
    "llm_answer": "86%\n",
    "context": [
      {
        "id": "d50bd955-993d-4140-b06f-46c6425afab9",
        "metadata": {
          "vector_store_key": "1908.06606-0",
          "chunk_id": 27,
          "document_id": "1908.06606",
          "start_idx": 15944,
          "end_idx": 16737
        },
        "page_content": "These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage of predictions that match any one of the ground truth answers exactly. The F$_1$-score metric is a looser metric measures the average overlap between the prediction and ground truth answer. To implement deep neural network models, we utilize the Keras library BIBREF36 with TensorFlow BIBREF37 backend.",
        "type": "Document"
      },
      {
        "id": "b2f8b59b-1092-4698-9fa3-c0a21594d3ff",
        "metadata": {
          "vector_store_key": "2003.07433-4",
          "chunk_id": 47,
          "document_id": "2003.07433",
          "start_idx": 25723,
          "end_idx": 26401
        },
        "page_content": "To compare the outperformance of our method, we also implemented Coppersmith et. al. proposed method and achieved an 86% overall accuracy of detecting PTSD users BIBREF11 following the same training-test dataset distribution. Fig FIGREF28 illustrates the comparisons between LAXARY and Coppersmith et. al. proposed method. Here we can see, the outperformance of our proposed method as well as the importance of $s-score$ estimation. We also illustrates the importance of $\\alpha -score$ and $S-score$ in Fig FIGREF30. Fig FIGREF30 illustrates that if we change the number of training samples (%), LAXARY models outperforms Coppersmith et. al. proposed model under any condition.",
        "type": "Document"
      },
      {
        "id": "bfa6dafe-f3a5-4f12-b123-2c9efbd588b0",
        "metadata": {
          "vector_store_key": "1907.05664-0",
          "chunk_id": 22,
          "document_id": "1907.05664",
          "start_idx": 11178,
          "end_idx": 11841
        },
        "page_content": "We will call that truthfulness of the attribution in regard to the computation, meaning that an attribution is truthful in regard to the computation if it actually highlights the important input features that the network attended to during prediction. We proceed to measure the truthfulness of the attributions by validating them quantitatively. We propose to validate the saliency maps in a similar way as Arras et al. Arras2017 by incrementally deleting \u201cimportant\" words from the input text and observe the change in the resulting generated summaries. We first define what \u201cimportant\" (and \u201cunimportant\") input words mean across the 50 saliency maps per texts.",
        "type": "Document"
      },
      {
        "id": "67afec42-656a-4836-a40a-c586306d5a8f",
        "metadata": {
          "vector_store_key": "2003.07433-0",
          "chunk_id": 39,
          "document_id": "2003.07433",
          "start_idx": 21105,
          "end_idx": 21770
        },
        "page_content": "For the binary method, the data matrix quantifies whether or not a word was used in a text file where \"1\" represents yes and \"0\" represents no. Once the data matrix is created, it is used to calculate Cronbach's alpha based on its inter-correlation matrix among the word percentages. We assess reliability based on our selected 210 users' Tweets which further generated a 23,562 response matrix after running the PTSD Linguistic Dictionary for each user. The response matrix yields reliability of .89 based on the uncorrected method, and .96 based on the binary method, which confirm the high reliability of our PTSD Dictionary created PTSD survey based categories.",
        "type": "Document"
      },
      {
        "id": "467ee1c6-fd3f-41b6-8362-12b11ee9ed02",
        "metadata": {
          "vector_store_key": "1910.04269-1",
          "chunk_id": 38,
          "document_id": "1910.04269",
          "start_idx": 21056,
          "end_idx": 21639
        },
        "page_content": "This method is able to bypass the computational overhead of conventional approaches which depend on generation of spectrograms as a necessary pre-procesing step. We were able to achieve an accauracy of 93.7% using this technique. Next, we discussed the enhancement in performance of 2D-ConvNet using mixup augmentation, which is a recently developed technique to prevent over\ufb01tting on test data. This approach achieved an accuracy of 95.4%. We also analysed how attention mechanism and recurrent layers impact the performance of networks. This approach achieved an accuracy of 95.0%.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do the authors define or exemplify 'incorrect words'?",
    "llm_answer": "The authors exemplify \"incorrect words\" as missing or incorrectly typed words in sentences.  They also refer to incorrect sentences formatted by native English speakers as part of the dataset creation process.\n",
    "context": [
      {
        "id": "73cfd577-8b68-42d0-b667-25c8485c0ca3",
        "metadata": {
          "vector_store_key": "2001.00137-1",
          "chunk_id": 0,
          "document_id": "2001.00137",
          "start_idx": 0,
          "end_idx": 799
        },
        "page_content": "Understanding a user's intent and sentiment is of utmost importance for current intelligent chatbots to respond appropriately to human requests. However, current systems are not able to perform to their best capacity when presented with incomplete data, meaning sentences with missing or incorrect words. This scenario is likely to happen when one considers human error done in writing. In fact, it is rather naive to assume that users will always type fully grammatically correct sentences. Panko BIBREF0 goes as far as claiming that human accuracy regarding research paper writing is none when considering the entire document. This has been aggravated with the advent of internet and social networks, which allowed language and modern communication to be been rapidly transformed BIBREF1, BIBREF2.",
        "type": "Document"
      },
      {
        "id": "1903e4a1-a3c7-4902-bbff-006975f9dbe8",
        "metadata": {
          "vector_store_key": "1910.14497-0",
          "chunk_id": 0,
          "document_id": "1910.14497",
          "start_idx": 0,
          "end_idx": 882
        },
        "page_content": "Word embeddings, or vector representations of words, are an important component of Natural Language Processing (NLP) models and necessary for many downstream tasks. However, word embeddings, including embeddings commonly deployed for public use, have been shown to exhibit unwanted societal stereotypes and biases, raising concerns about disparate impact on axes of gender, race, ethnicity, and religion BIBREF0, BIBREF1. The impact of this bias has manifested in a range of downstream tasks, ranging from autocomplete suggestions BIBREF2 to advertisement delivery BIBREF3, increasing the likelihood of amplifying harmful biases through the use of these models. The most well-established method thus far for mitigating bias relies on projecting target words onto a bias subspace (such as a gender subspace) and subtracting out the difference between the resulting distances BIBREF0.",
        "type": "Document"
      },
      {
        "id": "e7d0a447-e90c-4fde-a453-20ac8576450a",
        "metadata": {
          "vector_store_key": "2001.00137-1",
          "chunk_id": 8,
          "document_id": "2001.00137",
          "start_idx": 5052,
          "end_idx": 5740
        },
        "page_content": "To summarize, our contribution is two-fold: Novel model architecture that is more robust to incomplete data, including missing or incorrect words in text. Proposal of the novel tasks of incomplete intent and sentiment classification from incorrect sentences, and release of corpora related with these tasks. The remainder of this paper is organized in four sections, with Section SECREF2 explaining the proposed model. This is followed by Section SECREF3 which includes a detailed description of the dataset used for training and evaluation purposes and how it was obtained. Section SECREF4 covers the baseline models used for comparison, training specifications and experimental results.",
        "type": "Document"
      },
      {
        "id": "eb69197a-41c1-43e8-b236-477b13b95101",
        "metadata": {
          "vector_store_key": "2003.07433-2",
          "chunk_id": 34,
          "document_id": "2003.07433",
          "start_idx": 18439,
          "end_idx": 19095
        },
        "page_content": "Words in the WordStat dictionary file will be referred to as dictionary words. Groups of dictionary words that tap a particular domain (e.g., negative emotion words) are variously referred to as subdictionaries or word categories. Fig FIGREF8 is a sample WordStat dictionary. There are several steps to use this dictionary which are stated as follows: Pronoun selection: At first we have to define the pronouns of the target sentiment. Here we used first person singular number pronouns (i.e., I, me, mine etc.) that means we only count those sentences or segments which are only related to first person singular number i.e., related to the person himself.",
        "type": "Document"
      },
      {
        "id": "8c683d79-c181-40f1-a311-f89612619f87",
        "metadata": {
          "vector_store_key": "2001.00137-1",
          "chunk_id": 19,
          "document_id": "2001.00137",
          "start_idx": 11762,
          "end_idx": 12488
        },
        "page_content": "We use this platform to create tasks for native English speakers to format the original incorrect tweets into correct sentences. Some examples are shown in Table TABREF12. After obtaining the correct sentences, our two-class dataset has class distribution as shown in Table TABREF14. There are 200 sentences used in the training stage, with 100 belonging to the positive sentiment class and 100 to the negative class, and 50 samples being used in the evaluation stage, with 25 negative and 25 positive. This totals in 300 samples, with incorrect and correct sentences combined. Since our goal is to evaluate the model's performance and robustness in the presence of noise, we only consider incorrect data in the testing phase.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much do they outperform other models in the sentiment in intent classification tasks?",
    "llm_answer": "The context states that the proposed model outperformed baseline models by 6% to 8% in the Twitter Sentiment Classification task, but no performance difference is mentioned for intent classification tasks.\n",
    "context": [
      {
        "id": "713f2e66-b90a-4cf1-9d7e-a8c2a1ffd6ce",
        "metadata": {
          "vector_store_key": "2001.00137-1",
          "chunk_id": 4,
          "document_id": "2001.00137",
          "start_idx": 2454,
          "end_idx": 3225
        },
        "page_content": "In the sentiment classification task, the model aims to identify the user's sentiment given a tweet, written in informal language and without regards for sentence correctness. Current approaches for Text Classification tasks focus on efficient embedding representations. Kim et al. BIBREF9 use semantically enriched word embeddings to make synonym and antonym word vectors respectively more and less similar in order to improve intent classification performance. Devlin et al. BIBREF10 propose Bidirectional Encoder Representations from Transformers (BERT), a powerful bidirectional language representation model based on Transformers, achieving state-of-the-art results on eleven NLP tasks BIBREF11, including sentiment text classification. Concurrently, Shridhar et al.",
        "type": "Document"
      },
      {
        "id": "6e964ac3-a82e-490e-b0b4-e00967227066",
        "metadata": {
          "vector_store_key": "2001.00137-1",
          "chunk_id": 31,
          "document_id": "2001.00137",
          "start_idx": 18607,
          "end_idx": 19358
        },
        "page_content": "The stack of multilayer perceptrons are trained for 100 and 1,000 epochs with Adam Optimizer, learning rate of $10^{-3}$, weight decay of $10^{-5}$, MSE loss criterion and batch size the same as BERT (4 for the Twitter Sentiment Corpus and 8 for the Chatbot Intent Classification Corpus). Experimental results for the Twitter Sentiment Classification task on Kaggle's Sentiment140 Corpus dataset, displayed in Table TABREF37, show that our model has better F1-micros scores, outperforming the baseline models by 6$\\%$ to 8$\\%$. We evaluate our model and baseline models on three versions of the dataset. The first one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80$\\%$ against BERT's 72$\\%$.",
        "type": "Document"
      },
      {
        "id": "9f0639fa-3113-4ca1-8e01-fb93ba36f83a",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 27,
          "document_id": "2001.00137",
          "start_idx": 16181,
          "end_idx": 17057
        },
        "page_content": "A complete list of classifiers and training specifications are given in Section SECREF31. The baseline and proposed models are each trained 3 separate times for the incomplete intent classification task: complete data and one for each of the TTS-STT combinations (gtts-witai and macsay-witai). Regarding the sentiment classification from incorrect sentences task, the baseline and proposed models are each trained 3 times: original text, corrected text and incorrect with correct texts. The reported F1 scores are the best accuracies obtained from 10 runs. No settable training configurations available in the online platforms. Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively;",
        "type": "Document"
      },
      {
        "id": "557c9e18-0a45-4676-9612-c4389a02dc36",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 30,
          "document_id": "2001.00137",
          "start_idx": 17872,
          "end_idx": 18607
        },
        "page_content": "The model is fine-tuned with our dataset on 2 Titan X GPUs for 3 epochs with Adam Optimizer, learning rate of $2*10^{-5}$, maximum sequence length of 128, and warm up proportion of $0.1$. The train batch size is 4 for the Twitter Sentiment Corpus and 8 for the Chatbot Intent Classification Corpus. Our proposed model is trained in end-to-end manner on 2 Titan X GPUs, with training time depending on the size of the dataset and train batch size. The stack of multilayer perceptrons are trained for 100 and 1,000 epochs with Adam Optimizer, learning rate of $10^{-3}$, weight decay of $10^{-5}$, MSE loss criterion and batch size the same as BERT (4 for the Twitter Sentiment Corpus and 8 for the Chatbot Intent Classification Corpus).",
        "type": "Document"
      },
      {
        "id": "1106d6ee-284a-40ca-9cbd-a0a8005eeb94",
        "metadata": {
          "vector_store_key": "2001.00137-1",
          "chunk_id": 20,
          "document_id": "2001.00137",
          "start_idx": 12212,
          "end_idx": 12901
        },
        "page_content": "Since our goal is to evaluate the model's performance and robustness in the presence of noise, we only consider incorrect data in the testing phase. Note that BERT is a pre-trained model, meaning that small amounts of data are enough for appropriate fine-tuning. In the intent classification task, we are presented with a corpus that suffers from the opposite problem of the Twitter sentiment classification corpus. In the intent classification corpus, we have the complete sentences and intent labels but lack their corresponding incomplete sentences, and since our task revolves around text classification in incomplete or incorrect data, it is essential that we obtain this information.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which experiments are perfomed?",
    "llm_answer": "Experiments on subjective language detection using BERT-based models (BERT, RoBERTa, ALBERT);  experiments on clinical text structuring using rule-based methods, task-specific end-to-end methods, and pipeline methods; experiments on classifying perceptually similar languages (German and English) using GMM and RPLP features; experiments evaluating a proposed model for Chinese pathology report analysis.\n",
    "context": [
      {
        "id": "89091e82-f1f3-4e57-bd35-ea13918f845d",
        "metadata": {
          "vector_store_key": "1908.06606-2",
          "chunk_id": 6,
          "document_id": "1908.06606",
          "start_idx": 3805,
          "end_idx": 4512
        },
        "page_content": "Clinical text structuring is a final problem which is highly related to practical applications. Most of existing studies are case-by-case. Few of them are developed for the general purpose structuring task. These studies can be roughly divided into three categories: rule and dictionary based methods, task-specific end-to-end methods and pipeline methods. Rule and dictionary based methods BIBREF0, BIBREF1, BIBREF2 rely extremely on heuristics and handcrafted extraction rules which is more of an art than a science and incurring extensive trial-and-error experiments. Fukuda et al. BIBREF0 identified protein names from biological papers by dictionaries and several features of protein names. Wang et al.",
        "type": "Document"
      },
      {
        "id": "387b3080-a2d9-4000-874b-5fbd2e3a9609",
        "metadata": {
          "vector_store_key": "2002.06644-0",
          "chunk_id": 3,
          "document_id": "2002.06644",
          "start_idx": 2147,
          "end_idx": 2838
        },
        "page_content": "Their attempt to test their models in a general setting is dwarfed by the fact that they used revisions from a single Wikipedia article resulting in just 100 instances to evaluate their proposed models robustly. Consequently, we perform our experiments in the complete WNC corpus, which consists of $423,823$ revisions in Wikipedia marked by its editors over a period of 15 years, to simulate a more general setting for the bias. In this work, we investigate the application of BERT-based models for the task of subjective language detection. We explore various BERT-based models, including BERT, RoBERTa, ALBERT, with their base and large specifications along with their native classifiers.",
        "type": "Document"
      },
      {
        "id": "1a9e4638-0519-4b8d-8029-fa3c7b8f7c07",
        "metadata": {
          "vector_store_key": "2002.06644-3",
          "chunk_id": 12,
          "document_id": "2002.06644",
          "start_idx": 7598,
          "end_idx": 7891
        },
        "page_content": "BERT-based ensemble consisting of RoBERTa, ALBERT, DistillRoBERTa, and BERT led to the highest F1 and Accuracy. In the future, we would like to explore document-level detection of subjective bias, multi-word mitigation of the bias, applications of detecting the bias in recommendation systems.",
        "type": "Document"
      },
      {
        "id": "d96990f0-760d-4401-b868-e98fb5836455",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 10,
          "document_id": "1910.04269",
          "start_idx": 6004,
          "end_idx": 6789
        },
        "page_content": "GMM combined with RPLP features gave the most promising results and achieved an accuracy of 88.8% on ten languages. They designed their own dataset comprising of ten languages being Dutch, English, French, German, Italian, Russian, Spanish, Hindi, Telegu, and Bengali. Montavon BIBREF7 generated Mel spectrogram as features for a time-delay neural network (TDNN). This network had two-dimensional convolutional layers for feature extraction. An elaborate analysis of how deep architectures outperform their shallow counterparts is presented in this reseacrch. The difficulties in classifying perceptually similar languages like German and English were also put forward in this work. It is mentioned that the proposed approach is less robust to new speakers present in the test dataset.",
        "type": "Document"
      },
      {
        "id": "e0a05c05-3392-47fa-b698-6f263198f907",
        "metadata": {
          "vector_store_key": "1908.06606-2",
          "chunk_id": 26,
          "document_id": "1908.06606",
          "start_idx": 15152,
          "end_idx": 15944
        },
        "page_content": "Then we deploy the proposed model and load the fine tuned BERT weights, attach named entity information layers and retrain the model. In this section, we devote to experimentally evaluating our proposed task and approach. The best results in tables are in bold. Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is ROUGE their only baseline?",
    "llm_answer": "No.  The context mentions a \"Random+Seed baseline\".\n",
    "context": [
      {
        "id": "ae3a37e1-e835-4141-9d4d-fe761769f946",
        "metadata": {
          "vector_store_key": "1809.08731-3",
          "chunk_id": 32,
          "document_id": "1809.08731",
          "start_idx": 17978,
          "end_idx": 18673
        },
        "page_content": "In fact, sometimes it is difficult to distinguish which one is problematic: to illustrate this, we show some extreme examples\u2014compressions which got the highest fluency rating and the lowest possible content rating by at least one rater, but the lowest fluency score and the highest content score by another\u2014in Table 6 . We, thus, hypothesize that ROUGE should contain information about fluency which is complementary to SLOR, and want to make use of references for fluency evaluation, if available. In this section, we experiment with two reference-based metrics \u2013 one trainable one, and one that can be used without fluency annotations, i.e., in the same settings as pure word-overlap metrics.",
        "type": "Document"
      },
      {
        "id": "4467e178-8dc1-4e60-9dae-0e3d1d84c370",
        "metadata": {
          "vector_store_key": "1809.08731-0",
          "chunk_id": 25,
          "document_id": "1809.08731",
          "start_idx": 14284,
          "end_idx": 14936
        },
        "page_content": "Results are shown in Table 7 . First, we can see that using SVR (line 1) to combine ROUGE-L-mult and WPSLOR outperforms both individual scores (lines 3-4) by a large margin. This serves as a proof of concept: the information contained in the two approaches is indeed complementary. Next, we consider the setting where only references and no annotated examples are available. In contrast to SVR (line 1), ROUGE-LM (line 2) has only the same requirements as conventional word-overlap metrics (besides a large corpus for training the LM, which is easy to obtain for most languages). Thus, it can be used in the same settings as other word-overlap metrics.",
        "type": "Document"
      },
      {
        "id": "22410352-756c-4cce-9a90-5ad8e73d2dbb",
        "metadata": {
          "vector_store_key": "1809.08731-3",
          "chunk_id": 31,
          "document_id": "1809.08731",
          "start_idx": 17793,
          "end_idx": 18396
        },
        "page_content": "We leave a more detailed analysis of the importance of the training data's domain for future work. ROUGE was shown to correlate well with ratings of a generated text's content or meaning at the sentence level BIBREF2 . We further expect content and fluency ratings to be correlated. In fact, sometimes it is difficult to distinguish which one is problematic: to illustrate this, we show some extreme examples\u2014compressions which got the highest fluency rating and the lowest possible content rating by at least one rater, but the lowest fluency score and the highest content score by another\u2014in Table 6 .",
        "type": "Document"
      },
      {
        "id": "fedb2120-a4a2-41c7-9163-913dbd410dfd",
        "metadata": {
          "vector_store_key": "1809.08731-0",
          "chunk_id": 26,
          "document_id": "1809.08731",
          "start_idx": 14834,
          "end_idx": 15513
        },
        "page_content": "Thus, it can be used in the same settings as other word-overlap metrics. Since ROUGE-LM\u2014an uninformed combination\u2014performs significantly better than both ROUGE-L-mult and WPSLOR on their own, it should be the metric of choice for evaluating fluency with given references. The results per compression system (cf. Table 4 ) look different from the correlations in Table 3 : Pearson and MSE are both lower. This is due to the outputs of each given system being of comparable quality. Therefore, the datapoints are similar and, thus, easier to fit for the linear function used for MSE. Pearson, in contrast, is lower due to its invariance to linear transformations of both variables.",
        "type": "Document"
      },
      {
        "id": "439bc21a-fce8-4632-bebc-fabe777f5a61",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 19,
          "document_id": "1909.00694",
          "start_idx": 11154,
          "end_idx": 11804
        },
        "page_content": "The Random+Seed baseline made use of the seed lexicon and output the corresponding label (or the reverse of it for negation) if the event's predicate is in the seed lexicon. We can see that the seed lexicon itself had practically no impact on prediction. The models in the top block performed considerably better than the random baselines. The performance gaps with their (semi-)supervised counterparts, shown in the middle block, were less than 7%. This demonstrates the effectiveness of discourse relation-based label propagation. Comparing the model variants, we obtained the highest score with the BiGRU encoder trained with the AL+CA+CO dataset.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does their system outperform the lexicon-based models?",
    "llm_answer": "The provided text states that UniSent outperforms lexicon-based models, but does not give a specific quantitative measure of the difference.\n",
    "context": [
      {
        "id": "93869f9d-2b0b-48c2-849f-c03013e50b22",
        "metadata": {
          "vector_store_key": "1809.08731-0",
          "chunk_id": 23,
          "document_id": "1809.08731",
          "start_idx": 13092,
          "end_idx": 13822
        },
        "page_content": "Note that, since having a single reference is very common, this result is highly relevant for practical applications. Considering MSE, the top two metrics are still WordSLOR and WPSLOR, with a $0.008$ and, respectively, $0.002$ lower error than the third best metric, ROUGE-L-mult. Comparing WordSLOR and WPSLOR, we find no significant differences: $0.017$ for Pearson and $0.006$ for MSE. However, WPSLOR uses a more compact LM and, hence, has a shorter training time, since the vocabulary is smaller ( $16,000$ vs. $128,000$ tokens). Next, we find that WordNCE and WPNCE perform roughly on par with word-overlap metrics. This is interesting, since they, in contrast to traditional metrics, do not require reference compressions.",
        "type": "Document"
      },
      {
        "id": "fcfa31e7-486b-44a5-8563-92a55176ca02",
        "metadata": {
          "vector_store_key": "1904.09678-0",
          "chunk_id": 6,
          "document_id": "1904.09678",
          "start_idx": 2877,
          "end_idx": 3645
        },
        "page_content": "(ii) Baseline-Lexicon: we use words in the gold standard lexicon for the sentiment learning in the target domain; for this purpose we use words INLINEFORM0 . (iii) Evaluation-Lexicon: we randomly exclude a set of words the baseline-lexicon INLINEFORM0 . In selection of the sampling size we make sure that INLINEFORM1 and INLINEFORM2 would contain a comparable number of words. In Table TABREF13 we compare the quality of UniSent with the Baseline-Lexicon as well as with the gold standard lexicon for general domain data. The results show that (i) UniSent clearly outperforms the baseline for all languages (ii) the quality of UniSent is close to manually annotated data (iii) the domain adaptation method brings small improvements for morphologically poor languages.",
        "type": "Document"
      },
      {
        "id": "439bc21a-fce8-4632-bebc-fabe777f5a61",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 19,
          "document_id": "1909.00694",
          "start_idx": 11154,
          "end_idx": 11804
        },
        "page_content": "The Random+Seed baseline made use of the seed lexicon and output the corresponding label (or the reverse of it for negation) if the event's predicate is in the seed lexicon. We can see that the seed lexicon itself had practically no impact on prediction. The models in the top block performed considerably better than the random baselines. The performance gaps with their (semi-)supervised counterparts, shown in the middle block, were less than 7%. This demonstrates the effectiveness of discourse relation-based label propagation. Comparing the model variants, we obtained the highest score with the BiGRU encoder trained with the AL+CA+CO dataset.",
        "type": "Document"
      },
      {
        "id": "974245d0-aa86-48ae-8171-9ed89aa0a7a1",
        "metadata": {
          "vector_store_key": "1811.00942-2",
          "chunk_id": 1,
          "document_id": "1811.00942",
          "start_idx": 956,
          "end_idx": 1768
        },
        "page_content": "Computing the probability of a token sequence using non-neural techniques requires a number of phrase lookups and perhaps a few arithmetic operations, whereas model inference with NLMs require large matrix multiplications consuming perhaps millions of floating point operations (FLOPs). These performance tradeoffs are worth discussing. In truth, language models exist in a quality\u2013performance tradeoff space. As model quality increases (e.g., lower perplexity), performance as measured in terms of energy consumption, query latency, etc. tends to decrease. For applications primarily running in the cloud\u2014say, machine translation\u2014practitioners often solely optimize for the lowest perplexity. This is because such applications are embarrassingly parallel and hence trivial to scale in a data center environment.",
        "type": "Document"
      },
      {
        "id": "a664e38f-37c4-43de-8819-761019f05686",
        "metadata": {
          "vector_store_key": "2002.06644-3",
          "chunk_id": 11,
          "document_id": "2002.06644",
          "start_idx": 7055,
          "end_idx": 7829
        },
        "page_content": "We further observe that ensemble models perform better than optimized BERT-based models and distilled pretrained models. Our proposed ensemble comprising of $RoBERTa_{large}$, $ALBERT_{xxlarge.v2}$, $DistilRoBERTa$ and $BERT$ outperforms all the proposed models obtaining $0.704$ F1 score, $0.733$ precision, and $71.61\\%$ Accuracy. In this paper, we investigated BERT-based architectures for sentence level subjective bias detection. We perform our experiments on a general Wikipedia corpus consisting of more than $360k$ pre and post subjective bias neutralized sentences. We found our proposed architectures to outperform the existing baselines significantly. BERT-based ensemble consisting of RoBERTa, ALBERT, DistillRoBERTa, and BERT led to the highest F1 and Accuracy.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are the main international development topics that states raise identified?",
    "llm_answer": "By analyzing discussion of international development in annual UN General Debate statements, focusing on Topic 2 (economic development and the global system) and Topic 7 (sustainable development).\n",
    "context": [
      {
        "id": "8d420e44-6cf7-40a5-9e99-40b1c06339d4",
        "metadata": {
          "vector_store_key": "1708.05873-3",
          "chunk_id": 17,
          "document_id": "1708.05873",
          "start_idx": 9831,
          "end_idx": 10462
        },
        "page_content": "The topics appears to pick up issues that are broadly related to the Cold War. There is specific mention of the Soviet Union, and detente, as well as issues such as nuclear weapons, and the Helsinki Accords. Based on these topics, we examine Topic 2 and Topic 7 as the principal \u201cinternational development\u201d topics. While a number of other topics \u2013 for example post-conflict development, Africa, Latin America, etc. \u2013 are related to development issues, Topic 2 and Topic 7 most directly capture aspects of international development. We consider these two topics more closely by contrasting the main words linked to these two topics.",
        "type": "Document"
      },
      {
        "id": "746cc846-0181-4e8d-9bbd-08c8638e658e",
        "metadata": {
          "vector_store_key": "1708.05873-1",
          "chunk_id": 32,
          "document_id": "1708.05873",
          "start_idx": 18226,
          "end_idx": 18983
        },
        "page_content": "States in South Asia and East Asia and the Pacific discuss Topic 7 the most. The figure shows that countries in North America are likely to speak about Topic 7 least. The analysis of discussion of international development in annual UN General Debate statements therefore uncovers two principle development topics: economic development and sustainable development. We find that discussion of Topic 2 is not significantly impacted by country-specific factors, such as wealth, population, democracy, levels of ODA, and conflict (although there are regional effects). However, we find that the extent to which countries discuss sustainable development (Topic 7) in their annual GD statements varies considerably according to these different structural factors.",
        "type": "Document"
      },
      {
        "id": "d97d108f-bb07-4f86-b416-4dd70f2c842d",
        "metadata": {
          "vector_store_key": "1708.05873-1",
          "chunk_id": 28,
          "document_id": "1708.05873",
          "start_idx": 15825,
          "end_idx": 16707
        },
        "page_content": "Figure FIGREF12 shows the extent to which states discuss the international development topics according to their level of democracy. Discussion of Topic 2 is fairly constant across different levels of democracy (although there are some slight fluctuations). However, the extent to which states discuss Topic 7 (sustainable development) varies considerably across different levels of democracy. Somewhat surprisingly the most autocratic states tend to discuss Topic 7 more than the slightly less autocratic states. This may be because highly autocratic governments choose to discuss development and environmental issues to avoid a focus on democracy and human rights. There is then an increase in the expected topic proportion for Topic 7 as levels of democracy increase reaching a peak at around 5 on the Polity scale, after this there is a gradual decline in discussion of Topic 7.",
        "type": "Document"
      },
      {
        "id": "5e7a5f6f-9337-4d7a-94a2-e9ee13e6b108",
        "metadata": {
          "vector_store_key": "1708.05873-3",
          "chunk_id": 11,
          "document_id": "1708.05873",
          "start_idx": 6387,
          "end_idx": 7027
        },
        "page_content": "We provide a brief description of each of the 16 topics here. Topic 1 - Security and cooperation in Europe. The first topic is related to issues of security and cooperation, with a focus on Central and Eastern Europe. Topic 2 - Economic development and the global system. This topic is related to economic development, particularly around the global economic system. The focus on `trade', `growth', `econom-', `product', `growth', `financ-', and etc. suggests that Topic 2 represent a more traditional view of international development in that the emphasis is specifically on economic processes and relations. Topic 3 - Nuclear disarmament.",
        "type": "Document"
      },
      {
        "id": "ea8afd17-88f5-458f-b24e-7d6e725f0d22",
        "metadata": {
          "vector_store_key": "1708.05873-1",
          "chunk_id": 27,
          "document_id": "1708.05873",
          "start_idx": 15434,
          "end_idx": 16079
        },
        "page_content": "It is also worth noting the spikes in discussion of Topic 2 and Topic 7 for countries that receive negative levels of ODA. These are countries that are effectively repaying more in loans to lenders than they are receiving in ODA. These countries appear to raise development issues far more in their GD statements, which is perhaps not altogether surprising. We also consider the effects of democracy on the expected topic proportions of both development topics using the Polity IV measure of democracy BIBREF10 . Figure FIGREF12 shows the extent to which states discuss the international development topics according to their level of democracy.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is the semantic hierarchy representation used for any task?",
    "llm_answer": "Yes.\n",
    "context": [
      {
        "id": "e1311b0a-2e3f-400a-86de-ea2a2be0a74b",
        "metadata": {
          "vector_store_key": "1909.12140-0",
          "chunk_id": 13,
          "document_id": "1909.12140",
          "start_idx": 7116,
          "end_idx": 7420
        },
        "page_content": "We developed and implemented a discourse-aware syntactic TS approach that recursively splits and rephrases complex English or German sentences into a semantic hierarchy of simplified sentences. The resulting lightweight semantic representation can be used to facilitate and improve a variety of AI tasks.",
        "type": "Document"
      },
      {
        "id": "f7263449-5dc5-4f80-aef7-fcfaef97f84e",
        "metadata": {
          "vector_store_key": "1909.12140-0",
          "chunk_id": 12,
          "document_id": "1909.12140",
          "start_idx": 6266,
          "end_idx": 7116
        },
        "page_content": "Moreover, most current Open IE approaches output only a loose arrangement of extracted tuples that are hard to interpret as they ignore the context under which a proposition is complete and correct and thus lack the expressiveness needed for a proper interpretation of complex assertions BIBREF8. As illustrated in Figure FIGREF9, with the help of the semantic hierarchy generated by our discourse-aware sentence splitting approach the output of Open IE systems can be easily enriched with contextual information that allows to restore the semantic relationship between a set of propositions and, hence, preserve their interpretability in downstream tasks. We developed and implemented a discourse-aware syntactic TS approach that recursively splits and rephrases complex English or German sentences into a semantic hierarchy of simplified sentences.",
        "type": "Document"
      },
      {
        "id": "07c95130-d362-4bcc-89aa-6f66d690e36e",
        "metadata": {
          "vector_store_key": "1907.05664-2",
          "chunk_id": 3,
          "document_id": "1907.05664",
          "start_idx": 1820,
          "end_idx": 2435
        },
        "page_content": "Our goal in this paper is to test the limits of the use of such a technique for more complex tasks, where the notion of input importance might not be as simple as in topic classification or sentiment analysis. We changed from a classification task to a generative task and chose a more complex one than text translation (in which we can easily find a word to word correspondence/importance between input and output). We chose text summarization. We consider abstractive and informative text summarization, meaning that we write a summary \u201cin our own words\" and retain the important information of the original text.",
        "type": "Document"
      },
      {
        "id": "9b89b39c-0af3-47f6-b5c9-e4eb31c55c17",
        "metadata": {
          "vector_store_key": "1909.12140-4",
          "chunk_id": 5,
          "document_id": "1909.12140",
          "start_idx": 3284,
          "end_idx": 4020
        },
        "page_content": "To establish a semantic hierarchy between them, two subtasks are carried out: First, we set up a contextual hierarchy between the split sentences by connecting them with information about their hierarchical level, similar to the concept of nuclearity in RST. For this purpose, we distinguish core sentences (nuclei), which carry the key information of the input, from accompanying contextual sentences (satellites) that disclose additional information about it. To differentiate between those two types of constituents, the transformation patterns encode a simple syntax-based approach where subordinate clauses/phrases are classified as context sentences, while superordinate as well as coordinate clauses/phrases are labelled as core.",
        "type": "Document"
      },
      {
        "id": "f771669b-7551-49c2-8dd8-22fdbc5698ec",
        "metadata": {
          "vector_store_key": "1909.12140-4",
          "chunk_id": 6,
          "document_id": "1909.12140",
          "start_idx": 3488,
          "end_idx": 4243
        },
        "page_content": "To differentiate between those two types of constituents, the transformation patterns encode a simple syntax-based approach where subordinate clauses/phrases are classified as context sentences, while superordinate as well as coordinate clauses/phrases are labelled as core. Second, we aim to restore the semantic relationship between the disembedded components. For this purpose, we identify and classify the rhetorical relations that hold between the simplified sentences, making use of both syntactic features, which are derived from the input's parse tree structure, and lexical features in the form of cue phrases. Following the work of Taboada13, they are mapped to a predefined list of rhetorical cue words to infer the type of rhetorical relation.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is the model evaluated?",
    "llm_answer": "Yes.\n",
    "context": [
      {
        "id": "64edb5cd-35ef-4eaa-bfb9-4e585e04e2a3",
        "metadata": {
          "vector_store_key": "1809.04960-1",
          "chunk_id": 29,
          "document_id": "1809.04960",
          "start_idx": 16925,
          "end_idx": 17580
        },
        "page_content": "Inspired by the evaluation methods of dialogue models, we formulate the evaluation as a ranking problem. Given a piece of news and a set of candidate comments, the comment model should return the rank of the candidate comments. The candidate comment set consists of the following parts: Correct: The ground-truth comments of the corresponding news provided by the human. Plausible: The 50 most similar comments to the news. We use the news as the query to retrieve the comments that appear in the training set based on the cosine similarity of their tf-idf values. We select the top 50 comments that are not the correct comments as the plausible comments.",
        "type": "Document"
      },
      {
        "id": "11d162de-1c00-4142-aecc-2d7ca31e9d35",
        "metadata": {
          "vector_store_key": "1809.04960-1",
          "chunk_id": 35,
          "document_id": "1809.04960",
          "start_idx": 20260,
          "end_idx": 20947
        },
        "page_content": "Unlike seq2seq, IR is based on a retrieval framework, so it achieves much better performance. Following previous work BIBREF0 , we evaluate the models under the generative evaluation setting. The retrieval-based models generate the comments by selecting a comment from the candidate set. The candidate set contains the comments in the training set. Unlike the retrieval evaluation, the reference comments may not appear in the candidate set, which is closer to real-world settings. Generative-based models directly generate comments without a candidate set. We compare the generated comments of either the retrieval-based models or the generative models with the five reference comments.",
        "type": "Document"
      },
      {
        "id": "1ef3e3a7-71e4-44eb-9834-924dc0449d50",
        "metadata": {
          "vector_store_key": "1809.04960-1",
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 20725,
          "end_idx": 21462
        },
        "page_content": "We compare the generated comments of either the retrieval-based models or the generative models with the five reference comments. We select four popular metrics in text generation to compare the model outputs with the references: BLEU BIBREF14 , METEOR BIBREF15 , ROUGE BIBREF16 , CIDEr BIBREF17 . Table TABREF32 shows the performance for our models and the baselines in generative evaluation. Similar to the retrieval evaluation, our proposed model outperforms the other unsupervised methods, which are TF-IDF, NVDM, and LDA, in generative evaluation. Still, the supervised IR achieves better scores than the seq2seq model. With the help of our proposed model, both IR and S2S achieve an improvement under the semi-supervised scenarios.",
        "type": "Document"
      },
      {
        "id": "73bcb763-86e3-4d05-a621-c8c23559fd22",
        "metadata": {
          "vector_store_key": "2003.07433-7",
          "chunk_id": 55,
          "document_id": "2003.07433",
          "start_idx": 30011,
          "end_idx": 30401
        },
        "page_content": "As the clinical surveys are trusted and understandable method, we believe that this method will be able to gain trust of clinicians towards early detection of PTSD. Moreover, our proposed LAXARY model, which is first of its kind, can be used to develop any type of mental disorder Linguistic Dictionary providing a generalized and trustworthy mental health assessment framework of any kind.",
        "type": "Document"
      },
      {
        "id": "3f614345-db09-4d59-a98c-398de8ba699b",
        "metadata": {
          "vector_store_key": "1907.05664-0",
          "chunk_id": 25,
          "document_id": "1907.05664",
          "start_idx": 12886,
          "end_idx": 13719
        },
        "page_content": "We obtain mitigated results: for some texts, we observe a quick degradation when deleting important words which are not observed when deleting unimportant words (see Figure 4 ), but for other test examples we don't observe a significant difference between the two settings (see Figure 5 ). One might argue that the second summary in Figure 5 is better than the first one as it makes better sentences but as the model generates inaccurate summaries, we do not wish to make such a statement. This however allows us to say that the attribution generated for the text at the origin of the summaries in Figure 4 are truthful in regard to the network's computation and we may use it for further studies of the example, whereas for the text at the origin of Figure 5 we shouldn't draw any further conclusions from the attribution generated.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How better is accuracy of new model compared to previously reported models?",
    "llm_answer": "The new model outperforms previous models in both EM and F1 scores, with improvements ranging from 3.69% to 6.39% in F1 and 5.64% to 6.39% in EM score.  It also demonstrates greater robustness to noise in data.\n",
    "context": [
      {
        "id": "b0e48c22-4859-4ba0-8ab2-939d7e84a941",
        "metadata": {
          "vector_store_key": "2001.00137-3",
          "chunk_id": 40,
          "document_id": "2001.00137",
          "start_idx": 23303,
          "end_idx": 24041
        },
        "page_content": "The per-class F1 score was also evaluated in the form of normalized confusion matrices, showing that our model was able to improve the overall performance by better balancing the accuracy of each class, trading-off small decreases in high achieving class for significant improvements in lower performing ones. In the Chatbot dataset, accuracy improvement was achieved even without trade-off, with the highest achieving classes maintaining their accuracy while the lower achieving class saw improvement. Further evaluation on the F1-scores decay in the presence of noise demonstrated that our model is more robust than the baseline models when considering noisy data, be that in the form of incorrect sentences or sentences with STT error.",
        "type": "Document"
      },
      {
        "id": "c3b4b0ea-1fb0-4db4-a1e3-cca1d982ef2e",
        "metadata": {
          "vector_store_key": "1809.04960-4",
          "chunk_id": 38,
          "document_id": "1809.04960",
          "start_idx": 21924,
          "end_idx": 22725
        },
        "page_content": "It shows that IR+Proposed can outperform the supervised IR model given the same paired dataset. It concludes that the proposed model can exploit the unpaired data to further improve the performance of the supervised model. Although our proposed model can achieve better performance than previous models, there are still remaining two questions: why our model can outperform them, and how to further improve the performance. To address these queries, we perform error analysis to analyze the error types of our model and the baseline models. We select TF-IDF, S2S, and IR as the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types of comments as described in the above retrieval evaluation setting: Correct, Plausible, Popular, and Random.",
        "type": "Document"
      },
      {
        "id": "6ad65045-962e-44bb-ae24-b2d2ee55f4e2",
        "metadata": {
          "vector_store_key": "1908.06606-3",
          "chunk_id": 31,
          "document_id": "1908.06606",
          "start_idx": 18280,
          "end_idx": 19072
        },
        "page_content": "QANet outperformed BERT-Base with 3.56% score in F$_1$-score but underperformed it with 0.75% score in EM-score. Compared with BERT-Base, our model led to a 5.64% performance improvement in EM-score and 3.69% in F$_1$-score. Although our model didn't outperform much with QANet in F$_1$-score (only 0.13%), our model significantly outperformed it with 6.39% score in EM-score. To further investigate the effects of named entity information and two-stage training mechanism for our model, we apply ablation analysis to see the improvement brought by each of them, where $\\times $ refers to removing that part from our model. As demonstrated in Table TABREF25, with named entity information enabled, two-stage training mechanism improved the result by 4.36% in EM-score and 3.8% in F$_1$-score.",
        "type": "Document"
      },
      {
        "id": "059820ad-3f5b-44c5-a3e8-611cfcfed0d9",
        "metadata": {
          "vector_store_key": "1705.09665-0",
          "chunk_id": 35,
          "document_id": "1705.09665",
          "start_idx": 20067,
          "end_idx": 20954
        },
        "page_content": "We compared out-of-sample predictive performance via leave-one-community-out cross validation using random forest regressors with ensembles of size 100, and otherwise default hyperparameters BIBREF26 . A model predicting average monthly retention based on a community's average distinctiveness and dynamicity achieves an average mean squared error ( INLINEFORM0 ) of INLINEFORM1 and INLINEFORM2 , while an analogous model predicting based on a community's size and average activity level (both log-transformed) achieves INLINEFORM4 and INLINEFORM5 . The difference between the two models is not statistically significant ( INLINEFORM6 , Wilcoxon signed-rank test). However, combining features from both models results in a large and statistically significant improvement over each independent model ( INLINEFORM7 , INLINEFORM8 , INLINEFORM9 Bonferroni-corrected pairwise Wilcoxon tests).",
        "type": "Document"
      },
      {
        "id": "f5dba2ac-6a54-48f8-ba6c-7864ece5409c",
        "metadata": {
          "vector_store_key": "2001.00137-3",
          "chunk_id": 36,
          "document_id": "2001.00137",
          "start_idx": 21142,
          "end_idx": 21770
        },
        "page_content": "The table also indicates the level of noise in each dataset with the already mentioned iBLEU score, where 0 means no noise and higher values mean higher quantity of noise. As expected, the models' accuracy degrade with the increase in noise, thus F1-scores of gtts-witai are higher than macsay-witai. However, while the other models decay rapidly in the presence of noise, our model does not only outperform them but does so with a wider margin. This is shown with the increasing robustness curve in Fig. FIGREF41 and can be demonstrated by macsay-witai outperforming the baseline models by twice the gap achieved by gtts-witai.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does the active learning model work?",
    "llm_answer": "Active learning involves a learning engine (a classifier) and a selection engine (based on a sampling strategy).  The selection engine chooses samples needing annotation from unlabeled data.  These labeled samples are added to the training set, and the classifier is retrained.  This process iterates until desired accuracy is reached or a predefined threshold is met.  Different scoring models, like information entropy, or scoring models combining word score, link score, and sequence score (and using neural networks) can be used in the selection engine.\n",
    "context": [
      {
        "id": "9cbb6262-c495-4bf1-962e-723ecfdbce15",
        "metadata": {
          "vector_store_key": "1908.08419-1",
          "chunk_id": 13,
          "document_id": "1908.08419",
          "start_idx": 7933,
          "end_idx": 8693
        },
        "page_content": "However, in some complicated tasks, such as CWS and NER, only considering the uncertainty of classifier is obviously not enough. Active learning methods can generally be described into two parts: a learning engine and a selection engine BIBREF28 . The learning engine is essentially a classifier, which is mainly used for training of classification problems. The selection engine is based on the sampling strategy, which chooses samples that need to be relabeled by annotators from unlabeled data. Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, a CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively.",
        "type": "Document"
      },
      {
        "id": "7bace013-ece8-4157-b04f-ba386e8c94af",
        "metadata": {
          "vector_store_key": "1908.08419-1",
          "chunk_id": 14,
          "document_id": "1908.08419",
          "start_idx": 8441,
          "end_idx": 9126
        },
        "page_content": "In this paper, a CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively. Fig. FIGREF7 and Algorithm SECREF3 demonstrate the procedure of CWS based on active learning. First, we train a CRF-based segmenter by train set. Then, the segmenter is employed to annotate the unlabeled set roughly. Subsequently, information entropy based scoring model picks INLINEFORM0 -lowest ranking samples for annotators to relabel. Meanwhile, the train sets and unlabeled sets are updated. Finally, we re-train the segmenter. The above steps iterate until the desired accuracy is achieved or the number of iterations has reached a predefined threshold.",
        "type": "Document"
      },
      {
        "id": "fa8185d8-687d-4b00-93a8-75d7022bafc5",
        "metadata": {
          "vector_store_key": "1908.08419-1",
          "chunk_id": 3,
          "document_id": "1908.08419",
          "start_idx": 1904,
          "end_idx": 2606
        },
        "page_content": "Moreover, the task in medical domain is rarely dabbled, and only one related work on transfer learning is found in recent literatures BIBREF8 . However, researches related to transfer learning mostly remain in general domains, causing a major problem that a considerable amount of manually annotated data is required, when introducing the models into specific domains. One of the solutions for this obstacle is to use active learning, where only a small scale of samples are selected and labeled in an active manner. Active learning methods are favored by the researchers in many natural language processing (NLP) tasks, such as text classification BIBREF9 and named entity recognition (NER) BIBREF10 .",
        "type": "Document"
      },
      {
        "id": "e967d0a5-321a-4fd7-8476-c0ef6c37a82b",
        "metadata": {
          "vector_store_key": "1908.08419-1",
          "chunk_id": 11,
          "document_id": "1908.08419",
          "start_idx": 6704,
          "end_idx": 7439
        },
        "page_content": "Unlike the above method, we propose an active learning approach for CWS in medical text, which combines information entropy with neural network to effectively reduce annotation cost. Active learning BIBREF22 mainly aims to ease the data collection process by automatically deciding which instances should be labeled by annotators to train a model as quickly and effectively as possible BIBREF23 . The sampling strategy plays a key role in active learning. In the past decade, the rapid development of active learning has resulted in various sampling strategies, such as uncertainty sampling BIBREF24 , query-by-committee BIBREF25 and information gain BIBREF26 . Currently, the most mainstream sampling strategy is uncertainty sampling.",
        "type": "Document"
      },
      {
        "id": "f1d2ea89-a5f9-48e1-9646-477e08134cb3",
        "metadata": {
          "vector_store_key": "1908.08419-1",
          "chunk_id": 4,
          "document_id": "1908.08419",
          "start_idx": 2278,
          "end_idx": 3007
        },
        "page_content": "Active learning methods are favored by the researchers in many natural language processing (NLP) tasks, such as text classification BIBREF9 and named entity recognition (NER) BIBREF10 . However, only a handful of works are conducted on CWS BIBREF2 , and few focuses on medical domain tasks. Given the aforementioned challenges and current researches, we propose a word segmentation method based on active learning. To model the segmentation history, we incorporate a sampling strategy consisting of word score, link score and sequence score, which effectively evaluates the segmentation decisions. Specifically, we combine information branch and gated neural network to determine if the segment is a legal word, i.e., word score.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Did the annotators agreed and how much?",
    "llm_answer": "Inter-annotator agreement was found to be sufficiently high, and moderate to substantial, as measured by Fleiss' Kappa.  A qualified majority vote (at least 3 annotators agreeing) on coreference pairs reached 90.5%.\n",
    "context": [
      {
        "id": "c2650d93-c3c3-4583-b079-29db9f7fc689",
        "metadata": {
          "vector_store_key": "1703.05260-3",
          "chunk_id": 32,
          "document_id": "1703.05260",
          "start_idx": 17652,
          "end_idx": 18317
        },
        "page_content": "The annotation was rather time-consuming due to the complexity of the task, and thus we decided for single annotation mode. To assess annotation quality, a small sample of texts was annotated by all four annotators and their inter-annotator agreement was measured (see Section \"Inter-Annotator Agreement\" ). It was found to be sufficiently high. Annotation of the corpus together with some pre- and post-processing of the data required about 500 hours of work. All stories were annotated with event and participant types (a total of 12,188 and 43,946 instances, respectively). On average there were 7 coreference chains per story with an average length of 6 tokens.",
        "type": "Document"
      },
      {
        "id": "231a13cd-1fab-448e-a94c-8f9383c95da0",
        "metadata": {
          "vector_store_key": "1703.05260-3",
          "chunk_id": 41,
          "document_id": "1703.05260",
          "start_idx": 22695,
          "end_idx": 23634
        },
        "page_content": "Here, we introduced the additional ad-hoc participant label Scr_Part_hairdresser_customer. While Susan $_{\\textsc {\\scriptsize {ScrPart\\_hairdresser}}}$ is cutting my $_{\\textsc {\\scriptsize {ScrPart\\_customer}}}$ hair we $_{\\textsc {\\scriptsize Scr\\_Part\\_hairdresser\\_customer}}$ usually talk a bit. In order to calculate inter-annotator agreement, a total of 30 stories from 6 scenarios were randomly chosen for parallel annotation by all 4 annotators after the first annotation phase. We checked the agreement on these data using Fleiss' Kappa BIBREF4 . The results are shown in Figure 4 and indicate moderate to substantial agreement BIBREF5 . Interestingly, if we calculated the Kappa only on the subset of cases that were annotated with script-specific event and participant labels by all annotators, results were better than those of the evaluation on all labeled instances (including also unrelated and related non-script events).",
        "type": "Document"
      },
      {
        "id": "68e72fd8-5862-4eb1-8a6c-dcbda9f8ca12",
        "metadata": {
          "vector_store_key": "1703.05260-1",
          "chunk_id": 22,
          "document_id": "1703.05260",
          "start_idx": 12228,
          "end_idx": 12901
        },
        "page_content": "In the first annotation phase, annotators were asked to mark verbs and noun phrases that have an event or participant type, that is not listed in the template, as MissScrEv/ MissScrPart (missing script event or participant, resp.). These annotations were used as a basis for extending the templates (see Section \"Modification of the Schema\" ) and replaced later by newly introduced labels or ScrEv_other and ScrPart_other respectively. All noun phrases were annotated with coreference information indicating which entities denote the same discourse referent. The annotation was done by linking heads of NPs (see Example UID21 , where the links are indicated by coindexing).",
        "type": "Document"
      },
      {
        "id": "a7206ebd-d04d-47ec-b00f-0ae9d605cbde",
        "metadata": {
          "vector_store_key": "1703.05260-3",
          "chunk_id": 43,
          "document_id": "1703.05260",
          "start_idx": 23985,
          "end_idx": 24623
        },
        "page_content": "For coreference chain annotation, we calculated the percentage of pairs which were annotated by at least 3 annotators (qualified majority vote) compared to the set of those pairs annotated by at least one person (see Figure 4 ). We take the result of 90.5% between annotators to be a good agreement. Figure 5 gives an overview of the number of event and participant types provided in the templates. Taking a flight and getting a haircut stand out with a large number of both event and participant types, which is due to the inherent complexity of the scenarios. In contrast, planting a tree and going on a train contain the fewest labels.",
        "type": "Document"
      },
      {
        "id": "ed0aba31-17e0-44d0-9885-4f586a56aea7",
        "metadata": {
          "vector_store_key": "1703.05260-3",
          "chunk_id": 42,
          "document_id": "1703.05260",
          "start_idx": 23254,
          "end_idx": 23985
        },
        "page_content": "Interestingly, if we calculated the Kappa only on the subset of cases that were annotated with script-specific event and participant labels by all annotators, results were better than those of the evaluation on all labeled instances (including also unrelated and related non-script events). This indicates one of the challenges of the annotation task: In many cases it is difficult to decide whether a particular event should be considered a central script event, or an event loosely related or unrelated to the script. For coreference chain annotation, we calculated the percentage of pairs which were annotated by at least 3 annotators (qualified majority vote) compared to the set of those pairs annotated by at least one person",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What datasets are used to evaluate this approach?",
    "llm_answer": "A Chinese dataset (198,112 news articles from Tencent News), a dataset comprising 3 languages (English, French, German), and a dataset annotated based on Chinese pathology reports (1,899 training instances, 815 test instances).\n",
    "context": [
      {
        "id": "e0a05c05-3392-47fa-b698-6f263198f907",
        "metadata": {
          "vector_store_key": "1908.06606-2",
          "chunk_id": 26,
          "document_id": "1908.06606",
          "start_idx": 15152,
          "end_idx": 15944
        },
        "page_content": "Then we deploy the proposed model and load the fine tuned BERT weights, attach named entity information layers and retrain the model. In this section, we devote to experimentally evaluating our proposed task and approach. The best results in tables are in bold. Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences).",
        "type": "Document"
      },
      {
        "id": "4a014d75-2393-4835-8374-b2d3f7af40d4",
        "metadata": {
          "vector_store_key": "2003.07433-5",
          "chunk_id": 35,
          "document_id": "2003.07433",
          "start_idx": 19095,
          "end_idx": 19788
        },
        "page_content": "Here we used first person singular number pronouns (i.e., I, me, mine etc.) that means we only count those sentences or segments which are only related to first person singular number i.e., related to the person himself. Category selection: We have to define the categories of each word set thus we can analyze the categories as well as dimensions' text analysis scores. We chose three categories based on the three different surveys: 1) DOSPERT scale; 2) BSSS scale; and 3) VIAS scale. Dimension selection: We have to define the word sets (also called dimension) for each category. We chose one dimension for each of the questions under each category to reflect real survey system evaluation.",
        "type": "Document"
      },
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      },
      {
        "id": "6acc5ad9-079c-46ec-9298-4912e7c2aae7",
        "metadata": {
          "vector_store_key": "1809.04960-1",
          "chunk_id": 23,
          "document_id": "1809.04960",
          "start_idx": 13537,
          "end_idx": 14236
        },
        "page_content": "Hence, the model is trained on both unpaired data INLINEFORM3 , and paired data INLINEFORM4 . We select a large-scale Chinese dataset BIBREF0 with millions of real comments and a human-annotated test set to evaluate our model. The dataset is collected from Tencent News, which is one of the most popular Chinese websites for news and opinion articles. The dataset consists of 198,112 news articles. Each piece of news contains a title, the content of the article, and a list of the users' comments. Following the previous work BIBREF0 , we tokenize all text with the popular python package Jieba, and filter out short articles with less than 30 words in content and those with less than 20 comments.",
        "type": "Document"
      },
      {
        "id": "d50bd955-993d-4140-b06f-46c6425afab9",
        "metadata": {
          "vector_store_key": "1908.06606-0",
          "chunk_id": 27,
          "document_id": "1908.06606",
          "start_idx": 15944,
          "end_idx": 16737
        },
        "page_content": "These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage of predictions that match any one of the ground truth answers exactly. The F$_1$-score metric is a looser metric measures the average overlap between the prediction and ground truth answer. To implement deep neural network models, we utilize the Keras library BIBREF36 with TensorFlow BIBREF37 backend.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How was the dataset collected?",
    "llm_answer": "The datasets were collected from Tencent News (comments, articles), Shuguang Hospital Affiliated to Shanghai University of Traditional Chinese Medicine (EHRs), and Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital.  Dialogue data was collected through worker conversations.\n",
    "context": [
      {
        "id": "6acc5ad9-079c-46ec-9298-4912e7c2aae7",
        "metadata": {
          "vector_store_key": "1809.04960-1",
          "chunk_id": 23,
          "document_id": "1809.04960",
          "start_idx": 13537,
          "end_idx": 14236
        },
        "page_content": "Hence, the model is trained on both unpaired data INLINEFORM3 , and paired data INLINEFORM4 . We select a large-scale Chinese dataset BIBREF0 with millions of real comments and a human-annotated test set to evaluate our model. The dataset is collected from Tencent News, which is one of the most popular Chinese websites for news and opinion articles. The dataset consists of 198,112 news articles. Each piece of news contains a title, the content of the article, and a list of the users' comments. Following the previous work BIBREF0 , we tokenize all text with the popular python package Jieba, and filter out short articles with less than 30 words in content and those with less than 20 comments.",
        "type": "Document"
      },
      {
        "id": "60668133-2469-41a2-a2b8-f09012c66f8d",
        "metadata": {
          "vector_store_key": "1908.08419-3",
          "chunk_id": 32,
          "document_id": "1908.08419",
          "start_idx": 18688,
          "end_idx": 19393
        },
        "page_content": "We collect 204 EHRs with cardiovascular diseases from the Shuguang Hospital Affiliated to Shanghai University of Traditional Chinese Medicine and each contains 27 types of records. We choose 4 different types with a total of 3868 records from them, which are first course reports, medical records, chief ward round records and discharge records. The detailed information of EHRs are listed in Table TABREF32 . We split our datasets as follows. First, we randomly select 3200 records from 3868 records as unlabeled set. Then, we manually annotate remaining 668 records as labeled set, which contains 1170 sentences. Finally, we divide labeled set into train set and test set with the ratio of 7:3 randomly.",
        "type": "Document"
      },
      {
        "id": "24f8dfec-c722-47c3-89b3-12384c534e78",
        "metadata": {
          "vector_store_key": "2002.11893-9",
          "chunk_id": 14,
          "document_id": "2002.11893",
          "start_idx": 8370,
          "end_idx": 9067
        },
        "page_content": "Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states. Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three experts were employed to manually annotate dialogue acts and states for 50 dialogues. The results show that our annotations are of high quality.",
        "type": "Document"
      },
      {
        "id": "e0a05c05-3392-47fa-b698-6f263198f907",
        "metadata": {
          "vector_store_key": "1908.06606-2",
          "chunk_id": 26,
          "document_id": "1908.06606",
          "start_idx": 15152,
          "end_idx": 15944
        },
        "page_content": "Then we deploy the proposed model and load the fine tuned BERT weights, attach named entity information layers and retrain the model. In this section, we devote to experimentally evaluating our proposed task and approach. The best results in tables are in bold. Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences).",
        "type": "Document"
      },
      {
        "id": "ad94e893-7d83-4d45-9d8d-398abbc7991e",
        "metadata": {
          "vector_store_key": "1809.04960-1",
          "chunk_id": 30,
          "document_id": "1809.04960",
          "start_idx": 17386,
          "end_idx": 18100
        },
        "page_content": "We select the top 50 comments that are not the correct comments as the plausible comments. Popular: The 50 most popular comments from the dataset. We count the frequency of each comments in the training set, and select the 50 most frequent comments to form the popular comment set. The popular comments are the general and meaningless comments, such as \u201cYes\u201d, \u201cGreat\u201d, \u201cThat's right', and \u201cMake Sense\u201d. These comments are dull and do not carry any information, so they are regarded as incorrect comments. Random: After selecting the correct, plausible, and popular comments, we fill the candidate set with randomly selected comments from the training set so that there are 200 unique comments in the candidate set.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What models other than standalone BERT is new model compared to?",
    "llm_answer": "RoBERTa, ALBERT, and distilled BERT-based models.\n",
    "context": [
      {
        "id": "1fb8dfaa-e118-4b2a-8bf6-3e36fff8cf0c",
        "metadata": {
          "vector_store_key": "2002.06644-3",
          "chunk_id": 4,
          "document_id": "2002.06644",
          "start_idx": 2479,
          "end_idx": 3239
        },
        "page_content": "We explore various BERT-based models, including BERT, RoBERTa, ALBERT, with their base and large specifications along with their native classifiers. We propose an ensemble model exploiting predictions from these models using multiple ensembling techniques. We show that our model outperforms the baselines by a margin of $5.6$ of F1 score and $5.95\\%$ of Accuracy. In this section, we outline baseline models like $BERT_{large}$. We further propose three approaches: optimized BERT-based models, distilled pretrained models, and the use of ensemble methods for the task of subjectivity detection. FastTextBIBREF4: It uses bag of words and bag of n-grams as features for text classification, capturing partial information about the local word order efficiently.",
        "type": "Document"
      },
      {
        "id": "2de7e36b-9a99-411b-9e59-98d488461e99",
        "metadata": {
          "vector_store_key": "2002.06644-1",
          "chunk_id": 6,
          "document_id": "2002.06644",
          "start_idx": 3726,
          "end_idx": 4586
        },
        "page_content": "Optimized BERT-based models: We use BERT-based models optimized as in BIBREF6 and BIBREF7, pretrained on a dataset as large as twelve times as compared to $BERT_{large}$, with bigger batches, and longer sequences. ALBERT, introduced in BIBREF7, uses factorized embedding parameterization and cross-layer parameter sharing for parameter reduction. These optimizations have led both the models to outperform $BERT_{large}$ in various benchmarking tests, like GLUE for text classification and SQuAD for Question Answering. Distilled BERT-based models: Secondly, we propose to use distilled BERT-based models, as introduced in BIBREF8. They are smaller general-purpose language representation model, pre-trained by leveraging distillation knowledge. This results in significantly smaller and faster models with performance comparable to their undistilled versions.",
        "type": "Document"
      },
      {
        "id": "814822da-592c-40ae-bb7c-e78761735daf",
        "metadata": {
          "vector_store_key": "2002.06644-3",
          "chunk_id": 10,
          "document_id": "2002.06644",
          "start_idx": 6316,
          "end_idx": 7055
        },
        "page_content": "Our proposed methodology, the use of finetuned optimized BERT based models, and BERT-based ensemble models outperform the baselines for all the metrics. Among the optimized BERT based models, $RoBERTa_{large}$ outperforms all other non-ensemble models and the baselines for all metrics. It further achieves a maximum recall of $0.681$ for all the proposed models. We note that DistillRoBERTa, a distilled model, performs competitively, achieving $69.69\\%$ accuracy, and $0.672$ F1 score. This observation shows that distilled pretrained models can replace their undistilled counterparts in a low-computing environment. We further observe that ensemble models perform better than optimized BERT-based models and distilled pretrained models.",
        "type": "Document"
      },
      {
        "id": "aa94fb66-c910-42df-b61d-4a5241934b5b",
        "metadata": {
          "vector_store_key": "1908.06606-0",
          "chunk_id": 29,
          "document_id": "1908.06606",
          "start_idx": 17099,
          "end_idx": 17817
        },
        "page_content": "Due to the high cost of pre-training BERT language model, we directly adopt parameters pre-trained by Google in Chinese general corpus. The named entity recognition is applied on both pathology report texts and query texts. Since BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large. Prediction layer is attached at the end of the original BERT-Base model and we fine tune it on our dataset.",
        "type": "Document"
      },
      {
        "id": "df8a6fb3-d802-4d89-a594-11a767c5b554",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 29,
          "document_id": "2001.00137",
          "start_idx": 17443,
          "end_idx": 18171
        },
        "page_content": "Nearest Centroid with Euclidian metric, where classification is done by representing each class with a centroid; Bernoulli Naive Bayes with smoothing parameter $alpha=10^{-2}$; K-means clustering with 2 clusters and L2 penalty; and Logistic Regression classifier with L2 penalty, tolerance of $10^{-4}$ and regularization term of $1.0$. Most often, the best performing classifier was MLP. Conventional BERT is a BERT-base-uncased model, meaning that it has 12 transformer blocks $L$, hidden size $H$ of 768, and 12 self-attention heads $A$. The model is fine-tuned with our dataset on 2 Titan X GPUs for 3 epochs with Adam Optimizer, learning rate of $2*10^{-5}$, maximum sequence length of 128, and warm up proportion of $0.1$.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is the performance difference between this method and the baseline?",
    "llm_answer": "The context states that the proposed methodology outperforms the baselines for all metrics.  No specific performance difference is given.\n",
    "context": [
      {
        "id": "814822da-592c-40ae-bb7c-e78761735daf",
        "metadata": {
          "vector_store_key": "2002.06644-3",
          "chunk_id": 10,
          "document_id": "2002.06644",
          "start_idx": 6316,
          "end_idx": 7055
        },
        "page_content": "Our proposed methodology, the use of finetuned optimized BERT based models, and BERT-based ensemble models outperform the baselines for all the metrics. Among the optimized BERT based models, $RoBERTa_{large}$ outperforms all other non-ensemble models and the baselines for all metrics. It further achieves a maximum recall of $0.681$ for all the proposed models. We note that DistillRoBERTa, a distilled model, performs competitively, achieving $69.69\\%$ accuracy, and $0.672$ F1 score. This observation shows that distilled pretrained models can replace their undistilled counterparts in a low-computing environment. We further observe that ensemble models perform better than optimized BERT-based models and distilled pretrained models.",
        "type": "Document"
      },
      {
        "id": "1c3b07ea-e9fe-4ebb-904a-6c06e24dd2db",
        "metadata": {
          "vector_store_key": "2001.00137-0",
          "chunk_id": 34,
          "document_id": "2001.00137",
          "start_idx": 20087,
          "end_idx": 20704
        },
        "page_content": "Similarly to Table TABREF37, we evaluate our model with the original Twitter dataset, the corrected version and both original and corrected tweets. It can be seen that our model is able to improve the overall performance by improving the accuracy of the lower performing classes. In the Inc dataset, the true class 1 in BERT performs with approximately 50%. However, Stacked DeBERT is able to improve that to 72%, although to a cost of a small decrease in performance of class 0. A similar situation happens in the remaining two datasets, with improved accuracy in class 0 from 64% to 84% and 60% to 76% respectively.",
        "type": "Document"
      },
      {
        "id": "3e994528-05ae-4bc0-9cc2-11c767356392",
        "metadata": {
          "vector_store_key": "1908.08419-0",
          "chunk_id": 42,
          "document_id": "1908.08419",
          "start_idx": 24326,
          "end_idx": 25223
        },
        "page_content": "The experimental results of two sampling strategies with 30 iterations on four different proportions of relabeled data are shown in Fig. FIGREF45 , where x-axis represents the number of iterations and y-axis denotes the INLINEFORM0 -score of the segmenter. Scoring strategy shows consistent improvements over uncertainty sampling in the early iterations, indicating that scoring strategy is more capable of selecting representative samples. Furthermore, we also investigate the relations between the best INLINEFORM0 -score and corresponding number of iteration on two sampling strategies, which is depicted in Fig. FIGREF46 . It is observed that in our proposed scoring model, with the proportion of relabeled data increasing, the iteration number of reaching the optimal word segmentation result is decreasing, but the INLINEFORM0 -score of CRF-based word segmenter is also gradually decreasing.",
        "type": "Document"
      },
      {
        "id": "6ad65045-962e-44bb-ae24-b2d2ee55f4e2",
        "metadata": {
          "vector_store_key": "1908.06606-3",
          "chunk_id": 31,
          "document_id": "1908.06606",
          "start_idx": 18280,
          "end_idx": 19072
        },
        "page_content": "QANet outperformed BERT-Base with 3.56% score in F$_1$-score but underperformed it with 0.75% score in EM-score. Compared with BERT-Base, our model led to a 5.64% performance improvement in EM-score and 3.69% in F$_1$-score. Although our model didn't outperform much with QANet in F$_1$-score (only 0.13%), our model significantly outperformed it with 6.39% score in EM-score. To further investigate the effects of named entity information and two-stage training mechanism for our model, we apply ablation analysis to see the improvement brought by each of them, where $\\times $ refers to removing that part from our model. As demonstrated in Table TABREF25, with named entity information enabled, two-stage training mechanism improved the result by 4.36% in EM-score and 3.8% in F$_1$-score.",
        "type": "Document"
      },
      {
        "id": "ef7c47d8-b405-4e59-8a6b-ca5597f2f400",
        "metadata": {
          "vector_store_key": "1811.00942-2",
          "chunk_id": 4,
          "document_id": "1811.00942",
          "start_idx": 2533,
          "end_idx": 3286
        },
        "page_content": "We find that a 2.5 $\\times $ reduction in perplexity on PTB comes at a staggering cost in terms of performance: inference with NLMs takes 49 $\\times $ longer and requires 32 $\\times $ more energy. Furthermore, we find that impressive reductions in perplexity translate into at best modest improvements in next-word prediction, which is arguable a better metric for evaluating software keyboards on a smartphone. The contribution of this paper is the first known elucidation of this quality\u2013performance tradeoff. Note that we refrain from prescriptive recommendations: whether or not a tradeoff is worthwhile depends on the application. Nevertheless, NLP engineers should arguably keep these tradeoffs in mind when selecting a particular operating point.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What approaches without reinforcement learning have been tried?",
    "llm_answer": "Regression systems (NNR).\n",
    "context": [
      {
        "id": "32830c20-19a8-465e-bb01-66e2eed1609d",
        "metadata": {
          "vector_store_key": "1909.00542-4",
          "chunk_id": 16,
          "document_id": "1909.00542",
          "start_idx": 8939,
          "end_idx": 9713
        },
        "page_content": "We also experiment with the use of reinforcement learning techniques. Again these experiments are based on BIBREF2, who uses REINFORCE to train a global policy. The policy predictor uses a simple feedforward network with a hidden layer. The results reported by BIBREF2 used ROUGE Recall and indicated no improvement with respect to deep learning architectures. Human evaluation results are preferable over ROUGE but these were made available after the publication of the paper. When comparing the ROUGE and human evaluation results (Table TABREF29), we observe an inversion of the results. In particular, the reinforcement learning approaches (RL) of BIBREF2 receive good human evaluation results, and as a matter of fact they are the best of our runs in two of the batches.",
        "type": "Document"
      },
      {
        "id": "50e8353f-f0bc-42b9-be6d-65e064b5fb62",
        "metadata": {
          "vector_store_key": "1902.00330-9",
          "chunk_id": 62,
          "document_id": "1902.00330",
          "start_idx": 34365,
          "end_idx": 35054
        },
        "page_content": "It is well known for its great success in the game field, such as Go BIBREF37 and Atari games BIBREF38 . Recently, reinforcement learning has also been successfully applied to many natural language processing tasks and achieved good performance BIBREF12 , BIBREF39 , BIBREF5 . Feng et al. BIBREF5 used reinforcement learning for relation classification task by filtering out the noisy data from the sentence bag and they achieved huge improvements compared with traditional classifiers. Zhang et al. BIBREF40 applied the reinforcement learning on sentence representation by automatically discovering task-relevant structures. To automatic taxonomy induction from a set of terms, Han et al.",
        "type": "Document"
      },
      {
        "id": "ace7bf14-dcd7-4997-8709-763353110930",
        "metadata": {
          "vector_store_key": "1902.00330-9",
          "chunk_id": 61,
          "document_id": "1902.00330",
          "start_idx": 33845,
          "end_idx": 34482
        },
        "page_content": "Nguyen et al. BIBREF2 use the sequence model, but they simply encode the results of the greedy choice, and measure the similarities between the global encoding and the candidate entity representations. Their model does not consider the long-term impact of current decisions on subsequent choices, nor does they add the selected target entity information to the current state to help disambiguation. In the last few years, reinforcement learning has emerged as a powerful tool for solving complex sequential decision-making problems. It is well known for its great success in the game field, such as Go BIBREF37 and Atari games BIBREF38 .",
        "type": "Document"
      },
      {
        "id": "0c7bca26-987c-4fe6-9c67-69d2e7fc2615",
        "metadata": {
          "vector_store_key": "1909.00542-4",
          "chunk_id": 17,
          "document_id": "1909.00542",
          "start_idx": 9713,
          "end_idx": 10350
        },
        "page_content": "In particular, the reinforcement learning approaches (RL) of BIBREF2 receive good human evaluation results, and as a matter of fact they are the best of our runs in two of the batches. In contrast, the regression systems (NNR) fare relatively poorly. Section SECREF6 expands on the comparison between the ROUGE and human evaluation scores. Encouraged by the results of Table TABREF29, we decided to continue with our experiments with reinforcement learning. We use the same features as in BIBREF2, namely the length (in number of sentences) of the summary generated so far, plus the $tf.idf$ vectors of the following: Candidate sentence;",
        "type": "Document"
      },
      {
        "id": "24e8685b-5155-4811-8b7e-daf4752f15cf",
        "metadata": {
          "vector_store_key": "1908.08419-1",
          "chunk_id": 8,
          "document_id": "1908.08419",
          "start_idx": 4695,
          "end_idx": 5419
        },
        "page_content": "Recently, neural networks have been applied in CWS tasks. To name a few, Zheng et al. BIBREF14 used deep layers of neural networks to learn feature representations of characters. Chen et al. BIBREF6 adopted LSTM to capture the previous important information. Chen et al. BIBREF18 proposed a gated recursive neural network (GRNN), which contains reset and update gates to incorporate the complicated combinations of characters. Jiang and Tang BIBREF19 proposed a sequence-to-sequence transformer model to avoid overfitting and capture character information at the distant site of a sentence. Yang et al. BIBREF20 investigated subword information for CWS and integrated subword embeddings into a Lattice LSTM (LaLSTM) network.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which languages do they validate on?",
    "llm_answer": "English, French, and German.\n",
    "context": [
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      },
      {
        "id": "5ab82d1c-620b-40bb-afe5-ef89b356baff",
        "metadata": {
          "vector_store_key": "1910.04269-1",
          "chunk_id": 31,
          "document_id": "1910.04269",
          "start_idx": 17739,
          "end_idx": 18415
        },
        "page_content": "This results in significant variation of speech quality between samples making it more representative of real world scenarios. Our dataset consists of 1,500 samples for each of six languages. Out of 1,500 samples for each language, 1,200 were randomly selected as training dataset for that language and rest 300 as validation dataset using k-fold cross-validation. To sum up, we trained our model on 7,200 samples and validated it on 1800 samples comprising six languages. The results are discussed in next section. This paper discusses two end-to-end approaches which achieve state-of-the-art results in both the image as well as audio domain on the VoxForge dataset BIBREF6.",
        "type": "Document"
      },
      {
        "id": "9210eab3-d084-4395-859a-51482ae8da57",
        "metadata": {
          "vector_store_key": "1910.04269-1",
          "chunk_id": 34,
          "document_id": "1910.04269",
          "start_idx": 19227,
          "end_idx": 19993
        },
        "page_content": "2D attention models focused on the important features extracted by convolutional layers and bi-directional GRU captured the temporal features. Several of the spoken languages in Europe belong to the Indo-European family. Within this family, the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Katze in German and Ciao in Italian becomes Chao in Spanish.",
        "type": "Document"
      },
      {
        "id": "cb4078b8-00d7-4e01-a91b-7292c78d6351",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 12,
          "document_id": "1910.04269",
          "start_idx": 7197,
          "end_idx": 7983
        },
        "page_content": "The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel). Several state-of-the-art results on various audio classification tasks have been obtained by using log-Mel spectrograms of raw audio, as features BIBREF19. Convolutional Neural Networks have demonstrated an excellent performance gain in classification of these features BIBREF20, BIBREF21 against other machine learning techniques. It has been shown that using attention layers with ConvNets further enhanced their performance BIBREF22. This motivated us to develop a CNN-based architecture with attention since this approach hasn\u2019t been applied to the task of language identification before.",
        "type": "Document"
      },
      {
        "id": "27cca045-c8e0-4a41-9a7d-cd607ece91d6",
        "metadata": {
          "vector_store_key": "2003.07433-0",
          "chunk_id": 40,
          "document_id": "2003.07433",
          "start_idx": 21417,
          "end_idx": 22296
        },
        "page_content": "The response matrix yields reliability of .89 based on the uncorrected method, and .96 based on the binary method, which confirm the high reliability of our PTSD Dictionary created PTSD survey based categories. After assessing the reliability of the PTSD Linguistic dictionary, we focus on the two most common forms of construct validity: convergent validity and discriminant validity BIBREF25. Convergent validity provides evidence that two measures designed to assess the same construct are indeed related; discriminate validity involves evidence that two measures designed to assess different constructs are not too strongly related. In theory, we expect that the PTSD Linguistic dictionary should be positively correlated with other negative PTSD constructs to show convergent validity, and not strongly correlated with positive PTSD constructs to show discriminant validity.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the baseline method for the task?",
    "llm_answer": "TF-IDF (Lexical, Non-Neural) and LDA (Topic, Non-Neural) are unsupervised baseline models.\n",
    "context": [
      {
        "id": "e4fc706d-44a6-449c-b40f-897a083745cc",
        "metadata": {
          "vector_store_key": "1910.07181-5",
          "chunk_id": 29,
          "document_id": "1910.07181",
          "start_idx": 15945,
          "end_idx": 16597
        },
        "page_content": "Given these ingredients, our procedure consists of three steps: (i) splitting the dataset into a train set and a set of test candidates, (ii) training the baseline model on the train set and (iii) modifying a subset of the test candidates to generate the final test set. We partition $\\mathcal {D}$ into a train set $\\mathcal {D}_\\text{train}$ and a set of test candidates, $\\mathcal {D}_\\text{cand}$, with the latter containing all instances $(\\mathbf {x},y) \\in \\mathcal {D}$ such that for at least one word $w$ in $\\mathbf {x}$, $S(w) \\ne \\emptyset $. Additionally, we require that the training set consists of at least one third of the entire data.",
        "type": "Document"
      },
      {
        "id": "ed2b65a3-293a-4a8c-874e-949c480c257f",
        "metadata": {
          "vector_store_key": "2002.11893-4",
          "chunk_id": 13,
          "document_id": "2002.11893",
          "start_idx": 8009,
          "end_idx": 8625
        },
        "page_content": "Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context. To make workers understand the task more easily, we crafted templates to generate natural language descriptions for each structured goal. Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality.",
        "type": "Document"
      },
      {
        "id": "96576214-2c10-4049-ba11-db649d59677b",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 26,
          "document_id": "1809.04960",
          "start_idx": 15085,
          "end_idx": 15736
        },
        "page_content": "Unsupervised baseline models are as follows: TF-IDF (Lexical, Non-Neural) is an important unsupervised baseline. We use the concatenation of the title and the body as the query to retrieve the candidate comment set by means of the similarity of the tf-idf value. The model is trained on unpaired articles and comments, which is the same as our proposed model. LDA (Topic, Non-Neural) is a popular unsupervised topic model, which discovers the abstract \"topics\" that occur in a collection of documents. We train the LDA with the articles and comments in the training set. The model retrieves the comments by the similarity of the topic representations.",
        "type": "Document"
      },
      {
        "id": "e5025c6d-7833-4f82-9275-5f40aac5f5af",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 25,
          "document_id": "1809.04960",
          "start_idx": 14549,
          "end_idx": 15237
        },
        "page_content": "The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-supervised setting. We prune the vocabulary, and only leave 30,000 most frequent words in the vocabulary. We train the model for 20 epochs with the Adam optimizing algorithms BIBREF8 . In order to alleviate the KL vanishing problem, we set the initial learning to INLINEFORM2 , and use batch normalization BIBREF9 in each layer. We also gradually increase the KL term from 0 to 1 after each epoch. We compare our model with several unsupervised models and supervised models. Unsupervised baseline models are as follows: TF-IDF (Lexical, Non-Neural) is an important unsupervised baseline.",
        "type": "Document"
      },
      {
        "id": "b7e7e7c9-ac8d-4682-84ce-d358ef5a3286",
        "metadata": {
          "vector_store_key": "1909.00542-0",
          "chunk_id": 4,
          "document_id": "1909.00542",
          "start_idx": 2338,
          "end_idx": 3061
        },
        "page_content": "Phase A provides a biomedical question as an input, and participants are expected to find relevant concepts from designated terminologies and ontologies, relevant articles from PubMed, relevant snippets from the relevant articles, and relevant RDF triples from designated ontologies. Phase B provides a biomedical question and a list of relevant articles and snippets, and participant systems are expected to return the exact answers and the ideal answers. The training data is composed of the test data from all previous years, and amounts to 2,747 samples. There has been considerable research on the use of machine learning approaches for tasks related to text summarisation, especially on single-document summarisation.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what amounts of size were used on german-english?",
    "llm_answer": "159000 parallel sentences for training, and 7584 for development.\n",
    "context": [
      {
        "id": "d97fd4a7-ca11-429a-9a73-016e227c9661",
        "metadata": {
          "vector_store_key": "1905.11901-1",
          "chunk_id": 10,
          "document_id": "1905.11901",
          "start_idx": 5659,
          "end_idx": 6337
        },
        "page_content": "We use the TED data from the IWSLT 2014 German INLINEFORM0 English shared translation task BIBREF38 . We use the same data cleanup and train/dev split as BIBREF39 , resulting in 159000 parallel sentences of training data, and 7584 for development. As a second language pair, we evaluate our systems on a Korean\u2013English dataset with around 90000 parallel sentences of training data, 1000 for development, and 2000 for testing. For both PBSMT and NMT, we apply the same tokenization and truecasing using Moses scripts. For NMT, we also learn BPE subword segmentation with 30000 merge operations, shared between German and English, and independently for Korean INLINEFORM0 English.",
        "type": "Document"
      },
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      },
      {
        "id": "6474a14f-5450-4416-ad5f-50ad77a988ac",
        "metadata": {
          "vector_store_key": "1905.11901-2",
          "chunk_id": 6,
          "document_id": "1905.11901",
          "start_idx": 3531,
          "end_idx": 4236
        },
        "page_content": "For high-resource settings, the effect of vocabulary size on translation quality is relatively small; BIBREF32 report mixed results when comparing vocabularies of 30k and 90k subwords. In low-resource settings, large vocabularies result in low-frequency (sub)words being represented as atomic units at training time, and the ability to learn good high-dimensional representations of these is doubtful. BIBREF33 propose a minimum frequency threshold for subword units, and splitting any less frequent subword into smaller units or characters. We expect that such a threshold reduces the need to carefully tune the vocabulary size to the dataset, leading to more aggressive segmentation on smaller datasets.",
        "type": "Document"
      },
      {
        "id": "b9e44932-ba0d-44e0-90c0-6300912be837",
        "metadata": {
          "vector_store_key": "1905.11901-2",
          "chunk_id": 5,
          "document_id": "1905.11901",
          "start_idx": 3008,
          "end_idx": 3699
        },
        "page_content": "We consider the hyperparameters used by BIBREF3 to be our baseline. This baseline does not make use of various advances in NMT architectures and training tricks. In contrast to the baseline, we use a BiDeep RNN architecture BIBREF25 , label smoothing BIBREF26 , dropout BIBREF27 , word dropout BIBREF28 , layer normalization BIBREF29 and tied embeddings BIBREF30 . Subword representations such as BPE BIBREF31 have become a popular choice to achieve open-vocabulary translation. BPE has one hyperparameter, the number of merge operations, which determines the size of the final vocabulary. For high-resource settings, the effect of vocabulary size on translation quality is relatively small;",
        "type": "Document"
      },
      {
        "id": "b6ada273-6729-4ae2-b59d-8ca1ac47f969",
        "metadata": {
          "vector_store_key": "1905.11901-1",
          "chunk_id": 21,
          "document_id": "1905.11901",
          "start_idx": 12012,
          "end_idx": 12804
        },
        "page_content": "Hyperparameters were kept constant across different data settings, except for the validation interval and subword vocabulary size (see Table 1). Table TABREF24 shows some sample translations that represent typical errors of our PBSMT and NMT systems, trained with ultra-low (100k words) and low (3.2M words) amounts of data. For unknown words such as blutbefleckten (`bloodstained') or Spaniern (`Spaniards', `Spanish'), PBSMT systems default to copying, while NMT systems produce translations on a subword-level, with varying success (blue-flect, bleed; spaniers, Spanians). NMT systems learn some syntactic disambiguation even with very little data, for example the translation of das and die as relative pronouns ('that', 'which', 'who'), while PBSMT produces less grammatical translation.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is the dataset?",
    "llm_answer": "The sizes of the datasets vary.  There are 287,000 training pairs and 11,500 test pairs for one dataset. Another dataset has 191,502 training, 5,000 validation, and 1,610 test pieces of news.  A third dataset contains 198,112 news articles.  A fourth dataset has 2489 training messages and 700 test tweets/messages.\n",
    "context": [
      {
        "id": "da74a9a9-0751-4d68-9ca9-313bf450f848",
        "metadata": {
          "vector_store_key": "1907.05664-3",
          "chunk_id": 7,
          "document_id": "1907.05664",
          "start_idx": 3615,
          "end_idx": 4242
        },
        "page_content": "See2017 to then apply LRP on it. The CNN/Daily mail dataset BIBREF12 is a text summarization dataset adapted from the Deepmind question-answering dataset BIBREF13 . It contains around three hundred thousand news articles coupled with summaries of about three sentences. These summaries are in fact \u201chighlights\" of the articles provided by the media themselves. Articles have an average length of 780 words and the summaries of 50 words. We had 287 000 training pairs and 11 500 test pairs. Similarly to See et al. See2017, we limit during training and prediction the input text to 400 words and generate summaries of 200 words.",
        "type": "Document"
      },
      {
        "id": "088347bc-6391-466f-81eb-e37d14e108ba",
        "metadata": {
          "vector_store_key": "1705.09665-6",
          "chunk_id": 23,
          "document_id": "1705.09665",
          "start_idx": 13165,
          "end_idx": 13951
        },
        "page_content": "Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities ( INLINEFORM0 ), for a total of 4,872 community-months ( INLINEFORM1 ). Estimating linguistic measures. We estimate word frequencies INLINEFORM0 , and by extension each downstream measure, in a carefully controlled manner in order to ensure we capture robust and meaningful linguistic behaviour.",
        "type": "Document"
      },
      {
        "id": "5fa1d98c-68a5-468c-a996-829da5ff9fb7",
        "metadata": {
          "vector_store_key": "1809.04960-1",
          "chunk_id": 24,
          "document_id": "1809.04960",
          "start_idx": 13943,
          "end_idx": 14549
        },
        "page_content": "Following the previous work BIBREF0 , we tokenize all text with the popular python package Jieba, and filter out short articles with less than 30 words in content and those with less than 20 comments. The dataset is split into training/validation/test sets, and they contain 191,502/5,000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100.",
        "type": "Document"
      },
      {
        "id": "6acc5ad9-079c-46ec-9298-4912e7c2aae7",
        "metadata": {
          "vector_store_key": "1809.04960-1",
          "chunk_id": 23,
          "document_id": "1809.04960",
          "start_idx": 13537,
          "end_idx": 14236
        },
        "page_content": "Hence, the model is trained on both unpaired data INLINEFORM3 , and paired data INLINEFORM4 . We select a large-scale Chinese dataset BIBREF0 with millions of real comments and a human-annotated test set to evaluate our model. The dataset is collected from Tencent News, which is one of the most popular Chinese websites for news and opinion articles. The dataset consists of 198,112 news articles. Each piece of news contains a title, the content of the article, and a list of the users' comments. Following the previous work BIBREF0 , we tokenize all text with the popular python package Jieba, and filter out short articles with less than 30 words in content and those with less than 20 comments.",
        "type": "Document"
      },
      {
        "id": "8c714d0f-b982-4837-aefc-3b59003e3a67",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 15,
          "document_id": "1912.13109",
          "start_idx": 7410,
          "end_idx": 8070
        },
        "page_content": "Train-test split: The labelled dataset that was available for this task was very limited in number of examples and thus as noted above few data augmentation techniques were applied to boost the learning of the network. Before applying augmentation, a train-test split of 78%-22% was done from the original, cleansed data set. Thus, 700 tweets/messages were held out for testing. All model evaluation were done in on the test set that got generated by this process. The results presented in this report are based on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What MC abbreviate for?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "a8785215-1259-4091-b475-c14cb4c9afd2",
        "metadata": {
          "vector_store_key": "1810.06743-2",
          "chunk_id": 13,
          "document_id": "1810.06743",
          "start_idx": 7687,
          "end_idx": 8430
        },
        "page_content": "As it is a schema for marking morphology, its part of speech attribute does not have POS values for punctuation, symbols, or miscellany (Punct, Sym, and X in Universal Dependencies). Like the UD schema, the decomposition of a word into its lemma and MSD is directly comparable across languages. Its features are informed by a distinction between universal categories, which are widespread and psychologically real to speakers; and comparative concepts, only used by linguistic typologists to compare languages BIBREF11 . Additionally, it strives for identity of meaning across languages, not simply similarity of terminology. As a prime example, it does not regularly label a dative case for nouns, for reasons explained in depth by BIBREF11 .",
        "type": "Document"
      },
      {
        "id": "8ecc4aec-4ffa-4675-bf25-62eecb4ca245",
        "metadata": {
          "vector_store_key": "1705.09665-2",
          "chunk_id": 15,
          "document_id": "1705.09665",
          "start_idx": 8536,
          "end_idx": 9271
        },
        "page_content": "Our characterizations of words in a community are motivated by methodology from prior literature that compares the frequency of a word in a particular setting to its frequency in some background distribution, in order to identify instances of linguistic variation BIBREF21 , BIBREF19 . Our particular framework makes this comparison by way of pointwise mutual information (PMI). In the following, we use INLINEFORM0 to denote one community within a set INLINEFORM1 of communities, and INLINEFORM2 to denote one time period within the entire history INLINEFORM3 of INLINEFORM4 . We account for temporal as well as inter-community variation by computing word-level measures for each time period of each community's history, INLINEFORM5 .",
        "type": "Document"
      },
      {
        "id": "ad2647bb-40f1-41fe-83bf-57c37e536dcc",
        "metadata": {
          "vector_store_key": "1910.07181-2",
          "chunk_id": 30,
          "document_id": "1910.07181",
          "start_idx": 15946,
          "end_idx": 16647
        },
        "page_content": "We partition $\\mathcal {D}$ into a train set $\\mathcal {D}_\\text{train}$ and a set of test candidates, $\\mathcal {D}_\\text{cand}$, with the latter containing all instances $(\\mathbf {x},y) \\in \\mathcal {D}$ such that for at least one word $w$ in $\\mathbf {x}$, $S(w) \\ne \\emptyset $. Additionally, we require that the training set consists of at least one third of the entire data. We finetune $M$ on $\\mathcal {D}_\\text{train}$. Let $(\\mathbf {x}, y) \\in \\mathcal {D}_\\text{train}$ where $\\mathbf {x} = w_1, \\ldots , w_n$ is a sequence of words. We deviate from the standard finetuning procedure of BIBREF13 in three respects: We randomly replace 5% of all words in $\\mathbf {x}$ with a [MASK] token.",
        "type": "Document"
      },
      {
        "id": "4fb77cea-3dfa-405d-a4f1-c3f53355ec9f",
        "metadata": {
          "vector_store_key": "1810.06743-2",
          "chunk_id": 8,
          "document_id": "1810.06743",
          "start_idx": 4519,
          "end_idx": 5294
        },
        "page_content": "A slightly finer-grained schema for English is the Penn Treebank tagset BIBREF6 , which includes signals for English morphology. For instance, its VBZ tag pertains to the specially inflected 3rd-person singular, present-tense verb form (e.g. proves in tab:ptb). If the tag in a schema is detailed enough that it exactly specifies a slot in a paradigm, it is called a morphosyntactic description (MSD). These descriptions require varying amounts of detail: While the English verbal paradigm is small enough to fit on a page, the verbal paradigm of the Northeast Caucasian language Archi can have over 1500000 slots BIBREF7 . Unlike the Penn Treebank tags, the UD and UniMorph schemata are cross-lingual and include a fuller lexicon of attribute-value pairs, such as Person: 1.",
        "type": "Document"
      },
      {
        "id": "afc0b087-dd62-4710-bace-32dba5ed8518",
        "metadata": {
          "vector_store_key": "1705.09665-2",
          "chunk_id": 16,
          "document_id": "1705.09665",
          "start_idx": 9271,
          "end_idx": 9969
        },
        "page_content": "We account for temporal as well as inter-community variation by computing word-level measures for each time period of each community's history, INLINEFORM5 . Given a word INLINEFORM6 used within a particular community INLINEFORM7 at time INLINEFORM8 , we define two word-level measures: Specificity. We quantify the specificity INLINEFORM0 of INLINEFORM1 to INLINEFORM2 by calculating the PMI of INLINEFORM3 and INLINEFORM4 , relative to INLINEFORM5 , INLINEFORM6  where INLINEFORM0 is INLINEFORM1 's frequency in INLINEFORM2 . INLINEFORM3 is specific to INLINEFORM4 if it occurs more frequently in INLINEFORM5 than in the entire set INLINEFORM6 , hence distinguishing this community from the rest.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are their correlation results?",
    "llm_answer": "The text describes correlation calculations (Pearson, Kendall, Spearman, revised Kendall) between ROUGE metrics (recall, precision, F1 for ROUGE-2 and ROUGE-SU4) and human scores, and between predicted and manual scores for various models and domains.  Specific correlation values are presented in tables (TABREF23 and TABREF38), but not explicitly stated here.\n",
    "context": [
      {
        "id": "3285ea37-bbc2-4a73-90f4-925d2e8fe182",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 23,
          "document_id": "1909.00542",
          "start_idx": 12601,
          "end_idx": 13282
        },
        "page_content": "The human evaluation results were scraped from the BioASQ Results page, and the ROUGE results were kindly provided by the organisers. We compute the correlation of each of the ROUGE metrics (recall, precision, F1 for ROUGE-2 and ROUGE-SU4) against the average of the human scores. The correlation metrics are Pearson, Kendall, and a revised Kendall correlation explained below. The Pearson correlation between two variables is computed as the covariance of the two variables divided by the product of their standard deviations. This correlation is a good indication of a linear relation between the two variables, but may not be very effective when there is non-linear correlation.",
        "type": "Document"
      },
      {
        "id": "a5f6727a-77fe-4e95-85b5-8cc7d90ebe3d",
        "metadata": {
          "vector_store_key": "1809.08731-0",
          "chunk_id": 29,
          "document_id": "1809.08731",
          "start_idx": 16598,
          "end_idx": 17461
        },
        "page_content": "Looking next at the correlations for all models but different domains (cf. Table 5 ), we first observe that the results across domains are similar, i.e., we do not observe the same effect as in Subsection \"Analysis I: Fluency Evaluation per Compression System\" . This is due to the distributions of scores being uniform ( $\\text{Var}(Y) \\in [0.28, 0.36]$ ). Next, we focus on an important question: How much does the performance of our SLOR-based metrics depend on the domain, given that the respective LMs are trained on Gigaword, which consists of news data? Comparing the evaluation performance for individual metrics, we observe that, except for letters, WordSLOR and WPSLOR perform best across all domains: they outperform the best word-overlap metric by at least $0.019$ and at most $0.051$ Pearson correlation, and at least $0.004$ and at most $0.014$ MSE.",
        "type": "Document"
      },
      {
        "id": "3bb26b45-786b-4ea0-becb-09df223b165b",
        "metadata": {
          "vector_store_key": "1909.00578-1",
          "chunk_id": 16,
          "document_id": "1909.00578",
          "start_idx": 8451,
          "end_idx": 9107
        },
        "page_content": "We measure the correlation between the two (predicted vs. manual) across all contestants using Spearman's $\\rho $, Kendall's $\\tau $ and Pearson's $r$. We train and test the Sum-QE and BiGRU-ATT versions using a 3-fold procedure. In each fold, we train on two datasets (e.g., DUC-05, DUC-06) and test on the third (e.g., DUC-07). We follow the same procedure with the three BiGRU-based models. Hyper-perameters are tuned on a held out subset from the training set of each fold. Table TABREF23 shows Spearman's $\\rho $, Kendall's $\\tau $ and Pearson's $r$ for all datasets and models. The three fine-tuned BERT versions clearly outperform all other methods.",
        "type": "Document"
      },
      {
        "id": "752a7145-6b46-4165-80e7-c2135b64ce21",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 24,
          "document_id": "1909.00542",
          "start_idx": 12996,
          "end_idx": 13728
        },
        "page_content": "This correlation is a good indication of a linear relation between the two variables, but may not be very effective when there is non-linear correlation. The Spearman rank correlation and the Kendall rank correlation are two of the most popular among metrics that aim to detect non-linear correlations. The Spearman rank correlation between two variables can be computed as the Pearson correlation between the rank values of the two variables, whereas the Kendall rank correlation measures the ordinal association between the two variables using Equation DISPLAY_FORM36. It is useful to account for the fact that the results are from 28 independent sets (3 batches in BioASQ 1 and 5 batches each year between BioASQ 2 and BioASQ 6).",
        "type": "Document"
      },
      {
        "id": "0808cc8f-01f9-43cf-a8c1-8bee8661b87d",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 25,
          "document_id": "1909.00542",
          "start_idx": 13728,
          "end_idx": 14396
        },
        "page_content": "It is useful to account for the fact that the results are from 28 independent sets (3 batches in BioASQ 1 and 5 batches each year between BioASQ 2 and BioASQ 6). We therefore also compute a revised Kendall rank correlation measure that only considers pairs of variable values within the same set. The revised metric is computed using Equation DISPLAY_FORM37, where $S$ is the list of different sets. Table TABREF38 shows the results of all correlation metrics. Overall, ROUGE-2 and ROUGE-SU4 give similar correlation values but ROUGE-SU4 is marginally better. Among precision, recall and F1, both precision and F1 are similar, but precision gives a better correlation.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What simpler models do they look at?",
    "llm_answer": "Topic models with between 3 and 50 topics.\n",
    "context": [
      {
        "id": "0ab61107-67bf-493c-aa5f-199d370caaa3",
        "metadata": {
          "vector_store_key": "1703.04617-2",
          "chunk_id": 26,
          "document_id": "1703.04617",
          "start_idx": 14460,
          "end_idx": 15206
        },
        "page_content": "The previous models are often trained for all questions without explicitly discriminating different question types; however, for a target question, both the common features shared by all questions and the specific features for a specific type of question are further considered in this paper, as they could potentially obey different distributions. In this paper we further explicitly model different types of questions in the end-to-end training. We start from a simple way to first analyze the word frequency of all questions, and obtain top-10 most frequent question types: what, how, who, when, which, where, why, be, whose, and whom, in which be stands for the questions beginning with different forms of the word be such as is, am, and are.",
        "type": "Document"
      },
      {
        "id": "a48c2a61-14a8-4538-8b46-146fb87ab447",
        "metadata": {
          "vector_store_key": "1703.04617-2",
          "chunk_id": 0,
          "document_id": "1703.04617",
          "start_idx": 0,
          "end_idx": 722
        },
        "page_content": "Enabling computers to understand given documents and answer questions about their content has recently attracted intensive interest, including but not limited to the efforts as in BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 . Many specific problems such as machine comprehension and question answering often involve modeling such question-document pairs. The recent availability of relatively large training datasets (see Section \"Related Work\" for more details) has made it more feasible to train and estimate rather complex models in an end-to-end fashion for these problems, in which a whole model is fit directly with given question-answer tuples and the resulting model has shown to be rather effective.",
        "type": "Document"
      },
      {
        "id": "9dc69e28-1cf2-4d3c-9f8f-18a6f0169737",
        "metadata": {
          "vector_store_key": "1708.05873-2",
          "chunk_id": 9,
          "document_id": "1708.05873",
          "start_idx": 5213,
          "end_idx": 5988
        },
        "page_content": "Following BIBREF8 we generate a set of candidate models ranging between 3 and 50 topics. We then plot the exclusivity and semantic coherence (numbers closer to 0 indicate higher coherence), with a linear regression overlaid (Figure FIGREF3 ). Models above the regression line have a \u201cbetter\u201d exclusivity-semantic coherence trade off. We select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence. The topic quality is usually evaluated by highest probability words, which is presented in Figure FIGREF4 . Figure FIGREF4 provides a list of the main topics (and the highest probability words associated these topics) that emerge from the STM of UN General Debate statements.",
        "type": "Document"
      },
      {
        "id": "e1c48e75-79b0-48a8-b784-07584424d82f",
        "metadata": {
          "vector_store_key": "1809.04960-1",
          "chunk_id": 43,
          "document_id": "1809.04960",
          "start_idx": 24909,
          "end_idx": 25663
        },
        "page_content": "Park et al. ParkSDE16 propose a system called CommentIQ, which assist the comment moderators in identifying high quality comments. Napoles et al. NapolesTPRP17 propose to discriminating engaging, respectful, and informative conversations. They present a Yahoo news comment threads dataset and annotation scheme for the new task of identifying \u201cgood\u201d online conversations. More recently, Kolhaatkar and Taboada KolhatkarT17 propose a model to classify the comments into constructive comments and non-constructive comments. In this work, we are also inspired by the recent related work of natural language generation models BIBREF18 , BIBREF19 . Topic models BIBREF20 are among the most widely used models for learning unsupervised representations of text.",
        "type": "Document"
      },
      {
        "id": "8554c6d9-844e-48d1-8ece-a014b8922168",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 47,
          "document_id": "1809.04960",
          "start_idx": 27365,
          "end_idx": 27722
        },
        "page_content": "We introduce a variation topic model to represent the topics, and match the articles and comments by the similarity of their topics. Experiments show that our topic-based approach significantly outperforms previous lexicon-based models. The model can also profit from paired corpora and achieves state-of-the-art performance under semi-supervised scenarios.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What linguistic quality aspects are addressed?",
    "llm_answer": "Fluency.\n",
    "context": [
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      },
      {
        "id": "422262b3-1600-426e-8d77-6e2d06d31b33",
        "metadata": {
          "vector_store_key": "1809.08731-3",
          "chunk_id": 0,
          "document_id": "1809.08731",
          "start_idx": 0,
          "end_idx": 755
        },
        "page_content": "Producing sentences which are perceived as natural by a human addressee\u2014a property which we will denote as fluency throughout this paper \u2014is a crucial goal of all natural language generation (NLG) systems: it makes interactions more natural, avoids misunderstandings and, overall, leads to higher user satisfaction and user trust BIBREF0 . Thus, fluency evaluation is important, e.g., during system development, or for filtering unacceptable generations at application time. However, fluency evaluation of NLG systems constitutes a hard challenge: systems are often not limited to reusing words from the input, but can generate in an abstractive way. Hence, it is not guaranteed that a correct output will match any of a finite number of given references.",
        "type": "Document"
      },
      {
        "id": "fcfa31e7-486b-44a5-8563-92a55176ca02",
        "metadata": {
          "vector_store_key": "1904.09678-0",
          "chunk_id": 6,
          "document_id": "1904.09678",
          "start_idx": 2877,
          "end_idx": 3645
        },
        "page_content": "(ii) Baseline-Lexicon: we use words in the gold standard lexicon for the sentiment learning in the target domain; for this purpose we use words INLINEFORM0 . (iii) Evaluation-Lexicon: we randomly exclude a set of words the baseline-lexicon INLINEFORM0 . In selection of the sampling size we make sure that INLINEFORM1 and INLINEFORM2 would contain a comparable number of words. In Table TABREF13 we compare the quality of UniSent with the Baseline-Lexicon as well as with the gold standard lexicon for general domain data. The results show that (i) UniSent clearly outperforms the baseline for all languages (ii) the quality of UniSent is close to manually annotated data (iii) the domain adaptation method brings small improvements for morphologically poor languages.",
        "type": "Document"
      },
      {
        "id": "4bc79961-bbfe-4650-9d5f-01691d719007",
        "metadata": {
          "vector_store_key": "1809.08731-3",
          "chunk_id": 37,
          "document_id": "1809.08731",
          "start_idx": 20896,
          "end_idx": 21712
        },
        "page_content": "In contrast to ours, their approach is supervised. stent2005evaluating and cahill2009correlating found only low correlation between automatic metrics and fluency ratings for system-generated English paraphrases and the output of a German surface realiser, respectively. Explicit fluency evaluation of NLG, including compression and the related task of summarization, has mostly been performed manually. vadlapudi-katragadda:2010:SRW used LMs for the evaluation of summarization fluency, but their models were based on part-of-speech tags, which we do not require, and they were non-neural. Further, they evaluated longer texts, not single sentences like we do. toutanova2016dataset compared 80 word-overlap metrics for evaluating the content and fluency of compressions, finding only low correlation with the latter.",
        "type": "Document"
      },
      {
        "id": "b44fd67c-85e4-45e1-b49d-864aedb527e5",
        "metadata": {
          "vector_store_key": "1809.08731-1",
          "chunk_id": 8,
          "document_id": "1809.08731",
          "start_idx": 4746,
          "end_idx": 5427
        },
        "page_content": "Their experiments showed a strong correlation between human judgments and normalized sentence log-probabilities under a variety of LMs for artificial data they had created by translating and back-translating sentences with neural models. While they tried different types of LMs, best results were obtained for neural models, namely recurrent neural networks (RNNs). In this work, we investigate if approaches which have proven successful for modeling acceptability can be applied to the NLP problem of automatic fluency evaluation. In this section, we first describe SLOR and the intuition behind this score. Then, we introduce WordPieces, before explaining how we combine the two.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What dataset do they use?",
    "llm_answer": "AIDA-Train, Wikipedia, AIDA-B, AIDA-CoNLL, Stanford Question Answering Dataset (SQuAD), MCTest, CNN/Daily Mail, Children's Book Test (CBT), HEOT, FigureEight, ISEAR, Twitter Emotion Corpus (TEC).\n",
    "context": [
      {
        "id": "458b51fc-c0d4-482d-90b3-d4cefed66ac3",
        "metadata": {
          "vector_store_key": "1902.00330-3",
          "chunk_id": 37,
          "document_id": "1902.00330",
          "start_idx": 20853,
          "end_idx": 21565
        },
        "page_content": "We implement our models in Tensorflow and run experiments on 4 Tesla V100 GPU. We conduct experiments on several different types of public datasets including news and encyclopedia corpus. The training set is AIDA-Train and Wikipedia datasets, where AIDA-Train contains 18448 mentions and Wikipedia contains 25995 mentions. In order to compare with the previous methods, we evaluate our model on AIDA-B and other datasets. These datasets are well-known and have been used for the evaluation of most entity linking systems. The statistics of the datasets are shown in Table 1. AIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.",
        "type": "Document"
      },
      {
        "id": "8ef64fd9-ae4b-416a-994a-f082e80e88e4",
        "metadata": {
          "vector_store_key": "1703.04617-2",
          "chunk_id": 2,
          "document_id": "1703.04617",
          "start_idx": 973,
          "end_idx": 1680
        },
        "page_content": "We then viewed and modelled different types of questions and the information shared among them as an adaptation problem and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results on our competitive baselines. Recent advance on reading comprehension and question answering has been closely associated with the availability of various datasets. BIBREF0 released the MCTest data consisting of 500 short, fictional open-domain stories and 2000 questions. The CNN/Daily Mail dataset BIBREF1 contains news articles for close style machine comprehension, in which only entities are removed and tested for comprehension.",
        "type": "Document"
      },
      {
        "id": "2e11e8fc-5479-4b55-93a6-cd64a6debc51",
        "metadata": {
          "vector_store_key": "1703.04617-2",
          "chunk_id": 3,
          "document_id": "1703.04617",
          "start_idx": 1680,
          "end_idx": 2413
        },
        "page_content": "The CNN/Daily Mail dataset BIBREF1 contains news articles for close style machine comprehension, in which only entities are removed and tested for comprehension. Children's Book Test (CBT) BIBREF2 leverages named entities, common nouns, verbs, and prepositions to test reading comprehension. The Stanford Question Answering Dataset (SQuAD) BIBREF3 is more recently released dataset, which consists of more than 100,000 questions for documents taken from Wikipedia across a wide range of topics. The question-answer pairs are annotated through crowdsourcing. Answers are spans of text marked in the original documents. In this paper, we use SQuAD to evaluate our models. Many neural network models have been studied on the SQuAD task.",
        "type": "Document"
      },
      {
        "id": "5c4b1458-497e-4f2e-a4e7-c5e739d9cad0",
        "metadata": {
          "vector_store_key": "1912.13109-1",
          "chunk_id": 9,
          "document_id": "1912.13109",
          "start_idx": 4561,
          "end_idx": 5222
        },
        "page_content": "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al. This dataset was important to employ Transfer Learning to our task since the number of labeled dataset was very small. Basic summary and examples of the data from the dataset are below: The obtained data set had many challenges and thus a data preparation task was employed to clean the data and make it ready for the deep learning pipeline.",
        "type": "Document"
      },
      {
        "id": "59dbbb96-7ec6-444b-b42f-cdfacda0f1a3",
        "metadata": {
          "vector_store_key": "1909.02764-2",
          "chunk_id": 32,
          "document_id": "1909.02764",
          "start_idx": 17674,
          "end_idx": 18385
        },
        "page_content": "We train models on a variety of corpora, namely the common format published by BIBREF27 of the FigureEight (formally known as Crowdflower) data set of social media, the ISEAR data BIBREF40 (self-reported emotional events), and, the Twitter Emotion Corpus (TEC, weakly annotated Tweets with #anger, #disgust, #fear, #happy, #sadness, and #surprise, Mohammad2012). From all corpora, we use instances with labels fear, anger, or joy. These corpora are English, however, we do predictions on German utterances. Therefore, each corpus is preprocessed to German with Google Translate. We remove URLs, user tags (\u201c@Username\u201d), punctuation and hash signs. The distributions of the data sets are shown in Table TABREF12.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is typical GAN architecture for each text-to-image synhesis group?",
    "llm_answer": "The context describes the *motivations* and *contributions* of different GAN architectures, but does not explicitly state a typical architecture for each group.  It mentions *semantic enhancement GANs* using a neural network to encode text, but doesn't detail a typical architecture for the other groups (resolution, diversity, or motion enhancement).\n",
    "context": [
      {
        "id": "d434415b-9f44-4ade-ae47-2e26e30877d8",
        "metadata": {
          "vector_store_key": "1910.09399-8",
          "chunk_id": 120,
          "document_id": "1910.09399",
          "start_idx": 64076,
          "end_idx": 65081
        },
        "page_content": "These generated images often rely on generative adversarial networks (GANs), deep convolutional decoder networks, and multimodal learning methods. blackIn the paper, we first proposed a taxonomy to organize GAN based text-to-image synthesis frameworks into four major groups: semantic enhancement GANs, resolution enhancement GANs, diversity enhancement GANs, and motion enhancement GANs. The taxonomy provides a clear roadmap to show the motivations, architectures, and difference of different methods, and also outlines their evolution timeline and relationships. Following the proposed taxonomy, we reviewed important features of each method and their architectures. We indicated the model definition and key contributions from some advanced GAN framworks, including StackGAN, StackGAN++, AttnGAN, DC-GAN, AC-GAN, TAC-GAN, HDGAN, Text-SeGAn, StoryGAN etc. Many of the solutions surveyed in this paper tackled the highly complex challenge of generating photo-realistic images beyond swatch size samples.",
        "type": "Document"
      },
      {
        "id": "cd642e5a-d587-46c3-9c96-a0cc017c1005",
        "metadata": {
          "vector_store_key": "1910.09399-8",
          "chunk_id": 46,
          "document_id": "1910.09399",
          "start_idx": 24166,
          "end_idx": 24926
        },
        "page_content": "The fourth user group adds a new dimension in image synthesis, and aims to generate sequences of images which are coherent in temporal order, i.e. capture the motion information. black Based on the above descriptions, we categorize GAN based Text-to-Image Synthesis into a taxonomy with four major categories, as shown in Fig. FIGREF24. Semantic Enhancement GANs: Semantic enhancement GANs represent pioneer works of GAN frameworks for text-to-image synthesis. The main focus of the GAN frameworks is to ensure that the generated images are semantically related to the input texts. This objective is mainly achieved by using a neural network to encode texts as dense features, which are further fed to a second network to generate images matching to the texts.",
        "type": "Document"
      },
      {
        "id": "b1b9d0a0-0d15-4384-86f4-cd6a7313b1d9",
        "metadata": {
          "vector_store_key": "1910.09399-1",
          "chunk_id": 50,
          "document_id": "1910.09399",
          "start_idx": 26439,
          "end_idx": 27228
        },
        "page_content": "However, the semantic relevance is a rather subjective measure, and images are inherently rich in terms of its semantics and interpretations. Therefore, many GANs are further proposed to enhance the text-to-image synthesis from different perspectives. In this subsection, we will review several classical approaches which are commonly served as text-to-image synthesis baseline. black Deep convolution generative adversarial network (DC-GAN) BIBREF8 represents the pioneer work for text-to-image synthesis using GANs. Its main goal is to train a deep convolutional generative adversarial network (DC-GAN) on text features. During this process these text features are encoded by another neural network. This neural network is a hybrid convolutional recurrent network at the character level.",
        "type": "Document"
      },
      {
        "id": "f6648896-266c-41be-8dfc-b93b0d2ae00b",
        "metadata": {
          "vector_store_key": "1910.09399-1",
          "chunk_id": 38,
          "document_id": "1910.09399",
          "start_idx": 19983,
          "end_idx": 20739
        },
        "page_content": "To ensure text being used as GAN input, a feature embedding or feature representation learning BIBREF29, BIBREF30 function $\\varphi ()$ is often introduced to convert input text as numeric features, which are further concatenated with other features to train GANs. black Motivated by the GAN and conditional GAN (cGAN) design, many GAN based frameworks have been proposed to generate images, with different designs and architectures, such as using multiple discriminators, using progressively trained discriminators, or using hierarchical discriminators. Figure FIGREF17 outlines several advanced GAN frameworks in the literature. In addition to these frameworks, many news designs are being proposed to advance the field with rather sophisticated designs.",
        "type": "Document"
      },
      {
        "id": "c163ea20-5c6f-4e1e-8d8e-ca106d3c9b21",
        "metadata": {
          "vector_store_key": "1910.09399-1",
          "chunk_id": 8,
          "document_id": "1910.09399",
          "start_idx": 4563,
          "end_idx": 5384
        },
        "page_content": "This can include ideas from two or more learning approaches in order to create a robust implementation to solve an uncommon problem or improve a solution BIBREF8, BIBREF14, BIBREF15, BIBREF16, BIBREF17. black In this survey, we focus primarily on reviewing recent works that aim to solve the challenge of text-to-image synthesis using generative adversarial networks (GANs). In order to provide a clear roadmap, we propose a taxonomy to summarize reviewed GANs into four major categories. Our review will elaborate the motivations of methods in each category, analyze typical models, their network architectures, and possible drawbacks for further improvement. The visual abstract of the survey and the list of reviewed GAN frameworks is shown in Figure FIGREF8. black The remainder of the survey is organized as follows.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What language do the agents talk in?",
    "llm_answer": "Natural language.\n",
    "context": [
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      },
      {
        "id": "cb4078b8-00d7-4e01-a91b-7292c78d6351",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 12,
          "document_id": "1910.04269",
          "start_idx": 7197,
          "end_idx": 7983
        },
        "page_content": "The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel). Several state-of-the-art results on various audio classification tasks have been obtained by using log-Mel spectrograms of raw audio, as features BIBREF19. Convolutional Neural Networks have demonstrated an excellent performance gain in classification of these features BIBREF20, BIBREF21 against other machine learning techniques. It has been shown that using attention layers with ConvNets further enhanced their performance BIBREF22. This motivated us to develop a CNN-based architecture with attention since this approach hasn\u2019t been applied to the task of language identification before.",
        "type": "Document"
      },
      {
        "id": "ba1508fe-9613-429c-9869-24985e5077db",
        "metadata": {
          "vector_store_key": "1807.03367-3",
          "chunk_id": 2,
          "document_id": "1807.03367",
          "start_idx": 1073,
          "end_idx": 1999
        },
        "page_content": "Learning algorithms for natural language understanding, such as in machine translation and reading comprehension, have progressed at an unprecedented rate in recent years, but still rely on static, large-scale, text-only datasets that lack crucial aspects of how humans understand and produce natural language. Namely, humans develop language capabilities by being embodied in an environment which they can perceive, manipulate and move around in; and by interacting with other humans. Hence, we argue that we should incorporate all three fundamental aspects of human language acquisition\u2014perception, action and interactive communication\u2014and develop a task and dataset to that effect. We introduce the Talk the Walk dataset, where the aim is for two agents, a \u201cguide\u201d and a \u201ctourist\u201d, to interact with each other via natural language in order to achieve a common goal: having the tourist navigate towards the correct location.",
        "type": "Document"
      },
      {
        "id": "78f6379a-3da8-4cb5-bdb1-2cfee0e7794f",
        "metadata": {
          "vector_store_key": "1807.03367-3",
          "chunk_id": 3,
          "document_id": "1807.03367",
          "start_idx": 1999,
          "end_idx": 2553
        },
        "page_content": "We introduce the Talk the Walk dataset, where the aim is for two agents, a \u201cguide\u201d and a \u201ctourist\u201d, to interact with each other via natural language in order to achieve a common goal: having the tourist navigate towards the correct location. The guide has access to a map and knows the target location, but does not know where the tourist is; the tourist has a 360-degree view of the world, but knows neither the target location on the map nor the way to it. The agents need to work together through communication in order to successfully solve the task.",
        "type": "Document"
      },
      {
        "id": "1b527725-122a-406b-b571-f83bd484ddf6",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 0,
          "document_id": "1910.04269",
          "start_idx": 0,
          "end_idx": 676
        },
        "page_content": "Language Identification (LI) is a problem which involves classifying the language being spoken by a speaker. LI systems can be used in call centers to route international calls to an operator who is fluent in that identified language BIBREF0. In speech-based assistants, LI acts as the first step which chooses the corresponding grammar from a list of available languages for its further semantic analysis BIBREF1. It can also be used in multi-lingual voice-controlled information retrieval systems, for example, Apple Siri and Amazon Alexa. Over the years, studies have utilized many prosodic and acoustic features to construct machine learning models for LI systems BIBREF2.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much better is performance of proposed method than state-of-the-art methods in experiments?",
    "llm_answer": "The proposed method achieved 93.7% accuracy, outperforming the Coppersmith et al. method which achieved 86%.  Other proposed methods (mixup augmentation, attention mechanism, recurrent layers) also achieved higher accuracy than the baseline Coppersmith et al. method, but the context does not explicitly quantify these improvements.\n",
    "context": [
      {
        "id": "467ee1c6-fd3f-41b6-8362-12b11ee9ed02",
        "metadata": {
          "vector_store_key": "1910.04269-1",
          "chunk_id": 38,
          "document_id": "1910.04269",
          "start_idx": 21056,
          "end_idx": 21639
        },
        "page_content": "This method is able to bypass the computational overhead of conventional approaches which depend on generation of spectrograms as a necessary pre-procesing step. We were able to achieve an accauracy of 93.7% using this technique. Next, we discussed the enhancement in performance of 2D-ConvNet using mixup augmentation, which is a recently developed technique to prevent over\ufb01tting on test data. This approach achieved an accuracy of 95.4%. We also analysed how attention mechanism and recurrent layers impact the performance of networks. This approach achieved an accuracy of 95.0%.",
        "type": "Document"
      },
      {
        "id": "b2f8b59b-1092-4698-9fa3-c0a21594d3ff",
        "metadata": {
          "vector_store_key": "2003.07433-4",
          "chunk_id": 47,
          "document_id": "2003.07433",
          "start_idx": 25723,
          "end_idx": 26401
        },
        "page_content": "To compare the outperformance of our method, we also implemented Coppersmith et. al. proposed method and achieved an 86% overall accuracy of detecting PTSD users BIBREF11 following the same training-test dataset distribution. Fig FIGREF28 illustrates the comparisons between LAXARY and Coppersmith et. al. proposed method. Here we can see, the outperformance of our proposed method as well as the importance of $s-score$ estimation. We also illustrates the importance of $\\alpha -score$ and $S-score$ in Fig FIGREF30. Fig FIGREF30 illustrates that if we change the number of training samples (%), LAXARY models outperforms Coppersmith et. al. proposed model under any condition.",
        "type": "Document"
      },
      {
        "id": "c3b4b0ea-1fb0-4db4-a1e3-cca1d982ef2e",
        "metadata": {
          "vector_store_key": "1809.04960-4",
          "chunk_id": 38,
          "document_id": "1809.04960",
          "start_idx": 21924,
          "end_idx": 22725
        },
        "page_content": "It shows that IR+Proposed can outperform the supervised IR model given the same paired dataset. It concludes that the proposed model can exploit the unpaired data to further improve the performance of the supervised model. Although our proposed model can achieve better performance than previous models, there are still remaining two questions: why our model can outperform them, and how to further improve the performance. To address these queries, we perform error analysis to analyze the error types of our model and the baseline models. We select TF-IDF, S2S, and IR as the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types of comments as described in the above retrieval evaluation setting: Correct, Plausible, Popular, and Random.",
        "type": "Document"
      },
      {
        "id": "e1c3263d-eee1-4f32-bfff-116ac1efd11d",
        "metadata": {
          "vector_store_key": "1811.00942-2",
          "chunk_id": 3,
          "document_id": "1811.00942",
          "start_idx": 2070,
          "end_idx": 2843
        },
        "page_content": "In this paper, we examine the quality\u2013performance tradeoff in the shift from non-neural to neural language models. In particular, we compare Kneser\u2013Ney smoothing, widely accepted as the state of the art prior to NLMs, to the best NLMs today. The decrease in perplexity on standard datasets has been well documented BIBREF3 , but to our knowledge no one has examined the performances tradeoffs. With deployment on a mobile device in mind, we evaluate energy usage and inference latency on a Raspberry Pi (which shares the same ARM architecture as nearly all smartphones today). We find that a 2.5 $\\times $ reduction in perplexity on PTB comes at a staggering cost in terms of performance: inference with NLMs takes 49 $\\times $ longer and requires 32 $\\times $ more energy.",
        "type": "Document"
      },
      {
        "id": "814822da-592c-40ae-bb7c-e78761735daf",
        "metadata": {
          "vector_store_key": "2002.06644-3",
          "chunk_id": 10,
          "document_id": "2002.06644",
          "start_idx": 6316,
          "end_idx": 7055
        },
        "page_content": "Our proposed methodology, the use of finetuned optimized BERT based models, and BERT-based ensemble models outperform the baselines for all the metrics. Among the optimized BERT based models, $RoBERTa_{large}$ outperforms all other non-ensemble models and the baselines for all metrics. It further achieves a maximum recall of $0.681$ for all the proposed models. We note that DistillRoBERTa, a distilled model, performs competitively, achieving $69.69\\%$ accuracy, and $0.672$ F1 score. This observation shows that distilled pretrained models can replace their undistilled counterparts in a low-computing environment. We further observe that ensemble models perform better than optimized BERT-based models and distilled pretrained models.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What baseline model is used?",
    "llm_answer": "TF-IDF, LDA, Random+Seed\n",
    "context": [
      {
        "id": "96576214-2c10-4049-ba11-db649d59677b",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 26,
          "document_id": "1809.04960",
          "start_idx": 15085,
          "end_idx": 15736
        },
        "page_content": "Unsupervised baseline models are as follows: TF-IDF (Lexical, Non-Neural) is an important unsupervised baseline. We use the concatenation of the title and the body as the query to retrieve the candidate comment set by means of the similarity of the tf-idf value. The model is trained on unpaired articles and comments, which is the same as our proposed model. LDA (Topic, Non-Neural) is a popular unsupervised topic model, which discovers the abstract \"topics\" that occur in a collection of documents. We train the LDA with the articles and comments in the training set. The model retrieves the comments by the similarity of the topic representations.",
        "type": "Document"
      },
      {
        "id": "36dcf150-24e8-459c-a8c0-aa8fe1c0018f",
        "metadata": {
          "vector_store_key": "1909.02764-4",
          "chunk_id": 36,
          "document_id": "1909.02764",
          "start_idx": 19976,
          "end_idx": 20568
        },
        "page_content": "We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with 40 % of the data from the same domain (results shown in the column \u201cIn-Domain\u201d in Table TABREF19). Excluding AMMER, we achieve an average micro $\\text{F}_1$ of 68 %, with best results of F$_1$=73 % on TEC. The model trained on our AMMER corpus achieves an F1 score of 57%. This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set.",
        "type": "Document"
      },
      {
        "id": "439bc21a-fce8-4632-bebc-fabe777f5a61",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 19,
          "document_id": "1909.00694",
          "start_idx": 11154,
          "end_idx": 11804
        },
        "page_content": "The Random+Seed baseline made use of the seed lexicon and output the corresponding label (or the reverse of it for negation) if the event's predicate is in the seed lexicon. We can see that the seed lexicon itself had practically no impact on prediction. The models in the top block performed considerably better than the random baselines. The performance gaps with their (semi-)supervised counterparts, shown in the middle block, were less than 7%. This demonstrates the effectiveness of discourse relation-based label propagation. Comparing the model variants, we obtained the highest score with the BiGRU encoder trained with the AL+CA+CO dataset.",
        "type": "Document"
      },
      {
        "id": "c9f58c8f-f07e-4688-a8ca-5b17e0fe4786",
        "metadata": {
          "vector_store_key": "2002.06644-4",
          "chunk_id": 9,
          "document_id": "2002.06644",
          "start_idx": 5847,
          "end_idx": 6623
        },
        "page_content": "For all BERT-based models, we use a learning rate of $2*10^{-5}$, a maximum sequence length of 50, and a weight decay of $0.01$ while finetuning the model. We use FastText's recently open-sourced automatic hyperparameter optimization functionality while training the model. For the BiLSTM baseline, we use a dropout of $0.05$ along with a recurrent dropout of $0.2$ in two 64 unit sized stacked BiLSTMs, using softmax activation layer as the final dense layer. tab:experimental-results shows the performance of different models on the WNC corpus evaluated on the following four metrics: Precision, Recall, F1, and Accuracy. Our proposed methodology, the use of finetuned optimized BERT based models, and BERT-based ensemble models outperform the baselines for all the metrics.",
        "type": "Document"
      },
      {
        "id": "e5025c6d-7833-4f82-9275-5f40aac5f5af",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 25,
          "document_id": "1809.04960",
          "start_idx": 14549,
          "end_idx": 15237
        },
        "page_content": "The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-supervised setting. We prune the vocabulary, and only leave 30,000 most frequent words in the vocabulary. We train the model for 20 epochs with the Adam optimizing algorithms BIBREF8 . In order to alleviate the KL vanishing problem, we set the initial learning to INLINEFORM2 , and use batch normalization BIBREF9 in each layer. We also gradually increase the KL term from 0 to 1 after each epoch. We compare our model with several unsupervised models and supervised models. Unsupervised baseline models are as follows: TF-IDF (Lexical, Non-Neural) is an important unsupervised baseline.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What stylistic features are used to detect drunk texts?",
    "llm_answer": "Capitalization, spelling errors.\n",
    "context": [
      {
        "id": "be500e16-7c5a-4f3b-94ee-285078f4aa42",
        "metadata": {
          "vector_store_key": "1610.00879-0",
          "chunk_id": 16,
          "document_id": "1610.00879",
          "start_idx": 9034,
          "end_idx": 9713
        },
        "page_content": "First, we justify the need for drunk-texting prediction as means of identifying risky social behavior arising out of alcohol abuse, and the need to build tools that avoid privacy leaks due to drunk-texting. We then highlight the challenges of drunk-texting prediction: one of the challenges is selection of negative examples (sober tweets). Using hashtag-based supervision, we create three datasets annotated with drunk or sober labels. We then present SVM-based classifiers which use two sets of features: N-gram and stylistic features. Our drunk prediction system obtains a best accuracy of 78.1%. We observe that our stylistic features add negligible value to N-gram features.",
        "type": "Document"
      },
      {
        "id": "09223f78-cfa2-4bc1-bff2-3588399329bc",
        "metadata": {
          "vector_store_key": "1610.00879-3",
          "chunk_id": 1,
          "document_id": "1610.00879",
          "start_idx": 742,
          "end_idx": 1489
        },
        "page_content": "We use hashtag-based supervision so that the authors of the tweets mention if they were drunk at the time of posting a tweet. We create three datasets by using different strategies that are related to the use of hashtags. We then present SVM-based classifiers that use N-gram and stylistic features such as capitalisation, spelling errors, etc. Through our experiments, we make subtle points related to: (a) the performance of our features, (b) how our approach compares against human ability to detect drunk-texting, (c) most discriminative stylistic features, and (d) an error analysis that points to future work. To the best of our knowledge, this is a first study that shows the feasibility of text-based analysis for drunk-texting prediction.",
        "type": "Document"
      },
      {
        "id": "eb7746b7-2906-446d-87bb-2eb006ef7581",
        "metadata": {
          "vector_store_key": "1610.00879-3",
          "chunk_id": 0,
          "document_id": "1610.00879",
          "start_idx": 0,
          "end_idx": 742
        },
        "page_content": "The ubiquity of communication devices has made social media highly accessible. The content on these media reflects a user's day-to-day activities. This includes content created under the influence of alcohol. In popular culture, this has been referred to as `drunk-texting'. In this paper, we introduce automatic `drunk-texting prediction' as a computational task. Given a tweet, the goal is to automatically identify if it was written by a drunk user. We refer to tweets written under the influence of alcohol as `drunk tweets', and the opposite as `sober tweets'. A key challenge is to obtain an annotated dataset. We use hashtag-based supervision so that the authors of the tweets mention if they were drunk at the time of posting a tweet.",
        "type": "Document"
      },
      {
        "id": "e2085436-7e18-4980-ba70-f2405ae7d33f",
        "metadata": {
          "vector_store_key": "1610.00879-3",
          "chunk_id": 15,
          "document_id": "1610.00879",
          "start_idx": 8649,
          "end_idx": 9312
        },
        "page_content": "This example also highlights the difficulty of drunk-texting prediction. Pragmatic difficulty: The tweet `National dress of Ireland is one's one vomit.. my family is lovely' was correctly identified by our human annotators as a drunk tweet. This tweet contains an element of humour and topic change, but our classifier could not capture it. In this paper, we introduce automatic drunk-texting prediction as the task of predicting a tweet as drunk or sober. First, we justify the need for drunk-texting prediction as means of identifying risky social behavior arising out of alcohol abuse, and the need to build tools that avoid privacy leaks due to drunk-texting.",
        "type": "Document"
      },
      {
        "id": "ffad015f-4d46-4ca6-8fe4-827d9b424ad2",
        "metadata": {
          "vector_store_key": "1610.00879-0",
          "chunk_id": 12,
          "document_id": "1610.00879",
          "start_idx": 6930,
          "end_idx": 7621
        },
        "page_content": "This degradation implies that our features capture a subset of drunk tweets and that there are properties of drunk tweets that may be more subtle. When both N-gram and stylistic features are used, there is negligible improvement. The accuracy for Dataset 2 increases from 77.9% to 78.1%. Precision/Recall metrics do not change significantly either. The best accuracy of our classifier is 78.1% for all features, and 75.6% for stylistic features. This shows that text-based clues can indeed be used for drunk-texting prediction. Using held-out dataset H, we evaluate how our system performs in comparison to humans. Three annotators, A1-A3, mark each tweet in the Dataset H as drunk or sober.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the accuracy of the proposed technique?",
    "llm_answer": "96%\n",
    "context": [
      {
        "id": "467ee1c6-fd3f-41b6-8362-12b11ee9ed02",
        "metadata": {
          "vector_store_key": "1910.04269-1",
          "chunk_id": 38,
          "document_id": "1910.04269",
          "start_idx": 21056,
          "end_idx": 21639
        },
        "page_content": "This method is able to bypass the computational overhead of conventional approaches which depend on generation of spectrograms as a necessary pre-procesing step. We were able to achieve an accauracy of 93.7% using this technique. Next, we discussed the enhancement in performance of 2D-ConvNet using mixup augmentation, which is a recently developed technique to prevent over\ufb01tting on test data. This approach achieved an accuracy of 95.4%. We also analysed how attention mechanism and recurrent layers impact the performance of networks. This approach achieved an accuracy of 95.0%.",
        "type": "Document"
      },
      {
        "id": "b2f8b59b-1092-4698-9fa3-c0a21594d3ff",
        "metadata": {
          "vector_store_key": "2003.07433-4",
          "chunk_id": 47,
          "document_id": "2003.07433",
          "start_idx": 25723,
          "end_idx": 26401
        },
        "page_content": "To compare the outperformance of our method, we also implemented Coppersmith et. al. proposed method and achieved an 86% overall accuracy of detecting PTSD users BIBREF11 following the same training-test dataset distribution. Fig FIGREF28 illustrates the comparisons between LAXARY and Coppersmith et. al. proposed method. Here we can see, the outperformance of our proposed method as well as the importance of $s-score$ estimation. We also illustrates the importance of $\\alpha -score$ and $S-score$ in Fig FIGREF30. Fig FIGREF30 illustrates that if we change the number of training samples (%), LAXARY models outperforms Coppersmith et. al. proposed model under any condition.",
        "type": "Document"
      },
      {
        "id": "88cff440-1fb7-4102-b287-79d43a63389a",
        "metadata": {
          "vector_store_key": "1905.00563-6",
          "chunk_id": 27,
          "document_id": "1905.00563",
          "start_idx": 13737,
          "end_idx": 14412
        },
        "page_content": "To evaluate the quality of our approximations and compare with influence function (IF), we conduct leave one out experiments. In this setup, we take all the neighbors of a random target triple as candidate modifications, remove them one at a time, retrain the model each time, and compute the exact change in the score of the target triple. We can use the magnitude of this change in score to rank the candidate triples, and compare this exact ranking with ranking as predicted by: , influence function with and without Hessian matrix, and the original model score (with the intuition that facts that the model is most confident of will have the largest impact when removed).",
        "type": "Document"
      },
      {
        "id": "56099834-1ec3-4313-bf17-9537276d2950",
        "metadata": {
          "vector_store_key": "2003.07433-4",
          "chunk_id": 46,
          "document_id": "2003.07433",
          "start_idx": 24953,
          "end_idx": 25723
        },
        "page_content": "The distribution of this training-test dataset segmentation followed a 50% distribution of PTSD and No PTSD from the original dataset. Our final survey based classification results showed an accuracy of 96% in detecting PTSD and mean squared error of 1.2 in estimating its intensity given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively. Table TABREF29 shows the classification details of our experiment which provide the very good accuracy of our classification. To compare the outperformance of our method, we also implemented Coppersmith et. al. proposed method and achieved an 86% overall accuracy of detecting PTSD users BIBREF11 following the same training-test dataset distribution.",
        "type": "Document"
      },
      {
        "id": "3e994528-05ae-4bc0-9cc2-11c767356392",
        "metadata": {
          "vector_store_key": "1908.08419-0",
          "chunk_id": 42,
          "document_id": "1908.08419",
          "start_idx": 24326,
          "end_idx": 25223
        },
        "page_content": "The experimental results of two sampling strategies with 30 iterations on four different proportions of relabeled data are shown in Fig. FIGREF45 , where x-axis represents the number of iterations and y-axis denotes the INLINEFORM0 -score of the segmenter. Scoring strategy shows consistent improvements over uncertainty sampling in the early iterations, indicating that scoring strategy is more capable of selecting representative samples. Furthermore, we also investigate the relations between the best INLINEFORM0 -score and corresponding number of iteration on two sampling strategies, which is depicted in Fig. FIGREF46 . It is observed that in our proposed scoring model, with the proportion of relabeled data increasing, the iteration number of reaching the optimal word segmentation result is decreasing, but the INLINEFORM0 -score of CRF-based word segmenter is also gradually decreasing.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which retrieval system was used for baselines?",
    "llm_answer": "Sliding Window (SW), Sliding Window + Distance (SW+D), TF-IDF, LDA, IR.\n",
    "context": [
      {
        "id": "ddbf653d-2b06-40f5-b102-1128cf41b68a",
        "metadata": {
          "vector_store_key": "1707.03904-0",
          "chunk_id": 31,
          "document_id": "1707.03904",
          "start_idx": 17195,
          "end_idx": 17895
        },
        "page_content": "For Quasar-T we also test the Sliding Window (SW) and Sliding Window + Distance (SW+D) baselines proposed in BIBREF13 . The scores were computed for the list of candidate solutions described in Section \"Context Retrieval\" . For Quasar-S, since the answers come from a fixed vocabulary of entities, we test language model baselines which predict the most likely entity to appear in a given context. We train three n-gram baselines using the SRILM toolkit BIBREF21 for $n=3,4,5$ on the entire corpus of all Stack Overflow posts. The output predictions are restricted to the output vocabulary of entities. We also train a bidirectional Recurrent Neural Network (RNN) language model (based on GRU units).",
        "type": "Document"
      },
      {
        "id": "96576214-2c10-4049-ba11-db649d59677b",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 26,
          "document_id": "1809.04960",
          "start_idx": 15085,
          "end_idx": 15736
        },
        "page_content": "Unsupervised baseline models are as follows: TF-IDF (Lexical, Non-Neural) is an important unsupervised baseline. We use the concatenation of the title and the body as the query to retrieve the candidate comment set by means of the similarity of the tf-idf value. The model is trained on unpaired articles and comments, which is the same as our proposed model. LDA (Topic, Non-Neural) is a popular unsupervised topic model, which discovers the abstract \"topics\" that occur in a collection of documents. We train the LDA with the articles and comments in the training set. The model retrieves the comments by the similarity of the topic representations.",
        "type": "Document"
      },
      {
        "id": "318e0831-6243-47bc-8d74-a30a0b0a8120",
        "metadata": {
          "vector_store_key": "1704.05572-1",
          "chunk_id": 20,
          "document_id": "1704.05572",
          "start_idx": 10563,
          "end_idx": 11407
        },
        "page_content": "This corpus is used by the IR solver and also used to create the tuple KB T and on-the-fly tuples $T^{\\prime }_{qa}$ . Additionally, TableILP uses $\\sim $ 70 Curated tables (C) designed for 4th grade NY Regents exams. We compare TupleInf with two state-of-the-art baselines. IR is a simple yet powerful information-retrieval baseline BIBREF6 that selects the answer option with the best matching sentence in a corpus. TableILP is the state-of-the-art structured inference baseline BIBREF9 developed for science questions. Table 2 shows that TupleInf, with no curated knowledge, outperforms TableILP on both question sets by more than 11%. The lower half of the table shows that even when both solvers are given the same knowledge (C+T), the improved selection and simplified model of TupleInf results in a statistically significant improvement.",
        "type": "Document"
      },
      {
        "id": "16f09155-952c-45b5-8065-8b29320a5f71",
        "metadata": {
          "vector_store_key": "1707.03904-0",
          "chunk_id": 34,
          "document_id": "1707.03904",
          "start_idx": 18788,
          "end_idx": 19449
        },
        "page_content": "For Quasar-T we train and test GA on all instances where the answer is in the context and is a single token. The BiDAF model BIBREF9 is also a multi-layer neural network which predicts a span of text from the passage as the answer to a given query. At the time of writing it had state-of-the-art performance among published models on the Squad dataset. For Quasar-T we train and test BiDAF on all instances where the answer is in the retrieved context. Several baselines rely on the retrieved context to extract the answer to a question. For these, we refer to the fraction of instances for which the correct answer is present in the context as Search Accuracy.",
        "type": "Document"
      },
      {
        "id": "3699d425-11f4-45cc-aae5-6a93d3e32b86",
        "metadata": {
          "vector_store_key": "1707.03904-0",
          "chunk_id": 8,
          "document_id": "1707.03904",
          "start_idx": 4627,
          "end_idx": 5317
        },
        "page_content": "We evaluate Quasar against human testers, as well as several baselines ranging from na\u00efve heuristics to state-of-the-art machine readers. The best performing baselines achieve $33.6\\%$ and $28.5\\%$ on Quasar-S and Quasar-T, while human performance is $50\\%$ and $60.6\\%$ respectively. For the automatic systems, we see an interesting tension between searching and reading accuracies \u2013 retrieving more documents in the search phase leads to a higher coverage of answers, but makes the comprehension task more difficult. We also collect annotations on a subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much better was the BLSTM-CNN-CRF than the BLSTM-CRF?",
    "llm_answer": "Answer not found in the context.\n",
    "context": [
      {
        "id": "993cd19c-93b3-4820-be72-e55dcc52919c",
        "metadata": {
          "vector_store_key": "1911.07228-1",
          "chunk_id": 16,
          "document_id": "1911.07228",
          "start_idx": 7814,
          "end_idx": 8542
        },
        "page_content": "Before we decide to analysis on this result, we have run some other methods, but this one with Vietnamese pre-trained word embeddings provided by Kyubyong Park obtains the best result. Other results are shown in the Table 2. Table 2 shows our experiments on two models with and without different pre-trained word embedding \u2013 KP means the Kyubyong Park\u2019s pre-trained word embeddings and EG means Edouard Grave\u2019s pre-trained word embeddings. We compare the outputs of BLSTM-CNN-CRF model (predicted) to the annotated data (gold) and analyzed the errors. Table 3 shows perfomance of the BLSTM-CNN-CRF model. In our experiments, we use three evaluation parameters (precision, recall, and F1 score) to access our experimental result.",
        "type": "Document"
      },
      {
        "id": "66262267-1bd1-4f9b-9e40-caaa757fe6bd",
        "metadata": {
          "vector_store_key": "1911.07228-1",
          "chunk_id": 5,
          "document_id": "1911.07228",
          "start_idx": 3058,
          "end_idx": 3804
        },
        "page_content": "They have archived great results when it comes to NER tasks, for example, Guillaume Lample et al with BLSTM-CRF in BIBREF6 report 90.94 F1 score, Chiu et al with BLSTM-CNN in BIBREF7 got 91.62 F1 score, Xeuzhe Ma and Eduard Hovy with BLSTM-CNN-CRF in BIBREF8 achieved F1 score of 91.21, Thai-Hoang Pham and Phuong Le-Hong with BLSTM-CNN-CRF in BIBREF9 got 88.59% F1 score. These DNN models are also the state-of-the-art models. The results of our analysis experiments are reported in precision and recall over all labels (name of person, location, organization and miscellaneous). The process of analyzing errors has 2 steps: Step 1: We use two state-of-the-art models including BLSTM-CNN-CRF and BLSTM-CRF to train and test on VLSP\u2019s NER corpus.",
        "type": "Document"
      },
      {
        "id": "bc40a3cb-7555-49a3-a715-93e3d653bc48",
        "metadata": {
          "vector_store_key": "1911.07228-1",
          "chunk_id": 15,
          "document_id": "1911.07228",
          "start_idx": 7587,
          "end_idx": 8269
        },
        "page_content": "The format is the same at Kyubyong's, but their embedding is the vector of 300 dimension, and they have about 200k words Based on state-of-the-art methods for NER, BLSTM-CNN-CRF is the end-to-end deep neural network model that achieves the best result on F-score BIBREF9. Therefore, we decide to conduct the experiment on this model and analyze the errors. We run experiment with the Ma and Hovy (2016) model BIBREF8, source code provided by (Motoki Sato) and analysis the errors from this result. Before we decide to analysis on this result, we have run some other methods, but this one with Vietnamese pre-trained word embeddings provided by Kyubyong Park obtains the best result.",
        "type": "Document"
      },
      {
        "id": "be638846-a53c-48e4-8b9b-4a9387ee3be1",
        "metadata": {
          "vector_store_key": "1902.00330-3",
          "chunk_id": 45,
          "document_id": "1902.00330",
          "start_idx": 24946,
          "end_idx": 25628
        },
        "page_content": "Previous best results on this dataset are generated by BIBREF0 , BIBREF1 which both built CRF models. They calculate the pairwise scores between all candidate entities. Differently, our model only considers the consistency of the target entities and ignores the relationship between incorrect candidates. The experimental results show that our model can reduce the impact of noise data and improve the accuracy of disambiguation. Apart from experimenting on AIDA-B, we also conduct experiments on several different datasets to verify the generalization performance of our model. From Table 3, we can see that RLEL has achieved relatively good performances on ACE2004, CWEB and WIKI.",
        "type": "Document"
      },
      {
        "id": "c9f58c8f-f07e-4688-a8ca-5b17e0fe4786",
        "metadata": {
          "vector_store_key": "2002.06644-4",
          "chunk_id": 9,
          "document_id": "2002.06644",
          "start_idx": 5847,
          "end_idx": 6623
        },
        "page_content": "For all BERT-based models, we use a learning rate of $2*10^{-5}$, a maximum sequence length of 50, and a weight decay of $0.01$ while finetuning the model. We use FastText's recently open-sourced automatic hyperparameter optimization functionality while training the model. For the BiLSTM baseline, we use a dropout of $0.05$ along with a recurrent dropout of $0.2$ in two 64 unit sized stacked BiLSTMs, using softmax activation layer as the final dense layer. tab:experimental-results shows the performance of different models on the WNC corpus evaluated on the following four metrics: Precision, Recall, F1, and Accuracy. Our proposed methodology, the use of finetuned optimized BERT based models, and BERT-based ensemble models outperform the baselines for all the metrics.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What supplemental tasks are used for multitask learning?",
    "llm_answer": "Task A and Task B.\n",
    "context": [
      {
        "id": "25537dbc-62e8-462f-aec8-fa048800b48d",
        "metadata": {
          "vector_store_key": "1603.07044-4",
          "chunk_id": 23,
          "document_id": "1603.07044",
          "start_idx": 12559,
          "end_idx": 13202
        },
        "page_content": "For task C, pretraining by task A is also better than using SNLI (task A is $1\\%$ better than the baseline, while SNLI is almost the same). In summary, the in-domain pretraining seems better, but overall, the improvement is less than we expected, especially for task B, which only has very limited target data. We will not make a conclusion here since more investigation is needed. As mentioned in Section \"Modeling Question-External Comments\" , we also explored a multitask learning framework that jointly learns to predict the relationships of all three tasks. We set $0.8$ for the main task (task C) and $0.1$ for the other auxiliary tasks.",
        "type": "Document"
      },
      {
        "id": "c339f618-d827-414a-8c1a-a2ec44007ae4",
        "metadata": {
          "vector_store_key": "1908.06606-0",
          "chunk_id": 8,
          "document_id": "1908.06606",
          "start_idx": 5104,
          "end_idx": 5876
        },
        "page_content": "Task-specific end-to-end methods BIBREF3, BIBREF4 use large amount of data to automatically model the specific task. Topaz et al. BIBREF3 constructed an automated wound information identification model with five output. Tan et al. BIBREF4 identified patients undergoing radical cystectomy for bladder cancer. Although they achieved good performance, none of their models could be used to another task due to output format difference. This makes building a new model for a new task a costly job. Pipeline methods BIBREF7, BIBREF8, BIBREF9 break down the entire task into several basic natural language processing tasks. Bill et al. BIBREF7 focused on attributes extraction which mainly relied on dependency parsing and named entity recognition BIBREF10, BIBREF11, BIBREF12.",
        "type": "Document"
      },
      {
        "id": "fa8185d8-687d-4b00-93a8-75d7022bafc5",
        "metadata": {
          "vector_store_key": "1908.08419-1",
          "chunk_id": 3,
          "document_id": "1908.08419",
          "start_idx": 1904,
          "end_idx": 2606
        },
        "page_content": "Moreover, the task in medical domain is rarely dabbled, and only one related work on transfer learning is found in recent literatures BIBREF8 . However, researches related to transfer learning mostly remain in general domains, causing a major problem that a considerable amount of manually annotated data is required, when introducing the models into specific domains. One of the solutions for this obstacle is to use active learning, where only a small scale of samples are selected and labeled in an active manner. Active learning methods are favored by the researchers in many natural language processing (NLP) tasks, such as text classification BIBREF9 and named entity recognition (NER) BIBREF10 .",
        "type": "Document"
      },
      {
        "id": "3ac8c891-06f8-4226-8c58-9015ab7237fc",
        "metadata": {
          "vector_store_key": "1703.05260-2",
          "chunk_id": 7,
          "document_id": "1703.05260",
          "start_idx": 4104,
          "end_idx": 4754
        },
        "page_content": "We selected 10 scenarios from different available scenario lists (e.g. Regneri:2010 , VanDerMeer2009, and the OMICS corpus BIBREF1 ), including scripts of different complexity (Taking a bath vs. Flying in an airplane) and specificity (Riding a public bus vs. Repairing a flat bicycle tire). For the full scenario list see Table 2 . Texts were collected via the Amazon Mechanical Turk platform, which provides an opportunity to present an online task to humans (a.k.a. turkers). In order to gauge the effect of different M-Turk instructions on our task, we first conducted pilot experiments with different variants of instructions explaining the task.",
        "type": "Document"
      },
      {
        "id": "cb355127-c6ba-457f-bc45-a28180fb1025",
        "metadata": {
          "vector_store_key": "1905.11901-0",
          "chunk_id": 19,
          "document_id": "1905.11901",
          "start_idx": 10984,
          "end_idx": 11819
        },
        "page_content": "Our results show that low-resource NMT is very sensitive to hyperparameters such as BPE vocabulary size, word dropout, and others, and by following a set of best practices, we can train competitive NMT systems without relying on auxiliary resources. This has practical relevance for languages where large amounts of monolingual data, or multilingual data involving related languages, are not available. Even though we focused on only using parallel data, our results are also relevant for work on using auxiliary data to improve low-resource MT. Supervised systems serve as an important baseline to judge the effectiveness of semisupervised or unsupervised approaches, and the quality of supervised systems trained on little data can directly impact semi-supervised workflows, for instance for the back-translation of monolingual data.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much performance gap between their approach and the strong handcrafted method?",
    "llm_answer": "The context mentions outperforming Coppersmith et al.'s method, but does not quantify the performance gap.\n",
    "context": [
      {
        "id": "814822da-592c-40ae-bb7c-e78761735daf",
        "metadata": {
          "vector_store_key": "2002.06644-3",
          "chunk_id": 10,
          "document_id": "2002.06644",
          "start_idx": 6316,
          "end_idx": 7055
        },
        "page_content": "Our proposed methodology, the use of finetuned optimized BERT based models, and BERT-based ensemble models outperform the baselines for all the metrics. Among the optimized BERT based models, $RoBERTa_{large}$ outperforms all other non-ensemble models and the baselines for all metrics. It further achieves a maximum recall of $0.681$ for all the proposed models. We note that DistillRoBERTa, a distilled model, performs competitively, achieving $69.69\\%$ accuracy, and $0.672$ F1 score. This observation shows that distilled pretrained models can replace their undistilled counterparts in a low-computing environment. We further observe that ensemble models perform better than optimized BERT-based models and distilled pretrained models.",
        "type": "Document"
      },
      {
        "id": "b2f8b59b-1092-4698-9fa3-c0a21594d3ff",
        "metadata": {
          "vector_store_key": "2003.07433-4",
          "chunk_id": 47,
          "document_id": "2003.07433",
          "start_idx": 25723,
          "end_idx": 26401
        },
        "page_content": "To compare the outperformance of our method, we also implemented Coppersmith et. al. proposed method and achieved an 86% overall accuracy of detecting PTSD users BIBREF11 following the same training-test dataset distribution. Fig FIGREF28 illustrates the comparisons between LAXARY and Coppersmith et. al. proposed method. Here we can see, the outperformance of our proposed method as well as the importance of $s-score$ estimation. We also illustrates the importance of $\\alpha -score$ and $S-score$ in Fig FIGREF30. Fig FIGREF30 illustrates that if we change the number of training samples (%), LAXARY models outperforms Coppersmith et. al. proposed model under any condition.",
        "type": "Document"
      },
      {
        "id": "9e9dd30e-b89c-4262-8802-6205bee46ca9",
        "metadata": {
          "vector_store_key": "1909.00578-1",
          "chunk_id": 17,
          "document_id": "1909.00578",
          "start_idx": 9107,
          "end_idx": 9819
        },
        "page_content": "The three fine-tuned BERT versions clearly outperform all other methods. Multi-task versions seem to perform better than single-task ones in most cases. Especially for $\\mathcal {Q}4$ and $\\mathcal {Q}5$, which are highly correlated, the multi-task BERT versions achieve the best overall results. BiGRU-ATT also benefits from multi-task learning. The correlation of Sum-QE with human judgments is high or very high BIBREF23 for all $\\mathcal {Q}$s in all datasets, apart from $\\mathcal {Q}2$ in DUC-05 where it is only moderate. Manual scores for $\\mathcal {Q}2$ in DUC-05 are the highest among all $\\mathcal {Q}$s and years (between 4 and 5) and with the smallest standard deviation, as shown in Table TABREF24.",
        "type": "Document"
      },
      {
        "id": "1ef3e3a7-71e4-44eb-9834-924dc0449d50",
        "metadata": {
          "vector_store_key": "1809.04960-1",
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 20725,
          "end_idx": 21462
        },
        "page_content": "We compare the generated comments of either the retrieval-based models or the generative models with the five reference comments. We select four popular metrics in text generation to compare the model outputs with the references: BLEU BIBREF14 , METEOR BIBREF15 , ROUGE BIBREF16 , CIDEr BIBREF17 . Table TABREF32 shows the performance for our models and the baselines in generative evaluation. Similar to the retrieval evaluation, our proposed model outperforms the other unsupervised methods, which are TF-IDF, NVDM, and LDA, in generative evaluation. Still, the supervised IR achieves better scores than the seq2seq model. With the help of our proposed model, both IR and S2S achieve an improvement under the semi-supervised scenarios.",
        "type": "Document"
      },
      {
        "id": "059820ad-3f5b-44c5-a3e8-611cfcfed0d9",
        "metadata": {
          "vector_store_key": "1705.09665-0",
          "chunk_id": 35,
          "document_id": "1705.09665",
          "start_idx": 20067,
          "end_idx": 20954
        },
        "page_content": "We compared out-of-sample predictive performance via leave-one-community-out cross validation using random forest regressors with ensembles of size 100, and otherwise default hyperparameters BIBREF26 . A model predicting average monthly retention based on a community's average distinctiveness and dynamicity achieves an average mean squared error ( INLINEFORM0 ) of INLINEFORM1 and INLINEFORM2 , while an analogous model predicting based on a community's size and average activity level (both log-transformed) achieves INLINEFORM4 and INLINEFORM5 . The difference between the two models is not statistically significant ( INLINEFORM6 , Wilcoxon signed-rank test). However, combining features from both models results in a large and statistically significant improvement over each independent model ( INLINEFORM7 , INLINEFORM8 , INLINEFORM9 Bonferroni-corrected pairwise Wilcoxon tests).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is their model?",
    "llm_answer": "The answer is not found in the context.  While the context describes various models and their components, it does not specify the overall size of *any* model.\n",
    "context": [
      {
        "id": "873a1a3f-c16c-4653-b9c4-504b9f1451d5",
        "metadata": {
          "vector_store_key": "1902.09314-0",
          "chunk_id": 27,
          "document_id": "1902.09314",
          "start_idx": 16318,
          "end_idx": 16983
        },
        "page_content": "To figure out whether the proposed AEN-GloVe is a lightweight alternative of recurrent models, we study the model size of each model on the Restaurant dataset. Statistical results are reported in Table TABREF37 . We implement all the compared models base on the same source code infrastructure, use the same hyperparameters, and run them on the same GPU . RNN-based and BERT-based models indeed have larger model size. ATAE-LSTM, IAN, RAM, and AEN-GloVe-BiLSTM are all attention based RNN models, memory optimization for these models will be more difficult as the encoded hidden states must be kept simultaneously in memory in order to perform attention mechanisms.",
        "type": "Document"
      },
      {
        "id": "bb96f53b-98f0-4811-a691-0975f908cb31",
        "metadata": {
          "vector_store_key": "1909.00694-2",
          "chunk_id": 26,
          "document_id": "1909.00694",
          "start_idx": 15102,
          "end_idx": 15722
        },
        "page_content": "The dimension of hidden units was 256. The optimizer was Momentum SGD BIBREF21. The mini-batch size was 1024. We ran 100 epochs and selected the snapshot that achieved the highest score for the dev set. We used a Japanese BERT model pretrained with Japanese Wikipedia. The input sentences were segmented into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam BIBREF19.",
        "type": "Document"
      },
      {
        "id": "cb5bd2db-598a-4609-87f5-8fb7900ac3bd",
        "metadata": {
          "vector_store_key": "1902.09314-0",
          "chunk_id": 28,
          "document_id": "1902.09314",
          "start_idx": 16983,
          "end_idx": 17654
        },
        "page_content": "ATAE-LSTM, IAN, RAM, and AEN-GloVe-BiLSTM are all attention based RNN models, memory optimization for these models will be more difficult as the encoded hidden states must be kept simultaneously in memory in order to perform attention mechanisms. MemNet has the lowest model size as it only has one shared attention layer and two linear layers, it does not calculate hidden states of word embeddings. AEN-GloVe's lightweight level ranks second, since it takes some more parameters than MemNet in modeling hidden states of sequences. As a comparison, the model size of AEN-GloVe-BiLSTM is more than twice that of AEN-GloVe, but does not bring any performance improvements.",
        "type": "Document"
      },
      {
        "id": "4d2ebadf-f898-458b-925d-784a9f7ee7ad",
        "metadata": {
          "vector_store_key": "1807.03367-3",
          "chunk_id": 9,
          "document_id": "1807.03367",
          "start_idx": 5176,
          "end_idx": 5821
        },
        "page_content": "5) We show that our best model exceeds human performance under the assumption of \u201cperfect perception\u201d and with a learned emergent communication protocol, and sets a non-trivial baseline with natural language. We create a perceptual environment by manually capturing several neighborhoods of New York City (NYC) with a 360 camera. Most parts of the city are grid-like and uniform, which makes it well-suited for obtaining a 2D grid. For Talk The Walk, we capture parts of Hell's Kitchen, East Village, the Financial District, Williamsburg and the Upper East Side\u2014see Figure FIGREF66 in Appendix SECREF14 for their respective locations within NYC.",
        "type": "Document"
      },
      {
        "id": "5fa1d98c-68a5-468c-a996-829da5ff9fb7",
        "metadata": {
          "vector_store_key": "1809.04960-1",
          "chunk_id": 24,
          "document_id": "1809.04960",
          "start_idx": 13943,
          "end_idx": 14549
        },
        "page_content": "Following the previous work BIBREF0 , we tokenize all text with the popular python package Jieba, and filter out short articles with less than 30 words in content and those with less than 20 comments. The dataset is split into training/validation/test sets, and they contain 191,502/5,000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many emotions do they look at?",
    "llm_answer": "3 (joy, annoyance/anger, insecurity/fear)\n",
    "context": [
      {
        "id": "08da2629-499c-402a-b26e-00cca44afc78",
        "metadata": {
          "vector_store_key": "1909.02764-0",
          "chunk_id": 8,
          "document_id": "1909.02764",
          "start_idx": 4526,
          "end_idx": 5209
        },
        "page_content": "They found that bothered and confused are difficult to distinguish, while happy and concentrated are well identified. Aiming to reduce computational cost, Tews2011 apply a simple feature extraction using four dots in the face defining three facial areas. They analyze the variance of the three facial areas for the recognition of happy, anger and neutral. Ihme2018 aim at detecting frustration in a simulator environment. They induce the emotion with specific scenarios and a demanding secondary task and are able to associate specific face movements according to FACS. Paschero2012 use OpenCV (https://opencv.org/) to detect the eyes and the mouth region and track facial movements.",
        "type": "Document"
      },
      {
        "id": "734513e7-92a7-4bd8-b42f-45d6d3baa233",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 27,
          "document_id": "1909.02764",
          "start_idx": 15296,
          "end_idx": 15975
        },
        "page_content": "The emotion self-ratings from the participants yielded 90 utterances labeled with joy, 26 with annoyance, 49 with insecurity, 9 with boredom, 111 with relaxation and 3 with no emotion. One example interaction per interaction type and emotion is shown in Table TABREF7. For further experiments, we only use joy, annoyance/anger, and insecurity/fear due to the small sample size for boredom and no emotion and under the assumption that relaxation brings little expressivity. We preprocess the visual data by extracting the sequence of images for each interaction from the point where the agent's or the co-driver's question was completely uttered until the driver's response stops.",
        "type": "Document"
      },
      {
        "id": "bf684b1e-cc6b-45e9-af07-09b1f3ab74cf",
        "metadata": {
          "vector_store_key": "1909.02764-0",
          "chunk_id": 29,
          "document_id": "1909.02764",
          "start_idx": 16227,
          "end_idx": 16903
        },
        "page_content": "While joy corresponds directly to our annotation, we map anger to our label annoyance and fear to our label insecurity. The maximal average score across all frames constitutes the overall classification for the video sequence. Frames where the software is not able to detect the face are ignored. We extract the audio signal for the same sequence as described for facial expressions and apply an off-the-shelf tool for emotion recognition. The software delivers single classification scores for a set of 24 discrete emotions for the entire utterance. We consider the outputs for the states of joy, anger, and fear, mapping analogously to our classes as for facial expressions.",
        "type": "Document"
      },
      {
        "id": "6fdd8894-df8c-4b59-97c6-5f3a97990d27",
        "metadata": {
          "vector_store_key": "1909.02764-0",
          "chunk_id": 0,
          "document_id": "1909.02764",
          "start_idx": 0,
          "end_idx": 684
        },
        "page_content": "Automatic emotion recognition is commonly understood as the task of assigning an emotion to a predefined instance, for example an utterance (as audio signal), an image (for instance with a depicted face), or a textual unit (e.g., a transcribed utterance, a sentence, or a Tweet). The set of emotions is often following the original definition by Ekman Ekman1992, which includes anger, fear, disgust, sadness, joy, and surprise, or the extension by Plutchik Plutchik1980 who adds trust and anticipation. Most work in emotion detection is limited to one modality. Exceptions include Busso2004 and Sebe2005, who investigate multimodal approaches combining speech with facial information.",
        "type": "Document"
      },
      {
        "id": "e5d0a55a-f18a-4dd4-9ebd-628c10217093",
        "metadata": {
          "vector_store_key": "1909.02764-0",
          "chunk_id": 9,
          "document_id": "1909.02764",
          "start_idx": 5209,
          "end_idx": 5960
        },
        "page_content": "Paschero2012 use OpenCV (https://opencv.org/) to detect the eyes and the mouth region and track facial movements. They simulate different lightning conditions and apply a multilayer perceptron for the classification task of Ekman's set of fundamental emotions. Overall, we found that studies using facial features usually focus on continuous driver monitoring, often in driver-only scenarios. In contrast, our work investigates the potential of emotion recognition during speech interactions. Past research on emotion recognition from acoustics mainly concentrates on either feature selection or the development of appropriate classifiers. rao2013emotion as well as ververidis2004automatic compare local and global features in support vector machines.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What approach did previous models use for multi-span questions?",
    "llm_answer": "Only one model, BIBREF4, directly targeted multi-span questions.\n",
    "context": [
      {
        "id": "47ce9ff5-59db-47fa-8b43-b203588dbab0",
        "metadata": {
          "vector_store_key": "1909.13375-2",
          "chunk_id": 2,
          "document_id": "1909.13375",
          "start_idx": 823,
          "end_idx": 1502
        },
        "page_content": "Among these new types were questions that require simple numerical reasoning, i.e questions whose answer is the result of a simple arithmetic expression containing numbers from the passage, and questions whose answers consist of several spans taken from the paragraph or the question itself, what we will denote as \"multi-span questions\". Of all the existing models that tried to tackle DROP, only one model BIBREF4 directly targeted multi-span questions in a manner that wasn't just a by-product of the model's overall performance. In this paper, we propose a new method for tackling multi-span questions. Our method takes a different path from that of the aforementioned model.",
        "type": "Document"
      },
      {
        "id": "95ac1b28-c666-4c44-bc76-e8f9512cf094",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 47,
          "document_id": "1909.13375",
          "start_idx": 25093,
          "end_idx": 25821
        },
        "page_content": "Furthermore, we show that our model slightly eclipses the current state-of-the-art results on the entire DROP dataeset. Finally, we present some ablation studies, analyzing the benefit gained from individual components of our model. We believe that combining our tag-based approach for handling multi-span questions with current successful techniques for handling single-span questions could prove beneficial in finding better, more holistic ways, of tackling span questions in general. Currently, For each individual span, we optimize the average likelihood over all its possible tag sequences (see Section SECREF9). A different approach could be not taking each possible tag sequence into account but only the most likely one.",
        "type": "Document"
      },
      {
        "id": "93b8b741-b6d7-4597-9fce-2de82a1eb34e",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 42,
          "document_id": "1909.13375",
          "start_idx": 22170,
          "end_idx": 22785
        },
        "page_content": "Therefore, it could prove beneficial to try and analyze the reasons behind each model's (ours and MTMSN) relative advantages, and perhaps try to combine them into a more holistic approach of tackling span questions. Table TABREF25 shows the results on DROP's test set, with our model being the best overall as of the time of writing, and not just on multi-span questions. In order to analyze the effect of each of our changes, we conduct ablation studies on the development set, depicted in Table TABREF26. Not using the simple preprocessing from Section SECREF17 resulted in a 2.5 point decrease in both EM and F1.",
        "type": "Document"
      },
      {
        "id": "e81fee71-223a-449b-b9b3-75b7e7e7caec",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 41,
          "document_id": "1909.13375",
          "start_idx": 21792,
          "end_idx": 22634
        },
        "page_content": "When comparing single-span performance, our best model exhibits slightly better results, but it should be noted that it retains the single-span heads from NABERT+, while in MTMSN they have one head to predict both single-span and multi-span answers. For a fairer comparison, we trained our model with the single-span heads removed, where our multi-span head remained the only head aimed for handling span questions. With this no-single-span-heads setting, while our multi-span performance even improved a bit, our single-span performance suffered a slight drop, ending up trailing by 0.8 in EM and 0.6 in F1 compared to MTMSN. Therefore, it could prove beneficial to try and analyze the reasons behind each model's (ours and MTMSN) relative advantages, and perhaps try to combine them into a more holistic approach of tackling span questions.",
        "type": "Document"
      },
      {
        "id": "552a6694-b465-4ccf-8c43-206e40391d8c",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 46,
          "document_id": "1909.13375",
          "start_idx": 24311,
          "end_idx": 25093
        },
        "page_content": "This approach is based on individually tagging each token with a categorical tag, relying on the tokens' contextual representation to bridge the information gap resulting from the tokens being tagged individually. First, we show that integrating this new approach into an existing model, NABERT+, does not hinder performance on other questions types, while substantially improving the results on multi-span questions. Later, we compare our results to the current state-of-the-art on multi-span questions. We show that our model has a clear advantage in handling multi-span questions, with a 29.7 absolute improvement in EM, and a 15.1 absolute improvement in F1. Furthermore, we show that our model slightly eclipses the current state-of-the-art results on the entire DROP dataeset.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is difference in peformance between proposed model and state-of-the art on other question types?",
    "llm_answer": "The provided text states that the \"proposed+IR\" model achieves the best performance and better accuracy in discriminating plausible from correct comments compared to other models (IR, TF-IDF, and S2S).  However, it does not give specific performance numbers for different question types compared to the other models.\n",
    "context": [
      {
        "id": "0ab61107-67bf-493c-aa5f-199d370caaa3",
        "metadata": {
          "vector_store_key": "1703.04617-2",
          "chunk_id": 26,
          "document_id": "1703.04617",
          "start_idx": 14460,
          "end_idx": 15206
        },
        "page_content": "The previous models are often trained for all questions without explicitly discriminating different question types; however, for a target question, both the common features shared by all questions and the specific features for a specific type of question are further considered in this paper, as they could potentially obey different distributions. In this paper we further explicitly model different types of questions in the end-to-end training. We start from a simple way to first analyze the word frequency of all questions, and obtain top-10 most frequent question types: what, how, who, when, which, where, why, be, whose, and whom, in which be stands for the questions beginning with different forms of the word be such as is, am, and are.",
        "type": "Document"
      },
      {
        "id": "fa1332b3-08d3-4641-801f-3a356bc48326",
        "metadata": {
          "vector_store_key": "1809.04960-4",
          "chunk_id": 41,
          "document_id": "1809.04960",
          "start_idx": 23770,
          "end_idx": 24538
        },
        "page_content": "IR achieves better performance than TF-IDF and S2S. However, it still suffers from the discrimination between the plausible comments and correct comments. This is mainly because IR does not explicitly model the underlying topics. Therefore, the correct comments which are more relevant in topic with the articles get lower scores than the plausible comments which are more literally relevant with the articles. With the help of our proposed model, proposed+IR achieves the best performance, and achieves a better accuracy to discriminate the plausible comments and the correct comments. Our proposed model incorporates the topic information, so the correct comments which are more similar to the articles in topic obtain higher scores than the other types of comments.",
        "type": "Document"
      },
      {
        "id": "839d707b-6844-4972-9695-67edf2e30c72",
        "metadata": {
          "vector_store_key": "1703.04617-2",
          "chunk_id": 1,
          "document_id": "1703.04617",
          "start_idx": 722,
          "end_idx": 1486
        },
        "page_content": "The recent availability of relatively large training datasets (see Section \"Related Work\" for more details) has made it more feasible to train and estimate rather complex models in an end-to-end fashion for these problems, in which a whole model is fit directly with given question-answer tuples and the resulting model has shown to be rather effective. In this paper, we take a closer look at modeling questions in such an end-to-end neural network framework, since we regard question understanding is of importance for such problems. We first introduced syntactic information to help encode questions. We then viewed and modelled different types of questions and the information shared among them as an adaptation problem and proposed adaptation models for them.",
        "type": "Document"
      },
      {
        "id": "9dc69e28-1cf2-4d3c-9f8f-18a6f0169737",
        "metadata": {
          "vector_store_key": "1708.05873-2",
          "chunk_id": 9,
          "document_id": "1708.05873",
          "start_idx": 5213,
          "end_idx": 5988
        },
        "page_content": "Following BIBREF8 we generate a set of candidate models ranging between 3 and 50 topics. We then plot the exclusivity and semantic coherence (numbers closer to 0 indicate higher coherence), with a linear regression overlaid (Figure FIGREF3 ). Models above the regression line have a \u201cbetter\u201d exclusivity-semantic coherence trade off. We select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence. The topic quality is usually evaluated by highest probability words, which is presented in Figure FIGREF4 . Figure FIGREF4 provides a list of the main topics (and the highest probability words associated these topics) that emerge from the STM of UN General Debate statements.",
        "type": "Document"
      },
      {
        "id": "df925d04-51e0-4d83-b972-800b057e5067",
        "metadata": {
          "vector_store_key": "1703.04617-3",
          "chunk_id": 42,
          "document_id": "1703.04617",
          "start_idx": 23295,
          "end_idx": 23973
        },
        "page_content": "When letting the number of hidden question types ( $K$ ) to be 20, the performance improves to 68.73%/77.74% on EM and F1, respectively, which corresponds to the results of our model reported in Table 1 . Furthermore, after submitted our result, we have experimented with a large value of $K$ and found that when $K=100$ , we can achieve a better performance of 69.10%/78.38% on the development set. Figure UID61 shows the EM/F1 scores of different question types while Figure UID62 is the question type amount distribution on the development set. In Figure UID61 we can see that the average EM/F1 of the \"when\" question is highest and those of the \"why\" question is the lowest.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance of proposed model on entire DROP dataset?",
    "llm_answer": "The proposed model slightly eclipses the current state-of-the-art results on the entire DROP dataset.\n",
    "context": [
      {
        "id": "335e3024-2ba3-4693-a41e-a4d434862b98",
        "metadata": {
          "vector_store_key": "1703.04617-3",
          "chunk_id": 40,
          "document_id": "1703.04617",
          "start_idx": 22297,
          "end_idx": 22900
        },
        "page_content": "We apply dropout to the Encoder layer and aggregation layer with a dropout rate of 0.5. Table 1 shows the official leaderboard on SQuAD test set when we submitted our system. Our model achieves a 68.73% EM score and 77.39% F1 score, which is ranked among the state of the art single models (without model ensembling). Table 2 shows the ablation performances of various Q-code on the development set. Note that since the testset is hidden from us, we can only perform such an analysis on the development set. Our baseline model using no Q-code achieved a 68.00% and 77.36% EM and F1 scores, respectively.",
        "type": "Document"
      },
      {
        "id": "1b1f9986-e4da-44a7-a6cd-13931a730da6",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 38,
          "document_id": "1909.13375",
          "start_idx": 19979,
          "end_idx": 20678
        },
        "page_content": "For the large models we used a batch size of 3 with a learning rate of $5e^{-6}$ and trained for 5 epochs, except for the model without the single-span heads that was trained with a batch size of 2 for 7 epochs. F1 was used as our validation metric. All models were trained on a single GPU with 12-16GB of memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions.",
        "type": "Document"
      },
      {
        "id": "95ac1b28-c666-4c44-bc76-e8f9512cf094",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 47,
          "document_id": "1909.13375",
          "start_idx": 25093,
          "end_idx": 25821
        },
        "page_content": "Furthermore, we show that our model slightly eclipses the current state-of-the-art results on the entire DROP dataeset. Finally, we present some ablation studies, analyzing the benefit gained from individual components of our model. We believe that combining our tag-based approach for handling multi-span questions with current successful techniques for handling single-span questions could prove beneficial in finding better, more holistic ways, of tackling span questions in general. Currently, For each individual span, we optimize the average likelihood over all its possible tag sequences (see Section SECREF9). A different approach could be not taking each possible tag sequence into account but only the most likely one.",
        "type": "Document"
      },
      {
        "id": "0fbb350a-16ee-4a8b-a81f-4f933b2dab44",
        "metadata": {
          "vector_store_key": "1703.04617-3",
          "chunk_id": 44,
          "document_id": "1703.04617",
          "start_idx": 24348,
          "end_idx": 24966
        },
        "page_content": "And the other part accounts for 30.90%, of which the average F1 score is 30.03%. For the latter, we can further divide it into two sub-parts: one is where the F1 score equals to 0%, which means that predict answer is totally wrong. This part occupies 14.89% of the total development set. The other part accounts for 16.01% of the development set, of which average F1 score is 57.96%. From this analysis we can see that reducing the zero F1 score (14.89%) is potentially an important direction to further improve the system. Closely modelling questions could be of importance for question answering and machine reading.",
        "type": "Document"
      },
      {
        "id": "93b8b741-b6d7-4597-9fce-2de82a1eb34e",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 42,
          "document_id": "1909.13375",
          "start_idx": 22170,
          "end_idx": 22785
        },
        "page_content": "Therefore, it could prove beneficial to try and analyze the reasons behind each model's (ours and MTMSN) relative advantages, and perhaps try to combine them into a more holistic approach of tackling span questions. Table TABREF25 shows the results on DROP's test set, with our model being the best overall as of the time of writing, and not just on multi-span questions. In order to analyze the effect of each of our changes, we conduct ablation studies on the development set, depicted in Table TABREF26. Not using the simple preprocessing from Section SECREF17 resulted in a 2.5 point decrease in both EM and F1.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How accurate is the aspect based sentiment classifier trained only using the XR loss?",
    "llm_answer": "The context states the classifier is *accurate*.\n",
    "context": [
      {
        "id": "0d542b81-5862-4e61-861a-5ac797df7675",
        "metadata": {
          "vector_store_key": "1909.00430-2",
          "chunk_id": 5,
          "document_id": "1909.00430",
          "start_idx": 2650,
          "end_idx": 3316
        },
        "page_content": "(Sections SECREF12 and SECREF4 ). In other words, given a medium-sized sentiment corpus with sentence-level labels, and a large collection of un-annotated text from the same distribution, we can train an accurate aspect-level sentiment classifier. The XR loss allows us to use task A labels for training task B predictors. This ability seamlessly integrates into other semi-supervised schemes: we can use the XR loss on top of a pre-trained model to fine-tune the pre-trained representation to the target task, and we can also take the model trained using XR loss and plentiful data and fine-tune it to the target task using the available small-scale annotated data.",
        "type": "Document"
      },
      {
        "id": "cea2947e-8dc4-4cb9-9a19-a573988a66ed",
        "metadata": {
          "vector_store_key": "1909.00430-2",
          "chunk_id": 10,
          "document_id": "1909.00430",
          "start_idx": 4913,
          "end_idx": 5579
        },
        "page_content": "Here, we suggest using XR to train a target task (aspect-level sentiment) based on the output of a related source-task classifier (sentence-level sentiment). The main idea of XR is moving from a fully supervised situation in which each data-point INLINEFORM0 has an associated label INLINEFORM1 , to a setup in which sets of data points INLINEFORM2 are associated with corresponding label proportions INLINEFORM3 over that set. Formally, let INLINEFORM0 be a set of data points, INLINEFORM1 be a set of INLINEFORM2 class labels, INLINEFORM3 be a set of sets where INLINEFORM4 for every INLINEFORM5 , and let INLINEFORM6 be the label distribution of set INLINEFORM7 .",
        "type": "Document"
      },
      {
        "id": "cdcd3aac-d158-4515-9f80-d2fd4547bc8e",
        "metadata": {
          "vector_store_key": "1909.00430-2",
          "chunk_id": 3,
          "document_id": "1909.00430",
          "start_idx": 1559,
          "end_idx": 2432
        },
        "page_content": "While this expectation may be noisy on the individual example level, it holds well in aggregate: given a set of positively-labeled sentences, we can robustly estimate the proportion of positively-labeled aspects within this set. For example, in a random set of positive sentences, we expect to find 90% positive aspects, while in a set of negative sentences, we expect to find 70% negative aspects. These proportions can be easily either guessed or estimated from a small set. We propose a novel application of the XR framework for transfer learning in this setup. We present an algorithm (Sec SECREF12 ) that, given a corpus labeled for task A (sentence-level sentiment), learns a classifier for performing task B (aspect-level sentiment) instead, without a direct supervision signal for task B. We note that the label information for task A is only used at training time.",
        "type": "Document"
      },
      {
        "id": "49b6a565-88a7-4254-b5fa-ac5c0e452d0b",
        "metadata": {
          "vector_store_key": "1909.00430-4",
          "chunk_id": 23,
          "document_id": "1909.00430",
          "start_idx": 12402,
          "end_idx": 13288
        },
        "page_content": "We demonstrate the procedure given above by training Aspect-based Sentiment Classifier (ABSC) using sentence-level sentiment signals. We observe that while the sentence-level sentiment does not determine the sentiment of individual aspects (a positive sentence may contain negative remarks about some aspects), it is very predictive of the proportion of sentiment labels of the fragments within a sentence. Positively labeled sentences are likely to have more positive aspects and fewer negative ones, and vice-versa for negatively-labeled sentences. While these proportions may vary on the individual sentence level, we expect them to be stable when aggregating fragments from several sentences: when considering a large enough sample of fragments that all come from positively labeled sentences, we expect the different samples to have roughly similar label proportions to each other.",
        "type": "Document"
      },
      {
        "id": "937480a2-7d31-473a-934a-18f666ee523a",
        "metadata": {
          "vector_store_key": "1909.00430-4",
          "chunk_id": 4,
          "document_id": "1909.00430",
          "start_idx": 1896,
          "end_idx": 2650
        },
        "page_content": "We present an algorithm (Sec SECREF12 ) that, given a corpus labeled for task A (sentence-level sentiment), learns a classifier for performing task B (aspect-level sentiment) instead, without a direct supervision signal for task B. We note that the label information for task A is only used at training time. Furthermore, due to the stochastic nature of the estimation, the task A labels need not be fully accurate, allowing us to make use of noisy predictions which are assigned by an automatic classifier (Sections SECREF12 and SECREF4 ). In other words, given a medium-sized sentiment corpus with sentence-level labels, and a large collection of un-annotated text from the same distribution, we can train an accurate aspect-level sentiment classifier.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What were the non-neural baselines used for the task?",
    "llm_answer": "TF-IDF and LDA.\n",
    "context": [
      {
        "id": "96576214-2c10-4049-ba11-db649d59677b",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 26,
          "document_id": "1809.04960",
          "start_idx": 15085,
          "end_idx": 15736
        },
        "page_content": "Unsupervised baseline models are as follows: TF-IDF (Lexical, Non-Neural) is an important unsupervised baseline. We use the concatenation of the title and the body as the query to retrieve the candidate comment set by means of the similarity of the tf-idf value. The model is trained on unpaired articles and comments, which is the same as our proposed model. LDA (Topic, Non-Neural) is a popular unsupervised topic model, which discovers the abstract \"topics\" that occur in a collection of documents. We train the LDA with the articles and comments in the training set. The model retrieves the comments by the similarity of the topic representations.",
        "type": "Document"
      },
      {
        "id": "beee7b9e-00bc-4ef2-8232-722a694c8ed8",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 28,
          "document_id": "1909.00542",
          "start_idx": 15261,
          "end_idx": 15995
        },
        "page_content": "Therefore we do not comment on the results, other than observing that the \u201cfirst $n$\u201d baseline produces the same results as the neural regressor. As mentioned in Section SECREF3, the labels used for the classification experiments are the 5 sentences with highest ROUGE-SU4 F1 score. Macquarie University's participation in BioASQ 7 focused on the task of generating the ideal answers. The runs use query-based extractive techniques and we experiment with classification, regression, and reinforcement learning approaches. At the time of writing there were no human evaluation results, and based on ROUGE-F1 scores under cross-validation on the training data we observed that classification approaches outperform regression approaches.",
        "type": "Document"
      },
      {
        "id": "ddbf653d-2b06-40f5-b102-1128cf41b68a",
        "metadata": {
          "vector_store_key": "1707.03904-0",
          "chunk_id": 31,
          "document_id": "1707.03904",
          "start_idx": 17195,
          "end_idx": 17895
        },
        "page_content": "For Quasar-T we also test the Sliding Window (SW) and Sliding Window + Distance (SW+D) baselines proposed in BIBREF13 . The scores were computed for the list of candidate solutions described in Section \"Context Retrieval\" . For Quasar-S, since the answers come from a fixed vocabulary of entities, we test language model baselines which predict the most likely entity to appear in a given context. We train three n-gram baselines using the SRILM toolkit BIBREF21 for $n=3,4,5$ on the entire corpus of all Stack Overflow posts. The output predictions are restricted to the output vocabulary of entities. We also train a bidirectional Recurrent Neural Network (RNN) language model (based on GRU units).",
        "type": "Document"
      },
      {
        "id": "ab539416-9e33-43d1-aa4a-6abf50da70a2",
        "metadata": {
          "vector_store_key": "1807.03367-8",
          "chunk_id": 75,
          "document_id": "1807.03367",
          "start_idx": 43287,
          "end_idx": 44126
        },
        "page_content": "We train for 100 epochs and perform early stopping on the validation loss. The F1 scores for the described methods in Table TABREF65 . We compare to an \u201call positive\u201d baseline that always predicts that the landmark class is visible and observe that all presented models struggle to outperform this baseline. Although 256-dimensional ResNet features achieve slightly better precision on the validation set, it results in much worse recall and a lower F1 score. Our results indicate that perceptual grounding is a difficult task, which easily merits a paper of its own right, and so we leave further improvements (e.g. better text recognizers) for future work. paragraph4 0.1ex plus0.1ex minus.1ex-1em Dataset split We split the full dataset by assigning entire 4x4 grids (independent of the target location) to the train, valid or test set.",
        "type": "Document"
      },
      {
        "id": "77c46ff8-88d4-487b-8e10-029933a047c0",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 27,
          "document_id": "1909.00542",
          "start_idx": 14823,
          "end_idx": 15526
        },
        "page_content": "Fig. FIGREF40 shows the scatterplots of ROUGE-SU4 recall, precision and F1 with respect to the average human evaluation. We observe that the relation between ROUGE and the human evaluations is not linear, and that Precision and F1 have a clear correlation. Table TABREF41 shows the results and details of the runs submitted to BioASQ. The table uses ROUGE-SU4 Recall since this is the metric available at the time of writing this paper. However, note that, as explained in Section SECREF6, these results might differ from the final human evaluation results. Therefore we do not comment on the results, other than observing that the \u201cfirst $n$\u201d baseline produces the same results as the neural regressor.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the models evaluated on?",
    "llm_answer": "UN General Debate statements;  \"what\", \"how\", \"who\", \"when\", \"which\", \"where\", \"why\", \"be\", \"whose\", and \"whom\" questions.  Also, a test set (for QRNNs, first 350 words; for KN-5, entire test set).  A development set.\n",
    "context": [
      {
        "id": "9dc69e28-1cf2-4d3c-9f8f-18a6f0169737",
        "metadata": {
          "vector_store_key": "1708.05873-2",
          "chunk_id": 9,
          "document_id": "1708.05873",
          "start_idx": 5213,
          "end_idx": 5988
        },
        "page_content": "Following BIBREF8 we generate a set of candidate models ranging between 3 and 50 topics. We then plot the exclusivity and semantic coherence (numbers closer to 0 indicate higher coherence), with a linear regression overlaid (Figure FIGREF3 ). Models above the regression line have a \u201cbetter\u201d exclusivity-semantic coherence trade off. We select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence. The topic quality is usually evaluated by highest probability words, which is presented in Figure FIGREF4 . Figure FIGREF4 provides a list of the main topics (and the highest probability words associated these topics) that emerge from the STM of UN General Debate statements.",
        "type": "Document"
      },
      {
        "id": "0ab61107-67bf-493c-aa5f-199d370caaa3",
        "metadata": {
          "vector_store_key": "1703.04617-2",
          "chunk_id": 26,
          "document_id": "1703.04617",
          "start_idx": 14460,
          "end_idx": 15206
        },
        "page_content": "The previous models are often trained for all questions without explicitly discriminating different question types; however, for a target question, both the common features shared by all questions and the specific features for a specific type of question are further considered in this paper, as they could potentially obey different distributions. In this paper we further explicitly model different types of questions in the end-to-end training. We start from a simple way to first analyze the word frequency of all questions, and obtain top-10 most frequent question types: what, how, who, when, which, where, why, be, whose, and whom, in which be stands for the questions beginning with different forms of the word be such as is, am, and are.",
        "type": "Document"
      },
      {
        "id": "abde311f-db3e-44fd-96aa-882a0a2aeeb0",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 17,
          "document_id": "1912.13109",
          "start_idx": 8552,
          "end_idx": 9307
        },
        "page_content": "These processes and its implementation details will be discussed later but they were learning rate decay, early stopping, model checkpointing and reducing learning rate on plateau. For the loss function we chose categorical cross entropy loss in finding the most optimal weights/parameters of the model. Formally this loss function for the model is defined as below: The double sum is over the number of observations and the categories respectively. While the model probability is the probability that the observation i belongs to category c. Among the model architectures we experimented with and without data augmentation were: Fully Connected dense networks: Model hyperparameters were inspired from the previous work done by Vo et al and Mathur et al.",
        "type": "Document"
      },
      {
        "id": "f3776613-5f75-4322-8d4e-69a0ede7f7df",
        "metadata": {
          "vector_store_key": "1811.00942-0",
          "chunk_id": 16,
          "document_id": "1811.00942",
          "start_idx": 9219,
          "end_idx": 9822
        },
        "page_content": "For the QRNNs, we used the first 350 words of the test set, and averaged the ms/query and mJ/query. For KN-5, we used the entire test set for evaluation, since the latency was much lower. To adjust for the base power load, we subtracted idle power draw from energy usage. For a different perspective, we further evaluated all the models under a desktop environment, using an i7-4790k CPU and Titan V GPU. Because the base power load for powering a desktop is much higher than running neural language models, we collected only latency statistics. We used the entire test set, since the QRNN runs quickly.",
        "type": "Document"
      },
      {
        "id": "5567ec52-3c38-41b4-b624-2bfe13262a85",
        "metadata": {
          "vector_store_key": "1703.04617-3",
          "chunk_id": 43,
          "document_id": "1703.04617",
          "start_idx": 23973,
          "end_idx": 24558
        },
        "page_content": "In Figure UID61 we can see that the average EM/F1 of the \"when\" question is highest and those of the \"why\" question is the lowest. From Figure UID62 we can see the \"what\" question is the major class. Figure 5 shows the composition of F1 score. Take our best model as an example, we observed a 78.38% F1 score on the whole development set, which can be separated into two parts: one is where F1 score equals to 100%, which means an exact match. This part accounts for 69.10% of the entire development set. And the other part accounts for 30.90%, of which the average F1 score is 30.03%.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the results of multimodal compared to unimodal models?",
    "llm_answer": "The multimodal models (like TKM) end up using almost only the text input and produce results similar to textual models.\n",
    "context": [
      {
        "id": "56f56118-ed4f-4879-94fa-1f8ee08cd4d8",
        "metadata": {
          "vector_store_key": "1910.03814-3",
          "chunk_id": 40,
          "document_id": "1910.03814",
          "start_idx": 21721,
          "end_idx": 22352
        },
        "page_content": "Besides the different architectures, we have tried different training strategies, such as initializing the CNN weights with a model already trained solely with MMHS150K images or using dropout to force the multimodal models to use the visual information. Eventually, though, these models end up using almost only the text input for the prediction and producing very similar results to those of the textual models. The proposed multimodal models, such as TKM, have shown good performance in other tasks, such as VQA. Next, we analyze why they do not perform well in this task and with this data: [noitemsep,leftmargin=*] Noisy data.",
        "type": "Document"
      },
      {
        "id": "1858edb9-546d-4edc-9fc6-41fbc071abbd",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 39,
          "document_id": "1909.13375",
          "start_idx": 20678,
          "end_idx": 21407
        },
        "page_content": "The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could be found in Section (SECREF36). Notice that different BERTlarge models were used, so the comparison is less direct. Overall, our large models exhibits similar results to those of MTMSNlarge. For multi-span questions we achieve a significantly better performance. While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1.",
        "type": "Document"
      },
      {
        "id": "e81fee71-223a-449b-b9b3-75b7e7e7caec",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 41,
          "document_id": "1909.13375",
          "start_idx": 21792,
          "end_idx": 22634
        },
        "page_content": "When comparing single-span performance, our best model exhibits slightly better results, but it should be noted that it retains the single-span heads from NABERT+, while in MTMSN they have one head to predict both single-span and multi-span answers. For a fairer comparison, we trained our model with the single-span heads removed, where our multi-span head remained the only head aimed for handling span questions. With this no-single-span-heads setting, while our multi-span performance even improved a bit, our single-span performance suffered a slight drop, ending up trailing by 0.8 in EM and 0.6 in F1 compared to MTMSN. Therefore, it could prove beneficial to try and analyze the reasons behind each model's (ours and MTMSN) relative advantages, and perhaps try to combine them into a more holistic approach of tackling span questions.",
        "type": "Document"
      },
      {
        "id": "2e7cfe41-b41c-475c-83f6-e7d39903180d",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 40,
          "document_id": "1909.13375",
          "start_idx": 21052,
          "end_idx": 21792
        },
        "page_content": "While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1. All that, while keeping in mind we compare a base model to a large model (for reference, note the 8 point improvement between MTMSNbase and MTMSNlarge in both EM and F1). Our best model, large-squad, exhibits a huge improvement of 29.7 in EM and 15.1 in F1 compared to MTMSNlarge. When comparing single-span performance, our best model exhibits slightly better results, but it should be noted that it retains the single-span heads from NABERT+, while in MTMSN they have one head to predict both single-span and multi-span answers.",
        "type": "Document"
      },
      {
        "id": "93b8b741-b6d7-4597-9fce-2de82a1eb34e",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 42,
          "document_id": "1909.13375",
          "start_idx": 22170,
          "end_idx": 22785
        },
        "page_content": "Therefore, it could prove beneficial to try and analyze the reasons behind each model's (ours and MTMSN) relative advantages, and perhaps try to combine them into a more holistic approach of tackling span questions. Table TABREF25 shows the results on DROP's test set, with our model being the best overall as of the time of writing, and not just on multi-span questions. In order to analyze the effect of each of our changes, we conduct ablation studies on the development set, depicted in Table TABREF26. Not using the simple preprocessing from Section SECREF17 resulted in a 2.5 point decrease in both EM and F1.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What were their performance results?",
    "llm_answer": "The models achieved a 12.2 absolute improvement in EM and a 2.3 improvement in F1 compared to the base model.  Large-squad showed a 29.7 EM and 15.1 F1 improvement compared to MTMSNlarge.  A simple model trained in Experiment 1 had an average F$_1$ of 48%.  BERT achieved a 0.604 micro-F1 score in benchmark tests.\n",
    "context": [
      {
        "id": "2e7cfe41-b41c-475c-83f6-e7d39903180d",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 40,
          "document_id": "1909.13375",
          "start_idx": 21052,
          "end_idx": 21792
        },
        "page_content": "While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1. All that, while keeping in mind we compare a base model to a large model (for reference, note the 8 point improvement between MTMSNbase and MTMSNlarge in both EM and F1). Our best model, large-squad, exhibits a huge improvement of 29.7 in EM and 15.1 in F1 compared to MTMSNlarge. When comparing single-span performance, our best model exhibits slightly better results, but it should be noted that it retains the single-span heads from NABERT+, while in MTMSN they have one head to predict both single-span and multi-span answers.",
        "type": "Document"
      },
      {
        "id": "8b681439-d4eb-401b-8199-4cc0d2c2d0ed",
        "metadata": {
          "vector_store_key": "1909.02764-4",
          "chunk_id": 37,
          "document_id": "1909.02764",
          "start_idx": 20568,
          "end_idx": 21193
        },
        "page_content": "This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set. These results are mostly in line with Bostan2018. Now we analyze how well the models trained in Experiment 1 perform when applied to our data set. The results are shown in column \u201cSimple\u201d in Table TABREF19. We observe a clear drop in performance, with an average of F$_1$=48 %. The best performing model is again the one trained on TEC, en par with the one trained on the Figure8 data. The model trained on ISEAR performs second best in Experiment 1, it performs worst in Experiment 2.",
        "type": "Document"
      },
      {
        "id": "1bffa3f8-bbbc-48c1-81fe-55fb01d9b256",
        "metadata": {
          "vector_store_key": "1910.11769-3",
          "chunk_id": 12,
          "document_id": "1910.11769",
          "start_idx": 7313,
          "end_idx": 8027
        },
        "page_content": "We masked out named entities with entity-type specific placeholders to reduce the chance of benchmark models utilizing named entities as a basis for classification. Benchmark results are shown in Table TABREF17. The dataset is approximately balanced after discarding the Surprise and Disgust classes. We report the average micro-F1 scores, with 5-fold cross validation for each technique. We provide a brief overview of each benchmark experiment below. Among all of the benchmarks, Bidirectional Encoder Representations from Transformers (BERT) BIBREF11 achieved the best performance with a 0.604 micro-F1 score. Overall, we observed that deep-learning based techniques performed better than lexical based methods.",
        "type": "Document"
      },
      {
        "id": "36adf7ba-6ece-4ad5-84b3-0a38b137a4fc",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 16,
          "document_id": "1912.13109",
          "start_idx": 8070,
          "end_idx": 8831
        },
        "page_content": "The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we employed various operational techniques for optimizing on the learning process. These processes and its implementation details will be discussed later but they were learning rate decay, early stopping, model checkpointing and reducing learning rate on plateau.",
        "type": "Document"
      },
      {
        "id": "1b1f9986-e4da-44a7-a6cd-13931a730da6",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 38,
          "document_id": "1909.13375",
          "start_idx": 19979,
          "end_idx": 20678
        },
        "page_content": "For the large models we used a batch size of 3 with a learning rate of $5e^{-6}$ and trained for 5 epochs, except for the model without the single-span heads that was trained with a batch size of 2 for 7 epochs. F1 was used as our validation metric. All models were trained on a single GPU with 12-16GB of memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much did they outperform the other methods?",
    "llm_answer": "The provided text states that proposed methods outperformed baselines for all metrics, but does not give specific performance differences.\n",
    "context": [
      {
        "id": "92dd0ad6-8e89-4d29-9317-3cf4eca34bfd",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 37,
          "document_id": "1909.13375",
          "start_idx": 19698,
          "end_idx": 20343
        },
        "page_content": "We preferred to focus our efforts on the ablation studies, in hope to gain further insights on the effect of the components that we ourselves introduced. For ease of performance comparison, we followed NABERT+'s training settings: we used the BERT Adam optimizer from huggingface with default settings and a learning rate of $1e^{-5}$. The only difference was that we used a batch size of 12. We trained our base model for 20 epochs. For the large models we used a batch size of 3 with a learning rate of $5e^{-6}$ and trained for 5 epochs, except for the model without the single-span heads that was trained with a batch size of 2 for 7 epochs.",
        "type": "Document"
      },
      {
        "id": "b2f8b59b-1092-4698-9fa3-c0a21594d3ff",
        "metadata": {
          "vector_store_key": "2003.07433-4",
          "chunk_id": 47,
          "document_id": "2003.07433",
          "start_idx": 25723,
          "end_idx": 26401
        },
        "page_content": "To compare the outperformance of our method, we also implemented Coppersmith et. al. proposed method and achieved an 86% overall accuracy of detecting PTSD users BIBREF11 following the same training-test dataset distribution. Fig FIGREF28 illustrates the comparisons between LAXARY and Coppersmith et. al. proposed method. Here we can see, the outperformance of our proposed method as well as the importance of $s-score$ estimation. We also illustrates the importance of $\\alpha -score$ and $S-score$ in Fig FIGREF30. Fig FIGREF30 illustrates that if we change the number of training samples (%), LAXARY models outperforms Coppersmith et. al. proposed model under any condition.",
        "type": "Document"
      },
      {
        "id": "814822da-592c-40ae-bb7c-e78761735daf",
        "metadata": {
          "vector_store_key": "2002.06644-3",
          "chunk_id": 10,
          "document_id": "2002.06644",
          "start_idx": 6316,
          "end_idx": 7055
        },
        "page_content": "Our proposed methodology, the use of finetuned optimized BERT based models, and BERT-based ensemble models outperform the baselines for all the metrics. Among the optimized BERT based models, $RoBERTa_{large}$ outperforms all other non-ensemble models and the baselines for all metrics. It further achieves a maximum recall of $0.681$ for all the proposed models. We note that DistillRoBERTa, a distilled model, performs competitively, achieving $69.69\\%$ accuracy, and $0.672$ F1 score. This observation shows that distilled pretrained models can replace their undistilled counterparts in a low-computing environment. We further observe that ensemble models perform better than optimized BERT-based models and distilled pretrained models.",
        "type": "Document"
      },
      {
        "id": "217a52b3-a2f4-44e6-b832-5071f6d586af",
        "metadata": {
          "vector_store_key": "1910.03891-8",
          "chunk_id": 44,
          "document_id": "1910.03891",
          "start_idx": 25078,
          "end_idx": 25853
        },
        "page_content": "Experimental results of entity classification on the test sets of all the datasets is shown in Table TABREF25. The results is clearly demonstrate that our proposed method significantly outperforms state-of-art results on accuracy for three datasets. For more in-depth performance analysis, we note: (1) Among all baselines, Path-based methods and Attribute-incorporated methods outperform three typical methods. This indicates that incorporating extra information can improve the knowledge graph embedding performance; (2) Four variants of KANE always outperform baseline methods. The main reasons why KANE works well are two fold: 1) KANE can capture high-order structural information of KGs in an efficient, explicit manner and passe these information to their neighboring;",
        "type": "Document"
      },
      {
        "id": "9e9dd30e-b89c-4262-8802-6205bee46ca9",
        "metadata": {
          "vector_store_key": "1909.00578-1",
          "chunk_id": 17,
          "document_id": "1909.00578",
          "start_idx": 9107,
          "end_idx": 9819
        },
        "page_content": "The three fine-tuned BERT versions clearly outperform all other methods. Multi-task versions seem to perform better than single-task ones in most cases. Especially for $\\mathcal {Q}4$ and $\\mathcal {Q}5$, which are highly correlated, the multi-task BERT versions achieve the best overall results. BiGRU-ATT also benefits from multi-task learning. The correlation of Sum-QE with human judgments is high or very high BIBREF23 for all $\\mathcal {Q}$s in all datasets, apart from $\\mathcal {Q}2$ in DUC-05 where it is only moderate. Manual scores for $\\mathcal {Q}2$ in DUC-05 are the highest among all $\\mathcal {Q}$s and years (between 4 and 5) and with the smallest standard deviation, as shown in Table TABREF24.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the state of the art?",
    "llm_answer": "The state-of-the-art baselines achieve 33.6% and 28.5% accuracy on Quasar-S and Quasar-T, respectively.  Human performance is 50% and 60.6% on the same tasks.  Deep learning models, particularly ensembles of convolutional neural networks, are a strong trend in image and video recognition.  In the automotive domain, FACS (Facial Action Coding System) using support vector machines is still popular.  Word embedding methods using hash tokens are also being explored to address out-of-vocabulary issues.\n",
    "context": [
      {
        "id": "3f038778-c195-45ae-aab6-6a91ddb132bd",
        "metadata": {
          "vector_store_key": "1909.02764-0",
          "chunk_id": 7,
          "document_id": "1909.02764",
          "start_idx": 4078,
          "end_idx": 4875
        },
        "page_content": "As the reliability and reproducability of findings with this method have been critically discussed BIBREF12, the trend has increasingly shifted to perform the recognition directly on images and videos, especially with deep learning. For instance, jung2015joint developed a model which considers temporal geometry features and temporal appearance features from image sequences. kim2016hierarchical propose an ensemble of convolutional neural networks which outperforms isolated networks. In the automotive domain, FACS is still popular. Ma2017 use support vector machines to distinguish happy, bothered, confused, and concentrated based on data from a natural driving environment. They found that bothered and confused are difficult to distinguish, while happy and concentrated are well identified.",
        "type": "Document"
      },
      {
        "id": "3e561a91-d33e-4edf-bb45-28350d16064c",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 26,
          "document_id": "2001.00137",
          "start_idx": 15470,
          "end_idx": 16181
        },
        "page_content": "We focus on the three following services, where the first two are commercial services and last one is open source with two separate backends: Google Dialogflow (formerly Api.ai) , SAP Conversational AI (formerly Recast.ai) and Rasa (spacy and tensorflow backend) . Shridhar et al. BIBREF12 proposed a word embedding method that doesn't suffer from out-of-vocabulary issues. The authors achieve this by using hash tokens in the alphabet instead of a single word, making it vocabulary independent. For classification, classifiers such as Multilayer Perceptron (MLP), Support Vector Machine (SVM) and Random Forest are used. A complete list of classifiers and training specifications are given in Section SECREF31.",
        "type": "Document"
      },
      {
        "id": "588b4926-f483-4fe2-b503-5460cc39c40f",
        "metadata": {
          "vector_store_key": "1912.13109-2",
          "chunk_id": 28,
          "document_id": "1912.13109",
          "start_idx": 14578,
          "end_idx": 15657
        },
        "page_content": "Pennington, Jeffrey and Socher, Richard and Manning, Christopher Glove: Global vectors for word representation Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) [7] Zhang, Lei and Wang, Shuai and Liu, Bing Deep learning for sentiment analysis: A survey Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery [8] Caruana, Rich and Lawrence, Steve and Giles, C Lee Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping Advances in neural information processing systems [9] Beale, Mark Hudson and Hagan, Martin T and Demuth, Howard B Neural network toolbox user\u2019s guide The MathWorks Incs [10] Chollet, Fran\u00e7ois and others Keras: The python deep learning library Astrophysics Source Code Library [11] Wei, Jason and Zou, Kai EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
        "type": "Document"
      },
      {
        "id": "3699d425-11f4-45cc-aae5-6a93d3e32b86",
        "metadata": {
          "vector_store_key": "1707.03904-0",
          "chunk_id": 8,
          "document_id": "1707.03904",
          "start_idx": 4627,
          "end_idx": 5317
        },
        "page_content": "We evaluate Quasar against human testers, as well as several baselines ranging from na\u00efve heuristics to state-of-the-art machine readers. The best performing baselines achieve $33.6\\%$ and $28.5\\%$ on Quasar-S and Quasar-T, while human performance is $50\\%$ and $60.6\\%$ respectively. For the automatic systems, we see an interesting tension between searching and reading accuracies \u2013 retrieving more documents in the search phase leads to a higher coverage of answers, but makes the comprehension task more difficult. We also collect annotations on a subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short.",
        "type": "Document"
      },
      {
        "id": "80658bbd-27df-43c4-9667-7d716f163cd2",
        "metadata": {
          "vector_store_key": "1912.13109-2",
          "chunk_id": 29,
          "document_id": "1912.13109",
          "start_idx": 14579,
          "end_idx": 15448
        },
        "page_content": "Zhang, Lei and Wang, Shuai and Liu, Bing Deep learning for sentiment analysis: A survey Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery [8] Caruana, Rich and Lawrence, Steve and Giles, C Lee Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping Advances in neural information processing systems [9] Beale, Mark Hudson and Hagan, Martin T and Demuth, Howard B Neural network toolbox user\u2019s guide The MathWorks Incs [10] Chollet, Fran\u00e7ois and others Keras: The python deep learning library Astrophysics Source Code Library [11] Wei, Jason and Zou, Kai EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What difficulties does sentiment analysis on Twitter have, compared to sentiment analysis in other domains?",
    "llm_answer": "The context states that Twitter messages are generally short, often length-limited, and that this presents a challenge requiring specialized research.\n",
    "context": [
      {
        "id": "70fcc4ad-1cdc-4b87-aed2-ddad01a87c7e",
        "metadata": {
          "vector_store_key": "1710.01492-0",
          "chunk_id": 6,
          "document_id": "1710.01492",
          "start_idx": 3135,
          "end_idx": 3999
        },
        "page_content": "Sentiment analysis has a wide number of applications in areas such as market research, political and social sciences, and for studying public opinion in general, and Twitter is one of the most commonly-used platforms for this. This is due to its streaming nature, which allows for real-time analysis, to its social aspect, which encourages people to share opinions, and to the short size of the tweets, which simplifies linguistic analysis. There are several formulations of the task of Sentiment Analysis on Twitter that look at different sizes of the target (e.g., at the level of words vs. phrases vs. tweets vs. sets of tweets), at different types of semantic targets (e.g., aspect vs. topic vs. overall tweet), at the explicitness of the target (e.g., sentiment vs. stance detection), at the scale of the expected label (2-point vs. 3-point vs. ordinal), etc.",
        "type": "Document"
      },
      {
        "id": "18b52094-a986-4981-81e5-40f449faef71",
        "metadata": {
          "vector_store_key": "1710.01492-0",
          "chunk_id": 5,
          "document_id": "1710.01492",
          "start_idx": 2968,
          "end_idx": 3584
        },
        "page_content": "In addition to the genre difference, there is also a difference in length: social media messages are generally short, often length-limited by design as in Twitter, i.e., a sentence or a headline rather than a full document. How to handle such challenges has only recently been the subject of thorough research BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 . Sentiment analysis has a wide number of applications in areas such as market research, political and social sciences, and for studying public opinion in general, and Twitter is one of the most commonly-used platforms for this.",
        "type": "Document"
      },
      {
        "id": "1f3d1416-598b-46d1-872d-6bc761e12532",
        "metadata": {
          "vector_store_key": "1710.01492-0",
          "chunk_id": 0,
          "document_id": "1710.01492",
          "start_idx": 0,
          "end_idx": 661
        },
        "page_content": "Microblog sentiment analysis; Twitter opinion mining Sentiment Analysis: This is text analysis aiming to determine the attitude of a speaker or a writer with respect to some topic or the overall contextual polarity of a piece of text. Sentiment analysis on Twitter is the use of natural language processing techniques to identify and categorize opinions expressed in a tweet, in order to determine the author's attitude toward a particular topic or in general. Typically, discrete labels such as positive, negative, neutral, and objective are used for this purpose, but it is also possible to use labels on an ordinal scale, or even continuous numerical values.",
        "type": "Document"
      },
      {
        "id": "0764482d-6fd8-421b-93ca-c7a362ccc644",
        "metadata": {
          "vector_store_key": "1710.01492-0",
          "chunk_id": 51,
          "document_id": "1710.01492",
          "start_idx": 27591,
          "end_idx": 28273
        },
        "page_content": "Sentiment analysis on Twitter has applications in a number of areas, including political science BIBREF39 , BIBREF40 , BIBREF41 , economics BIBREF42 , BIBREF7 , social science BIBREF43 , and market research BIBREF44 , BIBREF45 . It is used to study company reputation online BIBREF45 , to measure customer satisfaction, to identify detractors and promoters, to forecast market growth BIBREF42 , to predict the future income from newly-released movies, to forecast the outcome of upcoming elections BIBREF41 , BIBREF7 , to study political polarization BIBREF39 , BIBREF9 , etc. We expect the quest for more interesting formulations of the general sentiment analysis task to continue.",
        "type": "Document"
      },
      {
        "id": "a3b68d8c-a767-4a3d-a7a7-78fc02f17cab",
        "metadata": {
          "vector_store_key": "1710.01492-0",
          "chunk_id": 7,
          "document_id": "1710.01492",
          "start_idx": 3999,
          "end_idx": 4677
        },
        "page_content": "There are several formulations of the task of Sentiment Analysis on Twitter that look at different sizes of the target (e.g., at the level of words vs. phrases vs. tweets vs. sets of tweets), at different types of semantic targets (e.g., aspect vs. topic vs. overall tweet), at the explicitness of the target (e.g., sentiment vs. stance detection), at the scale of the expected label (2-point vs. 3-point vs. ordinal), etc. All these are explored at SemEval, the International Workshop on Semantic Evaluation, which has created a number of benchmark datasets and has enabled direct comparison between different systems and approaches, both as part of the competition and beyond.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many sentence transformations on average are available per unique sentence in dataset?",
    "llm_answer": "15\n",
    "context": [
      {
        "id": "99ad02db-5ec2-4581-9917-d2c099b5f1b3",
        "metadata": {
          "vector_store_key": "1912.01673-1",
          "chunk_id": 22,
          "document_id": "1912.01673",
          "start_idx": 12632,
          "end_idx": 13376
        },
        "page_content": "The 293 annotations are split into groups depending on how many annotators saw the same input sentence: 30 annotations were annotated by one person only, 30 annotations by two different persons etc. The last column shows the number of unique outputs obtained in that group. Across all cases, 96.8% of produced strings were unique. In line with instructions, the annotators were using the IMPOSSIBLE option scarcely (95 times, i.e. only 2%). It was also a case of 7 annotators only; the remaining 5 annotators were capable of producing all requested transformations. The top three transformations considered unfeasible were different meaning (using the same set of words), past (esp. for sentences already in the past tense) and simple sentence.",
        "type": "Document"
      },
      {
        "id": "7e41f955-4622-4d3a-b465-fd3d8521a43d",
        "metadata": {
          "vector_store_key": "1912.01673-1",
          "chunk_id": 21,
          "document_id": "1912.01673",
          "start_idx": 12287,
          "end_idx": 13032
        },
        "page_content": "The time needed to carry out one piece of annotation (i.e. to provide one seed sentence with all 15 transformations) was on average almost 20 minutes but some annotators easily needed even half an hour. Out of the 4262 distinct sentences, only 188 was recorded more than once. In other words, the chance of two annotators producing the same output string is quite low. The most repeated transformations are by far past, future and ban. The least repeated is paraphrase with only single one repeated. multiple-annots documents this in another way. The 293 annotations are split into groups depending on how many annotators saw the same input sentence: 30 annotations were annotated by one person only, 30 annotations by two different persons etc.",
        "type": "Document"
      },
      {
        "id": "c1baf3a0-2fb1-4e4a-8c3e-f052a1ac62cf",
        "metadata": {
          "vector_store_key": "1909.12140-0",
          "chunk_id": 8,
          "document_id": "1909.12140",
          "start_idx": 4575,
          "end_idx": 5334
        },
        "page_content": "The result of the transformation process is either written to the console or stored in a specified output file in JSON format. We also provide a browser-based user interface, where the user can directly type in sentences to be processed (see Figure FIGREF1). For the English version, we performed both a thorough manual analysis and automatic evaluation across three commonly used TS datasets from two different domains in order to assess the performance of our framework with regard to the sentence splitting subtask. The results show that our proposed sentence splitting approach outperforms the state of the art in structural TS, returning fine-grained simplified sentences that achieve a high level of grammaticality and preserve the meaning of the input.",
        "type": "Document"
      },
      {
        "id": "2a15703d-2be0-4fe2-8dd9-cbb9caa315b3",
        "metadata": {
          "vector_store_key": "1912.01673-1",
          "chunk_id": 20,
          "document_id": "1912.01673",
          "start_idx": 11437,
          "end_idx": 12287
        },
        "page_content": "Unfortunately, a single typo can significantly influence the resulting embedding BIBREF26. After collecting all the sentence variations, we applied the statistical spellchecker and grammar checker Korektor BIBREF27 in order to minimize influence of typos to performance of embedding methods. We manually inspected 519 errors identified by Korektor and fixed 129, which were identified correctly. In the second round, we collected 293 annotations from 12 annotators. After Korektor, there are 4262 unique sentences (including 150 seed sentences) that form the COSTRA 1.0 dataset. Statistics of individual annotators are available in tab:statistics. The time needed to carry out one piece of annotation (i.e. to provide one seed sentence with all 15 transformations) was on average almost 20 minutes but some annotators easily needed even half an hour.",
        "type": "Document"
      },
      {
        "id": "088347bc-6391-466f-81eb-e37d14e108ba",
        "metadata": {
          "vector_store_key": "1705.09665-6",
          "chunk_id": 23,
          "document_id": "1705.09665",
          "start_idx": 13165,
          "end_idx": 13951
        },
        "page_content": "Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities ( INLINEFORM0 ), for a total of 4,872 community-months ( INLINEFORM1 ). Estimating linguistic measures. We estimate word frequencies INLINEFORM0 , and by extension each downstream measure, in a carefully controlled manner in order to ensure we capture robust and meaningful linguistic behaviour.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What annotations are available in the dataset?",
    "llm_answer": "Dialogue acts and states (Joy, Sadness, Anger, Fear, Anticipation, Surprise, Love, Disgust, Neutral);  MissScrEv/MissScrPart (missing script event/participant); coreference information for noun phrases; question text, context document, candidate solutions, and correct solution.\n",
    "context": [
      {
        "id": "24f8dfec-c722-47c3-89b3-12384c534e78",
        "metadata": {
          "vector_store_key": "2002.11893-9",
          "chunk_id": 14,
          "document_id": "2002.11893",
          "start_idx": 8370,
          "end_idx": 9067
        },
        "page_content": "Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states. Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three experts were employed to manually annotate dialogue acts and states for 50 dialogues. The results show that our annotations are of high quality.",
        "type": "Document"
      },
      {
        "id": "35f1bb75-de90-4ab3-bf26-255e60b9b505",
        "metadata": {
          "vector_store_key": "1910.11769-0",
          "chunk_id": 5,
          "document_id": "1910.11769",
          "start_idx": 3245,
          "end_idx": 3978
        },
        "page_content": "Hence, we modified our annotation scheme by removing Trust and adding Love. We also added the Neutral category to denote passages that do not exhibit any emotional content. The final annotation categories for the dataset are: Joy, Sadness, Anger, Fear, Anticipation, Surprise, Love, Disgust, Neutral. We selected both classic and modern narratives in English for this dataset. The modern narratives were sampled based on popularity from Wattpad. We parsed selected narratives into passages, where a passage is considered to be eligible for annotation if it contained between 40 and 200 tokens. In long-form narratives, many non-conversational passages are intended for transition or scene introduction, and may not carry any emotion.",
        "type": "Document"
      },
      {
        "id": "68e72fd8-5862-4eb1-8a6c-dcbda9f8ca12",
        "metadata": {
          "vector_store_key": "1703.05260-1",
          "chunk_id": 22,
          "document_id": "1703.05260",
          "start_idx": 12228,
          "end_idx": 12901
        },
        "page_content": "In the first annotation phase, annotators were asked to mark verbs and noun phrases that have an event or participant type, that is not listed in the template, as MissScrEv/ MissScrPart (missing script event or participant, resp.). These annotations were used as a basis for extending the templates (see Section \"Modification of the Schema\" ) and replaced later by newly introduced labels or ScrEv_other and ScrPart_other respectively. All noun phrases were annotated with coreference information indicating which entities denote the same discourse referent. The annotation was done by linking heads of NPs (see Example UID21 , where the links are indicated by coindexing).",
        "type": "Document"
      },
      {
        "id": "6e5f8a60-4570-4372-9ceb-97776f851901",
        "metadata": {
          "vector_store_key": "1909.00430-2",
          "chunk_id": 0,
          "document_id": "1909.00430",
          "start_idx": 0,
          "end_idx": 709
        },
        "page_content": "Data annotation is a key bottleneck in many data driven algorithms. Specifically, deep learning models, which became a prominent tool in many data driven tasks in recent years, require large datasets to work well. However, many tasks require manual annotations which are relatively hard to obtain at scale. An attractive alternative is lightly supervised learning BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , in which the objective function is supplemented by a set of domain-specific soft-constraints over the model's predictions on unlabeled data. For example, in label regularization BIBREF0 the model is trained to fit the true label proportions of an unlabeled dataset.",
        "type": "Document"
      },
      {
        "id": "a7fe717f-04bc-4708-96e2-e0d3744977b2",
        "metadata": {
          "vector_store_key": "1707.03904-4",
          "chunk_id": 9,
          "document_id": "1707.03904",
          "start_idx": 5317,
          "end_idx": 6034
        },
        "page_content": "We also collect annotations on a subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short. We plan to release these annotations along with the datasets, and our retrieved documents for each question. Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated for each Quasar variant. The software question set was built from the definitional \u201cexcerpt\u201d entry for each tag (entity) on StackOverflow.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are possible sentence transformations represented in dataset, as new sentences?",
    "llm_answer": "The context describes *possible* sentence transformations, but does not specify how they are *represented* as *new* sentences in the dataset.  It only mentions that the dataset illustrates patterns like formalness, concreteness, and speaker confidence.\n",
    "context": [
      {
        "id": "d220df38-e8a5-4ab8-8ae4-b8f424175219",
        "metadata": {
          "vector_store_key": "1912.01673-2",
          "chunk_id": 3,
          "document_id": "1912.01673",
          "start_idx": 2088,
          "end_idx": 2788
        },
        "page_content": "In its first version, the dataset is limited to sample sentences in Czech. The goal is to support studies of semantic and syntactic relations between sentences in the continuous space. Our dataset is the prerequisite for one of possible ways of exploring sentence meaning relatability: we envision that the continuous space of sentences induced by an ideal embedding method would exhibit topological similarity to the graph of sentence variations. For instance, one could argue that a subset of sentences could be organized along a linear scale reflecting the formalness of the language used. Another set of sentences could form a partially ordered set of gradually less and less concrete statements.",
        "type": "Document"
      },
      {
        "id": "1f30a2de-f1f9-4719-af1b-b46abe040479",
        "metadata": {
          "vector_store_key": "1912.01673-2",
          "chunk_id": 5,
          "document_id": "1912.01673",
          "start_idx": 3352,
          "end_idx": 4016
        },
        "page_content": "We prefer this behaviour to emerge, as it happened for word vector operations, but regardless if the behaviour is emergent or trained, we need a dataset of sentences illustrating these patterns. If large enough, such a dataset could serve for training. If it will be smaller, it will provide a test set. In either case, these sentences could provide a \u201cskeleton\u201d to the continuous space of sentence embeddings. The paper is structured as follows: related summarizes existing methods of sentence embeddings evaluation and related work. annotation describes our methodology for constructing our dataset. data details the obtained dataset and some first observations.",
        "type": "Document"
      },
      {
        "id": "f1163bf0-4e2e-4fc0-bd70-dc703f1e1d33",
        "metadata": {
          "vector_store_key": "1912.01673-2",
          "chunk_id": 4,
          "document_id": "1912.01673",
          "start_idx": 2607,
          "end_idx": 3352
        },
        "page_content": "Another set of sentences could form a partially ordered set of gradually less and less concrete statements. And yet another set, intersecting both of the previous ones in multiple sentences could be partially or linearly ordered according to the strength of the speakers confidence in the claim. Our long term goal is to search for an embedding method which exhibits this behaviour, i.e. that the topological map of the embedding space corresponds to meaningful operations or changes in the set of sentences of a language (or more languages at once). We prefer this behaviour to emerge, as it happened for word vector operations, but regardless if the behaviour is emergent or trained, we need a dataset of sentences illustrating these patterns.",
        "type": "Document"
      },
      {
        "id": "c1baf3a0-2fb1-4e4a-8c3e-f052a1ac62cf",
        "metadata": {
          "vector_store_key": "1909.12140-0",
          "chunk_id": 8,
          "document_id": "1909.12140",
          "start_idx": 4575,
          "end_idx": 5334
        },
        "page_content": "The result of the transformation process is either written to the console or stored in a specified output file in JSON format. We also provide a browser-based user interface, where the user can directly type in sentences to be processed (see Figure FIGREF1). For the English version, we performed both a thorough manual analysis and automatic evaluation across three commonly used TS datasets from two different domains in order to assess the performance of our framework with regard to the sentence splitting subtask. The results show that our proposed sentence splitting approach outperforms the state of the art in structural TS, returning fine-grained simplified sentences that achieve a high level of grammaticality and preserve the meaning of the input.",
        "type": "Document"
      },
      {
        "id": "d08f2969-11a4-47dc-b77d-65fd397e59d3",
        "metadata": {
          "vector_store_key": "1912.01673-2",
          "chunk_id": 6,
          "document_id": "1912.01673",
          "start_idx": 3569,
          "end_idx": 4356
        },
        "page_content": "The paper is structured as follows: related summarizes existing methods of sentence embeddings evaluation and related work. annotation describes our methodology for constructing our dataset. data details the obtained dataset and some first observations. We conclude and provide the link to the dataset in conclusion As hinted above, there are many methods of converting a sequence of words into a vector in a highly dimensional space. To name a few: BiLSTM with the max-pooling trained for natural language inference BIBREF13, masked language modeling and next sentence prediction using bidirectional Transformer BIBREF14, max-pooling last states of neural machine translation among many languages BIBREF15 or the encoder final state in attentionless neural machine translation BIBREF16.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are all 15 types of modifications ilustrated in the dataset?",
    "llm_answer": "The context does not list 15 types of modifications.  It mentions several, including changes to subject, predicate, and object, changes in word order, transformations to interrogative/imperative sentences, fairy-tale style, excessive use of diminutives/vulgarisms, dadaism, and swap of roles.  Specific modifications like $\\langle s, r^{\\prime }, o^{\\prime }\\rangle $ and $\\langle s, r^{\\prime }, o\\rangle $ are mentioned, but not counted as individual types.\n",
    "context": [
      {
        "id": "e244cdf7-f116-4211-a8c7-4a880bb1f702",
        "metadata": {
          "vector_store_key": "1905.00563-6",
          "chunk_id": 12,
          "document_id": "1905.00563",
          "start_idx": 6344,
          "end_idx": 7053
        },
        "page_content": "We analyze other forms of modifications such as $\\langle s, r^{\\prime }, o^{\\prime }\\rangle $ and $\\langle s, r^{\\prime }, o\\rangle $ in appendices \"Modifications of the Form \u2329s,r ' ,o ' \u232a\\langle s, r^{\\prime }, o^{\\prime } \\rangle \" and \"Modifications of the Form \u2329s,r ' ,o\u232a\\langle s, r^{\\prime }, o \\rangle \" , and leave empirical evaluation of these modifications for future work. For explaining a target prediction, we are interested in identifying the observed fact that has the most influence (according to the model) on the prediction. We define influence of an observed fact on the prediction as the change in the prediction score if the observed fact was not present when the embeddings were learned.",
        "type": "Document"
      },
      {
        "id": "964d1942-7185-4b48-bd2c-fc2fb5a5285a",
        "metadata": {
          "vector_store_key": "1912.01673-1",
          "chunk_id": 14,
          "document_id": "1912.01673",
          "start_idx": 8116,
          "end_idx": 8816
        },
        "page_content": "Unfortunately, these examples turned out to be highly influential on the annotators' decisions and they correspond to almost two thirds of all of modifications gathered in the first round. Other very common transformations include change of a word order or transformation into a interrogative/imperative sentence. Other interesting modification were also proposed such as change into a fairy-tale style, excessive use of diminutives/vulgarisms or dadaism\u2014a swap of roles in the sentence so that the resulting sentence is grammatically correct but nonsensical in our world. Of these suggestions, we selected only the dadaistic swap of roles for the current exploration (see nonsense in Table TABREF7).",
        "type": "Document"
      },
      {
        "id": "ca59acee-d443-488e-b18c-e59f24a44302",
        "metadata": {
          "vector_store_key": "1905.00563-6",
          "chunk_id": 11,
          "document_id": "1905.00563",
          "start_idx": 5688,
          "end_idx": 6344
        },
        "page_content": "For a target triple $\\langle s, r, o\\rangle $ , we constrain the possible triples that we can remove (or inject) to be in the form of $\\langle s^{\\prime }, r^{\\prime }, o\\rangle $ i.e $s^{\\prime }$ and $r^{\\prime }$ may be different from the target, but the object is not. We analyze other forms of modifications such as $\\langle s, r^{\\prime }, o^{\\prime }\\rangle $ and $\\langle s, r^{\\prime }, o\\rangle $ in appendices \"Modifications of the Form \u2329s,r ' ,o ' \u232a\\langle s, r^{\\prime }, o^{\\prime } \\rangle \" and \"Modifications of the Form \u2329s,r ' ,o\u232a\\langle s, r^{\\prime }, o \\rangle \" , and leave empirical evaluation of these modifications for future work.",
        "type": "Document"
      },
      {
        "id": "abde311f-db3e-44fd-96aa-882a0a2aeeb0",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 17,
          "document_id": "1912.13109",
          "start_idx": 8552,
          "end_idx": 9307
        },
        "page_content": "These processes and its implementation details will be discussed later but they were learning rate decay, early stopping, model checkpointing and reducing learning rate on plateau. For the loss function we chose categorical cross entropy loss in finding the most optimal weights/parameters of the model. Formally this loss function for the model is defined as below: The double sum is over the number of observations and the categories respectively. While the model probability is the probability that the observation i belongs to category c. Among the model architectures we experimented with and without data augmentation were: Fully Connected dense networks: Model hyperparameters were inspired from the previous work done by Vo et al and Mathur et al.",
        "type": "Document"
      },
      {
        "id": "646b5ac4-ffc9-4f7f-a84b-7bff685bbd28",
        "metadata": {
          "vector_store_key": "1810.06743-1",
          "chunk_id": 3,
          "document_id": "1810.06743",
          "start_idx": 1929,
          "end_idx": 2569
        },
        "page_content": "When one dataset disobeys its schema or disagrees with a related language, the flaws may not be noticed except by such a methodological dive into the resources. When the maintainers of the resources ameliorate these flaws, the resources move closer to the goal of a universal, cross-lingual inventory of features for morphological annotation. The contributions of this work are: Morphological inflection is the act of altering the base form of a word (the lemma, represented in fixed-width type) to encode morphosyntactic features. As an example from English, prove takes on the form proved to indicate that the action occurred in the past.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What were their results on the three datasets?",
    "llm_answer": "The context mentions that HDGAN performed relatively better on CUB and Oxford datasets, and AttnGAN on COCO.  Specific quantitative results for IS, FID, FCN, or human classifiers are not provided.\n",
    "context": [
      {
        "id": "b8aeb046-fd9a-45a0-a171-7bf964e5714f",
        "metadata": {
          "vector_store_key": "1910.09399-6",
          "chunk_id": 112,
          "document_id": "1910.09399",
          "start_idx": 59912,
          "end_idx": 60523
        },
        "page_content": "While we gathered all the data we could find on scores for each model on the CUB, Oxford, and COCO datasets using IS, FID, FCN, and human classifiers, we unfortunately were unable to find certain data for AttnGAN and HDGAN (missing in Table TABREF48). The best evaluation we can give for those with missing data is our own opinions by looking at examples of generated images provided in their papers. In this regard, we observed that HDGAN produced relatively better visual results on the CUB and Oxford datasets while AttnGAN produced far more impressive results than the rest on the more complex COCO dataset.",
        "type": "Document"
      },
      {
        "id": "8b681439-d4eb-401b-8199-4cc0d2c2d0ed",
        "metadata": {
          "vector_store_key": "1909.02764-4",
          "chunk_id": 37,
          "document_id": "1909.02764",
          "start_idx": 20568,
          "end_idx": 21193
        },
        "page_content": "This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set. These results are mostly in line with Bostan2018. Now we analyze how well the models trained in Experiment 1 perform when applied to our data set. The results are shown in column \u201cSimple\u201d in Table TABREF19. We observe a clear drop in performance, with an average of F$_1$=48 %. The best performing model is again the one trained on TEC, en par with the one trained on the Figure8 data. The model trained on ISEAR performs second best in Experiment 1, it performs worst in Experiment 2.",
        "type": "Document"
      },
      {
        "id": "1858edb9-546d-4edc-9fc6-41fbc071abbd",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 39,
          "document_id": "1909.13375",
          "start_idx": 20678,
          "end_idx": 21407
        },
        "page_content": "The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could be found in Section (SECREF36). Notice that different BERTlarge models were used, so the comparison is less direct. Overall, our large models exhibits similar results to those of MTMSNlarge. For multi-span questions we achieve a significantly better performance. While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1.",
        "type": "Document"
      },
      {
        "id": "458b51fc-c0d4-482d-90b3-d4cefed66ac3",
        "metadata": {
          "vector_store_key": "1902.00330-3",
          "chunk_id": 37,
          "document_id": "1902.00330",
          "start_idx": 20853,
          "end_idx": 21565
        },
        "page_content": "We implement our models in Tensorflow and run experiments on 4 Tesla V100 GPU. We conduct experiments on several different types of public datasets including news and encyclopedia corpus. The training set is AIDA-Train and Wikipedia datasets, where AIDA-Train contains 18448 mentions and Wikipedia contains 25995 mentions. In order to compare with the previous methods, we evaluate our model on AIDA-B and other datasets. These datasets are well-known and have been used for the evaluation of most entity linking systems. The statistics of the datasets are shown in Table 1. AIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.",
        "type": "Document"
      },
      {
        "id": "15582f72-af81-4d7b-9557-6e9d41b790d1",
        "metadata": {
          "vector_store_key": "1905.00563-9",
          "chunk_id": 35,
          "document_id": "1905.00563",
          "start_idx": 17427,
          "end_idx": 18165
        },
        "page_content": "Uncertain-Test To better understand the effect of attacks, we consider a subset of test triples that 1) the model predicts correctly, 2) difference between their scores and the negative sample with the highest score is minimum. This \u201cUncertain-Test\u201d subset contains 100 triples from each of the original test sets, and we provide results of attacks on this data in Table 4 . The attacks are much more effective in this scenario, causing a considerable drop in the metrics. Further, in addition to significantly outperforming other baselines, they indicate that ConvE's confidence is much more robust. Relation Breakdown We perform additional analysis on the YAGO3-10 dataset to gain a deeper understanding of the performance of our model.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What semantic rules are proposed?",
    "llm_answer": "Five semantic rules.\n",
    "context": [
      {
        "id": "15ece6da-9949-4373-9ade-cf972243007b",
        "metadata": {
          "vector_store_key": "1706.08032-4",
          "chunk_id": 14,
          "document_id": "1706.08032",
          "start_idx": 8279,
          "end_idx": 8929
        },
        "page_content": "We use five rules in the semantic rules set because other five rules are only used to compute polarity of words after POS tagging or Parsing steps. We follow the same naming convention for rules utilized by BIBREF11 to represent the rules utilized in our proposed method. The rules utilized in the proposed method are displayed in Table TABREF15 in which is included examples from STS Corpus and output after using the rules. Table TABREF16 illustrates the number of processed sentences on each dataset. To construct embedding inputs for our model, we use a fixed-sized word vocabulary INLINEFORM0 and a fixed-sized character vocabulary INLINEFORM1 .",
        "type": "Document"
      },
      {
        "id": "3125de89-a6f4-470b-9869-924a2b6d979d",
        "metadata": {
          "vector_store_key": "1701.00185-5",
          "chunk_id": 65,
          "document_id": "1701.00185",
          "start_idx": 35221,
          "end_idx": 35700
        },
        "page_content": "In the future, how to select and incorporate more effective semantic features into the proposed framework would call for more research. We would like to thank reviewers for their comments, and acknowledge Kaggle and BioASQ for making the datasets available. This work is supported by the National Natural Science Foundation of China (No. 61602479, No. 61303172, No. 61403385) and the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDB02070005).",
        "type": "Document"
      },
      {
        "id": "c630b6be-cc52-4297-bad8-134c4951c3a8",
        "metadata": {
          "vector_store_key": "1909.12140-3",
          "chunk_id": 3,
          "document_id": "1909.12140",
          "start_idx": 2151,
          "end_idx": 2767
        },
        "page_content": "It takes a sentence as input and performs a recursive transformation process that is based upon a small set of 35 hand-crafted grammar rules for the English version and 29 rules for the German approach. These patterns were heuristically determined in a comprehensive linguistic analysis and encode syntactic and lexical features that can be derived from a sentence's parse tree. Each rule specifies (1) how to split up and rephrase the input into structurally simplified sentences and (2) how to set up a semantic hierarchy between them. They are recursively applied on a given source sentence in a top-down fashion.",
        "type": "Document"
      },
      {
        "id": "643ca322-f409-4f20-a3c4-d71d9774cb96",
        "metadata": {
          "vector_store_key": "1706.08032-4",
          "chunk_id": 13,
          "document_id": "1706.08032",
          "start_idx": 7848,
          "end_idx": 8509
        },
        "page_content": "However, the main sentiment is only in the sub-sentences following but and however. This inspires a processing step to remove unessential parts in a tweet. Rule-based approach can assists these problems in handling negation and dealing with specific PoS particles led to effectively affect the final output of classification BIBREF11 BIBREF16 . BIBREF11 summarized a full presentation of their semantic rules approach and devised ten semantic rules in their hybrid approach based on the presentation of BIBREF16 . We use five rules in the semantic rules set because other five rules are only used to compute polarity of words after POS tagging or Parsing steps.",
        "type": "Document"
      },
      {
        "id": "cfbe0530-aa07-4ae0-9c14-e3a39232b626",
        "metadata": {
          "vector_store_key": "1909.12140-3",
          "chunk_id": 4,
          "document_id": "1909.12140",
          "start_idx": 2487,
          "end_idx": 3284
        },
        "page_content": "They are recursively applied on a given source sentence in a top-down fashion. When no more rule matches, the algorithm stops and returns the generated discourse tree. In a first step, source sentences that present a complex linguistic form are turned into clean, compact structures by decomposing clausal and phrasal components. For this purpose, the transformation rules encode both the splitting points and rephrasing procedure for reconstructing proper sentences. Each split will create two or more sentences with a simplified syntax. To establish a semantic hierarchy between them, two subtasks are carried out: First, we set up a contextual hierarchy between the split sentences by connecting them with information about their hierarchical level, similar to the concept of nuclearity in RST.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance of the model?",
    "llm_answer": "The context describes improvements in model performance across various datasets (DROP, Quasar-S, Quasar-T, and others), with large models significantly outperforming base models and baselines.  Specific results are given for different models and metrics, like EM, F1, and accuracy.\n",
    "context": [
      {
        "id": "1b1f9986-e4da-44a7-a6cd-13931a730da6",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 38,
          "document_id": "1909.13375",
          "start_idx": 19979,
          "end_idx": 20678
        },
        "page_content": "For the large models we used a batch size of 3 with a learning rate of $5e^{-6}$ and trained for 5 epochs, except for the model without the single-span heads that was trained with a batch size of 2 for 7 epochs. F1 was used as our validation metric. All models were trained on a single GPU with 12-16GB of memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions.",
        "type": "Document"
      },
      {
        "id": "245d9a70-772c-4b81-b29d-8987a0ef16f8",
        "metadata": {
          "vector_store_key": "1707.03904-0",
          "chunk_id": 37,
          "document_id": "1707.03904",
          "start_idx": 20485,
          "end_idx": 21217
        },
        "page_content": "The GA model achieves $48.3\\%$ accuracy on the set of instances for which the answer is in context, however, a search accuracy of only $65\\%$ means its overall performance is lower. This can improve with improved retrieval. For Quasar-T, both the neural models significantly outperform the heuristic models, with BiDAF getting the highest F1 score of $28.5\\%$ . The best performing baselines, however, lag behind human performance by $16.4\\%$ and $32.1\\%$ for Quasar-S and Quasar-T respectively, indicating the strong potential for improvement. Interestingly, for human performance we observe that non-experts are able to match or beat the performance of experts when given access to the background corpus for searching the answers.",
        "type": "Document"
      },
      {
        "id": "2e7cfe41-b41c-475c-83f6-e7d39903180d",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 40,
          "document_id": "1909.13375",
          "start_idx": 21052,
          "end_idx": 21792
        },
        "page_content": "While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1. All that, while keeping in mind we compare a base model to a large model (for reference, note the 8 point improvement between MTMSNbase and MTMSNlarge in both EM and F1). Our best model, large-squad, exhibits a huge improvement of 29.7 in EM and 15.1 in F1 compared to MTMSNlarge. When comparing single-span performance, our best model exhibits slightly better results, but it should be noted that it retains the single-span heads from NABERT+, while in MTMSN they have one head to predict both single-span and multi-span answers.",
        "type": "Document"
      },
      {
        "id": "df925d04-51e0-4d83-b972-800b057e5067",
        "metadata": {
          "vector_store_key": "1703.04617-3",
          "chunk_id": 42,
          "document_id": "1703.04617",
          "start_idx": 23295,
          "end_idx": 23973
        },
        "page_content": "When letting the number of hidden question types ( $K$ ) to be 20, the performance improves to 68.73%/77.74% on EM and F1, respectively, which corresponds to the results of our model reported in Table 1 . Furthermore, after submitted our result, we have experimented with a large value of $K$ and found that when $K=100$ , we can achieve a better performance of 69.10%/78.38% on the development set. Figure UID61 shows the EM/F1 scores of different question types while Figure UID62 is the question type amount distribution on the development set. In Figure UID61 we can see that the average EM/F1 of the \"when\" question is highest and those of the \"why\" question is the lowest.",
        "type": "Document"
      },
      {
        "id": "1bffa3f8-bbbc-48c1-81fe-55fb01d9b256",
        "metadata": {
          "vector_store_key": "1910.11769-3",
          "chunk_id": 12,
          "document_id": "1910.11769",
          "start_idx": 7313,
          "end_idx": 8027
        },
        "page_content": "We masked out named entities with entity-type specific placeholders to reduce the chance of benchmark models utilizing named entities as a basis for classification. Benchmark results are shown in Table TABREF17. The dataset is approximately balanced after discarding the Surprise and Disgust classes. We report the average micro-F1 scores, with 5-fold cross validation for each technique. We provide a brief overview of each benchmark experiment below. Among all of the benchmarks, Bidirectional Encoder Representations from Transformers (BERT) BIBREF11 achieved the best performance with a 0.604 micro-F1 score. Overall, we observed that deep-learning based techniques performed better than lexical based methods.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was the performance of both approaches on their dataset?",
    "llm_answer": "The context describes performance on DROP's test set, with the author's model being the best overall.  It also describes ablation studies on the development set.  Performance on other datasets or tasks is mentioned, but no overall comparison between the two approaches (author's and MTMSN) on their dataset is provided.\n",
    "context": [
      {
        "id": "92dd0ad6-8e89-4d29-9317-3cf4eca34bfd",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 37,
          "document_id": "1909.13375",
          "start_idx": 19698,
          "end_idx": 20343
        },
        "page_content": "We preferred to focus our efforts on the ablation studies, in hope to gain further insights on the effect of the components that we ourselves introduced. For ease of performance comparison, we followed NABERT+'s training settings: we used the BERT Adam optimizer from huggingface with default settings and a learning rate of $1e^{-5}$. The only difference was that we used a batch size of 12. We trained our base model for 20 epochs. For the large models we used a batch size of 3 with a learning rate of $5e^{-6}$ and trained for 5 epochs, except for the model without the single-span heads that was trained with a batch size of 2 for 7 epochs.",
        "type": "Document"
      },
      {
        "id": "93b8b741-b6d7-4597-9fce-2de82a1eb34e",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 42,
          "document_id": "1909.13375",
          "start_idx": 22170,
          "end_idx": 22785
        },
        "page_content": "Therefore, it could prove beneficial to try and analyze the reasons behind each model's (ours and MTMSN) relative advantages, and perhaps try to combine them into a more holistic approach of tackling span questions. Table TABREF25 shows the results on DROP's test set, with our model being the best overall as of the time of writing, and not just on multi-span questions. In order to analyze the effect of each of our changes, we conduct ablation studies on the development set, depicted in Table TABREF26. Not using the simple preprocessing from Section SECREF17 resulted in a 2.5 point decrease in both EM and F1.",
        "type": "Document"
      },
      {
        "id": "059820ad-3f5b-44c5-a3e8-611cfcfed0d9",
        "metadata": {
          "vector_store_key": "1705.09665-0",
          "chunk_id": 35,
          "document_id": "1705.09665",
          "start_idx": 20067,
          "end_idx": 20954
        },
        "page_content": "We compared out-of-sample predictive performance via leave-one-community-out cross validation using random forest regressors with ensembles of size 100, and otherwise default hyperparameters BIBREF26 . A model predicting average monthly retention based on a community's average distinctiveness and dynamicity achieves an average mean squared error ( INLINEFORM0 ) of INLINEFORM1 and INLINEFORM2 , while an analogous model predicting based on a community's size and average activity level (both log-transformed) achieves INLINEFORM4 and INLINEFORM5 . The difference between the two models is not statistically significant ( INLINEFORM6 , Wilcoxon signed-rank test). However, combining features from both models results in a large and statistically significant improvement over each independent model ( INLINEFORM7 , INLINEFORM8 , INLINEFORM9 Bonferroni-corrected pairwise Wilcoxon tests).",
        "type": "Document"
      },
      {
        "id": "c938d47c-e132-4294-bd51-25295fb776d2",
        "metadata": {
          "vector_store_key": "1603.07044-3",
          "chunk_id": 28,
          "document_id": "1603.07044",
          "start_idx": 15079,
          "end_idx": 15744
        },
        "page_content": "Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited training data is given, both feature-rich based system and our system are worse than the IR system. For task C, our system also got comparable results with the feature-rich based system. If we do a simple system combination (average the rank score) between our system and the IR system, the combined system will give large gains on tasks B and C.",
        "type": "Document"
      },
      {
        "id": "e81fee71-223a-449b-b9b3-75b7e7e7caec",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 41,
          "document_id": "1909.13375",
          "start_idx": 21792,
          "end_idx": 22634
        },
        "page_content": "When comparing single-span performance, our best model exhibits slightly better results, but it should be noted that it retains the single-span heads from NABERT+, while in MTMSN they have one head to predict both single-span and multi-span answers. For a fairer comparison, we trained our model with the single-span heads removed, where our multi-span head remained the only head aimed for handling span questions. With this no-single-span-heads setting, while our multi-span performance even improved a bit, our single-span performance suffered a slight drop, ending up trailing by 0.8 in EM and 0.6 in F1 compared to MTMSN. Therefore, it could prove beneficial to try and analyze the reasons behind each model's (ours and MTMSN) relative advantages, and perhaps try to combine them into a more holistic approach of tackling span questions.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What genres are covered?",
    "llm_answer": "Entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation, advertisement.\n",
    "context": [
      {
        "id": "d67e66a4-0402-4a97-b450-b0ac1c122764",
        "metadata": {
          "vector_store_key": "1911.01799-0",
          "chunk_id": 5,
          "document_id": "1911.01799",
          "start_idx": 3260,
          "end_idx": 3980
        },
        "page_content": "We intentionally collected data from 11 genres, including entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement. The speech of a particular speaker may be in more than 5 genres. As a comparison, most of the utterances in VoxCeleb were extracted from interview videos. The diversity in genres makes our database more representative for the true scenarios in unconstrained conditions, but also more challenging. CN-Celeb is not fully automated, but involves human check. We found that more complex the genre is, more errors the automated pipeline tends to produce. Ironically, the error-pron segments could be highly valuable as they tend to be boundary samples.",
        "type": "Document"
      },
      {
        "id": "8b3b8e7a-f900-4842-9ea7-071e73595046",
        "metadata": {
          "vector_store_key": "1910.11769-0",
          "chunk_id": 10,
          "document_id": "1910.11769",
          "start_idx": 6256,
          "end_idx": 6946
        },
        "page_content": "All of the modern narratives were written after the year 2000, with notable amount of themes in coming-of-age, strong-female-lead, and LGBTQ+. The genre distribution is listed in Table TABREF8. In the final dataset, 21.0% of the data has consensus between all annotators, 73.5% has majority agreement, and 5.48% has labels assigned after consultation with in-house annotators. The distribution of data points over labels with top lexicons (lower-cased, normalized) is shown in Table TABREF9. Note that the Disgust category is very small and should be discarded. Furthermore, we suspect that the data labelled as Surprise may be noisier than other categories and should be discarded as well.",
        "type": "Document"
      },
      {
        "id": "37c82d8e-90d3-49c2-92b7-d87d2cf74da4",
        "metadata": {
          "vector_store_key": "1911.01799-3",
          "chunk_id": 9,
          "document_id": "1911.01799",
          "start_idx": 5428,
          "end_idx": 6107
        },
        "page_content": "More details of these challenges are as follows. Most of the utterances involve real-world noise, including ambient noise, background babbling, music, cheers and laugh. A certain amount of utterances involve strong and overlapped background speakers, especially in the dram and movie genres. Most of speakers have different genres of utterances, which results in significant variation in speaking styles. The utterances of the same speaker may be recorded at different time and with different devices, leading to serious cross-time and cross-channel problems. Most of the utterances are short, which meets the scenarios of most real applications but leads to unreliable decision.",
        "type": "Document"
      },
      {
        "id": "a83277bb-e5ed-4e56-8f99-e3035ec9f266",
        "metadata": {
          "vector_store_key": "1707.03904-0",
          "chunk_id": 29,
          "document_id": "1707.03904",
          "start_idx": 16115,
          "end_idx": 16824
        },
        "page_content": "In the remaining 132, a total of 214 genres were annotated (a question could be annotated with multiple genres), while 10 questions had conflicting entity-type annotations which we discarded, leaving 122 total entity-type annotations. Figure 3 shows the distribution of these annotations. We evaluate several baselines on Quasar, ranging from simple heuristics to deep neural networks. Some predict a single token / entity as the answer, while others predict a span of tokens. MF-i (Maximum Frequency) counts the number of occurrences of each candidate answer in the retrieved context and returns the one with maximum frequency. MF-e is the same as MF-i except it excludes the candidates present in the query.",
        "type": "Document"
      },
      {
        "id": "b7e8dc24-b14d-4948-8835-e1d3f03dd43b",
        "metadata": {
          "vector_store_key": "1911.01799-0",
          "chunk_id": 4,
          "document_id": "1911.01799",
          "start_idx": 2424,
          "end_idx": 3260
        },
        "page_content": "The utterances were collected from open-source media using a fully automated pipeline based on computer vision techniques, in particular face detection, tracking and recognition, plus video-audio synchronization. The automated pipeline is almost costless, and thus greatly improves the efficiency of data collection. In this paper, we re-implement the automated pipeline of VoxCeleb and collect a new large-scale speaker dataset, named CN-Celeb. Compared with VoxCeleb, CN-Celeb has three distinct features: CN-Celeb specially focuses on Chinese celebrities, and contains more than $130,000$ utterances from $1,000$ persons. CN-Celeb covers more genres of speech. We intentionally collected data from 11 genres, including entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which of the two speech recognition models works better overall on CN-Celeb?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "e3f048a0-7ff5-4e83-8709-567da786a901",
        "metadata": {
          "vector_store_key": "1911.01799-0",
          "chunk_id": 26,
          "document_id": "1911.01799",
          "start_idx": 14624,
          "end_idx": 15331
        },
        "page_content": "More importantly, with re-training the back-end model with VoxCeleb(L) (4th row), the performance on SITW becomes better than the same-source result on CN-Celeb(E) (11.34% vs 14.24%). All these results reconfirmed the significant difference between the two datasets, and indicates that CN-Celeb is more challenging than VoxCeleb. We introduced a free dataset CN-Celeb for speaker recognition research. The dataset contains more than $130k$ utterances from $1,000$ Chinese celebrities, and covers 11 different genres in real world. We compared CN-Celeb and VoxCeleb, a widely used dataset in speaker recognition, by setting up a series of experiments based on two state-of-the-art speaker recognition models.",
        "type": "Document"
      },
      {
        "id": "f27ed195-f47c-49ac-8ff9-e879be64217f",
        "metadata": {
          "vector_store_key": "1911.01799-0",
          "chunk_id": 8,
          "document_id": "1911.01799",
          "start_idx": 4739,
          "end_idx": 5428
        },
        "page_content": "This leads to three features that we have discussed in the previous section: Chinese focused, complex genres, and quality guarantee by human check. In summary, CN-Celeb contains over $130,000$ utterances from $1,000$ Chinese celebrities. It covers 11 genres and the total amount of speech waveforms is 274 hours. Table TABREF5 gives the data distribution over the genres, and Table TABREF6 presents the data distribution over the length of utterances. Table TABREF13 summarizes the main difference between CN-Celeb and VoxCeleb. Compared to VoxCeleb, CN-Celeb is a more complex dataset and more challenging for speaker recognition research. More details of these challenges are as follows.",
        "type": "Document"
      },
      {
        "id": "3a557c9d-ad2a-4795-8489-e9368443cdd0",
        "metadata": {
          "vector_store_key": "1911.01799-0",
          "chunk_id": 10,
          "document_id": "1911.01799",
          "start_idx": 5940,
          "end_idx": 6781
        },
        "page_content": "Most of the utterances are short, which meets the scenarios of most real applications but leads to unreliable decision. CN-Celeb was collected following a two-stage strategy: firstly we used an automated pipeline to extract potential segments of the Person of Interest (POI), and then applied a human check to remove incorrect segments. This process is much faster than purely human-based segmentation, and reduces errors caused by a purely automated process. Briefly, the automated pipeline we used is similar to the one used to collect VoxCeleb1 BIBREF14 and VoxCeleb2 BIBREF15, though we made some modification to increase efficiency and precision. Especially, we introduced a new face-speaker double check step that fused the information from both the image and speech signals to increase the recall rate while maintaining the precision.",
        "type": "Document"
      },
      {
        "id": "1fb9e248-cb39-4b4e-887d-3760e1dd05d6",
        "metadata": {
          "vector_store_key": "1911.01799-0",
          "chunk_id": 27,
          "document_id": "1911.01799",
          "start_idx": 15331,
          "end_idx": 15850
        },
        "page_content": "We compared CN-Celeb and VoxCeleb, a widely used dataset in speaker recognition, by setting up a series of experiments based on two state-of-the-art speaker recognition models. Experimental results demonstrated that CN-Celeb is significantly different from VoxCeleb, and it is more challenging for speaker recognition research. The EER performance we obtained in this paper suggests that in unconstrained conditions, the performance of the current speaker recognition techniques might be much worse than it was thought.",
        "type": "Document"
      },
      {
        "id": "b7e8dc24-b14d-4948-8835-e1d3f03dd43b",
        "metadata": {
          "vector_store_key": "1911.01799-0",
          "chunk_id": 4,
          "document_id": "1911.01799",
          "start_idx": 2424,
          "end_idx": 3260
        },
        "page_content": "The utterances were collected from open-source media using a fully automated pipeline based on computer vision techniques, in particular face detection, tracking and recognition, plus video-audio synchronization. The automated pipeline is almost costless, and thus greatly improves the efficiency of data collection. In this paper, we re-implement the automated pipeline of VoxCeleb and collect a new large-scale speaker dataset, named CN-Celeb. Compared with VoxCeleb, CN-Celeb has three distinct features: CN-Celeb specially focuses on Chinese celebrities, and contains more than $130,000$ utterances from $1,000$ persons. CN-Celeb covers more genres of speech. We intentionally collected data from 11 genres, including entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much is performance on CN-Celeb inferior to performance on VoxCeleb?",
    "llm_answer": "The context states that performance on CN-Celeb(E) is much worse than performance on SITW and SITW(S), and that a re-trained model on VoxCeleb(L) performs better on SITW than the same-source CN-Celeb(E).  However, it does not provide specific performance numbers for a direct comparison.  Therefore, the answer is not found in the context.\n",
    "context": [
      {
        "id": "58513150-5fc1-45c5-a793-9d894292a063",
        "metadata": {
          "vector_store_key": "1911.01799-4",
          "chunk_id": 25,
          "document_id": "1911.01799",
          "start_idx": 14480,
          "end_idx": 15151
        },
        "page_content": "Firstly, it can be seen that the system trained purely on VoxCeleb obtained good performance on SITW(S) (1st row). This is understandable as VoxCeleb and SITW(S) were collected from the same source. For the pure CN-Celeb system (2nd row), although CN-Celeb(T) and CN-Celeb(E) are from the same source, the performance is still poor (14.24%). More importantly, with re-training the back-end model with VoxCeleb(L) (4th row), the performance on SITW becomes better than the same-source result on CN-Celeb(E) (11.34% vs 14.24%). All these results reconfirmed the significant difference between the two datasets, and indicates that CN-Celeb is more challenging than VoxCeleb.",
        "type": "Document"
      },
      {
        "id": "e3f048a0-7ff5-4e83-8709-567da786a901",
        "metadata": {
          "vector_store_key": "1911.01799-0",
          "chunk_id": 26,
          "document_id": "1911.01799",
          "start_idx": 14624,
          "end_idx": 15331
        },
        "page_content": "More importantly, with re-training the back-end model with VoxCeleb(L) (4th row), the performance on SITW becomes better than the same-source result on CN-Celeb(E) (11.34% vs 14.24%). All these results reconfirmed the significant difference between the two datasets, and indicates that CN-Celeb is more challenging than VoxCeleb. We introduced a free dataset CN-Celeb for speaker recognition research. The dataset contains more than $130k$ utterances from $1,000$ Chinese celebrities, and covers 11 different genres in real world. We compared CN-Celeb and VoxCeleb, a widely used dataset in speaker recognition, by setting up a series of experiments based on two state-of-the-art speaker recognition models.",
        "type": "Document"
      },
      {
        "id": "5dd85672-e051-44bc-bbaf-0218d34509e8",
        "metadata": {
          "vector_store_key": "1911.01799-4",
          "chunk_id": 23,
          "document_id": "1911.01799",
          "start_idx": 13454,
          "end_idx": 14155
        },
        "page_content": "It can be observed that for both the i-vector system and the x-vector system, the performance on CN-Celeb(E) is much worse than the performance on SITW and SITW(S). This indicates that there is big difference between these two datasets. From another perspective, it demonstrates that the model trained with VoxCeleb does not generalize well, although it has achieved reasonable performance on data from a similar source (SITW). To further compare CN-Celeb and VoxCeleb in a quantitative way, we built systems based on CN-Celeb and VoxCeleb, respectively. For a fair comparison, we randomly sampled 800 speakers from VoxCeleb and built a new dataset VoxCeleb(L) whose size is comparable to CN-Celeb(T).",
        "type": "Document"
      },
      {
        "id": "982430f5-0c51-470e-a2bd-228ebe238f10",
        "metadata": {
          "vector_store_key": "1911.01799-4",
          "chunk_id": 24,
          "document_id": "1911.01799",
          "start_idx": 13845,
          "end_idx": 14480
        },
        "page_content": "For a fair comparison, we randomly sampled 800 speakers from VoxCeleb and built a new dataset VoxCeleb(L) whose size is comparable to CN-Celeb(T). This data set was used for back-end (LDA-PLDA) training. The experimental results are shown in Table TABREF26. Note that the performance of all the comparative experiments show the same trend with the i-vector system and the x-vector system, we therefore only analyze the i-vector results. Firstly, it can be seen that the system trained purely on VoxCeleb obtained good performance on SITW(S) (1st row). This is understandable as VoxCeleb and SITW(S) were collected from the same source.",
        "type": "Document"
      },
      {
        "id": "1fb9e248-cb39-4b4e-887d-3760e1dd05d6",
        "metadata": {
          "vector_store_key": "1911.01799-0",
          "chunk_id": 27,
          "document_id": "1911.01799",
          "start_idx": 15331,
          "end_idx": 15850
        },
        "page_content": "We compared CN-Celeb and VoxCeleb, a widely used dataset in speaker recognition, by setting up a series of experiments based on two state-of-the-art speaker recognition models. Experimental results demonstrated that CN-Celeb is significantly different from VoxCeleb, and it is more challenging for speaker recognition research. The EER performance we obtained in this paper suggests that in unconstrained conditions, the performance of the current speaker recognition techniques might be much worse than it was thought.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do the authors measure performance?",
    "llm_answer": "The authors use F<sub>1</sub> score, EM-score, and training/testing curves (averaged over 3 random seeds).  Human evaluation is also used, as well as comparison against baselines.  Summarization is evaluated using Pyramid and ROUGE metrics.\n",
    "context": [
      {
        "id": "6a443055-4b4a-46e1-a2dc-b3211ac62d38",
        "metadata": {
          "vector_store_key": "1908.10449-0",
          "chunk_id": 20,
          "document_id": "1908.10449",
          "start_idx": 10741,
          "end_idx": 11449
        },
        "page_content": "Since iMRC involves both MRC and RL, we adopt evaluation metrics from both settings. First, as a question answering task, we use $\\text{F}_1$ score to compare predicted answers against ground-truth, as in previous works. When there exist multiple ground-truth answers, we report the max $\\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we evaluate an agent's performance during both its training and testing phases. During training, we report training curves averaged over 3 random seeds. During test, we follow common practice in supervised learning tasks where we report the agent's test performance corresponding to its best validation performance .",
        "type": "Document"
      },
      {
        "id": "3699d425-11f4-45cc-aae5-6a93d3e32b86",
        "metadata": {
          "vector_store_key": "1707.03904-0",
          "chunk_id": 8,
          "document_id": "1707.03904",
          "start_idx": 4627,
          "end_idx": 5317
        },
        "page_content": "We evaluate Quasar against human testers, as well as several baselines ranging from na\u00efve heuristics to state-of-the-art machine readers. The best performing baselines achieve $33.6\\%$ and $28.5\\%$ on Quasar-S and Quasar-T, while human performance is $50\\%$ and $60.6\\%$ respectively. For the automatic systems, we see an interesting tension between searching and reading accuracies \u2013 retrieving more documents in the search phase leads to a higher coverage of answers, but makes the comprehension task more difficult. We also collect annotations on a subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short.",
        "type": "Document"
      },
      {
        "id": "d50bd955-993d-4140-b06f-46c6425afab9",
        "metadata": {
          "vector_store_key": "1908.06606-0",
          "chunk_id": 27,
          "document_id": "1908.06606",
          "start_idx": 15944,
          "end_idx": 16737
        },
        "page_content": "These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage of predictions that match any one of the ground truth answers exactly. The F$_1$-score metric is a looser metric measures the average overlap between the prediction and ground truth answer. To implement deep neural network models, we utilize the Keras library BIBREF36 with TensorFlow BIBREF37 backend.",
        "type": "Document"
      },
      {
        "id": "c7a5f4b3-a8d8-4418-b4ed-2106461df1d1",
        "metadata": {
          "vector_store_key": "2003.07433-0",
          "chunk_id": 36,
          "document_id": "2003.07433",
          "start_idx": 19458,
          "end_idx": 20219
        },
        "page_content": "We chose one dimension for each of the questions under each category to reflect real survey system evaluation. Our chosen categories are state in Fig FIGREF20. Score calculation $\\alpha $-score: $\\alpha $-scores refer to the Cronbach's alphas for the internal reliability of the specific words within each category. The binary alphas are computed on the ratio of occurrence and non-occurrence of each dictionary word whereas the raw or uncorrected alphas are based on the percentage of use of each of the category words within texts. After the PTSD Linguistic Dictionary has been created, we empirically evaluate its psychometric properties such as reliability and validity as per American Standards for educational and psychological testing guideline BIBREF24.",
        "type": "Document"
      },
      {
        "id": "f221ac3d-8ea6-49e8-8ce8-1528ccd79f99",
        "metadata": {
          "vector_store_key": "1909.00578-2",
          "chunk_id": 2,
          "document_id": "1909.00578",
          "start_idx": 1185,
          "end_idx": 1928
        },
        "page_content": "Sum-QE achieves very high correlations with human ratings, showing the ability of BERT to model linguistic qualities that relate to both text content and form. Summarization evaluation metrics like Pyramid BIBREF5 and ROUGE BIBREF3, BIBREF2 are recall-oriented; they basically measure the content from a model (reference) summary that is preserved in peer (system generated) summaries. Pyramid requires substantial human effort, even in its more recent versions that involve the use of word embeddings BIBREF8 and a lightweight crowdsourcing scheme BIBREF9. ROUGE is the most commonly used evaluation metric BIBREF10, BIBREF11, BIBREF12. Inspired by BLEU BIBREF4, it relies on common $n$-grams or subsequences between peer and model summaries.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the latest paper covered by this survey?",
    "llm_answer": "Wei, Jason and Zou, Kai EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)\n",
    "context": [
      {
        "id": "588b4926-f483-4fe2-b503-5460cc39c40f",
        "metadata": {
          "vector_store_key": "1912.13109-2",
          "chunk_id": 28,
          "document_id": "1912.13109",
          "start_idx": 14578,
          "end_idx": 15657
        },
        "page_content": "Pennington, Jeffrey and Socher, Richard and Manning, Christopher Glove: Global vectors for word representation Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) [7] Zhang, Lei and Wang, Shuai and Liu, Bing Deep learning for sentiment analysis: A survey Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery [8] Caruana, Rich and Lawrence, Steve and Giles, C Lee Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping Advances in neural information processing systems [9] Beale, Mark Hudson and Hagan, Martin T and Demuth, Howard B Neural network toolbox user\u2019s guide The MathWorks Incs [10] Chollet, Fran\u00e7ois and others Keras: The python deep learning library Astrophysics Source Code Library [11] Wei, Jason and Zou, Kai EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
        "type": "Document"
      },
      {
        "id": "80658bbd-27df-43c4-9667-7d716f163cd2",
        "metadata": {
          "vector_store_key": "1912.13109-2",
          "chunk_id": 29,
          "document_id": "1912.13109",
          "start_idx": 14579,
          "end_idx": 15448
        },
        "page_content": "Zhang, Lei and Wang, Shuai and Liu, Bing Deep learning for sentiment analysis: A survey Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery [8] Caruana, Rich and Lawrence, Steve and Giles, C Lee Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping Advances in neural information processing systems [9] Beale, Mark Hudson and Hagan, Martin T and Demuth, Howard B Neural network toolbox user\u2019s guide The MathWorks Incs [10] Chollet, Fran\u00e7ois and others Keras: The python deep learning library Astrophysics Source Code Library [11] Wei, Jason and Zou, Kai EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
        "type": "Document"
      },
      {
        "id": "cdd54fda-9e69-4476-8c4e-5cd17b358f0d",
        "metadata": {
          "vector_store_key": "1910.09399-1",
          "chunk_id": 12,
          "document_id": "1910.09399",
          "start_idx": 6914,
          "end_idx": 7651
        },
        "page_content": "Recently, many new GAN architectures and designs have been proposed to use GANs for different applications, e.g. using GANs to generate sentimental texts BIBREF18, or using GANs to transform natural images into cartoons BIBREF19. Although GANs are becoming increasingly popular, very few survey papers currently exist to summarize and outline contemporaneous technical innovations and contributions of different GAN architectures BIBREF20, BIBREF21. Survey papers specifically attuned to analyzing different contributions to text-to-image synthesis using GANs are even more scarce. We have thus found two surveys BIBREF6, BIBREF7 on image synthesis using GANs, which are the two most closely related publications to our survey objective.",
        "type": "Document"
      },
      {
        "id": "f4815548-ea8f-435a-97fe-fb779d7aa33f",
        "metadata": {
          "vector_store_key": "1910.09399-1",
          "chunk_id": 9,
          "document_id": "1910.09399",
          "start_idx": 5384,
          "end_idx": 6129
        },
        "page_content": "The visual abstract of the survey and the list of reviewed GAN frameworks is shown in Figure FIGREF8. black The remainder of the survey is organized as follows. Section 2 presents a brief summary of existing works on subjects similar to that of this paper and highlights the key distinctions making ours unique. Section 3 gives a short introduction to GANs and some preliminary concepts related to image generation, as they are the engines that make text-to-image synthesis possible and are essential building blocks to achieve photo-realistic images from text descriptions. Section 4 proposes a taxonomy to summarize GAN based text-to-image synthesis, discusses models and architectures of novel works focused solely on text-to-image synthesis.",
        "type": "Document"
      },
      {
        "id": "e900003c-e4a4-4760-af29-1415e5bd0e01",
        "metadata": {
          "vector_store_key": "1708.05873-0",
          "chunk_id": 35,
          "document_id": "1708.05873",
          "start_idx": 20206,
          "end_idx": 20343
        },
        "page_content": "The paper more broadly demonstrates how text analytic approaches can help us to better understand different aspects of global governance.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What learning paradigms do they cover in this survey?",
    "llm_answer": "Supervised learning, semi-supervised learning, and using distant supervision.\n",
    "context": [
      {
        "id": "0ab61107-67bf-493c-aa5f-199d370caaa3",
        "metadata": {
          "vector_store_key": "1703.04617-2",
          "chunk_id": 26,
          "document_id": "1703.04617",
          "start_idx": 14460,
          "end_idx": 15206
        },
        "page_content": "The previous models are often trained for all questions without explicitly discriminating different question types; however, for a target question, both the common features shared by all questions and the specific features for a specific type of question are further considered in this paper, as they could potentially obey different distributions. In this paper we further explicitly model different types of questions in the end-to-end training. We start from a simple way to first analyze the word frequency of all questions, and obtain top-10 most frequent question types: what, how, who, when, which, where, why, be, whose, and whom, in which be stands for the questions beginning with different forms of the word be such as is, am, and are.",
        "type": "Document"
      },
      {
        "id": "a6dff15d-6323-4ff2-9d1f-f503bd1a9ef5",
        "metadata": {
          "vector_store_key": "1710.01492-5",
          "chunk_id": 35,
          "document_id": "1710.01492",
          "start_idx": 19077,
          "end_idx": 19782
        },
        "page_content": "Traditionally, the above features were fed into classifiers such as Maximum Entropy (MaxEnt) and Support Vector Machines (SVM) with various kernels. However, observation over the SemEval Twitter sentiment task in recent years shows growing interest in, and by now clear dominance of methods based on deep learning. In particular, the best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks BIBREF53 , BIBREF54 . Conversely, kernel machines seem to be less frequently used than in the past, and the use of learning methods other than the ones mentioned above is at this point scarce. All these models are examples of supervised learning as they need labeled training data.",
        "type": "Document"
      },
      {
        "id": "57769074-6a7a-439a-a94d-fabc5a6761da",
        "metadata": {
          "vector_store_key": "1905.08949-4",
          "chunk_id": 11,
          "document_id": "1905.08949",
          "start_idx": 6657,
          "end_idx": 7620
        },
        "page_content": "Recently, with the growth of various QA applications such as Knowledge Base Question Answering (KBQA) BIBREF27 and Visual Question Answering (VQA) BIBREF28 , NQG research has also widened the spectrum of sources to include knowledge bases BIBREF29 and images BIBREF10 . This trend is also spurred by the remarkable success of neural models in feature representation, especially on image features BIBREF30 and knowledge representations BIBREF31 . We discuss adapting NQG models to other input modalities in Section \"Wider Input Modalities\" . Finally, we consider the required cognitive process behind question asking, a distinguishing factor for questions BIBREF32 . A typical framework that attempts to categorize the cognitive levels involved in question asking comes from Bloom's taxonomy BIBREF33 , which has undergone several revisions and currently has six cognitive levels: Remembering, Understanding, Applying, Analyzing, Evaluating and Creating BIBREF32 .",
        "type": "Document"
      },
      {
        "id": "0f37864f-01d8-47e3-bda3-1b8630c4df0d",
        "metadata": {
          "vector_store_key": "1703.04617-2",
          "chunk_id": 6,
          "document_id": "1703.04617",
          "start_idx": 3512,
          "end_idx": 4156
        },
        "page_content": "Different types of questions are often used to seek for different types of information. For example, a \"what\" question could have very different property from that of a \"why\" question, while they may share information and need to be trained together instead of separately. We view this as a \"adaptation\" problem to let different types of questions share a basic model but still discriminate them when needed. Specifically, we are motivated by the ideas \"i-vector\" BIBREF18 in speech recognition, where neural network based adaptation is performed among different (groups) of speakers and we focused instead on different types of questions here.",
        "type": "Document"
      },
      {
        "id": "a2564a73-35d9-49f8-82a7-db0623fe55ab",
        "metadata": {
          "vector_store_key": "1710.01492-5",
          "chunk_id": 8,
          "document_id": "1710.01492",
          "start_idx": 4000,
          "end_idx": 4867
        },
        "page_content": "All these are explored at SemEval, the International Workshop on Semantic Evaluation, which has created a number of benchmark datasets and has enabled direct comparison between different systems and approaches, both as part of the competition and beyond. Traditionally, the task has been addressed using supervised and semi-supervised methods, as well as using distant supervision, with the most important resource being sentiment polarity lexicons, and with feature-rich approaches as the dominant research direction for years. With the recent rise of deep learning, which in many cases eliminates the need for any explicit feature modeling, the importance of both lexicons and features diminishes, while at the same time attention is shifting towards learning from large unlabeled data, which is needed to train the high number of parameters of such complex models.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are all the input modalities considered in prior work in question generation?",
    "llm_answer": "Raw text, database, or semantic representation.\n",
    "context": [
      {
        "id": "da5e5fd1-ee99-40cb-b9c0-c6cd51c0baaa",
        "metadata": {
          "vector_store_key": "1905.08949-0",
          "chunk_id": 30,
          "document_id": "1905.08949",
          "start_idx": 16993,
          "end_idx": 17695
        },
        "page_content": "The switch between different modes is controlled by a discrete variable produced by a learnable module of the model in each decoding step. Determining the appropriate question word harks back to question type identification, which is correlated with the question intention, as different intents may yield different questions, even when presented with the same (passage, answer) input pair. This points to the direction of exploring question pragmatics, where external contextual information (such as intent) can inform and influence how questions should optimally be generated. Leveraging rich paragraph-level contexts around the input text is another natural consideration to produce better questions.",
        "type": "Document"
      },
      {
        "id": "af0f5b4e-ba5a-422d-9235-4155e849896d",
        "metadata": {
          "vector_store_key": "1905.08949-7",
          "chunk_id": 6,
          "document_id": "1905.08949",
          "start_idx": 3366,
          "end_idx": 4159
        },
        "page_content": "Given a sentence or a paragraph as input, content selection selects a particular salient topic worthwhile to ask about and determines the question type (What, When, Who, etc.). Approaches either take a syntactic BIBREF11 , BIBREF12 , BIBREF13 or semantic BIBREF14 , BIBREF3 , BIBREF15 , BIBREF16 tack, both starting by applying syntactic or semantic parsing, respectively, to obtain intermediate symbolic representations. Question construction then converts intermediate representations to a natural language question, taking either a tranformation- or template-based approach. The former BIBREF17 , BIBREF18 , BIBREF13 rearranges the surface form of the input sentence to produce the question; the latter BIBREF19 , BIBREF20 , BIBREF21 generates questions from pre-defined question templates.",
        "type": "Document"
      },
      {
        "id": "3fcc3bd0-34a7-465a-9253-d631267d3868",
        "metadata": {
          "vector_store_key": "1905.08949-0",
          "chunk_id": 0,
          "document_id": "1905.08949",
          "start_idx": 0,
          "end_idx": 664
        },
        "page_content": "Question Generation (QG) concerns the task of \u201cautomatically generating questions from various inputs such as raw text, database, or semantic representation\" BIBREF0 . People have the ability to ask rich, creative, and revealing questions BIBREF1 ; e.g., asking Why did Gollum betray his master Frodo Baggins? after reading the fantasy novel The Lord of the Rings. How can machines be endowed with the ability to ask relevant and to-the-point questions, given various inputs? This is a challenging, complementary task to Question Answering (QA). Both QA and QG require an in-depth understanding of the input source and the ability to reason over relevant contexts.",
        "type": "Document"
      },
      {
        "id": "b7c14830-d91b-4529-ba74-161a132be57a",
        "metadata": {
          "vector_store_key": "1703.04617-2",
          "chunk_id": 27,
          "document_id": "1703.04617",
          "start_idx": 15206,
          "end_idx": 16112
        },
        "page_content": "We start from a simple way to first analyze the word frequency of all questions, and obtain top-10 most frequent question types: what, how, who, when, which, where, why, be, whose, and whom, in which be stands for the questions beginning with different forms of the word be such as is, am, and are. We explicitly encode question-type information to be an 11-dimensional one-hot vector (the top-10 question types and \"other\" question type). Each question type is with a trainable embedding vector. We call this explicit question type code, $\\mathbf {ET}\\in \\mathbb {R} ^{d_{ET}}$ . Then the vector for each question type is tuned during training, and is added to the system with the following equation:  $$\\mathbf {I}_{new}=[\\mathbf {I}, repmat(\\mathbf {ET})]$$   (Eq. 38)  As discussed, different types of questions and their answers may share common regularity and have separate property at the same time.",
        "type": "Document"
      },
      {
        "id": "3b6e3a1c-c567-46d6-8ad9-00c9497e5a59",
        "metadata": {
          "vector_store_key": "1905.08949-0",
          "chunk_id": 4,
          "document_id": "1905.08949",
          "start_idx": 2212,
          "end_idx": 2877
        },
        "page_content": "For the sake of clean exposition, we first provide a broad overview of QG by conceptualizing the problem from the perspective of the three introduced aspects: (1) its learning paradigm, (2) its input modalities, and (3) the cognitive level it involves. This combines past research with recent trends, providing insights on how NQG connects to traditional QG research. QG research traditionally considers two fundamental aspects in question asking: \u201cWhat to ask\u201d and \u201cHow to ask\u201d. A typical QG task considers the identification of the important aspects to ask about (\u201cwhat to ask\u201d), and learning to realize such identified aspects as natural language (\u201chow to ask\u201d).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do this framework facilitate demographic inference from social media?",
    "llm_answer": "The framework uses supervised models with features like screen names, profile descriptions, content from external resources, emoticons, online behaviors, and life stage information.  It also incorporates visual imagery (profile and shared images) and textual content (weighted lexicons of terms) for demographic prediction.  Analysis of language cues and homophily interaction information are also employed.\n",
    "context": [
      {
        "id": "230a5d4c-019d-4664-8356-8b93baf2dd26",
        "metadata": {
          "vector_store_key": "1902.06843-0",
          "chunk_id": 18,
          "document_id": "1902.06843",
          "start_idx": 10508,
          "end_idx": 11372
        },
        "page_content": "Demographic information inference on Social Media:  There is a growing interest in understanding online user's demographic information due to its numerous applications in healthcare BIBREF36 , BIBREF37 . A supervised model developed by BIBREF38 for determining users' gender by employing features such as screen-name, full-name, profile description and content on external resources (e.g., personal blog). Employing features including emoticons, acronyms, slangs, punctuations, capitalization, sentence length and included links/images, along with online behaviors such as number of friends, post time, and commenting activity, a supervised model was built for predicting user's age group BIBREF39 . Utilizing users life stage information such as secondary school student, college student, and employee, BIBREF40 builds age inference model for Dutch Twitter users.",
        "type": "Document"
      },
      {
        "id": "d3117fd0-cfba-44ed-a165-9bf1029043e6",
        "metadata": {
          "vector_store_key": "1902.06843-0",
          "chunk_id": 19,
          "document_id": "1902.06843",
          "start_idx": 11372,
          "end_idx": 12074
        },
        "page_content": "Utilizing users life stage information such as secondary school student, college student, and employee, BIBREF40 builds age inference model for Dutch Twitter users. Similarly, relying on profile descriptions while devising a set of rules and patterns, a novel model introduced for extracting age for Twitter users BIBREF41 . They also parse description for occupation by consulting the SOC2010 list of occupations and validating it through social surveys. A novel age inference model was developed while relying on homophily interaction information and content for predicting age of Twitter users BIBREF42 . The limitations of textual content for predicting age and gender was highlighted by BIBREF43 .",
        "type": "Document"
      },
      {
        "id": "31bbfc96-19bb-49ad-b765-79d5676aced6",
        "metadata": {
          "vector_store_key": "1705.09665-4",
          "chunk_id": 8,
          "document_id": "1705.09665",
          "start_idx": 4723,
          "end_idx": 5582
        },
        "page_content": "(Section SECREF5 ). Interestingly, while established members of distinctive communities more avidly respond to temporal updates than newcomers, in more generic communities it is the outsiders who engage more with volatile content, perhaps suggesting that such content may serve as an entry-point to the community (but not necessarily a reason to stay). Such insights into the relation between collective identity and user engagement can be informative to community maintainers seeking to better understand growth patterns within their online communities. More generally, our methodology stands as an example of how sociological questions can be addressed in a multi-community setting. In performing our analyses across a rich variety of communities, we reveal both the diversity of phenomena that can occur, as well as the systematic nature of this diversity.",
        "type": "Document"
      },
      {
        "id": "452f4d35-484b-43b6-8b14-5286380af3bb",
        "metadata": {
          "vector_store_key": "1902.06843-3",
          "chunk_id": 50,
          "document_id": "1902.06843",
          "start_idx": 28427,
          "end_idx": 29203
        },
        "page_content": "Prediction with Visual Imagery: Inspired by BIBREF56 's approach for facial landmark localization, we use their pretrained CNN consisting of convolutional layers, including unshared and fully-connected layers, to predict gender and age from both the profile and shared images. We evaluate the performance for gender and age prediction task on INLINEFORM0 and INLINEFORM1 respectively as shown in Table TABREF42 and Table TABREF44 . Demographic Prediction Analysis: We delve deeper into the benefits and drawbacks of each data modality for demographic information prediction. This is crucial as the differences between language cues between age groups above age 35 tend to become smaller (see Figure FIGREF39 -A,B,C) and making the prediction harder for older people BIBREF74 .",
        "type": "Document"
      },
      {
        "id": "aa10d06d-ecaa-447f-9817-d25b51d3e228",
        "metadata": {
          "vector_store_key": "1902.06843-3",
          "chunk_id": 47,
          "document_id": "1902.06843",
          "start_idx": 27159,
          "end_idx": 27966
        },
        "page_content": "*** alpha = 0.001, ** alpha = 0.01, * alpha = 0.05 We leverage both the visual and textual content for predicting age and gender. Prediction with Textual Content: We employ BIBREF73 's weighted lexicon of terms that uses the dataset of 75,394 Facebook users who shared their status, age and gender. The predictive power of this lexica was evaluated on Twitter, blog, and Facebook, showing promising results BIBREF73 . Utilizing these two weighted lexicon of terms, we are predicting the demographic information (age or gender) of INLINEFORM0 (denoted by INLINEFORM1 ) using following equation: INLINEFORM2  where INLINEFORM0 is the lexicon weight of the term, and INLINEFORM1 represents the frequency of the term in the user generated INLINEFORM2 , and INLINEFORM3 measures total word count in INLINEFORM4 .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the data annotated?",
    "llm_answer": "Workers made dialogues, received feedback, and then were paired to converse.  Workers annotated user and system states.  Rules were used for automatic dialogue act annotation based on states and history.  Three experts manually annotated 50 dialogues to evaluate automatic annotation quality.  The scheme was modified to include categories like Joy, Sadness, Anger, Fear, etc., and a binary label for semantic tuples.  Three experts also manually annotated dialogue acts and states for 50 dialogues (806 utterances) in the modified scheme.\n",
    "context": [
      {
        "id": "24f8dfec-c722-47c3-89b3-12384c534e78",
        "metadata": {
          "vector_store_key": "2002.11893-9",
          "chunk_id": 14,
          "document_id": "2002.11893",
          "start_idx": 8370,
          "end_idx": 9067
        },
        "page_content": "Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states. Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three experts were employed to manually annotate dialogue acts and states for 50 dialogues. The results show that our annotations are of high quality.",
        "type": "Document"
      },
      {
        "id": "35f1bb75-de90-4ab3-bf26-255e60b9b505",
        "metadata": {
          "vector_store_key": "1910.11769-0",
          "chunk_id": 5,
          "document_id": "1910.11769",
          "start_idx": 3245,
          "end_idx": 3978
        },
        "page_content": "Hence, we modified our annotation scheme by removing Trust and adding Love. We also added the Neutral category to denote passages that do not exhibit any emotional content. The final annotation categories for the dataset are: Joy, Sadness, Anger, Fear, Anticipation, Surprise, Love, Disgust, Neutral. We selected both classic and modern narratives in English for this dataset. The modern narratives were sampled based on popularity from Wattpad. We parsed selected narratives into passages, where a passage is considered to be eligible for annotation if it contained between 40 and 200 tokens. In long-form narratives, many non-conversational passages are intended for transition or scene introduction, and may not carry any emotion.",
        "type": "Document"
      },
      {
        "id": "b72b6824-2157-4377-8af7-f812f214d7a9",
        "metadata": {
          "vector_store_key": "2002.11893-9",
          "chunk_id": 31,
          "document_id": "2002.11893",
          "start_idx": 18005,
          "end_idx": 18636
        },
        "page_content": "We also obtained a binary label for each semantic tuple in the user state, which indicates whether this semantic tuple has been selected to be expressed by the user. This annotation directly illustrates the progress of the conversation. To evaluate the quality of the annotation of dialogue acts and states (both user and system states), three experts were employed to manually annotate dialogue acts and states for the same 50 dialogues (806 utterances), 10 for each goal type (see Section SECREF4). Since dialogue act annotation is not a classification problem, we didn't use Fleiss' kappa to measure the agreement among experts.",
        "type": "Document"
      },
      {
        "id": "3ac67028-d2cf-4f6d-9474-8d919ec7df01",
        "metadata": {
          "vector_store_key": "1911.07228-4",
          "chunk_id": 25,
          "document_id": "1911.07228",
          "start_idx": 12613,
          "end_idx": 13553
        },
        "page_content": "After considering the training and test data, we realized that this data has many problems need to be fixed in the next run experiments. The annotators are not consistent between the training data and the test data, more details are shown as follow: The organizations are labeled in the train data but not labeled in the test data: Training data: vietnam\u27e8ORG\u27e9 S\u1edf Y_t\u1ebf \u27e8ORG\u27e9 (Department of Health) Test data: vietnamS\u1edf Y_t\u1ebf (Department of Health) Explanation: vietnam\"S\u1edf Y_t\u1ebf\" in train and test are the same name of organization entity. However the one in test data is not labeled. The entity has the same meaning but is assigned differently between the train data and the test: Training data: vietnam\u27e8MISC\u27e9 ng\u01b0\u1eddi Vi\u1ec7t \u27e8MISC\u27e9 (Vietnamese people) Test data: vietnamd\u00e2n \u27e8LOC\u27e9 Vi\u1ec7t \u27e8LOC\u27e9 (Vietnamese people) Explanation: vietnamBoth \"ng\u01b0\u1eddi Vi\u1ec7t\" in train data and \"d\u00e2n Vi\u1ec7t\" in test data are the same meaning, but they are assigned differently.",
        "type": "Document"
      },
      {
        "id": "6e5f8a60-4570-4372-9ceb-97776f851901",
        "metadata": {
          "vector_store_key": "1909.00430-2",
          "chunk_id": 0,
          "document_id": "1909.00430",
          "start_idx": 0,
          "end_idx": 709
        },
        "page_content": "Data annotation is a key bottleneck in many data driven algorithms. Specifically, deep learning models, which became a prominent tool in many data driven tasks in recent years, require large datasets to work well. However, many tasks require manual annotations which are relatively hard to obtain at scale. An attractive alternative is lightly supervised learning BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , in which the objective function is supplemented by a set of domain-specific soft-constraints over the model's predictions on unlabeled data. For example, in label regularization BIBREF0 the model is trained to fit the true label proportions of an unlabeled dataset.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Where does the information on individual-level demographics come from?",
    "llm_answer": "User-generated content (screen names, full names, profile descriptions, content on external resources, emoticons, acronyms, slangs, punctuation, capitalization, sentence length, links/images), online behavior (number of friends, post time, commenting activity), and life stage information (secondary school, college, employee).  Also, self-reported data from Facebook users.\n",
    "context": [
      {
        "id": "230a5d4c-019d-4664-8356-8b93baf2dd26",
        "metadata": {
          "vector_store_key": "1902.06843-0",
          "chunk_id": 18,
          "document_id": "1902.06843",
          "start_idx": 10508,
          "end_idx": 11372
        },
        "page_content": "Demographic information inference on Social Media:  There is a growing interest in understanding online user's demographic information due to its numerous applications in healthcare BIBREF36 , BIBREF37 . A supervised model developed by BIBREF38 for determining users' gender by employing features such as screen-name, full-name, profile description and content on external resources (e.g., personal blog). Employing features including emoticons, acronyms, slangs, punctuations, capitalization, sentence length and included links/images, along with online behaviors such as number of friends, post time, and commenting activity, a supervised model was built for predicting user's age group BIBREF39 . Utilizing users life stage information such as secondary school student, college student, and employee, BIBREF40 builds age inference model for Dutch Twitter users.",
        "type": "Document"
      },
      {
        "id": "452f4d35-484b-43b6-8b14-5286380af3bb",
        "metadata": {
          "vector_store_key": "1902.06843-3",
          "chunk_id": 50,
          "document_id": "1902.06843",
          "start_idx": 28427,
          "end_idx": 29203
        },
        "page_content": "Prediction with Visual Imagery: Inspired by BIBREF56 's approach for facial landmark localization, we use their pretrained CNN consisting of convolutional layers, including unshared and fully-connected layers, to predict gender and age from both the profile and shared images. We evaluate the performance for gender and age prediction task on INLINEFORM0 and INLINEFORM1 respectively as shown in Table TABREF42 and Table TABREF44 . Demographic Prediction Analysis: We delve deeper into the benefits and drawbacks of each data modality for demographic information prediction. This is crucial as the differences between language cues between age groups above age 35 tend to become smaller (see Figure FIGREF39 -A,B,C) and making the prediction harder for older people BIBREF74 .",
        "type": "Document"
      },
      {
        "id": "4eb26366-24a5-4cab-abff-14d0bc2db963",
        "metadata": {
          "vector_store_key": "1705.09665-4",
          "chunk_id": 22,
          "document_id": "1705.09665",
          "start_idx": 12536,
          "end_idx": 13165
        },
        "page_content": "They also vary along a rich multitude of structural dimensions, such as the number of users, the amount of conversation and social interaction, and the social norms determining which types of content become popular. The diversity and scope of Reddit's multicommunity ecosystem make it an ideal landscape in which to closely examine the relation between varying community identities and social dynamics. Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history.",
        "type": "Document"
      },
      {
        "id": "aa10d06d-ecaa-447f-9817-d25b51d3e228",
        "metadata": {
          "vector_store_key": "1902.06843-3",
          "chunk_id": 47,
          "document_id": "1902.06843",
          "start_idx": 27159,
          "end_idx": 27966
        },
        "page_content": "*** alpha = 0.001, ** alpha = 0.01, * alpha = 0.05 We leverage both the visual and textual content for predicting age and gender. Prediction with Textual Content: We employ BIBREF73 's weighted lexicon of terms that uses the dataset of 75,394 Facebook users who shared their status, age and gender. The predictive power of this lexica was evaluated on Twitter, blog, and Facebook, showing promising results BIBREF73 . Utilizing these two weighted lexicon of terms, we are predicting the demographic information (age or gender) of INLINEFORM0 (denoted by INLINEFORM1 ) using following equation: INLINEFORM2  where INLINEFORM0 is the lexicon weight of the term, and INLINEFORM1 represents the frequency of the term in the user generated INLINEFORM2 , and INLINEFORM3 measures total word count in INLINEFORM4 .",
        "type": "Document"
      },
      {
        "id": "31bbfc96-19bb-49ad-b765-79d5676aced6",
        "metadata": {
          "vector_store_key": "1705.09665-4",
          "chunk_id": 8,
          "document_id": "1705.09665",
          "start_idx": 4723,
          "end_idx": 5582
        },
        "page_content": "(Section SECREF5 ). Interestingly, while established members of distinctive communities more avidly respond to temporal updates than newcomers, in more generic communities it is the outsiders who engage more with volatile content, perhaps suggesting that such content may serve as an entry-point to the community (but not necessarily a reason to stay). Such insights into the relation between collective identity and user engagement can be informative to community maintainers seeking to better understand growth patterns within their online communities. More generally, our methodology stands as an example of how sociological questions can be addressed in a multi-community setting. In performing our analyses across a rich variety of communities, we reveal both the diversity of phenomena that can occur, as well as the systematic nature of this diversity.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the source of the user interaction data? ",
    "llm_answer": "A driving simulator.\n",
    "context": [
      {
        "id": "e9281199-73f6-498e-9895-e277e85ebfde",
        "metadata": {
          "vector_store_key": "1705.09665-4",
          "chunk_id": 54,
          "document_id": "1705.09665",
          "start_idx": 30603,
          "end_idx": 31369
        },
        "page_content": "Our language-based typology and analysis of user engagement draws on and contributes to several distinct research threads, in addition to the many foundational studies cited in the previous sections. Multicommunity studies. Our investigation of user engagement in multicommunity settings follows prior literature which has examined differences in user and community dynamics across various online groups, such as email listservs. Such studies have primarily related variations in user behaviour to structural features such as group size and volume of content BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 . In focusing on the linguistic content of communities, we extend this research by providing a content-based framework through which user engagement can be examined.",
        "type": "Document"
      },
      {
        "id": "734513e7-92a7-4bd8-b42f-45d6d3baa233",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 27,
          "document_id": "1909.02764",
          "start_idx": 15296,
          "end_idx": 15975
        },
        "page_content": "The emotion self-ratings from the participants yielded 90 utterances labeled with joy, 26 with annoyance, 49 with insecurity, 9 with boredom, 111 with relaxation and 3 with no emotion. One example interaction per interaction type and emotion is shown in Table TABREF7. For further experiments, we only use joy, annoyance/anger, and insecurity/fear due to the small sample size for boredom and no emotion and under the assumption that relaxation brings little expressivity. We preprocess the visual data by extracting the sequence of images for each interaction from the point where the agent's or the co-driver's question was completely uttered until the driver's response stops.",
        "type": "Document"
      },
      {
        "id": "8dc10416-60bf-407b-99e5-6c326e9c379f",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 28,
          "document_id": "1909.02764",
          "start_idx": 15585,
          "end_idx": 16227
        },
        "page_content": "We preprocess the visual data by extracting the sequence of images for each interaction from the point where the agent's or the co-driver's question was completely uttered until the driver's response stops. The average length is 16.3 seconds, with the minimum at 2.2s and the maximum at 54.7s. We apply an off-the-shelf tool for emotion recognition (the manufacturer cannot be disclosed due to licensing restrictions). It delivers frame-by-frame scores ($\\in [0;100]$) for discrete emotional states of joy, anger and fear. While joy corresponds directly to our annotation, we map anger to our label annoyance and fear to our label insecurity.",
        "type": "Document"
      },
      {
        "id": "f9abd7fc-544f-4709-b7f1-a18aaeadadbd",
        "metadata": {
          "vector_store_key": "1705.09665-4",
          "chunk_id": 1,
          "document_id": "1705.09665",
          "start_idx": 595,
          "end_idx": 1475
        },
        "page_content": "However, the sheer variety of online platforms complicates the task of generalizing insights beyond these isolated, single-community glimpses. A new way to reason about the variation across multiple communities is needed in order to systematically characterize the relationship between properties of a community and the dynamics taking place within. One especially important component of community dynamics is user engagement. We can aim to understand why users join certain communities BIBREF6 , what factors influence user retention BIBREF7 , and how users react to innovation BIBREF5 . While striking patterns of user engagement have been uncovered in prior case studies of individual communities BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , we do not know whether these observations hold beyond these cases, or when we can draw analogies between different communities.",
        "type": "Document"
      },
      {
        "id": "41985bf6-5b12-4da5-8b2c-fc2ee8c1ac22",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 18,
          "document_id": "1909.02764",
          "start_idx": 10160,
          "end_idx": 10873
        },
        "page_content": "We focus on the drivers' interactions with both a virtual agent as well as a co-driver. To collect the data in a safe and controlled environment and to be able to consider a variety of predefined driving situations, the study was conducted in a driving simulator. The study environment consists of a fixed-base driving simulator running Vires's VTD (Virtual Test Drive, v2.2.0) simulation software (https://vires.com/vtd-vires-virtual-test-drive/). The vehicle has an automatic transmission, a steering wheel and gas and brake pedals. We collect data from video, speech and biosignals (Empatica E4 to record heart rate, electrodermal activity, skin temperature, not further used in this paper) and questionnaires.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the source of the textual data? ",
    "llm_answer": "English Wiktionary, Stack Overflow, SemEval-2016 (Laptop and Restaurant datasets), electronic health records (EHRs).\n",
    "context": [
      {
        "id": "2f8e9d5b-32b1-4b15-bfdb-c136c904ee9c",
        "metadata": {
          "vector_store_key": "1910.11493-2",
          "chunk_id": 9,
          "document_id": "1910.11493",
          "start_idx": 5320,
          "end_idx": 6006
        },
        "page_content": "Data for all but four languages (Basque, Kurmanji, Murrinhpatha, and Sorani) are extracted from English Wiktionary, a large multi-lingual crowd-sourced dictionary with morphological paradigms for many lemmata. 20 of the 100 language pairs are either distantly related or unrelated; this allows speculation into the relative importance of data quantity and linguistic relatedness. For each language, the basic data consists of triples of the form (lemma, feature bundle, inflected form), as in tab:sub1data. The first feature in the bundle always specifies the core part of speech (e.g., verb). For each language pair, separate files contain the high- and low-resource training examples.",
        "type": "Document"
      },
      {
        "id": "d7c1f42d-1bcb-4a33-bcc1-9a938c802756",
        "metadata": {
          "vector_store_key": "1908.06606-2",
          "chunk_id": 17,
          "document_id": "1908.06606",
          "start_idx": 10326,
          "end_idx": 11109
        },
        "page_content": "As shown in Fig. FIGREF8, paragraph text $X$ is first passed to a clinical named entity recognition (CNER) model BIBREF12 to capture named entity information and obtain one-hot CNER output tagging sequence for query text $I_{nq}$ and paragraph text $I_{nt}$ with BIEOS (Begin, Inside, End, Outside, Single) tag scheme. $I_{nq}$ and $I_{nt}$ are then integrated together into $I_n$. Meanwhile, the paragraph text $X$ and query text $Q$ are organized and passed to contextualized representation model which is pre-trained language model BERT BIBREF26 here to obtain the contextualized representation vector $V_s$ of both text and query. Afterwards, $V_s$ and $I_n$ are integrated together and fed into a feed forward network to calculate the start and end index of answer-related text.",
        "type": "Document"
      },
      {
        "id": "c337fd5e-d0a7-4693-a4cf-bc50e47df7e4",
        "metadata": {
          "vector_store_key": "1908.06606-2",
          "chunk_id": 0,
          "document_id": "1908.06606",
          "start_idx": 0,
          "end_idx": 734
        },
        "page_content": "Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained. It is important to extract structured data from clinical text because bio-medical systems or bio-medical researches greatly rely on structured data but they cannot obtain them directly. In addition, clinical text often contains abundant healthcare information. CTS is able to provide large-scale extracted structured data for enormous down-stream clinical researches.",
        "type": "Document"
      },
      {
        "id": "50963b6c-1085-4834-ac26-c110699e630f",
        "metadata": {
          "vector_store_key": "1707.03904-4",
          "chunk_id": 16,
          "document_id": "1707.03904",
          "start_idx": 8622,
          "end_idx": 9268
        },
        "page_content": "For Quasar-S, the pool of text for each question was composed of 50+ question-and-answer threads scraped from http://stackoverflow.com. StackOverflow keeps a running tally of the top-voted questions for each tag in their knowledge base; we used Scrapy to pull the top 50 question posts for each tag, along with any answer-post responses and metadata (tags, authorship, comments). From each thread we pulled all text not marked as code, and split it into sentences using the Stanford NLP sentence segmenter, truncating sentences to 2048 characters. Each sentence was marked with a thread identifier, a post identifier, and the tags for the thread.",
        "type": "Document"
      },
      {
        "id": "1b887290-2c77-48b6-b1a2-fc513d707afc",
        "metadata": {
          "vector_store_key": "1909.00124-1",
          "chunk_id": 20,
          "document_id": "1909.00124",
          "start_idx": 11467,
          "end_idx": 12190
        },
        "page_content": "The other two datasets are laptop and restaurant datasets collected from SemEval-2016 . The former consists of laptop review sentences and the latter consists of restaurant review sentences. The original datasets (i.e., Laptop and Restaurant) were annotated with aspect polarity in each sentence. We used all sentences with only one polarity (positive or negative) for their aspects. That is, we only used sentences with aspects having the same sentiment label in each sentence. Thus, the sentiment of each aspect gives the ground-truth as the sentiments of all aspects are the same. For each clean-labeled dataset, the sentences are randomly partitioned into training set and test set with $80\\%$ and $20\\%$, respectively.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the source of the visual data? ",
    "llm_answer": "Image data captured from photo-ready devices (like smartphones) and online social media services.\n",
    "context": [
      {
        "id": "7225db03-4a16-456f-9c96-dbbdb7a44f60",
        "metadata": {
          "vector_store_key": "1910.09399-5",
          "chunk_id": 93,
          "document_id": "1910.09399",
          "start_idx": 49776,
          "end_idx": 50570
        },
        "page_content": "In order to convert the visual scene graph as input for GAN to generate images, this method uses graph convolution to process input graphs. It computes a scene layout by predicting bounding boxes and segmentation masks for objects. After that, it converts the computed layout to an image with a cascaded re\ufb01nement network. black Instead of focusing on generating static images, another line of text-to-image synthesis research focuses on generating videos (i.e. sequences of images) from texts. In this context, the synthesised videos are often useful resources for automated assistance or story telling. black One early/interesting work of motion enhancement GANs is to generate spoofed speech and lip-sync videos (or talking face) of Barack Obama (i.e. ObamaNet) based on text input BIBREF62.",
        "type": "Document"
      },
      {
        "id": "6ce711af-7218-4321-8a60-94af8420689b",
        "metadata": {
          "vector_store_key": "1910.09399-2",
          "chunk_id": 104,
          "document_id": "1910.09399",
          "start_idx": 55502,
          "end_idx": 56273
        },
        "page_content": "Though flowers and birds are the most common objects studied thus far, research has been applied to other classes as well. For example, there have been studies focused solely on human faces BIBREF7, BIBREF8, BIBREF71, BIBREF72. It\u2019s a fascinating time for computer vision AI and deep learning researchers and enthusiasts. The consistent advancement in hardware, software, and contemporaneous development of computer vision AI research disrupts multiple industries. These advances in technology allow for the extraction of several data types from a variety of sources. For example, image data captured from a variety of photo-ready devices, such as smart-phones, and online social media services opened the door to the analysis of large amounts of media datasets BIBREF70.",
        "type": "Document"
      },
      {
        "id": "b6344922-6553-41ba-aafb-fc1a5d4b77bd",
        "metadata": {
          "vector_store_key": "1905.00563-4",
          "chunk_id": 37,
          "document_id": "1905.00563",
          "start_idx": 18487,
          "end_idx": 19281
        },
        "page_content": "Examples Sample adversarial attacks are provided in Table 5 . attacks mostly try to change the type of the target triple's object by associating it with a subject and a relation that require a different entity types. To be able to understand and interpret why a link is predicted using the opaque, dense embeddings, we need to find out which part of the graph was most influential on the prediction. To provide such explanations for each predictions, we identify the most influential fact using . Instead of focusing on individual predictions, we aggregate the explanations over the whole dataset for each relation using a simple rule extraction technique: we find simple patterns on subgraphs that surround the target triple and the removed fact from , and appear more than $90\\%$ of the time.",
        "type": "Document"
      },
      {
        "id": "8e25fd91-f728-42c1-80ad-c7a71e12ac30",
        "metadata": {
          "vector_store_key": "1910.03814-3",
          "chunk_id": 12,
          "document_id": "1910.03814",
          "start_idx": 6797,
          "end_idx": 7432
        },
        "page_content": "On the other hand, instead of explicitly learning an alignment between two spaces, the goal of Visual Question Answering (VQA) is to merge both data modalities in order to decide which answer is correct. This problem requires modeling very precise correlations between the image and the question representations. The VQA task requirements are similar to our hate speech detection problem in multimodal publications, where we have a visual and a textual input and we need to combine both sources of information to understand the global context and make a decision. We thus take inspiration from the VQA literature for the tested models.",
        "type": "Document"
      },
      {
        "id": "c1419b87-6f75-442f-bf7f-ebc29e273146",
        "metadata": {
          "vector_store_key": "1909.08859-4",
          "chunk_id": 7,
          "document_id": "1909.08859",
          "start_idx": 4102,
          "end_idx": 4707
        },
        "page_content": "The visual ordering task is about grasping the temporal flow of visual events with the help of the given recipe text. The questions show a set of four images from the recipe and the task is to sort jumbled images into the correct order. Here, a model needs to infer the temporal relations between the images and align them with the recipe steps. In the following, we explain our Procedural Reasoning Networks model. Its architecture is based on a bi-directional attention flow (BiDAF) model BIBREF6, but also equipped with an explicit reasoning module that acts on entity-specific relational memory units.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What result from experiments suggest that natural language based agents are more robust?",
    "llm_answer": "Natural language representation achieved better performance, even on difficult tasks or tasks with task-nuisances.\n",
    "context": [
      {
        "id": "b44fd67c-85e4-45e1-b49d-864aedb527e5",
        "metadata": {
          "vector_store_key": "1809.08731-1",
          "chunk_id": 8,
          "document_id": "1809.08731",
          "start_idx": 4746,
          "end_idx": 5427
        },
        "page_content": "Their experiments showed a strong correlation between human judgments and normalized sentence log-probabilities under a variety of LMs for artificial data they had created by translating and back-translating sentences with neural models. While they tried different types of LMs, best results were obtained for neural models, namely recurrent neural networks (RNNs). In this work, we investigate if approaches which have proven successful for modeling acceptability can be applied to the NLP problem of automatic fluency evaluation. In this section, we first describe SLOR and the intuition behind this score. Then, we introduce WordPieces, before explaining how we combine the two.",
        "type": "Document"
      },
      {
        "id": "ab04d509-d1ef-4b8f-8693-e70aae36eedd",
        "metadata": {
          "vector_store_key": "1807.03367-1",
          "chunk_id": 47,
          "document_id": "1807.03367",
          "start_idx": 27302,
          "end_idx": 28211
        },
        "page_content": "In this section, we describe the findings of various experiments. First, we analyze how much information needs to be communicated for accurate localization in the Talk The Walk environment, and find that a short random path (including actions) is necessary. Next, for emergent language, we show that the MASC architecture can achieve very high localization accuracy, significantly outperforming the baseline that does not include this mechanism. We then turn our attention to the natural language experiments, and find that localization from human utterances is much harder, reaching an accuracy level that is below communicating a single landmark observation. We show that generated utterances from a conditional language model leads to significantly better localization performance, by successfully grounding the utterance on a single landmark observation (but not yet on multiple observations and actions).",
        "type": "Document"
      },
      {
        "id": "c217f9ff-4bc9-4f1c-93fa-0201728d6c27",
        "metadata": {
          "vector_store_key": "1910.02789-9",
          "chunk_id": 5,
          "document_id": "1910.02789",
          "start_idx": 3167,
          "end_idx": 4089
        },
        "page_content": "Moreover, our results indicate that natural language is a strong alternative to current complementary methods for semantic representations of a state. In this work we assume a state can be described using natural language sentences. We use distributional embedding methods in order to represent sentences, processed with a standard Convolutional Neural Network for feature extraction. In Section SECREF2 we describe the basic frameworks we rely on. We discuss possible semantic representations in Section SECREF3, namely, raw visual inputs, semantic segmentation, feature vectors, and natural language representations. Then, in Section SECREF4 we compare NLP representations with their alternatives. Our results suggest that representation of the state using natural language can achieve better performance, even on difficult tasks, or tasks in which the description of the state is saturated with task-nuisances BIBREF17.",
        "type": "Document"
      },
      {
        "id": "b12d51a4-a467-42ce-ae1e-983e1f8bd890",
        "metadata": {
          "vector_store_key": "1807.03367-1",
          "chunk_id": 53,
          "document_id": "1807.03367",
          "start_idx": 31165,
          "end_idx": 32051
        },
        "page_content": "We also show that MASC outperform no-MASC models for natural language communication. paragraph4 0.1ex plus0.1ex minus.1ex-1em Generated utterances We also investigate generated tourist utterances from conditional language models. Interestingly, we observe that the supervised model (with greedy and beam-search decoding) as well as the policy gradient model leads to an improvement of more than 10 accuracy points over the human utterances. However, their level of accuracy is slightly below the baseline of communicating a single observation, indicating that these models only learn to ground utterances in a single landmark observation. paragraph4 0.1ex plus0.1ex minus.1ex-1em Better grounding of generated utterances We analyze natural language samples in Table TABREF38 , and confirm that, unlike human utterances, the generated utterances are talking about the observed landmarks.",
        "type": "Document"
      },
      {
        "id": "4703c0b0-284f-4e5d-aede-1db00b27e67b",
        "metadata": {
          "vector_store_key": "1910.02789-1",
          "chunk_id": 28,
          "document_id": "1910.02789",
          "start_idx": 14814,
          "end_idx": 15597
        },
        "page_content": "We converted the parsed state into embedded representations of fixed length. We tested both a DQN and a PPO based agent, and compared the natural language representation to the other representation techniques, namely the raw image, feature vector, and semantic segmentation representations. In order to effectively compare the performance of the different representation methods, we conducted our experiments under similar conditions for all agents. The same hyper-parameters were used under all tested representations. Moreover, to rule out effects of architectural expressiveness, the number of weights in all neural networks was approximately matched, regardless of the input type. Finally, we ensured the \u201csuper\" scenario was positively biased toward image-based representations.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which datasets are used in the paper?",
    "llm_answer": "Image datasets:  CUB, MSCOCO, Oxford-102, MNIST, CUB, Oxford, COCO, CIFAR-10.\nQuestion Answering datasets: Stanford Question Answering Dataset (SQuAD), MCTest, CNN/Daily Mail.\nOther datasets: BioASQ challenge data, SearchSnippets, StackOverflow, Chinese dataset from Tencent News.\n",
    "context": [
      {
        "id": "1e2bf238-b0a0-40af-9d8d-e6fea4ad79d0",
        "metadata": {
          "vector_store_key": "1910.09399-3",
          "chunk_id": 105,
          "document_id": "1910.09399",
          "start_idx": 56273,
          "end_idx": 57009
        },
        "page_content": "For example, image data captured from a variety of photo-ready devices, such as smart-phones, and online social media services opened the door to the analysis of large amounts of media datasets BIBREF70. The availability of large media datasets allow new frameworks and algorithms to be proposed and tested on real-world data. A summary of some reviewed methods and benchmark datasets used for validation is reported in Table TABREF43. In addition, the performance of different GANs with respect to the benchmark datasets and performance metrics is reported in Table TABREF48. In order to synthesize images from text descriptions, many frameworks have taken a minimalistic approach by creating small and background-less images BIBREF73.",
        "type": "Document"
      },
      {
        "id": "8ef64fd9-ae4b-416a-994a-f082e80e88e4",
        "metadata": {
          "vector_store_key": "1703.04617-2",
          "chunk_id": 2,
          "document_id": "1703.04617",
          "start_idx": 973,
          "end_idx": 1680
        },
        "page_content": "We then viewed and modelled different types of questions and the information shared among them as an adaptation problem and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results on our competitive baselines. Recent advance on reading comprehension and question answering has been closely associated with the availability of various datasets. BIBREF0 released the MCTest data consisting of 500 short, fictional open-domain stories and 2000 questions. The CNN/Daily Mail dataset BIBREF1 contains news articles for close style machine comprehension, in which only entities are removed and tested for comprehension.",
        "type": "Document"
      },
      {
        "id": "eac31fee-fbdc-4a80-be44-f2145c81562a",
        "metadata": {
          "vector_store_key": "1910.09399-3",
          "chunk_id": 106,
          "document_id": "1910.09399",
          "start_idx": 56647,
          "end_idx": 57443
        },
        "page_content": "In order to synthesize images from text descriptions, many frameworks have taken a minimalistic approach by creating small and background-less images BIBREF73. In most cases, the experiments were conducted on simple datasets, initially containing images of birds and flowers. BIBREF8 contributed to these data sets by adding corresponding natural language text descriptions to subsets of the CUB, MSCOCO, and Oxford-102 datasets, which facilitated the work on text-to-image synthesis for several papers released more recently. While most deep learning algorithms use MNIST BIBREF74 dataset as the benchmark, there are three main datasets that are commonly used for evaluation of proposed GAN models for text-to-image synthesis: CUB BIBREF75, Oxford BIBREF76, COCO BIBREF77, and CIFAR-10 BIBREF78.",
        "type": "Document"
      },
      {
        "id": "413f45ab-c88d-48ed-95ac-641283f82d18",
        "metadata": {
          "vector_store_key": "1701.00185-7",
          "chunk_id": 35,
          "document_id": "1701.00185",
          "start_idx": 19061,
          "end_idx": 19668
        },
        "page_content": "We use the challenge data published in BioASQ's official website. In our experiments, we randomly select 20, 000 paper titles from 20 different MeSH major topics as in Table TABREF25 . As described in Table TABREF24 , the max length of selected paper titles is 53. For these datasets, we randomly select 10% of data as the development set. Since SearchSnippets has been pre-processed by Phan et al. BIBREF41 , we do not further process this dataset. In StackOverflow, texts contain lots of computer terminology, and symbols and capital letters are meaningful, thus we do not do any pre-processed procedures.",
        "type": "Document"
      },
      {
        "id": "6acc5ad9-079c-46ec-9298-4912e7c2aae7",
        "metadata": {
          "vector_store_key": "1809.04960-1",
          "chunk_id": 23,
          "document_id": "1809.04960",
          "start_idx": 13537,
          "end_idx": 14236
        },
        "page_content": "Hence, the model is trained on both unpaired data INLINEFORM3 , and paired data INLINEFORM4 . We select a large-scale Chinese dataset BIBREF0 with millions of real comments and a human-annotated test set to evaluate our model. The dataset is collected from Tencent News, which is one of the most popular Chinese websites for news and opinion articles. The dataset consists of 198,112 news articles. Each piece of news contains a title, the content of the article, and a list of the users' comments. Following the previous work BIBREF0 , we tokenize all text with the popular python package Jieba, and filter out short articles with less than 30 words in content and those with less than 20 comments.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they quantify moral relevance?",
    "llm_answer": "They compute diachronic moral relevance scores using the probability of a concept being in a moral category given a word, and then perform a linear regression to measure the rate of change.  They also compare model predictions to human ratings of moral relevance and acceptability from the Pew Research Center's survey.\n",
    "context": [
      {
        "id": "83a24139-9e19-4688-9c00-c7a445f5df8c",
        "metadata": {
          "vector_store_key": "2001.07209-4",
          "chunk_id": 29,
          "document_id": "2001.07209",
          "start_idx": 16128,
          "end_idx": 16772
        },
        "page_content": "We selected the 10,000 nouns with highest total frequency in the 1800\u20131999 period according to data from BIBREF30, restricted to words labelled as nouns in WordNet BIBREF34 for validation. For each such word $\\mathbf {q}$, we computed diachronic moral relevance scores $R_i = p(c_1\\,|\\,\\mathbf {q}), i=1,\\ldots ,20$ for the 20 decades in our time span. Then, we performed a linear regression of $R$ on $T = 1,\\ldots ,n$ and took the fitted slope as a measure of moral relevance change. We repeated the same procedure for moral polarity. Finally, we removed words with average relevance score below $0.5$ to focus on morally relevant retrievals.",
        "type": "Document"
      },
      {
        "id": "9dc55b66-fe32-441f-8bb2-3af09001aa3e",
        "metadata": {
          "vector_store_key": "2001.07209-2",
          "chunk_id": 27,
          "document_id": "2001.07209",
          "start_idx": 14960,
          "end_idx": 15678
        },
        "page_content": "We used data from the Pew Research Center's 2013 Global Attitudes survey BIBREF33, in which participants from 40 countries judged 8 topics such as abortion and homosexuality as one of \u201cacceptable\", \u201cunacceptable\", and \u201cnot a moral issue\". We compared human ratings with model predictions at two tiers: for moral relevance, we paired the proportion of \u201cnot a moral issue\u201d human responses with irrelevance predictions $p(c_0\\,|\\,\\mathbf {q})$ for each topic, and for moral acceptability, we paired the proportion of \u201cacceptable\u201d responses with positive predictions $p(c_+\\,|\\,\\mathbf {q})$. We used 1990s word embeddings, and obtained predictions for two-word topics by querying the model with their averaged embeddings.",
        "type": "Document"
      },
      {
        "id": "3bc29877-9668-442a-959c-56b4b349fa92",
        "metadata": {
          "vector_store_key": "2001.07209-4",
          "chunk_id": 32,
          "document_id": "2001.07209",
          "start_idx": 17534,
          "end_idx": 18199
        },
        "page_content": "To test this hypothesis, we performed a multiple linear regression analysis on rate of change toward moral relevance of a large repertoire of words against concept concreteness ratings, word frequency BIBREF35, and word length BIBREF36. We obtained norms of concreteness ratings from BIBREF28. We collected the same set of high-frequency nouns as in the previous analysis, along with their fitted slopes of moral relevance change. Since we were interested in moral relevance change within this large set of words, we restricted our analysis to those words whose model predictions indicate change in moral relevance, in either direction, from the 1800s to the 1990s.",
        "type": "Document"
      },
      {
        "id": "e4599bd1-0d00-4a84-9d9e-0a5a8d2f498a",
        "metadata": {
          "vector_store_key": "2001.07209-2",
          "chunk_id": 30,
          "document_id": "2001.07209",
          "start_idx": 16477,
          "end_idx": 17272
        },
        "page_content": "Finally, we removed words with average relevance score below $0.5$ to focus on morally relevant retrievals. Table TABREF17 shows the words with steepest predicted change toward moral relevance, along with their predicted fine-grained moral categories in modern times (i.e., 1900\u20131999). Table TABREF18 shows the words with steepest predicted change toward the positive and negative moral poles. To further investigate the moral sentiment that may have led to such polarity shifts, we also show the predicted fine-grained moral categories of each word at its earliest time of predicted moral relevance and in modern times. Although we do not have access to ground truth for this application, these results offer initial insight into the historical moral landscape of the English language at scale.",
        "type": "Document"
      },
      {
        "id": "e6a1dcfb-0ab4-4e4f-abfe-95505bd3db5f",
        "metadata": {
          "vector_store_key": "2001.07209-2",
          "chunk_id": 26,
          "document_id": "2001.07209",
          "start_idx": 14161,
          "end_idx": 14960
        },
        "page_content": "On the other hand, predictions for slavery trend down and suggest a drop around the 1860s, coinciding with the American Civil War. We also observe changes in the dominant fine-grained moral categories, such as the perception of democracy as a fair concept, suggesting potential mechanisms behind the polarity changes and providing further insight into the public sentiment toward these concepts as evidenced by text. We explored the predictive potential of our framework by comparing model predictions with human judgments of moral relevance and acceptability. We used data from the Pew Research Center's 2013 Global Attitudes survey BIBREF33, in which participants from 40 countries judged 8 topics such as abortion and homosexuality as one of \u201cacceptable\", \u201cunacceptable\", and \u201cnot a moral issue\".",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much is proposed model better in perplexity and BLEU score than typical UMT models?",
    "llm_answer": "The answer is not found in the context.  While the text discusses improvements over a \"naive transformer\" and other models, it does not compare the proposed model to typical UMT (universal machine translation) models.\n",
    "context": [
      {
        "id": "4aed7ca1-6932-400c-a93a-5b86fd2c6d57",
        "metadata": {
          "vector_store_key": "1909.00279-2",
          "chunk_id": 29,
          "document_id": "1909.00279",
          "start_idx": 16745,
          "end_idx": 17553
        },
        "page_content": "According to experiment results, perplexity, BLEU scores and total scores in human evaluation are consistent with each other. We observe all BLEU scores are fairly low, we believe it is reasonable as there could be multiple ways to compose a poem given a vernacular paragraph. Among transformer-based models, both +Anti OT and +Anti UT outperforms the naive transformer, while Anti OT&UT shows the best performance, this demonstrates alleviating under-translation and over-translation both helps generate better poems. Specifically, +Anti UT shows bigger improvement than +Anti OT. According to human evaluation, among the four perspectives, our Anti OT&UT brought most score improvement in Semantic preservability, this proves our improvement on semantic preservability was most obvious to human evaluators.",
        "type": "Document"
      },
      {
        "id": "439bf15f-d8d4-423a-8658-32bd52679566",
        "metadata": {
          "vector_store_key": "1909.00279-2",
          "chunk_id": 28,
          "document_id": "1909.00279",
          "start_idx": 16135,
          "end_idx": 16905
        },
        "page_content": "(3)Transformer + Anti OT (RL loss); (4)Transformer + Anti UT (phrase segmentation-based padding); (5)Transformer + Anti OT&UT. As illustrated in Table TABREF12 (ID 1). Given the vernacular translation of each gold poem in test set, we generate five poems using our models. Intuitively, the more the generated poem resembles the gold poem, the better the model is. We report mean perplexity and BLEU scores in Table TABREF19 (Where +Anti OT refers to adding the reinforcement loss to mitigate over-fitting and +Anti UT refers to adding phrase segmentation-based padding to mitigate under-translation), human evaluation results in Table TABREF20. According to experiment results, perplexity, BLEU scores and total scores in human evaluation are consistent with each other.",
        "type": "Document"
      },
      {
        "id": "9dae7465-7177-49e6-8bdd-9f37d95d14e9",
        "metadata": {
          "vector_store_key": "1909.00279-2",
          "chunk_id": 25,
          "document_id": "1909.00279",
          "start_idx": 14613,
          "end_idx": 15254
        },
        "page_content": "Intuitively, a better model would yield higher probability (lower perplexity) on the gold poem. BLEU As a standard evaluation metric for machine translation, BLEU BIBREF18 measures the intersection of n-grams between the generated poem and the gold poem. A better generated poem usually achieves higher BLEU score, as it shares more n-gram with the gold poem. Human evaluation While perplexity and BLEU are objective metrics that could be applied to large-volume test set, evaluating Chinese poems is after all a subjective task. We invited 30 human evaluators to join our human evaluation. The human evaluators were divided into two groups.",
        "type": "Document"
      },
      {
        "id": "2e7cfe41-b41c-475c-83f6-e7d39903180d",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 40,
          "document_id": "1909.13375",
          "start_idx": 21052,
          "end_idx": 21792
        },
        "page_content": "While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1. All that, while keeping in mind we compare a base model to a large model (for reference, note the 8 point improvement between MTMSNbase and MTMSNlarge in both EM and F1). Our best model, large-squad, exhibits a huge improvement of 29.7 in EM and 15.1 in F1 compared to MTMSNlarge. When comparing single-span performance, our best model exhibits slightly better results, but it should be noted that it retains the single-span heads from NABERT+, while in MTMSN they have one head to predict both single-span and multi-span answers.",
        "type": "Document"
      },
      {
        "id": "f5dba2ac-6a54-48f8-ba6c-7864ece5409c",
        "metadata": {
          "vector_store_key": "2001.00137-3",
          "chunk_id": 36,
          "document_id": "2001.00137",
          "start_idx": 21142,
          "end_idx": 21770
        },
        "page_content": "The table also indicates the level of noise in each dataset with the already mentioned iBLEU score, where 0 means no noise and higher values mean higher quantity of noise. As expected, the models' accuracy degrade with the increase in noise, thus F1-scores of gtts-witai are higher than macsay-witai. However, while the other models decay rapidly in the presence of noise, our model does not only outperform them but does so with a wider margin. This is shown with the increasing robustness curve in Fig. FIGREF41 and can be demonstrated by macsay-witai outperforming the baseline models by twice the gap achieved by gtts-witai.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Do they train a different training method except from scheduled sampling?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "dfe8ebea-1ac7-43da-905c-92d6e9ea4047",
        "metadata": {
          "vector_store_key": "1908.08419-0",
          "chunk_id": 40,
          "document_id": "1908.08419",
          "start_idx": 23162,
          "end_idx": 23876
        },
        "page_content": "First, we employ CRF-based segmenter to annotate the unlabeled set. Then, sampling strategy in active learning selects a part of samples for annotators to relabel. Finally, the relabeled samples are added to train set for segmenter to re-train. Our proposed scoring strategy selects samples according to the sequence scores of the segmented sentences, while uncertainty sampling suggests relabeling samples that are closest to the segmenter\u2019s decision boundary. Generally, two main parameters in active learning are the numbers of iterations and samples selected per iteration. To fairly investigate the influence of two parameters, we compare our proposed strategy with uncertainty sampling on the same parameter.",
        "type": "Document"
      },
      {
        "id": "9cbb6262-c495-4bf1-962e-723ecfdbce15",
        "metadata": {
          "vector_store_key": "1908.08419-1",
          "chunk_id": 13,
          "document_id": "1908.08419",
          "start_idx": 7933,
          "end_idx": 8693
        },
        "page_content": "However, in some complicated tasks, such as CWS and NER, only considering the uncertainty of classifier is obviously not enough. Active learning methods can generally be described into two parts: a learning engine and a selection engine BIBREF28 . The learning engine is essentially a classifier, which is mainly used for training of classification problems. The selection engine is based on the sampling strategy, which chooses samples that need to be relabeled by annotators from unlabeled data. Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, a CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively.",
        "type": "Document"
      },
      {
        "id": "142c7b5a-b32f-4722-a07c-3833ff13a76d",
        "metadata": {
          "vector_store_key": "1909.00430-2",
          "chunk_id": 6,
          "document_id": "1909.00430",
          "start_idx": 2726,
          "end_idx": 3373
        },
        "page_content": "This ability seamlessly integrates into other semi-supervised schemes: we can use the XR loss on top of a pre-trained model to fine-tune the pre-trained representation to the target task, and we can also take the model trained using XR loss and plentiful data and fine-tune it to the target task using the available small-scale annotated data. In Section SECREF56 we explore these options and show that our XR framework improves the results also when applied on top of a pre-trained Bert-based model BIBREF9 . Finally, to make the XR framework applicable to large-scale deep-learning setups, we propose a stochastic batched approximation procedure",
        "type": "Document"
      },
      {
        "id": "36adf7ba-6ece-4ad5-84b3-0a38b137a4fc",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 16,
          "document_id": "1912.13109",
          "start_idx": 8070,
          "end_idx": 8831
        },
        "page_content": "The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we employed various operational techniques for optimizing on the learning process. These processes and its implementation details will be discussed later but they were learning rate decay, early stopping, model checkpointing and reducing learning rate on plateau.",
        "type": "Document"
      },
      {
        "id": "e967d0a5-321a-4fd7-8476-c0ef6c37a82b",
        "metadata": {
          "vector_store_key": "1908.08419-1",
          "chunk_id": 11,
          "document_id": "1908.08419",
          "start_idx": 6704,
          "end_idx": 7439
        },
        "page_content": "Unlike the above method, we propose an active learning approach for CWS in medical text, which combines information entropy with neural network to effectively reduce annotation cost. Active learning BIBREF22 mainly aims to ease the data collection process by automatically deciding which instances should be labeled by annotators to train a model as quickly and effectively as possible BIBREF23 . The sampling strategy plays a key role in active learning. In the past decade, the rapid development of active learning has resulted in various sampling strategies, such as uncertainty sampling BIBREF24 , query-by-committee BIBREF25 and information gain BIBREF26 . Currently, the most mainstream sampling strategy is uncertainty sampling.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they define upward and downward reasoning?",
    "llm_answer": "The context describes upward entailing inferences as those where adding a phrase to the premise strengthens the conclusion, and downward entailing inferences as those where adding a phrase to the premise weakens the conclusion.  However, a formal definition of upward and downward reasoning is not explicitly given.\n",
    "context": [
      {
        "id": "aaa46b0f-891f-473f-8944-67807f8d77e7",
        "metadata": {
          "vector_store_key": "1906.06448-3",
          "chunk_id": 45,
          "document_id": "1906.06448",
          "start_idx": 25576,
          "end_idx": 26321
        },
        "page_content": "$P$ : I saw a dog $H$ : I saw a dog or a small cat with green eyes  Such pragmatic factors would be one of the reasons why it is difficult to obtain downward inferences in naturally occurring texts. We introduced a large monotonicity entailment dataset, called MED. To illustrate the usefulness of MED, we tested state-of-the-art NLI models, and found that performance on the new test set was substantially worse for all state-of-the-art NLI models. In addition, the accuracy on downward inferences was inversely proportional to the one on upward inferences. An experiment with the data augmentation technique showed that accuracy on upward and downward inferences depends on the proportion of upward and downward inferences in the training set.",
        "type": "Document"
      },
      {
        "id": "75cc260c-36c5-489e-8c50-15c9362be84e",
        "metadata": {
          "vector_store_key": "1906.06448-3",
          "chunk_id": 44,
          "document_id": "1906.06448",
          "start_idx": 24512,
          "end_idx": 25166
        },
        "page_content": "$P$ : No racin' on the Range $H$ : No horse racing is allowed on the Range  One possible reason why there are few downward inferences is that certain pragmatic factors can block people to draw a downward inference. For instance, in the case of the inference problem in ( \"Discussion\" ), unless the added disjunct in $H$ , i.e., a small cat with green eyes, is salient in the context, it would be difficult to draw the conclusion $H$ from the premise $P$ . $P$ : I saw a dog $H$ : I saw a dog or a small cat with green eyes  Such pragmatic factors would be one of the reasons why it is difficult to obtain downward inferences in naturally occurring texts.",
        "type": "Document"
      },
      {
        "id": "159b58cf-9608-4450-becd-599146981eec",
        "metadata": {
          "vector_store_key": "1906.06448-0",
          "chunk_id": 11,
          "document_id": "1906.06448",
          "start_idx": 6667,
          "end_idx": 7433
        },
        "page_content": "To create monotonicity inference problems, we should satisfy three requirements: (a) detect the monotonicity operators and their arguments; (b) based on the syntactic structure, induce the polarity of the argument positions; and (c) replace the phrase in the argument position with a more general or specific phrase in natural and various ways (e.g., by using lexical knowledge or logical connectives). For (a) and (b), we first conduct polarity computation on a syntactic structure for each sentence, and then select premises involving upward/downward expressions. For (c), we use crowdsourcing to narrow or broaden the arguments. The motivation for using crowdsourcing is to collect naturally alike monotonicity inference problems that include various expressions.",
        "type": "Document"
      },
      {
        "id": "da280854-44f8-4ab5-83c8-ef7e9fabc8cb",
        "metadata": {
          "vector_store_key": "1906.06448-0",
          "chunk_id": 1,
          "document_id": "1906.06448",
          "start_idx": 741,
          "end_idx": 1427
        },
        "page_content": "Concerning logical inferences, monotonicity reasoning BIBREF6 , BIBREF7 , which is a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures. Consider examples in ( \"Introduction\" ) and ( \"Introduction\" ). All [ workers $\\leavevmode {\\color {blue!80!black}\\downarrow }$ ] [joined for a French dinner $\\leavevmode {\\color {red!80!black}\\uparrow }$ ] All workers joined for a dinner All new workers joined for a French dinner Not all [new workers $\\leavevmode {\\color {red!80!black}\\uparrow }$ ] joined for a dinner Not all workers joined for a dinner  A context is upward entailing (shown by [... $\\leavevmode",
        "type": "Document"
      },
      {
        "id": "83e50091-ee64-481e-80a5-543c3a67fd0d",
        "metadata": {
          "vector_store_key": "1906.06448-3",
          "chunk_id": 23,
          "document_id": "1906.06448",
          "start_idx": 13153,
          "end_idx": 13803
        },
        "page_content": "( UID16 ).  $P$ : The moon has no atmosphere $H$ : The moon has no atmosphere, and the gravity force is too low  The hypothesis $H$ was created by asking workers to make atmosphere in the premise $P$ more specific. However, the additional phrase and the gravity force is too low does not form constituents with atmosphere. Thus, such examples are not strict downward monotone inferences. In such cases as (a) and (b), we do not swap the premise and the hypothesis. In the end, we collected 4,068 examples from crowdsourced datasets. We also collect monotonicity inference problems from previous manually curated datasets and linguistics publications.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Do they annotate their own dataset or use an existing one?",
    "llm_answer": "They annotate their own dataset for some tasks (e.g., software questions, dialogue acts, and entity linking), and use existing public datasets (e.g., AIDA, Wikipedia) for other tasks.\n",
    "context": [
      {
        "id": "a7fe717f-04bc-4708-96e2-e0d3744977b2",
        "metadata": {
          "vector_store_key": "1707.03904-4",
          "chunk_id": 9,
          "document_id": "1707.03904",
          "start_idx": 5317,
          "end_idx": 6034
        },
        "page_content": "We also collect annotations on a subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short. We plan to release these annotations along with the datasets, and our retrieved documents for each question. Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated for each Quasar variant. The software question set was built from the definitional \u201cexcerpt\u201d entry for each tag (entity) on StackOverflow.",
        "type": "Document"
      },
      {
        "id": "24f8dfec-c722-47c3-89b3-12384c534e78",
        "metadata": {
          "vector_store_key": "2002.11893-9",
          "chunk_id": 14,
          "document_id": "2002.11893",
          "start_idx": 8370,
          "end_idx": 9067
        },
        "page_content": "Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states. Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three experts were employed to manually annotate dialogue acts and states for 50 dialogues. The results show that our annotations are of high quality.",
        "type": "Document"
      },
      {
        "id": "15a540b8-4836-48dd-b52f-f65d133d26da",
        "metadata": {
          "vector_store_key": "1810.06743-0",
          "chunk_id": 1,
          "document_id": "1810.06743",
          "start_idx": 860,
          "end_idx": 1674
        },
        "page_content": "On a dataset-by-dataset basis, they incorporate annotator errors, omissions, and human decisions when the schemata are underspecified; one such example is in fig:disagreement. A dataset-by-dataset problem demands a dataset-by-dataset solution; our task is not to translate a schema, but to translate a resource. Starting from the idealized schema, we create a rule-based tool for converting UD-schema annotations to UniMorph annotations, incorporating language-specific post-edits that both correct infelicities and also increase harmony between the datasets themselves (rather than the schemata). We apply this conversion to the 31 languages with both UD and UniMorph data, and we report our method's recall, showing an improvement over the strategy which just maps corresponding schematic features to each other.",
        "type": "Document"
      },
      {
        "id": "6e5f8a60-4570-4372-9ceb-97776f851901",
        "metadata": {
          "vector_store_key": "1909.00430-2",
          "chunk_id": 0,
          "document_id": "1909.00430",
          "start_idx": 0,
          "end_idx": 709
        },
        "page_content": "Data annotation is a key bottleneck in many data driven algorithms. Specifically, deep learning models, which became a prominent tool in many data driven tasks in recent years, require large datasets to work well. However, many tasks require manual annotations which are relatively hard to obtain at scale. An attractive alternative is lightly supervised learning BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , in which the objective function is supplemented by a set of domain-specific soft-constraints over the model's predictions on unlabeled data. For example, in label regularization BIBREF0 the model is trained to fit the true label proportions of an unlabeled dataset.",
        "type": "Document"
      },
      {
        "id": "458b51fc-c0d4-482d-90b3-d4cefed66ac3",
        "metadata": {
          "vector_store_key": "1902.00330-3",
          "chunk_id": 37,
          "document_id": "1902.00330",
          "start_idx": 20853,
          "end_idx": 21565
        },
        "page_content": "We implement our models in Tensorflow and run experiments on 4 Tesla V100 GPU. We conduct experiments on several different types of public datasets including news and encyclopedia corpus. The training set is AIDA-Train and Wikipedia datasets, where AIDA-Train contains 18448 mentions and Wikipedia contains 25995 mentions. In order to compare with the previous methods, we evaluate our model on AIDA-B and other datasets. These datasets are well-known and have been used for the evaluation of most entity linking systems. The statistics of the datasets are shown in Table 1. AIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What kind of neural network architecture do they use?",
    "llm_answer": "Bi-directional attention flow (BiDAF) with an explicit reasoning module, a hybrid convolutional recurrent network at the character level, and a Gated Combination Neural Network (GCNN).  Also uses a Dynamic Convolutional Neural Network (DCNN).\n",
    "context": [
      {
        "id": "3cf1728c-0711-4400-a389-d0f3ab12da96",
        "metadata": {
          "vector_store_key": "1909.08859-0",
          "chunk_id": 8,
          "document_id": "1909.08859",
          "start_idx": 4401,
          "end_idx": 5098
        },
        "page_content": "Its architecture is based on a bi-directional attention flow (BiDAF) model BIBREF6, but also equipped with an explicit reasoning module that acts on entity-specific relational memory units. Fig. FIGREF4 shows an overview of the network architecture. It consists of five main modules: An input module, an attention module, a reasoning module, a modeling module, and an output module. Note that the question answering tasks we consider here are multimodal in that while the context is a procedural text, the question and the multiple choice answers are composed of images. Input Module extracts vector representations of inputs at different levels of granularity by using several different encoders.",
        "type": "Document"
      },
      {
        "id": "baded064-dc7c-4abc-8528-ccaec17ac5de",
        "metadata": {
          "vector_store_key": "1908.06606-0",
          "chunk_id": 28,
          "document_id": "1908.06606",
          "start_idx": 16487,
          "end_idx": 17099
        },
        "page_content": "To implement deep neural network models, we utilize the Keras library BIBREF36 with TensorFlow BIBREF37 backend. Each model is run on a single NVIDIA GeForce GTX 1080 Ti GPU. The models are trained by Adam optimization algorithm BIBREF38 whose parameters are the same as the default settings except for learning rate set to $5\\times 10^{-5}$. Batch size is set to 3 or 4 due to the lack of graphical memory. We select BERT-base as the pre-trained language model in this paper. Due to the high cost of pre-training BERT language model, we directly adopt parameters pre-trained by Google in Chinese general corpus.",
        "type": "Document"
      },
      {
        "id": "3bf6a939-7c9a-41cb-a77d-e81d26529617",
        "metadata": {
          "vector_store_key": "1910.09399-2",
          "chunk_id": 51,
          "document_id": "1910.09399",
          "start_idx": 27228,
          "end_idx": 28102
        },
        "page_content": "This neural network is a hybrid convolutional recurrent network at the character level. Concurrently, both neural networks have also feed-forward inference in the way they condition text features. Generating realistic images automatically from natural language text is the motivation of several of the works proposed in this computer vision field. However, actual artificial intelligence (AI) systems are far from achieving this task BIBREF8, BIBREF39, BIBREF40, BIBREF41, BIBREF42, BIBREF22, BIBREF26. Lately, recurrent neural networks led the way to develop frameworks that learn discriminatively on text features. At the same time, generative adversarial networks (GANs) began recently to show some promise on generating compelling images of a whole host of elements including but not limited to faces, birds, flowers, and non-common images such as room interiorsBIBREF8.",
        "type": "Document"
      },
      {
        "id": "a2d0ee39-5533-4b25-a943-81272c1ff8d1",
        "metadata": {
          "vector_store_key": "1908.08419-2",
          "chunk_id": 22,
          "document_id": "1908.08419",
          "start_idx": 13284,
          "end_idx": 14050
        },
        "page_content": "The architecture of word score model is depicted in Fig. FIGREF12 . Gated Combination Neural Network (GCNN) To effectively learn word representations through character embeddings, we use GCNN BIBREF32 . The architecture of GCNN is demonstrated in Fig. FIGREF13 , which includes update gate and reset gate. The gated mechanism not only captures the characteristics of the characters themselves, but also utilizes the interaction between the characters. There are two types of gates in this network structure: reset gates and update gates. These two gated vectors determine the final output of the gated recurrent neural network, where the update gate helps the model determine what to be passed, and the reset gate primarily helps the model decide what to be cleared.",
        "type": "Document"
      },
      {
        "id": "ace84193-291f-4a82-8154-45d122358eb8",
        "metadata": {
          "vector_store_key": "1701.00185-6",
          "chunk_id": 20,
          "document_id": "1701.00185",
          "start_idx": 10918,
          "end_idx": 11591
        },
        "page_content": "In this section, we briefly review one popular deep convolutional neural network, Dynamic Convolutional Neural Network (DCNN) BIBREF10 as an instance of CNN in the following sections, which as the foundation of our proposed method has been successfully proposed for the completely supervised learning task, text classification. Taking a neural network with two convolutional layers in Figure FIGREF9 as an example, the network transforms raw input text to a powerful representation. Particularly, each raw text vector INLINEFORM0 is projected into a matrix representation INLINEFORM1 by looking up a word embedding INLINEFORM2 , where INLINEFORM3 is the length of one text.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What web and user-generated NER datasets are used for the analysis?",
    "llm_answer": "UMBC Twitter corpus BIBREF33, Web content from ACE BIBREF30 and OntoNotes BIBREF29, and Usenet newsgroups from ACE BIBREF30.\n",
    "context": [
      {
        "id": "fafef8a0-39be-4007-b0e6-a36081a57786",
        "metadata": {
          "vector_store_key": "1701.02877-1",
          "chunk_id": 8,
          "document_id": "1701.02877",
          "start_idx": 3965,
          "end_idx": 4697
        },
        "page_content": "As detailed in Section \"Datasets\" , the corpora studied are OntoNotes BIBREF29 , ACE BIBREF30 , MUC 7 BIBREF31 , the Ritter NER corpus BIBREF21 , the MSM 2013 corpus BIBREF32 , and the UMBC Twitter corpus BIBREF33 . To eliminate potential bias from the choice of statistical NER approach, experiments are carried out with three differently-principled NER approaches, namely Stanford NER BIBREF34 , SENNA BIBREF35 and CRFSuite BIBREF36 (see Section \"NER Models and Features\" for details). Since the goal of this study is to compare NER performance on corpora from diverse domains and genres, seven benchmark NER corpora are included, spanning newswire, broadcast conversation, Web content, and social media (see Table 1 for details).",
        "type": "Document"
      },
      {
        "id": "2931baeb-cf5d-4cf6-b187-7353329a1b45",
        "metadata": {
          "vector_store_key": "1701.02877-1",
          "chunk_id": 7,
          "document_id": "1701.02877",
          "start_idx": 3480,
          "end_idx": 4340
        },
        "page_content": "In line with prior analyses of NER performance BIBREF2 , BIBREF11 , we carry out corpus analysis and introduce briefly the NER methods used for experimentation. Unlike prior efforts, however, our main objectives are to uncover the impact of NE diversity and context diversity on performance (measured primarily by F1 score), and also to study the relationship between OOV NEs and features and F1. See Section \"Experiments\" for details. To ensure representativeness and comprehensiveness, our experimental findings are based on key benchmark NER corpora spanning multiple genres, time periods, and corpus annotation methodologies and guidelines. As detailed in Section \"Datasets\" , the corpora studied are OntoNotes BIBREF29 , ACE BIBREF30 , MUC 7 BIBREF31 , the Ritter NER corpus BIBREF21 , the MSM 2013 corpus BIBREF32 , and the UMBC Twitter corpus BIBREF33 .",
        "type": "Document"
      },
      {
        "id": "668d9a87-3d1f-451e-9683-cd2bb3deffb4",
        "metadata": {
          "vector_store_key": "1701.02877-1",
          "chunk_id": 13,
          "document_id": "1701.02877",
          "start_idx": 6654,
          "end_idx": 7578
        },
        "page_content": "The corpus of this evaluation effort is now one of the most popular gold standards for NER, with new NER approaches and methods often reporting performance on that. Later evaluation campaigns began addressing NER for genres other than newswire, specifically ACE BIBREF30 and OntoNotes BIBREF29 . Both of those contain subcorpora in several genres, namely newswire, broadcast news, broadcast conversation, weblogs, and conversational telephone speech. ACE, in addition, contains a subcorpus with usenet newsgroups. Like CoNLL 2003, the OntoNotes corpus is also a popular benchmark dataset for NER. The languages covered are English, Arabic and Chinese. A further difference between the ACE and OntoNotes corpora on one hand, and CoNLL and MUC on the other, is that they contain annotations not only for NER, but also for other tasks such as coreference resolution, relation and event extraction and word sense disambiguation.",
        "type": "Document"
      },
      {
        "id": "6c24cc72-a125-4856-b377-b23c4a7b54ce",
        "metadata": {
          "vector_store_key": "1701.02877-3",
          "chunk_id": 36,
          "document_id": "1701.02877",
          "start_idx": 18647,
          "end_idx": 19392
        },
        "page_content": "Finally, we use the classical NER approach from CRFsuite BIBREF36 , which also uses first-order CRFs. This frames NER as a structured sequence prediction task, using features derived directly from the training text. Unlike the other systems, no external knowledge (e.g. gazetteers and unsupervised representations) are used. This provides a strong basic supervised system, and \u2013 unlike Stanford NER and SENNA \u2013 has not been tuned for any particular domain, giving potential to reveal more challenging domains without any intrinsic bias. We use the feature extractors natively distributed with the NER frameworks. For Stanford NER we use the feature set \u201cchris2009\u201d without distributional similarity, which has been tuned for the CoNLL 2003 data.",
        "type": "Document"
      },
      {
        "id": "de8ede46-0834-4cb4-9c31-6200ceab0909",
        "metadata": {
          "vector_store_key": "1701.02877-3",
          "chunk_id": 32,
          "document_id": "1701.02877",
          "start_idx": 16458,
          "end_idx": 17224
        },
        "page_content": "There are only minor differences between the tag density and the normalised tag density, since corpus size as such does not impact tag density. To avoid system-specific bias in our experiments, three widely-used supervised statistical approaches to NER are included: Stanford NER, SENNA, and CRFSuite. These systems each have contrasting notable attributes. Stanford NER BIBREF34 is the most popular of the three, deployed widely in both research and commerce. The system has been developed in terms of both generalising the underlying technology and also specific additions for certain languages. The majority of openly-available additions to Stanford NER, in terms of models, gazetteers, prefix/suffix handling and so on, have been created for newswire-style text.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which unlabeled data do they pretrain with?",
    "llm_answer": "Unlabeled audio data.\n",
    "context": [
      {
        "id": "3e91a9bf-4586-44a5-a64c-9f6d5d3fc0ed",
        "metadata": {
          "vector_store_key": "1904.05862-4",
          "chunk_id": 0,
          "document_id": "1904.05862",
          "start_idx": 0,
          "end_idx": 651
        },
        "page_content": "Current state of the art models for speech recognition require large amounts of transcribed audio data to attain good performance BIBREF1 . Recently, pre-training of neural networks has emerged as an effective technique for settings where labeled data is scarce. The key idea is to learn general representations in a setup where substantial amounts of labeled or unlabeled data is available and to leverage the learned representations to improve performance on a downstream task for which the amount of data is limited. This is particularly interesting for tasks where substantial effort is required to obtain labeled data, such as speech recognition.",
        "type": "Document"
      },
      {
        "id": "f870a55c-6138-4670-8d8e-7c4a62a3e961",
        "metadata": {
          "vector_store_key": "1909.00124-1",
          "chunk_id": 21,
          "document_id": "1909.00124",
          "start_idx": 12190,
          "end_idx": 12874
        },
        "page_content": "For each clean-labeled dataset, the sentences are randomly partitioned into training set and test set with $80\\%$ and $20\\%$, respectively. Following BIBREF25, We also randomly select $10\\%$ of the test data for validation to check the model during training. Summary statistics of the training, validation, and test data are shown in Table TABREF9. Noisy-labeled Training Datasets. For the above three domains (movie, laptop, and restaurant), we collected 2,000 reviews for each domain from the same review source. We extracted sentences from each review and assigned review's label to its sentences. Like previous work, we treat 4 or 5 stars as positive and 1 or 2 stars as negative.",
        "type": "Document"
      },
      {
        "id": "05cf73c6-2a91-4a1b-9e53-faf02abc7f6a",
        "metadata": {
          "vector_store_key": "1904.05862-4",
          "chunk_id": 2,
          "document_id": "1904.05862",
          "start_idx": 1154,
          "end_idx": 1955
        },
        "page_content": "In speech processing, pre-training has focused on emotion recogniton BIBREF12 , speaker identification BIBREF13 , phoneme discrimination BIBREF14 , BIBREF15 as well as transferring ASR representations from one language to another BIBREF16 . There has been work on unsupervised learning for speech but the resulting representations have not been applied to improve supervised speech recognition BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 . In this paper, we apply unsupervised pre-training to improve supervised speech recognition. This enables exploiting unlabeled audio data which is much easier to collect than labeled data. Our model, , is a convolutional neural network that takes raw audio as input and computes a general representation that can be input to a speech recognition system.",
        "type": "Document"
      },
      {
        "id": "a1ae1ca7-4a6d-4b9a-9410-3877902dffc1",
        "metadata": {
          "vector_store_key": "1710.01492-2",
          "chunk_id": 38,
          "document_id": "1710.01492",
          "start_idx": 20785,
          "end_idx": 21467
        },
        "page_content": "Training these parts needs no labels. While these parts can be also pre-trained, it is easier, and often better, to use self training. Distantly-supervised learning. Another way to make use of large unannotated datasets is to rely on distant supervision BIBREF41 . For example, one can annotate tweets for sentiment polarity based on whether they contain a positive or a negative emoticon. This results in noisy labels, which can be used to train a system BIBREF54 , to induce sentiment-specific word embeddings BIBREF57 , sentiment-polarity lexicons BIBREF25 , etc. Unsupervised learning. Fully unsupervised learning is not a popular method for addressing sentiment analysis tasks.",
        "type": "Document"
      },
      {
        "id": "39d04bfb-c08e-485b-8a25-34aba9111af7",
        "metadata": {
          "vector_store_key": "1812.06705-4",
          "chunk_id": 26,
          "document_id": "1812.06705",
          "start_idx": 15443,
          "end_idx": 16142
        },
        "page_content": "When the task-specific dataset is with more than two different labels, we should re-train a label size compatible label embeddings layer instead of directly fine-tuning the pre-trained one. Six benchmark classification datasets are listed in table 1 . Following Kim BIBREF24 , for a dataset without validation data, we use 10% of its training set for the validation set. Summary statistics of six classification datasets are shown in table 1. SST BIBREF25 SST (Stanford Sentiment Treebank) is a dataset for sentiment classification on movie reviews, which are annotated with five labels (SST5: very positive, positive, neutral, negative, or very negative) or two labels (SST2: positive or negative).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many convolutional layers does their model have?",
    "llm_answer": "Two\n",
    "context": [
      {
        "id": "c37ff81b-ef62-42dd-bf83-07dc675c372d",
        "metadata": {
          "vector_store_key": "1811.00942-3",
          "chunk_id": 8,
          "document_id": "1811.00942",
          "start_idx": 4843,
          "end_idx": 5855
        },
        "page_content": "Given an input $\\mathbf {X} \\in \\mathbb {R}^{k \\times n}$ , the convolution layer is $\n\\mathbf {Z} = \\tanh (\\mathbf {W}_z \\cdot \\mathbf {X})\\\\\n\\mathbf {F} = \\sigma (\\mathbf {W}_f \\cdot \\mathbf {X})\\\\\n\\mathbf {O} = \\sigma (\\mathbf {W}_o \\cdot \\mathbf {X})\n$  where $\\sigma $ denotes the sigmoid function, $\\cdot $ represents masked convolution across time, and $\\mathbf {W}_{\\lbrace z, f, o\\rbrace } \\in \\mathbb {R}^{m \\times k \\times r}$ are convolution weights with $k$ input channels, $m$ output channels, and a window size of $r$ . In the recurrent pooling layer, the convolution outputs are combined sequentially: $\n\\mathbf {c}_t &= \\mathbf {f}_t \\odot \\mathbf {c}_{t-1} + (1 -\n\\mathbf {f}_t) \\odot \\mathbf {z}_t\\\\\n\\mathbf {h}_t &= \\mathbf {o}_t \\odot \\mathbf {c}_t\n$  Multiple QRNN layers can be stacked for deeper hierarchical representation, with the output $\\mathbf {h}_{1:t}$ being fed as the input into the subsequent layer: In language modeling, a four-layer QRNN is a standard architecture BIBREF11 .",
        "type": "Document"
      },
      {
        "id": "14cb4a16-54f0-4b61-b8d6-9aa52a7ba8ab",
        "metadata": {
          "vector_store_key": "1910.02789-2",
          "chunk_id": 51,
          "document_id": "1910.02789",
          "start_idx": 28358,
          "end_idx": 29007
        },
        "page_content": "The semantic segmentation image was of resolution 640X480X1, where the pixel value represents the object's class, generated using the VizDoom label API. the network consisted of two convolutional layers, two hidden linear layers and an output layer. The first convolutional layer has 8 6X6 filters with stride 3 and ReLU activation. The second convolutional layer has 16 3X3 filters with stride 2 and ReLU activation. The fully connected layers has 32 and 16 units, both of them are followed by ReLU activation. The output layer's size is the amount of actions the agent has available in the trained scenario. Used in the feature vector based agent.",
        "type": "Document"
      },
      {
        "id": "f173ab64-7181-4710-80da-c7b74d741d1d",
        "metadata": {
          "vector_store_key": "1910.04269-3",
          "chunk_id": 16,
          "document_id": "1910.04269",
          "start_idx": 9333,
          "end_idx": 10024
        },
        "page_content": "We applied the following design principles to all our models: Every convolutional layer is always followed by an appropriate max pooling layer. This helps in containing the explosion of parameters and keeps the model small and nimble. Convolutional blocks are defined as an individual block with multiple pairs of one convolutional layer and one max pooling layer. Each convolutional block is preceded or succeded by a convolutional layer. Batch Normalization and Rectified linear unit activations were applied after each convolutional layer. Batch Normalization helps speed up convergence during training of a neural network. Model ends with a dense layer which acts the final output layer.",
        "type": "Document"
      },
      {
        "id": "b90b5667-0046-4454-a0ea-8d4393be1165",
        "metadata": {
          "vector_store_key": "1910.02789-2",
          "chunk_id": 54,
          "document_id": "1910.02789",
          "start_idx": 29840,
          "end_idx": 30474
        },
        "page_content": "Following the convolution layer there is a ReLU activation and a max pool layer. Finally, there are two fully connected layers; The first layer has 32 units, and second one has 16 units. Both of them are followed by ReLU activation. All architectures have the same output, regardless of the input type. The DQN network is a regression network, with its output size the number of available actions. The PPO agent has 2 networks; actor and critic. The actor network has a Softmax activation with size equal to the available amount of actions. The critic network is a regression model with a single output representing the state's value.",
        "type": "Document"
      },
      {
        "id": "34462739-80f8-4873-995b-69a1f82d6f05",
        "metadata": {
          "vector_store_key": "1701.00185-3",
          "chunk_id": 24,
          "document_id": "1701.00185",
          "start_idx": 12869,
          "end_idx": 13595
        },
        "page_content": "Given a fixed pooling parameter INLINEFORM8 for the topmost convolutional layer, the parameter INLINEFORM9 of INLINEFORM10 -max pooling in the INLINEFORM11 -th convolutional layer can be computed as follows: DISPLAYFORM0  where INLINEFORM0 is the total number of convolutional layers in the network. As described in Figure FIGREF5 , the dimensionality reduction function is defined as follows: DISPLAYFORM0  where, INLINEFORM0 are the INLINEFORM1 -dimensional reduced latent space representations. Here, we take four popular dimensionality reduction methods as examples in our framework. Average Embedding (AE): This method directly averages the word embeddings which are respectively weighted with TF and TF-IDF. Huang et al.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big are the datasets?",
    "llm_answer": "Not found in the context.  The context describes *types* of datasets, but not sizes.\n",
    "context": [
      {
        "id": "438ab7e6-9abd-44e6-b671-b930ca0cefb2",
        "metadata": {
          "vector_store_key": "1910.09399-3",
          "chunk_id": 107,
          "document_id": "1910.09399",
          "start_idx": 57443,
          "end_idx": 58221
        },
        "page_content": "While most deep learning algorithms use MNIST BIBREF74 dataset as the benchmark, there are three main datasets that are commonly used for evaluation of proposed GAN models for text-to-image synthesis: CUB BIBREF75, Oxford BIBREF76, COCO BIBREF77, and CIFAR-10 BIBREF78. CUB BIBREF75 contains 200 birds with matching text descriptions and Oxford BIBREF76 contains 102 categories of flowers with 40-258 images each and matching text descriptions. These datasets contain individual objects, with the text description corresponding to that object, making them relatively simple. COCO BIBREF77 is much more complex, containing 328k images with 91 different object types. CIFAI-10 BIBREF78 dataset consists of 60000 32$times$32 colour images in 10 classes, with 6000 images per class.",
        "type": "Document"
      },
      {
        "id": "1e2bf238-b0a0-40af-9d8d-e6fea4ad79d0",
        "metadata": {
          "vector_store_key": "1910.09399-3",
          "chunk_id": 105,
          "document_id": "1910.09399",
          "start_idx": 56273,
          "end_idx": 57009
        },
        "page_content": "For example, image data captured from a variety of photo-ready devices, such as smart-phones, and online social media services opened the door to the analysis of large amounts of media datasets BIBREF70. The availability of large media datasets allow new frameworks and algorithms to be proposed and tested on real-world data. A summary of some reviewed methods and benchmark datasets used for validation is reported in Table TABREF43. In addition, the performance of different GANs with respect to the benchmark datasets and performance metrics is reported in Table TABREF48. In order to synthesize images from text descriptions, many frameworks have taken a minimalistic approach by creating small and background-less images BIBREF73.",
        "type": "Document"
      },
      {
        "id": "73a83f60-b2c9-406d-b4cd-5720639e5f06",
        "metadata": {
          "vector_store_key": "1812.07023-4",
          "chunk_id": 18,
          "document_id": "1812.07023",
          "start_idx": 10358,
          "end_idx": 10986
        },
        "page_content": "After 10 such qa turns, the questioner wraps up by writing a summary of the video based on the conversation. Dataset statistics such as the number of dialogues, turns, and words for the avsd dataset are presented in Table TABREF5 . For the initially released prototype dataset, the training set of the avsd dataset corresponds to videos taken from the training set of the Charades dataset while the validation and test sets of the avsd dataset correspond to videos taken from the validation set of the Charades dataset. For the official dataset, training, validation and test sets are drawn from the corresponding Charades sets.",
        "type": "Document"
      },
      {
        "id": "731d76b9-6365-4a8c-872e-c14badc3b626",
        "metadata": {
          "vector_store_key": "1911.07228-0",
          "chunk_id": 12,
          "document_id": "1911.07228",
          "start_idx": 6115,
          "end_idx": 6702
        },
        "page_content": "The dataset contains four different types of label: Location (LOC), Person (PER), Organization (ORG) and Miscellaneous - Name of an entity that do not belong to 3 types above (Table TABREF15). Although the corpus has more information about the POS and chunks, but we do not use them as features in our model. There are two folders with 267 text files of training data and 45 text files of test data. They all have their own format. We take 21 first text files and 22 last text files and 22 sentences of the 22th text file and 55 sentences of the 245th text file to be a development data.",
        "type": "Document"
      },
      {
        "id": "eac31fee-fbdc-4a80-be44-f2145c81562a",
        "metadata": {
          "vector_store_key": "1910.09399-3",
          "chunk_id": 106,
          "document_id": "1910.09399",
          "start_idx": 56647,
          "end_idx": 57443
        },
        "page_content": "In order to synthesize images from text descriptions, many frameworks have taken a minimalistic approach by creating small and background-less images BIBREF73. In most cases, the experiments were conducted on simple datasets, initially containing images of birds and flowers. BIBREF8 contributed to these data sets by adding corresponding natural language text descriptions to subsets of the CUB, MSCOCO, and Oxford-102 datasets, which facilitated the work on text-to-image synthesis for several papers released more recently. While most deep learning algorithms use MNIST BIBREF74 dataset as the benchmark, there are three main datasets that are commonly used for evaluation of proposed GAN models for text-to-image synthesis: CUB BIBREF75, Oxford BIBREF76, COCO BIBREF77, and CIFAR-10 BIBREF78.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What baselines did they compare their model with?",
    "llm_answer": "NABERT+ baseline.\n",
    "context": [
      {
        "id": "dcbb3e93-d5ad-43bf-80ac-ec1ae660eda5",
        "metadata": {
          "vector_store_key": "1812.07023-3",
          "chunk_id": 31,
          "document_id": "1812.07023",
          "start_idx": 17294,
          "end_idx": 18075
        },
        "page_content": "Their approach, however, is not public as of yet. We observe the following for our models: Since the official test set has not been released publicly, results reported on the official test set have been provided by the challenge organizers. For the prototype test set and for the ablation study presented in Table TABREF24 , we use the same code for evaluation metrics as used by BIBREF11 for fairness and comparability. We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below: Our primary architectural differences over the baseline model are: not concatenating the question, answer pairs before encoding them, the auxiliary decoder module, and using the Time-Extended FiLM module for feature extraction.",
        "type": "Document"
      },
      {
        "id": "36dcf150-24e8-459c-a8c0-aa8fe1c0018f",
        "metadata": {
          "vector_store_key": "1909.02764-4",
          "chunk_id": 36,
          "document_id": "1909.02764",
          "start_idx": 19976,
          "end_idx": 20568
        },
        "page_content": "We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with 40 % of the data from the same domain (results shown in the column \u201cIn-Domain\u201d in Table TABREF19). Excluding AMMER, we achieve an average micro $\\text{F}_1$ of 68 %, with best results of F$_1$=73 % on TEC. The model trained on our AMMER corpus achieves an F1 score of 57%. This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set.",
        "type": "Document"
      },
      {
        "id": "1b1f9986-e4da-44a7-a6cd-13931a730da6",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 38,
          "document_id": "1909.13375",
          "start_idx": 19979,
          "end_idx": 20678
        },
        "page_content": "For the large models we used a batch size of 3 with a learning rate of $5e^{-6}$ and trained for 5 epochs, except for the model without the single-span heads that was trained with a batch size of 2 for 7 epochs. F1 was used as our validation metric. All models were trained on a single GPU with 12-16GB of memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions.",
        "type": "Document"
      },
      {
        "id": "6824b4f2-136e-4c26-a2a7-a10cf7d4c97e",
        "metadata": {
          "vector_store_key": "1810.00663-8",
          "chunk_id": 46,
          "document_id": "1810.00663",
          "start_idx": 25788,
          "end_idx": 26550
        },
        "page_content": "The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as can be seen in performance drops in Table TABREF28 for the new environments. Nonetheless, the insights from the previous section still hold: masking in the output layer and reordering the graph triplets tend to increase performance. Even though the results in Table TABREF28 suggest that there is room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM.",
        "type": "Document"
      },
      {
        "id": "508a8c4d-00ed-4e56-8dc0-984b999fbac6",
        "metadata": {
          "vector_store_key": "1810.00663-8",
          "chunk_id": 47,
          "document_id": "1810.00663",
          "start_idx": 26550,
          "end_idx": 27204
        },
        "page_content": "For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM. Note that the average number of actions in the ground truth output sequences is 7.07 for the Test-New set. Our model's predictions are just INLINEFORM1 edits off on average from the correct navigation plans. This section discusses qualitative results to better understand how the proposed model uses the navigation graph. We analyze the evolution of the attention weights INLINEFORM0 in Eq. () to assess if the decoder layer of the proposed model is attending to the correct parts of the behavioral graph when making predictions.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was the performance of their model?",
    "llm_answer": "Their large models exhibited a substantial improvement across all metrics compared to base models, surpassing the NABERT+ baseline.  Their best model (large-squad) showed a huge improvement of 29.7 in EM and 15.1 in F1 compared to MTMSNlarge.  They achieved a 12.2 absolute improvement in EM and a 2.3 improvement in F1 compared to their base model, even when comparing to MTMSNlarge.\n",
    "context": [
      {
        "id": "1b1f9986-e4da-44a7-a6cd-13931a730da6",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 38,
          "document_id": "1909.13375",
          "start_idx": 19979,
          "end_idx": 20678
        },
        "page_content": "For the large models we used a batch size of 3 with a learning rate of $5e^{-6}$ and trained for 5 epochs, except for the model without the single-span heads that was trained with a batch size of 2 for 7 epochs. F1 was used as our validation metric. All models were trained on a single GPU with 12-16GB of memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions.",
        "type": "Document"
      },
      {
        "id": "6824b4f2-136e-4c26-a2a7-a10cf7d4c97e",
        "metadata": {
          "vector_store_key": "1810.00663-8",
          "chunk_id": 46,
          "document_id": "1810.00663",
          "start_idx": 25788,
          "end_idx": 26550
        },
        "page_content": "The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as can be seen in performance drops in Table TABREF28 for the new environments. Nonetheless, the insights from the previous section still hold: masking in the output layer and reordering the graph triplets tend to increase performance. Even though the results in Table TABREF28 suggest that there is room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM.",
        "type": "Document"
      },
      {
        "id": "874d1312-9fdb-46fe-865b-ce75dcb445d6",
        "metadata": {
          "vector_store_key": "1812.07023-3",
          "chunk_id": 32,
          "document_id": "1812.07023",
          "start_idx": 17666,
          "end_idx": 18428
        },
        "page_content": "We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below: Our primary architectural differences over the baseline model are: not concatenating the question, answer pairs before encoding them, the auxiliary decoder module, and using the Time-Extended FiLM module for feature extraction. These, combined with using scheduled sampling and running hyperparameter optimization over the validation set to select hyperparameters, give us the observed performance boost. We observe that our models generate fairly relevant responses to questions in the dialogues, and models with audio-visual inputs respond to audio-visual questions (e.g. \u201cis there any voices or music ?\u201d) correctly more often.",
        "type": "Document"
      },
      {
        "id": "2e7cfe41-b41c-475c-83f6-e7d39903180d",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 40,
          "document_id": "1909.13375",
          "start_idx": 21052,
          "end_idx": 21792
        },
        "page_content": "While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1. All that, while keeping in mind we compare a base model to a large model (for reference, note the 8 point improvement between MTMSNbase and MTMSNlarge in both EM and F1). Our best model, large-squad, exhibits a huge improvement of 29.7 in EM and 15.1 in F1 compared to MTMSNlarge. When comparing single-span performance, our best model exhibits slightly better results, but it should be noted that it retains the single-span heads from NABERT+, while in MTMSN they have one head to predict both single-span and multi-span answers.",
        "type": "Document"
      },
      {
        "id": "1858edb9-546d-4edc-9fc6-41fbc071abbd",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 39,
          "document_id": "1909.13375",
          "start_idx": 20678,
          "end_idx": 21407
        },
        "page_content": "The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could be found in Section (SECREF36). Notice that different BERTlarge models were used, so the comparison is less direct. Overall, our large models exhibits similar results to those of MTMSNlarge. For multi-span questions we achieve a significantly better performance. While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What evaluation metrics are used?",
    "llm_answer": "Human ratings (Likert scale), BLEU, METEOR, ROUGE, Pyramid, F<sub>1</sub> score, Pearson correlation, Mean Squared Error (MSE), Cronbach's alpha.\n",
    "context": [
      {
        "id": "8c81c383-8e40-4094-8e18-ab0a2d32126a",
        "metadata": {
          "vector_store_key": "1905.08949-6",
          "chunk_id": 18,
          "document_id": "1905.08949",
          "start_idx": 10340,
          "end_idx": 11074
        },
        "page_content": "Although the datasets are commonly shared between QG and QA, it is not the case for evaluation: it is challenging to define a gold standard of proper questions to ask. Meaningful, syntactically correct, semantically sound and natural are all useful criteria, yet they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks. As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE BIBREF43 , are also widely used.",
        "type": "Document"
      },
      {
        "id": "f221ac3d-8ea6-49e8-8ce8-1528ccd79f99",
        "metadata": {
          "vector_store_key": "1909.00578-2",
          "chunk_id": 2,
          "document_id": "1909.00578",
          "start_idx": 1185,
          "end_idx": 1928
        },
        "page_content": "Sum-QE achieves very high correlations with human ratings, showing the ability of BERT to model linguistic qualities that relate to both text content and form. Summarization evaluation metrics like Pyramid BIBREF5 and ROUGE BIBREF3, BIBREF2 are recall-oriented; they basically measure the content from a model (reference) summary that is preserved in peer (system generated) summaries. Pyramid requires substantial human effort, even in its more recent versions that involve the use of word embeddings BIBREF8 and a lightweight crowdsourcing scheme BIBREF9. ROUGE is the most commonly used evaluation metric BIBREF10, BIBREF11, BIBREF12. Inspired by BLEU BIBREF4, it relies on common $n$-grams or subsequences between peer and model summaries.",
        "type": "Document"
      },
      {
        "id": "6a443055-4b4a-46e1-a2dc-b3211ac62d38",
        "metadata": {
          "vector_store_key": "1908.10449-0",
          "chunk_id": 20,
          "document_id": "1908.10449",
          "start_idx": 10741,
          "end_idx": 11449
        },
        "page_content": "Since iMRC involves both MRC and RL, we adopt evaluation metrics from both settings. First, as a question answering task, we use $\\text{F}_1$ score to compare predicted answers against ground-truth, as in previous works. When there exist multiple ground-truth answers, we report the max $\\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we evaluate an agent's performance during both its training and testing phases. During training, we report training curves averaged over 3 random seeds. During test, we follow common practice in supervised learning tasks where we report the agent's test performance corresponding to its best validation performance .",
        "type": "Document"
      },
      {
        "id": "85620a36-4465-42bf-a217-83811397b9bc",
        "metadata": {
          "vector_store_key": "1809.08731-4",
          "chunk_id": 20,
          "document_id": "1809.08731",
          "start_idx": 11375,
          "end_idx": 12197
        },
        "page_content": "Following earlier work BIBREF2 , we evaluate our metrics using Pearson correlation with human judgments. It is defined as the covariance divided by the product of the standard deviations:  $$\\rho _{X,Y} = \\frac{\\text{cov}(X,Y)}{\\sigma _X \\sigma _Y}$$   (Eq. 28)  Pearson cannot accurately judge a metric's performance for sentences of very similar quality, i.e., in the extreme case of rating outputs of identical quality, the correlation is either not defined or 0, caused by noise of the evaluation model. Thus, we additionally evaluate using mean squared error (MSE), which is defined as the squares of residuals after a linear transformation, divided by the sample size:  $$\\text{MSE}_{X,Y} = \\underset{f}{\\min }\\frac{1}{|X|}\\sum \\limits _{i = 1}^{|X|}{(f(x_i) - y_i)^2}$$   (Eq. 30)  with $f$ being a linear function.",
        "type": "Document"
      },
      {
        "id": "c7a5f4b3-a8d8-4418-b4ed-2106461df1d1",
        "metadata": {
          "vector_store_key": "2003.07433-0",
          "chunk_id": 36,
          "document_id": "2003.07433",
          "start_idx": 19458,
          "end_idx": 20219
        },
        "page_content": "We chose one dimension for each of the questions under each category to reflect real survey system evaluation. Our chosen categories are state in Fig FIGREF20. Score calculation $\\alpha $-score: $\\alpha $-scores refer to the Cronbach's alphas for the internal reliability of the specific words within each category. The binary alphas are computed on the ratio of occurrence and non-occurrence of each dictionary word whereas the raw or uncorrected alphas are based on the percentage of use of each of the category words within texts. After the PTSD Linguistic Dictionary has been created, we empirically evaluate its psychometric properties such as reliability and validity as per American Standards for educational and psychological testing guideline BIBREF24.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How were the navigation instructions collected?",
    "llm_answer": "Collected through Mechanical Turk using 100 simulated environments with corresponding topological maps.\n",
    "context": [
      {
        "id": "11ee97c2-d653-45ad-8051-b0f4418c6701",
        "metadata": {
          "vector_store_key": "1810.00663-5",
          "chunk_id": 33,
          "document_id": "1810.00663",
          "start_idx": 18652,
          "end_idx": 19324
        },
        "page_content": "Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said \u201cturn right and advance\u201d to describe part of a route, while another person said \u201cgo straight after turning right\u201d in a similar situation. The high variability present in the natural language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort. This section describes our evaluation of the proposed approach for interpreting navigation commands in natural language.",
        "type": "Document"
      },
      {
        "id": "b251c9e7-7cc5-48b6-80b5-ee133293ccc0",
        "metadata": {
          "vector_store_key": "1810.00663-5",
          "chunk_id": 8,
          "document_id": "1810.00663",
          "start_idx": 4443,
          "end_idx": 5340
        },
        "page_content": "We investigate both generalization to new instructions in known and in new environments. We conclude this paper by discussing the benefits of the proposed approach as well as opportunities for future research based on our findings. This section reviews relevant prior work on following navigation instructions. Readers interested in an in-depth review of methods to interpret spatial natural language for robotics are encouraged to refer to BIBREF11 . Typical approaches to follow navigation commands deal with the complexity of natural language by manually parsing commands, constraining language descriptions, or using statistical machine translation methods. While manually parsing commands is often impractical, the first type of approaches are foundational: they showed that it is possible to leverage the compositionality of semantic units to interpret spatial language BIBREF12 , BIBREF13 .",
        "type": "Document"
      },
      {
        "id": "407ce520-4bae-4136-921f-1141682ec0ad",
        "metadata": {
          "vector_store_key": "1810.00663-5",
          "chunk_id": 7,
          "document_id": "1810.00663",
          "start_idx": 3960,
          "end_idx": 4668
        },
        "page_content": "This work also contributes a new dataset of INLINEFORM0 pairs of free-form natural language instructions and high-level navigation plans. This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation. The dataset opens up opportunities to explore data-driven methods for grounding navigation commands into high-level motion plans. We conduct extensive experiments to study the generalization capabilities of the proposed model for following natural language instructions. We investigate both generalization to new instructions in known and in new environments.",
        "type": "Document"
      },
      {
        "id": "9807f8e2-43e3-453f-95e2-8fbcb20126b7",
        "metadata": {
          "vector_store_key": "1807.03367-5",
          "chunk_id": 21,
          "document_id": "1807.03367",
          "start_idx": 11520,
          "end_idx": 12701
        },
        "page_content": "Specifically, we let trained tourist models undertake random walks, using the following protocol: at each step, the tourist communicates its observations and actions to the guide, who predicts the tourist's location. If the guide predicts that the tourist is at target, we evaluate its location. If successful, the task ends, otherwise we continue until there have been three wrong evaluations. The protocol is given as pseudo-code in Appendix SECREF12 . The designed navigation protocol relies on a trained localization model that predicts the tourist's location from a communicated message. Before we formalize this localization sub-task in Section UID21 , we further introduce two simplifying assumptions\u2014perfect perception and orientation-agnosticism\u2014so as to overcome some of the difficulties we encountered in preliminary experiments. paragraph4 0.1ex plus0.1ex minus.1ex-1em Perfect Perception Early experiments revealed that perceptual grounding of landmarks is difficult: we set up a landmark classification problem, on which models with extracted CNN BIBREF7 or text recognition features BIBREF8 barely outperform a random baseline\u2014see Appendix SECREF13 for full details.",
        "type": "Document"
      },
      {
        "id": "1066adfe-ed0d-4e7e-9b10-aa6ad182c13d",
        "metadata": {
          "vector_store_key": "1810.00663-4",
          "chunk_id": 2,
          "document_id": "1810.00663",
          "start_idx": 1107,
          "end_idx": 1813
        },
        "page_content": "Resolving such ambiguities often requires reasoning about \u201ccommon-sense\u201d concepts, as well as interpreting spatial information and landmarks, e.g., in sentences such as \u201cthe room on the left right before the end of the corridor\u201d and \u201cthe room which is in the middle of two vases\u201d. In this work, we pose the problem of interpreting navigation instructions as finding a mapping (or grounding) of the commands into an executable navigation plan. While the plan is typically modeled as a formal specification of low-level motions BIBREF2 or a grammar BIBREF3 , BIBREF4 , we focus specifically on translating instructions to a high-level navigation plan based on a topological representation of the environment.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What language is the experiment done in?",
    "llm_answer": "English\n",
    "context": [
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      },
      {
        "id": "56fe66b6-5e7f-4d68-a3e5-95871fe6ebb1",
        "metadata": {
          "vector_store_key": "1911.03894-1",
          "chunk_id": 27,
          "document_id": "1911.03894",
          "start_idx": 16033,
          "end_idx": 16717
        },
        "page_content": "To evaluate a model on a language other than English (such as French), we consider the two following settings: TRANSLATE-TEST: The French test set is machine translated into English, and then used with an English classification model. This setting provides a reasonable, although imperfect, way to circumvent the fact that no such data set exists for French, and results in very strong baseline scores. TRANSLATE-TRAIN: The French model is fine-tuned on the machine-translated English training set and then evaluated on the French test set. This is the setting that we used for CamemBERT. For the TRANSLATE-TEST setting, we report results of the English RoBERTa to act as a reference.",
        "type": "Document"
      },
      {
        "id": "24e7a576-ab3d-4f0a-8326-f14fe7d7f139",
        "metadata": {
          "vector_store_key": "1911.03894-4",
          "chunk_id": 26,
          "document_id": "1911.03894",
          "start_idx": 15178,
          "end_idx": 16033
        },
        "page_content": "We also evaluate our model on the Natural Language Inference (NLI) task, using the French part of the XNLI dataset BIBREF50. NLI consists in predicting whether a hypothesis sentence is entailed, neutral or contradicts a premise sentence. The XNLI dataset is the extension of the Multi-Genre NLI (MultiNLI) corpus BIBREF51 to 15 languages by translating the validation and test sets manually into each of those languages. The English training set is also machine translated for all languages. The dataset is composed of 122k train, 2490 valid and 5010 test examples. As usual, NLI performance is evaluated using accuracy. To evaluate a model on a language other than English (such as French), we consider the two following settings: TRANSLATE-TEST: The French test set is machine translated into English, and then used with an English classification model.",
        "type": "Document"
      },
      {
        "id": "c835fe0c-f240-4e5f-96f5-494421121339",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 6,
          "document_id": "1910.04269",
          "start_idx": 3498,
          "end_idx": 4224
        },
        "page_content": "The model uses a cyclic learning rate where learning rate increases and then decreases linearly. Maximum learning rate for a cycle is set by finding the optimal learning rate using fastai BIBREF15 library. The model classified six languages \u2013 English, French, Spanish, Russian, Italian and German \u2013 and achieving an accuracy of 89.0%. Gazeau et al. BIBREF16 in his research showed how Neural Networks, Support Vector Machine and Hidden Markov Model (HMM) can be used to identify French, English, Spanish and German. Dataset was prepared using voice samples from Youtube News BIBREF17and VoxForge BIBREF6 datasets. Hidden Markov models convert speech into a sequence of vectors, was used to capture temporal features in speech.",
        "type": "Document"
      },
      {
        "id": "dd0a3096-9615-48ec-a2e0-80c08aa831d0",
        "metadata": {
          "vector_store_key": "1911.00069-1",
          "chunk_id": 43,
          "document_id": "1911.00069",
          "start_idx": 25020,
          "end_idx": 25681
        },
        "page_content": "The source language, English, belongs to the SVO (Subject, Verb, Object) language family where in a sentence the subject comes first, the verb second, and the object third. Spanish, Italian, Portuguese, German (in conventional typology) and Chinese also belong to the SVO language family, and our approach achieves over $70\\%$ relative accuracy for these languages. On the other hand, Japanese belongs to the SOV (Subject, Object, Verb) language family and Arabic belongs to the VSO (Verb, Subject, Object) language family, and our approach achieves lower relative accuracy for these two languages. There are a few weakly supervised cross-lingual RE approaches.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What additional features are proposed for future work?",
    "llm_answer": "Additional features for future work include: deeper analysis of clinical narratives in EHRs, distinguishing between clinically positive and negative phenomena within each risk factor domain,  incorporation of available metadata, preprocessing data with morphological normalization and out-of-vocabulary mappings, word-by-word and history-aware attention mechanisms, and more semantic features.\n",
    "context": [
      {
        "id": "d9713915-59b2-4542-9426-44e46e482804",
        "metadata": {
          "vector_store_key": "1809.05752-2",
          "chunk_id": 40,
          "document_id": "1809.05752",
          "start_idx": 22915,
          "end_idx": 23602
        },
        "page_content": "In future versions of our risk factor domain classification model we will explore increasing robustness through sequence modeling that considers more contextual information. Our current feature set for training a machine learning classifier is relatively small, consisting of paragraph domain scores, bag-of-words, length of stay, and number of previous admissions, but we intend to factor in many additional features that extend beyond the scope of the present study. These include a deeper analysis of clinical narratives in EHRs: our next task will be to extend our EHR data pipeline by distinguishing between clinically positive and negative phenomena within each risk factor domain.",
        "type": "Document"
      },
      {
        "id": "3125de89-a6f4-470b-9869-924a2b6d979d",
        "metadata": {
          "vector_store_key": "1701.00185-5",
          "chunk_id": 65,
          "document_id": "1701.00185",
          "start_idx": 35221,
          "end_idx": 35700
        },
        "page_content": "In the future, how to select and incorporate more effective semantic features into the proposed framework would call for more research. We would like to thank reviewers for their comments, and acknowledge Kaggle and BioASQ for making the datasets available. This work is supported by the National Natural Science Foundation of China (No. 61602479, No. 61303172, No. 61403385) and the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDB02070005).",
        "type": "Document"
      },
      {
        "id": "52bc1d69-e0df-42c4-9417-22afe119bdac",
        "metadata": {
          "vector_store_key": "1603.07044-0",
          "chunk_id": 35,
          "document_id": "1603.07044",
          "start_idx": 19069,
          "end_idx": 19435
        },
        "page_content": "Future work could proceed in two directions: first, we can enrich the existing system by incorporating available metadata and preprocessing data with morphological normalization and out-of-vocabulary mappings; second, we can reinforce our model by carrying out word-by-word and history-aware attention mechanisms instead of attending only when reading the last word.",
        "type": "Document"
      },
      {
        "id": "daec2a35-d9cc-41c2-a73d-4aed05e3c0c1",
        "metadata": {
          "vector_store_key": "1710.01492-5",
          "chunk_id": 53,
          "document_id": "1710.01492",
          "start_idx": 28274,
          "end_idx": 29049
        },
        "page_content": "We see competitions such as those at SemEval as the engine of this innovation, as they not only perform head-to-head comparisons, but also create databases and tools that enable follow-up research for many years afterward. In terms of methods, we believe that deep learning BIBREF55 , BIBREF54 , BIBREF56 , together with semi-supervised and distantly-supervised methods BIBREF67 , BIBREF57 , will be the main focus of future research. We also expect more attention to be paid to linguistic structure and sentiment compositionality BIBREF68 , BIBREF69 . Moreover, we forecast more interest for languages other than English, and for cross-lingual methods BIBREF40 , BIBREF70 , BIBREF71 , which will allow leveraging on the rich resources that are already available for English.",
        "type": "Document"
      },
      {
        "id": "482acd0f-398b-47d3-9b79-e01b3396adec",
        "metadata": {
          "vector_store_key": "1908.08419-1",
          "chunk_id": 6,
          "document_id": "1908.08419",
          "start_idx": 3477,
          "end_idx": 4155
        },
        "page_content": "To sum up, the main contributions of our work are summarized as follows: The rest of this paper is organized as follows. Section SECREF2 briefly reviews the related work on CWS and active learning. Section SECREF3 presents an active learning method for CWS. We experimentally evaluate our proposed method in Section SECREF4 . Finally, Section SECREF5 concludes the paper and envisions on future work. In past decades, researches on CWS have a long history and various methods have been proposed BIBREF13 , BIBREF14 , BIBREF15 , which is an important task for Chinese NLP BIBREF7 . These methods are mainly focus on two categories: supervised learning and deep learning BIBREF2 .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are their initial results on this task?",
    "llm_answer": "The context states that the \"first n\" baseline produced the same results as the neural regressor, but provides no further details on specific initial results.  No initial performance metrics are given.\n",
    "context": [
      {
        "id": "7ceccd64-b4d3-4845-9ad0-bdb918525c5b",
        "metadata": {
          "vector_store_key": "1703.04617-0",
          "chunk_id": 39,
          "document_id": "1703.04617",
          "start_idx": 21823,
          "end_idx": 22532
        },
        "page_content": "And the first momentum is set to be 0.9 and the second 0.999. The initial learning rate is 0.0004 and the batch size is 32. We will half learning rate when meet a bad iteration, and the patience is 7. Our early stop evaluation is the EM and F1 score of validation set. All hidden states of GRUs, and TreeLSTMs are 500 dimensions, while word-level embedding $d_w$ is 300 dimensions. We set max length of document to 500, and drop the question-document pairs beyond this on training set. Explicit question-type dimension $d_{ET}$ is 50. We apply dropout to the Encoder layer and aggregation layer with a dropout rate of 0.5. Table 1 shows the official leaderboard on SQuAD test set when we submitted our system.",
        "type": "Document"
      },
      {
        "id": "2a2a04c5-1cd7-42ef-b8f5-17812a2a38dc",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 25,
          "document_id": "1909.02764",
          "start_idx": 14081,
          "end_idx": 15115
        },
        "page_content": "After that, the co-driving experimenter started with the instruction in the simulator which was followed by a familiarization drive consisting of highway and city driving and covering different driving maneuvers such as tight corners, lane changing and strong braking. Subsequently, participants started with the main driving task. The drive had a duration of 20 minutes containing the eight previously mentioned speech interactions. After the completion of the drive, the actual goal of improving automatic emotional recognition was revealed and a standard emotional intelligence questionnaire, namely the TEIQue-SF BIBREF32, was handed to the participants. Finally, a retrospective interview was conducted, in which participants were played recordings of their in-car interactions and asked to give discrete (annoyance, insecurity, joy, relaxation, boredom, none, following BIBREF8) was well as dimensional (valence, arousal, dominance BIBREF33 on a 11-point scale) emotion ratings for the interactions and the according situations.",
        "type": "Document"
      },
      {
        "id": "beee7b9e-00bc-4ef2-8232-722a694c8ed8",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 28,
          "document_id": "1909.00542",
          "start_idx": 15261,
          "end_idx": 15995
        },
        "page_content": "Therefore we do not comment on the results, other than observing that the \u201cfirst $n$\u201d baseline produces the same results as the neural regressor. As mentioned in Section SECREF3, the labels used for the classification experiments are the 5 sentences with highest ROUGE-SU4 F1 score. Macquarie University's participation in BioASQ 7 focused on the task of generating the ideal answers. The runs use query-based extractive techniques and we experiment with classification, regression, and reinforcement learning approaches. At the time of writing there were no human evaluation results, and based on ROUGE-F1 scores under cross-validation on the training data we observed that classification approaches outperform regression approaches.",
        "type": "Document"
      },
      {
        "id": "8c714d0f-b982-4837-aefc-3b59003e3a67",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 15,
          "document_id": "1912.13109",
          "start_idx": 7410,
          "end_idx": 8070
        },
        "page_content": "Train-test split: The labelled dataset that was available for this task was very limited in number of examples and thus as noted above few data augmentation techniques were applied to boost the learning of the network. Before applying augmentation, a train-test split of 78%-22% was done from the original, cleansed data set. Thus, 700 tweets/messages were held out for testing. All model evaluation were done in on the test set that got generated by this process. The results presented in this report are based on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data.",
        "type": "Document"
      },
      {
        "id": "8785152c-e78a-46ae-9684-63437d0cc834",
        "metadata": {
          "vector_store_key": "1906.06448-4",
          "chunk_id": 20,
          "document_id": "1906.06448",
          "start_idx": 11704,
          "end_idx": 12410
        },
        "page_content": "To collect high-quality annotation results, we imposed ten test questions on each worker, and removed workers who gave more than three wrong answers. We also set the minimum time it should take to complete each question to 200 seconds. 1,237 workers contributed to this task, and we annotated gold labels of 15,339 premise-hypothesis pairs. Table 3 shows the numbers of cases where answers matched gold labels automatically determined by monotonicity calculus. This table shows that there exist inference pairs whose labels are difficult even for humans to determine; there are 3,354 premise-hypothesis pairs whose gold labels as annotated by polarity computations match with those answered by all workers.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is morphology knowledge implemented in the method?",
    "llm_answer": "A morphological word segmentation method segments complex words into morphemes, reducing vocabulary size.  Different segmentation strategies (stem with combined suffix, stem with singular suffix, BPE on stem with combined suffix, BPE on stem with singular suffix) are investigated.\n",
    "context": [
      {
        "id": "c630b6be-cc52-4297-bad8-134c4951c3a8",
        "metadata": {
          "vector_store_key": "1909.12140-3",
          "chunk_id": 3,
          "document_id": "1909.12140",
          "start_idx": 2151,
          "end_idx": 2767
        },
        "page_content": "It takes a sentence as input and performs a recursive transformation process that is based upon a small set of 35 hand-crafted grammar rules for the English version and 29 rules for the German approach. These patterns were heuristically determined in a comprehensive linguistic analysis and encode syntactic and lexical features that can be derived from a sentence's parse tree. Each rule specifies (1) how to split up and rephrase the input into structurally simplified sentences and (2) how to set up a semantic hierarchy between them. They are recursively applied on a given source sentence in a top-down fashion.",
        "type": "Document"
      },
      {
        "id": "c13e5080-bd9b-4c8f-beee-70c98fa6e36e",
        "metadata": {
          "vector_store_key": "2001.01589-2",
          "chunk_id": 23,
          "document_id": "2001.01589",
          "start_idx": 12750,
          "end_idx": 13687
        },
        "page_content": "The NMT system is typically trained with a limited vocabulary, which creates bottleneck on translation accuracy and generalization capability. Many word segmentation methods have been proposed to cope with the above problems, which consider the morphological properties of different languages. Bradbury and Socher BIBREF16 employed the modified Morfessor to provide morphology knowledge into word segmentation, but they neglected the morphological varieties between subword units, which might result in ambiguous translation results. Sanchez-Cartagena and Toral BIBREF17 proposed a rule-based morphological word segmentation for Finnish, which applies BPE on all the morpheme units uniformly without distinguishing their inner morphological roles. Huck BIBREF18 explored target-side segmentation method for German, which shows that the cascading of suffix splitting and compound splitting with BPE can achieve better translation results.",
        "type": "Document"
      },
      {
        "id": "bc8e30fc-fe77-4d0c-8a56-27e0e680bd5f",
        "metadata": {
          "vector_store_key": "2001.01589-2",
          "chunk_id": 3,
          "document_id": "2001.01589",
          "start_idx": 1823,
          "end_idx": 2615
        },
        "page_content": "Moreover, due to the semantic context, the same word generally has different segmentation forms in the training corpus. For the purpose of incorporating morphology knowledge of agglutinative languages into word segmentation for NMT, we propose a morphological word segmentation method on the source-side of Turkish-English and Uyghur-Chinese machine translation tasks, which segments the complex words into simple and effective morpheme units while reducing the vocabulary size for model training. In this paper, we investigate and compare the following segmentation strategies: Stem with combined suffix Stem with singular suffix Byte Pair Encoding (BPE) BPE on stem with combined suffix BPE on stem with singular suffix The latter two segmentation strategies are our newly proposed methods.",
        "type": "Document"
      },
      {
        "id": "bfbaf498-8b9f-4a43-9b94-e6a35b0a50a8",
        "metadata": {
          "vector_store_key": "1707.03904-1",
          "chunk_id": 32,
          "document_id": "1707.03904",
          "start_idx": 17679,
          "end_idx": 18351
        },
        "page_content": "We also train a bidirectional Recurrent Neural Network (RNN) language model (based on GRU units). This model encodes both the left and right context of an entity using forward and backward GRUs, and then concatenates the final states from both to predict the entity through a softmax layer. Training is performed on the entire corpus of Stack Overflow posts, with the loss computed only over mentions of entities in the output vocabulary. This approach benefits from looking at both sides of the cloze in a query to predict the entity, as compared to the single-sided n-gram baselines. Reading comprehension models are trained to extract the answer from the given passage.",
        "type": "Document"
      },
      {
        "id": "f02ceaa0-3942-4405-9395-868ff94d3c84",
        "metadata": {
          "vector_store_key": "1703.04617-1",
          "chunk_id": 31,
          "document_id": "1703.04617",
          "start_idx": 17713,
          "end_idx": 18809
        },
        "page_content": "More specifically, the adaptation algorithm contains two steps: adapting and updating, which is detailed as follows: Adapting In the adapting step, we first compute the similarity score between an input question vector $\\mathbf {x}\\in \\mathbb {R} ^{h}$ and each centroid vector of $K$ clusters $~\\mathbf {\\bar{x}}\\in \\mathbb {R} ^{K \\times h}$ . Each cluster here models a question type. Unlike the explicit question type modeling discussed above, here we do not specify what question types we are modeling but let the system to learn. Specifically, we only need to pre-specific how many clusters, $K$ , we are modeling. The similarity between an input question and cluster centroid can be used to compute similarity weight $\\mathbf {w}^a$ :  $$w_k^a = softmax(cos\\_sim(\\mathbf {x}, \\mathbf {\\bar{x}}_k), \\alpha ), \\forall k \\in [1, \\dots , K]$$   (Eq. 43)  $$cos\\_sim(\\mathbf {u}, \\mathbf {v}) = \\frac{<\\mathbf {u},\\mathbf {v}>}{||\\mathbf {u}|| \\cdot ||\\mathbf {v}||}$$   (Eq. 44)  We set $\\alpha $ equals 50 to make sure only closest class will have a high weight while maintain differentiable.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the performance on the task evaluated?",
    "llm_answer": "The performance is evaluated using F<sub>1</sub> score for question answering, training and testing curves (averaged over 3 random seeds),  test performance corresponding to best validation performance, F<sub>1info</sub> score, Search Accuracy, Reading Accuracy, Overall Accuracy, average accuracy of predictions, NoOffer rate, Multi-query rate, Goal change rate, and dialogue length distribution.\n",
    "context": [
      {
        "id": "6a443055-4b4a-46e1-a2dc-b3211ac62d38",
        "metadata": {
          "vector_store_key": "1908.10449-0",
          "chunk_id": 20,
          "document_id": "1908.10449",
          "start_idx": 10741,
          "end_idx": 11449
        },
        "page_content": "Since iMRC involves both MRC and RL, we adopt evaluation metrics from both settings. First, as a question answering task, we use $\\text{F}_1$ score to compare predicted answers against ground-truth, as in previous works. When there exist multiple ground-truth answers, we report the max $\\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we evaluate an agent's performance during both its training and testing phases. During training, we report training curves averaged over 3 random seeds. During test, we follow common practice in supervised learning tasks where we report the agent's test performance corresponding to its best validation performance .",
        "type": "Document"
      },
      {
        "id": "4f019990-5d6d-4d5e-a988-4cc371da0483",
        "metadata": {
          "vector_store_key": "1908.10449-0",
          "chunk_id": 45,
          "document_id": "1908.10449",
          "start_idx": 24390,
          "end_idx": 25098
        },
        "page_content": "The agent's test performance is reported in Table TABREF41. In addition, to support our claim that the challenging part of iMRC tasks is information seeking rather than answering questions given sufficient information, we also report the $\\text{F}_1$ score of an agent when it has reached the piece of text that contains the answer, which we denote as $\\text{F}_{1\\text{info}}$. From Table TABREF41 (and validation curves provided in appendix) we can observe that QA-DQN's performance during evaluation matches its training performance in most settings. $\\text{F}_{1\\text{info}}$ scores are consistently higher than the overall $\\text{F}_1$ scores, and they have much less variance across different settings.",
        "type": "Document"
      },
      {
        "id": "097dcb3f-7129-4a69-8615-32f32df0ff09",
        "metadata": {
          "vector_store_key": "1707.03904-0",
          "chunk_id": 35,
          "document_id": "1707.03904",
          "start_idx": 19449,
          "end_idx": 20060
        },
        "page_content": "For these, we refer to the fraction of instances for which the correct answer is present in the context as Search Accuracy. The performance of the baseline among these instances is referred to as the Reading Accuracy, and the overall performance (which is a product of the two) is referred to as the Overall Accuracy. In Figure 4 we compare how these three vary as the number of context documents is varied. Naturally, the search accuracy increases as the context size increases, however at the same time reading performance decreases since the task of extracting the answer becomes harder for longer documents.",
        "type": "Document"
      },
      {
        "id": "3ce49f78-b184-4185-86f5-3cb0edb7a6de",
        "metadata": {
          "vector_store_key": "1707.03904-0",
          "chunk_id": 23,
          "document_id": "1707.03904",
          "start_idx": 12736,
          "end_idx": 13455
        },
        "page_content": "Once context documents had been built, we extracted the subset of questions where the answer string, excluded from the query for the two-phase search, was nonetheless present in the context document. This subset allows us to evaluate the performance of the reading system independently from the search system, while the full set allows us to evaluate the performance of Quasar as a whole. We also split the full set into training, validation and test sets. The final size of each data subset after all discards is listed in Table 1 . Evaluation is straightforward on Quasar-S since each answer comes from a fixed output vocabulary of entities, and we report the average accuracy of predictions as the evaluation metric.",
        "type": "Document"
      },
      {
        "id": "87ca473d-2032-49c5-ac06-9a1b554c964b",
        "metadata": {
          "vector_store_key": "2002.11893-3",
          "chunk_id": 37,
          "document_id": "2002.11893",
          "start_idx": 21224,
          "end_idx": 21967
        },
        "page_content": "In this situation, the wizard will try to relax some constraints and issue multiple queries to find some results for a recommendation while the user will compromise and change the original goal. The negotiation process is captured by \"NoOffer rate\", \"Multi-query rate\", and \"Goal change rate\" in Table TABREF26. In addition, \"Multi-query rate\" suggests that each sub-goal in M and M+T is as easy to finish as the goal in S. The distribution of dialogue length is shown in Figure FIGREF27, which is an indicator of the task complexity. Most single-domain dialogues terminate within 10 turns. The curves of M and M+T are almost of the same shape, which implies that the traffic task requires two additional turns on average to complete the task.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what datasets were used?",
    "llm_answer": "Laptop, Restaurant, avsd, Charades, CUB, MSCOCO, Oxford-102, MNIST, Oxford, COCO, CIFAR-10.\n",
    "context": [
      {
        "id": "1b887290-2c77-48b6-b1a2-fc513d707afc",
        "metadata": {
          "vector_store_key": "1909.00124-1",
          "chunk_id": 20,
          "document_id": "1909.00124",
          "start_idx": 11467,
          "end_idx": 12190
        },
        "page_content": "The other two datasets are laptop and restaurant datasets collected from SemEval-2016 . The former consists of laptop review sentences and the latter consists of restaurant review sentences. The original datasets (i.e., Laptop and Restaurant) were annotated with aspect polarity in each sentence. We used all sentences with only one polarity (positive or negative) for their aspects. That is, we only used sentences with aspects having the same sentiment label in each sentence. Thus, the sentiment of each aspect gives the ground-truth as the sentiments of all aspects are the same. For each clean-labeled dataset, the sentences are randomly partitioned into training set and test set with $80\\%$ and $20\\%$, respectively.",
        "type": "Document"
      },
      {
        "id": "73a83f60-b2c9-406d-b4cd-5720639e5f06",
        "metadata": {
          "vector_store_key": "1812.07023-4",
          "chunk_id": 18,
          "document_id": "1812.07023",
          "start_idx": 10358,
          "end_idx": 10986
        },
        "page_content": "After 10 such qa turns, the questioner wraps up by writing a summary of the video based on the conversation. Dataset statistics such as the number of dialogues, turns, and words for the avsd dataset are presented in Table TABREF5 . For the initially released prototype dataset, the training set of the avsd dataset corresponds to videos taken from the training set of the Charades dataset while the validation and test sets of the avsd dataset correspond to videos taken from the validation set of the Charades dataset. For the official dataset, training, validation and test sets are drawn from the corresponding Charades sets.",
        "type": "Document"
      },
      {
        "id": "1e2bf238-b0a0-40af-9d8d-e6fea4ad79d0",
        "metadata": {
          "vector_store_key": "1910.09399-3",
          "chunk_id": 105,
          "document_id": "1910.09399",
          "start_idx": 56273,
          "end_idx": 57009
        },
        "page_content": "For example, image data captured from a variety of photo-ready devices, such as smart-phones, and online social media services opened the door to the analysis of large amounts of media datasets BIBREF70. The availability of large media datasets allow new frameworks and algorithms to be proposed and tested on real-world data. A summary of some reviewed methods and benchmark datasets used for validation is reported in Table TABREF43. In addition, the performance of different GANs with respect to the benchmark datasets and performance metrics is reported in Table TABREF48. In order to synthesize images from text descriptions, many frameworks have taken a minimalistic approach by creating small and background-less images BIBREF73.",
        "type": "Document"
      },
      {
        "id": "eac31fee-fbdc-4a80-be44-f2145c81562a",
        "metadata": {
          "vector_store_key": "1910.09399-3",
          "chunk_id": 106,
          "document_id": "1910.09399",
          "start_idx": 56647,
          "end_idx": 57443
        },
        "page_content": "In order to synthesize images from text descriptions, many frameworks have taken a minimalistic approach by creating small and background-less images BIBREF73. In most cases, the experiments were conducted on simple datasets, initially containing images of birds and flowers. BIBREF8 contributed to these data sets by adding corresponding natural language text descriptions to subsets of the CUB, MSCOCO, and Oxford-102 datasets, which facilitated the work on text-to-image synthesis for several papers released more recently. While most deep learning algorithms use MNIST BIBREF74 dataset as the benchmark, there are three main datasets that are commonly used for evaluation of proposed GAN models for text-to-image synthesis: CUB BIBREF75, Oxford BIBREF76, COCO BIBREF77, and CIFAR-10 BIBREF78.",
        "type": "Document"
      },
      {
        "id": "731d76b9-6365-4a8c-872e-c14badc3b626",
        "metadata": {
          "vector_store_key": "1911.07228-0",
          "chunk_id": 12,
          "document_id": "1911.07228",
          "start_idx": 6115,
          "end_idx": 6702
        },
        "page_content": "The dataset contains four different types of label: Location (LOC), Person (PER), Organization (ORG) and Miscellaneous - Name of an entity that do not belong to 3 types above (Table TABREF15). Although the corpus has more information about the POS and chunks, but we do not use them as features in our model. There are two folders with 267 text files of training data and 45 text files of test data. They all have their own format. We take 21 first text files and 22 last text files and 22 sentences of the 22th text file and 55 sentences of the 245th text file to be a development data.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they obtain human judgements?",
    "llm_answer": "Human judgments are obtained via valence ratings for nearly 14,000 English nouns collected by BIBREF28, and for acceptability judgments in the context of dialogue model evaluation.  For the latter, human judgments are the ground-truth comments of corresponding news.\n",
    "context": [
      {
        "id": "5f7d468c-b40d-4dfc-99f9-6c48ffb2c777",
        "metadata": {
          "vector_store_key": "2001.07209-3",
          "chunk_id": 20,
          "document_id": "2001.07209",
          "start_idx": 10638,
          "end_idx": 11331
        },
        "page_content": "We evaluated our models in two ways: classification of moral seed words on all three tiers (moral relevance, polarity, and fine-grained categories), and correlation of model predictions with human judgments. In this evaluation, we assessed the ability of our models to classify the seed words that compose our moral environment in a leave-one-out classification task. We performed the evaluation for all three classification tiers: 1) moral relevance, where seed words are split into morally relevant and morally irrelevant; 2) moral polarity, where moral seed words are split into positive and negative; 3) fine-grained categories, where moral seed words are split into the 10 MFT categories.",
        "type": "Document"
      },
      {
        "id": "c1b30775-44da-486c-99e5-cf9f5206635c",
        "metadata": {
          "vector_store_key": "2001.07209-2",
          "chunk_id": 22,
          "document_id": "2001.07209",
          "start_idx": 11711,
          "end_idx": 12526
        },
        "page_content": "We also observe that models using word embeddings trained on Google N-grams perform better than those trained on COHA, which could be expected given the larger corpus size of the former. In the remaining analyses, we employ the Centroid model, which offers competitive accuracy and a simple, parameter-free specification. We evaluated the approximate agreement between our methodology and human judgments using valence ratings, i.e., the degree of pleasantness or unpleasantness of a stimulus. Our assumption is that the valence of a concept should correlate with its perceived moral polarity, e.g., morally repulsive ideas should evoke an unpleasant feeling. However, we do not expect this correspondence to be perfect; for example, the concept of dessert evokes a pleasant reaction without being morally relevant.",
        "type": "Document"
      },
      {
        "id": "39a5dea2-5692-47b5-8ce7-671576688d56",
        "metadata": {
          "vector_store_key": "2001.07209-2",
          "chunk_id": 23,
          "document_id": "2001.07209",
          "start_idx": 12526,
          "end_idx": 13238
        },
        "page_content": "However, we do not expect this correspondence to be perfect; for example, the concept of dessert evokes a pleasant reaction without being morally relevant. In this analysis, we took the valence ratings for the nearly 14,000 English nouns collected by BIBREF28 and, for each query word $q$, we generated a corresponding prediction of positive moral polarity from our model, $P(c_+\\,|\\,\\mathbf {q})$. Table TABREF16 shows the correlations between human valence ratings and predictions of positive moral polarity generated by models trained on each of our corpora. We observe that the correlations are significant, suggesting the ability of our methodology to capture relevant features of moral sentiment from text.",
        "type": "Document"
      },
      {
        "id": "64edb5cd-35ef-4eaa-bfb9-4e585e04e2a3",
        "metadata": {
          "vector_store_key": "1809.04960-1",
          "chunk_id": 29,
          "document_id": "1809.04960",
          "start_idx": 16925,
          "end_idx": 17580
        },
        "page_content": "Inspired by the evaluation methods of dialogue models, we formulate the evaluation as a ranking problem. Given a piece of news and a set of candidate comments, the comment model should return the rank of the candidate comments. The candidate comment set consists of the following parts: Correct: The ground-truth comments of the corresponding news provided by the human. Plausible: The 50 most similar comments to the news. We use the news as the query to retrieve the comments that appear in the training set based on the cosine similarity of their tf-idf values. We select the top 50 comments that are not the correct comments as the plausible comments.",
        "type": "Document"
      },
      {
        "id": "53a88b81-9643-488f-8482-2ab1717eac88",
        "metadata": {
          "vector_store_key": "1809.08731-1",
          "chunk_id": 7,
          "document_id": "1809.08731",
          "start_idx": 4442,
          "end_idx": 5210
        },
        "page_content": "One subject of debates are acceptability judgments: while, for many, acceptability is a binary condition on membership in a set of well-formed sentences BIBREF3 , others assume that it is gradient in nature BIBREF13 , BIBREF2 . Tackling this research question, lau2017grammaticality aimed at modeling human acceptability judgments automatically, with the goal to gain insight into the nature of human perception of acceptability. In particular, they tried to answer the question: Do humans judge acceptability on a gradient scale? Their experiments showed a strong correlation between human judgments and normalized sentence log-probabilities under a variety of LMs for artificial data they had created by translating and back-translating sentences with neural models.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Does this approach perform better in the multi-domain or single-domain setting?",
    "llm_answer": "Multi-domain.\n",
    "context": [
      {
        "id": "aedfd81c-8fc1-4370-a491-73bedbb556ab",
        "metadata": {
          "vector_store_key": "1909.02764-4",
          "chunk_id": 39,
          "document_id": "1909.02764",
          "start_idx": 21643,
          "end_idx": 22364
        },
        "page_content": "With this procedure we achieve an average performance of F$_1$=75 %, being better than the results from the in-domain Experiment 1. The best performance of F$_1$=76 % is achieved with the model pre-trained on each data set, except for ISEAR. All transfer learning models clearly outperform their simple out-of-domain counterpart. To ensure that this performance increase is not only due to the larger data set, we compare these results to training the model without transfer on a corpus consisting of each corpus together with AMMER (again, in leave-one-out crossvalidation). These results are depicted in column \u201cJoint C.\u201d. Thus, both settings, \u201ctransfer learning\u201d and \u201cjoint corpus\u201d have access to the same information.",
        "type": "Document"
      },
      {
        "id": "e81fee71-223a-449b-b9b3-75b7e7e7caec",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 41,
          "document_id": "1909.13375",
          "start_idx": 21792,
          "end_idx": 22634
        },
        "page_content": "When comparing single-span performance, our best model exhibits slightly better results, but it should be noted that it retains the single-span heads from NABERT+, while in MTMSN they have one head to predict both single-span and multi-span answers. For a fairer comparison, we trained our model with the single-span heads removed, where our multi-span head remained the only head aimed for handling span questions. With this no-single-span-heads setting, while our multi-span performance even improved a bit, our single-span performance suffered a slight drop, ending up trailing by 0.8 in EM and 0.6 in F1 compared to MTMSN. Therefore, it could prove beneficial to try and analyze the reasons behind each model's (ours and MTMSN) relative advantages, and perhaps try to combine them into a more holistic approach of tackling span questions.",
        "type": "Document"
      },
      {
        "id": "93b8b741-b6d7-4597-9fce-2de82a1eb34e",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 42,
          "document_id": "1909.13375",
          "start_idx": 22170,
          "end_idx": 22785
        },
        "page_content": "Therefore, it could prove beneficial to try and analyze the reasons behind each model's (ours and MTMSN) relative advantages, and perhaps try to combine them into a more holistic approach of tackling span questions. Table TABREF25 shows the results on DROP's test set, with our model being the best overall as of the time of writing, and not just on multi-span questions. In order to analyze the effect of each of our changes, we conduct ablation studies on the development set, depicted in Table TABREF26. Not using the simple preprocessing from Section SECREF17 resulted in a 2.5 point decrease in both EM and F1.",
        "type": "Document"
      },
      {
        "id": "ed2b65a3-293a-4a8c-874e-949c480c257f",
        "metadata": {
          "vector_store_key": "2002.11893-4",
          "chunk_id": 13,
          "document_id": "2002.11893",
          "start_idx": 8009,
          "end_idx": 8625
        },
        "page_content": "Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context. To make workers understand the task more easily, we crafted templates to generate natural language descriptions for each structured goal. Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality.",
        "type": "Document"
      },
      {
        "id": "03ea926c-f9d9-40fd-9d42-6be5a2b1be55",
        "metadata": {
          "vector_store_key": "1701.02877-9",
          "chunk_id": 82,
          "document_id": "1701.02877",
          "start_idx": 41831,
          "end_idx": 42570
        },
        "page_content": "As demonstrated by the above experiments, and in line with related work, NERC performance varies across domains while also being influenced by the size of the available in-domain training data. Prior work on transfer learning and domain adaptation (e.g. BIBREF16 ) has aimed at increasing performance in domains where only small amounts of training data are available. This is achieved by adding out-of domain data from domains where larger amounts of training data exist. For domain adaptation to be successful, however, the seed domain needs to be similar to the target domain, i.e. if there is no or very little overlap in terms of contexts of the training and testing instances, the model does not learn any additional helpful weights.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many samples did they generate for the artificial language?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "bb96f53b-98f0-4811-a691-0975f908cb31",
        "metadata": {
          "vector_store_key": "1909.00694-2",
          "chunk_id": 26,
          "document_id": "1909.00694",
          "start_idx": 15102,
          "end_idx": 15722
        },
        "page_content": "The dimension of hidden units was 256. The optimizer was Momentum SGD BIBREF21. The mini-batch size was 1024. We ran 100 epochs and selected the snapshot that achieved the highest score for the dev set. We used a Japanese BERT model pretrained with Japanese Wikipedia. The input sentences were segmented into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam BIBREF19.",
        "type": "Document"
      },
      {
        "id": "c835fe0c-f240-4e5f-96f5-494421121339",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 6,
          "document_id": "1910.04269",
          "start_idx": 3498,
          "end_idx": 4224
        },
        "page_content": "The model uses a cyclic learning rate where learning rate increases and then decreases linearly. Maximum learning rate for a cycle is set by finding the optimal learning rate using fastai BIBREF15 library. The model classified six languages \u2013 English, French, Spanish, Russian, Italian and German \u2013 and achieving an accuracy of 89.0%. Gazeau et al. BIBREF16 in his research showed how Neural Networks, Support Vector Machine and Hidden Markov Model (HMM) can be used to identify French, English, Spanish and German. Dataset was prepared using voice samples from Youtube News BIBREF17and VoxForge BIBREF6 datasets. Hidden Markov models convert speech into a sequence of vectors, was used to capture temporal features in speech.",
        "type": "Document"
      },
      {
        "id": "5ab82d1c-620b-40bb-afe5-ef89b356baff",
        "metadata": {
          "vector_store_key": "1910.04269-1",
          "chunk_id": 31,
          "document_id": "1910.04269",
          "start_idx": 17739,
          "end_idx": 18415
        },
        "page_content": "This results in significant variation of speech quality between samples making it more representative of real world scenarios. Our dataset consists of 1,500 samples for each of six languages. Out of 1,500 samples for each language, 1,200 were randomly selected as training dataset for that language and rest 300 as validation dataset using k-fold cross-validation. To sum up, we trained our model on 7,200 samples and validated it on 1800 samples comprising six languages. The results are discussed in next section. This paper discusses two end-to-end approaches which achieve state-of-the-art results in both the image as well as audio domain on the VoxForge dataset BIBREF6.",
        "type": "Document"
      },
      {
        "id": "088347bc-6391-466f-81eb-e37d14e108ba",
        "metadata": {
          "vector_store_key": "1705.09665-6",
          "chunk_id": 23,
          "document_id": "1705.09665",
          "start_idx": 13165,
          "end_idx": 13951
        },
        "page_content": "Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities ( INLINEFORM0 ), for a total of 4,872 community-months ( INLINEFORM1 ). Estimating linguistic measures. We estimate word frequencies INLINEFORM0 , and by extension each downstream measure, in a carefully controlled manner in order to ensure we capture robust and meaningful linguistic behaviour.",
        "type": "Document"
      },
      {
        "id": "8c714d0f-b982-4837-aefc-3b59003e3a67",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 15,
          "document_id": "1912.13109",
          "start_idx": 7410,
          "end_idx": 8070
        },
        "page_content": "Train-test split: The labelled dataset that was available for this task was very limited in number of examples and thus as noted above few data augmentation techniques were applied to boost the learning of the network. Before applying augmentation, a train-test split of 78%-22% was done from the original, cleansed data set. Thus, 700 tweets/messages were held out for testing. All model evaluation were done in on the test set that got generated by this process. The results presented in this report are based on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Why does not the approach from English work on other languages?",
    "llm_answer": "The approach struggles with subtle relationships between words, particularly conjunctions, and lacks co-reference information.  It also performs less well on languages with different sentence structures (SOV, VSO) compared to those with SVO structure.  Finally,  the approach may not handle the different morphological features of other languages as effectively as it does English.\n",
    "context": [
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      },
      {
        "id": "43428dd0-ea93-432b-bcf3-a6846d85f69a",
        "metadata": {
          "vector_store_key": "1906.04571-1",
          "chunk_id": 22,
          "document_id": "1906.04571",
          "start_idx": 12669,
          "end_idx": 13335
        },
        "page_content": "This is because they reflect well-known rules for each language. That said, they have lower recall than our approach because they fail to capture more subtle relationships. For both languages, our approach struggles with conjunctions. For example, consider the phrase \u00e9l es un ingeniero y escritor (he is an engineer and a writer). Replacing ingeniero with ingeniera does not necessarily result in escritor being changed to escritora. This is because two nouns do not normally need to have the same gender when they are conjoined. Moreover, our MRF does not include co-reference information, so it cannot tell that, in this case, both nouns refer to the same person.",
        "type": "Document"
      },
      {
        "id": "dd0a3096-9615-48ec-a2e0-80c08aa831d0",
        "metadata": {
          "vector_store_key": "1911.00069-1",
          "chunk_id": 43,
          "document_id": "1911.00069",
          "start_idx": 25020,
          "end_idx": 25681
        },
        "page_content": "The source language, English, belongs to the SVO (Subject, Verb, Object) language family where in a sentence the subject comes first, the verb second, and the object third. Spanish, Italian, Portuguese, German (in conventional typology) and Chinese also belong to the SVO language family, and our approach achieves over $70\\%$ relative accuracy for these languages. On the other hand, Japanese belongs to the SOV (Subject, Object, Verb) language family and Arabic belongs to the VSO (Verb, Subject, Object) language family, and our approach achieves lower relative accuracy for these two languages. There are a few weakly supervised cross-lingual RE approaches.",
        "type": "Document"
      },
      {
        "id": "50580b33-3f37-4708-bb48-721919a2f597",
        "metadata": {
          "vector_store_key": "1910.05456-7",
          "chunk_id": 18,
          "document_id": "1910.05456",
          "start_idx": 9647,
          "end_idx": 10340
        },
        "page_content": "Spanish (SPA), in contrast, is morphologically rich, and disposes of much larger verbal paradigms than English. Like English, it is a suffixing language, and it additionally makes use of internal stem changes (e.g., o $\\rightarrow $ ue). Since English and Spanish are both Indo-European languages, and, thus, relatively similar, we further add a third, unrelated target language. We choose Zulu (ZUL), a Bantoid language. In contrast to the first two, it is strongly prefixing. For pretraining, we choose languages with different degrees of relatedness and varying morphological similarity to English, Spanish, and Zulu. We limit our experiments to languages which are written in Latin script.",
        "type": "Document"
      },
      {
        "id": "632c17fe-09a7-412d-9b64-8ac8bbe6bfe6",
        "metadata": {
          "vector_store_key": "1910.11471-1",
          "chunk_id": 5,
          "document_id": "1910.11471",
          "start_idx": 2929,
          "end_idx": 3589
        },
        "page_content": "However, the authors encountered challenges to support different features of both languages. One of the motivations behind this paper is - as long as it is about programming, there is a finite and small set of expression which is used in human vocabulary. For instance, programmers express a for-loop in a very few specific ways BIBREF3. Variable declaration and value assignment expressions are also limited in nature. Although all codes are executable, human representation through text may not due to the semantic brittleness of code. Since high-level languages have a wide range of syntax, programmers use different linguistic expressions to explain those.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they measure grammaticality?",
    "llm_answer": "The provided text describes a method for evaluating fluency, not grammaticality.  No method for measuring grammaticality is described.\n",
    "context": [
      {
        "id": "76d3d173-f4c6-4f2d-9d08-ac7cff5154ab",
        "metadata": {
          "vector_store_key": "1809.08731-3",
          "chunk_id": 36,
          "document_id": "1809.08731",
          "start_idx": 20112,
          "end_idx": 20896
        },
        "page_content": "Fluency evaluation is related to grammatical error detection BIBREF19 , BIBREF20 , BIBREF21 , BIBREF22 and grammatical error correction BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26 , BIBREF27 . However, it differs from those in several aspects; most importantly, it is concerned with the degree to which errors matter to humans. Work on automatic fluency evaluation in NLP has been rare. heilman2014predicting predicted the fluency (which they called grammaticality) of sentences written by English language learners. In contrast to ours, their approach is supervised. stent2005evaluating and cahill2009correlating found only low correlation between automatic metrics and fluency ratings for system-generated English paraphrases and the output of a German surface realiser, respectively.",
        "type": "Document"
      },
      {
        "id": "b278d524-a8ae-4324-95f9-606c8bb32cd8",
        "metadata": {
          "vector_store_key": "2003.07433-0",
          "chunk_id": 37,
          "document_id": "2003.07433",
          "start_idx": 20219,
          "end_idx": 20914
        },
        "page_content": "After the PTSD Linguistic Dictionary has been created, we empirically evaluate its psychometric properties such as reliability and validity as per American Standards for educational and psychological testing guideline BIBREF24. In psychometrics, reliability is most commonly evaluated by Cronbach's alpha, which assesses internal consistency based on inter-correlations and the number of measured items. In the text analysis scenario, each word in our PTSD Linguistic dictionary is considered an item, and reliability is calculated based on each text file's response to each word item, which forms an $N$(number of text files) $\\times $ $J$(number of words or stems in a dictionary) data matrix.",
        "type": "Document"
      },
      {
        "id": "a83016e8-cd42-4140-8227-73248ddcfbb1",
        "metadata": {
          "vector_store_key": "1809.08731-3",
          "chunk_id": 1,
          "document_id": "1809.08731",
          "start_idx": 755,
          "end_idx": 1497
        },
        "page_content": "Hence, it is not guaranteed that a correct output will match any of a finite number of given references. This results in difficulties for current reference-based evaluation, especially of fluency, causing word-overlap metrics like ROUGE BIBREF1 to correlate only weakly with human judgments BIBREF2 . As a result, fluency evaluation of NLG is often done manually, which is costly and time-consuming. Evaluating sentences on their fluency, on the other hand, is a linguistic ability of humans which has been the subject of a decade-long debate in cognitive science. In particular, the question has been raised whether the grammatical knowledge that underlies this ability is probabilistic or categorical in nature BIBREF3 , BIBREF4 , BIBREF5 .",
        "type": "Document"
      },
      {
        "id": "4bc79961-bbfe-4650-9d5f-01691d719007",
        "metadata": {
          "vector_store_key": "1809.08731-3",
          "chunk_id": 37,
          "document_id": "1809.08731",
          "start_idx": 20896,
          "end_idx": 21712
        },
        "page_content": "In contrast to ours, their approach is supervised. stent2005evaluating and cahill2009correlating found only low correlation between automatic metrics and fluency ratings for system-generated English paraphrases and the output of a German surface realiser, respectively. Explicit fluency evaluation of NLG, including compression and the related task of summarization, has mostly been performed manually. vadlapudi-katragadda:2010:SRW used LMs for the evaluation of summarization fluency, but their models were based on part-of-speech tags, which we do not require, and they were non-neural. Further, they evaluated longer texts, not single sentences like we do. toutanova2016dataset compared 80 word-overlap metrics for evaluating the content and fluency of compressions, finding only low correlation with the latter.",
        "type": "Document"
      },
      {
        "id": "0a7fa348-9c0a-4026-9c47-bb54db96637f",
        "metadata": {
          "vector_store_key": "1910.11493-3",
          "chunk_id": 0,
          "document_id": "1910.11493",
          "start_idx": 0,
          "end_idx": 749
        },
        "page_content": "While producing a sentence, humans combine various types of knowledge to produce fluent output\u2014various shades of meaning are expressed through word selection and tone, while the language is made to conform to underlying structural rules via syntax and morphology. Native speakers are often quick to identify disfluency, even if the meaning of a sentence is mostly clear. Automatic systems must also consider these constraints when constructing or processing language. Strong enough language models can often reconstruct common syntactic structures, but are insufficient to properly model morphology. Many languages implement large inflectional paradigms that mark both function and content words with a varying levels of morphosyntactic information.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the difference in recall score between the systems?",
    "llm_answer": "The answer is not found in the context.  While the text discusses recall scores for different models and scenarios, it does not provide a direct comparison of recall scores *between* systems.\n",
    "context": [
      {
        "id": "0f988aff-b981-4c53-bf75-f0a2f8723456",
        "metadata": {
          "vector_store_key": "1910.03814-1",
          "chunk_id": 36,
          "document_id": "1910.03814",
          "start_idx": 19592,
          "end_idx": 20335
        },
        "page_content": "Fig. FIGREF32 shows the Precision vs Recall plot and the ROC curve (which plots the True Positive Rate vs the False Positive Rate) of the different models. First, notice that given the subjectivity of the task and the discrepancies between annotators, getting optimal scores in the evaluation metrics is virtually impossible. However, a system with relatively low metric scores can still be very useful for hate speech detection in a real application: it will fire on publications for which most annotators agree they are hate, which are often the stronger attacks. The proposed LSTM to detect hate speech when only text is available, gets similar results as the method presented in BIBREF7, which we trained with MMHS150K and the same splits.",
        "type": "Document"
      },
      {
        "id": "37743af6-19bd-417e-b520-ce818629c1a2",
        "metadata": {
          "vector_store_key": "1809.04960-4",
          "chunk_id": 37,
          "document_id": "1809.04960",
          "start_idx": 21462,
          "end_idx": 22131
        },
        "page_content": "With the help of our proposed model, both IR and S2S achieve an improvement under the semi-supervised scenarios. We analyze the performance of the proposed method under the semi-supervised setting. We train the supervised IR model with different numbers of paired data. Figure FIGREF39 shows the curve (blue) of the recall1 score. As expected, the performance grows as the paired dataset becomes larger. We further combine the supervised IR with our unsupervised model, which is trained with full unpaired data (4.8M) and different number of paired data (from 50K to 4.8M). It shows that IR+Proposed can outperform the supervised IR model given the same paired dataset.",
        "type": "Document"
      },
      {
        "id": "b793e1d3-56ff-490e-9245-8753da5bf16c",
        "metadata": {
          "vector_store_key": "1911.07228-2",
          "chunk_id": 17,
          "document_id": "1911.07228",
          "start_idx": 8542,
          "end_idx": 9180
        },
        "page_content": "In our experiments, we use three evaluation parameters (precision, recall, and F1 score) to access our experimental result. They will be described as follow in Table 3. The \"correctNE\", the number of correct label for entity that the model can found. The \"goldNE\", number of the real label annotated by annotator in the gold data. The \"foundNE\", number of the label the model find out (no matter if they are correct or not). In Table 3 above, we can see that recall score on ORG label is lowest. The reason is almost all the ORG label on test file is name of some brands that do not appear on training data and pre-trained word embedding.",
        "type": "Document"
      },
      {
        "id": "25869863-4787-4726-b7fc-252a3b6e6266",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 26,
          "document_id": "1909.00542",
          "start_idx": 14127,
          "end_idx": 14823
        },
        "page_content": "Among precision, recall and F1, both precision and F1 are similar, but precision gives a better correlation. Recall shows poor correlation, and virtually no correlation when using the revised Kendall measure. For reporting the evaluation of results, it will be therefore more useful to use precision or F1. However, given the small difference between precision and F1, and given that precision may favour short summaries when used as a function to optimise in a machine learning setting (e.g. using reinforcement learning), it may be best to use F1 as the metric to optimise. Fig. FIGREF40 shows the scatterplots of ROUGE-SU4 recall, precision and F1 with respect to the average human evaluation.",
        "type": "Document"
      },
      {
        "id": "e6795723-3216-44ca-8502-5a1d9c159a1c",
        "metadata": {
          "vector_store_key": "1701.02877-4",
          "chunk_id": 54,
          "document_id": "1701.02877",
          "start_idx": 27935,
          "end_idx": 28736
        },
        "page_content": "Memorisation is discussed in more depth in Section \"RQ5: Out-Of-Domain NER Performance and Memorisation\" . When NERC results are compared to the corpus diversity statistics, i.e. NE/Unique NE ratios (Table 4 ), token/type ratios (Table 5 ), and tag density (Table 6 ), the strongest predictor for F1 is tag density, as can be evidenced by the R correlation values between the ratios and F1 scores with the Stanford NER system, shown in the respective tables. There is a positive correlation between high F1 and high tag density (R of 0.57 and R of 0.62 with normalised tag density), a weak positive correlation for NE/unique ratios (R of 0.20 and R of 0.15 for normalised ratio), whereas for token/type ratios, no such clear correlation can be observed (R of 0.25 and R of -0.07 for normalised ratio).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is their f1 score and recall?",
    "llm_answer": "The provided text discusses F1 scores and recall in relation to various tasks and datasets, but does not give a single, overall F1 score and recall for a specific model or dataset.  The answer is not found in the context.\n",
    "context": [
      {
        "id": "0fbb350a-16ee-4a8b-a81f-4f933b2dab44",
        "metadata": {
          "vector_store_key": "1703.04617-3",
          "chunk_id": 44,
          "document_id": "1703.04617",
          "start_idx": 24348,
          "end_idx": 24966
        },
        "page_content": "And the other part accounts for 30.90%, of which the average F1 score is 30.03%. For the latter, we can further divide it into two sub-parts: one is where the F1 score equals to 0%, which means that predict answer is totally wrong. This part occupies 14.89% of the total development set. The other part accounts for 16.01% of the development set, of which average F1 score is 57.96%. From this analysis we can see that reducing the zero F1 score (14.89%) is potentially an important direction to further improve the system. Closely modelling questions could be of importance for question answering and machine reading.",
        "type": "Document"
      },
      {
        "id": "ca03d5d1-b87e-43e6-8b4e-185db97b1ef6",
        "metadata": {
          "vector_store_key": "1701.02877-0",
          "chunk_id": 70,
          "document_id": "1701.02877",
          "start_idx": 35785,
          "end_idx": 36572
        },
        "page_content": "Interestingly, average F1 on seen NEs in the Twitter corpora (MSM and Ritter) is around 80, whereas average F1 on the ACE corpora, which are of similar size, is lower, at around 70. To summarise, our findings are: [noitemsep] F1 on unseen NEs is significantly lower than F1 on seen NEs for all three NERC approaches, which is mostly due to recall on unseen NEs being lower than that on seen NEs. Performance on seen NEs is significantly and consistently higher than that of unseen NEs in different corpora, with the lower scores mostly attributable to lower recall. However, there are still significant differences at labelling seen NEs in different corpora, which means that if NEs are seen or unseen does not account for all of the difference of F1 between corpora of different genres.",
        "type": "Document"
      },
      {
        "id": "ad92e3f4-226c-4b8b-be2b-6a58745b9775",
        "metadata": {
          "vector_store_key": "2002.11402-1",
          "chunk_id": 15,
          "document_id": "2002.11402",
          "start_idx": 8763,
          "end_idx": 9339
        },
        "page_content": "As you can see in Table 2 & 3, recall is great for our model but precision is not good as Model is also trying to detect new potential topics which are not there even in reference Wikipedia-Titles and NER sets. In capturing Wikipedia topics our model clearly surpasses other models in all scores. Spacy results are good despite not being trained for case-less data. In terms of F1 and overall stability Spacy did better than Stanford NER, on our News Validation set. Similarly, Stanford did well in Precision but could not catch up with Spacy and our model in terms of Recall.",
        "type": "Document"
      },
      {
        "id": "5567ec52-3c38-41b4-b624-2bfe13262a85",
        "metadata": {
          "vector_store_key": "1703.04617-3",
          "chunk_id": 43,
          "document_id": "1703.04617",
          "start_idx": 23973,
          "end_idx": 24558
        },
        "page_content": "In Figure UID61 we can see that the average EM/F1 of the \"when\" question is highest and those of the \"why\" question is the lowest. From Figure UID62 we can see the \"what\" question is the major class. Figure 5 shows the composition of F1 score. Take our best model as an example, we observed a 78.38% F1 score on the whole development set, which can be separated into two parts: one is where F1 score equals to 100%, which means an exact match. This part accounts for 69.10% of the entire development set. And the other part accounts for 30.90%, of which the average F1 score is 30.03%.",
        "type": "Document"
      },
      {
        "id": "47dfa681-6856-4951-9180-c2a5082d7d78",
        "metadata": {
          "vector_store_key": "1701.02877-4",
          "chunk_id": 48,
          "document_id": "1701.02877",
          "start_idx": 24702,
          "end_idx": 25448
        },
        "page_content": "To do this we study again Precision (P), Recall (R) and F1 metrics on size-normalised corpora (Table UID9 ), on original corpora (Tables \"RQ1: NER performance with Different Approaches\" and \"RQ1: NER performance with Different Approaches\" ), and we further test performance per genre in a separate table (Table 3 ). F1 scores over size-normalised corpora vary widely (Table UID9 ). For example, the SENNA scores range from 9.35% F1 (ACE UN) to 71.48% (CoNLL Test A). Lowest results are consistently observed for the ACE subcorpora, UMBC, and OntoNotes BC and WB. The ACE corpora are large and so may be more prone to non-uniformities emerging during downsampling; they also have special rules for some kinds of organisation which can skew results",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many layers does their system have?",
    "llm_answer": "The provided texts describe various architectures with differing numbers of layers.  A precise total layer count is not given for any single, unified system.\n",
    "context": [
      {
        "id": "48d54f1a-e495-407b-8e82-233c5484c02b",
        "metadata": {
          "vector_store_key": "1911.00069-3",
          "chunk_id": 20,
          "document_id": "1911.00069",
          "start_idx": 11208,
          "end_idx": 11936
        },
        "page_content": "The first layer is the embedding layer which maps input words in a sentence to word embeddings. The second layer is a context layer which transforms the word embeddings to context-aware vector representations using a recurrent or convolutional neural network layer. The third layer is a summarization layer which summarizes the vectors in a sentence by grouping and pooling. The final layer is the output layer which returns the classification label for the relation type. For an English sentence with $n$ words $\\mathbf {s}=(w_1,w_2,...,w_n)$, the embedding layer maps each word $w_t$ to a real-valued vector (word embedding) $\\mathbf {x}_t\\in \\mathbb {R}^{d \\times 1}$ using the English word embedding model (Section SECREF9).",
        "type": "Document"
      },
      {
        "id": "b90b5667-0046-4454-a0ea-8d4393be1165",
        "metadata": {
          "vector_store_key": "1910.02789-2",
          "chunk_id": 54,
          "document_id": "1910.02789",
          "start_idx": 29840,
          "end_idx": 30474
        },
        "page_content": "Following the convolution layer there is a ReLU activation and a max pool layer. Finally, there are two fully connected layers; The first layer has 32 units, and second one has 16 units. Both of them are followed by ReLU activation. All architectures have the same output, regardless of the input type. The DQN network is a regression network, with its output size the number of available actions. The PPO agent has 2 networks; actor and critic. The actor network has a Softmax activation with size equal to the available amount of actions. The critic network is a regression model with a single output representing the state's value.",
        "type": "Document"
      },
      {
        "id": "8b2489d4-7cfc-499f-ab64-0e60a9581038",
        "metadata": {
          "vector_store_key": "1810.00663-1",
          "chunk_id": 21,
          "document_id": "1810.00663",
          "start_idx": 12269,
          "end_idx": 13064
        },
        "page_content": "The model consists of six layers: Embed layer: The model first encodes each word and symbol in the input sequences INLINEFORM0 and INLINEFORM1 into fixed-length representations. The instructions INLINEFORM2 are embedded into a 100-dimensional pre-trained GloVe vector BIBREF24 . Each of the triplet components, INLINEFORM3 , INLINEFORM4 , and INLINEFORM5 of the graph INLINEFORM6 , are one-hot encoded into vectors of dimensionality INLINEFORM7 , where INLINEFORM8 and INLINEFORM9 are the number of nodes and edges in INLINEFORM10 , respectively. Encoder layer: The model then uses two bidirectional Gated Recurrent Units (GRUs) BIBREF25 to independently process the information from INLINEFORM0 and INLINEFORM1 , and incorporate contextual cues from the surrounding embeddings in each sequence.",
        "type": "Document"
      },
      {
        "id": "3cf1728c-0711-4400-a389-d0f3ab12da96",
        "metadata": {
          "vector_store_key": "1909.08859-0",
          "chunk_id": 8,
          "document_id": "1909.08859",
          "start_idx": 4401,
          "end_idx": 5098
        },
        "page_content": "Its architecture is based on a bi-directional attention flow (BiDAF) model BIBREF6, but also equipped with an explicit reasoning module that acts on entity-specific relational memory units. Fig. FIGREF4 shows an overview of the network architecture. It consists of five main modules: An input module, an attention module, a reasoning module, a modeling module, and an output module. Note that the question answering tasks we consider here are multimodal in that while the context is a procedural text, the question and the multiple choice answers are composed of images. Input Module extracts vector representations of inputs at different levels of granularity by using several different encoders.",
        "type": "Document"
      },
      {
        "id": "14cb4a16-54f0-4b61-b8d6-9aa52a7ba8ab",
        "metadata": {
          "vector_store_key": "1910.02789-2",
          "chunk_id": 51,
          "document_id": "1910.02789",
          "start_idx": 28358,
          "end_idx": 29007
        },
        "page_content": "The semantic segmentation image was of resolution 640X480X1, where the pixel value represents the object's class, generated using the VizDoom label API. the network consisted of two convolutional layers, two hidden linear layers and an output layer. The first convolutional layer has 8 6X6 filters with stride 3 and ReLU activation. The second convolutional layer has 16 3X3 filters with stride 2 and ReLU activation. The fully connected layers has 32 and 16 units, both of them are followed by ReLU activation. The output layer's size is the amount of actions the agent has available in the trained scenario. Used in the feature vector based agent.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What context modelling methods are evaluated?",
    "llm_answer": "The provided text mentions context modeling methods, but does not explicitly list them.  It does describe some, such as \"concatenation method,\" but doesn't enumerate all evaluated methods.\n",
    "context": [
      {
        "id": "54caa678-6186-45e8-814a-36c5d0179c0e",
        "metadata": {
          "vector_store_key": "2002.00652-2",
          "chunk_id": 1,
          "document_id": "2002.00652",
          "start_idx": 737,
          "end_idx": 1480
        },
        "page_content": "In general, there are two sorts of contextual phenomena in dialogues: Coreference and Ellipsis BIBREF1. Figure FIGREF1 shows a dialogue from the dataset SParC BIBREF2. After the question \u201cWhat is id of the car with the max horsepower?\u201d, the user poses an elliptical question \u201cHow about with the max mpg?\u201d, and a question containing pronouns \u201cShow its Make!\u201d. Only when completely understanding the context, could a parser successfully parse the incomplete questions into their corresponding SQL queries. A number of context modeling methods have been suggested in the literature to address SPC BIBREF3, BIBREF4, BIBREF2, BIBREF5, BIBREF6. These methods proposed to leverage two categories of context: recent questions and precedent logic form.",
        "type": "Document"
      },
      {
        "id": "616b928e-485e-4f1a-be43-7cc22f9cc48c",
        "metadata": {
          "vector_store_key": "2002.00652-1",
          "chunk_id": 39,
          "document_id": "2002.00652",
          "start_idx": 23980,
          "end_idx": 24850
        },
        "page_content": "Under the same task, BIBREF1 summarized contextual phenomena in a coarse-grained level, while BIBREF0 performed a wizard-of-oz experiment to study the most frequent phenomena. What makes our work different from them is that we not only summarize contextual phenomena by fine-grained types, but also perform an analysis on context modeling methods. This work conducts an exploratory study on semantic parsing in context, to realize how far we are from effective context modeling. Through a thorough comparison, we find that existing context modeling methods are not as effective as expected. A simple concatenation method can be much competitive. Furthermore, by performing a fine-grained analysis, we summarize two potential directions as our future work: incorporating common sense for better pronouns inference, and modeling contextual clues in a more explicit manner.",
        "type": "Document"
      },
      {
        "id": "7bfbaec5-2eff-4c7a-95ad-b6323bbbc8bc",
        "metadata": {
          "vector_store_key": "2002.00652-1",
          "chunk_id": 4,
          "document_id": "2002.00652",
          "start_idx": 2374,
          "end_idx": 3065
        },
        "page_content": "Second, none of previous works verified their proposed context modeling methods with the grammar-based decoding technique, which has been developed for years and proven to be highly effective in semantic parsing BIBREF7, BIBREF8, BIBREF9. To obtain better performance, it is worthwhile to study how context modeling methods collaborate with the grammar-based decoding. Last but not the least, there is limited understanding of how context modeling methods perform on various contextual phenomena. An in-depth analysis can shed light on potential research directions. In this paper, we try to fulfill the above insufficiency via an exploratory study on real-world semantic parsing in context.",
        "type": "Document"
      },
      {
        "id": "7b6aa69f-efe5-44a9-b5d9-0490ed3452fa",
        "metadata": {
          "vector_store_key": "2002.00652-1",
          "chunk_id": 3,
          "document_id": "2002.00652",
          "start_idx": 1950,
          "end_idx": 2715
        },
        "page_content": "With such a context, the decoder can attend over it, or reuse it via a copy mechanism BIBREF4, BIBREF5. Intuitively, methods that fall into this category enjoy better generalizability, as they only rely on the last logic form as context, no matter at which turn. Notably, these two categories of context can be used simultaneously. However, it remains unclear how far we are from effective context modeling. First, there is a lack of thorough comparisons of typical context modeling methods on complex SPC (e.g. cross-domain). Second, none of previous works verified their proposed context modeling methods with the grammar-based decoding technique, which has been developed for years and proven to be highly effective in semantic parsing BIBREF7, BIBREF8, BIBREF9.",
        "type": "Document"
      },
      {
        "id": "76cb8c62-f42c-4ce3-b259-a9b70ed392bf",
        "metadata": {
          "vector_store_key": "2002.00652-1",
          "chunk_id": 38,
          "document_id": "2002.00652",
          "start_idx": 23141,
          "end_idx": 23980
        },
        "page_content": "With respect to other logic forms, BIBREF25 focuses on understanding execution commands in context, BIBREF26 on question answering over knowledge base in a conversation, and BIBREF27 on code generation in environment context. Our work is different from theirs as we perform an exploratory study, not fulfilled by previous works. There are also several related works that provided studies on context. BIBREF17 explored the contextual representations in context-independent semantic parsing, and BIBREF28 studied how conversational agents use conversation history to generate response. Different from them, our task focuses on context modeling for semantic parsing. Under the same task, BIBREF1 summarized contextual phenomena in a coarse-grained level, while BIBREF0 performed a wizard-of-oz experiment to study the most frequent phenomena.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is the baseline a non-heirarchical model like BERT?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "1fb8dfaa-e118-4b2a-8bf6-3e36fff8cf0c",
        "metadata": {
          "vector_store_key": "2002.06644-3",
          "chunk_id": 4,
          "document_id": "2002.06644",
          "start_idx": 2479,
          "end_idx": 3239
        },
        "page_content": "We explore various BERT-based models, including BERT, RoBERTa, ALBERT, with their base and large specifications along with their native classifiers. We propose an ensemble model exploiting predictions from these models using multiple ensembling techniques. We show that our model outperforms the baselines by a margin of $5.6$ of F1 score and $5.95\\%$ of Accuracy. In this section, we outline baseline models like $BERT_{large}$. We further propose three approaches: optimized BERT-based models, distilled pretrained models, and the use of ensemble methods for the task of subjectivity detection. FastTextBIBREF4: It uses bag of words and bag of n-grams as features for text classification, capturing partial information about the local word order efficiently.",
        "type": "Document"
      },
      {
        "id": "814822da-592c-40ae-bb7c-e78761735daf",
        "metadata": {
          "vector_store_key": "2002.06644-3",
          "chunk_id": 10,
          "document_id": "2002.06644",
          "start_idx": 6316,
          "end_idx": 7055
        },
        "page_content": "Our proposed methodology, the use of finetuned optimized BERT based models, and BERT-based ensemble models outperform the baselines for all the metrics. Among the optimized BERT based models, $RoBERTa_{large}$ outperforms all other non-ensemble models and the baselines for all metrics. It further achieves a maximum recall of $0.681$ for all the proposed models. We note that DistillRoBERTa, a distilled model, performs competitively, achieving $69.69\\%$ accuracy, and $0.672$ F1 score. This observation shows that distilled pretrained models can replace their undistilled counterparts in a low-computing environment. We further observe that ensemble models perform better than optimized BERT-based models and distilled pretrained models.",
        "type": "Document"
      },
      {
        "id": "98b4d303-8583-426c-99b9-5ab269a46ebb",
        "metadata": {
          "vector_store_key": "1906.06448-2",
          "chunk_id": 40,
          "document_id": "1906.06448",
          "start_idx": 23311,
          "end_idx": 24137
        },
        "page_content": "This indicates that BERT might fail to capture the monotonicity property that conditionals create a downward entailing context in their scope while they create an upward entailing context out of their scope. Regarding lexical knowledge, the data augmentation technique improved the performance much better on downward inferences which do not require lexical knowledge. However, among the 394 problems for which all models provided wrong answers, 244 problems are non-lexical inference problems. This indicates that some non-lexical inference problems are more difficult than lexical inference problems, though accuracy on non-lexical inference problems was better than that on lexical inference problems. One of our findings is that there is a type of downward inferences to which every model fails to provide correct answers.",
        "type": "Document"
      },
      {
        "id": "4284dd2a-a0cd-4e1f-9bf9-605c0e9a1742",
        "metadata": {
          "vector_store_key": "1910.07181-1",
          "chunk_id": 43,
          "document_id": "1910.07181",
          "start_idx": 22968,
          "end_idx": 23672
        },
        "page_content": "Noticeably, despite being both based on and integrated into a BERT$_\\text{base}$ model, our architecture even outperforms a standalone BERT$_\\text{large}$ model by a large margin. To measure the effect of adding Bertram to BERT on downstream tasks, we apply the procedure described in Section SECREF4 to a commonly used textual entailment dataset as well as two text classification datasets: MNLI BIBREF21, AG's News BIBREF22 and DBPedia BIBREF23. For all three datasets, we use BERT$_\\text{base}$ as a baseline model and create the substitution dictionary $S$ using the synonym relation of WordNet BIBREF20 and the pattern library BIBREF34 to make sure that all synonyms have consistent parts of speech.",
        "type": "Document"
      },
      {
        "id": "630c939e-9886-4075-be6e-cbfdb7e5ef56",
        "metadata": {
          "vector_store_key": "1906.06448-2",
          "chunk_id": 35,
          "document_id": "1906.06448",
          "start_idx": 20236,
          "end_idx": 20924
        },
        "page_content": "These two figures show that, as the size of the upward training set increased, BERT performed better on upward inferences but worse on downward inferences, and vice versa. Figure 2 (iii) shows performance on a different ratio of upward and downward inference training sets. When downward inference examples constitute more than half of the training set, accuracies on upward and downward inferences were reversed. As the ratio of downward inferences increased, BERT performed much worse on upward inferences. This indicates that a training set in one direction (upward or downward entailing) of monotonicity might be harmful to models when learning the opposite direction of monotonicity.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How better are results compared to baseline models?",
    "llm_answer": "The model significantly outperforms baseline models in most cases,  especially when considering sufficient training data.  Results vary depending on the specific task and data set.\n",
    "context": [
      {
        "id": "874d1312-9fdb-46fe-865b-ce75dcb445d6",
        "metadata": {
          "vector_store_key": "1812.07023-3",
          "chunk_id": 32,
          "document_id": "1812.07023",
          "start_idx": 17666,
          "end_idx": 18428
        },
        "page_content": "We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below: Our primary architectural differences over the baseline model are: not concatenating the question, answer pairs before encoding them, the auxiliary decoder module, and using the Time-Extended FiLM module for feature extraction. These, combined with using scheduled sampling and running hyperparameter optimization over the validation set to select hyperparameters, give us the observed performance boost. We observe that our models generate fairly relevant responses to questions in the dialogues, and models with audio-visual inputs respond to audio-visual questions (e.g. \u201cis there any voices or music ?\u201d) correctly more often.",
        "type": "Document"
      },
      {
        "id": "c938d47c-e132-4294-bd51-25295fb776d2",
        "metadata": {
          "vector_store_key": "1603.07044-3",
          "chunk_id": 28,
          "document_id": "1603.07044",
          "start_idx": 15079,
          "end_idx": 15744
        },
        "page_content": "Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited training data is given, both feature-rich based system and our system are worse than the IR system. For task C, our system also got comparable results with the feature-rich based system. If we do a simple system combination (average the rank score) between our system and the IR system, the combined system will give large gains on tasks B and C.",
        "type": "Document"
      },
      {
        "id": "6824b4f2-136e-4c26-a2a7-a10cf7d4c97e",
        "metadata": {
          "vector_store_key": "1810.00663-8",
          "chunk_id": 46,
          "document_id": "1810.00663",
          "start_idx": 25788,
          "end_idx": 26550
        },
        "page_content": "The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as can be seen in performance drops in Table TABREF28 for the new environments. Nonetheless, the insights from the previous section still hold: masking in the output layer and reordering the graph triplets tend to increase performance. Even though the results in Table TABREF28 suggest that there is room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM.",
        "type": "Document"
      },
      {
        "id": "36dcf150-24e8-459c-a8c0-aa8fe1c0018f",
        "metadata": {
          "vector_store_key": "1909.02764-4",
          "chunk_id": 36,
          "document_id": "1909.02764",
          "start_idx": 19976,
          "end_idx": 20568
        },
        "page_content": "We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with 40 % of the data from the same domain (results shown in the column \u201cIn-Domain\u201d in Table TABREF19). Excluding AMMER, we achieve an average micro $\\text{F}_1$ of 68 %, with best results of F$_1$=73 % on TEC. The model trained on our AMMER corpus achieves an F1 score of 57%. This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set.",
        "type": "Document"
      },
      {
        "id": "8b681439-d4eb-401b-8199-4cc0d2c2d0ed",
        "metadata": {
          "vector_store_key": "1909.02764-4",
          "chunk_id": 37,
          "document_id": "1909.02764",
          "start_idx": 20568,
          "end_idx": 21193
        },
        "page_content": "This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set. These results are mostly in line with Bostan2018. Now we analyze how well the models trained in Experiment 1 perform when applied to our data set. The results are shown in column \u201cSimple\u201d in Table TABREF19. We observe a clear drop in performance, with an average of F$_1$=48 %. The best performing model is again the one trained on TEC, en par with the one trained on the Figure8 data. The model trained on ISEAR performs second best in Experiment 1, it performs worst in Experiment 2.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is dataset used for training/testing?",
    "llm_answer": "The answer is not found in the context.  While various datasets and their splits are described, there's no single, overall answer to the size of the training/testing dataset.\n",
    "context": [
      {
        "id": "9ca5340d-587b-4e94-bcbe-160570788673",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 35,
          "document_id": "1910.12618",
          "start_idx": 19294,
          "end_idx": 19871
        },
        "page_content": "For instance for random forests it can correspond to the maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014 and 2015 serve as validation set (730 days).",
        "type": "Document"
      },
      {
        "id": "36adf7ba-6ece-4ad5-84b3-0a38b137a4fc",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 16,
          "document_id": "1912.13109",
          "start_idx": 8070,
          "end_idx": 8831
        },
        "page_content": "The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we employed various operational techniques for optimizing on the learning process. These processes and its implementation details will be discussed later but they were learning rate decay, early stopping, model checkpointing and reducing learning rate on plateau.",
        "type": "Document"
      },
      {
        "id": "8c714d0f-b982-4837-aefc-3b59003e3a67",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 15,
          "document_id": "1912.13109",
          "start_idx": 7410,
          "end_idx": 8070
        },
        "page_content": "Train-test split: The labelled dataset that was available for this task was very limited in number of examples and thus as noted above few data augmentation techniques were applied to boost the learning of the network. Before applying augmentation, a train-test split of 78%-22% was done from the original, cleansed data set. Thus, 700 tweets/messages were held out for testing. All model evaluation were done in on the test set that got generated by this process. The results presented in this report are based on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data.",
        "type": "Document"
      },
      {
        "id": "6b8da48a-62bf-4779-a0c8-0c543a0bf17a",
        "metadata": {
          "vector_store_key": "1907.00758-2",
          "chunk_id": 23,
          "document_id": "1907.00758",
          "start_idx": 13439,
          "end_idx": 14036
        },
        "page_content": "We obtain 243,764 samples for UXTD (13.5hrs), 333,526 for UXSSD (18.5hrs), and 572,078 for UPX (31.8 hrs), or a total 1,149,368 samples (63.9hrs) which we divide into training, validation and test sets. We aim to test whether our model generalises to data from new speakers, and to data from new sessions recorded with known speakers. To simulate this, we select a group of speakers from each dataset, and hold out all of their data either for validation or for testing. Additionally, we hold out one entire session from each of the remaining speakers, and use the rest of their data for training.",
        "type": "Document"
      },
      {
        "id": "f3776613-5f75-4322-8d4e-69a0ede7f7df",
        "metadata": {
          "vector_store_key": "1811.00942-0",
          "chunk_id": 16,
          "document_id": "1811.00942",
          "start_idx": 9219,
          "end_idx": 9822
        },
        "page_content": "For the QRNNs, we used the first 350 words of the test set, and averaged the ms/query and mJ/query. For KN-5, we used the entire test set for evaluation, since the latency was much lower. To adjust for the base power load, we subtracted idle power draw from energy usage. For a different perspective, we further evaluated all the models under a desktop environment, using an i7-4790k CPU and Titan V GPU. Because the base power load for powering a desktop is much higher than running neural language models, we collected only latency statistics. We used the entire test set, since the QRNN runs quickly.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What geometric properties do embeddings display?",
    "llm_answer": "The embeddings display geometric properties pertaining to the behavior of the time series, as observed in the t-SNE projection.  Thematic clusters, such as winter/summer and weekdays/weekends, are evident.\n",
    "context": [
      {
        "id": "94501b38-dcae-4424-83c3-7d21d6603263",
        "metadata": {
          "vector_store_key": "1910.12618-6",
          "chunk_id": 26,
          "document_id": "1910.12618",
          "start_idx": 14341,
          "end_idx": 15087
        },
        "page_content": "However in our case we want a vector encoding which is tailored for the technical vocabulary of our weather reports and for the subsequent prediction task. This is why we decided to train our own word embedding from scratch during the learning phase of our recurrent or convolutional neural network. Aside from the much more restricted size of our corpora, the major difference with the aforementioned embeddings is that in our case it is obtained by minimizing a squared loss on the prediction. In that framework there is no explicit reason for our representation to display any geometric structure. However as detailed in section SECREF36, our word vectors nonetheless display geometric properties pertaining to the behavior of the time series.",
        "type": "Document"
      },
      {
        "id": "78b98202-ca30-4cb9-ae89-4cecc3322007",
        "metadata": {
          "vector_store_key": "1910.12618-8",
          "chunk_id": 66,
          "document_id": "1910.12618",
          "start_idx": 35507,
          "end_idx": 36223
        },
        "page_content": "In order to achieve a global view of the embeddings, the t-SNE algorithm BIBREF30 is applied to project an embedding matrix into a 2 dimensional space, for both languages. The observations for the few aforementioned words are confirmed by this representation, as plotted in figure FIGREF44. Thematic clusters can be observed, roughly corresponding to winter, summer, week-days, week-end days for both languages. Globally summer and winter seem opposed, although one should keep in mind that the t-SNE representation does not preserve the cosine distance. The clusters of the French embedding appear much more compact than the UK one, comforting the observations made when explicitly calculating the cosine distances.",
        "type": "Document"
      },
      {
        "id": "b0174348-59ae-44c0-8b01-9a217e4013dc",
        "metadata": {
          "vector_store_key": "1909.08859-0",
          "chunk_id": 15,
          "document_id": "1909.08859",
          "start_idx": 8252,
          "end_idx": 8848
        },
        "page_content": "Then these embeddings are passed first to a multilayer perceptron (MLP) and then its outputs are fed to a BiLSTM. We then form a matrix $\\mathbf {Q}^{\\prime } \\in \\mathbb {R}^{2d \\times M}$ for the question by concatenating the cell states of the BiLSTM. For the visual ordering task, to represent the sequence of images in the answer with a single vector, we additionally use a BiLSTM and define the answering embedding by the summation of the cell states of the BiLSTM. Finally, for all tasks, these computations produce answer embeddings denoted by $\\mathbf {a} \\in \\mathbb {R}^{2d \\times 1}$.",
        "type": "Document"
      },
      {
        "id": "fb522362-a249-46d7-aa73-a2d62713173c",
        "metadata": {
          "vector_store_key": "1910.12618-8",
          "chunk_id": 60,
          "document_id": "1910.12618",
          "start_idx": 32200,
          "end_idx": 32942
        },
        "page_content": "We investigated the distances between word vectors, the relevant metric being the cosine distance given by: where $\\overrightarrow{w_1}$ and $\\overrightarrow{w_2}$ are given word vectors. Thus a cosine distance lower than 1 means similarity between word vectors, whereas a greater than 1 corresponds to opposition. The initial analyses of the embedding matrices for both the UK and France revealed that in general, words were grouped by context or influence on the electricity consumption. For instance, we observed that winter words were together and far away from summer ones. Week days were grouped as well and far from week-end days. However considering the vocabulary was reduced to $V^* = 52$ words, those results lacked of consistency.",
        "type": "Document"
      },
      {
        "id": "b5f52c89-6f3a-4157-8419-c95eb9f70981",
        "metadata": {
          "vector_store_key": "1812.06705-1",
          "chunk_id": 17,
          "document_id": "1812.06705",
          "start_idx": 10386,
          "end_idx": 11108
        },
        "page_content": "The differences are the input representation and training procedure. The input embeddings of BERT are the sum of the token embeddings, the segmentation embeddings and the position embeddings. For the segmentation embeddings in BERT, a learned sentence A embedding is added to every token of the first sentence, and if a second sentence exists, a sentence B embedding will be added to every token of the second sentence. However, the segmentation embeddings has no connection to the actual annotated labels of a sentence, like sense, sentiment or subjectivity, so predicted word is not always compatible with annotated labels. For example, given a positive movie remark \u201cthis actor is good\", we have the word \u201cgood\" masked.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How accurate is model trained on text exclusively?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "52dcadea-7c20-41ad-a4b0-dfe21ec0265e",
        "metadata": {
          "vector_store_key": "1610.00879-0",
          "chunk_id": 17,
          "document_id": "1610.00879",
          "start_idx": 9713,
          "end_idx": 10145
        },
        "page_content": "We observe that our stylistic features add negligible value to N-gram features. We use our heldout dataset to compare how our system performs against human annotators. While human annotators achieve an accuracy of 68.8%, our system reaches reasonably close and performs with a best accuracy of 64%. Our analysis of the task and experimental findings make a case for drunk-texting prediction as a useful and feasible NLP application.",
        "type": "Document"
      },
      {
        "id": "73cfd577-8b68-42d0-b667-25c8485c0ca3",
        "metadata": {
          "vector_store_key": "2001.00137-1",
          "chunk_id": 0,
          "document_id": "2001.00137",
          "start_idx": 0,
          "end_idx": 799
        },
        "page_content": "Understanding a user's intent and sentiment is of utmost importance for current intelligent chatbots to respond appropriately to human requests. However, current systems are not able to perform to their best capacity when presented with incomplete data, meaning sentences with missing or incorrect words. This scenario is likely to happen when one considers human error done in writing. In fact, it is rather naive to assume that users will always type fully grammatically correct sentences. Panko BIBREF0 goes as far as claiming that human accuracy regarding research paper writing is none when considering the entire document. This has been aggravated with the advent of internet and social networks, which allowed language and modern communication to be been rapidly transformed BIBREF1, BIBREF2.",
        "type": "Document"
      },
      {
        "id": "98f48987-4d6e-417f-83db-8fbd2608832d",
        "metadata": {
          "vector_store_key": "2001.00137-3",
          "chunk_id": 41,
          "document_id": "2001.00137",
          "start_idx": 24041,
          "end_idx": 24728
        },
        "page_content": "Further evaluation on the F1-scores decay in the presence of noise demonstrated that our model is more robust than the baseline models when considering noisy data, be that in the form of incorrect sentences or sentences with STT error. Not only that, experiments on the Twitter dataset also showed improved accuracy in clean data, with complete sentences. We infer that this is due to our model being able to extract richer data representations from the input data regardless of the completeness of the sentence. For future works, we plan on evaluating the robustness of our model against other types of noise, such as word reordering, word insertion, and spelling mistakes in sentences.",
        "type": "Document"
      },
      {
        "id": "dbdf269c-fe97-4416-b6be-91113bd07f3a",
        "metadata": {
          "vector_store_key": "1907.00758-4",
          "chunk_id": 31,
          "document_id": "1907.00758",
          "start_idx": 17373,
          "end_idx": 18401
        },
        "page_content": "(Section SECREF4 ). Analysis: We analyse the performance of our model across different conditions. Table shows the model accuracy broken down by utterance type. The model achieves 91.2% accuracy on utterances containing words, sentences, and conversations, all of which exhibit natural variation in speech. The model is less successful with Articulatory utterances, which contain isolated phones occurring once or repeated (e.g., \u201csh sh sh\"). Such utterances contain subtle tongue movement, making it more challenging to correlate the visual signal with the audio. And indeed, the model finds the correct offset for only 55.9% of Articulatory utterances. A further analysis shows that 84.4% (N INLINEFORM0 90) of stop consonants (e.g., \u201ct\u201d), which are relied upon by therapists as the most salient audiovisual synchronisation cues BIBREF3 , are correctly synchronised by our model, compared to 48.6% (N INLINEFORM1 140) of vowels, which contain less distinct movement and are also more challenging for therapists to synchronise.",
        "type": "Document"
      },
      {
        "id": "e0c5c1ea-3566-4bc4-9d2e-d9432320995c",
        "metadata": {
          "vector_store_key": "2001.00137-1",
          "chunk_id": 3,
          "document_id": "2001.00137",
          "start_idx": 2001,
          "end_idx": 2847
        },
        "page_content": "Tasks such as text generation, sentence correction, image captioning and text classification, have been possible via models such as Convolutional Neural Networks and Recurrent Neural Networks BIBREF5, BIBREF6, BIBREF7. More recently, state-of-the-art results have been achieved with attention models, more specifically Transformers BIBREF8. Surprisingly, however, there is currently no research on incomplete text classification in the NLP community. Realizing the need of research in that area, we make it the focus of this paper. In this novel task, the model aims to identify the user's intent or sentiment by analyzing a sentence with missing and/or incorrect words. In the sentiment classification task, the model aims to identify the user's sentiment given a tweet, written in informal language and without regards for sentence correctness.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was their result on Stance Sentiment Emotion Corpus?",
    "llm_answer": "The provided text evaluates the approach on the Stance Sentiment Emotion Corpus (SSEC), but does not state any specific result.\n",
    "context": [
      {
        "id": "cae17895-975f-478b-882f-7ae1ff46d6bf",
        "metadata": {
          "vector_store_key": "1911.12569-2",
          "chunk_id": 23,
          "document_id": "1911.12569",
          "start_idx": 14103,
          "end_idx": 14751
        },
        "page_content": "In this section we present the details of the datasets used for the experiments, results that we obtain and the necessary analysis. We evaluate our proposed approach for joint sentiment and emotion analysis on the benchmark dataset of SemEval 2016 Task 6 BIBREF7 and Stance Sentiment Emotion Corpus (SSEC) BIBREF15. The SSEC corpus is an annotation of the SemEval 2016 Task 6 corpus with emotion labels. The re-annotation of the SemEval 2016 Task 6 corpus helps to bridge the gap between the unavailability of a corpus with sentiment and emotion labels. The SemEval 2016 corpus contains tweets which are classified into positive, negative or other.",
        "type": "Document"
      },
      {
        "id": "89ad953b-29df-4d57-8717-f96f683aca90",
        "metadata": {
          "vector_store_key": "1710.01492-6",
          "chunk_id": 26,
          "document_id": "1710.01492",
          "start_idx": 13646,
          "end_idx": 14235
        },
        "page_content": "A task related to, but arguably different in some respect from sentiment analysis, is that of stance detection. The goal here is to determine whether the author of a piece of text is in favor of, against, or neutral toward a proposition or a target BIBREF36 . For example, in (8) the author has a negative stance toward the proposition w\u200bomen have the right to abortion, even though the target is not mentioned at all. Similarly, in (9\u00a7) the author expresses a negative sentiment toward Mitt Romney, from which one can imply that s/he has a positive stance toward the target \u200bBarack Obama.",
        "type": "Document"
      },
      {
        "id": "101ba0cf-e076-42c4-b018-e88a65a64b1f",
        "metadata": {
          "vector_store_key": "1710.01492-1",
          "chunk_id": 10,
          "document_id": "1710.01492",
          "start_idx": 5102,
          "end_idx": 5998
        },
        "page_content": "Initially, it was regarded as standard document classification into topics such as business, sport, and politics BIBREF10 . However, researchers soon realized that it was quite different from standard document classification BIBREF11 , and that it crucially needed external knowledge in the form of sentiment polarity lexicons. Around the same time, other researchers realized the importance of external sentiment lexicons, e.g., Turney BIBREF12 proposed an unsupervised approach to learn the sentiment orientation of words/phrases: positive vs. negative. Later work studied the linguistic aspects of expressing opinions, evaluations, and speculations BIBREF13 , the role of context in determining the sentiment orientation BIBREF14 , of deeper linguistic processing such as negation handling BIBREF15 , of finer-grained sentiment distinctions BIBREF16 , of positional information BIBREF17 , etc.",
        "type": "Document"
      },
      {
        "id": "bc5c4b3c-895e-4956-aec4-c73a1597b5d5",
        "metadata": {
          "vector_store_key": "1909.00694-0",
          "chunk_id": 6,
          "document_id": "1909.00694",
          "start_idx": 3064,
          "end_idx": 3855
        },
        "page_content": "BIBREF0 constructed a network of events using word embedding-derived similarities. Compared with this method, our discourse relation-based linking of events is much simpler and more intuitive. Some previous studies made use of document structure to understand the sentiment. BIBREF11 proposed a sentiment-specific pre-training strategy using unlabeled dialog data (tweet-reply pairs). BIBREF12 proposed a method of building a polarity-tagged corpus (ACP Corpus). They automatically gathered sentences that had positive or negative opinions utilizing HTML layout structures in addition to linguistic patterns. Our method depends only on raw texts and thus has wider applicability. Our goal is to learn the polarity function $p(x)$, which predicts the sentiment polarity score of an event $x$.",
        "type": "Document"
      },
      {
        "id": "1d3888fd-81c9-4864-8911-156f0a980f68",
        "metadata": {
          "vector_store_key": "1911.12569-1",
          "chunk_id": 37,
          "document_id": "1911.12569",
          "start_idx": 22328,
          "end_idx": 23049
        },
        "page_content": "This helped M2 to correctly identify the positive sentiment of the example. The presence of emotional information helped the system to alter its sentiment decision (negative by S2) as it had better understanding of the text. A sentiment directly does not invoke a particular emotion always and a sentiment can be associated with more than one emotion. However, emotions like joy and trust are associated with positive sentiment mostly whereas, anger, disgust and sadness are associated with negative sentiment particularly. This might be the reason of the extra sentiment information not helping the multi-task system for emotion analysis and hence, a decreased performance for emotion analysis in the multi-task setting.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What performance did they obtain on the SemEval dataset?",
    "llm_answer": "Our system improved the existing best system of SemEval 2016 task 6 by 3.2 F-score points for sentiment analysis.\n",
    "context": [
      {
        "id": "8745fb4b-9f54-4544-8930-edfeb7bd93b3",
        "metadata": {
          "vector_store_key": "1911.12569-2",
          "chunk_id": 32,
          "document_id": "1911.12569",
          "start_idx": 19400,
          "end_idx": 20085
        },
        "page_content": "Our system comfortably surpasses the existing best system at SemEval. Our system manages to improve the existing best system of SemEval 2016 task 6 by 3.2 F-score points for sentiment analysis. We also compare our system with the state-of-the-art systems proposed by BIBREF15 on the emotion dataset. The comparison is demonstrated in Table TABREF22. Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN were the five individual systems used by BIBREF15. Overall, our proposed system achieves an improvement of 5 F-Score points over the existing state-of-the-art system for emotion analysis. Individually, the proposed system improves the existing F-scores for all the emotions except surprise.",
        "type": "Document"
      },
      {
        "id": "6e9156a1-1bd2-4904-9fe1-0957c0410a9d",
        "metadata": {
          "vector_store_key": "1909.13375-3",
          "chunk_id": 43,
          "document_id": "1909.13375",
          "start_idx": 22785,
          "end_idx": 23407
        },
        "page_content": "Not using the simple preprocessing from Section SECREF17 resulted in a 2.5 point decrease in both EM and F1. The numeric questions were the most affected, with their performance dropping by 3.5 points. Given that number questions make up about 61% of the dataset, we can deduce that our improved number handling is responsible for about a 2.1 point gain, while the rest could be be attributed to the improved Wikipedia parsing. Although NER span cleaning (Section SECREF23) affected only 3% of the multi-span questions, it provided a solid improvement of 5.4 EM in multi-span questions and 1.5 EM in single-span questions.",
        "type": "Document"
      },
      {
        "id": "a2564a73-35d9-49f8-82a7-db0623fe55ab",
        "metadata": {
          "vector_store_key": "1710.01492-5",
          "chunk_id": 8,
          "document_id": "1710.01492",
          "start_idx": 4000,
          "end_idx": 4867
        },
        "page_content": "All these are explored at SemEval, the International Workshop on Semantic Evaluation, which has created a number of benchmark datasets and has enabled direct comparison between different systems and approaches, both as part of the competition and beyond. Traditionally, the task has been addressed using supervised and semi-supervised methods, as well as using distant supervision, with the most important resource being sentiment polarity lexicons, and with feature-rich approaches as the dominant research direction for years. With the recent rise of deep learning, which in many cases eliminates the need for any explicit feature modeling, the importance of both lexicons and features diminishes, while at the same time attention is shifting towards learning from large unlabeled data, which is needed to train the high number of parameters of such complex models.",
        "type": "Document"
      },
      {
        "id": "1b1f9986-e4da-44a7-a6cd-13931a730da6",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 38,
          "document_id": "1909.13375",
          "start_idx": 19979,
          "end_idx": 20678
        },
        "page_content": "For the large models we used a batch size of 3 with a learning rate of $5e^{-6}$ and trained for 5 epochs, except for the model without the single-span heads that was trained with a batch size of 2 for 7 epochs. F1 was used as our validation metric. All models were trained on a single GPU with 12-16GB of memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions.",
        "type": "Document"
      },
      {
        "id": "b8bb3cba-76ec-440b-a522-7f81806b053e",
        "metadata": {
          "vector_store_key": "1911.12569-2",
          "chunk_id": 31,
          "document_id": "1911.12569",
          "start_idx": 18916,
          "end_idx": 19651
        },
        "page_content": "The p-values for anger, anticipation, fear, disgust, joy, sadness, surprise and trust are 0.000002, 0.000143, 0.00403, 0.000015, 0.004607, 0.069, 0.000001 and 0.000001, respectively. These results provide a good indication of statistical significance. Table TABREF19 shows the comparison of our proposed system with the existing state-of-the-art system of SemEval 2016 Task 6 for the sentiment dataset. BIBREF7 used feature-based SVM, BIBREF39 used keyword rules, LitisMind relied on hashtag rules on external data, BIBREF38 utilized a combination of sentiment classifiers and rules, whereas BIBREF37 used a maximum entropy classifier with domain-specific features. Our system comfortably surpasses the existing best system at SemEval.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the state-of-the-art systems?",
    "llm_answer": "Deep convolutional networks.\n",
    "context": [
      {
        "id": "a6dff15d-6323-4ff2-9d1f-f503bd1a9ef5",
        "metadata": {
          "vector_store_key": "1710.01492-5",
          "chunk_id": 35,
          "document_id": "1710.01492",
          "start_idx": 19077,
          "end_idx": 19782
        },
        "page_content": "Traditionally, the above features were fed into classifiers such as Maximum Entropy (MaxEnt) and Support Vector Machines (SVM) with various kernels. However, observation over the SemEval Twitter sentiment task in recent years shows growing interest in, and by now clear dominance of methods based on deep learning. In particular, the best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks BIBREF53 , BIBREF54 . Conversely, kernel machines seem to be less frequently used than in the past, and the use of learning methods other than the ones mentioned above is at this point scarce. All these models are examples of supervised learning as they need labeled training data.",
        "type": "Document"
      },
      {
        "id": "3e561a91-d33e-4edf-bb45-28350d16064c",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 26,
          "document_id": "2001.00137",
          "start_idx": 15470,
          "end_idx": 16181
        },
        "page_content": "We focus on the three following services, where the first two are commercial services and last one is open source with two separate backends: Google Dialogflow (formerly Api.ai) , SAP Conversational AI (formerly Recast.ai) and Rasa (spacy and tensorflow backend) . Shridhar et al. BIBREF12 proposed a word embedding method that doesn't suffer from out-of-vocabulary issues. The authors achieve this by using hash tokens in the alphabet instead of a single word, making it vocabulary independent. For classification, classifiers such as Multilayer Perceptron (MLP), Support Vector Machine (SVM) and Random Forest are used. A complete list of classifiers and training specifications are given in Section SECREF31.",
        "type": "Document"
      },
      {
        "id": "df8a6fb3-d802-4d89-a594-11a767c5b554",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 29,
          "document_id": "2001.00137",
          "start_idx": 17443,
          "end_idx": 18171
        },
        "page_content": "Nearest Centroid with Euclidian metric, where classification is done by representing each class with a centroid; Bernoulli Naive Bayes with smoothing parameter $alpha=10^{-2}$; K-means clustering with 2 clusters and L2 penalty; and Logistic Regression classifier with L2 penalty, tolerance of $10^{-4}$ and regularization term of $1.0$. Most often, the best performing classifier was MLP. Conventional BERT is a BERT-base-uncased model, meaning that it has 12 transformer blocks $L$, hidden size $H$ of 768, and 12 self-attention heads $A$. The model is fine-tuned with our dataset on 2 Titan X GPUs for 3 epochs with Adam Optimizer, learning rate of $2*10^{-5}$, maximum sequence length of 128, and warm up proportion of $0.1$.",
        "type": "Document"
      },
      {
        "id": "c93f2257-9006-466a-af1f-9340e6b4ab7d",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 28,
          "document_id": "2001.00137",
          "start_idx": 16720,
          "end_idx": 17443
        },
        "page_content": "Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector Classifier with L1 and L2 penalty and tolerance of $10^{-3}$; Regularized linear classifier with Stochastic Gradient Descent (SGD) learning with regularization term $alpha=10^{-4}$ and L1, L2 and Elastic-Net penalty; Nearest Centroid with Euclidian metric, where classification is done by representing each class with a centroid;",
        "type": "Document"
      },
      {
        "id": "ad1d7753-3848-45ad-a7ec-2f9bff0e735c",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 18,
          "document_id": "1912.13109",
          "start_idx": 9307,
          "end_idx": 10125
        },
        "page_content": "While the model probability is the probability that the observation i belongs to category c. Among the model architectures we experimented with and without data augmentation were: Fully Connected dense networks: Model hyperparameters were inspired from the previous work done by Vo et al and Mathur et al. This was also used as a baseline model but we did not get appreciable performance on such architecture due to FC networks not being able to capture local and long term dependencies. Convolution based architectures: Architecture and hyperparameter choices were chosen from the past study Deon on the subject. We were able to boost the performance as compared to only FC based network but we noticed better performance from architectures that are suitable to sequences such as text messages or any timeseries data.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the size of their collected dataset?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "73a83f60-b2c9-406d-b4cd-5720639e5f06",
        "metadata": {
          "vector_store_key": "1812.07023-4",
          "chunk_id": 18,
          "document_id": "1812.07023",
          "start_idx": 10358,
          "end_idx": 10986
        },
        "page_content": "After 10 such qa turns, the questioner wraps up by writing a summary of the video based on the conversation. Dataset statistics such as the number of dialogues, turns, and words for the avsd dataset are presented in Table TABREF5 . For the initially released prototype dataset, the training set of the avsd dataset corresponds to videos taken from the training set of the Charades dataset while the validation and test sets of the avsd dataset correspond to videos taken from the validation set of the Charades dataset. For the official dataset, training, validation and test sets are drawn from the corresponding Charades sets.",
        "type": "Document"
      },
      {
        "id": "33a7bab6-0ec4-430d-b240-0fb307f97d0c",
        "metadata": {
          "vector_store_key": "1910.12618-0",
          "chunk_id": 16,
          "document_id": "1910.12618",
          "start_idx": 8863,
          "end_idx": 9481
        },
        "page_content": "The national average is obtained by combining the data from all stations with a weight proportional to the city population the station is located in. For France the stations' data is provided by the French meteorological office, M\u00e9t\u00e9o France, while the British ones are scrapped from stations of the National Oceanic and Atmospheric Administration (NOAA). Available on the same time span as the consumption, they usually have a 3 hours temporal resolution but are averaged to a daily one as well. Finally the time series were scaled to the range $[0,1]$ before the training phase, and re-scaled during prediction time.",
        "type": "Document"
      },
      {
        "id": "088347bc-6391-466f-81eb-e37d14e108ba",
        "metadata": {
          "vector_store_key": "1705.09665-6",
          "chunk_id": 23,
          "document_id": "1705.09665",
          "start_idx": 13165,
          "end_idx": 13951
        },
        "page_content": "Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities ( INLINEFORM0 ), for a total of 4,872 community-months ( INLINEFORM1 ). Estimating linguistic measures. We estimate word frequencies INLINEFORM0 , and by extension each downstream measure, in a carefully controlled manner in order to ensure we capture robust and meaningful linguistic behaviour.",
        "type": "Document"
      },
      {
        "id": "da74a9a9-0751-4d68-9ca9-313bf450f848",
        "metadata": {
          "vector_store_key": "1907.05664-3",
          "chunk_id": 7,
          "document_id": "1907.05664",
          "start_idx": 3615,
          "end_idx": 4242
        },
        "page_content": "See2017 to then apply LRP on it. The CNN/Daily mail dataset BIBREF12 is a text summarization dataset adapted from the Deepmind question-answering dataset BIBREF13 . It contains around three hundred thousand news articles coupled with summaries of about three sentences. These summaries are in fact \u201chighlights\" of the articles provided by the media themselves. Articles have an average length of 780 words and the summaries of 50 words. We had 287 000 training pairs and 11 500 test pairs. Similarly to See et al. See2017, we limit during training and prediction the input text to 400 words and generate summaries of 200 words.",
        "type": "Document"
      },
      {
        "id": "a5dbbbf1-2283-487d-89f4-f5cbdfec92c4",
        "metadata": {
          "vector_store_key": "1905.06566-1",
          "chunk_id": 31,
          "document_id": "1905.06566",
          "start_idx": 17089,
          "end_idx": 17889
        },
        "page_content": "For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from New York Times dataset. We used the same training/validation/test splits as in xu:2019:arxiv, which contain 137,778 documents for training, 17,222 for validation and 17,223 for test. To create sentence level labels for extractive summarization, we used a strategy similar to nallapati:2017:aaai. We label the subset of sentences in a document that maximizes Rouge BIBREF39 (against the human summary) as True and all other sentences as False.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What kind of features are used by the HMM models, and how interpretable are those?",
    "llm_answer": "The HMM models identify spaces, indentation, and special characters (like comment symbols in Linux data).  The interpretability is described as being high enough for the model's features to be visualized and understood.\n",
    "context": [
      {
        "id": "2ca3ce25-2fe5-4a35-afc0-91bdb0106048",
        "metadata": {
          "vector_store_key": "1606.05320-0",
          "chunk_id": 15,
          "document_id": "1606.05320",
          "start_idx": 8007,
          "end_idx": 8725
        },
        "page_content": "We interpret the HMM and LSTM states in the hybrid algorithm with 10 LSTM state dimensions and 10 HMM states in Figures 3 and 3 , showing which features are identified by the HMM and LSTM components. In Figures 3 and 3 , we color-code the training data with the 10 HMM states. In Figures 3 and 3 , we apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters. The HMM and LSTM states pick up on spaces, indentation, and special characters in the data (such as comment symbols in Linux data). We see some examples where the HMM and LSTM complement each other, such as learning different things about spaces and comments on Linux data, or punctuation on the Shakespeare data.",
        "type": "Document"
      },
      {
        "id": "8f1e16e9-50b8-4c06-ab42-88b13a593ec2",
        "metadata": {
          "vector_store_key": "1606.05320-0",
          "chunk_id": 17,
          "document_id": "1606.05320",
          "start_idx": 9007,
          "end_idx": 9156
        },
        "page_content": "We use visualizations to show how the LSTM and HMM components of the hybrid algorithm complement each other in terms of features learned in the data.",
        "type": "Document"
      },
      {
        "id": "9803af60-5c6c-4442-b662-85acfb19b5ae",
        "metadata": {
          "vector_store_key": "1606.05320-4",
          "chunk_id": 10,
          "document_id": "1606.05320",
          "start_idx": 5297,
          "end_idx": 6005
        },
        "page_content": "For the HMM models, we do a forward pass on the validation set (no backward pass unlike the full FFBS), and compute the HMM state distribution vector $p_t$ for each time step $t$ . Then we compute the predictive likelihood for the next observation as follows: $ P(y_{t+1} | p_t) =\\sum _{x_t=1}^n \\sum _{x_{t+1}=1}^n p_{tx_t} \\cdot T_{x_t, x_{t+1}} \\cdot P(y_{t+1} | x_{t+1})$  where $n$ is the number of hidden states in the HMM. Our main hybrid model is put together sequentially, as shown in Figure 1 . We first run the discrete HMM on the data, outputting the hidden state distributions obtained by the HMM's forward pass, and then add this information to the architecture in parallel with a 1-layer LSTM.",
        "type": "Document"
      },
      {
        "id": "85bcdb7b-501c-44b4-8a56-04c6eb5f9eea",
        "metadata": {
          "vector_store_key": "1606.05320-0",
          "chunk_id": 16,
          "document_id": "1606.05320",
          "start_idx": 8345,
          "end_idx": 9007
        },
        "page_content": "We see some examples where the HMM and LSTM complement each other, such as learning different things about spaces and comments on Linux data, or punctuation on the Shakespeare data. In Figure 2 , we see that some individual LSTM hidden state dimensions identify similar features, such as comment symbols in the Linux data. Hybrid HMM-RNN approaches combine the interpretability of HMMs with the predictive power of RNNs. Sometimes, a small hybrid model can perform better than a standalone LSTM of the same size. We use visualizations to show how the LSTM and HMM components of the hybrid algorithm complement each other in terms of features learned in the data.",
        "type": "Document"
      },
      {
        "id": "45945c8b-f234-4054-bb62-7d4525e24d7f",
        "metadata": {
          "vector_store_key": "1606.05320-4",
          "chunk_id": 11,
          "document_id": "1606.05320",
          "start_idx": 6005,
          "end_idx": 6638
        },
        "page_content": "We first run the discrete HMM on the data, outputting the hidden state distributions obtained by the HMM's forward pass, and then add this information to the architecture in parallel with a 1-layer LSTM. The linear layer between the LSTM and the prediction layer is augmented with an extra column for each HMM state. The LSTM component of this architecture can be smaller than a standalone LSTM, since it only needs to fill in the gaps in the HMM's predictions. The HMM is written in Python, and the rest of the architecture is in Torch. We also build a joint hybrid model, where the LSTM and HMM are simultaneously trained in Torch.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What kind of information do the HMMs learn that the LSTMs don't?",
    "llm_answer": "The context states that HMMs and LSTMs learn *different* things about spaces and comments in Linux data, and punctuation in Shakespeare data.  However, it does not specify *what* the HMMs specifically learn that LSTMs don't.\n",
    "context": [
      {
        "id": "85bcdb7b-501c-44b4-8a56-04c6eb5f9eea",
        "metadata": {
          "vector_store_key": "1606.05320-0",
          "chunk_id": 16,
          "document_id": "1606.05320",
          "start_idx": 8345,
          "end_idx": 9007
        },
        "page_content": "We see some examples where the HMM and LSTM complement each other, such as learning different things about spaces and comments on Linux data, or punctuation on the Shakespeare data. In Figure 2 , we see that some individual LSTM hidden state dimensions identify similar features, such as comment symbols in the Linux data. Hybrid HMM-RNN approaches combine the interpretability of HMMs with the predictive power of RNNs. Sometimes, a small hybrid model can perform better than a standalone LSTM of the same size. We use visualizations to show how the LSTM and HMM components of the hybrid algorithm complement each other in terms of features learned in the data.",
        "type": "Document"
      },
      {
        "id": "2ca3ce25-2fe5-4a35-afc0-91bdb0106048",
        "metadata": {
          "vector_store_key": "1606.05320-0",
          "chunk_id": 15,
          "document_id": "1606.05320",
          "start_idx": 8007,
          "end_idx": 8725
        },
        "page_content": "We interpret the HMM and LSTM states in the hybrid algorithm with 10 LSTM state dimensions and 10 HMM states in Figures 3 and 3 , showing which features are identified by the HMM and LSTM components. In Figures 3 and 3 , we color-code the training data with the 10 HMM states. In Figures 3 and 3 , we apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters. The HMM and LSTM states pick up on spaces, indentation, and special characters in the data (such as comment symbols in Linux data). We see some examples where the HMM and LSTM complement each other, such as learning different things about spaces and comments on Linux data, or punctuation on the Shakespeare data.",
        "type": "Document"
      },
      {
        "id": "869c99bf-dcfd-426b-9746-c9771f593d8f",
        "metadata": {
          "vector_store_key": "1606.05320-1",
          "chunk_id": 7,
          "document_id": "1606.05320",
          "start_idx": 3744,
          "end_idx": 4320
        },
        "page_content": "(Figures 3 , 3 ). We compare a hybrid HMM-LSTM approach with a continuous emission HMM (trained on the hidden states of a 2-layer LSTM), and a discrete emission HMM (trained directly on data). We use a character-level LSTM with 1 layer and no dropout, based on the Element-Research library. We train the LSTM for 10 epochs, starting with a learning rate of 1, where the learning rate is halved whenever $\\exp (-l_t) > \\exp (-l_{t-1}) + 1$ , where $l_t$ is the log likelihood score at epoch $t$ . The $L_2$ -norm of the parameter gradient vector is clipped at a threshold of 5.",
        "type": "Document"
      },
      {
        "id": "8f1e16e9-50b8-4c06-ab42-88b13a593ec2",
        "metadata": {
          "vector_store_key": "1606.05320-0",
          "chunk_id": 17,
          "document_id": "1606.05320",
          "start_idx": 9007,
          "end_idx": 9156
        },
        "page_content": "We use visualizations to show how the LSTM and HMM components of the hybrid algorithm complement each other in terms of features learned in the data.",
        "type": "Document"
      },
      {
        "id": "8cd90f92-f9fe-47b1-87b2-d4ce1449bd85",
        "metadata": {
          "vector_store_key": "1606.05320-1",
          "chunk_id": 12,
          "document_id": "1606.05320",
          "start_idx": 6340,
          "end_idx": 6993
        },
        "page_content": "We also build a joint hybrid model, where the LSTM and HMM are simultaneously trained in Torch. We implemented an HMM Torch module, optimized using stochastic gradient descent rather than FFBS. Similarly to the sequential hybrid model, we concatenate the LSTM outputs with the HMM state probabilities. We test the models on several text data sets on the character level: the Penn Tree Bank (5M characters), and two data sets used by BIBREF4 , Tiny Shakespeare (1M characters) and Linux Kernel (5M characters). We chose $k=20$ for the continuous HMM based on a PCA analysis of the LSTM states, as the first 20 components captured almost all the variance.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How large is the gap in performance between the HMMs and the LSTMs?",
    "llm_answer": "The hybrid algorithm performs a bit better than the standalone LSTM on all text datasets.  The effect of the HMM diminishes as the LSTM size increases.\n",
    "context": [
      {
        "id": "869c99bf-dcfd-426b-9746-c9771f593d8f",
        "metadata": {
          "vector_store_key": "1606.05320-1",
          "chunk_id": 7,
          "document_id": "1606.05320",
          "start_idx": 3744,
          "end_idx": 4320
        },
        "page_content": "(Figures 3 , 3 ). We compare a hybrid HMM-LSTM approach with a continuous emission HMM (trained on the hidden states of a 2-layer LSTM), and a discrete emission HMM (trained directly on data). We use a character-level LSTM with 1 layer and no dropout, based on the Element-Research library. We train the LSTM for 10 epochs, starting with a learning rate of 1, where the learning rate is halved whenever $\\exp (-l_t) > \\exp (-l_{t-1}) + 1$ , where $l_t$ is the log likelihood score at epoch $t$ . The $L_2$ -norm of the parameter gradient vector is clipped at a threshold of 5.",
        "type": "Document"
      },
      {
        "id": "d9476869-731e-4fac-846c-38ccf5efc581",
        "metadata": {
          "vector_store_key": "1606.05320-1",
          "chunk_id": 14,
          "document_id": "1606.05320",
          "start_idx": 7385,
          "end_idx": 8007
        },
        "page_content": "The hybrid algorithm with 20 HMM states does better than the one with 10 HMM states. The joint hybrid algorithm outperforms the sequential hybrid on Shakespeare data, but does worse on PTB and Linux data, which suggests that the joint hybrid is more helpful for smaller data sets. The joint hybrid is an order of magnitude slower than the sequential hybrid, as the SGD-based HMM is slower to train than the FFBS-based HMM. We interpret the HMM and LSTM states in the hybrid algorithm with 10 LSTM state dimensions and 10 HMM states in Figures 3 and 3 , showing which features are identified by the HMM and LSTM components.",
        "type": "Document"
      },
      {
        "id": "ac722094-ed9c-471a-a975-8890461c9aa0",
        "metadata": {
          "vector_store_key": "1606.05320-1",
          "chunk_id": 13,
          "document_id": "1606.05320",
          "start_idx": 6993,
          "end_idx": 7612
        },
        "page_content": "We chose $k=20$ for the continuous HMM based on a PCA analysis of the LSTM states, as the first 20 components captured almost all the variance. Table 1 shows the predictive log likelihood of the next text character for each method. On all text data sets, the hybrid algorithm performs a bit better than the standalone LSTM with the same LSTM state dimension. This effect gets smaller as we increase the LSTM size and the HMM makes less difference to the prediction (though it can still make a difference in terms of interpretability). The hybrid algorithm with 20 HMM states does better than the one with 10 HMM states.",
        "type": "Document"
      },
      {
        "id": "8cd90f92-f9fe-47b1-87b2-d4ce1449bd85",
        "metadata": {
          "vector_store_key": "1606.05320-1",
          "chunk_id": 12,
          "document_id": "1606.05320",
          "start_idx": 6340,
          "end_idx": 6993
        },
        "page_content": "We also build a joint hybrid model, where the LSTM and HMM are simultaneously trained in Torch. We implemented an HMM Torch module, optimized using stochastic gradient descent rather than FFBS. Similarly to the sequential hybrid model, we concatenate the LSTM outputs with the HMM state probabilities. We test the models on several text data sets on the character level: the Penn Tree Bank (5M characters), and two data sets used by BIBREF4 , Tiny Shakespeare (1M characters) and Linux Kernel (5M characters). We chose $k=20$ for the continuous HMM based on a PCA analysis of the LSTM states, as the first 20 components captured almost all the variance.",
        "type": "Document"
      },
      {
        "id": "2e7cfe41-b41c-475c-83f6-e7d39903180d",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 40,
          "document_id": "1909.13375",
          "start_idx": 21052,
          "end_idx": 21792
        },
        "page_content": "While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1. All that, while keeping in mind we compare a base model to a large model (for reference, note the 8 point improvement between MTMSNbase and MTMSNlarge in both EM and F1). Our best model, large-squad, exhibits a huge improvement of 29.7 in EM and 15.1 in F1 compared to MTMSNlarge. When comparing single-span performance, our best model exhibits slightly better results, but it should be noted that it retains the single-span heads from NABERT+, while in MTMSN they have one head to predict both single-span and multi-span answers.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what was their system's f1 performance?",
    "llm_answer": "The system achieved an F1 score of 78.38% on the whole development set.\n",
    "context": [
      {
        "id": "ca03d5d1-b87e-43e6-8b4e-185db97b1ef6",
        "metadata": {
          "vector_store_key": "1701.02877-0",
          "chunk_id": 70,
          "document_id": "1701.02877",
          "start_idx": 35785,
          "end_idx": 36572
        },
        "page_content": "Interestingly, average F1 on seen NEs in the Twitter corpora (MSM and Ritter) is around 80, whereas average F1 on the ACE corpora, which are of similar size, is lower, at around 70. To summarise, our findings are: [noitemsep] F1 on unseen NEs is significantly lower than F1 on seen NEs for all three NERC approaches, which is mostly due to recall on unseen NEs being lower than that on seen NEs. Performance on seen NEs is significantly and consistently higher than that of unseen NEs in different corpora, with the lower scores mostly attributable to lower recall. However, there are still significant differences at labelling seen NEs in different corpora, which means that if NEs are seen or unseen does not account for all of the difference of F1 between corpora of different genres.",
        "type": "Document"
      },
      {
        "id": "0fbb350a-16ee-4a8b-a81f-4f933b2dab44",
        "metadata": {
          "vector_store_key": "1703.04617-3",
          "chunk_id": 44,
          "document_id": "1703.04617",
          "start_idx": 24348,
          "end_idx": 24966
        },
        "page_content": "And the other part accounts for 30.90%, of which the average F1 score is 30.03%. For the latter, we can further divide it into two sub-parts: one is where the F1 score equals to 0%, which means that predict answer is totally wrong. This part occupies 14.89% of the total development set. The other part accounts for 16.01% of the development set, of which average F1 score is 57.96%. From this analysis we can see that reducing the zero F1 score (14.89%) is potentially an important direction to further improve the system. Closely modelling questions could be of importance for question answering and machine reading.",
        "type": "Document"
      },
      {
        "id": "36dcf150-24e8-459c-a8c0-aa8fe1c0018f",
        "metadata": {
          "vector_store_key": "1909.02764-4",
          "chunk_id": 36,
          "document_id": "1909.02764",
          "start_idx": 19976,
          "end_idx": 20568
        },
        "page_content": "We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with 40 % of the data from the same domain (results shown in the column \u201cIn-Domain\u201d in Table TABREF19). Excluding AMMER, we achieve an average micro $\\text{F}_1$ of 68 %, with best results of F$_1$=73 % on TEC. The model trained on our AMMER corpus achieves an F1 score of 57%. This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set.",
        "type": "Document"
      },
      {
        "id": "b0e48c22-4859-4ba0-8ab2-939d7e84a941",
        "metadata": {
          "vector_store_key": "2001.00137-3",
          "chunk_id": 40,
          "document_id": "2001.00137",
          "start_idx": 23303,
          "end_idx": 24041
        },
        "page_content": "The per-class F1 score was also evaluated in the form of normalized confusion matrices, showing that our model was able to improve the overall performance by better balancing the accuracy of each class, trading-off small decreases in high achieving class for significant improvements in lower performing ones. In the Chatbot dataset, accuracy improvement was achieved even without trade-off, with the highest achieving classes maintaining their accuracy while the lower achieving class saw improvement. Further evaluation on the F1-scores decay in the presence of noise demonstrated that our model is more robust than the baseline models when considering noisy data, be that in the form of incorrect sentences or sentences with STT error.",
        "type": "Document"
      },
      {
        "id": "5567ec52-3c38-41b4-b624-2bfe13262a85",
        "metadata": {
          "vector_store_key": "1703.04617-3",
          "chunk_id": 43,
          "document_id": "1703.04617",
          "start_idx": 23973,
          "end_idx": 24558
        },
        "page_content": "In Figure UID61 we can see that the average EM/F1 of the \"when\" question is highest and those of the \"why\" question is the lowest. From Figure UID62 we can see the \"what\" question is the major class. Figure 5 shows the composition of F1 score. Take our best model as an example, we observed a 78.38% F1 score on the whole development set, which can be separated into two parts: one is where F1 score equals to 100%, which means an exact match. This part accounts for 69.10% of the entire development set. And the other part accounts for 30.90%, of which the average F1 score is 30.03%.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much more coverage is in the new dataset?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "c86113ee-192d-4181-b86f-2c801466130d",
        "metadata": {
          "vector_store_key": "1904.05862-2",
          "chunk_id": 25,
          "document_id": "1904.05862",
          "start_idx": 13422,
          "end_idx": 14093
        },
        "page_content": "Accuracy steadily increases with more data for pre-training and the best accuracy is achieved when we use the largest amount of data for pre-training. In this section we analyze some of the design choices we made for . We pre-train on the 80 hour subset of clean Librispeech and evaluate on TIMIT. Table shows that increasing the number of negative samples only helps up to ten samples. Thereafter, performance plateaus while training time increases. We suspect that this is because the training signal from the positive samples decreases as the number of negative samples increases. In this experiment, everything is kept equal except for the number of negative samples.",
        "type": "Document"
      },
      {
        "id": "6e9156a1-1bd2-4904-9fe1-0957c0410a9d",
        "metadata": {
          "vector_store_key": "1909.13375-3",
          "chunk_id": 43,
          "document_id": "1909.13375",
          "start_idx": 22785,
          "end_idx": 23407
        },
        "page_content": "Not using the simple preprocessing from Section SECREF17 resulted in a 2.5 point decrease in both EM and F1. The numeric questions were the most affected, with their performance dropping by 3.5 points. Given that number questions make up about 61% of the dataset, we can deduce that our improved number handling is responsible for about a 2.1 point gain, while the rest could be be attributed to the improved Wikipedia parsing. Although NER span cleaning (Section SECREF23) affected only 3% of the multi-span questions, it provided a solid improvement of 5.4 EM in multi-span questions and 1.5 EM in single-span questions.",
        "type": "Document"
      },
      {
        "id": "6824b4f2-136e-4c26-a2a7-a10cf7d4c97e",
        "metadata": {
          "vector_store_key": "1810.00663-8",
          "chunk_id": 46,
          "document_id": "1810.00663",
          "start_idx": 25788,
          "end_idx": 26550
        },
        "page_content": "The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as can be seen in performance drops in Table TABREF28 for the new environments. Nonetheless, the insights from the previous section still hold: masking in the output layer and reordering the graph triplets tend to increase performance. Even though the results in Table TABREF28 suggest that there is room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM.",
        "type": "Document"
      },
      {
        "id": "52a5c7b9-197c-4c52-9a22-93a7f3f1bb7a",
        "metadata": {
          "vector_store_key": "1910.12618-2",
          "chunk_id": 45,
          "document_id": "1910.12618",
          "start_idx": 24114,
          "end_idx": 24750
        },
        "page_content": "In order to tone it down and to increase the consistency of our results, the different models are run $B=10$ times. The results presented hereafter correspond to the average and standard-deviation on those runs. The RF model denoted as \"sel\" is the one with the reduced number of features, whereas the other RF uses the full vocabulary. We also considered an aggregated forecaster (abridged Agg), consisting of the average of the two best individual ones in terms of RMSE. All the neural network methods have a reduced vocabulary size $V^*$. The results for the French and UK data are respectively given by tables TABREF26 and TABREF27.",
        "type": "Document"
      },
      {
        "id": "8c714d0f-b982-4837-aefc-3b59003e3a67",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 15,
          "document_id": "1912.13109",
          "start_idx": 7410,
          "end_idx": 8070
        },
        "page_content": "Train-test split: The labelled dataset that was available for this task was very limited in number of examples and thus as noted above few data augmentation techniques were applied to boost the learning of the network. Before applying augmentation, a train-test split of 78%-22% was done from the original, cleansed data set. Thus, 700 tweets/messages were held out for testing. All model evaluation were done in on the test set that got generated by this process. The results presented in this report are based on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How was quality measured?",
    "llm_answer": "Human evaluation using a 5-point Likert scale, average rank, percentage of best-ranked questions, Pearson correlation with human judgments, and mean squared error (MSE).  Evaluation of question quality also used a validation step where workers judged the validity of questions, not answers.\n",
    "context": [
      {
        "id": "8c81c383-8e40-4094-8e18-ab0a2d32126a",
        "metadata": {
          "vector_store_key": "1905.08949-6",
          "chunk_id": 18,
          "document_id": "1905.08949",
          "start_idx": 10340,
          "end_idx": 11074
        },
        "page_content": "Although the datasets are commonly shared between QG and QA, it is not the case for evaluation: it is challenging to define a gold standard of proper questions to ask. Meaningful, syntactically correct, semantically sound and natural are all useful criteria, yet they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks. As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE BIBREF43 , are also widely used.",
        "type": "Document"
      },
      {
        "id": "b793e1d3-56ff-490e-9245-8753da5bf16c",
        "metadata": {
          "vector_store_key": "1911.07228-2",
          "chunk_id": 17,
          "document_id": "1911.07228",
          "start_idx": 8542,
          "end_idx": 9180
        },
        "page_content": "In our experiments, we use three evaluation parameters (precision, recall, and F1 score) to access our experimental result. They will be described as follow in Table 3. The \"correctNE\", the number of correct label for entity that the model can found. The \"goldNE\", number of the real label annotated by annotator in the gold data. The \"foundNE\", number of the label the model find out (no matter if they are correct or not). In Table 3 above, we can see that recall score on ORG label is lowest. The reason is almost all the ORG label on test file is name of some brands that do not appear on training data and pre-trained word embedding.",
        "type": "Document"
      },
      {
        "id": "f2ccbf11-92ca-4546-b7d7-d1243199c4a9",
        "metadata": {
          "vector_store_key": "1909.00279-3",
          "chunk_id": 26,
          "document_id": "1909.00279",
          "start_idx": 15254,
          "end_idx": 15992
        },
        "page_content": "The human evaluators were divided into two groups. The expert group contains 15 people who hold a bachelor degree in Chinese literature, and the amateur group contains 15 people who holds a bachelor degree in other fields. All 30 human evaluators are native Chinese speakers. We ask evaluators to grade each generated poem from four perspectives: 1) Fluency: Is the generated poem grammatically and rhythmically well formed, 2) Semantic coherence: Is the generated poem itself semantic coherent and meaningful, 3) Semantic preservability: Does the generated poem preserve the semantic of the modern Chinese translation, 4) Poeticness: Does the generated poem display the characteristic of a poem and does the poem build good poetic image.",
        "type": "Document"
      },
      {
        "id": "85620a36-4465-42bf-a217-83811397b9bc",
        "metadata": {
          "vector_store_key": "1809.08731-4",
          "chunk_id": 20,
          "document_id": "1809.08731",
          "start_idx": 11375,
          "end_idx": 12197
        },
        "page_content": "Following earlier work BIBREF2 , we evaluate our metrics using Pearson correlation with human judgments. It is defined as the covariance divided by the product of the standard deviations:  $$\\rho _{X,Y} = \\frac{\\text{cov}(X,Y)}{\\sigma _X \\sigma _Y}$$   (Eq. 28)  Pearson cannot accurately judge a metric's performance for sentences of very similar quality, i.e., in the extreme case of rating outputs of identical quality, the correlation is either not defined or 0, caused by noise of the evaluation model. Thus, we additionally evaluate using mean squared error (MSE), which is defined as the squares of residuals after a linear transformation, divided by the sample size:  $$\\text{MSE}_{X,Y} = \\underset{f}{\\min }\\frac{1}{|X|}\\sum \\limits _{i = 1}^{|X|}{(f(x_i) - y_i)^2}$$   (Eq. 30)  with $f$ being a linear function.",
        "type": "Document"
      },
      {
        "id": "d895390e-64a9-4a6b-9862-3c916d39a838",
        "metadata": {
          "vector_store_key": "1911.03243-0",
          "chunk_id": 5,
          "document_id": "1911.03243",
          "start_idx": 3258,
          "end_idx": 3996
        },
        "page_content": "Since crowd-workers had no prior training, quality was established using an additional validation step, where workers had to ascertain the validity of the question, but not of its answers. Instead, the validator provided additional answers, independent of the other annotators. Each verb in the corpus was annotated by a single QA-generating worker and validated by two others. In a reserved part of the corpus (Dense), targeted for parser evaluation, verbs were densely validated with 5 workers, approving questions judged as valid by at least 4/5 validators. Notably, adding validators to the Dense annotation pipeline accounts mostly for precision errors, while role coverage solely relies upon the single generator's set of questions.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is different in the improved annotation protocol?",
    "llm_answer": "The improved annotation protocol uses a template with scenario-specific event and participant labels, annotates all NP heads with participant labels and all verbs with event labels, and includes coreference chains between NPs.  It also uses a single annotation mode.\n",
    "context": [
      {
        "id": "6ac93a11-3bbd-439f-a9ee-157a9bd58381",
        "metadata": {
          "vector_store_key": "1912.01673-1",
          "chunk_id": 13,
          "document_id": "1912.01673",
          "start_idx": 7638,
          "end_idx": 8392
        },
        "page_content": "In the first one, we were looking for original and uncommon sentence change suggestions. In the second one, we collected sentence alternations using ideas from the first round. The first and second rounds of annotation could be broadly called as collecting ideas and collecting data, respectively. We manually selected 15 newspaper headlines. Eleven annotators were asked to modify each headline up to 20 times and describe the modification with a short name. They were given an example sentence and several of its possible alternations, see tab:firstroundexamples. Unfortunately, these examples turned out to be highly influential on the annotators' decisions and they correspond to almost two thirds of all of modifications gathered in the first round.",
        "type": "Document"
      },
      {
        "id": "5207408d-e8da-4dfb-a788-6d2442fee140",
        "metadata": {
          "vector_store_key": "1703.05260-1",
          "chunk_id": 14,
          "document_id": "1703.05260",
          "start_idx": 7669,
          "end_idx": 8341
        },
        "page_content": "This refinement was necessary due to the complexity of the annotation. For each of the scenarios, we designed a specific annotation template. A script template consists of scenario-specific event and participant labels. An example of a template is shown in Table 1 . All NP heads in the corpus were annotated with a participant label; all verbs were annotated with an event label. For both participants and events, we also offered the label unclear if the annotator could not assign another label. We additionally annotated coreference chains between NPs. Thus, the process resulted in three layers of annotation: event types, participant types and coreference annotation.",
        "type": "Document"
      },
      {
        "id": "034ce07f-b20c-4fa8-929f-c39168acbfe1",
        "metadata": {
          "vector_store_key": "1701.02877-5",
          "chunk_id": 14,
          "document_id": "1701.02877",
          "start_idx": 7142,
          "end_idx": 7875
        },
        "page_content": "A further difference between the ACE and OntoNotes corpora on one hand, and CoNLL and MUC on the other, is that they contain annotations not only for NER, but also for other tasks such as coreference resolution, relation and event extraction and word sense disambiguation. In this paper, however, we restrict ourselves purely to the English NER annotations, for consistency across datasets. The ACE corpus contains HEAD as well as EXTENT annotations for NE spans. For our experiments we use the EXTENT tags. With the emergence of social media, studying NER performance on this genre gained momentum. So far, there have been no big evaluation efforts, such as ACE and OntoNotes, resulting in substantial amounts of gold standard data.",
        "type": "Document"
      },
      {
        "id": "c2650d93-c3c3-4583-b079-29db9f7fc689",
        "metadata": {
          "vector_store_key": "1703.05260-3",
          "chunk_id": 32,
          "document_id": "1703.05260",
          "start_idx": 17652,
          "end_idx": 18317
        },
        "page_content": "The annotation was rather time-consuming due to the complexity of the task, and thus we decided for single annotation mode. To assess annotation quality, a small sample of texts was annotated by all four annotators and their inter-annotator agreement was measured (see Section \"Inter-Annotator Agreement\" ). It was found to be sufficiently high. Annotation of the corpus together with some pre- and post-processing of the data required about 500 hours of work. All stories were annotated with event and participant types (a total of 12,188 and 43,946 instances, respectively). On average there were 7 coreference chains per story with an average length of 6 tokens.",
        "type": "Document"
      },
      {
        "id": "d7b58e09-4dd2-4da6-a15a-b72d3c0ccd0e",
        "metadata": {
          "vector_store_key": "1810.06743-0",
          "chunk_id": 20,
          "document_id": "1810.06743",
          "start_idx": 11861,
          "end_idx": 12643
        },
        "page_content": "Moreover, we contend that ambiguity in the schemata leave the door open to allow for such imperfections. With no strict guidance, it's natural that annotators would take different paths. Nevertheless, modulo annotator disagreement, we assume that within a particular corpus, one word form will always be consistently annotated. Three categories of annotation difficulty are missing values, language-specific attributes, and multiword expressions. In our work, the goal is not simply to translate one schema into the other, but to translate one resource (the imperfect manifestation of the schema) to match the other. The differences between the schemata and discrepancies in annotation mean that the transformation of annotations from one schema to the other is not straightforward.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What data were they used to train the multilingual encoder?",
    "llm_answer": "En \u2194 Fr and Fr \u2194 En examples.\n",
    "context": [
      {
        "id": "c35b0f2a-e248-4f37-9d19-21cc1f0eb705",
        "metadata": {
          "vector_store_key": "1809.04686-1",
          "chunk_id": 7,
          "document_id": "1809.04686",
          "start_idx": 4147,
          "end_idx": 4920
        },
        "page_content": "After training such a multilingual NMT model, the decoder and the corresponding attention mechanisms (which are target-language specific) are discarded, while the multilingual encoder is used to initialize the encoder of our proposed Encoder-Classifier model. In order to leverage pre-trained multilingual representations introduced in Section \"Analyses\" , our encoder strictly follows the structure of a regular Recurrent Neural Network (RNN) based NMT encoder BIBREF33 with a stacked layout BIBREF34 . Given an input sequence ${\\mathbf {x}} = (x_{1}, x_{2}, \\ldots , x_{T_x})$ of length $T_x$ , our encoder contextualizes or encodes the input sequence into a set of vectors C, by first applying a bi-directional RNN BIBREF35 , followed by a stack of uni-directional RNNs.",
        "type": "Document"
      },
      {
        "id": "9437bcdc-d26c-4c74-be0b-6da5af3729eb",
        "metadata": {
          "vector_store_key": "1809.04686-3",
          "chunk_id": 33,
          "document_id": "1809.04686",
          "start_idx": 18945,
          "end_idx": 19672
        },
        "page_content": "In particular, we study (1) the effect of shared sub-word vocabulary, (2) the amount of multilingual training data to measure the influence of multilinguality, (3) encoder/classifier capacity to measure the influence of representation power, and (4) model behavior on different training phases to assess the relation between generalization performance on English and zero-shot performance on French. In this paper, we have demonstrated a simple yet effective approach to perform cross-lingual transfer learning using representations from a multilingual NMT model. Our proposed approach of reusing the encoder from a multilingual NMT system as a pre-trained component provides significant improvements on three downstream tasks.",
        "type": "Document"
      },
      {
        "id": "da8b8bec-6bf7-44c5-a766-76823a2743d1",
        "metadata": {
          "vector_store_key": "1809.04686-1",
          "chunk_id": 20,
          "document_id": "1809.04686",
          "start_idx": 10893,
          "end_idx": 11637
        },
        "page_content": "Similar to BIBREF30 , multilingual NMT model is trained in a multi-task learning setup, where each decoder is augmented with a task-specific loss, minimizing the negative conditional log-likelihood of the target sequence given the source sequence. During training, mini-batches of En $\\rightarrow $ Fr and Fr $\\rightarrow $ En examples are interleaved. We picked the best model based on the best average development set BLEU score on both of the language pairs. The Encoder-Classifier model here uses the encoder defined previously. With regards to the classifier, the pre- and post-pooling networks ( $f_{pre}$ , $f_{post}$ ) are both one-layer feed forward networks to cast the dimension size from 512 to 128 and from 128 to 32, respectively.",
        "type": "Document"
      },
      {
        "id": "a440212d-750c-4498-9795-15df3b9daf6d",
        "metadata": {
          "vector_store_key": "1809.04686-1",
          "chunk_id": 25,
          "document_id": "1809.04686",
          "start_idx": 13977,
          "end_idx": 14793
        },
        "page_content": "Table 1 summarizes the accuracy of our proposed system for these three different approaches and the state-of-the-art results on all the tasks. The first row in the table shows the baseline accuracy of our system for all four datasets. The second row shows the result from initializing with a pre-trained multilingual NMT encoder. It can be seen that this provides a significant improvement in accuracy, an average of 4.63%, across all the tasks. This illustrates that the multilingual NMT encoder has successfully learned transferable contextualized representations that are leveraged by the classifier component of our proposed system. These results are in line with the results in BIBREF5 where the authors used the representations from the top NMT encoder layer as an additional input to the task-specific system.",
        "type": "Document"
      },
      {
        "id": "9d58f368-1832-4cfc-bd63-3a1added7a13",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 18,
          "document_id": "1909.00694",
          "start_idx": 9543,
          "end_idx": 10328
        },
        "page_content": "For the details of ${\\rm Encoder}$, see Sections SECREF30. We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\mathcal {L}_{\\rm AL}$, $\\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$, $\\mathcal {L}_{\\rm ACP}$, and $\\mathcal {L}_{\\rm ACP} + \\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$. Table TABREF23 shows accuracy. As the Random baseline suggests, positive and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon and output the corresponding label (or the reverse of it for negation) if the event's predicate is in the seed lexicon.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "From when are many VQA datasets collected?",
    "llm_answer": "Late 2014.\n",
    "context": [
      {
        "id": "d1c4f66e-a324-46df-8d8c-8be7183c373b",
        "metadata": {
          "vector_store_key": "1703.09684-0",
          "chunk_id": 4,
          "document_id": "1703.09684",
          "start_idx": 2245,
          "end_idx": 2950
        },
        "page_content": "In addition, it also allowed us to answer several key questions about VQA algorithms, such as, `Is the generalization capacity of the algorithms hindered by the bias in the dataset?', `Does the use of spatial attention help answer specific question-types?', `How successful are the VQA algorithms in answering less-common questions?', and 'Can the VQA algorithms differentiate between real and absurd questions?' Six datasets for VQA with natural images have been released between 2014\u20132016: DAQUAR BIBREF0 , COCO-QA BIBREF3 , FM-IQA BIBREF4 , The VQA Dataset BIBREF1 , Visual7W BIBREF5 , and Visual Genome BIBREF6 . FM-IQA needs human judges and has not been widely used, so we do not discuss it further.",
        "type": "Document"
      },
      {
        "id": "3e761518-42ce-43b8-b9d7-2bfc2decb970",
        "metadata": {
          "vector_store_key": "1703.09684-0",
          "chunk_id": 0,
          "document_id": "1703.09684",
          "start_idx": 0,
          "end_idx": 655
        },
        "page_content": "In open-ended visual question answering (VQA) an algorithm must produce answers to arbitrary text-based questions about images BIBREF0 , BIBREF1 . VQA is an exciting computer vision problem that requires a system to be capable of many tasks. Truly solving VQA would be a milestone in artificial intelligence, and would significantly advance human computer interaction. However, VQA datasets must test a wide range of abilities for progress to be adequately measured. VQA research began in earnest in late 2014 when the DAQUAR dataset was released BIBREF0 . Including DAQUAR, six major VQA datasets have been released, and algorithms have rapidly improved.",
        "type": "Document"
      },
      {
        "id": "11259e8d-c7d6-4b81-915d-e9dac62c46d1",
        "metadata": {
          "vector_store_key": "1703.09684-0",
          "chunk_id": 1,
          "document_id": "1703.09684",
          "start_idx": 655,
          "end_idx": 1497
        },
        "page_content": "Including DAQUAR, six major VQA datasets have been released, and algorithms have rapidly improved. On the most popular dataset, `The VQA Dataset' BIBREF1 , the best algorithms are now approaching 70% accuracy BIBREF2 (human performance is 83%). While these results are promising, there are critical problems with existing datasets in terms of multiple kinds of biases. Moreover, because existing datasets do not group instances into meaningful categories, it is not easy to compare the abilities of individual algorithms. For example, one method may excel at color questions compared to answering questions requiring spatial reasoning. Because color questions are far more common in the dataset, an algorithm that performs well at spatial reasoning will not be appropriately rewarded for that feat due to the evaluation metrics that are used.",
        "type": "Document"
      },
      {
        "id": "c9b393de-f911-46e4-97f3-50f5cf5eb6a9",
        "metadata": {
          "vector_store_key": "1703.09684-0",
          "chunk_id": 5,
          "document_id": "1703.09684",
          "start_idx": 2950,
          "end_idx": 3594
        },
        "page_content": "FM-IQA needs human judges and has not been widely used, so we do not discuss it further. Table 1 shows statistics for the other datasets. Following others BIBREF7 , BIBREF8 , BIBREF9 , we refer to the portion of The VQA Dataset containing natural images as COCO-VQA. Detailed dataset reviews can be found in BIBREF10 and BIBREF11 . All of the aforementioned VQA datasets are biased. DAQUAR and COCO-QA are small and have a limited variety of question-types. Visual Genome, Visual7W, and COCO-VQA are larger, but they suffer from several biases. Bias takes the form of both the kinds of questions asked and the answers that people give for them.",
        "type": "Document"
      },
      {
        "id": "7d2fdb0e-a13a-44b2-bd23-97bc3d50e3ea",
        "metadata": {
          "vector_store_key": "1703.09684-0",
          "chunk_id": 66,
          "document_id": "1703.09684",
          "start_idx": 36870,
          "end_idx": 37626
        },
        "page_content": "Due to a these stringent criteria, we could only create a small number of questions using Visual Genome annotations compared to other sources. The number of questions produced via each source is shown in Table 4 . Figure 3 shows the answer distribution for the different question-types. We can see that some categories, such as counting, scene recognition and sentiment understanding, have a very large share of questions represented by only a few top answers. In such cases, the performance of a VQA algorithm can be inflated unless the evaluation metric compensates for this bias. In other cases, such as positional reasoning and object utility and affordances, the answers are much more varied, with top-50 answers covering less than 60% of all answers.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is task success rate achieved? ",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "5aba0c8a-1e24-40b4-8d8d-8579cf563fcd",
        "metadata": {
          "vector_store_key": "2002.11893-3",
          "chunk_id": 35,
          "document_id": "2002.11893",
          "start_idx": 20128,
          "end_idx": 20845
        },
        "page_content": "1,759 dialogues have multiple sub-goals (2$\\sim $5) in HAR domains with cross-domain informable slots. 572 dialogues have multiple sub-goals in HAR domains with cross-domain informable slots and at least one sub-goal in the metro or taxi domain (3$\\sim $5 sub-goals). The data statistics are shown in Table TABREF26. As mentioned in Section SECREF14, we generate independent multi-domain, cross multi-domain, and traffic domain sub-goals one by one. Thus in terms of the task complexity, we have S<M<CM and M<M+T<CM+T, which is supported by the average number of sub-goals, semantic tuples, and turns per dialogue in Table TABREF26. The average number of tokens also becomes larger when the goal becomes more complex.",
        "type": "Document"
      },
      {
        "id": "87ca473d-2032-49c5-ac06-9a1b554c964b",
        "metadata": {
          "vector_store_key": "2002.11893-3",
          "chunk_id": 37,
          "document_id": "2002.11893",
          "start_idx": 21224,
          "end_idx": 21967
        },
        "page_content": "In this situation, the wizard will try to relax some constraints and issue multiple queries to find some results for a recommendation while the user will compromise and change the original goal. The negotiation process is captured by \"NoOffer rate\", \"Multi-query rate\", and \"Goal change rate\" in Table TABREF26. In addition, \"Multi-query rate\" suggests that each sub-goal in M and M+T is as easy to finish as the goal in S. The distribution of dialogue length is shown in Figure FIGREF27, which is an indicator of the task complexity. Most single-domain dialogues terminate within 10 turns. The curves of M and M+T are almost of the same shape, which implies that the traffic task requires two additional turns on average to complete the task.",
        "type": "Document"
      },
      {
        "id": "3bf75a5d-c712-46b6-8e95-7eddfbe89d9c",
        "metadata": {
          "vector_store_key": "1911.11744-2",
          "chunk_id": 22,
          "document_id": "1911.11744",
          "start_idx": 11627,
          "end_idx": 12259
        },
        "page_content": "The figure shows that the goal position of a red bowl has a relatively small distribution independently of the used sentence or location on the table, where as an invalid target (green) produces a significantly larger distribution, indicating that the requested task may be invalid. To test our model, we generated 500 new scenario testing each of the three features to identify the correct target among other bowls. A task is considered to be successfully completed when the cube is withing the boundaries of the targeted bowl. Bowls have a bounding box of 12.5 and 17.5cm edge length for the small and large variant, respectively.",
        "type": "Document"
      },
      {
        "id": "6a443055-4b4a-46e1-a2dc-b3211ac62d38",
        "metadata": {
          "vector_store_key": "1908.10449-0",
          "chunk_id": 20,
          "document_id": "1908.10449",
          "start_idx": 10741,
          "end_idx": 11449
        },
        "page_content": "Since iMRC involves both MRC and RL, we adopt evaluation metrics from both settings. First, as a question answering task, we use $\\text{F}_1$ score to compare predicted answers against ground-truth, as in previous works. When there exist multiple ground-truth answers, we report the max $\\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we evaluate an agent's performance during both its training and testing phases. During training, we report training curves averaged over 3 random seeds. During test, we follow common practice in supervised learning tasks where we report the agent's test performance corresponding to its best validation performance .",
        "type": "Document"
      },
      {
        "id": "af13491e-06a9-4b6e-bb91-9be06f299400",
        "metadata": {
          "vector_store_key": "1909.00754-7",
          "chunk_id": 32,
          "document_id": "1909.00754",
          "start_idx": 16421,
          "end_idx": 17065
        },
        "page_content": "For a model having an ITC of $O(1)$ with respect to the number of slots $n$ , and values $m$ , the ITM will be a multiplier of 2.15x, while for an ITC of $O(n)$ , it will be a multiplier of 25.1, and 1,143 for $O(mn)$ . As a convention, the metric of joint goal accuracy is used to compare our model to previous work. The joint goal accuracy only regards the model making a successful belief state prediction if all of the slots and values predicted are exactly matched with the labels provided. This metric gives a strict measurement that tells how often the DST module will not propagate errors to the downstream modules in a dialogue system.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Does proposed end-to-end approach learn in reinforcement or supervised learning manner?",
    "llm_answer": "Reinforcement learning.\n",
    "context": [
      {
        "id": "7ac0f9e6-bb59-472c-802a-202e9c91c25d",
        "metadata": {
          "vector_store_key": "1902.00330-9",
          "chunk_id": 63,
          "document_id": "1902.00330",
          "start_idx": 35054,
          "end_idx": 35797
        },
        "page_content": "To automatic taxonomy induction from a set of terms, Han et al. BIBREF41 designed an end-to-end reinforcement learning model to determine which term to select and where to place it on the taxonomy, which effectively reduced the error propagation between two phases. Inspired by the above works, we also add reinforcement learning to our framework. In this paper we consider entity linking as a sequence decision problem and present a reinforcement learning based model. Our model learns the policy on selecting target entities in a sequential manner and makes decisions based on current state and previous ones. By utilizing the information of previously referred entities, we can take advantage of global consistency to disambiguate mentions.",
        "type": "Document"
      },
      {
        "id": "cdf5a54a-c331-4849-a981-8b182dbe73ea",
        "metadata": {
          "vector_store_key": "1911.11744-1",
          "chunk_id": 2,
          "document_id": "1911.11744",
          "start_idx": 1169,
          "end_idx": 1929
        },
        "page_content": "While imitation learning has been successfully applied to a wide range of tasks including table-tennis BIBREF3, locomotion BIBREF4, and human-robot interaction BIBREF5 an important question is how to incorporate language and vision into a differentiable end-to-end system for complex robot control. In this paper, we present an imitation learning approach that combines language, vision, and motion in order to synthesize natural language-conditioned control policies that have strong generalization capabilities while also capturing the semantics of the task. We argue that such a multi-modal teaching approach enables robots to acquire complex policies that generalize to a wide variety of environmental conditions based on descriptions of the intended task.",
        "type": "Document"
      },
      {
        "id": "e5d28ff5-89c8-4a07-8a46-8985674d8ab7",
        "metadata": {
          "vector_store_key": "1810.00663-5",
          "chunk_id": 52,
          "document_id": "1810.00663",
          "start_idx": 29198,
          "end_idx": 29997
        },
        "page_content": "This work introduced behavioral navigation through free-form natural language instructions as a challenging and a novel task that falls at the intersection of natural language processing and robotics. This problem has a range of interesting cross-domain applications, including information retrieval. We proposed an end-to-end system to translate user instructions to a high-level navigation plan. Our model utilized an attention mechanism to merge relevant information from the navigation instructions with a behavioral graph of the environment. The model then used a decoder to predict a sequence of navigation behaviors that matched the input commands. As part of this effort, we contributed a new dataset of 11,051 pairs of user instructions and navigation plans from 100 different environments.",
        "type": "Document"
      },
      {
        "id": "ace7bf14-dcd7-4997-8709-763353110930",
        "metadata": {
          "vector_store_key": "1902.00330-9",
          "chunk_id": 61,
          "document_id": "1902.00330",
          "start_idx": 33845,
          "end_idx": 34482
        },
        "page_content": "Nguyen et al. BIBREF2 use the sequence model, but they simply encode the results of the greedy choice, and measure the similarities between the global encoding and the candidate entity representations. Their model does not consider the long-term impact of current decisions on subsequent choices, nor does they add the selected target entity information to the current state to help disambiguation. In the last few years, reinforcement learning has emerged as a powerful tool for solving complex sequential decision-making problems. It is well known for its great success in the game field, such as Go BIBREF37 and Atari games BIBREF38 .",
        "type": "Document"
      },
      {
        "id": "25e310e3-8c52-43c5-8721-b0e969273a0a",
        "metadata": {
          "vector_store_key": "1810.00663-9",
          "chunk_id": 51,
          "document_id": "1810.00663",
          "start_idx": 28769,
          "end_idx": 29535
        },
        "page_content": "\u201cExit the room 0 and turn right, go to the end of the corridor and turn left, go straight to the end of the corridor and turn left again. After passing bookshelf on your left and table on your right, Enter the kitchen on your right.\u201d For both routes, the proposed model was able to predict the correct sequence of navigation behaviors. This result suggests that the model is indeed using the input instructions and is not just approximating shortest paths in the behavioral graph. Other examples on the prediction of sub-obtimal paths are described in the Appendix. This work introduced behavioral navigation through free-form natural language instructions as a challenging and a novel task that falls at the intersection of natural language processing and robotics.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is performance of this system measured?",
    "llm_answer": "Mean rank, ratings (rank converted to 7-i), student t-test, accuracy (Quasar-S and Quasar-T), system combination (average rank score), MAPE, RMSE, MAE, $R^2$, EM-score, F$_1$-score.\n",
    "context": [
      {
        "id": "7e3794a8-4c49-4ef6-b670-6e8d7e4dae71",
        "metadata": {
          "vector_store_key": "1905.06566-4",
          "chunk_id": 45,
          "document_id": "1905.06566",
          "start_idx": 25128,
          "end_idx": 25731
        },
        "page_content": "We asked the subjects to rank the outputs of these systems from best to worst. As shown in Table 4 , the output of $\\text{\\sc Hibert}_M$ is selected as the best in 30% of cases and we obtained lower mean rank than all systems except for Human. We also converted the rank numbers into ratings (rank $i$ to $7-i$ ) and applied student $t$ -test on the ratings. $\\text{\\sc Hibert}_M$ is significantly different from all systems in comparison ( $p < 0.05$ ), which indicates our model still lags behind Human, but is better than all other systems. As mentioned earlier, our pre-training includes two stages.",
        "type": "Document"
      },
      {
        "id": "3699d425-11f4-45cc-aae5-6a93d3e32b86",
        "metadata": {
          "vector_store_key": "1707.03904-0",
          "chunk_id": 8,
          "document_id": "1707.03904",
          "start_idx": 4627,
          "end_idx": 5317
        },
        "page_content": "We evaluate Quasar against human testers, as well as several baselines ranging from na\u00efve heuristics to state-of-the-art machine readers. The best performing baselines achieve $33.6\\%$ and $28.5\\%$ on Quasar-S and Quasar-T, while human performance is $50\\%$ and $60.6\\%$ respectively. For the automatic systems, we see an interesting tension between searching and reading accuracies \u2013 retrieving more documents in the search phase leads to a higher coverage of answers, but makes the comprehension task more difficult. We also collect annotations on a subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short.",
        "type": "Document"
      },
      {
        "id": "c523250e-f72a-4bd9-b294-8f8f7c0efe13",
        "metadata": {
          "vector_store_key": "1603.07044-3",
          "chunk_id": 29,
          "document_id": "1603.07044",
          "start_idx": 15744,
          "end_idx": 16390
        },
        "page_content": "If we do a simple system combination (average the rank score) between our system and the IR system, the combined system will give large gains on tasks B and C. This implies that our system is complimentary with the IR system. In addition to quantitative analysis, it is natural to qualitatively evaluate the performance of the attention mechanism by visualizing the weight distribution of each instance. We randomly picked several instances from the test set in task A, for which the sentence lengths are more moderate for demonstration. These examples are shown in Figure 5 , and categorized into short, long, and noisy sentences for discussion.",
        "type": "Document"
      },
      {
        "id": "416141bf-5fd8-4079-8749-1519922ad6a1",
        "metadata": {
          "vector_store_key": "1910.12618-1",
          "chunk_id": 40,
          "document_id": "1910.12618",
          "start_idx": 21360,
          "end_idx": 21946
        },
        "page_content": "The metrics of evaluation are the Mean Absolute Percentage Error (MAPE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and the $R^2$ coefficient given by: where $T$ is the number of test samples, $y_t$ and $\\hat{y}_t$ are respectively the ground truth and the prediction for the document of day $t$, and $\\overline{y}$ is the empirical average of the time series over the test sample. A known problem with MAPE is that it unreasonably increases the error score for values close to 0. While for the load it isn't an issue at all, it can be for the meteorological time series.",
        "type": "Document"
      },
      {
        "id": "d50bd955-993d-4140-b06f-46c6425afab9",
        "metadata": {
          "vector_store_key": "1908.06606-0",
          "chunk_id": 27,
          "document_id": "1908.06606",
          "start_idx": 15944,
          "end_idx": 16737
        },
        "page_content": "These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage of predictions that match any one of the ground truth answers exactly. The F$_1$-score metric is a looser metric measures the average overlap between the prediction and ground truth answer. To implement deep neural network models, we utilize the Keras library BIBREF36 with TensorFlow BIBREF37 backend.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big dataset is used for training this system?",
    "llm_answer": "7934 messages (after augmentation) for the training set.  The WSJ corpus (81 hours), an 80 hour subset of clean Librispeech, the full 960 hour Librispeech training set, or a combination of all of them are used for pre-training.\n",
    "context": [
      {
        "id": "9ca5340d-587b-4e94-bcbe-160570788673",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 35,
          "document_id": "1910.12618",
          "start_idx": 19294,
          "end_idx": 19871
        },
        "page_content": "For instance for random forests it can correspond to the maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014 and 2015 serve as validation set (730 days).",
        "type": "Document"
      },
      {
        "id": "36adf7ba-6ece-4ad5-84b3-0a38b137a4fc",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 16,
          "document_id": "1912.13109",
          "start_idx": 8070,
          "end_idx": 8831
        },
        "page_content": "The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we employed various operational techniques for optimizing on the learning process. These processes and its implementation details will be discussed later but they were learning rate decay, early stopping, model checkpointing and reducing learning rate on plateau.",
        "type": "Document"
      },
      {
        "id": "1b1f9986-e4da-44a7-a6cd-13931a730da6",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 38,
          "document_id": "1909.13375",
          "start_idx": 19979,
          "end_idx": 20678
        },
        "page_content": "For the large models we used a batch size of 3 with a learning rate of $5e^{-6}$ and trained for 5 epochs, except for the model without the single-span heads that was trained with a batch size of 2 for 7 epochs. F1 was used as our validation metric. All models were trained on a single GPU with 12-16GB of memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions.",
        "type": "Document"
      },
      {
        "id": "2619486b-0b73-4f3d-86f3-2cc9adfaeb7a",
        "metadata": {
          "vector_store_key": "1904.05862-0",
          "chunk_id": 12,
          "document_id": "1904.05862",
          "start_idx": 6422,
          "end_idx": 7080
        },
        "page_content": "Wall Street Journal (WSJ; Woodland et al., 1994) comprises about 81 hours of transcribed audio data. We train on si284, validate on nov93dev and test on nov92. Librispeech BIBREF27 contains a total of 960 hours of clean and noisy speech for training. For pre-training, we use either the full 81 hours of the WSJ corpus, an 80 hour subset of clean Librispeech, the full 960 hour Librispeech training set, or a combination of all of them. To train the baseline acoustic model we compute 80 log-mel filterbank coefficients for a 25ms sliding window with stride 10ms. Final models are evaluated in terms of both word error rate (WER) and letter error rate (LER).",
        "type": "Document"
      },
      {
        "id": "c93f2257-9006-466a-af1f-9340e6b4ab7d",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 28,
          "document_id": "2001.00137",
          "start_idx": 16720,
          "end_idx": 17443
        },
        "page_content": "Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector Classifier with L1 and L2 penalty and tolerance of $10^{-3}$; Regularized linear classifier with Stochastic Gradient Descent (SGD) learning with regularization term $alpha=10^{-4}$ and L1, L2 and Elastic-Net penalty; Nearest Centroid with Euclidian metric, where classification is done by representing each class with a centroid;",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they obtain word lattices from words?",
    "llm_answer": "They construct word lattices from an existing lookup vocabulary, or from various word segmentations with different strategies, including using character sequence.\n",
    "context": [
      {
        "id": "23e2b3de-c69e-4f07-97f0-dc51489b01a8",
        "metadata": {
          "vector_store_key": "1902.09087-4",
          "chunk_id": 13,
          "document_id": "1902.09087",
          "start_idx": 7346,
          "end_idx": 8006
        },
        "page_content": "For the example shown in Figure FIGREF6 , the word \u201ccitizen\u201d is the center word of four text spans with length 3: \u201cChina - citizen - life\u201d, \u201cChina - citizen - alive\u201d, \u201ccountry - citizen - life\u201d, \u201ccountry - citizen - alive\u201d, so four feature vectors will be produced for width-3 convolutional kernels for \u201ccitizen\u201d. As shown in Figure FIGREF4 , a word lattice is a directed graph INLINEFORM0 , where INLINEFORM1 represents a node set and INLINEFORM2 represents a edge set. For a sentence in Chinese, which is a sequence of Chinese characters INLINEFORM3 , all of its possible substrings that can be considered as words are treated as vertexes, i.e. INLINEFORM4 .",
        "type": "Document"
      },
      {
        "id": "11e73d7b-a5d7-4fca-8358-4da344763ce1",
        "metadata": {
          "vector_store_key": "1902.09087-4",
          "chunk_id": 16,
          "document_id": "1902.09087",
          "start_idx": 8631,
          "end_idx": 9380
        },
        "page_content": "Therefore, it is not necessary to make explicit decisions regarding specific word segmentations, but just embed all possible information into the lattice and take them to the next CNN layers. The inherent graph structure of a word lattice allows all possible words represented explicitly, no matter the overlapping and nesting cases, and all of them can contribute directly to the sentence representations. As we mentioned in previous section, we can not directly apply standard CNNs to take word lattice as input, since there could be multiple feature vectors produced for a given word. Inspired by previous lattice LSTM models BIBREF10 , BIBREF11 , here we propose a lattice based CNN layers to allow standard CNNs to work over word lattice input.",
        "type": "Document"
      },
      {
        "id": "1b6456bd-8373-480a-9702-c7e5d12c91de",
        "metadata": {
          "vector_store_key": "1902.09087-6",
          "chunk_id": 22,
          "document_id": "1902.09087",
          "start_idx": 11980,
          "end_idx": 12663
        },
        "page_content": "Finally, given a sentence that has been constructed into a word-lattice form, for each node in the lattice, an LCN layer will produce one feature vector similar to original CNNs, which makes it easier to stack multiple LCN layers to obtain more abstract feature representations. Our experiments are designed to answer: (1) whether multi-granularity information in word lattice helps in matching based QA tasks, (2) whether LCNs capture the multi-granularity information through lattice well, and (3) how to balance the noisy and informative words introduced by word lattice. We conduct experiments on two Chinese question answering datasets from NLPCC-2016 evaluation task BIBREF13 .",
        "type": "Document"
      },
      {
        "id": "9549fd45-4abf-4b9b-9589-61c74edbf57f",
        "metadata": {
          "vector_store_key": "1902.09087-6",
          "chunk_id": 54,
          "document_id": "1902.09087",
          "start_idx": 30394,
          "end_idx": 31378
        },
        "page_content": "Rather than relying on a word sequence only, our model takes word lattice as input. By performing CNNs over multiple n-gram context to exploit multi-granularity information, LCNs can relieve the word mismatch challenges. Thorough experiments show that our model can better explore the word lattice via convolutional operations and rich context-aware pooling, thus outperforms the state-of-the-art models and competitive baselines by a large margin. Further analyses exhibit that lattice input takes advantages of word and character level information, and the vocabulary based lattice constructor outperforms the strategies that combine characters and different word segmentations together. This work is supported by Natural Science Foundation of China (Grant No. 61672057, 61672058, 61872294); the UK Engineering and Physical Sciences Research Council under grants EP/M01567X/1 (SANDeRs) and EP/M015793/1 (DIVIDEND); and the Royal Society International Collaboration Grant (IE161012).",
        "type": "Document"
      },
      {
        "id": "43ca50a6-faed-4932-9f80-bc5da9f2364d",
        "metadata": {
          "vector_store_key": "1902.09087-6",
          "chunk_id": 45,
          "document_id": "1902.09087",
          "start_idx": 24916,
          "end_idx": 25726
        },
        "page_content": "This demonstrates that LCNs can effectively take advantages of different granularities, and the combination will not be harmful even when the matching clues present in extreme cases. How to Create Word Lattice In previous experiments, we construct word lattice via an existing lookup vocabulary, which will introduce some noisy words inevitably. Here we construct from various word segmentations with different strategies to investigate the balance between the noisy words and additional information introduced by word lattice. We only use the DBQA dataset because word lattices here are more complex, so the construction strategies have more influence. Pilot experiments show that word lattices constructed based on character sequence perform better, so the strategies in Table TABREF33 are based on CNN-char.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How better is proposed method than baselines perpexity wise?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "c938d47c-e132-4294-bd51-25295fb776d2",
        "metadata": {
          "vector_store_key": "1603.07044-3",
          "chunk_id": 28,
          "document_id": "1603.07044",
          "start_idx": 15079,
          "end_idx": 15744
        },
        "page_content": "Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited training data is given, both feature-rich based system and our system are worse than the IR system. For task C, our system also got comparable results with the feature-rich based system. If we do a simple system combination (average the rank score) between our system and the IR system, the combined system will give large gains on tasks B and C.",
        "type": "Document"
      },
      {
        "id": "be4cf067-7451-4f52-af7c-dcef6c444404",
        "metadata": {
          "vector_store_key": "1806.00722-3",
          "chunk_id": 31,
          "document_id": "1806.00722",
          "start_idx": 17679,
          "end_idx": 18386
        },
        "page_content": "While adding dense connections influences the per-iteration training slightly (8.1% reduction of speed), it uses many fewer epochs, and achieves a better BLEU score. In terms of training time, DenseNMT uses 29.3%(before finetuning)/22.9%(total) less time than the baseline. Table TABREF32 shows the results for De-En, Tr-En, Tr-En-morph datasets, where the best accuracy for models with the same depth and of similar sizes are marked in boldface. In almost all genres, DenseNMT models are significantly better than the baselines. With embedding size 256, where all models achieve their best scores, DenseNMT outperforms baselines by 0.7-1.0 BLEU on De-En, 0.5-1.3 BLEU on Tr-En, 0.8-1.5 BLEU on Tr-En-morph.",
        "type": "Document"
      },
      {
        "id": "02bef31f-a2b5-4b89-8063-62dc0b0f91eb",
        "metadata": {
          "vector_store_key": "1911.01799-4",
          "chunk_id": 19,
          "document_id": "1911.01799",
          "start_idx": 11188,
          "end_idx": 11946
        },
        "page_content": "Two state-of-the-art baseline systems were built following the Kaldi SITW recipe BIBREF22: an i-vector system BIBREF3 and an x-vector system BIBREF10. For the i-vector system, the acoustic feature involved 24-dimensional MFCCs plus the log energy, augmented by the first- and second-order derivatives. We also applied the cepstral mean normalization (CMN) and the energy-based voice active detection (VAD). The universal background model (UBM) consisted of $2,048$ Gaussian components, and the dimensionality of the i-vector space was 400. LDA was applied to reduce the dimensionality of the i-vectors to 150. The PLDA model was used for scoring BIBREF4. For the x-vector system, the feature-learning component was a 5-layer time-delay neural network (TDNN).",
        "type": "Document"
      },
      {
        "id": "b894763e-cda2-4d73-ba78-3a6268b85113",
        "metadata": {
          "vector_store_key": "1902.09087-2",
          "chunk_id": 29,
          "document_id": "1902.09087",
          "start_idx": 15885,
          "end_idx": 16714
        },
        "page_content": "Our second set of baselines combines different word segmentations. Specifically, we concatenate the sentence embeddings from different segment results, which gives four different word+word models: jieba+PKU, PKU+CTB, CTB+jieba, and PKU+CTB+jieba. Inspired by previous works BIBREF2 , BIBREF3 , we also concatenate word and character embeddings at the input level. Specially, when the basic sequence is in word level, each word may be constructed by multiple characters through a pooling operation (Word+Char). Our pilot experiments show that average-pooling is the best for DBQA while max-pooling after a dense layer is the best for KBQA. When the basic sequence is in character level, we simply concatenate the character embedding with its corresponding word embedding (Char+Word), since each character belongs to one word only.",
        "type": "Document"
      },
      {
        "id": "395c830b-b60b-4194-a85e-cad92ef11ee6",
        "metadata": {
          "vector_store_key": "1707.03904-0",
          "chunk_id": 36,
          "document_id": "1707.03904",
          "start_idx": 19734,
          "end_idx": 20485
        },
        "page_content": "Naturally, the search accuracy increases as the context size increases, however at the same time reading performance decreases since the task of extracting the answer becomes harder for longer documents. Hence, simply retrieving more documents is not sufficient \u2013 finding the few most relevant ones will allow the reader to work best. In Tables 2 and 3 we compare all baselines when the context size is tuned to maximize the overall accuracy on the validation set. For Quasar-S the best performing baseline is the BiRNN language model, which achieves $33.6\\%$ accuracy. The GA model achieves $48.3\\%$ accuracy on the set of instances for which the answer is in context, however, a search accuracy of only $65\\%$ means its overall performance is lower.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Who manually annotated the semantic roles for the set of learner texts?",
    "llm_answer": "Two senior students majoring in Applied Linguistics.\n",
    "context": [
      {
        "id": "d4531a97-6940-41be-9cc9-f7c12cd940e3",
        "metadata": {
          "vector_store_key": "1808.09409-3",
          "chunk_id": 12,
          "document_id": "1808.09409",
          "start_idx": 6868,
          "end_idx": 7738
        },
        "page_content": "We take the mother languages of the learners into consideration, which have a great impact on grammatical errors and hence automatic semantic analysis. We hope that four selected mother tongues guarantee a good coverage of typologies. The annotated corpus can be used both for linguistic investigation and as test data for NLP systems. Semantic role labeling (SRL) is the process of assigning semantic roles to constituents or their head words in a sentence according to their relationship to the predicates expressed in the sentence. Typical semantic roles can be divided into core arguments and adjuncts. The core arguments include Agent, Patient, Source, Goal, etc, while the adjuncts include Location, Time, Manner, Cause, etc. To create a standard semantic-role-labeled corpus for learner Chinese, we first annotate a 50-sentence trial set for each native language.",
        "type": "Document"
      },
      {
        "id": "441d2cc4-330b-4e0e-974e-b151e4ba6492",
        "metadata": {
          "vector_store_key": "1808.09409-3",
          "chunk_id": 13,
          "document_id": "1808.09409",
          "start_idx": 7738,
          "end_idx": 8491
        },
        "page_content": "The core arguments include Agent, Patient, Source, Goal, etc, while the adjuncts include Location, Time, Manner, Cause, etc. To create a standard semantic-role-labeled corpus for learner Chinese, we first annotate a 50-sentence trial set for each native language. Two senior students majoring in Applied Linguistics conducted the annotation. Based on a total of 400 sentences, we adjudicate an initial gold standard, adapting and refining CPB specification as our annotation heuristics. Then the two annotators proceed to annotate a 100-sentence set for each language independently. It is on these larger sets that we report the inter-annotator agreement. In the final stage, we also produce an adjudicated gold standard for all 600 annotated sentences.",
        "type": "Document"
      },
      {
        "id": "8ef230e5-f58b-4155-8cc6-8a7441860b78",
        "metadata": {
          "vector_store_key": "1808.09409-3",
          "chunk_id": 18,
          "document_id": "1808.09409",
          "start_idx": 10114,
          "end_idx": 10831
        },
        "page_content": "For unnecessary roles in L2 caused by mistakes of verb subcategorization (see examples in Figure FIGREF30 ), we would leave those roles unlabeled. Table TABREF10 further reports agreements on each argument (AN) and adjunct (AM) in detail, according to which the high scores are attributed to the high agreement on arguments (AN). The labels of A3 and A4 have no disagreement since they are sparse in CPB and are usually used to label specific semantic roles that have little ambiguity. We also conducted in-depth analysis on inter-annotator disagreement. For further details, please refer to duan2018argument. The work on SRL has included a broad spectrum of machine learning and deep learning approaches to the task.",
        "type": "Document"
      },
      {
        "id": "de132a16-dd81-43c6-82c7-6c6d0d157a32",
        "metadata": {
          "vector_store_key": "1808.09409-4",
          "chunk_id": 44,
          "document_id": "1808.09409",
          "start_idx": 24401,
          "end_idx": 25092
        },
        "page_content": "Although there have been some initial studies on defining annotation specification as well as corpora for syntactic analysis, there is almost no work on semantic parsing for interlanguages. This paper discusses this topic, taking Semantic Role Labeling as a case task and learner Chinese as a case language. We reveal three unknown facts that are important towards a deeper analysis of learner languages: (1) the robustness of language comprehension for interlanguage, (2) the weakness of applying L1-sentence-trained systems to process learner texts, and (3) the significance of syntactic parsing and L2-L1 parallel data in building more generalizable SRL models that transfer better to L2.",
        "type": "Document"
      },
      {
        "id": "12a5b32e-2ac7-4eb9-8679-501c9c902f20",
        "metadata": {
          "vector_store_key": "1911.03243-2",
          "chunk_id": 0,
          "document_id": "1911.03243",
          "start_idx": 0,
          "end_idx": 800
        },
        "page_content": "Semantic Role Labeling (SRL) provides explicit annotation of predicate-argument relations, which have been found useful in various downstream tasks BIBREF0, BIBREF1, BIBREF2, BIBREF3. Question-Answer driven Semantic Role Labeling (QA-SRL) BIBREF4 is an SRL scheme in which roles are captured by natural language questions, while arguments represent their answers, making the annotations intuitive, semantically rich, and easily attainable by laymen. For example, in Table TABREF4, the question Who cut something captures the traditional \u201cagent\u201d role. Previous attempts to annotate QA-SRL initially involved trained annotators BIBREF4 but later resorted to crowdsourcing BIBREF5 to achieve scalability. Naturally, employing crowd workers raises challenges when annotating semantic structures like SRL.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they obtain region descriptions and object annotations?",
    "llm_answer": "From the Visual Genome dataset.\n",
    "context": [
      {
        "id": "3cf36d26-b877-4484-b1af-b4977aa0b3c6",
        "metadata": {
          "vector_store_key": "1808.00265-1",
          "chunk_id": 25,
          "document_id": "1808.00265",
          "start_idx": 14705,
          "end_idx": 15383
        },
        "page_content": "Furthermore, for each of its more than 100K images, VG also provides region and object annotations by means of bounding boxes. In terms of visual grounding, these region and object annotations provide complementary information. As an example, as shown in Figure FIGREF3 , for questions related to interaction between objects, region annotations result highly relevant. In contrast, for questions related to properties of specific objects, object annotations result more valuable. Consequently, in this section we present a method to automatically select region and object annotations from VG that can be used as labels to implement visual grounding as an auxiliary task for VQA.",
        "type": "Document"
      },
      {
        "id": "f42892a8-9db8-44ed-b34f-84f6c7406f54",
        "metadata": {
          "vector_store_key": "1808.00265-1",
          "chunk_id": 26,
          "document_id": "1808.00265",
          "start_idx": 15059,
          "end_idx": 15762
        },
        "page_content": "Consequently, in this section we present a method to automatically select region and object annotations from VG that can be used as labels to implement visual grounding as an auxiliary task for VQA. For region annotations, we propose a simple heuristic to mine visual groundings: for each INLINEFORM0 we enumerate all the region descriptions of INLINEFORM1 and pick the description INLINEFORM2 that has the most (at least two) overlapped informative words with INLINEFORM3 and INLINEFORM4 . Informative words are all nouns and verbs, where two informative words are matched if at least one of the following conditions is met: (1) Their raw text as they appear in INLINEFORM5 or INLINEFORM6 are the same;",
        "type": "Document"
      },
      {
        "id": "a14a861a-c077-4341-887c-ecc7604c69d2",
        "metadata": {
          "vector_store_key": "1809.05752-2",
          "chunk_id": 18,
          "document_id": "1809.05752",
          "start_idx": 10576,
          "end_idx": 11361
        },
        "page_content": "In total, we constructed a corpus of roughly 100,000 paragraphs consisting of 7,000,000 tokens for training our model. In order to evaluate our models, we annotated 1,654 paragraphs selected from the 240,000 paragraphs extracted from Meditech with the clinically relevant domains described in Table TABREF3 . The annotation task was completed by three licensed clinicians. All paragraphs were removed from the surrounding EHR context to ensure annotators were not influenced by the additional contextual information. Our domain classification models consider each paragraph independently and thus we designed the annotation task to mirror the information available to the models. The annotators were instructed to label each paragraph with one or more of the seven risk factor domains.",
        "type": "Document"
      },
      {
        "id": "cb264c64-afef-49c9-ac68-c0417ccd2d67",
        "metadata": {
          "vector_store_key": "1808.00265-0",
          "chunk_id": 7,
          "document_id": "1808.00265",
          "start_idx": 4130,
          "end_idx": 5176
        },
        "page_content": "We accomplish this by leveraging region descriptions and object annotations available in the Visual Genome dataset, and using these to automatically construct attention maps that can be used for attention supervision, instead of requiring human annotators to manually provide grounding labels. Our framework achieves competitive state-of-the-art VQA performance, while generating visual groundings that outperform other algorithms that use human annotated attention during training. The contributions of this paper are: (1) we introduce a mechanism to automatically obtain meaningful attention supervision from both region descriptions and object annotations in the Visual Genome dataset; (2) we show that by using the prediction of region and object label attention maps as auxiliary tasks in a VQA application, it is possible to obtain more interpretable intermediate representations. (3) we experimentally demonstrate state-of-the-art performances in VQA benchmarks as well as visual grounding that closely matches human attention annotations.",
        "type": "Document"
      },
      {
        "id": "1ff5bf3c-ab7b-41b1-b097-d65f35d06d5e",
        "metadata": {
          "vector_store_key": "1808.00265-1",
          "chunk_id": 27,
          "document_id": "1808.00265",
          "start_idx": 15762,
          "end_idx": 16499
        },
        "page_content": "Informative words are all nouns and verbs, where two informative words are matched if at least one of the following conditions is met: (1) Their raw text as they appear in INLINEFORM5 or INLINEFORM6 are the same; (2) Their lemmatizations (using NLTK BIBREF22 ) are the same; (3) Their synsets in WordNet BIBREF23 are the same; (4) Their aliases (provided from VG) are the same. We refer to the resulting labels as region-level groundings. Figure FIGREF3 (a) illustrates an example of a region-level grounding. In terms of object annotations, for each image in a INLINEFORM0 triplet we select the bounding box of an object as a valid grounding label, if the object name matches one of the informative nouns in INLINEFORM1 or INLINEFORM2 .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which training dataset allowed for the best generalization to benchmark sets?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "39d04bfb-c08e-485b-8a25-34aba9111af7",
        "metadata": {
          "vector_store_key": "1812.06705-4",
          "chunk_id": 26,
          "document_id": "1812.06705",
          "start_idx": 15443,
          "end_idx": 16142
        },
        "page_content": "When the task-specific dataset is with more than two different labels, we should re-train a label size compatible label embeddings layer instead of directly fine-tuning the pre-trained one. Six benchmark classification datasets are listed in table 1 . Following Kim BIBREF24 , for a dataset without validation data, we use 10% of its training set for the validation set. Summary statistics of six classification datasets are shown in table 1. SST BIBREF25 SST (Stanford Sentiment Treebank) is a dataset for sentiment classification on movie reviews, which are annotated with five labels (SST5: very positive, positive, neutral, negative, or very negative) or two labels (SST2: positive or negative).",
        "type": "Document"
      },
      {
        "id": "9eadb857-6f48-4b56-aa48-bf2ff9221588",
        "metadata": {
          "vector_store_key": "1810.00663-5",
          "chunk_id": 32,
          "document_id": "1810.00663",
          "start_idx": 17939,
          "end_idx": 18652
        },
        "page_content": "To the best of our knowledge, this is the first benchmark for comparing translation models in the context of behavioral robot navigation. As shown in Table TABREF16 , the dataset consists of 8066 pairs of free-form natural language instructions and navigation plans for training. This training data was collected from 88 unique simulated environments, totaling 6064 distinct navigation plans (2002 plans have two different navigation instructions each; the rest has one). The dataset contains two test set variants: While the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous.",
        "type": "Document"
      },
      {
        "id": "1bffa3f8-bbbc-48c1-81fe-55fb01d9b256",
        "metadata": {
          "vector_store_key": "1910.11769-3",
          "chunk_id": 12,
          "document_id": "1910.11769",
          "start_idx": 7313,
          "end_idx": 8027
        },
        "page_content": "We masked out named entities with entity-type specific placeholders to reduce the chance of benchmark models utilizing named entities as a basis for classification. Benchmark results are shown in Table TABREF17. The dataset is approximately balanced after discarding the Surprise and Disgust classes. We report the average micro-F1 scores, with 5-fold cross validation for each technique. We provide a brief overview of each benchmark experiment below. Among all of the benchmarks, Bidirectional Encoder Representations from Transformers (BERT) BIBREF11 achieved the best performance with a 0.604 micro-F1 score. Overall, we observed that deep-learning based techniques performed better than lexical based methods.",
        "type": "Document"
      },
      {
        "id": "36adf7ba-6ece-4ad5-84b3-0a38b137a4fc",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 16,
          "document_id": "1912.13109",
          "start_idx": 8070,
          "end_idx": 8831
        },
        "page_content": "The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we employed various operational techniques for optimizing on the learning process. These processes and its implementation details will be discussed later but they were learning rate decay, early stopping, model checkpointing and reducing learning rate on plateau.",
        "type": "Document"
      },
      {
        "id": "eac31fee-fbdc-4a80-be44-f2145c81562a",
        "metadata": {
          "vector_store_key": "1910.09399-3",
          "chunk_id": 106,
          "document_id": "1910.09399",
          "start_idx": 56647,
          "end_idx": 57443
        },
        "page_content": "In order to synthesize images from text descriptions, many frameworks have taken a minimalistic approach by creating small and background-less images BIBREF73. In most cases, the experiments were conducted on simple datasets, initially containing images of birds and flowers. BIBREF8 contributed to these data sets by adding corresponding natural language text descriptions to subsets of the CUB, MSCOCO, and Oxford-102 datasets, which facilitated the work on text-to-image synthesis for several papers released more recently. While most deep learning algorithms use MNIST BIBREF74 dataset as the benchmark, there are three main datasets that are commonly used for evaluation of proposed GAN models for text-to-image synthesis: CUB BIBREF75, Oxford BIBREF76, COCO BIBREF77, and CIFAR-10 BIBREF78.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which models were compared?",
    "llm_answer": "RF model (with reduced features), RF model (full vocabulary), aggregated forecaster (Agg), and neural network methods (reduced vocabulary).  BERT, ESIM + ELMo, and MEED are also mentioned as specific models.\n",
    "context": [
      {
        "id": "52a5c7b9-197c-4c52-9a22-93a7f3f1bb7a",
        "metadata": {
          "vector_store_key": "1910.12618-2",
          "chunk_id": 45,
          "document_id": "1910.12618",
          "start_idx": 24114,
          "end_idx": 24750
        },
        "page_content": "In order to tone it down and to increase the consistency of our results, the different models are run $B=10$ times. The results presented hereafter correspond to the average and standard-deviation on those runs. The RF model denoted as \"sel\" is the one with the reduced number of features, whereas the other RF uses the full vocabulary. We also considered an aggregated forecaster (abridged Agg), consisting of the average of the two best individual ones in terms of RMSE. All the neural network methods have a reduced vocabulary size $V^*$. The results for the French and UK data are respectively given by tables TABREF26 and TABREF27.",
        "type": "Document"
      },
      {
        "id": "c796705d-58ac-4ec1-b59f-a845de45b611",
        "metadata": {
          "vector_store_key": "1810.09774-3",
          "chunk_id": 24,
          "document_id": "1810.09774",
          "start_idx": 12086,
          "end_idx": 12710
        },
        "page_content": "The most surprising result was that the accuracy of all models drops significantly even when the models were trained on MultiNLI and tested on SNLI (3.6-11.1 points). This is surprising as both of these datasets have been constructed with a similar data collection method using the same definition of entailment, contradiction and neutral. The sentences included in SNLI are also much simpler compared to those in MultiNLI, as they are taken from the Flickr image captions. This might also explain why the difference in accuracy for all of the six models is lowest when the models are trained on MultiNLI and tested on SNLI.",
        "type": "Document"
      },
      {
        "id": "5cc44f3d-1c70-4272-8659-7e6139f2cfe3",
        "metadata": {
          "vector_store_key": "1908.07816-0",
          "chunk_id": 36,
          "document_id": "1908.07816",
          "start_idx": 20250,
          "end_idx": 20944
        },
        "page_content": "Table TABREF34 gives the perplexity scores obtained by the three models on the two validation sets and the test set. As shown in the table, MEED achieves the lowest perplexity score on all three sets. We also conducted t-test on the perplexity obtained, and results show significant improvements (with $p$-value $<0.05$). Table TABREF34, TABREF35 and TABREF35 summarize the human evaluation results on the responses' grammatical correctness, contextual coherence, and emotional appropriateness, respectively. In the tables, we give the percentage of votes each model received for the three scores, the average score obtained with improvements over S2S, and the agreement score among the raters.",
        "type": "Document"
      },
      {
        "id": "7ed66adc-6aa2-419e-a1d9-cba0e620bb13",
        "metadata": {
          "vector_store_key": "1810.09774-3",
          "chunk_id": 25,
          "document_id": "1810.09774",
          "start_idx": 12710,
          "end_idx": 13514
        },
        "page_content": "This might also explain why the difference in accuracy for all of the six models is lowest when the models are trained on MultiNLI and tested on SNLI. It is also very surprising that the model with the biggest difference in accuracy was ESIM + ELMo which includes a pre-trained ELMo language model. BERT performed significantly better than the other models in this experiment having an accuracy of 80.4% and only 3.6 point difference in accuracy. The poor performance of most of the models with the MultiNLI-SNLI dataset pair is also very surprising given that neural network models do not seem to suffer a lot from introduction of new genres to the test set which were not included in the training set, as can be seen from the small difference in test accuracies for the matched and mismatched test sets",
        "type": "Document"
      },
      {
        "id": "8b681439-d4eb-401b-8199-4cc0d2c2d0ed",
        "metadata": {
          "vector_store_key": "1909.02764-4",
          "chunk_id": 37,
          "document_id": "1909.02764",
          "start_idx": 20568,
          "end_idx": 21193
        },
        "page_content": "This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set. These results are mostly in line with Bostan2018. Now we analyze how well the models trained in Experiment 1 perform when applied to our data set. The results are shown in column \u201cSimple\u201d in Table TABREF19. We observe a clear drop in performance, with an average of F$_1$=48 %. The best performing model is again the one trained on TEC, en par with the one trained on the Figure8 data. The model trained on ISEAR performs second best in Experiment 1, it performs worst in Experiment 2.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is private dashboard?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "083d66f6-5063-4722-b8ad-c981b745ff2b",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 19,
          "document_id": "1909.02764",
          "start_idx": 10873,
          "end_idx": 11564
        },
        "page_content": "We collect data from video, speech and biosignals (Empatica E4 to record heart rate, electrodermal activity, skin temperature, not further used in this paper) and questionnaires. Two RGB cameras are fixed in the vehicle to capture the drivers face, one at the sun shield above the drivers seat and one in the middle of the dashboard. A microphone is placed on the center console. One experimenter sits next to the driver, the other behind the simulator. The virtual agent accompanying the drive is realized as Wizard-of-Oz prototype which enables the experimenter to manually trigger prerecorded voice samples playing trough the in-car speakers and to bring new content to the center screen.",
        "type": "Document"
      },
      {
        "id": "6b114808-7052-4ad2-9173-e999eb4c1256",
        "metadata": {
          "vector_store_key": "1910.02789-2",
          "chunk_id": 55,
          "document_id": "1910.02789",
          "start_idx": 30474,
          "end_idx": 30631
        },
        "page_content": "The critic network is a regression model with a single output representing the state's value. Reward plots for the PPO agent can be found in Figure FIGREF47.",
        "type": "Document"
      },
      {
        "id": "8fb5173e-bf83-4864-9f26-5b3982c24571",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 20,
          "document_id": "1909.02764",
          "start_idx": 11149,
          "end_idx": 11888
        },
        "page_content": "The virtual agent accompanying the drive is realized as Wizard-of-Oz prototype which enables the experimenter to manually trigger prerecorded voice samples playing trough the in-car speakers and to bring new content to the center screen. Figure FIGREF4 shows the driving simulator. The experimental setting is comparable to an everyday driving task. Participants are told that the goal of the study is to evaluate and to improve an intelligent driving assistant. To increase the probability of emotions to arise, participants are instructed to reach the destination of the route as fast as possible while following traffic rules and speed limits. They are informed that the time needed for the task would be compared to other participants.",
        "type": "Document"
      },
      {
        "id": "9b52276e-b60c-405d-b58f-bb8c4cdbebc7",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 23,
          "document_id": "1909.02764",
          "start_idx": 12845,
          "end_idx": 13619
        },
        "page_content": "Pretending to be aware of the current situation, e. g., to recognize unusual driving behavior such as strong braking, the agent asks the driver to explain his subjective perception of these events in detail. Additionally, we trigger two more interactions with the intelligent agent at the beginning and at the end of the drive, where participants are asked to describe their mood and thoughts regarding the (upcoming) drive. This results in five interactions between the driver and the virtual agent. Furthermore, the co-driver asks three different questions during sessions with light traffic and low cognitive demand (Driver-Co-Driver Interactions, D\u2013Co). These questions are more general and non-traffic-related and aim at triggering the participants' memory and fantasy.",
        "type": "Document"
      },
      {
        "id": "4feec127-a317-48cf-b429-3baef4f76603",
        "metadata": {
          "vector_store_key": "1910.02789-8",
          "chunk_id": 44,
          "document_id": "1910.02789",
          "start_idx": 24265,
          "end_idx": 24975
        },
        "page_content": "The API offers the user the ability to run game instances, query the game state, and execute actions. The original purpose of VizDoom is to provide a research platform for vision based reinforcement learning. Thus, a natural language representation for the game was needed to be implemented. ViZDoom emulates the \"Doom\" game and enables us to access data within a certain frame using Python dictionaries. This makes it possible to extract valuable data including player health, ammo, enemy locations etc. Each game frame contains \"labels\", which contain data on visible objects in the game (the player, enemies, medkits, etc). We used \"Doom Builder\" in order to edit some of the scenarios and design a new one.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is public dashboard?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "6b114808-7052-4ad2-9173-e999eb4c1256",
        "metadata": {
          "vector_store_key": "1910.02789-2",
          "chunk_id": 55,
          "document_id": "1910.02789",
          "start_idx": 30474,
          "end_idx": 30631
        },
        "page_content": "The critic network is a regression model with a single output representing the state's value. Reward plots for the PPO agent can be found in Figure FIGREF47.",
        "type": "Document"
      },
      {
        "id": "4feec127-a317-48cf-b429-3baef4f76603",
        "metadata": {
          "vector_store_key": "1910.02789-8",
          "chunk_id": 44,
          "document_id": "1910.02789",
          "start_idx": 24265,
          "end_idx": 24975
        },
        "page_content": "The API offers the user the ability to run game instances, query the game state, and execute actions. The original purpose of VizDoom is to provide a research platform for vision based reinforcement learning. Thus, a natural language representation for the game was needed to be implemented. ViZDoom emulates the \"Doom\" game and enables us to access data within a certain frame using Python dictionaries. This makes it possible to extract valuable data including player health, ammo, enemy locations etc. Each game frame contains \"labels\", which contain data on visible objects in the game (the player, enemies, medkits, etc). We used \"Doom Builder\" in order to edit some of the scenarios and design a new one.",
        "type": "Document"
      },
      {
        "id": "e900003c-e4a4-4760-af29-1415e5bd0e01",
        "metadata": {
          "vector_store_key": "1708.05873-0",
          "chunk_id": 35,
          "document_id": "1708.05873",
          "start_idx": 20206,
          "end_idx": 20343
        },
        "page_content": "The paper more broadly demonstrates how text analytic approaches can help us to better understand different aspects of global governance.",
        "type": "Document"
      },
      {
        "id": "083d66f6-5063-4722-b8ad-c981b745ff2b",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 19,
          "document_id": "1909.02764",
          "start_idx": 10873,
          "end_idx": 11564
        },
        "page_content": "We collect data from video, speech and biosignals (Empatica E4 to record heart rate, electrodermal activity, skin temperature, not further used in this paper) and questionnaires. Two RGB cameras are fixed in the vehicle to capture the drivers face, one at the sun shield above the drivers seat and one in the middle of the dashboard. A microphone is placed on the center console. One experimenter sits next to the driver, the other behind the simulator. The virtual agent accompanying the drive is realized as Wizard-of-Oz prototype which enables the experimenter to manually trigger prerecorded voice samples playing trough the in-car speakers and to bring new content to the center screen.",
        "type": "Document"
      },
      {
        "id": "3743d36b-aa6a-42eb-a7c1-9a174a37b6b9",
        "metadata": {
          "vector_store_key": "1710.01492-8",
          "chunk_id": 1,
          "document_id": "1710.01492",
          "start_idx": 661,
          "end_idx": 1386
        },
        "page_content": "Typically, discrete labels such as positive, negative, neutral, and objective are used for this purpose, but it is also possible to use labels on an ordinal scale, or even continuous numerical values. Internet and the proliferation of smart mobile devices have changed the way information is created, shared, and spreads, e.g., microblogs such as Twitter, weblogs such as LiveJournal, social networks such as Facebook, and instant messengers such as Skype and WhatsApp are now commonly used to share thoughts and opinions about anything in the surrounding world. This has resulted in the proliferation of social media content, thus creating new opportunities to study public opinion at a scale that was never possible before.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What dataset do they use?",
    "llm_answer": "MS COCO, Bing, Flickr, avsd, Charades, CUB, MSCOCO, Oxford-102, MNIST, CIFAR-10, AIDA-Train, Wikipedia, AIDA-B, AIDA-CoNLL.\n",
    "context": [
      {
        "id": "947a5cf6-1656-4ca3-827b-105c8344c1d6",
        "metadata": {
          "vector_store_key": "1910.11949-3",
          "chunk_id": 16,
          "document_id": "1910.11949",
          "start_idx": 8700,
          "end_idx": 9318
        },
        "page_content": "In particular, we use two types of datasets to train our models: A dataset that maps pictures with questions, and an open-domain conversation dataset. The details of the two datasets are as follows. We use MS COCO, Bing and Flickr datasets from BIBREF26 to train the model that generates questions. These datasets contain natural questions about images with the purpose of knowing more about the picture. As can be seen in the Figure FIGREF8, questions cannot be answered by only looking at the image. Each source contains 5,000 images with 5 questions per image, adding a total of 15,000 images with 75,000 questions.",
        "type": "Document"
      },
      {
        "id": "73a83f60-b2c9-406d-b4cd-5720639e5f06",
        "metadata": {
          "vector_store_key": "1812.07023-4",
          "chunk_id": 18,
          "document_id": "1812.07023",
          "start_idx": 10358,
          "end_idx": 10986
        },
        "page_content": "After 10 such qa turns, the questioner wraps up by writing a summary of the video based on the conversation. Dataset statistics such as the number of dialogues, turns, and words for the avsd dataset are presented in Table TABREF5 . For the initially released prototype dataset, the training set of the avsd dataset corresponds to videos taken from the training set of the Charades dataset while the validation and test sets of the avsd dataset correspond to videos taken from the validation set of the Charades dataset. For the official dataset, training, validation and test sets are drawn from the corresponding Charades sets.",
        "type": "Document"
      },
      {
        "id": "1e2bf238-b0a0-40af-9d8d-e6fea4ad79d0",
        "metadata": {
          "vector_store_key": "1910.09399-3",
          "chunk_id": 105,
          "document_id": "1910.09399",
          "start_idx": 56273,
          "end_idx": 57009
        },
        "page_content": "For example, image data captured from a variety of photo-ready devices, such as smart-phones, and online social media services opened the door to the analysis of large amounts of media datasets BIBREF70. The availability of large media datasets allow new frameworks and algorithms to be proposed and tested on real-world data. A summary of some reviewed methods and benchmark datasets used for validation is reported in Table TABREF43. In addition, the performance of different GANs with respect to the benchmark datasets and performance metrics is reported in Table TABREF48. In order to synthesize images from text descriptions, many frameworks have taken a minimalistic approach by creating small and background-less images BIBREF73.",
        "type": "Document"
      },
      {
        "id": "eac31fee-fbdc-4a80-be44-f2145c81562a",
        "metadata": {
          "vector_store_key": "1910.09399-3",
          "chunk_id": 106,
          "document_id": "1910.09399",
          "start_idx": 56647,
          "end_idx": 57443
        },
        "page_content": "In order to synthesize images from text descriptions, many frameworks have taken a minimalistic approach by creating small and background-less images BIBREF73. In most cases, the experiments were conducted on simple datasets, initially containing images of birds and flowers. BIBREF8 contributed to these data sets by adding corresponding natural language text descriptions to subsets of the CUB, MSCOCO, and Oxford-102 datasets, which facilitated the work on text-to-image synthesis for several papers released more recently. While most deep learning algorithms use MNIST BIBREF74 dataset as the benchmark, there are three main datasets that are commonly used for evaluation of proposed GAN models for text-to-image synthesis: CUB BIBREF75, Oxford BIBREF76, COCO BIBREF77, and CIFAR-10 BIBREF78.",
        "type": "Document"
      },
      {
        "id": "458b51fc-c0d4-482d-90b3-d4cefed66ac3",
        "metadata": {
          "vector_store_key": "1902.00330-3",
          "chunk_id": 37,
          "document_id": "1902.00330",
          "start_idx": 20853,
          "end_idx": 21565
        },
        "page_content": "We implement our models in Tensorflow and run experiments on 4 Tesla V100 GPU. We conduct experiments on several different types of public datasets including news and encyclopedia corpus. The training set is AIDA-Train and Wikipedia datasets, where AIDA-Train contains 18448 mentions and Wikipedia contains 25995 mentions. In order to compare with the previous methods, we evaluate our model on AIDA-B and other datasets. These datasets are well-known and have been used for the evaluation of most entity linking systems. The statistics of the datasets are shown in Table 1. AIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Do the use word embeddings alone or they replace some previous features of the model with word embeddings?",
    "llm_answer": "Word embeddings replace some previous features of the model with word embeddings.\n",
    "context": [
      {
        "id": "5603f527-cf97-47b9-8799-eb636cb66ef2",
        "metadata": {
          "vector_store_key": "1703.04617-0",
          "chunk_id": 8,
          "document_id": "1703.04617",
          "start_idx": 4608,
          "end_idx": 5396
        },
        "page_content": "And we use the pre-trained 300-D GloVe vectors BIBREF20 (see the experiment section for details) to initialize our word-level embedding. Each word is therefore represented as the concatenation of the character-composition vector and word-level embedding. This is performed on both questions and documents, resulting in two matrices: the $\\mathbf {Q}^e \\in \\mathbb {R} ^{N\\times d_w}$ for a question and the $\\mathbf {D}^e \\in \\mathbb {R} ^{M\\times d_w}$ for a document, where $N$ is the question length (number of word tokens), $M$ is the document length, and $d_w$ is the embedding dimensionality. The above word representation focuses on representing individual words, and an input encoder here employs recurrent neural networks to obtain the representation of a word under its context.",
        "type": "Document"
      },
      {
        "id": "54a85d5b-bea4-4cae-bc75-6481eeb97d44",
        "metadata": {
          "vector_store_key": "1911.12569-0",
          "chunk_id": 22,
          "document_id": "1911.12569",
          "start_idx": 13375,
          "end_idx": 14103
        },
        "page_content": "This fact is established by our evaluation results as the model performs better when the DT expansion and primary attentions are a part of the final multi-task system. Word embeddings represent words in a low-dimensional numerical form. They are useful for solving many NLP problems. We use the pre-trained 300 dimensional Google Word2Vec BIBREF32 embeddings. The word embedding for each word in the sentence is fed to the BiLSTM network to get the current hidden state. Moreover, the primary attention mechanism is also applied to the word embeddings of the candidate terms for the current word. In this section we present the details of the datasets used for the experiments, results that we obtain and the necessary analysis.",
        "type": "Document"
      },
      {
        "id": "c80b83af-2365-421f-83c6-d874d5bcda17",
        "metadata": {
          "vector_store_key": "1706.08032-0",
          "chunk_id": 15,
          "document_id": "1706.08032",
          "start_idx": 8929,
          "end_idx": 9809
        },
        "page_content": "To construct embedding inputs for our model, we use a fixed-sized word vocabulary INLINEFORM0 and a fixed-sized character vocabulary INLINEFORM1 . Given a word INLINEFORM2 is composed from characters INLINEFORM3 , the character-level embeddings are encoded by column vectors INLINEFORM4 in the embedding matrix INLINEFORM5 , where INLINEFORM6 is the size of the character vocabulary. For word-level embedding INLINEFORM7 , we use a pre-trained word-level embedding with dimension 200 or 300. A pre-trained word-level embedding can capture the syntactic and semantic information of words BIBREF17 . We build every word INLINEFORM8 into an embedding INLINEFORM9 which is constructed by two sub-vectors: the word-level embedding INLINEFORM10 and the character fixed-size feature vector INLINEFORM11 of INLINEFORM12 where INLINEFORM13 is the length of the filter of wide convolutions.",
        "type": "Document"
      },
      {
        "id": "c5aaf07a-4f1e-4fc9-8ac4-9161eb906b33",
        "metadata": {
          "vector_store_key": "1908.07816-0",
          "chunk_id": 29,
          "document_id": "1908.07816",
          "start_idx": 16856,
          "end_idx": 17561
        },
        "page_content": "For all the models, the vocabulary consists of 20,000 most frequent words in the Cornell and DailyDialog datasets, plus three extra tokens: <unk> for words that do not exist in the vocabulary, <go> indicating the begin of an utterance, and <eos> indicating the end of an utterance. Here we summarize the configurations and parameters of our experiments: We set the word embedding size to 256. We initialized the word embeddings in the models with word2vec BIBREF22 vectors first trained on Cornell and then fine-tuned on DailyDialog, consistent with the training procedure of the models. We set the number of hidden units of each RNN to 256, the word-level attention depth to 256, and utterance-level 128.",
        "type": "Document"
      },
      {
        "id": "e033fcf1-2977-4b36-a537-de74c22a51c1",
        "metadata": {
          "vector_store_key": "1910.12618-6",
          "chunk_id": 24,
          "document_id": "1910.12618",
          "start_idx": 13337,
          "end_idx": 13999
        },
        "page_content": "The second representation is a neural word embedding. It consists in representing every word in the corpus by a real-valued vector of dimension $q$. Such models are usually obtained by learning a vector representation from word co-occurrences in a very large corpus (typically hundred thousands of documents, such as Wikipedia articles for example). The two most popular embeddings are probably Google's Word2Vec BIBREF19 and Standford's GloVe BIBREF20. In the former, a neural network is trained to predict a word given its context (continuous bag of word model), whereas in the latter a matrix factorization scheme on the log co-occurences of words is applied.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many natural language explanations are human-written?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "d2421bc0-1446-4b12-86a4-954ce812f817",
        "metadata": {
          "vector_store_key": "1808.09409-4",
          "chunk_id": 10,
          "document_id": "1808.09409",
          "start_idx": 5614,
          "end_idx": 6317
        },
        "page_content": "The final corpus consists of 717,241 learner sentences from writers of 61 different native languages, in which English and Japanese constitute the majority. As for completeness, 82.78% of the Chinese Second Language sentences on Lang-8 are corrected by native human annotators. One sentence gets corrected approximately 1.53 times on average. In this paper, we manually annotate the predicate\u2013argument structures for the 600 L2-L1 pairs as the basis for the semantic analysis of learner Chinese. It is from the above corpus that we carefully select 600 pairs of L2-L1 parallel sentences. We would choose the most appropriate one among multiple versions of corrections and recorrect the L1s if necessary.",
        "type": "Document"
      },
      {
        "id": "578c3628-c6a7-44f1-891e-898451b7f924",
        "metadata": {
          "vector_store_key": "1910.02789-9",
          "chunk_id": 3,
          "document_id": "1910.02789",
          "start_idx": 1974,
          "end_idx": 2775
        },
        "page_content": "Humans use rich natural language to describe and communicate their visual perceptions, feelings, beliefs, strategies, and more. The semantics inherent to natural language carry knowledge and cues of complex types of content, including: events, spatial relations, temporal relations, semantic roles, logical structures, support for inference and entailment, as well as predicates and arguments BIBREF6. The expressive nature of language can thus act as an alternative semantic state representation. Over the past few years, Natural Language Processing (NLP) has shown an acceleration in progress on a wide range of downstream applications ranging from Question Answering BIBREF7, BIBREF8, to Natural Language Inference BIBREF9, BIBREF10, BIBREF11 through Syntactic Parsing BIBREF12, BIBREF13, BIBREF14.",
        "type": "Document"
      },
      {
        "id": "e58d1209-2698-42aa-b50e-678ab448cb50",
        "metadata": {
          "vector_store_key": "1808.09409-4",
          "chunk_id": 43,
          "document_id": "1808.09409",
          "start_idx": 24157,
          "end_idx": 24855
        },
        "page_content": "Around 8.5 \u2013 11.9% of the sentence can be taken as high L1/L2 recall sentences, which serves as a reflection that argument structure is vital for language acquisition and difficult for learners to master, as proposed in vazquez2004learning and shin2010contribution. The threshold ( INLINEFORM0 ) for selecting sentences is set upon the development data. For example, we use additional 156,520 sentences to enhance the Berkeley parser. Statistical models of annotating learner texts are making rapid progress. Although there have been some initial studies on defining annotation specification as well as corpora for syntactic analysis, there is almost no work on semantic parsing for interlanguages.",
        "type": "Document"
      },
      {
        "id": "5f5e769f-a98a-40f6-9b34-3ae4e3a2d6bb",
        "metadata": {
          "vector_store_key": "1808.09409-4",
          "chunk_id": 9,
          "document_id": "1808.09409",
          "start_idx": 5149,
          "end_idx": 5909
        },
        "page_content": "The proficiency levels of the learners are diverse, but most of the learners, according to our judgment, is of intermediate or lower level. Our initial collection consists of 1,108,907 sentence pairs from 135,754 essays. As there is lots of noise in raw sentences, we clean up the data by (1) ruling out redundant content, (2) excluding sentences containing foreign words or Chinese phonetic alphabet by checking the Unicode values, (3) dropping overly simple sentences which may not be informative, and (4) utilizing a rule-based classifier to determine whether to include the sentence into the corpus. The final corpus consists of 717,241 learner sentences from writers of 61 different native languages, in which English and Japanese constitute the majority.",
        "type": "Document"
      },
      {
        "id": "b12d51a4-a467-42ce-ae1e-983e1f8bd890",
        "metadata": {
          "vector_store_key": "1807.03367-1",
          "chunk_id": 53,
          "document_id": "1807.03367",
          "start_idx": 31165,
          "end_idx": 32051
        },
        "page_content": "We also show that MASC outperform no-MASC models for natural language communication. paragraph4 0.1ex plus0.1ex minus.1ex-1em Generated utterances We also investigate generated tourist utterances from conditional language models. Interestingly, we observe that the supervised model (with greedy and beam-search decoding) as well as the policy gradient model leads to an improvement of more than 10 accuracy points over the human utterances. However, their level of accuracy is slightly below the baseline of communicating a single observation, indicating that these models only learn to ground utterances in a single landmark observation. paragraph4 0.1ex plus0.1ex minus.1ex-1em Better grounding of generated utterances We analyze natural language samples in Table TABREF38 , and confirm that, unlike human utterances, the generated utterances are talking about the observed landmarks.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the dataset used as input to the Word2Vec algorithm?",
    "llm_answer": "A dump of Italian Wikipedia (2019.04.01), main categories of Italian Google News (WORLD, NATION, BUSINESS, TECHNOLOGY, ENTERTAINMENT, SPORTS, SCIENCE, HEALTH), and anonymized chats between users and a customer care chatbot (Laila).\n",
    "context": [
      {
        "id": "0dbfec57-8b6f-481a-a2d5-ec7c4af4fce0",
        "metadata": {
          "vector_store_key": "2003.06279-8",
          "chunk_id": 18,
          "document_id": "2003.06279",
          "start_idx": 10865,
          "end_idx": 11531
        },
        "page_content": "For example, taking as reference the word \u201cRobin\u201d, the model decides that \u201cHood\u201d is more likely to follow the reference word than any other word. The vectors are obtained as follows: given the vocabulary (generated from all corpus words), the model trains a neural network with the sentences of the corpus. Then, for a given word, the probabilities that each word follows the reference word are obtained. Once the neural network is trained, the weights of the hidden layer are used as vectors of each corpus word. FastText: this method is another extension of the Word2Vec model BIBREF41. Unlike Word2Vec, FastText represents each word as a bag of character n-grams.",
        "type": "Document"
      },
      {
        "id": "28f11fb4-b611-40ab-9a10-43aaaf2aeb19",
        "metadata": {
          "vector_store_key": "2003.06279-8",
          "chunk_id": 17,
          "document_id": "2003.06279",
          "start_idx": 10493,
          "end_idx": 11114
        },
        "page_content": "The final result is a learning model that oftentimes yields better word vector representations BIBREF40. Word2Vec: this is a predictive model that finds dense vector representations of words using a three-layer neural network with a single hidden layer BIBREF39. It can be defined in a two-fold way: continuous bag-of-words and skip-gram model. In the latter, the model analyzes the words of a set of sentences (or corpus) and attempts to predict the neighbors of such words. For example, taking as reference the word \u201cRobin\u201d, the model decides that \u201cHood\u201d is more likely to follow the reference word than any other word.",
        "type": "Document"
      },
      {
        "id": "c42a1113-78d3-4504-8fbd-e2dd12f9a196",
        "metadata": {
          "vector_store_key": "2001.09332-3",
          "chunk_id": 7,
          "document_id": "2001.09332",
          "start_idx": 3963,
          "end_idx": 4563
        },
        "page_content": "The text is in fact divided into sentences, and for each word of a given sentence a window of words is taken from the right and from the left to define the context. The central word is coupled with each of the words forming the set of pairs for training. Depending on the fact that the central word represents the output or the input in training pairs, the CBOW and Skip-gram models are obtained respectively. Regardless of whether W2V is trained to predict the context or the target word, it is used as a word embedding in a substantially different manner from the one for which it has been trained.",
        "type": "Document"
      },
      {
        "id": "c5aaf07a-4f1e-4fc9-8ac4-9161eb906b33",
        "metadata": {
          "vector_store_key": "1908.07816-0",
          "chunk_id": 29,
          "document_id": "1908.07816",
          "start_idx": 16856,
          "end_idx": 17561
        },
        "page_content": "For all the models, the vocabulary consists of 20,000 most frequent words in the Cornell and DailyDialog datasets, plus three extra tokens: <unk> for words that do not exist in the vocabulary, <go> indicating the begin of an utterance, and <eos> indicating the end of an utterance. Here we summarize the configurations and parameters of our experiments: We set the word embedding size to 256. We initialized the word embeddings in the models with word2vec BIBREF22 vectors first trained on Cornell and then fine-tuned on DailyDialog, consistent with the training procedure of the models. We set the number of hidden units of each RNN to 256, the word-level attention depth to 256, and utterance-level 128.",
        "type": "Document"
      },
      {
        "id": "4e230006-2c4b-4bad-a5ee-0214efd26042",
        "metadata": {
          "vector_store_key": "2001.09332-3",
          "chunk_id": 14,
          "document_id": "2001.09332",
          "start_idx": 6836,
          "end_idx": 7575
        },
        "page_content": "The probability of selecting a negative word to backpropagate depends on its frequency, in particular through the formula: Negative samples are then selected by choosing a sort of \u201cunigram distribution\", so that the most frequent words are also the most often backpropated ones. The dataset needed to train the W2V was obtained using the information extracted from a dump of the Italian Wikipedia (dated 2019.04.01), from the main categories of Italian Google News (WORLD, NATION, BUSINESS, TECHNOLOGY, ENTERTAINMENT, SPORTS, SCIENCE, HEALTH) and from some anonymized chats between users and a customer care chatbot (Laila). The dataset (composed of 2.6 GB of raw text) includes $421\\,829\\,960$ words divided into $17\\,305\\,401$ sentences.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What methodology is used to compensate for limited labelled data?",
    "llm_answer": "Lightly supervised learning, self-training, and label regularization.\n",
    "context": [
      {
        "id": "6e5f8a60-4570-4372-9ceb-97776f851901",
        "metadata": {
          "vector_store_key": "1909.00430-2",
          "chunk_id": 0,
          "document_id": "1909.00430",
          "start_idx": 0,
          "end_idx": 709
        },
        "page_content": "Data annotation is a key bottleneck in many data driven algorithms. Specifically, deep learning models, which became a prominent tool in many data driven tasks in recent years, require large datasets to work well. However, many tasks require manual annotations which are relatively hard to obtain at scale. An attractive alternative is lightly supervised learning BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , in which the objective function is supplemented by a set of domain-specific soft-constraints over the model's predictions on unlabeled data. For example, in label regularization BIBREF0 the model is trained to fit the true label proportions of an unlabeled dataset.",
        "type": "Document"
      },
      {
        "id": "731d76b9-6365-4a8c-872e-c14badc3b626",
        "metadata": {
          "vector_store_key": "1911.07228-0",
          "chunk_id": 12,
          "document_id": "1911.07228",
          "start_idx": 6115,
          "end_idx": 6702
        },
        "page_content": "The dataset contains four different types of label: Location (LOC), Person (PER), Organization (ORG) and Miscellaneous - Name of an entity that do not belong to 3 types above (Table TABREF15). Although the corpus has more information about the POS and chunks, but we do not use them as features in our model. There are two folders with 267 text files of training data and 45 text files of test data. They all have their own format. We take 21 first text files and 22 last text files and 22 sentences of the 22th text file and 55 sentences of the 245th text file to be a development data.",
        "type": "Document"
      },
      {
        "id": "c423100d-2a4a-4065-9a8f-7d21179ca784",
        "metadata": {
          "vector_store_key": "1908.06606-2",
          "chunk_id": 1,
          "document_id": "1908.06606",
          "start_idx": 734,
          "end_idx": 1400
        },
        "page_content": "CTS is able to provide large-scale extracted structured data for enormous down-stream clinical researches. However, end-to-end CTS is a very challenging task. Different CTS tasks often have non-uniform output formats, such as specific-class classifications (e.g. tumor stage), strings in the original text (e.g. result for a laboratory test) and inferred values from part of the original text (e.g. calculated tumor size). Researchers have to construct different models for it, which is already costly, and hence it calls for a lot of labeled data for each model. Moreover, labeling necessary amount of data for training neural network requires expensive labor cost.",
        "type": "Document"
      },
      {
        "id": "2192ff0b-b538-4ea7-a2bc-fd1ec7bced81",
        "metadata": {
          "vector_store_key": "1909.00542-3",
          "chunk_id": 8,
          "document_id": "1909.00542",
          "start_idx": 4786,
          "end_idx": 5456
        },
        "page_content": "Introducing a classifier makes labelling the training data not trivial, since the target summaries are human-generated and they do not have a perfect mapping to the input sentences. In addition, some samples have multiple reference summaries. BIBREF11 showed that different data labelling approaches influence the quality of the final summary, and some labelling approaches may lead to better results than using regression. In this paper we experiment with the following labelling approaches: : Label as \u201csummary\u201d all sentences from the input text that have a ROUGE score above a threshold $t$. : Label as \u201csummary\u201d the $m$ input text sentences with highest ROUGE score.",
        "type": "Document"
      },
      {
        "id": "4b26dd1f-f696-43ab-808f-c93668146f0c",
        "metadata": {
          "vector_store_key": "1710.01492-2",
          "chunk_id": 37,
          "document_id": "1710.01492",
          "start_idx": 20207,
          "end_idx": 20785
        },
        "page_content": "A popular way to solve this latter problem is to use self training, a form of semi-supervised learning, where first a system is trained on the available training data only, then this system is applied to make predictions on a large unannotated set of tweets, and finally it is trained for a few more iterations on its own predictions. This works because parts of the network, e.g., with convolution or with LSTMs BIBREF55 , BIBREF54 , BIBREF56 , need to learn something like a language model, i.e., which word is likely to follow which one. Training these parts needs no labels.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the baseline state of the art models?",
    "llm_answer": "Pre-trained BERT<sub>base</sub> model fine-tuned on the CNNDM dataset, and a hierarchical transformer summarization model (HeriTransfomer) without pre-training.  Also, linear or multi-layer perceptron (MLP) classifiers.\n",
    "context": [
      {
        "id": "ad1d7753-3848-45ad-a7ec-2f9bff0e735c",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 18,
          "document_id": "1912.13109",
          "start_idx": 9307,
          "end_idx": 10125
        },
        "page_content": "While the model probability is the probability that the observation i belongs to category c. Among the model architectures we experimented with and without data augmentation were: Fully Connected dense networks: Model hyperparameters were inspired from the previous work done by Vo et al and Mathur et al. This was also used as a baseline model but we did not get appreciable performance on such architecture due to FC networks not being able to capture local and long term dependencies. Convolution based architectures: Architecture and hyperparameter choices were chosen from the past study Deon on the subject. We were able to boost the performance as compared to only FC based network but we noticed better performance from architectures that are suitable to sequences such as text messages or any timeseries data.",
        "type": "Document"
      },
      {
        "id": "4196e073-3314-48e8-ad4c-8488e8a91274",
        "metadata": {
          "vector_store_key": "1905.06566-1",
          "chunk_id": 42,
          "document_id": "1905.06566",
          "start_idx": 23254,
          "end_idx": 24010
        },
        "page_content": "One is the hierarchical transformer summarization model (HeriTransfomer; described in \"Extractive Summarization\" ) without pre-training. Note the setting for HeriTransfomer is ( $L=4$ , $H=300$ and $A=4$ ) . We can see that the pre-training (details in Section \"Pre-training\" ) leads to a +1.25 ROUGE improvement. Another baseline is based on a pre-trained BERT BIBREF0 and finetuned on the CNNDM dataset. We used the $\\text{BERT}_{\\text{base}}$ model because our 16G RAM V100 GPU cannot fit $\\text{BERT}_{\\text{large}}$ for the summarization task even with batch size of 1. The positional embedding of BERT supports input length up to 512 words, we therefore split documents with more than 10 sentences into multiple blocks (each block with 10 sentences).",
        "type": "Document"
      },
      {
        "id": "a6dff15d-6323-4ff2-9d1f-f503bd1a9ef5",
        "metadata": {
          "vector_store_key": "1710.01492-5",
          "chunk_id": 35,
          "document_id": "1710.01492",
          "start_idx": 19077,
          "end_idx": 19782
        },
        "page_content": "Traditionally, the above features were fed into classifiers such as Maximum Entropy (MaxEnt) and Support Vector Machines (SVM) with various kernels. However, observation over the SemEval Twitter sentiment task in recent years shows growing interest in, and by now clear dominance of methods based on deep learning. In particular, the best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks BIBREF53 , BIBREF54 . Conversely, kernel machines seem to be less frequently used than in the past, and the use of learning methods other than the ones mentioned above is at this point scarce. All these models are examples of supervised learning as they need labeled training data.",
        "type": "Document"
      },
      {
        "id": "abde311f-db3e-44fd-96aa-882a0a2aeeb0",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 17,
          "document_id": "1912.13109",
          "start_idx": 8552,
          "end_idx": 9307
        },
        "page_content": "These processes and its implementation details will be discussed later but they were learning rate decay, early stopping, model checkpointing and reducing learning rate on plateau. For the loss function we chose categorical cross entropy loss in finding the most optimal weights/parameters of the model. Formally this loss function for the model is defined as below: The double sum is over the number of observations and the categories respectively. While the model probability is the probability that the observation i belongs to category c. Among the model architectures we experimented with and without data augmentation were: Fully Connected dense networks: Model hyperparameters were inspired from the previous work done by Vo et al and Mathur et al.",
        "type": "Document"
      },
      {
        "id": "f5eae161-3799-4ef1-8955-becc9f5bcf23",
        "metadata": {
          "vector_store_key": "1703.09684-5",
          "chunk_id": 35,
          "document_id": "1703.09684",
          "start_idx": 19844,
          "end_idx": 20503
        },
        "page_content": "We briefly review some of these systems, focusing on the models we compare in experiments. For a more comprehensive review, see BIBREF10 and BIBREF11 . Two simple VQA baselines are linear or multi-layer perceptron (MLP) classifiers that take as input the question and image embeddings concatenated to each other BIBREF1 , BIBREF7 , BIBREF8 , where the image features come from the last hidden layer of a CNN. These simple approaches often work well and can be competitive with complex attentive models BIBREF7 , BIBREF8 . Spatial attention has been heavily investigated in VQA models BIBREF2 , BIBREF20 , BIBREF28 , BIBREF30 , BIBREF27 , BIBREF24 , BIBREF21 .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they extract causality from text?",
    "llm_answer": "They constructed \"cause-trees,\" binary trees rooted at \"caused,\" \"causes,\" or \"causing,\" illustrating the most frequent n-grams beginning or ending with those root words.\n",
    "context": [
      {
        "id": "a249cdbc-b08e-408d-8f0a-e62716a3a62e",
        "metadata": {
          "vector_store_key": "1604.05781-1",
          "chunk_id": 30,
          "document_id": "1604.05781",
          "start_idx": 16231,
          "end_idx": 17015
        },
        "page_content": "To explore these structures specifically in the causal corpus, we constructed \u201ccause-trees\u201d, shown in Fig. 2 . Inspired by association mining BIBREF41 , a cause-tree is a binary tree rooted at either `caused', `causes', or `causing', that illustrates the most frequently occurring $n$ -grams that either begin or end with that root cause word (see Methods for details). The \u201ccauses\u201d tree shows the focused writing (sentence segments) that many people use to express either the relationship between their own actions and a cause-and-effect (\u201ceven if it causes\u201d), or the uncontrollable effect a cause may have on themselves: \u201ccauses me to have\u201d shows a person's inability to control a causal event (\u201c[...] i have central heterochromia which causes me to have dual colors in both eyes\u201d).",
        "type": "Document"
      },
      {
        "id": "52b42122-5be1-4290-9d3b-719f1f3b07c9",
        "metadata": {
          "vector_store_key": "1604.05781-0",
          "chunk_id": 1,
          "document_id": "1604.05781",
          "start_idx": 733,
          "end_idx": 1395
        },
        "page_content": "Causal inference is a crucial way that humans comprehend the world, and it has been a major focus of philosophy, statistics, mathematics, psychology, and the cognitive sciences. Philosophers such as Hume and Kant have long argued whether causality is a human-centric illusion or the discovery of a priori truth BIBREF10 , BIBREF11 . Causal inference in science is incredibly important, and researchers have developed statistical measures such as Granger causality BIBREF12 , mathematical and probabilistic frameworks BIBREF13 , BIBREF14 , BIBREF15 , BIBREF16 , and text mining procedures BIBREF17 , BIBREF18 , BIBREF19 to better infer causal influence from data.",
        "type": "Document"
      },
      {
        "id": "8e64ac7b-90e0-4493-9a55-4c41ba2a019f",
        "metadata": {
          "vector_store_key": "1604.05781-0",
          "chunk_id": 6,
          "document_id": "1604.05781",
          "start_idx": 2923,
          "end_idx": 3591
        },
        "page_content": "Instead, here we focus on statements that are with high certainty causal statements, with the goal to better understand key characteristics about causal statements that differ from everyday online communication. The rest of this paper is organized as follows: In Sec. \"Materials and Methods\" we discuss our materials and methods, including the dataset we studied, how we preprocessed that data and extracted a `causal' corpus and a corresponding `control' corpus, and the details of the statistical and language analysis tools we studied these corpora with. In Sec. \"Results\" we present results using these tools to compare the causal statements to control statements.",
        "type": "Document"
      },
      {
        "id": "4332a2f4-dc5d-4e48-a2b2-c193c278d625",
        "metadata": {
          "vector_store_key": "1604.05781-0",
          "chunk_id": 2,
          "document_id": "1604.05781",
          "start_idx": 889,
          "end_idx": 1657
        },
        "page_content": "Causal inference in science is incredibly important, and researchers have developed statistical measures such as Granger causality BIBREF12 , mathematical and probabilistic frameworks BIBREF13 , BIBREF14 , BIBREF15 , BIBREF16 , and text mining procedures BIBREF17 , BIBREF18 , BIBREF19 to better infer causal influence from data. In the cognitive sciences, the famous perception experiments of Michotte et al. led to a long line of research exploring the cognitive biases that humans possess when attempting to link cause and effect BIBREF20 , BIBREF21 , BIBREF22 . How humans understand and communicate cause and effect relationships is complicated, and is influenced by language structure BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26 and sentiment or valence BIBREF27 .",
        "type": "Document"
      },
      {
        "id": "23547b97-bdb7-46bd-8885-e56f468a127d",
        "metadata": {
          "vector_store_key": "1604.05781-0",
          "chunk_id": 5,
          "document_id": "1604.05781",
          "start_idx": 2531,
          "end_idx": 3269
        },
        "page_content": "The goal of this paper is to explore the language and topics of causal statements in a large corpus of social media taken from Twitter. We hypothesize that language and sentiment biases play a significant role in these statements, and that tools from natural language processing and computational linguistics can be used to study them. We do not attempt to study the factual correctness of these statements or offer any degree of verification, nor do we exhaustively identify and extract all causal statements from these data. Instead, here we focus on statements that are with high certainty causal statements, with the goal to better understand key characteristics about causal statements that differ from everyday online communication.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the source of the \"control\" corpus?",
    "llm_answer": "Randomly selected documents, matched temporally to causal documents.\n",
    "context": [
      {
        "id": "ade4d20b-2154-4151-ad8a-158bdfa59734",
        "metadata": {
          "vector_store_key": "1604.05781-2",
          "chunk_id": 11,
          "document_id": "1604.05781",
          "start_idx": 5994,
          "end_idx": 6814
        },
        "page_content": "These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present. The end result of this procedure identified 965,560 causal and 965,560 control documents. Each of the three \u201ccause-words\u201d, `caused', `causes', and `causing' appeared in 38.2%, 35.0%, and 26.8% of causal documents, respectively. Documents were further studied by annotating their unigrams with Parts-of-Speech (POS) and Named Entities (NE) tags. POS tagging was done using NLTK v3.1 BIBREF29 which implements an averaged perceptron classifier BIBREF32 trained on the Brown Corpus BIBREF33 .",
        "type": "Document"
      },
      {
        "id": "a487187f-323f-4d8e-9339-b01b8cb303ae",
        "metadata": {
          "vector_store_key": "1604.05781-3",
          "chunk_id": 14,
          "document_id": "1604.05781",
          "start_idx": 7648,
          "end_idx": 8300
        },
        "page_content": "Unigrams, POS, and NEs were compared between the cause and control corpora using odds ratios (ORs):  $$\\operatorname{OR}(x) = \\frac{p_C(x)/ (1-p_C(x))}{p_N(x) / (1-p_N(x))},$$   (Eq. 1)   where $p_C(x)$ and $p_N(x)$ are the probabilities that a unigram, POS, or NE $x$ occurs in the causal and control corpus, respectively. These probabilities were computed for each corpus separately as $p(x) = f(x) / \\sum _{x^{\\prime } \\in V} f(x^{\\prime })$ , where $f(x)$ is the total number of occurrences of $x$ in the corpus and $V$ is the relevant set of unigrams, POS, or NEs. Confidence intervals for the ORs were computed using Wald's methodology BIBREF36 .",
        "type": "Document"
      },
      {
        "id": "d90e40da-fb73-40f4-8b01-ca2ea0b01c04",
        "metadata": {
          "vector_store_key": "2003.06279-5",
          "chunk_id": 51,
          "document_id": "2003.06279",
          "start_idx": 31567,
          "end_idx": 32805
        },
        "page_content": "The following words were considered as stopwords in our analysis: all, just, don't, being, over, both, through, yourselves, its, before, o, don, hadn, herself, ll, had, should, to, only, won, under, ours,has, should've, haven't, do, them, his, very, you've, they, not, during, now, him, nor, wasn't, d, did, didn, this, she, each, further, won't, where, mustn't, isn't, few, because, you'd, doing, some, hasn, hasn't, are, our, ourselves, out, what, for, needn't, below, re, does, shouldn't, above, between, mustn, t, be, we, who, mightn't, doesn't, were, here, shouldn, hers, aren't, by, on, about, couldn, of, wouldn't, against, s, isn, or, own, into, yourself, down, hadn't, mightn, couldn't, wasn, your, you're, from, her, their, aren, it's, there, been, whom, too, wouldn, themselves, weren, was, until, more, himself, that, didn't, but, that'll, with, than, those, he, me, myself, ma, weren't, these, up, will, while, ain, can, theirs, my, and, ve, then, is, am, it, doesn, an, as, itself, at, have, in, any, if, again, no, when, same, how, other, which, you, shan't, shan, needn, haven, after, most, such, why, a, off i, m, yours, you'll, so, y, she's, the, having, once. The list of books is shown in Tables TABREF25 and TABREF26.",
        "type": "Document"
      },
      {
        "id": "aa2eaabd-e3cd-4ed8-8c29-f9b20f4375fe",
        "metadata": {
          "vector_store_key": "1703.05260-6",
          "chunk_id": 5,
          "document_id": "1703.05260",
          "start_idx": 2958,
          "end_idx": 3770
        },
        "page_content": "Additionally, the texts have been annotated with coreference information in order to facilitate the study of the interdependence between script structure and coreference. The InScript corpus is a unique resource that provides a basis for studying various aspects of the role of script knowledge in language processing by humans. The acquisition of this corpus is part of a larger research effort that aims at using script knowledge to model the surprisal and information density in written text. Besides InScript, this project also released a corpus of generic descriptions of script activities called DeScript (for Describing Script Structure, Wanzare2016). DeScript contains a range of short and textually simple phrases that describe script events in the style of OMICS or SMILE (singh2002open, Regneri:2010).",
        "type": "Document"
      },
      {
        "id": "267fe014-cde3-49e8-a9cd-e352c2eaa369",
        "metadata": {
          "vector_store_key": "1604.05781-2",
          "chunk_id": 9,
          "document_id": "1604.05781",
          "start_idx": 4740,
          "end_idx": 5454
        },
        "page_content": "There is useful information here, but it is either not natural language text, or it is Twitter-specific, or both. Documents were broken into individual words (unigrams) on whitespace. Casing information was retained, as we will use it for our Named Entity analysis, but otherwise all words were considered lowercase only (see also the Appendix). Stemming BIBREF30 and lemmatization BIBREF31 were not performed. Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'. The word `cause' was not included due to its use as a popular contraction for `because'. One `cause-word' per document restricted the analysis to single relationships between two relata.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the selection criteria for \"causal statements\"?",
    "llm_answer": "The selection criteria are not explicitly stated.  The text describes *how* causal statements were identified and analyzed but not *why* or *how* they were chosen in the first place.\n",
    "context": [
      {
        "id": "b3571c18-8725-431f-8a1f-753fac53bd05",
        "metadata": {
          "vector_store_key": "1604.05781-0",
          "chunk_id": 46,
          "document_id": "1604.05781",
          "start_idx": 25154,
          "end_idx": 25880
        },
        "page_content": "Temporal dynamics of causal statements are also interesting: do time-of-day or time-of-year factors play a role in how causal statements are made? Our work here focused on a limited subset of causal statements, but more generally, these results may inform new methods for automatically detecting causal statements from unstructured, natural language text BIBREF17 . Better computational tools focused on causal statements are an important step towards further understanding misinformation campaigns and other online activities. Lastly, an important but deeply challenging open question is how, if it is even possible, to validate the accuracy of causal statements. Can causal statements be ranked by some confidence metric(s)?",
        "type": "Document"
      },
      {
        "id": "90d4dbea-d070-4935-8a19-14e1adcf54e4",
        "metadata": {
          "vector_store_key": "1604.05781-0",
          "chunk_id": 39,
          "document_id": "1604.05781",
          "start_idx": 21238,
          "end_idx": 21872
        },
        "page_content": "We have found language (Figs. 1 and 2 ) and sentiment (Fig. 3 ) differences between causal statements made on social media compared with other social media statements. But what is being discussed? What are the topical foci of causal statements? To study this, for our last analysis we applied topic models to the causal statements. Topic modeling finds groups of related terms (unigrams) by considering similarities between how those terms co-occur across a set of documents. We used the popular topic modeling method Latent Dirichlet Allocation (LDA) BIBREF39 . We ranked unigrams by how strongly associated they were with the topic.",
        "type": "Document"
      },
      {
        "id": "8e64ac7b-90e0-4493-9a55-4c41ba2a019f",
        "metadata": {
          "vector_store_key": "1604.05781-0",
          "chunk_id": 6,
          "document_id": "1604.05781",
          "start_idx": 2923,
          "end_idx": 3591
        },
        "page_content": "Instead, here we focus on statements that are with high certainty causal statements, with the goal to better understand key characteristics about causal statements that differ from everyday online communication. The rest of this paper is organized as follows: In Sec. \"Materials and Methods\" we discuss our materials and methods, including the dataset we studied, how we preprocessed that data and extracted a `causal' corpus and a corresponding `control' corpus, and the details of the statistical and language analysis tools we studied these corpora with. In Sec. \"Results\" we present results using these tools to compare the causal statements to control statements.",
        "type": "Document"
      },
      {
        "id": "b26cc401-ba9e-4d22-9b15-2ee92e99187f",
        "metadata": {
          "vector_store_key": "1604.05781-0",
          "chunk_id": 45,
          "document_id": "1604.05781",
          "start_idx": 24656,
          "end_idx": 25377
        },
        "page_content": "Are causal statements more likely to spread online and, if so, in which ways? What types of social media users are more or less likely to make causal statements? Will a user be more likely to make a causal statement if they have recently been exposed to one or more causal statements from other users? The topics of causal statements also bring forth important questions to be addressed: how timely are causal statements? Are certain topics always being discussed in causal statements? Are there causal topics that are very popular for only brief periods and then forgotten? Temporal dynamics of causal statements are also interesting: do time-of-day or time-of-year factors play a role in how causal statements are made?",
        "type": "Document"
      },
      {
        "id": "52b42122-5be1-4290-9d3b-719f1f3b07c9",
        "metadata": {
          "vector_store_key": "1604.05781-0",
          "chunk_id": 1,
          "document_id": "1604.05781",
          "start_idx": 733,
          "end_idx": 1395
        },
        "page_content": "Causal inference is a crucial way that humans comprehend the world, and it has been a major focus of philosophy, statistics, mathematics, psychology, and the cognitive sciences. Philosophers such as Hume and Kant have long argued whether causality is a human-centric illusion or the discovery of a priori truth BIBREF10 , BIBREF11 . Causal inference in science is incredibly important, and researchers have developed statistical measures such as Granger causality BIBREF12 , mathematical and probabilistic frameworks BIBREF13 , BIBREF14 , BIBREF15 , BIBREF16 , and text mining procedures BIBREF17 , BIBREF18 , BIBREF19 to better infer causal influence from data.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Do they use expert annotations, crowdsourcing, or only automatic methods to analyze the corpora?",
    "llm_answer": "Expert annotations, crowdsourcing, and automatic methods are used.\n",
    "context": [
      {
        "id": "24f8dfec-c722-47c3-89b3-12384c534e78",
        "metadata": {
          "vector_store_key": "2002.11893-9",
          "chunk_id": 14,
          "document_id": "2002.11893",
          "start_idx": 8370,
          "end_idx": 9067
        },
        "page_content": "Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states. Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three experts were employed to manually annotate dialogue acts and states for 50 dialogues. The results show that our annotations are of high quality.",
        "type": "Document"
      },
      {
        "id": "7541e4a5-40b5-4746-827a-020fdc016b15",
        "metadata": {
          "vector_store_key": "1701.02877-1",
          "chunk_id": 15,
          "document_id": "1701.02877",
          "start_idx": 7875,
          "end_idx": 8631
        },
        "page_content": "So far, there have been no big evaluation efforts, such as ACE and OntoNotes, resulting in substantial amounts of gold standard data. Instead, benchmark corpora were created as part of smaller challenges or individual projects. The first such corpus is the UMBC corpus for Twitter NER BIBREF33 , where researchers used crowdsourcing to obtain annotations for persons, locations and organisations. A further Twitter NER corpus was created by BIBREF21 , which, in contrast to other corpora, contains more fine-grained classes defined by the Freebase schema BIBREF41 . Next, the Making Sense of Microposts initiative BIBREF32 (MSM) provides single annotated data for named entity recognition on Twitter for persons, locations, organisations and miscellaneous.",
        "type": "Document"
      },
      {
        "id": "3a8bfb2a-8384-4f3b-b050-411124048591",
        "metadata": {
          "vector_store_key": "1703.09684-4",
          "chunk_id": 29,
          "document_id": "1703.09684",
          "start_idx": 16454,
          "end_idx": 17129
        },
        "page_content": "The methods we used are similar to those used with COCO, but additional precautions were needed due to quirks in their annotations. Additional details are provided in the Appendix. Creating sentiment understanding and object utility/affordance questions cannot be readily done using templates, so we used manual annotation to create these. Twelve volunteer annotators were trained to generate these questions, and they used a web-based annotation tool that we developed. They were shown random images from COCO and Visual Genome and could also upload images. Post processing was performed on questions from all sources. All numbers were converted to text, e.g., 2 became two.",
        "type": "Document"
      },
      {
        "id": "3699d425-11f4-45cc-aae5-6a93d3e32b86",
        "metadata": {
          "vector_store_key": "1707.03904-0",
          "chunk_id": 8,
          "document_id": "1707.03904",
          "start_idx": 4627,
          "end_idx": 5317
        },
        "page_content": "We evaluate Quasar against human testers, as well as several baselines ranging from na\u00efve heuristics to state-of-the-art machine readers. The best performing baselines achieve $33.6\\%$ and $28.5\\%$ on Quasar-S and Quasar-T, while human performance is $50\\%$ and $60.6\\%$ respectively. For the automatic systems, we see an interesting tension between searching and reading accuracies \u2013 retrieving more documents in the search phase leads to a higher coverage of answers, but makes the comprehension task more difficult. We also collect annotations on a subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short.",
        "type": "Document"
      },
      {
        "id": "fafef8a0-39be-4007-b0e6-a36081a57786",
        "metadata": {
          "vector_store_key": "1701.02877-1",
          "chunk_id": 8,
          "document_id": "1701.02877",
          "start_idx": 3965,
          "end_idx": 4697
        },
        "page_content": "As detailed in Section \"Datasets\" , the corpora studied are OntoNotes BIBREF29 , ACE BIBREF30 , MUC 7 BIBREF31 , the Ritter NER corpus BIBREF21 , the MSM 2013 corpus BIBREF32 , and the UMBC Twitter corpus BIBREF33 . To eliminate potential bias from the choice of statistical NER approach, experiments are carried out with three differently-principled NER approaches, namely Stanford NER BIBREF34 , SENNA BIBREF35 and CRFSuite BIBREF36 (see Section \"NER Models and Features\" for details). Since the goal of this study is to compare NER performance on corpora from diverse domains and genres, seven benchmark NER corpora are included, spanning newswire, broadcast conversation, Web content, and social media (see Table 1 for details).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "how do they collect the comparable corpus?",
    "llm_answer": "A corpus of poems and a corpus of vernacular literature were collected from online resources.  The poem corpus contains 163K quatrain poems from Tang Poems and Song Poems, and the vernacular literature corpus contains 337K short paragraphs from 281 famous books.\n",
    "context": [
      {
        "id": "fadaf1c5-453f-4bb8-8f46-e15847c8d9f4",
        "metadata": {
          "vector_store_key": "1902.00330-5",
          "chunk_id": 19,
          "document_id": "1902.00330",
          "start_idx": 10953,
          "end_idx": 11716
        },
        "page_content": "As mentioned above, the mentions should be sorted according to their contextual information and topical coherence. So, we firstly divide the adjacent mentions into a segment by the order they appear in the document based on the observation that the topical consistency attenuates along with the distance between the mentions. Then, we sort mentions in a segment based on the local similarity and place the mention that has a higher similarity value in the front of the sequence. In Equation 1, we define the local similarity of $m_i$ and its corresponding candidate entity $e_t^i$ . On this basis, we define $\\Psi _{max}(m_i, e_i^a)$ as the the maximum local similarity between the $m_i$ and its candidate set $C_{m_i} = \\lbrace e_i^1, e_i^2,..., e_i^n\\rbrace $ .",
        "type": "Document"
      },
      {
        "id": "de2465b8-bc43-4557-b11e-b5ad1b2f0d72",
        "metadata": {
          "vector_store_key": "1909.00279-3",
          "chunk_id": 23,
          "document_id": "1909.00279",
          "start_idx": 13381,
          "end_idx": 14083
        },
        "page_content": "(Section SECREF27) To this end, we built a dataset as described in Section SECREF18. Evaluation metrics and baselines are described in Section SECREF21 and SECREF22. For the implementation details of building the dataset and models, please refer to supplementary materials. Training and Validation Sets We collected a corpus of poems and a corpus of vernacular literature from online resources. The poem corpus contains 163K quatrain poems from Tang Poems and Song Poems, the vernacular literature corpus contains 337K short paragraphs from 281 famous books, the corpus covers various literary forms including prose, fiction and essay. Note that our poem corpus and a vernacular corpus are not aligned.",
        "type": "Document"
      },
      {
        "id": "98d737c7-5b77-4594-8cbe-d3a7307ae9ac",
        "metadata": {
          "vector_store_key": "2003.06279-3",
          "chunk_id": 26,
          "document_id": "2003.06279",
          "start_idx": 15495,
          "end_idx": 16268
        },
        "page_content": "In summary, the methodology used in this paper encompasses the following steps: Network construction: here texts are mapped into a co-occurrence networks. Some variations exists in the literature, however here we focused in the most usual variation, i.e. the possibility of considering or disregarding stopwords. A network with co-occurrence links is obtained after this step. Network enrichment: in this step, the network is enriched with virtual edges established via similarity of word embeddings. After this step, we are given a complete network with weighted links. Virtually, any embedding technique could be used to gauge the similarity between nodes. Network filtering: in order to eliminate spurious links included in the last step, the weakest edges are filtered.",
        "type": "Document"
      },
      {
        "id": "26f43714-f142-4335-9790-54cfc0451b61",
        "metadata": {
          "vector_store_key": "1809.08731-4",
          "chunk_id": 18,
          "document_id": "1809.08731",
          "start_idx": 10155,
          "end_idx": 10841
        },
        "page_content": "Generated and reference compressions are tokenized and lowercased. For multiple references, we only make use of the one with the highest score for each example. We compare to the best n-gram-overlap metrics from toutanova2016dataset; combinations of linguistic units (bi-grams (LR2) and tri-grams (LR3)) and scoring measures (recall (R) and F-score (F)). With multiple references, we consider the union of the sets of n-grams. Again, generated and reference compressions are tokenized and lowercased. We further compare to the negative LM cross-entropy, i.e., the log-probability which is only normalized by sentence length. The score of a sentence $S$ is calculated as  $$\\text{NCE}(S)",
        "type": "Document"
      },
      {
        "id": "bdd3efe8-8f40-43ef-ad70-373ff0e46ef9",
        "metadata": {
          "vector_store_key": "1902.09087-1",
          "chunk_id": 32,
          "document_id": "1902.09087",
          "start_idx": 17423,
          "end_idx": 18209
        },
        "page_content": "We also implement several state-of-the-art matching models using the open-source project MatchZoo BIBREF19 , where we tune hyper-parameters using grid search, e.g., whether using word or character inputs. Arc1, Arc2, CDSSM are traditional CNNs based matching models proposed by BIBREF20 , BIBREF21 . Arc1 and CDSSM compute the similarity via sentence representations and Arc2 uses the word pair similarities. MV-LSTM BIBREF22 computes the matching score by examining the interaction between the representations from two sentences obtained by a shared BiLSTM encoder. MatchPyramid(MP) BIBREF23 utilizes 2D convolutions and pooling strategies over word pair similarity matrices to compute the matching scores. We also compare with the state-of-the-art models in DBQA BIBREF15 , BIBREF16 .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they collect the control corpus?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "579108ed-b055-4580-8ae2-e71728e57e94",
        "metadata": {
          "vector_store_key": "2002.02224-3",
          "chunk_id": 24,
          "document_id": "2002.02224",
          "start_idx": 14172,
          "end_idx": 14910
        },
        "page_content": "Further processing included: control and repair of incompletely identified court identifiers (manual); identification and sorting of identifiers as belonging to Supreme Court, Supreme Administrative Court or Constitutional Court (rule-based, manual); standardisation of different types of court identifiers (rule-based, manual); parsing of identifiers with court decisions available in CzCDC 1.0. Overall, through the process described in Section SECREF3, we have retrieved three datasets of extracted references - one dataset per each of the apex courts. These datasets consist of the individual pairs containing the identification of the decision from which the reference was retrieved, and the identification of the referred documents.",
        "type": "Document"
      },
      {
        "id": "3f604ba6-c047-42b7-af44-f8921b21e1d1",
        "metadata": {
          "vector_store_key": "1909.00694-4",
          "chunk_id": 16,
          "document_id": "1909.00694",
          "start_idx": 9543,
          "end_idx": 10158
        },
        "page_content": "Although the ACP corpus was originally constructed in the context of sentiment analysis, we found that it could roughly be regarded as a collection of affective events. We parsed each sentence and extracted the last clause in it. The train/dev/test split of the data is shown in Table TABREF19. The objective function for supervised training is:  where $v_i$ is the $i$-th event, $R_i$ is the reference score of $v_i$, and $N_{\\rm ACP}$ is the number of the events of the ACP Corpus. To optimize the hyperparameters, we used the dev set of the ACP Corpus. For the evaluation, we used the test set of the ACP Corpus.",
        "type": "Document"
      },
      {
        "id": "a487187f-323f-4d8e-9339-b01b8cb303ae",
        "metadata": {
          "vector_store_key": "1604.05781-3",
          "chunk_id": 14,
          "document_id": "1604.05781",
          "start_idx": 7648,
          "end_idx": 8300
        },
        "page_content": "Unigrams, POS, and NEs were compared between the cause and control corpora using odds ratios (ORs):  $$\\operatorname{OR}(x) = \\frac{p_C(x)/ (1-p_C(x))}{p_N(x) / (1-p_N(x))},$$   (Eq. 1)   where $p_C(x)$ and $p_N(x)$ are the probabilities that a unigram, POS, or NE $x$ occurs in the causal and control corpus, respectively. These probabilities were computed for each corpus separately as $p(x) = f(x) / \\sum _{x^{\\prime } \\in V} f(x^{\\prime })$ , where $f(x)$ is the total number of occurrences of $x$ in the corpus and $V$ is the relevant set of unigrams, POS, or NEs. Confidence intervals for the ORs were computed using Wald's methodology BIBREF36 .",
        "type": "Document"
      },
      {
        "id": "1fd04b91-9f8e-438f-bce2-00607a578cf5",
        "metadata": {
          "vector_store_key": "2002.02224-1",
          "chunk_id": 20,
          "document_id": "2002.02224",
          "start_idx": 12025,
          "end_idx": 12681
        },
        "page_content": "The model was trained for each type independently. In order to obtain the citation data of the Czech apex courts, it was necessary to recognize and extract the references from the CzCDC 1.0. Given that training data for both the reference recognition model BIBREF13, BIBREF34 and the text segmentation model BIBREF33 are publicly available, we were able to conduct extensive error analysis and put together a pipeline to arguably achieve the maximum efficiency in the task. The pipeline described in this part is graphically represented in Figure FIGREF10. As the first step, every document in the CzCDC 1.0 was segmented using the text segmentation model.",
        "type": "Document"
      },
      {
        "id": "781bc486-b7b0-46d3-bad5-41256b34e69f",
        "metadata": {
          "vector_store_key": "1905.06566-1",
          "chunk_id": 39,
          "document_id": "1905.06566",
          "start_idx": 21541,
          "end_idx": 22262
        },
        "page_content": "Each document is annotated by three different subjects. Our main results on the CNNDM dataset are shown in Table 1 , with abstractive models in the top block and extractive models in the bottom block. Pointer+Coverage BIBREF9 , Abstract-ML+RL BIBREF10 and DCA BIBREF42 are all sequence to sequence learning based models with copy and coverage modeling, reinforcement learning and deep communicating agents extensions. SentRewrite BIBREF26 and InconsisLoss BIBREF25 all try to decompose the word by word summary generation into sentence selection from document and \u201csentence\u201d level summarization (or compression). Bottom-Up BIBREF27 generates summaries by combines a word prediction model with the decoder attention model.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are resolution model variables?",
    "llm_answer": "The provided text describes how different features are used in a learning algorithm, but it does not define \"resolution model variables.\"\n",
    "context": [
      {
        "id": "afeca8ae-b494-4699-bc64-a75d7387f1b6",
        "metadata": {
          "vector_store_key": "1603.04553-2",
          "chunk_id": 28,
          "document_id": "1603.04553",
          "start_idx": 15471,
          "end_idx": 15572
        },
        "page_content": "The above derivations correspond to the learning algorithm in Algorithm \"Resolution Mode Variables\" .",
        "type": "Document"
      },
      {
        "id": "13484938-48e7-4311-873d-4dac5900a3c8",
        "metadata": {
          "vector_store_key": "1603.04553-4",
          "chunk_id": 14,
          "document_id": "1603.04553",
          "start_idx": 7746,
          "end_idx": 8356
        },
        "page_content": "Specifically, as shown in Table 1 , we use different features under different resolution modes. It should be noted that only the Distance feature is designed for parameter $q$ , all other features are designed for parameter $t$ . For model learning, we run EM algorithm BIBREF19 on our Model, treating $D$ as observed data and $C$ as latent variables. We run EM with 10 iterations and select the parameters achieving the best performance on the development data. Each iteration takes around 12 hours with 10 CPUs parallelly. The best parameters appear at around the 5th iteration, according to our experiments.",
        "type": "Document"
      },
      {
        "id": "7ba37e69-51dd-420d-baa0-26b7030ca669",
        "metadata": {
          "vector_store_key": "1910.02789-3",
          "chunk_id": 40,
          "document_id": "1910.02789",
          "start_idx": 21794,
          "end_idx": 22565
        },
        "page_content": "This includes examples such as user-based domains, in which user profiles and comments are part of the state, or the stock market, in which stocks are described by analysts and other readily available text. 3D physical environments such as VizDoom also fall into this category, as semantic segmentation maps can be easily described using natural language. Subjective information: Subjectivity refers to aspects used to express opinions, evaluations, and speculations. These may include strategies for a game, the way a doctor feels about her patient, the mood of a driver, and more. Unstructured information: In these cases, features might be measured by different units, with an arbitrary position in the state's feature vector, rendering them sensitive to permutations.",
        "type": "Document"
      },
      {
        "id": "c1ad97ee-60dd-4fdc-b91e-97dd0075443b",
        "metadata": {
          "vector_store_key": "1806.00722-1",
          "chunk_id": 16,
          "document_id": "1806.00722",
          "start_idx": 9601,
          "end_idx": 10257
        },
        "page_content": "Since the number of features fed into nonlinear operation is accumulated along the path, the parameter size increases accordingly. For example, for the INLINEFORM0 -th encoder layer, the input dimension of features is INLINEFORM1 , where INLINEFORM2 is the feature dimension in previous layers, INLINEFORM3 is the embedding size. In order to avoid the calculation bottleneck for later layers due to large INLINEFORM4 , we introduce the summary layer for deeper models. It summarizes the features for all previous layers and projects back to the embedding size, so that later layers of both the encoder and the decoder side do not need to look back further.",
        "type": "Document"
      },
      {
        "id": "7ced1b06-be7c-4de8-8db5-2ba60af974ce",
        "metadata": {
          "vector_store_key": "2004.03744-0",
          "chunk_id": 37,
          "document_id": "2004.03744",
          "start_idx": 20280,
          "end_idx": 21075
        },
        "page_content": "We highlight that the relevance of explanations is in terms of whether the explanation reflects ground-truth reasons supporting the correct label. This is not to be confused with whether an explanation is correctly illustrating the inner working of the model, which is left as future work. It is also important to note that on a similar experimental setting, Camburu report as low as 34.68% correct explanations, training with explanations that were actually collected for their task. Lastly, the model selection criterion at validation time was the prediction balanced accuracy, which may contribute to the low quality of explanations. While we show that adding an explanation module does not harm prediction performance, more work is necessary to get models that output trustable explanations.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is the model presented in the paper state of the art?",
    "llm_answer": "Yes.\n",
    "context": [
      {
        "id": "1f7e4710-1099-4b7c-bb94-2b1ee2d1b0d4",
        "metadata": {
          "vector_store_key": "1706.08032-2",
          "chunk_id": 4,
          "document_id": "1706.08032",
          "start_idx": 2399,
          "end_idx": 3090
        },
        "page_content": "The paper makes the following contributions: The organization of the present paper is as follows: In section 2, we describe the model architecture which introduces the structure of the model. We explain the basic idea of model and the way of constructing the model. Section 3 show results and analysis and section 4 summarize this paper. Our proposed model consists of a deep learning classifier and a tweet processor. The deep learning classifier is a combination of DeepCNN and Bi-LSTM. The tweet processor standardizes tweets and then applies semantic rules on datasets. We construct a framework that treats the deep learning classifier and the tweet processor as two distinct components.",
        "type": "Document"
      },
      {
        "id": "11bbd522-28ef-4580-9084-84720bfb9758",
        "metadata": {
          "vector_store_key": "2001.06888-0",
          "chunk_id": 42,
          "document_id": "2001.06888",
          "start_idx": 22556,
          "end_idx": 22770
        },
        "page_content": "Evaluation results of our proposed model compared to other state of the art methods show its superiority to these methods in overall while in two categories (Person and Miscellaneous) our model outperformed others.",
        "type": "Document"
      },
      {
        "id": "6824b4f2-136e-4c26-a2a7-a10cf7d4c97e",
        "metadata": {
          "vector_store_key": "1810.00663-8",
          "chunk_id": 46,
          "document_id": "1810.00663",
          "start_idx": 25788,
          "end_idx": 26550
        },
        "page_content": "The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as can be seen in performance drops in Table TABREF28 for the new environments. Nonetheless, the insights from the previous section still hold: masking in the output layer and reordering the graph triplets tend to increase performance. Even though the results in Table TABREF28 suggest that there is room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM.",
        "type": "Document"
      },
      {
        "id": "508a8c4d-00ed-4e56-8dc0-984b999fbac6",
        "metadata": {
          "vector_store_key": "1810.00663-8",
          "chunk_id": 47,
          "document_id": "1810.00663",
          "start_idx": 26550,
          "end_idx": 27204
        },
        "page_content": "For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM. Note that the average number of actions in the ground truth output sequences is 7.07 for the Test-New set. Our model's predictions are just INLINEFORM1 edits off on average from the correct navigation plans. This section discusses qualitative results to better understand how the proposed model uses the navigation graph. We analyze the evolution of the attention weights INLINEFORM0 in Eq. () to assess if the decoder layer of the proposed model is attending to the correct parts of the behavioral graph when making predictions.",
        "type": "Document"
      },
      {
        "id": "6befb2ab-ce21-418d-ab75-2e3285536b35",
        "metadata": {
          "vector_store_key": "1810.00663-8",
          "chunk_id": 45,
          "document_id": "1810.00663",
          "start_idx": 25272,
          "end_idx": 25986
        },
        "page_content": "The rearrangement of the graph triplets also helps to reduce ED and increase F1. Lastly, it is worth noting that our proposed model (last row of Table TABREF28 ) outperforms all other models in previously seen environments. In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models. The previous section evaluated model performance on new instructions (and corresponding navigation plans) for environments that were previously seen at training time. Here, we examine whether the trained models succeed on environments that are completely new. The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was the result of the highest performing system?",
    "llm_answer": "Human\n",
    "context": [
      {
        "id": "7e3794a8-4c49-4ef6-b670-6e8d7e4dae71",
        "metadata": {
          "vector_store_key": "1905.06566-4",
          "chunk_id": 45,
          "document_id": "1905.06566",
          "start_idx": 25128,
          "end_idx": 25731
        },
        "page_content": "We asked the subjects to rank the outputs of these systems from best to worst. As shown in Table 4 , the output of $\\text{\\sc Hibert}_M$ is selected as the best in 30% of cases and we obtained lower mean rank than all systems except for Human. We also converted the rank numbers into ratings (rank $i$ to $7-i$ ) and applied student $t$ -test on the ratings. $\\text{\\sc Hibert}_M$ is significantly different from all systems in comparison ( $p < 0.05$ ), which indicates our model still lags behind Human, but is better than all other systems. As mentioned earlier, our pre-training includes two stages.",
        "type": "Document"
      },
      {
        "id": "904a361a-2d3c-41fa-8807-2303b37ca27b",
        "metadata": {
          "vector_store_key": "1603.04553-3",
          "chunk_id": 21,
          "document_id": "1603.04553",
          "start_idx": 11450,
          "end_idx": 12153
        },
        "page_content": "To make a thorough empirical comparison with previous studies, Table 3 (below the dashed line) also shows the results of some state-of-the-art supervised coreference resolution systems \u2014 IMS: the second best system in the CoNLL 2012 shared task BIBREF28 ; Latent-Tree: the latent tree model BIBREF29 obtaining the best results in the shared task; Berkeley: the Berkeley system with the final feature set BIBREF12 ; LaSO: the structured perceptron system with non-local features BIBREF30 ; Latent-Strc: the latent structure system BIBREF31 ; Model-Stack: the entity-centric system with model stacking BIBREF32 ; and Non-Linear: the non-linear mention-ranking model with feature representations BIBREF33 .",
        "type": "Document"
      },
      {
        "id": "c938d47c-e132-4294-bd51-25295fb776d2",
        "metadata": {
          "vector_store_key": "1603.07044-3",
          "chunk_id": 28,
          "document_id": "1603.07044",
          "start_idx": 15079,
          "end_idx": 15744
        },
        "page_content": "Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited training data is given, both feature-rich based system and our system are worse than the IR system. For task C, our system also got comparable results with the feature-rich based system. If we do a simple system combination (average the rank score) between our system and the IR system, the combined system will give large gains on tasks B and C.",
        "type": "Document"
      },
      {
        "id": "adfdbf2b-9644-4b97-abad-9dee91314188",
        "metadata": {
          "vector_store_key": "1910.11493-4",
          "chunk_id": 24,
          "document_id": "1910.11493",
          "start_idx": 13659,
          "end_idx": 14499
        },
        "page_content": "By combining the gated dual-head attention with a SparseMax activation function, they are able to jointly learn stem and affix modifications, improving significantly over the baseline system. The relative system performance is described in tab:sub2team, which shows the average per-language accuracy of each system. The table reflects the fact that some teams submitted more than one system (e.g. Tuebingen-1 & Tuebingen-2 in the table). Nine teams submitted system papers for Task 2, with several interesting modifications to either the baseline or other prior work that led to modest improvements. Charles-Saarland achieved the highest overall tagging accuracy by leveraging multi-lingual BERT embeddings fine-tuned on a concatenation of all available languages, effectively transporting the cross-lingual objective of Task 1 into Task 2.",
        "type": "Document"
      },
      {
        "id": "aa97b630-1f34-4c1d-8ea0-86388a52b430",
        "metadata": {
          "vector_store_key": "1603.04553-1",
          "chunk_id": 19,
          "document_id": "1603.04553",
          "start_idx": 10672,
          "end_idx": 11346
        },
        "page_content": "All the results are given by the latest version of CoNLL-2012 scorer  Table 3 illustrates the results of our model together as baseline with two deterministic systems, namely Stanford: the Stanford system BIBREF10 and Multigraph: the unsupervised multigraph system BIBREF26 , and one unsupervised system, namely MIR: the unsupervised system using most informative relations BIBREF27 . Our model outperforms the three baseline systems on all the evaluation metrics. Specifically, our model achieves improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford system, the winner of the CoNLL 2011 shared task, on the CoNLL 2012 development and test sets, respectively.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What do they mean by answer styles?",
    "llm_answer": "Answer styles are different forms of expressing an answer, such as concise phrases without context, or well-formed sentences that make sense without context.  They also include the expected form of the answer (e.g., text span, human-generated, multiple choice).\n",
    "context": [
      {
        "id": "3d0bb49c-dd27-4728-b54e-276b92fecb35",
        "metadata": {
          "vector_store_key": "1901.02262-2",
          "chunk_id": 2,
          "document_id": "1901.02262",
          "start_idx": 1082,
          "end_idx": 1751
        },
        "page_content": "Moreover, to satisfy various information needs, intelligent agents should be capable of answering one question in multiple styles, such as concise phrases that do not contain the context of the question and well-formed sentences that make sense even without the context of the question. These capabilities complement each other; however, the methods used in previous studies cannot utilize and control different answer styles within a model. In this study, we propose a generative model, called Masque, for multi-passage RC. On the MS MARCO 2.1 dataset, Masque achieves state-of-the-art performance on the dataset's two tasks, Q&A and NLG, with different answer styles.",
        "type": "Document"
      },
      {
        "id": "0bdb53d8-9cc1-4bba-a4d2-e44ae293ac69",
        "metadata": {
          "vector_store_key": "1905.08949-8",
          "chunk_id": 16,
          "document_id": "1905.08949",
          "start_idx": 9209,
          "end_idx": 9885
        },
        "page_content": "The second factor is the answer type, i.e., the expected form of the answer, typically having four settings: (1) the answer is a text span in the passage, which is usually the case for factoid questions, (2) human-generated, abstractive answer that may not appear in the passage, usually the case for deep questions, (3) multiple choice question where question and its distractors should be jointly generated, and (4) no given answer, which requires the model to automatically learn what is worthy to ask. The design of NQG system differs accordingly. Table 1 presents a listing of the NQG corpora grouped by their cognitive level and answer type, along with their statistics.",
        "type": "Document"
      },
      {
        "id": "2c2c3b8f-0a29-49d2-802f-b0ed42156489",
        "metadata": {
          "vector_store_key": "1901.02262-2",
          "chunk_id": 44,
          "document_id": "1901.02262",
          "start_idx": 22794,
          "end_idx": 23457
        },
        "page_content": "The key strength of our model is its high accuracy of generating abstractive summaries from the question and passages; our model achieved state-of-the-art performance in terms of Rouge-L on the Q&A and NLG tasks of MS MARCO 2.1 that have different answer styles BIBREF5 . The styles considered in this paper are only related to the context of the question in the answer sentence; our model will be promising for controlling other styles such as length and speaking styles. Future work will involve exploring the potential of hybrid models combining extractive and abstractive approaches and improving the passage re-ranking and answerable question identification.",
        "type": "Document"
      },
      {
        "id": "7802c76b-bcc5-4a53-87f2-06b101bda193",
        "metadata": {
          "vector_store_key": "1901.02262-1",
          "chunk_id": 18,
          "document_id": "1901.02262",
          "start_idx": 9626,
          "end_idx": 10203
        },
        "page_content": "Let $y = \\lbrace y_1, \\ldots , y_{T}\\rbrace $ represent one-hot vectors of words in the answer. This layer has the same components as the word embedding layer of the question-passages reader, except that it uses a unidirectional ELMo in order to ensure that the predictions for position $t$ depend only on the known outputs at positions less than $t$ . Moreover, to be able to make use of multiple answer styles within a single system, our model introduces an artificial token corresponding to the target style at the beginning of the answer sentence ( $y_1$ ), like BIBREF14 .",
        "type": "Document"
      },
      {
        "id": "cc867d8d-244d-4212-9839-a620a391c64d",
        "metadata": {
          "vector_store_key": "1901.02262-2",
          "chunk_id": 6,
          "document_id": "1901.02262",
          "start_idx": 2654,
          "end_idx": 3411
        },
        "page_content": "Masque directly models the conditional probability $p(y|x^q, \\lbrace x^{p_k}\\rbrace , s)$ . In addition to multi-style learning, it considers passage ranking and answer possibility classification together as multi-task learning in order to improve accuracy. Figure 2 shows the model architecture. It consists of the following modules. 1 The question-passages reader (\u00a7 \"Question-Passages Reader\" ) models interactions between the question and passages. 2 The passage ranker (\u00a7 \"Passage Ranker\" ) finds relevant passages to the question. 3 The answer possibility classifier (\u00a7 \"Answer Possibility Classifier\" ) identifies answerable questions. 4 The answer sentence decoder (\u00a7 \"Answer Sentence Decoder\" ) outputs a sequence of words conditioned on the style.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the baselines that Masque is compared against?",
    "llm_answer": "\"first n\" baseline and the neural regressor.  MTMSNlarge and large-squad models.\n",
    "context": [
      {
        "id": "cc867d8d-244d-4212-9839-a620a391c64d",
        "metadata": {
          "vector_store_key": "1901.02262-2",
          "chunk_id": 6,
          "document_id": "1901.02262",
          "start_idx": 2654,
          "end_idx": 3411
        },
        "page_content": "Masque directly models the conditional probability $p(y|x^q, \\lbrace x^{p_k}\\rbrace , s)$ . In addition to multi-style learning, it considers passage ranking and answer possibility classification together as multi-task learning in order to improve accuracy. Figure 2 shows the model architecture. It consists of the following modules. 1 The question-passages reader (\u00a7 \"Question-Passages Reader\" ) models interactions between the question and passages. 2 The passage ranker (\u00a7 \"Passage Ranker\" ) finds relevant passages to the question. 3 The answer possibility classifier (\u00a7 \"Answer Possibility Classifier\" ) identifies answerable questions. 4 The answer sentence decoder (\u00a7 \"Answer Sentence Decoder\" ) outputs a sequence of words conditioned on the style.",
        "type": "Document"
      },
      {
        "id": "c3b91df3-30da-4158-9d8d-004c3fa30105",
        "metadata": {
          "vector_store_key": "1901.02262-2",
          "chunk_id": 3,
          "document_id": "1901.02262",
          "start_idx": 1751,
          "end_idx": 2389
        },
        "page_content": "On the MS MARCO 2.1 dataset, Masque achieves state-of-the-art performance on the dataset's two tasks, Q&A and NLG, with different answer styles. The main contributions of this study are that our model enables the following two abilities. The task considered in this paper, is defined as: Problem 1 Given a question with $J$ words $x^q = \\lbrace x^q_1, \\ldots , x^q_J\\rbrace $ , a set of $K$ passages, where each $k$ -th passage is composed of $L$ words $x^{p_k} = \\lbrace x^{p_k}_1, \\ldots , x^{p_k}_{L}\\rbrace $ , and an answer style $s$ , an RC system outputs an answer $y = \\lbrace y_1, \\ldots , y_T \\rbrace $ conditioned on the style.",
        "type": "Document"
      },
      {
        "id": "f340ca58-46dd-4c7d-b755-65dd5298a642",
        "metadata": {
          "vector_store_key": "1901.02262-2",
          "chunk_id": 42,
          "document_id": "1901.02262",
          "start_idx": 21888,
          "end_idx": 22606
        },
        "page_content": "Also, we should note that our model does not guarantee the consistency in terms of meaning across the answer styles. We randomly selected 100 questions and compared the answers our model generated with the NLG and Q&A styles. The consistency ratio was 0.81, where major errors were due to copying words from different parts of the passages and generating different words, especially yes/no, from a fixed vocabulary. Appendix \"Reading Comprehension Examples generated by Masque from MS MARCO 2.1\" shows examples of generated answers. We found (d) style errors; (e) yes/no classification errors; (f) copy errors with respect to numerical values; and (c,e) grammatical errors that were originally contained in the inputs.",
        "type": "Document"
      },
      {
        "id": "beee7b9e-00bc-4ef2-8232-722a694c8ed8",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 28,
          "document_id": "1909.00542",
          "start_idx": 15261,
          "end_idx": 15995
        },
        "page_content": "Therefore we do not comment on the results, other than observing that the \u201cfirst $n$\u201d baseline produces the same results as the neural regressor. As mentioned in Section SECREF3, the labels used for the classification experiments are the 5 sentences with highest ROUGE-SU4 F1 score. Macquarie University's participation in BioASQ 7 focused on the task of generating the ideal answers. The runs use query-based extractive techniques and we experiment with classification, regression, and reinforcement learning approaches. At the time of writing there were no human evaluation results, and based on ROUGE-F1 scores under cross-validation on the training data we observed that classification approaches outperform regression approaches.",
        "type": "Document"
      },
      {
        "id": "2e7cfe41-b41c-475c-83f6-e7d39903180d",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 40,
          "document_id": "1909.13375",
          "start_idx": 21052,
          "end_idx": 21792
        },
        "page_content": "While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1. All that, while keeping in mind we compare a base model to a large model (for reference, note the 8 point improvement between MTMSNbase and MTMSNlarge in both EM and F1). Our best model, large-squad, exhibits a huge improvement of 29.7 in EM and 15.1 in F1 compared to MTMSNlarge. When comparing single-span performance, our best model exhibits slightly better results, but it should be noted that it retains the single-span heads from NABERT+, while in MTMSN they have one head to predict both single-span and multi-span answers.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance achieved on NarrativeQA?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "5151eb5d-0f58-4ac6-8701-22b81cc2fc48",
        "metadata": {
          "vector_store_key": "1603.07044-0",
          "chunk_id": 18,
          "document_id": "1603.07044",
          "start_idx": 9722,
          "end_idx": 10344
        },
        "page_content": "The cQA data is organized as follows: there are 267 original questions, each question has 10 related question, and each related question has 10 comments. Therefore, for task A, there are a total number of 26,700 question-comment pairs. For task B, there are 2,670 question-question pairs. For task C, there are 26,700 question-comment pairs. The test dataset includes 50 questions, 500 related questions and 5,000 comments which do not overlap with the training set. To evaluate the performance, we use mean average precision (MAP) and F1 score. Table 2 shows the initial results using the RNN encoder for different tasks.",
        "type": "Document"
      },
      {
        "id": "071b8e44-d470-4ee3-a8b6-10d72955ab53",
        "metadata": {
          "vector_store_key": "1909.08859-2",
          "chunk_id": 43,
          "document_id": "1909.08859",
          "start_idx": 24516,
          "end_idx": 25252
        },
        "page_content": "Our experimental analysis on visual reasoning tasks in the RecipeQA dataset shows that the model significantly improves the results of the previous models, indicating that it better understands the procedural text and the accompanying images. Additionally, we carefully analyze our results and find that our approach learns meaningful dynamic representations of entities without any entity-level supervision. Although we achieve state-of-the-art results on RecipeQA, clearly there is still room for improvement compared to human performance. We also believe that the PRN architecture will be of value to other visual and textual sequential reasoning tasks. We thank the anonymous reviewers and area chairs for their invaluable feedback.",
        "type": "Document"
      },
      {
        "id": "874d1312-9fdb-46fe-865b-ce75dcb445d6",
        "metadata": {
          "vector_store_key": "1812.07023-3",
          "chunk_id": 32,
          "document_id": "1812.07023",
          "start_idx": 17666,
          "end_idx": 18428
        },
        "page_content": "We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below: Our primary architectural differences over the baseline model are: not concatenating the question, answer pairs before encoding them, the auxiliary decoder module, and using the Time-Extended FiLM module for feature extraction. These, combined with using scheduled sampling and running hyperparameter optimization over the validation set to select hyperparameters, give us the observed performance boost. We observe that our models generate fairly relevant responses to questions in the dialogues, and models with audio-visual inputs respond to audio-visual questions (e.g. \u201cis there any voices or music ?\u201d) correctly more often.",
        "type": "Document"
      },
      {
        "id": "151d796e-21c6-45ab-b6c0-5ecb30217555",
        "metadata": {
          "vector_store_key": "1908.10449-5",
          "chunk_id": 44,
          "document_id": "1908.10449",
          "start_idx": 23741,
          "end_idx": 24390
        },
        "page_content": "Not surprisingly, QA-DQN performs worse in general on iNewsQA, in all experiments. As shown in Table TABREF16, the average number of sentences per document in iNewsQA is about 6 times more than in iSQuAD. This is analogous to games with larger maps in the RL literature, where the environment is partially observable. A better exploration (in our case, jumping) strategy may help QA-DQN to master such harder games. To study QA-DQN's ability to generalize, we select the best performing agent in each experimental setting on the validation set and report their performance on the test set. The agent's test performance is reported in Table TABREF41.",
        "type": "Document"
      },
      {
        "id": "a5e119ca-6278-4509-ae96-a92a09bbc10c",
        "metadata": {
          "vector_store_key": "1808.00265-2",
          "chunk_id": 9,
          "document_id": "1808.00265",
          "start_idx": 5154,
          "end_idx": 5980
        },
        "page_content": "Its multimodal nature and more precise evaluation protocol than alternative multimodal scenarios, such as image captioning, help to explain this interest. Furthermore, the proliferation of suitable datasets and potential applications, are also key elements behind this increasing activity. Most state-of-the-art methods follow a joint embedding approach, where deep models are used to project the textual question and visual input to a joint feature space that is then used to build the answer. Furthermore, most modern approaches pose VQA as a classification problem, where classes correspond to a set of pre-defined candidate answers. As an example, most entries to the VQA challenge BIBREF2 select as output classes the most common 3000 answers in this dataset, which account for 92% of the instances in the validation set.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is an \"answer style\"?",
    "llm_answer": "Answer styles are related to the context of the question in the answer sentence, and can also include length and speaking styles.\n",
    "context": [
      {
        "id": "3d0bb49c-dd27-4728-b54e-276b92fecb35",
        "metadata": {
          "vector_store_key": "1901.02262-2",
          "chunk_id": 2,
          "document_id": "1901.02262",
          "start_idx": 1082,
          "end_idx": 1751
        },
        "page_content": "Moreover, to satisfy various information needs, intelligent agents should be capable of answering one question in multiple styles, such as concise phrases that do not contain the context of the question and well-formed sentences that make sense even without the context of the question. These capabilities complement each other; however, the methods used in previous studies cannot utilize and control different answer styles within a model. In this study, we propose a generative model, called Masque, for multi-passage RC. On the MS MARCO 2.1 dataset, Masque achieves state-of-the-art performance on the dataset's two tasks, Q&A and NLG, with different answer styles.",
        "type": "Document"
      },
      {
        "id": "2c2c3b8f-0a29-49d2-802f-b0ed42156489",
        "metadata": {
          "vector_store_key": "1901.02262-2",
          "chunk_id": 44,
          "document_id": "1901.02262",
          "start_idx": 22794,
          "end_idx": 23457
        },
        "page_content": "The key strength of our model is its high accuracy of generating abstractive summaries from the question and passages; our model achieved state-of-the-art performance in terms of Rouge-L on the Q&A and NLG tasks of MS MARCO 2.1 that have different answer styles BIBREF5 . The styles considered in this paper are only related to the context of the question in the answer sentence; our model will be promising for controlling other styles such as length and speaking styles. Future work will involve exploring the potential of hybrid models combining extractive and abstractive approaches and improving the passage re-ranking and answerable question identification.",
        "type": "Document"
      },
      {
        "id": "7802c76b-bcc5-4a53-87f2-06b101bda193",
        "metadata": {
          "vector_store_key": "1901.02262-1",
          "chunk_id": 18,
          "document_id": "1901.02262",
          "start_idx": 9626,
          "end_idx": 10203
        },
        "page_content": "Let $y = \\lbrace y_1, \\ldots , y_{T}\\rbrace $ represent one-hot vectors of words in the answer. This layer has the same components as the word embedding layer of the question-passages reader, except that it uses a unidirectional ELMo in order to ensure that the predictions for position $t$ depend only on the known outputs at positions less than $t$ . Moreover, to be able to make use of multiple answer styles within a single system, our model introduces an artificial token corresponding to the target style at the beginning of the answer sentence ( $y_1$ ), like BIBREF14 .",
        "type": "Document"
      },
      {
        "id": "fb42a52a-7e62-48a5-ba06-99ab0a342acd",
        "metadata": {
          "vector_store_key": "1901.02262-2",
          "chunk_id": 41,
          "document_id": "1901.02262",
          "start_idx": 21462,
          "end_idx": 22152
        },
        "page_content": "Figure 4 shows the precision-recall curve of answer possibility classification on the ALL dev. set, where the positive class is the answerable data. Our model identified the answerable questions well. The maximum $F_1$ score was 0.7893. This is the first report on answer possibility classification with MS MARCO 2.1. Figure 5 shows the lengths of the answers generated by our model, which are broken down by answer style and query type. The generated answers were relatively shorter than the reference answers but well controlled with the target style in every query type. Also, we should note that our model does not guarantee the consistency in terms of meaning across the answer styles.",
        "type": "Document"
      },
      {
        "id": "33a6fc04-f945-4b24-8ec0-7bf28779c77a",
        "metadata": {
          "vector_store_key": "1901.02262-1",
          "chunk_id": 4,
          "document_id": "1901.02262",
          "start_idx": 1845,
          "end_idx": 2654
        },
        "page_content": "The task considered in this paper, is defined as: Problem 1 Given a question with $J$ words $x^q = \\lbrace x^q_1, \\ldots , x^q_J\\rbrace $ , a set of $K$ passages, where each $k$ -th passage is composed of $L$ words $x^{p_k} = \\lbrace x^{p_k}_1, \\ldots , x^{p_k}_{L}\\rbrace $ , and an answer style $s$ , an RC system outputs an answer $y = \\lbrace y_1, \\ldots , y_T \\rbrace $ conditioned on the style. In short, for inference, given a set of 3-tuples $(x^q, \\lbrace x^{p_k}\\rbrace , s)$ , the system predicts $P(y)$ . The training data is a set of 6-tuples: $(x^q, \\lbrace x^{p_k}\\rbrace , s, y, a, \\lbrace r^{p_k}\\rbrace )$ , where $a$ is 1 if the question is answerable with the provided passages and 0 otherwise, and $r^{p_k}$ is 1 if the $k$ -th passage is required to formulate the answer and 0 otherwise.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are the EAU text spans annotated?",
    "llm_answer": "Three annotations per instance are collected.  Workers choose a label, highlight words in the hypothesis, and use at least half of the highlighted words to write an explanation for their decision.\n",
    "context": [
      {
        "id": "12096397-9b1b-43df-a5a2-85d917b82e4e",
        "metadata": {
          "vector_store_key": "1906.03338-1",
          "chunk_id": 17,
          "document_id": "1906.03338",
          "start_idx": 9628,
          "end_idx": 10323
        },
        "page_content": "In the very frequent case that discourse features cross the boundaries of embedding context and EAU span, we only take them into account for INLINEFORM0 . We use the element-wise sum of 300-dimensional pre-trained GloVe vectors BIBREF24 corresponding to the words within the EAU span ( INLINEFORM0 ) and the words of the EAU-surrounding context ( INLINEFORM1 ). Additionally, we compute the element-wise subtraction of the source EAU vector from the target EAU vector, with the aim of modelling directions in distributional space, similarly to BIBREF25 . Words with no corresponding pre-trained word vector and empty sequences (e.g., no preceding context available) are treated as a zero-vector.",
        "type": "Document"
      },
      {
        "id": "d4668781-80e0-4be8-a6b9-4622cedf52f6",
        "metadata": {
          "vector_store_key": "2004.03744-6",
          "chunk_id": 3,
          "document_id": "2004.03744",
          "start_idx": 2034,
          "end_idx": 2817
        },
        "page_content": "To ensure high quality annotations, we used a series of quality control measures, such as in-browser checks, inserting trusted examples, and collecting three annotations per instance. Secondly, we re-evaluate current image-text understanding systems, such as the bottom-up top-down attention network (BUTD) BIBREF5 on VTE using our corrected dataset, which we call SNLI-VE-2.0. Thirdly, we introduce the e-SNLI-VE-2.0 corpus, which we form by appending human-written natural language explanations to SNLI-VE-2.0. These explanations were collected in e-SNLI BIBREF6 to support textual entailment for SNLI. For the same reasons as above, we re-annotate the explanations for the neutral pairs in the validation and test sets, while keeping the explanations from e-SNLI for all the rest.",
        "type": "Document"
      },
      {
        "id": "39a72440-1aec-4b85-9f68-776713b42b1e",
        "metadata": {
          "vector_store_key": "2004.03744-6",
          "chunk_id": 7,
          "document_id": "2004.03744",
          "start_idx": 4417,
          "end_idx": 5180
        },
        "page_content": "Our annotations are available at https://github.com/virginie-do/e-SNLI-VE/tree/master/annotations/gt_labels.csv In this work, we only collect new labels for the neutral pairs in the validation and test sets of SNLI-VE. While the procedure of re-annotation is generic, we limit our re-annotation to these splits as a first step to verify the difference in performance that current models have when evaluated on the corrected test set as well as the effect of model selection on the corrected validation set. We leave for future work re-annotation of the training set, which would likely lead to training better VTE models. We also chose not to re-annotate entailment and contradiction classes, as their error rates are much lower ($<$1% as reported by Vu BIBREF3).",
        "type": "Document"
      },
      {
        "id": "9469d5d7-709e-4b55-b5fc-9bff36591b19",
        "metadata": {
          "vector_store_key": "2004.03744-6",
          "chunk_id": 27,
          "document_id": "2004.03744",
          "start_idx": 14894,
          "end_idx": 15526
        },
        "page_content": "To form e-SNLI-VE-2.0, we append to SNLI-VE-2.0 the explanations from e-SNLI for all except the neutral pairs in the validation and test sets of SNLI-VE, which we replace with newly crowdsourced explanations collected at the same time as the labels for these splits (see Figure FIGREF21). Statistics of e-SNLI-VE-2.0 are shown in Appendix SECREF39, Table TABREF40. As mentioned before, in order to submit the annotation of an image-sentence pair, three steps must be completed: workers must choose a label, highlight words in the hypothesis, and use at least half of the highlighted words to write an explanation for their decision.",
        "type": "Document"
      },
      {
        "id": "ded4b72b-3232-4b31-900d-00efb3534b0b",
        "metadata": {
          "vector_store_key": "1906.03338-4",
          "chunk_id": 23,
          "document_id": "1906.03338",
          "start_idx": 13123,
          "end_idx": 13794
        },
        "page_content": "In summary our findings are as follows: (i) models which see the EAU span (content-based, INLINEFORM0 ) are significantly outperformed by models that have no access to the span itself (content-ignorant, INLINEFORM1 ) across all settings; (ii) in two of three prediction settings ( INLINEFORM2 and INLINEFORM3 ), the model which only has access to the context even outperforms the model that has access to all information in the input. The fact that using features derived exclusively from the EAU embedding context ( INLINEFORM4 ) can lead to better results than using a full feature-system ( INLINEFORM5 ) suggests that some information from the EAU can even be harmful.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which Twitter corpus was used to train the word vectors?",
    "llm_answer": "Russian Twitter stream.\n",
    "context": [
      {
        "id": "258db7ec-2b9f-4596-b5c8-7812796c8bf8",
        "metadata": {
          "vector_store_key": "1602.08741-1",
          "chunk_id": 30,
          "document_id": "1602.08741",
          "start_idx": 16688,
          "end_idx": 17383
        },
        "page_content": "It was shown that word vectors can have multiple degrees of similarity. In particular, it is possible to model simple relations, like \"country\"-\"capital city\", gender, syntactic relations with algebraic operations over these vectors. Authors of BIBREF2 propose to assess quality of these vectors on task of exact prediction of these word relations. However, word vectors learned from Twitter seem to perform poorly on this task. We don\u2019t make systematic research on this subject here because it goes outside of the scope of the current paper, though it is an important direction of future studies. Twitter post often contains three special types of words: user mentions, hashtags and hyperlinks.",
        "type": "Document"
      },
      {
        "id": "7042226a-4110-4edf-b073-7104d32893f9",
        "metadata": {
          "vector_store_key": "1602.08741-1",
          "chunk_id": 33,
          "document_id": "1602.08741",
          "start_idx": 18427,
          "end_idx": 19059
        },
        "page_content": "We use HJ-dataset, which was created for RUSSE contest BIBREF4 to measure correlation between similarity of word vectors and human judgements on word pairs similarity. We achieve results comparable with results obtained while training Word2Vec on traditional corpora, like Wikipedia and Web pages BIBREF3 , BIBREF11 . This is especially important because Twitter data is highly dynamic, and traditional sources are mostly static (rarely change over time). Thus verbal data acquired from Twitter may be used to estimate word vectors for neologisms, or determine other changes in word semantic, as soon as they appear in human speech.",
        "type": "Document"
      },
      {
        "id": "1ccebfd0-e53f-4893-9fc0-da94bb8662b4",
        "metadata": {
          "vector_store_key": "1602.08741-4",
          "chunk_id": 4,
          "document_id": "1602.08741",
          "start_idx": 2178,
          "end_idx": 2986
        },
        "page_content": "In this paper we concentrate on usage of Russian Twitter stream as training corpus for Word2Vec model in semantic similarity task, and show results comparable with current (trained on a single corpus). This research is part of molva.spb.ru project, which is a trending topic detection engine for Russian Twitter. Thus the choice of language of interest is narrowed down to only Russian, although there is strong intuition that one can achieve similar results with other languages. The primary goal of this paper is to prove usefulness of Russian Twitter stream as word semantic similarity resource. Twitter is a popular social network, or also called \"microblogging service\", which enables users to share and interact with short messages instantly and publicly (although private accounts are also available).",
        "type": "Document"
      },
      {
        "id": "2970b3df-75f8-4afb-83a4-8d6e019fb847",
        "metadata": {
          "vector_store_key": "1602.08741-1",
          "chunk_id": 18,
          "document_id": "1602.08741",
          "start_idx": 10187,
          "end_idx": 10776
        },
        "page_content": "In order to compute similarity for those pairs, and having in mind that Twitter data is pre-stemmed, we have to stem HJ-dataset with same parser as well. We use Word2Vec to obtain word vectors from Twitter corpus. In this model word vectors are initialized randomly for each unique word and are fed to a sort of neural network. Authors of Word2Vec propose two different models: Skip-gram and CBOW. The first one is trained to predict the context of the word given just the word vector itself. The second one is somewhat opposite: it is trained to predict the word vector given its context.",
        "type": "Document"
      },
      {
        "id": "ddd0f302-4f95-4fd8-99bd-6e57fc993b1a",
        "metadata": {
          "vector_store_key": "1602.08741-4",
          "chunk_id": 5,
          "document_id": "1602.08741",
          "start_idx": 2986,
          "end_idx": 3732
        },
        "page_content": "Twitter is a popular social network, or also called \"microblogging service\", which enables users to share and interact with short messages instantly and publicly (although private accounts are also available). Users all over the world generate hundreds of millions of tweets per day, all over the world, in many languages, generating enormous amount of verbal data. Traditional corpora for the word semantic similarity task are News, Wikipedia, electronic libraries and others (e.g. RUSSE workshop BIBREF4 ). It was shown that type of corpus used for training affects the resulting accuracy. Twitter is not usually considered, and intuition behind this is that probably every-day language is too simple and too occasional to produce good results.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does proposed word embeddings compare to Sindhi fastText word representations?",
    "llm_answer": "The proposed Sindhi word embeddings surpassed SdfastText in intrinsic evaluation.\n",
    "context": [
      {
        "id": "593188ba-1f1d-4995-b16c-cf953b516fcf",
        "metadata": {
          "vector_store_key": "1911.12579-5",
          "chunk_id": 80,
          "document_id": "1911.12579",
          "start_idx": 45888,
          "end_idx": 46696
        },
        "page_content": "Our empirical results demonstrate that our proposed Sindhi word embeddings have captured high semantic relatedness in nearest neighboring words, word pair relationship, country, and capital and WordSim353. The SG yields the best performance than CBoW and GloVe models subsequently. However, the performance of GloVe is low on the same vocabulary because of character-level learning of word representations and sub-sampling approaches in SG and CBoW. Our proposed Sindhi word embeddings have surpassed SdfastText in the intrinsic evaluation matrix. Also, the vocabulary of SdfastText is limited because they are trained on a small Wikipedia corpus of Sindhi Persian-Arabic. We will further investigate the extrinsic performance of proposed word embeddings on the Sindhi text classification task in the future.",
        "type": "Document"
      },
      {
        "id": "060325c5-b8a8-4517-9ade-be8318938170",
        "metadata": {
          "vector_store_key": "1911.12579-5",
          "chunk_id": 11,
          "document_id": "1911.12579",
          "start_idx": 6511,
          "end_idx": 7278
        },
        "page_content": "Generate word embeddings using GloVe, CBoW, and SG Word2Vec algorithms also evaluate and compare them using the intrinsic evaluation approaches of cosine similarity matrix and WordSim353. We are the first to evaluate SdfastText word representations and compare them with our proposed Sindhi word embeddings. The remaining sections of the paper are organized as; Section SECREF2 presents the literature survey regarding computational resources, Sindhi corpus construction, and word embedding models. Afterwards, Section SECREF3 presents the employed methodology, Section SECREF4 consist of statistical analysis of the developed corpus. Section SECREF5 present the experimental setup. The intrinsic evaluation results along with comparison are given in Section SECREF6.",
        "type": "Document"
      },
      {
        "id": "a847110e-8d11-4374-a839-18bce15d7fe7",
        "metadata": {
          "vector_store_key": "1911.12579-5",
          "chunk_id": 81,
          "document_id": "1911.12579",
          "start_idx": 46356,
          "end_idx": 46655
        },
        "page_content": "We will further investigate the extrinsic performance of proposed word embeddings on the Sindhi text classification task in the future. The proposed resources along with systematic evaluation will be a sophisticated addition to the computational resources for statistical Sindhi language processing.",
        "type": "Document"
      },
      {
        "id": "e186b70f-8854-4886-9d64-b47a2236e63e",
        "metadata": {
          "vector_store_key": "1911.12579-5",
          "chunk_id": 79,
          "document_id": "1911.12579",
          "start_idx": 44990,
          "end_idx": 45888
        },
        "page_content": "Secondly, the list of Sindhi stop words is constructed by finding their high frequency and least importance with the help of Sindhi linguistic expert. Thirdly, the unsupervised Sindhi word embeddings are generated using state-of-the-art CBoW, SG and GloVe algorithms and evaluated using popular intrinsic evaluation approaches of cosine similarity matrix and WordSim353 for the first time in Sindhi language processing. We translate English WordSim353 using the English-Sindhi bilingual dictionary, which will also be a good resource for the evaluation of Sindhi word embeddings. Moreover, the proposed word embeddings are also compared with recently revealed SdfastText word representations. Our empirical results demonstrate that our proposed Sindhi word embeddings have captured high semantic relatedness in nearest neighboring words, word pair relationship, country, and capital and WordSim353.",
        "type": "Document"
      },
      {
        "id": "07f483fe-45dc-4a88-b72f-d26d5f098e81",
        "metadata": {
          "vector_store_key": "1911.12579-5",
          "chunk_id": 77,
          "document_id": "1911.12579",
          "start_idx": 43727,
          "end_idx": 44543
        },
        "page_content": "However, in algorithmic perspective, the character-level learning approach in SG and CBoW improves the quality of representation learning, and overall window size, learning rate, number of epochs are the core parameters that largely influence the performance of word embeddings models. Ultimately, the new corpus of low-resourced Sindhi language, list of stop words and pretrained word embeddings along with empirical evaluation, will be a good supplement for future research in SSLP applications. In the future, we aim to use the corpus for annotation projects such as parts-of-speech tagging, named entity recognition. The proposed word embeddings will be refined further by creating custom benchmarks and the extrinsic evaluation approach will be employed for the performance analysis of proposed word embeddings.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many uniue words are in the dataset?",
    "llm_answer": "618,224\n",
    "context": [
      {
        "id": "d737a5bf-ce39-4820-99fc-0f50564ca1a7",
        "metadata": {
          "vector_store_key": "2001.09332-2",
          "chunk_id": 15,
          "document_id": "2001.09332",
          "start_idx": 7575,
          "end_idx": 8365
        },
        "page_content": "The dataset (composed of 2.6 GB of raw text) includes $421\\,829\\,960$ words divided into $17\\,305\\,401$ sentences. The text was previously preprocessed by removing the words whose absolute frequency was less than 5 and eliminating all special characters. Since it is impossible to represent every imaginable numerical value, but not wanting to eliminate the concept of \u201cnumerical representation\" linked to certain words, it was also decided to replace every number present in the text with the particular $\\langle NUM \\rangle $ token; which probably also assumes a better representation in the embedding space (not separating into the various possible values). All the words were then transformed to lowercase (to avoid a double presence) finally producing a vocabulary of $618\\,224$ words.",
        "type": "Document"
      },
      {
        "id": "088347bc-6391-466f-81eb-e37d14e108ba",
        "metadata": {
          "vector_store_key": "1705.09665-6",
          "chunk_id": 23,
          "document_id": "1705.09665",
          "start_idx": 13165,
          "end_idx": 13951
        },
        "page_content": "Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities ( INLINEFORM0 ), for a total of 4,872 community-months ( INLINEFORM1 ). Estimating linguistic measures. We estimate word frequencies INLINEFORM0 , and by extension each downstream measure, in a carefully controlled manner in order to ensure we capture robust and meaningful linguistic behaviour.",
        "type": "Document"
      },
      {
        "id": "c5aaf07a-4f1e-4fc9-8ac4-9161eb906b33",
        "metadata": {
          "vector_store_key": "1908.07816-0",
          "chunk_id": 29,
          "document_id": "1908.07816",
          "start_idx": 16856,
          "end_idx": 17561
        },
        "page_content": "For all the models, the vocabulary consists of 20,000 most frequent words in the Cornell and DailyDialog datasets, plus three extra tokens: <unk> for words that do not exist in the vocabulary, <go> indicating the begin of an utterance, and <eos> indicating the end of an utterance. Here we summarize the configurations and parameters of our experiments: We set the word embedding size to 256. We initialized the word embeddings in the models with word2vec BIBREF22 vectors first trained on Cornell and then fine-tuned on DailyDialog, consistent with the training procedure of the models. We set the number of hidden units of each RNN to 256, the word-level attention depth to 256, and utterance-level 128.",
        "type": "Document"
      },
      {
        "id": "5f5e769f-a98a-40f6-9b34-3ae4e3a2d6bb",
        "metadata": {
          "vector_store_key": "1808.09409-4",
          "chunk_id": 9,
          "document_id": "1808.09409",
          "start_idx": 5149,
          "end_idx": 5909
        },
        "page_content": "The proficiency levels of the learners are diverse, but most of the learners, according to our judgment, is of intermediate or lower level. Our initial collection consists of 1,108,907 sentence pairs from 135,754 essays. As there is lots of noise in raw sentences, we clean up the data by (1) ruling out redundant content, (2) excluding sentences containing foreign words or Chinese phonetic alphabet by checking the Unicode values, (3) dropping overly simple sentences which may not be informative, and (4) utilizing a rule-based classifier to determine whether to include the sentence into the corpus. The final corpus consists of 717,241 learner sentences from writers of 61 different native languages, in which English and Japanese constitute the majority.",
        "type": "Document"
      },
      {
        "id": "10b85ff3-caff-4373-97df-a7b948fde7af",
        "metadata": {
          "vector_store_key": "1911.12579-3",
          "chunk_id": 46,
          "document_id": "1911.12579",
          "start_idx": 26292,
          "end_idx": 27020
        },
        "page_content": "The letter frequencies in our developed corpus are depicted in Figure FIGREF55; however, the corpus contains 187,620,276 total number of the character set. Sindhi Persian-Arabic alphabet consists of 52 letters but in the vocabulary 59 letters are detected, additional seven letters are modified uni-grams and standalone honorific symbols. We denote the combination of letter occurrences in a word as n-grams, where each letter is a gram in a word. The letter n-gram frequency is carefully analyzed in order to find the length of words which is essential to develop NLP systems, including learning of word embeddings such as choosing the minimum or maximum length of sub-word for character-level representation learning BIBREF24.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which baseline methods are used?",
    "llm_answer": "Sliding Window (SW), Sliding Window + Distance (SW+D), language model baselines (n-gram, RNN), K-means, Skip-thought Vectors, Recursive Neural Network, Paragraph Vector, TF-IDF, LDA, i-vector, x-vector, direct methods, hierarchical methods, iterative methods.\n",
    "context": [
      {
        "id": "ddbf653d-2b06-40f5-b102-1128cf41b68a",
        "metadata": {
          "vector_store_key": "1707.03904-0",
          "chunk_id": 31,
          "document_id": "1707.03904",
          "start_idx": 17195,
          "end_idx": 17895
        },
        "page_content": "For Quasar-T we also test the Sliding Window (SW) and Sliding Window + Distance (SW+D) baselines proposed in BIBREF13 . The scores were computed for the list of candidate solutions described in Section \"Context Retrieval\" . For Quasar-S, since the answers come from a fixed vocabulary of entities, we test language model baselines which predict the most likely entity to appear in a given context. We train three n-gram baselines using the SRILM toolkit BIBREF21 for $n=3,4,5$ on the entire corpus of all Stack Overflow posts. The output predictions are restricted to the output vocabulary of entities. We also train a bidirectional Recurrent Neural Network (RNN) language model (based on GRU units).",
        "type": "Document"
      },
      {
        "id": "46e563f1-f3ad-4871-9d28-8409ceeb53b8",
        "metadata": {
          "vector_store_key": "1701.00185-0",
          "chunk_id": 38,
          "document_id": "1701.00185",
          "start_idx": 20509,
          "end_idx": 21535
        },
        "page_content": "Besides K-means, Skip-thought Vectors, Recursive Neural Network and Paragraph Vector based clustering methods, four baseline clustering methods are directly based on the popular unsupervised dimensionality reduction methods as described in Section SECREF11 . We further compare our approach with some other non-biased neural networks, such as bidirectional RNN. More details are listed as follows: K-means K-means BIBREF42 on original keyword features which are respectively weighted with term frequency (TF) and term frequency-inverse document frequency (TF-IDF). Skip-thought Vectors (SkipVec) This baseline BIBREF35 gives an off-the-shelf encoder to produce highly generic sentence representations. The encoder is trained using a large collection of novels and provides three encoder modes, that are unidirectional encoder (SkipVec (Uni)) with 2,400 dimensions, bidirectional encoder (SkipVec (Bi)) with 2,400 dimensions and combined encoder (SkipVec (Combine)) with SkipVec (Uni) and SkipVec (Bi) of 2,400 dimensions each.",
        "type": "Document"
      },
      {
        "id": "02bef31f-a2b5-4b89-8063-62dc0b0f91eb",
        "metadata": {
          "vector_store_key": "1911.01799-4",
          "chunk_id": 19,
          "document_id": "1911.01799",
          "start_idx": 11188,
          "end_idx": 11946
        },
        "page_content": "Two state-of-the-art baseline systems were built following the Kaldi SITW recipe BIBREF22: an i-vector system BIBREF3 and an x-vector system BIBREF10. For the i-vector system, the acoustic feature involved 24-dimensional MFCCs plus the log energy, augmented by the first- and second-order derivatives. We also applied the cepstral mean normalization (CMN) and the energy-based voice active detection (VAD). The universal background model (UBM) consisted of $2,048$ Gaussian components, and the dimensionality of the i-vector space was 400. LDA was applied to reduce the dimensionality of the i-vectors to 150. The PLDA model was used for scoring BIBREF4. For the x-vector system, the feature-learning component was a 5-layer time-delay neural network (TDNN).",
        "type": "Document"
      },
      {
        "id": "30c56f33-9f60-4fb7-a6fe-0005148bf3c3",
        "metadata": {
          "vector_store_key": "1910.09399-3",
          "chunk_id": 15,
          "document_id": "1910.09399",
          "start_idx": 8740,
          "end_idx": 9570
        },
        "page_content": "Three types of approaches for image generation are reviewed, including direct methods (single generator and discriminator), hierarchical methods (two or more generator-discriminator pairs, each with a different goal), and iterative methods (each generator-discriminator pair generates a gradually higher-resolution image). Following the introduction, BIBREF6 discusses methods for text-to-image and image-to-image synthesis, respectively, and also describes several evaluation metrics for synthetic images, including inception scores and Frechet Inception Distance (FID), and explains the significance of the discriminators acting as learned loss functions as opposed to fixed loss functions. Different from the above survey, which has a relatively broad scope in GANs, our objective is heavily focused on text-to-image synthesis.",
        "type": "Document"
      },
      {
        "id": "96576214-2c10-4049-ba11-db649d59677b",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 26,
          "document_id": "1809.04960",
          "start_idx": 15085,
          "end_idx": 15736
        },
        "page_content": "Unsupervised baseline models are as follows: TF-IDF (Lexical, Non-Neural) is an important unsupervised baseline. We use the concatenation of the title and the body as the query to retrieve the candidate comment set by means of the similarity of the tf-idf value. The model is trained on unpaired articles and comments, which is the same as our proposed model. LDA (Topic, Non-Neural) is a popular unsupervised topic model, which discovers the abstract \"topics\" that occur in a collection of documents. We train the LDA with the articles and comments in the training set. The model retrieves the comments by the similarity of the topic representations.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much is the BLEU score?",
    "llm_answer": "The answer is not found in the context.  While the text discusses BLEU scores, it does not give a specific numerical value for one.\n",
    "context": [
      {
        "id": "c4c9a0b7-b26f-43d1-bbb8-aef3a134a8e8",
        "metadata": {
          "vector_store_key": "1707.00110-3",
          "chunk_id": 20,
          "document_id": "1707.00110",
          "start_idx": 10541,
          "end_idx": 11175
        },
        "page_content": "BLEU scores are calculated on tokenized data using the multi-bleu.perl script in Moses. We decode using beam search with a beam size of 10 BIBREF18 . Table 1 shows the BLEU scores of our model on different sequence lengths while varying $K$ . This is a study of the trade-off between computational time and representational power. A large $K$ allows us to compute complex source representations, while a $K$ of 1 limits the source representation to a single vector. We can see that performance consistently increases with $K$ up to a point that depends on the data length, with longer sequences requiring more complex representations.",
        "type": "Document"
      },
      {
        "id": "a5ea3dc1-a54b-42a5-8186-7230f9607ca6",
        "metadata": {
          "vector_store_key": "2004.03744-0",
          "chunk_id": 36,
          "document_id": "2004.03744",
          "start_idx": 19491,
          "end_idx": 20280
        },
        "page_content": "This is encouraging, since it shows that one can obtain additional natural language explanations without sacrificing performance (and eventually even improving the label performance, however, future work is needed to conclude whether the difference $0.48\\%$ improvement in performance is statistically significant). Camburu BIBREF6 mentioned that the BLEU score was not an appropriate measure for the quality of explanations and suggested human evaluation instead. We therefore manually scored the relevance of 100 explanations that were generated when the model predicted correct labels. We found that only 20% of explanations were relevant. We highlight that the relevance of explanations is in terms of whether the explanation reflects ground-truth reasons supporting the correct label.",
        "type": "Document"
      },
      {
        "id": "e3d5c602-1b71-4778-b4e1-39dc857ecd54",
        "metadata": {
          "vector_store_key": "1809.08731-4",
          "chunk_id": 19,
          "document_id": "1809.08731",
          "start_idx": 10841,
          "end_idx": 11540
        },
        "page_content": "The score of a sentence $S$ is calculated as  $$\\text{NCE}(S) = \\tfrac{1}{|S|} \\ln (p_M(S))$$   (Eq. 22)  with $p_M(S)$ being the probability assigned to the sentence by a LM. We employ the same LMs as for SLOR, i.e., LMs trained on words (WordNCE) and WordPieces (WPNCE). Our next baseline is perplexity, which corresponds to the exponentiated cross-entropy:  $$\\text{PPL}(S) = \\exp (-\\text{NCE}(S))$$   (Eq. 24)  Due to its popularity, we also performed initial experiments with BLEU BIBREF17 . Its correlation with human scores was so low that we do not consider it in our final experiments. Following earlier work BIBREF2 , we evaluate our metrics using Pearson correlation with human judgments.",
        "type": "Document"
      },
      {
        "id": "e8c36a90-ced7-4c69-8a42-ff6bc56d97dc",
        "metadata": {
          "vector_store_key": "1707.00110-3",
          "chunk_id": 19,
          "document_id": "1707.00110",
          "start_idx": 10090,
          "end_idx": 10775
        },
        "page_content": "All models are implemented using TensorFlow based on the seq2seq implementation of BIBREF15 and trained on a single machine with a Nvidia K40m GPU. We use a 2-layer 256-unit, a bidirectional LSTM BIBREF16 encoder, a 2-layer 256-unit LSTM decoder, and 256-dimensional embeddings. For the attention baseline, we use the standard parametrized attention BIBREF2 . Dropout of 0.2 (0.8 keep probability) is applied to the input of each cell and we optimize using Adam BIBREF17 at a learning rate of 0.0001 and batch size 128. We train for at most 200,000 steps (see Figure 3 for sample learning curves). BLEU scores are calculated on tokenized data using the multi-bleu.perl script in Moses.",
        "type": "Document"
      },
      {
        "id": "8e832e78-8de6-485d-8b21-14bce2cd6e32",
        "metadata": {
          "vector_store_key": "1909.00578-2",
          "chunk_id": 7,
          "document_id": "1909.00578",
          "start_idx": 4260,
          "end_idx": 5076
        },
        "page_content": "The overall score for a contestant with respect to a specific $\\mathcal {Q}$ is the average of the manual scores assigned to the summaries generated by the contestant. Note that the DUC-04 shared task involved seven $\\mathcal {Q}$s, but some of them were found to be highly overlapping and were grouped into five in subsequent years BIBREF20. We address these five criteria and use DUC data from 2005 onwards in our experiments. In Sum-QE, each peer summary is converted into a sequence of token embeddings, consumed by an encoder $\\mathcal {E}$ to produce a (dense vector) summary representation $h$. Then, a regressor $\\mathcal {R}$ predicts a quality score $S_{\\mathcal {Q}}$ as an affine transformation of $h$: Non-linear regression could also be used, but a linear (affine) $\\mathcal {R}$ already performs well.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which datasets are used in experiments?",
    "llm_answer": "CUB, MSCOCO, Oxford-102, MNIST, CUB, Oxford, COCO, CIFAR-10, SQuAD, MCTest, CNN/Daily Mail, MS COCO, Bing, Flickr, AIDA-Train, Wikipedia, AIDA-B, AIDA-CoNLL.\n",
    "context": [
      {
        "id": "eac31fee-fbdc-4a80-be44-f2145c81562a",
        "metadata": {
          "vector_store_key": "1910.09399-3",
          "chunk_id": 106,
          "document_id": "1910.09399",
          "start_idx": 56647,
          "end_idx": 57443
        },
        "page_content": "In order to synthesize images from text descriptions, many frameworks have taken a minimalistic approach by creating small and background-less images BIBREF73. In most cases, the experiments were conducted on simple datasets, initially containing images of birds and flowers. BIBREF8 contributed to these data sets by adding corresponding natural language text descriptions to subsets of the CUB, MSCOCO, and Oxford-102 datasets, which facilitated the work on text-to-image synthesis for several papers released more recently. While most deep learning algorithms use MNIST BIBREF74 dataset as the benchmark, there are three main datasets that are commonly used for evaluation of proposed GAN models for text-to-image synthesis: CUB BIBREF75, Oxford BIBREF76, COCO BIBREF77, and CIFAR-10 BIBREF78.",
        "type": "Document"
      },
      {
        "id": "1e2bf238-b0a0-40af-9d8d-e6fea4ad79d0",
        "metadata": {
          "vector_store_key": "1910.09399-3",
          "chunk_id": 105,
          "document_id": "1910.09399",
          "start_idx": 56273,
          "end_idx": 57009
        },
        "page_content": "For example, image data captured from a variety of photo-ready devices, such as smart-phones, and online social media services opened the door to the analysis of large amounts of media datasets BIBREF70. The availability of large media datasets allow new frameworks and algorithms to be proposed and tested on real-world data. A summary of some reviewed methods and benchmark datasets used for validation is reported in Table TABREF43. In addition, the performance of different GANs with respect to the benchmark datasets and performance metrics is reported in Table TABREF48. In order to synthesize images from text descriptions, many frameworks have taken a minimalistic approach by creating small and background-less images BIBREF73.",
        "type": "Document"
      },
      {
        "id": "8ef64fd9-ae4b-416a-994a-f082e80e88e4",
        "metadata": {
          "vector_store_key": "1703.04617-2",
          "chunk_id": 2,
          "document_id": "1703.04617",
          "start_idx": 973,
          "end_idx": 1680
        },
        "page_content": "We then viewed and modelled different types of questions and the information shared among them as an adaptation problem and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results on our competitive baselines. Recent advance on reading comprehension and question answering has been closely associated with the availability of various datasets. BIBREF0 released the MCTest data consisting of 500 short, fictional open-domain stories and 2000 questions. The CNN/Daily Mail dataset BIBREF1 contains news articles for close style machine comprehension, in which only entities are removed and tested for comprehension.",
        "type": "Document"
      },
      {
        "id": "947a5cf6-1656-4ca3-827b-105c8344c1d6",
        "metadata": {
          "vector_store_key": "1910.11949-3",
          "chunk_id": 16,
          "document_id": "1910.11949",
          "start_idx": 8700,
          "end_idx": 9318
        },
        "page_content": "In particular, we use two types of datasets to train our models: A dataset that maps pictures with questions, and an open-domain conversation dataset. The details of the two datasets are as follows. We use MS COCO, Bing and Flickr datasets from BIBREF26 to train the model that generates questions. These datasets contain natural questions about images with the purpose of knowing more about the picture. As can be seen in the Figure FIGREF8, questions cannot be answered by only looking at the image. Each source contains 5,000 images with 5 questions per image, adding a total of 15,000 images with 75,000 questions.",
        "type": "Document"
      },
      {
        "id": "458b51fc-c0d4-482d-90b3-d4cefed66ac3",
        "metadata": {
          "vector_store_key": "1902.00330-3",
          "chunk_id": 37,
          "document_id": "1902.00330",
          "start_idx": 20853,
          "end_idx": 21565
        },
        "page_content": "We implement our models in Tensorflow and run experiments on 4 Tesla V100 GPU. We conduct experiments on several different types of public datasets including news and encyclopedia corpus. The training set is AIDA-Train and Wikipedia datasets, where AIDA-Train contains 18448 mentions and Wikipedia contains 25995 mentions. In order to compare with the previous methods, we evaluate our model on AIDA-B and other datasets. These datasets are well-known and have been used for the evaluation of most entity linking systems. The statistics of the datasets are shown in Table 1. AIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are new best results on standard benchmark?",
    "llm_answer": "BERT BIBREF11 achieved a 0.604 micro-F1 score.\n",
    "context": [
      {
        "id": "93869f9d-2b0b-48c2-849f-c03013e50b22",
        "metadata": {
          "vector_store_key": "1809.08731-0",
          "chunk_id": 23,
          "document_id": "1809.08731",
          "start_idx": 13092,
          "end_idx": 13822
        },
        "page_content": "Note that, since having a single reference is very common, this result is highly relevant for practical applications. Considering MSE, the top two metrics are still WordSLOR and WPSLOR, with a $0.008$ and, respectively, $0.002$ lower error than the third best metric, ROUGE-L-mult. Comparing WordSLOR and WPSLOR, we find no significant differences: $0.017$ for Pearson and $0.006$ for MSE. However, WPSLOR uses a more compact LM and, hence, has a shorter training time, since the vocabulary is smaller ( $16,000$ vs. $128,000$ tokens). Next, we find that WordNCE and WPNCE perform roughly on par with word-overlap metrics. This is interesting, since they, in contrast to traditional metrics, do not require reference compressions.",
        "type": "Document"
      },
      {
        "id": "1bffa3f8-bbbc-48c1-81fe-55fb01d9b256",
        "metadata": {
          "vector_store_key": "1910.11769-3",
          "chunk_id": 12,
          "document_id": "1910.11769",
          "start_idx": 7313,
          "end_idx": 8027
        },
        "page_content": "We masked out named entities with entity-type specific placeholders to reduce the chance of benchmark models utilizing named entities as a basis for classification. Benchmark results are shown in Table TABREF17. The dataset is approximately balanced after discarding the Surprise and Disgust classes. We report the average micro-F1 scores, with 5-fold cross validation for each technique. We provide a brief overview of each benchmark experiment below. Among all of the benchmarks, Bidirectional Encoder Representations from Transformers (BERT) BIBREF11 achieved the best performance with a 0.604 micro-F1 score. Overall, we observed that deep-learning based techniques performed better than lexical based methods.",
        "type": "Document"
      },
      {
        "id": "1b1f9986-e4da-44a7-a6cd-13931a730da6",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 38,
          "document_id": "1909.13375",
          "start_idx": 19979,
          "end_idx": 20678
        },
        "page_content": "For the large models we used a batch size of 3 with a learning rate of $5e^{-6}$ and trained for 5 epochs, except for the model without the single-span heads that was trained with a batch size of 2 for 7 epochs. F1 was used as our validation metric. All models were trained on a single GPU with 12-16GB of memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions.",
        "type": "Document"
      },
      {
        "id": "895b9934-11e7-438a-aedf-2435c41cdf73",
        "metadata": {
          "vector_store_key": "1905.08949-8",
          "chunk_id": 17,
          "document_id": "1905.08949",
          "start_idx": 9885,
          "end_idx": 10631
        },
        "page_content": "Table 1 presents a listing of the NQG corpora grouped by their cognitive level and answer type, along with their statistics. Among them, SQuAD was used by most groups as the benchmark to evaluate their NQG models. This provides a fair comparison between different techniques. However, it raises the issue that most NQG models work on factoid questions with answer as text span, leaving other types of QG problems less investigated, such as generating deep multi-choice questions. To overcome this, a wider variety of corpora should be benchmarked against in future NQG research. Although the datasets are commonly shared between QG and QA, it is not the case for evaluation: it is challenging to define a gold standard of proper questions to ask.",
        "type": "Document"
      },
      {
        "id": "6824b4f2-136e-4c26-a2a7-a10cf7d4c97e",
        "metadata": {
          "vector_store_key": "1810.00663-8",
          "chunk_id": 46,
          "document_id": "1810.00663",
          "start_idx": 25788,
          "end_idx": 26550
        },
        "page_content": "The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as can be seen in performance drops in Table TABREF28 for the new environments. Nonetheless, the insights from the previous section still hold: masking in the output layer and reordering the graph triplets tend to increase performance. Even though the results in Table TABREF28 suggest that there is room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How better is performance compared to competitive baselines?",
    "llm_answer": "Our model significantly outperforms baseline models.  The best baselines achieve 33.6% and 28.5% on Quasar-S and Quasar-T, respectively, while human performance is 50% and 60.6%.  Our model is also superior to heuristic models on Quasar-T.\n",
    "context": [
      {
        "id": "874d1312-9fdb-46fe-865b-ce75dcb445d6",
        "metadata": {
          "vector_store_key": "1812.07023-3",
          "chunk_id": 32,
          "document_id": "1812.07023",
          "start_idx": 17666,
          "end_idx": 18428
        },
        "page_content": "We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below: Our primary architectural differences over the baseline model are: not concatenating the question, answer pairs before encoding them, the auxiliary decoder module, and using the Time-Extended FiLM module for feature extraction. These, combined with using scheduled sampling and running hyperparameter optimization over the validation set to select hyperparameters, give us the observed performance boost. We observe that our models generate fairly relevant responses to questions in the dialogues, and models with audio-visual inputs respond to audio-visual questions (e.g. \u201cis there any voices or music ?\u201d) correctly more often.",
        "type": "Document"
      },
      {
        "id": "245d9a70-772c-4b81-b29d-8987a0ef16f8",
        "metadata": {
          "vector_store_key": "1707.03904-0",
          "chunk_id": 37,
          "document_id": "1707.03904",
          "start_idx": 20485,
          "end_idx": 21217
        },
        "page_content": "The GA model achieves $48.3\\%$ accuracy on the set of instances for which the answer is in context, however, a search accuracy of only $65\\%$ means its overall performance is lower. This can improve with improved retrieval. For Quasar-T, both the neural models significantly outperform the heuristic models, with BiDAF getting the highest F1 score of $28.5\\%$ . The best performing baselines, however, lag behind human performance by $16.4\\%$ and $32.1\\%$ for Quasar-S and Quasar-T respectively, indicating the strong potential for improvement. Interestingly, for human performance we observe that non-experts are able to match or beat the performance of experts when given access to the background corpus for searching the answers.",
        "type": "Document"
      },
      {
        "id": "47dfa681-6856-4951-9180-c2a5082d7d78",
        "metadata": {
          "vector_store_key": "1701.02877-4",
          "chunk_id": 48,
          "document_id": "1701.02877",
          "start_idx": 24702,
          "end_idx": 25448
        },
        "page_content": "To do this we study again Precision (P), Recall (R) and F1 metrics on size-normalised corpora (Table UID9 ), on original corpora (Tables \"RQ1: NER performance with Different Approaches\" and \"RQ1: NER performance with Different Approaches\" ), and we further test performance per genre in a separate table (Table 3 ). F1 scores over size-normalised corpora vary widely (Table UID9 ). For example, the SENNA scores range from 9.35% F1 (ACE UN) to 71.48% (CoNLL Test A). Lowest results are consistently observed for the ACE subcorpora, UMBC, and OntoNotes BC and WB. The ACE corpora are large and so may be more prone to non-uniformities emerging during downsampling; they also have special rules for some kinds of organisation which can skew results",
        "type": "Document"
      },
      {
        "id": "3699d425-11f4-45cc-aae5-6a93d3e32b86",
        "metadata": {
          "vector_store_key": "1707.03904-0",
          "chunk_id": 8,
          "document_id": "1707.03904",
          "start_idx": 4627,
          "end_idx": 5317
        },
        "page_content": "We evaluate Quasar against human testers, as well as several baselines ranging from na\u00efve heuristics to state-of-the-art machine readers. The best performing baselines achieve $33.6\\%$ and $28.5\\%$ on Quasar-S and Quasar-T, while human performance is $50\\%$ and $60.6\\%$ respectively. For the automatic systems, we see an interesting tension between searching and reading accuracies \u2013 retrieving more documents in the search phase leads to a higher coverage of answers, but makes the comprehension task more difficult. We also collect annotations on a subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short.",
        "type": "Document"
      },
      {
        "id": "1bffa3f8-bbbc-48c1-81fe-55fb01d9b256",
        "metadata": {
          "vector_store_key": "1910.11769-3",
          "chunk_id": 12,
          "document_id": "1910.11769",
          "start_idx": 7313,
          "end_idx": 8027
        },
        "page_content": "We masked out named entities with entity-type specific placeholders to reduce the chance of benchmark models utilizing named entities as a basis for classification. Benchmark results are shown in Table TABREF17. The dataset is approximately balanced after discarding the Surprise and Disgust classes. We report the average micro-F1 scores, with 5-fold cross validation for each technique. We provide a brief overview of each benchmark experiment below. Among all of the benchmarks, Bidirectional Encoder Representations from Transformers (BERT) BIBREF11 achieved the best performance with a 0.604 micro-F1 score. Overall, we observed that deep-learning based techniques performed better than lexical based methods.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What 6 language pairs is experimented on?",
    "llm_answer": "English, French, Spanish, Russian, Italian, and German.\n",
    "context": [
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      },
      {
        "id": "0a7d831d-b88f-470e-931c-6ce545b154ab",
        "metadata": {
          "vector_store_key": "1910.05456-2",
          "chunk_id": 45,
          "document_id": "1910.05456",
          "start_idx": 25600,
          "end_idx": 26346
        },
        "page_content": "To name some specific examples, marques2011study investigated the effect of a stay abroad on Spanish L2 acquisition, including learning of its verbal morphology in English speakers. jia2003acquisition studied how Mandarin Chinese-speaking children learned the English plural morpheme. nicoladis2012young studied the English past tense acquisition in Chinese\u2013English and French\u2013English bilingual children. They found that, while both groups showed similar production accuracy, they differed slightly in the type of errors they made. Also considering the effect of the native language explicitly, yang2004impact investigated the acquisition of the tense-aspect system in an L2 for speakers of a native language which does not mark tense explicitly.",
        "type": "Document"
      },
      {
        "id": "d67d54b6-3a64-4e64-8965-111710b50cd8",
        "metadata": {
          "vector_store_key": "1910.05456-7",
          "chunk_id": 40,
          "document_id": "1910.05456",
          "start_idx": 21931,
          "end_idx": 22624
        },
        "page_content": "A limitation of our work is that we only include languages that are written in Latin script. An interesting question for future work might, thus, regard the effect of disjoint L1 and L2 alphabets. Furthermore, none of the languages included in our study exhibits a templatic morphology. We make this choice because data for templatic languages is currently mostly available in non-Latin alphabets. Future work could investigate languages with templatic morphology as source or target languages, if needed by mapping the language's alphabet to Latin characters. Finally, while we intend to choose a diverse set of languages for this study, our overall number of languages is still rather small.",
        "type": "Document"
      },
      {
        "id": "9210eab3-d084-4395-859a-51482ae8da57",
        "metadata": {
          "vector_store_key": "1910.04269-1",
          "chunk_id": 34,
          "document_id": "1910.04269",
          "start_idx": 19227,
          "end_idx": 19993
        },
        "page_content": "2D attention models focused on the important features extracted by convolutional layers and bi-directional GRU captured the temporal features. Several of the spoken languages in Europe belong to the Indo-European family. Within this family, the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Katze in German and Ciao in Italian becomes Chao in Spanish.",
        "type": "Document"
      },
      {
        "id": "c835fe0c-f240-4e5f-96f5-494421121339",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 6,
          "document_id": "1910.04269",
          "start_idx": 3498,
          "end_idx": 4224
        },
        "page_content": "The model uses a cyclic learning rate where learning rate increases and then decreases linearly. Maximum learning rate for a cycle is set by finding the optimal learning rate using fastai BIBREF15 library. The model classified six languages \u2013 English, French, Spanish, Russian, Italian and German \u2013 and achieving an accuracy of 89.0%. Gazeau et al. BIBREF16 in his research showed how Neural Networks, Support Vector Machine and Hidden Markov Model (HMM) can be used to identify French, English, Spanish and German. Dataset was prepared using voice samples from Youtube News BIBREF17and VoxForge BIBREF6 datasets. Hidden Markov models convert speech into a sequence of vectors, was used to capture temporal features in speech.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they enrich the positional embedding with length information",
    "llm_answer": "The first variant replaces the variable `pos` in positional encoding equations with the difference `len - pos`, where `len` is the sentence length.  This new length encoding is only in the decoder network.\n",
    "context": [
      {
        "id": "92d6fe6a-c05a-4cfc-855a-5ffa54b925e9",
        "metadata": {
          "vector_store_key": "1910.10408-3",
          "chunk_id": 9,
          "document_id": "1910.10408",
          "start_idx": 4570,
          "end_idx": 5338
        },
        "page_content": "Thus, in order to enrich input embeddings in source and target with positional information, they are summed with positional vectors of the same dimension $d$, which are computed with the following trigonometric encoding ($\\text{PE}$): for $i=1,\\ldots ,d/2$. Recently, an extension of the positional encoding BIBREF11 was proposed to model the output length for text summarization. The goal is achieved by computing the distance from every position to the end of the sentence. The new length encoding is present only in the decoder network as an additional vector summed to the input embedding. The authors proposed two different variants. The first variant replaces the variable pos in equations (1-2) with the difference $len - pos$, where len is the sentence length.",
        "type": "Document"
      },
      {
        "id": "22b913b3-266c-40fe-adc1-813047a68289",
        "metadata": {
          "vector_store_key": "1910.12618-6",
          "chunk_id": 30,
          "document_id": "1910.12618",
          "start_idx": 16308,
          "end_idx": 16981
        },
        "page_content": "In order to obtain the embedding, the texts are first converted into a sequence of integers: each word is given a number ranging from 1 to $V$, where $V$ is the vocabulary size (0 is used for padding or unknown words in the test set). One must then calculate the maximum sequence length $S$, and sentences of length shorter than $S$ are then padded by zeros. During the training process of the network, for each word a $q$ dimensional real-valued vector representation is calculated simultaneously to the rest of the weights of the network. Ergo a sentence of $S$ words is translated into a sequence of $S$ $q$-sized vectors, which is then fed into a recurrent neural unit.",
        "type": "Document"
      },
      {
        "id": "5ef5004c-435d-48de-b882-d58a8abefb9f",
        "metadata": {
          "vector_store_key": "2001.00137-2",
          "chunk_id": 12,
          "document_id": "2001.00137",
          "start_idx": 7417,
          "end_idx": 8073
        },
        "page_content": "In our case, since our data are formed of single sentences, the segment is 1 until the first `[SEP]' character appears (indicating segment A) and then it becomes 0 (segment B). The position embedding layer, as the name indicates, adds information related to the token's position in the sentence. This prepares the data to be considered by the layers of vanilla bidirectional transformers, which outputs a hidden embedding that can be used by our novel layers of denoising transformers. Although BERT has shown to perform better than other baseline models when handling incomplete data, it is still not enough to completely and efficiently handle such data.",
        "type": "Document"
      },
      {
        "id": "b0174348-59ae-44c0-8b01-9a217e4013dc",
        "metadata": {
          "vector_store_key": "1909.08859-0",
          "chunk_id": 15,
          "document_id": "1909.08859",
          "start_idx": 8252,
          "end_idx": 8848
        },
        "page_content": "Then these embeddings are passed first to a multilayer perceptron (MLP) and then its outputs are fed to a BiLSTM. We then form a matrix $\\mathbf {Q}^{\\prime } \\in \\mathbb {R}^{2d \\times M}$ for the question by concatenating the cell states of the BiLSTM. For the visual ordering task, to represent the sequence of images in the answer with a single vector, we additionally use a BiLSTM and define the answering embedding by the summation of the cell states of the BiLSTM. Finally, for all tasks, these computations produce answer embeddings denoted by $\\mathbf {a} \\in \\mathbb {R}^{2d \\times 1}$.",
        "type": "Document"
      },
      {
        "id": "3f7a2966-e213-4322-8885-d6c8079f4ed0",
        "metadata": {
          "vector_store_key": "1807.03367-4",
          "chunk_id": 32,
          "document_id": "1807.03367",
          "start_idx": 17876,
          "end_idx": 18717
        },
        "page_content": "In addition, we embed action INLINEFORM6 into a INLINEFORM7 -dimensional embedding INLINEFORM8 via a look-up table INLINEFORM9 . We experiment with three types of communication channel. paragraph4 0.1ex plus0.1ex minus.1ex-1em Continuous vectors The tourist has access to observations of several time steps, whose order is important for accurate localization. Because summing embeddings is order-invariant, we introduce a sum over positionally-gated embeddings, which, conditioned on time step INLINEFORM0 , pushes embedding information into the appropriate dimensions. More specifically, we generate an observation message INLINEFORM1 , where INLINEFORM2 is a learned gating vector for time step INLINEFORM3 . In a similar fashion, we produce action message INLINEFORM4 and send the concatenated vectors INLINEFORM5 as message to the guide.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they condition the output to a given target-source class?",
    "llm_answer": "By adding conditions to the training samples, such that the GAN is trained with respect to the underlying conditions.  The condition vector, which can be text, is fed to both the generator and discriminator.\n",
    "context": [
      {
        "id": "a5f189c0-1a12-4086-9853-7c05b41c44c4",
        "metadata": {
          "vector_store_key": "1910.05608-0",
          "chunk_id": 16,
          "document_id": "1910.05608",
          "start_idx": 8804,
          "end_idx": 9441
        },
        "page_content": "The output is the probability of each class. The dataset in this HSD task is really imbalance. Clean class dominates with 91.5%, offensive class takes 5% and the rest belongs to hate class with 3.5%. To make model being able to learn with this imbalance data, we inject class weight to the loss function with the corresponding ratio (clean, offensive, hate) is $(0.09, 0.95, 0.96)$. Formular DISPLAY_FORM17 is the loss function apply for all models in our system. $w_i$ is the class weight, $y_i$ is the ground truth and $\\hat{y}_i$ is the output of the model. If the class weight is not set, we find that model cannot adjust parameters.",
        "type": "Document"
      },
      {
        "id": "62ed39f2-9f61-4de6-adc2-27401f7d20b5",
        "metadata": {
          "vector_store_key": "1910.09399-0",
          "chunk_id": 35,
          "document_id": "1910.09399",
          "start_idx": 18675,
          "end_idx": 19543
        },
        "page_content": "If the model in Figure FIGREF14 was trained with the same set of real data (red birds) but the condition text was \"Yellow fish\", the generator would learn to create images of red birds when conditioned with the text \"Yellow fish\". Note that the condition vector in cGAN can come in many forms, such as texts, not just limited to the class label. Such a unique design provides a direct solution to generate images conditioned by predefined specifications. As a result, cGAN has been used in text-to-image synthesis since the very first day of its invention although modern approaches can deliver much better text-to-image synthesis results. black In order to generate images from text, one simple solution is to employ the conditional GAN (cGAN) designs and add conditions to the training samples, such that the GAN is trained with respect to the underlying conditions.",
        "type": "Document"
      },
      {
        "id": "bf356315-a1a5-40fc-8a86-fcf686a8fa58",
        "metadata": {
          "vector_store_key": "1910.09399-9",
          "chunk_id": 25,
          "document_id": "1910.09399",
          "start_idx": 13597,
          "end_idx": 14335
        },
        "page_content": "Therefore, given a true image $x$, the ideal output from the discriminator $D_{\\theta _d}(x)$ would be 1. Given a fake image generated from the generator $G_{\\theta _g}(z)$, the ideal prediction from the discriminator $D_{\\theta _d}(G_{\\theta _g}(z))$ would be 0, indicating the sample is a fake image. Following the above definition, the $\\min \\max $ objective function in Eq. (DISPLAY_FORM10) aims to learn parameters for the discriminator ($\\theta _d$) and generator ($\\theta _g$) to reach an optimization goal: The discriminator intends to differentiate true vs. fake images with maximum capability $\\max _{\\theta _d}$ whereas the generator intends to minimize the difference between a fake image vs. a true image $\\min _{\\theta _g}$.",
        "type": "Document"
      },
      {
        "id": "5b5b202e-fd1f-4622-a39c-da589815d00e",
        "metadata": {
          "vector_store_key": "1910.09399-9",
          "chunk_id": 34,
          "document_id": "1910.09399",
          "start_idx": 17997,
          "end_idx": 18675
        },
        "page_content": "Whereas the original GAN was trained only with samples from the data distribution, resulting in the generated sample reflecting the general data distribution, cGAN enables directing the model to generate more tailored outputs. In Figure FIGREF14, the condition vector is the class label (text string) \"Red bird\", which is fed to both the generator and discriminator. It is important, however, that the condition vector is related to the real data. If the model in Figure FIGREF14 was trained with the same set of real data (red birds) but the condition text was \"Yellow fish\", the generator would learn to create images of red birds when conditioned with the text \"Yellow fish\".",
        "type": "Document"
      },
      {
        "id": "ec737405-3378-4171-a942-1f4accc4ced6",
        "metadata": {
          "vector_store_key": "1809.04686-4",
          "chunk_id": 10,
          "document_id": "1809.04686",
          "start_idx": 5307,
          "end_idx": 5990
        },
        "page_content": "Formally, given the context set C, we extract a sentence vector $\\mathbf {q}$ in three steps, using three networks, (1) pre-pooling feed-forward network $f_{pre}$ , (2) pooling network $f_{pool}$ and (3) post-pooling feed-forward network $f_{post}$ , $\n\\mathbf {q} = f_{post}( f_{pool} ( f_{pre} (\\textbf {C}) ) ).\n$   Finally, given the sentence vector $\\mathbf {q}$ , a class label $y$ is predicted by employing a softmax function. We evaluate the proposed method on three common NLP tasks: Amazon Reviews, SST and SNLI. We utilize parallel data to train our multilingual NMT system, as detailed below. For the MT task, we use the WMT 2014 En $\\leftrightarrow $ Fr parallel corpus.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is this library implemented into Torch or is framework agnostic?",
    "llm_answer": "The library is implemented into PyTorch.\n",
    "context": [
      {
        "id": "9cdf5e59-e3ed-46b1-a9a0-6406bccef0cb",
        "metadata": {
          "vector_store_key": "2002.00876-1",
          "chunk_id": 22,
          "document_id": "2002.00876",
          "start_idx": 12374,
          "end_idx": 13173
        },
        "page_content": "We note that many of the terms necessary in the case-study can be computed with variant semirings, negating the need for specialized algorithms. Torch-Struct aims for computational and memory efficiency. Implemented naively, dynamic programming algorithms in Python are prohibitively slow. As such Torch-Struct provides key primitives to help batch and vectorize these algorithms to take advantage of GPU computation and to minimize the overhead of backpropagating through chart-based dynamic programmming. Figure FIGREF17 shows the impact of these optimizations on the core algorithms. The commutative properties of semiring algorithms allow flexibility in the order in which we compute $A(\\ell )$. Typical implementations of dynamic programming algorithms are serial in the length of the sequence.",
        "type": "Document"
      },
      {
        "id": "97ee039d-d1d1-4ff6-ae82-c47313626ec6",
        "metadata": {
          "vector_store_key": "2002.00876-1",
          "chunk_id": 3,
          "document_id": "2002.00876",
          "start_idx": 1999,
          "end_idx": 2779
        },
        "page_content": "With this challenge in mind, we introduce Torch-Struct with three specific contributions: Modularity: models are represented as distributions with a standard flexible API integrated into a deep learning framework. Completeness: a broad array of classical algorithms are implemented and new models can easily be added in Python. Efficiency: implementations target computational/memory efficiency for GPUs and the backend includes extensions for optimization. In this system description, we first motivate the approach taken by the library, then present a technical description of the methods used, and finally present several example use cases. Several software libraries target structured prediction. Optimization tools, such as SVM-struct BIBREF18, focus on parameter estimation.",
        "type": "Document"
      },
      {
        "id": "09159139-d93a-4cb9-ac5f-1763ac0a082e",
        "metadata": {
          "vector_store_key": "2002.00876-1",
          "chunk_id": 12,
          "document_id": "2002.00876",
          "start_idx": 6537,
          "end_idx": 7307
        },
        "page_content": "Figure FIGREF11 demonstrates this API for a binary tree CRF over an ordered sequence, such as $p(z \\ | \\ y ;\\phi )$ from the previous section. The distribution takes in log-potentials $\\ell $ which score each possible span in the input. The distribution converts these to probabilities of a specific tree. This distribution can be queried for predicting over the set of trees, sampling a tree for model structure, or even computing entropy over all trees. Table TABREF2 shows all of the structures and distributions implemented in Torch-Struct. While each is internally implemented using different specialized algorithms and optimizations, from the user's perspective they all utilize the same external distributional API, and pass a generic set of distributional tests.",
        "type": "Document"
      },
      {
        "id": "c6581f77-c5f5-41d2-82ce-85e5245f3f4e",
        "metadata": {
          "vector_store_key": "2002.00876-1",
          "chunk_id": 5,
          "document_id": "2002.00876",
          "start_idx": 3264,
          "end_idx": 4151
        },
        "page_content": "Most ambitiously, inference libraries such as Dyna BIBREF24 allow for declarative specifications of dynamic programming algorithms to support inference for generic algorithms. Torch-Struct takes a different approach and integrates a library of optimized structured distributions into a vectorized deep learning system. We begin by motivating this approach with a case study. While structured prediction is traditionally presented at the output layer, recent applications have deployed structured models broadly within neural networks BIBREF15, BIBREF25, BIBREF16. Torch-Struct aims to encourage this general use case. To illustrate, we consider a latent tree model. ListOps BIBREF26 is a dataset of mathematical functions. Each data point consists of a prefix expression $x$ and its result $y$, e.g. Models such as a flat RNN will fail to capture the hierarchical structure of this task.",
        "type": "Document"
      },
      {
        "id": "b3db22ea-ed25-4a91-9473-973a8b8729ea",
        "metadata": {
          "vector_store_key": "2002.00876-1",
          "chunk_id": 28,
          "document_id": "2002.00876",
          "start_idx": 15710,
          "end_idx": 16646
        },
        "page_content": "We present Torch-Struct, a library for deep structured prediction. The library achieves modularity through its adoption of a generic distributional API, completeness by utilizing CRFs and semirings to make it easy to add new algorithms, and efficiency through core optimizations to vectorize important dynamic programming steps. In addition to the problems discussed so far, Torch-Struct also includes several other example implementations including supervised dependency parsing with BERT, unsupervised tagging, structured attention, and connectionist temporal classification (CTC) for speech. The full library is available at https://github.com/harvardnlp/pytorch-struct. In the future, we hope to support research and production applications employing structured models. We also believe the library provides a strong foundation for building generic tools for interpretablity, control, and visualization through its probabilistic API.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does this compare to traditional calibration methods like Platt Scaling?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "467ee1c6-fd3f-41b6-8362-12b11ee9ed02",
        "metadata": {
          "vector_store_key": "1910.04269-1",
          "chunk_id": 38,
          "document_id": "1910.04269",
          "start_idx": 21056,
          "end_idx": 21639
        },
        "page_content": "This method is able to bypass the computational overhead of conventional approaches which depend on generation of spectrograms as a necessary pre-procesing step. We were able to achieve an accauracy of 93.7% using this technique. Next, we discussed the enhancement in performance of 2D-ConvNet using mixup augmentation, which is a recently developed technique to prevent over\ufb01tting on test data. This approach achieved an accuracy of 95.4%. We also analysed how attention mechanism and recurrent layers impact the performance of networks. This approach achieved an accuracy of 95.0%.",
        "type": "Document"
      },
      {
        "id": "24a5dd6f-8910-4dac-b7c6-c660f7a3cd4d",
        "metadata": {
          "vector_store_key": "2003.07433-7",
          "chunk_id": 48,
          "document_id": "2003.07433",
          "start_idx": 26016,
          "end_idx": 26710
        },
        "page_content": "Fig FIGREF30 illustrates that if we change the number of training samples (%), LAXARY models outperforms Coppersmith et. al. proposed model under any condition. In terms of intensity, Coppersmith et. al. totally fails to provide any idea however LAXARY provides extremely accurate measures of intensity estimation for PTSD sufferers (as shown in Fig FIGREF31) which can be explained simply providing LAXARY model filled out survey details. Table TABREF29 shows the details of accuracies of both PTSD detection and intensity estimation. Fig FIGREF32 shows the classification accuracy changes over the training sample sizes for each survey which shows that DOSPERT scale outperform other surveys.",
        "type": "Document"
      },
      {
        "id": "2deb90c4-8ef7-4c01-8406-58c21f5d3722",
        "metadata": {
          "vector_store_key": "1909.00430-2",
          "chunk_id": 32,
          "document_id": "1909.00430",
          "start_idx": 17329,
          "end_idx": 18037
        },
        "page_content": "This achieves the best results, and surpasses also the semi-supervised BIBREF35 baseline on accuracy, and matching it on F1. We report significance tests for the robustness of the method under random parameter initialization. Our reported numbers are averaged over five random initialization. Since the datasets are unbalanced w.r.t the label distribution, we report both accuracy and macro-F1. The XR training is also more stable than the other semi-supervised baselines, achieving substantially lower standard deviations across different runs. In each experiment in this section we estimate the proportions using the SemEval-2015 train set. How does the XR training scale with the amount of unlabeled data?",
        "type": "Document"
      },
      {
        "id": "594fe7e8-5314-4485-a855-1108b86ee8e8",
        "metadata": {
          "vector_store_key": "1703.09684-9",
          "chunk_id": 10,
          "document_id": "1703.09684",
          "start_idx": 5508,
          "end_idx": 6196
        },
        "page_content": "For related reasons, major benchmarks released in the last decade do not use simple accuracy for evaluating image recognition and related computer vision tasks, but instead use metrics such as mean-per-class accuracy that compensates for unbalanced categories. For example, on Caltech-101 BIBREF13 , even with balanced training data, simple accuracy fails to address the fact that some categories were much easier to classify than others (e.g., faces and planes were easy and also had the largest number of test images). Mean per-class accuracy compensates for this by requiring a system to do well on each category, even when the amount of test instances in categories vary considerably.",
        "type": "Document"
      },
      {
        "id": "e34ec430-93a0-4067-875a-f939ddf574fd",
        "metadata": {
          "vector_store_key": "1703.09684-9",
          "chunk_id": 9,
          "document_id": "1703.09684",
          "start_idx": 5278,
          "end_idx": 5981
        },
        "page_content": "For example, on COCO-VQA, improving accuracy on `Is/Are' questions by 15% will increase overall accuracy by over 5%, but answering all `Why/Where' questions correctly will increase accuracy by only 4.1% BIBREF10 . Due to the inability of the existing evaluation metrics to properly address these biases, algorithms trained on these datasets learn to exploit these biases, resulting in systems that work poorly when deployed in the real-world. For related reasons, major benchmarks released in the last decade do not use simple accuracy for evaluating image recognition and related computer vision tasks, but instead use metrics such as mean-per-class accuracy that compensates for unbalanced categories.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What's the input representation of OpenIE tuples into the model?",
    "llm_answer": "One-hot encoded vectors of dimensionality determined by the number of nodes and edges in the graph.\n",
    "context": [
      {
        "id": "632b3f84-cbf8-4695-a342-e9f9bd8f8c97",
        "metadata": {
          "vector_store_key": "1704.05572-4",
          "chunk_id": 17,
          "document_id": "1704.05572",
          "start_idx": 9000,
          "end_idx": 9655
        },
        "page_content": "Since an Open IE tuple expresses a fact about the tuple's subject, we require the subject to be active in the support graph. To avoid issues such as (Planet; orbit; Sun) matching the sample question in the introduction (\u201cWhich object $\\ldots $ orbits around a planet\u201d), we also add an ordering constraint (third group in Table 1 ). Its worth mentioning that TupleInf only combines parallel evidence i.e. each tuple must connect words in the question to the answer choice. For reliable multi-hop reasoning using OpenIE tuples, we can add inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates.",
        "type": "Document"
      },
      {
        "id": "8b2489d4-7cfc-499f-ab64-0e60a9581038",
        "metadata": {
          "vector_store_key": "1810.00663-1",
          "chunk_id": 21,
          "document_id": "1810.00663",
          "start_idx": 12269,
          "end_idx": 13064
        },
        "page_content": "The model consists of six layers: Embed layer: The model first encodes each word and symbol in the input sequences INLINEFORM0 and INLINEFORM1 into fixed-length representations. The instructions INLINEFORM2 are embedded into a 100-dimensional pre-trained GloVe vector BIBREF24 . Each of the triplet components, INLINEFORM3 , INLINEFORM4 , and INLINEFORM5 of the graph INLINEFORM6 , are one-hot encoded into vectors of dimensionality INLINEFORM7 , where INLINEFORM8 and INLINEFORM9 are the number of nodes and edges in INLINEFORM10 , respectively. Encoder layer: The model then uses two bidirectional Gated Recurrent Units (GRUs) BIBREF25 to independently process the information from INLINEFORM0 and INLINEFORM1 , and incorporate contextual cues from the surrounding embeddings in each sequence.",
        "type": "Document"
      },
      {
        "id": "00527c67-9526-4166-889d-88a6be18fc57",
        "metadata": {
          "vector_store_key": "1905.13413-1",
          "chunk_id": 0,
          "document_id": "1905.13413",
          "start_idx": 0,
          "end_idx": 704
        },
        "page_content": "Open information extraction (IE, sekine2006demand, Banko:2007:OIE) aims to extract open-domain assertions represented in the form of $n$ -tuples (e.g., was born in; Barack Obama; Hawaii) from natural language sentences (e.g., Barack Obama was born in Hawaii). Open IE started from rule-based BIBREF0 and syntax-driven systems BIBREF1 , BIBREF2 , and recently has used neural networks for supervised learning BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 . A key step in open IE is confidence modeling, which ranks a list of candidate extractions based on their estimated quality. This is important for downstream tasks, which rely on trade-offs between the precision and recall of extracted assertions.",
        "type": "Document"
      },
      {
        "id": "72351bc1-45a4-4c13-8e08-e78201bdfa0c",
        "metadata": {
          "vector_store_key": "1810.00663-1",
          "chunk_id": 22,
          "document_id": "1810.00663",
          "start_idx": 12639,
          "end_idx": 13345
        },
        "page_content": "Encoder layer: The model then uses two bidirectional Gated Recurrent Units (GRUs) BIBREF25 to independently process the information from INLINEFORM0 and INLINEFORM1 , and incorporate contextual cues from the surrounding embeddings in each sequence. The outputs of the encoder layer are the matrix INLINEFORM2 for the navigational commands and the matrix INLINEFORM3 for the behavioral graph, where INLINEFORM4 is the hidden size of each GRU, INLINEFORM5 is the number of words in the instruction INLINEFORM6 , and INLINEFORM7 is the number of triplets in the graph INLINEFORM8 . Attention layer: Matrices INLINEFORM0 and INLINEFORM1 generated by the encoder layer are combined using an attention mechanism.",
        "type": "Document"
      },
      {
        "id": "5ce9a194-dab5-496b-b20b-b9b425bd8763",
        "metadata": {
          "vector_store_key": "1704.05572-1",
          "chunk_id": 18,
          "document_id": "1704.05572",
          "start_idx": 9348,
          "end_idx": 10087
        },
        "page_content": "For reliable multi-hop reasoning using OpenIE tuples, we can add inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules for the Science domain is an open problem and potential avenue of future work. Comparing our method with two state-of-the-art systems for 4th and 8th grade science exams, we demonstrate that (a) TupleInf with only automatically extracted tuples significantly outperforms TableILP with its original curated knowledge as well as with additional tuples, and (b) TupleInf's complementary approach to IR leads to an improved ensemble. Numbers in bold indicate statistical significance based on the Binomial exact test BIBREF20 at $p=0.05$ .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is CRWIZ already used for data collection, what are the results?",
    "llm_answer": "Yes, CRWIZ is used for data collection.  A dataset was collected and compared to one collected in a controlled lab setting.  The advantages/disadvantages of both approaches are discussed.\n",
    "context": [
      {
        "id": "bdb09364-b81e-4f9b-82af-de402db31dd6",
        "metadata": {
          "vector_store_key": "2003.05995-2",
          "chunk_id": 7,
          "document_id": "2003.05995",
          "start_idx": 4139,
          "end_idx": 4932
        },
        "page_content": "Our contributions are as follows: The release of a platform for the CRWIZ Intelligent Wizard Interface to allow for the collection of dialogue data for longer complex tasks, by providing a dynamic selection of relevant dialogue acts. A survey of existing datasets and data collection platforms, with a comparison to the CRWIZ data collection for Wizarded crowdsourced data in task-based interactions. Table TABREF3 gives an overview of prior work and datasets. We report various factors to compare to the CRWIZ dataset corresponding to columns in Table TABREF3: whether or not the person was aware they were talking to a bot; whether each dialogue had a single or multiple participants per role; whether the data collection was crowdsourced; and the modality of the interaction and the domain.",
        "type": "Document"
      },
      {
        "id": "1b3ac1f0-a0f4-464a-a078-21784d8ef583",
        "metadata": {
          "vector_store_key": "2003.05995-2",
          "chunk_id": 6,
          "document_id": "2003.05995",
          "start_idx": 3390,
          "end_idx": 4139
        },
        "page_content": "In this paper, we provide a brief survey of existing datasets and describe the CRWIZ framework for pairing crowdworkers and having half of them acting as Wizards by limiting their dialogue options only to relevant and plausible ones, at any one point in the interaction. We then perform a data collection and compare our dataset to a similar dataset collected in a more controlled lab setting with a single Wizard BIBREF4 and discuss the advantages/disadvantages of both approaches. Finally, we present future work. Our contributions are as follows: The release of a platform for the CRWIZ Intelligent Wizard Interface to allow for the collection of dialogue data for longer complex tasks, by providing a dynamic selection of relevant dialogue acts.",
        "type": "Document"
      },
      {
        "id": "b1127f8f-2145-44b5-9671-4ac5e4f58ed2",
        "metadata": {
          "vector_store_key": "2003.05995-2",
          "chunk_id": 3,
          "document_id": "2003.05995",
          "start_idx": 1941,
          "end_idx": 2670
        },
        "page_content": "We present the CRWIZ Intelligent Wizard Interface that enables a crowdsourced Wizard to make intelligent, relevant choices without such intensive training by providing a restricted list of valid and relevant dialogue task actions, which changes dynamically based on the context, as the interaction evolves. Prior crowdsourced wizarded data collections have divided the dialogue up into turns and each worker's job consists of one turn utterance generation given a static dialogue context, as in the MultiWoZ dataset BIBREF2. However, this can limit naturalness of the dialogues by restricting forward planning, collaboration and use of memory that humans use for complex multi-stage tasks in a shared dynamic environment/context.",
        "type": "Document"
      },
      {
        "id": "6fda761a-bf56-4e4e-a4d9-cc7230be8f4c",
        "metadata": {
          "vector_store_key": "2003.05995-2",
          "chunk_id": 2,
          "document_id": "2003.05995",
          "start_idx": 1224,
          "end_idx": 1941
        },
        "page_content": "However, these have traditionally not been scalable because of the scarcity of Wizard experts or the expense to train up workers. This was the situation with an initial study reported in BIBREF4, which was conducted in a traditional lab setting and where the Wizard (an academic researcher) had to learn, through training and reading manuals, how best to perform operations in our domain of emergency response. We present the CRWIZ Intelligent Wizard Interface that enables a crowdsourced Wizard to make intelligent, relevant choices without such intensive training by providing a restricted list of valid and relevant dialogue task actions, which changes dynamically based on the context, as the interaction evolves.",
        "type": "Document"
      },
      {
        "id": "02b88972-eecb-436a-8271-91b70e9f6dcc",
        "metadata": {
          "vector_store_key": "2003.05995-2",
          "chunk_id": 24,
          "document_id": "2003.05995",
          "start_idx": 12363,
          "end_idx": 13042
        },
        "page_content": "The advantage of the CRWIZ framework is that it can easily be adapted to different domains and procedures by simply modifying the dialogue states loaded at initialisation. These files are in YAML format and have a simple structure that defines their NLG templates (the FSM will pick one template at random if there is more than one) and the states that it can transition to. Note, that some further modifications may be necessary if the scenario is a slot-filling dialogue requiring specific information at various stages. Once the dialogue between the participants finishes, they receive a code in the chat, which can then be submitted to the crowdsourcing platform for payment.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What contextual features are used?",
    "llm_answer": "The contextual feature vector is defined by the frequency of a token in the whole corpus and the frequency of contextual keywords from windowed portions of text centered on the token.  Contextual keywords are automatically extracted and satisfy specific conditions related to manually annotated IOCs and a frequency lower bound.\n",
    "context": [
      {
        "id": "7b30f882-a76c-4c96-aed4-d450c8016765",
        "metadata": {
          "vector_store_key": "1907.02636-3",
          "chunk_id": 37,
          "document_id": "1907.02636",
          "start_idx": 20024,
          "end_idx": 20779
        },
        "page_content": "The proposed model provides an intuitive way to inspect the contextual information of each given token. As described in Section SECREF14 , we initialize the contextual features of each given token using the automatically extracted contextual keywords and jointly learn them during the process of training with the whole ANN model. To prove the effectiveness of the contextual features, we visualize the learned weights martix of each contextual keyword of contextual feature and show several examples in Fig. FIGREF28 . Each row of the matrix in each plot indicates the weights of contextual keywords for the given tokens. From this we see which contextual keyword were considered more important to represent the contextual information of the given token.",
        "type": "Document"
      },
      {
        "id": "a52861f8-f1a0-4832-8a76-3d8d23e0d580",
        "metadata": {
          "vector_store_key": "1907.02636-3",
          "chunk_id": 19,
          "document_id": "1907.02636",
          "start_idx": 10566,
          "end_idx": 11600
        },
        "page_content": "For example, a human user can infer that the word \u201cntdll.exe\u201d is the name of a malicious file on the basis of the words \u201cdownload\u201d and \u201ccompromised\u201d from the text shown in Fig. FIGREF1 . By analyzing the whole corpus, it is interesting that malicious file names tends to co-occur with words such as \"download\", \"malware\", \"malicious\", etc. In this work, we consider words that can indicate the characteristics of the neighbor words as contextual keywords and develop an approach to generate features from the automatically extracted contextual keywords. Taking the above into account, we introduce the contextual feature vector INLINEFORM0 for a given input token INLINEFORM1 , where the INLINEFORM2 element of INLINEFORM3 is defined as follows: DISPLAYFORM0   INLINEFORM0 is the frequency of token INLINEFORM1 in the whole corpus, while INLINEFORM2 is the frequency of contextual keyword INLINEFORM3 from the windowed portions of the texts centering on the token INLINEFORM4 in the whole corpus and INLINEFORM5 is the size of window.",
        "type": "Document"
      },
      {
        "id": "dfb0f420-9a68-410a-876f-713fc5f5017c",
        "metadata": {
          "vector_store_key": "1902.00330-1",
          "chunk_id": 56,
          "document_id": "1902.00330",
          "start_idx": 30914,
          "end_idx": 31725
        },
        "page_content": "Various methods have been proposed to model mention's local context ranging from binary classification BIBREF17 to rank models BIBREF26 , BIBREF27 . In these methods, a large number of hand-designed features are applied. For some marginal mentions that are difficult to extract features, researchers also exploit the data retrieved by search engines BIBREF28 , BIBREF29 or Wikipedia sentences BIBREF30 . However, the feature engineering and search engine methods are both time-consuming and laborious. Recently, with the popularity of deep learning models, representation learning is utilized to automatically find semantic features BIBREF31 , BIBREF32 . The learned entity representations which by jointly modeling textual contexts and knowledge base are effective in combining multiple sources of information.",
        "type": "Document"
      },
      {
        "id": "54caa678-6186-45e8-814a-36c5d0179c0e",
        "metadata": {
          "vector_store_key": "2002.00652-2",
          "chunk_id": 1,
          "document_id": "2002.00652",
          "start_idx": 737,
          "end_idx": 1480
        },
        "page_content": "In general, there are two sorts of contextual phenomena in dialogues: Coreference and Ellipsis BIBREF1. Figure FIGREF1 shows a dialogue from the dataset SParC BIBREF2. After the question \u201cWhat is id of the car with the max horsepower?\u201d, the user poses an elliptical question \u201cHow about with the max mpg?\u201d, and a question containing pronouns \u201cShow its Make!\u201d. Only when completely understanding the context, could a parser successfully parse the incomplete questions into their corresponding SQL queries. A number of context modeling methods have been suggested in the literature to address SPC BIBREF3, BIBREF4, BIBREF2, BIBREF5, BIBREF6. These methods proposed to leverage two categories of context: recent questions and precedent logic form.",
        "type": "Document"
      },
      {
        "id": "1abdb0dd-5b13-4046-871e-6efee4f9354a",
        "metadata": {
          "vector_store_key": "1907.02636-3",
          "chunk_id": 20,
          "document_id": "1907.02636",
          "start_idx": 10934,
          "end_idx": 11731
        },
        "page_content": "Taking the above into account, we introduce the contextual feature vector INLINEFORM0 for a given input token INLINEFORM1 , where the INLINEFORM2 element of INLINEFORM3 is defined as follows: DISPLAYFORM0   INLINEFORM0 is the frequency of token INLINEFORM1 in the whole corpus, while INLINEFORM2 is the frequency of contextual keyword INLINEFORM3 from the windowed portions of the texts centering on the token INLINEFORM4 in the whole corpus and INLINEFORM5 is the size of window. The set of contextual keywords INLINEFORM6 are automatically extracted from the annotated texts, where each contextual keyword INLINEFORM7 ( INLINEFORM8 ) satisfies the following conditions:  INLINEFORM0 , where INLINEFORM1 is the set of manually annotated IOCs and INLINEFORM2 is a the lower bound of the frequency.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the data in RAFAEL labelled?",
    "llm_answer": "The knowledge base is indexed by a search engine, annotated using NLP tools (possibly also during the answering stage), and annotated with named entities (NER) using NERF, Liner2, Quant, and DeepER.  The UD schema also annotates for features interpretable as derivational.\n",
    "context": [
      {
        "id": "350c04e6-3dac-4425-8ef4-3401573c8c53",
        "metadata": {
          "vector_store_key": "1605.08675-4",
          "chunk_id": 9,
          "document_id": "1605.08675",
          "start_idx": 5247,
          "end_idx": 5994
        },
        "page_content": "To avoid overfitting, the final system evaluation is executed on a separate test set, previously unused in development, and is checked manually. The results are shown in section SECREF93 and discussed in chapter SECREF6 . Finally, chapter SECREF7 concludes the paper. As stated in previous chapter, RAFAEL is a computer system solving a task of Polish text-based, open-domain, factoid question answering. It means that provided questions, knowledge base and returned answers are expressed in Polish and may belong to any domain. The system analyses the knowledge base, consisting of a set of plain text documents, and returns answers (as concise as possible, e.g. a person name), supplied with information about supporting sentences and documents.",
        "type": "Document"
      },
      {
        "id": "34015681-204f-459a-b415-e25f6c5f23b9",
        "metadata": {
          "vector_store_key": "1605.08675-4",
          "chunk_id": 44,
          "document_id": "1605.08675",
          "start_idx": 24593,
          "end_idx": 25373
        },
        "page_content": "Appendix A contains details of implementation of named entity recognition in RAFAEL, including a description of Quant and a mapping between question types and named entity types available in NERF and Liner2. An alternative being in focus of this work, i.e. DeepER approach, is thorougly discussed in chapter SECREF3 . RAFAEL may use any of the two approaches to entity recognition: NER (via NERF, Liner2 and Quant) or novel DeepER; this choice affects its overall performance. Experiments showing precision and recall of the whole system with respect to applied entity recognition technique are demonstrated in section SECREF88 . An entity recognition step is performed within the question answering process and aims at selecting all entity mentions in a given annotated document.",
        "type": "Document"
      },
      {
        "id": "0c50259e-cf52-4241-8652-2d3edc438b40",
        "metadata": {
          "vector_store_key": "1605.08675-4",
          "chunk_id": 25,
          "document_id": "1605.08675",
          "start_idx": 14454,
          "end_idx": 15162
        },
        "page_content": "The authors projected word-sense annotations of publicly available corpora to supersenses and applied perceptron-trained Hidden Markov Model for sequence classification, obtaining precision and recall around 77 per cent. A general architectural scheme of RAFAEL (figure FIGREF11 ) has been inspired by similar systems developed for English; for examples see works by BIBREF22 and BIBREF23 . Two of the steps in the diagram concern offline processing of a knowledge base. Firstly, it is indexed by a search engine to ensure efficient searching in further stages (INDEXING). Secondly, it may be annotated using a set of tools (NLP), but this could also happen at an answering stage for selected documents only.",
        "type": "Document"
      },
      {
        "id": "81d1435c-e440-4fcc-bc64-1b606ba4b82f",
        "metadata": {
          "vector_store_key": "1605.08675-4",
          "chunk_id": 43,
          "document_id": "1605.08675",
          "start_idx": 24062,
          "end_idx": 24885
        },
        "page_content": "It recognizes 13 types of NEs, possibly nested (e.g. Warsaw in University of Warsaw). Liner2 BIBREF30 also employs CRFs, but differentiates NEs of 56 types (which could be reduced to 5 for higher precision). Annotation using both of the tools happens offline within the KB preprocessing, so in the currently described stage it suffices to browse the annotations and find matching entities. As the above tools lack recognition of quantitative expressions, a new one has been developed especially for RAFAEL and called Quant. It is able to handle both numbers and quantities (using WordNet) in a variety of notations. Appendix A contains details of implementation of named entity recognition in RAFAEL, including a description of Quant and a mapping between question types and named entity types available in NERF and Liner2.",
        "type": "Document"
      },
      {
        "id": "55832e81-3222-4316-87dc-297e2923cfba",
        "metadata": {
          "vector_store_key": "1810.06743-2",
          "chunk_id": 11,
          "document_id": "1810.06743",
          "start_idx": 6504,
          "end_idx": 7228
        },
        "page_content": "Additionally, the UD schema annotates for features which can be interpreted as derivational in some languages. For example, the Czech UD guidance uses a Coll value for the Number feature to denote mass nouns (for example, \"lidstvo\" \"humankind\" from the root \"lid\" \"people\"). UD represents a confederation of datasets BIBREF8 annotated with dependency relationships (which are not the focus of this work) and morphosyntactic descriptions. Each dataset is an annotated treebank, making it a resource of token-level annotations. The schema is guided by these treebanks, with feature names chosen for relevance to native speakers. (In sec:unimorph, we will contrast this with UniMorph's treatment of morphosyntactic categories.)",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the fluctuation in the sense of the word and its neighbors measured?",
    "llm_answer": "Surrounding uniformity, calculated from the statistical fluctuation in vectors corresponding to words in the neighbor.\n",
    "context": [
      {
        "id": "d8e8c4f9-b154-4bca-8f56-e67865f45c6a",
        "metadata": {
          "vector_store_key": "1709.08858-0",
          "chunk_id": 2,
          "document_id": "1709.08858",
          "start_idx": 952,
          "end_idx": 1748
        },
        "page_content": "To measure the effect of a minor sense, this paper proposes to use the concept of surrounding uniformity. The surrounding uniformity roughly corresponds to statistical fluctuation in the vectors that correspond to the words in the neighbor. We have found that there is a difference in the surrounding uniformity between a monosemic word and a polysemic word. This paper describes how to compute surrounding uniformity for a given word, and discuss the relationship between surrounding uniformity and polysemy. The distributed word representation can be computed as weight vectors of neurons, which learn language modeling BIBREF0 . We can obtain a distributed representation of a word using the Word2Vec software BIBREF1 which enable us to perform vector addition/subtraction on a word's meaning.",
        "type": "Document"
      },
      {
        "id": "54e7343f-aac6-41bb-a04c-b67ed285f5e6",
        "metadata": {
          "vector_store_key": "1709.08858-0",
          "chunk_id": 29,
          "document_id": "1709.08858",
          "start_idx": 15319,
          "end_idx": 15932
        },
        "page_content": "At the same time, since we assume that we can measure the statistical fluctuation from the neighbors, we need to exclude words of a different nature from the neighbors. It is natural that the right number for a neighbor may be different according to the word. The number that we choose is the minimum value for the statistical test, and has room to adjust for improvement. We computed the neighbor and surrounding uniformity of the 1000 most frequently used words in FIL9. We observed that proper nouns tend to have a large surrounding uniformity, whereas prepositions tend to have a small surrounding uniformity.",
        "type": "Document"
      },
      {
        "id": "3a731560-c965-4f2d-a837-3a658a0ca4c5",
        "metadata": {
          "vector_store_key": "1709.08858-0",
          "chunk_id": 1,
          "document_id": "1709.08858",
          "start_idx": 571,
          "end_idx": 1184
        },
        "page_content": "We have observed that the neighbor of a polysemic word consists of words that resemble the primary sense of the polysemic word. We can explain this fact as follows. Even though a word may be a polysemic, it usually corresponds to a single vector in distributed representation. This vector is primarily determined by the major sense, which is most frequently used. The information about a word's minor sense is subtle, and the effect of a minor sense is difficult to distinguish from statistical fluctuation. To measure the effect of a minor sense, this paper proposes to use the concept of surrounding uniformity.",
        "type": "Document"
      },
      {
        "id": "c172bf8c-5a59-4c06-bd1c-b62868715493",
        "metadata": {
          "vector_store_key": "1709.08858-2",
          "chunk_id": 0,
          "document_id": "1709.08858",
          "start_idx": 0,
          "end_idx": 571
        },
        "page_content": "Distributed representation of word sense provides us with the ability to perform several operations on the word. One of the most important operations on a word is to obtain the set of words whose meaning is similar to the word, or whose usage in text is similar to the word. We call this set the neighbor of the word. When a word has several senses, it is called a polysemic word. When a word has only one sense, it is called a monosemic word. We have observed that the neighbor of a polysemic word consists of words that resemble the primary sense of the polysemic word.",
        "type": "Document"
      },
      {
        "id": "afc0b087-dd62-4710-bace-32dba5ed8518",
        "metadata": {
          "vector_store_key": "1705.09665-2",
          "chunk_id": 16,
          "document_id": "1705.09665",
          "start_idx": 9271,
          "end_idx": 9969
        },
        "page_content": "We account for temporal as well as inter-community variation by computing word-level measures for each time period of each community's history, INLINEFORM5 . Given a word INLINEFORM6 used within a particular community INLINEFORM7 at time INLINEFORM8 , we define two word-level measures: Specificity. We quantify the specificity INLINEFORM0 of INLINEFORM1 to INLINEFORM2 by calculating the PMI of INLINEFORM3 and INLINEFORM4 , relative to INLINEFORM5 , INLINEFORM6  where INLINEFORM0 is INLINEFORM1 's frequency in INLINEFORM2 . INLINEFORM3 is specific to INLINEFORM4 if it occurs more frequently in INLINEFORM5 than in the entire set INLINEFORM6 , hence distinguishing this community from the rest.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does SPNet outperforms state-of-the-art abstractive summarization methods on evaluation metrics?",
    "llm_answer": "SPNet outperforms other methods in all automatic evaluation metrics with a large margin.\n",
    "context": [
      {
        "id": "a9b51386-d24a-4eaf-a71a-3f9cdf025c15",
        "metadata": {
          "vector_store_key": "1910.00825-1",
          "chunk_id": 27,
          "document_id": "1910.00825",
          "start_idx": 16706,
          "end_idx": 17444
        },
        "page_content": "We observe that SPNet outperforms other methods in all the automatic evaluation metrics with a big margin, as it incorporates all the three semantic scaffolds. Semantic slot contributes the most to SPNet's increased performance, bringing the largest increase on all automatic evaluation metrics. We also perform human evaluation to verify if our method's increased performance on automatic evaluation metrics entails better human perceived quality. We randomly select 100 test samples from MultiWOZ test set for evaluation. We recruit 150 crowd workers from Amazon Mechanical Turk. For each sample, we show the conversation, reference summary, as well as summaries generated by Pointer-Generator and SPNet to three different participants.",
        "type": "Document"
      },
      {
        "id": "3f605a8f-02c8-4dc5-a74d-a9b7cc787239",
        "metadata": {
          "vector_store_key": "1910.00825-1",
          "chunk_id": 29,
          "document_id": "1910.00825",
          "start_idx": 17857,
          "end_idx": 18599
        },
        "page_content": "Ground truth is still perceived as more relevant and readable than SPNet results. However, ground truth does not get a high absolute score. From the feedback of the evaluators, we found that they think that the ground truth has not covered all the necessary information in the conversation, and the description is not so natural. This motivates us to collect a dialog summarization dataset with high-quality human-written summaries in the future. Results in the ranking evaluation show more differences between different summaries. SPNet outperforms Pointer-Generator with a large margin. Its performance is relatively close to the ground truth summary. Table TABREF25 shows an example summary from all models along with ground truth summary.",
        "type": "Document"
      },
      {
        "id": "f0db8231-947c-48f7-ac22-930fe08d6653",
        "metadata": {
          "vector_store_key": "1910.00825-1",
          "chunk_id": 4,
          "document_id": "1910.00825",
          "start_idx": 2550,
          "end_idx": 3348
        },
        "page_content": "Finally, we incorporate dialog domain scaffold by jointly optimizing dialog domain classification task along with the summarization task. We evaluate SPNet with both automatic and human evaluation metrics on MultiWOZ. SPNet outperforms Pointer-Generator BIBREF5 and Transformer BIBREF6 on all the metrics. BIBREF7 first applied modern neural models to abstractive summarization. Their approach is based on Seq2Seq framework BIBREF8 and attention mechanism BIBREF9, achieving state-of-the-art results on Gigaword and DUC-2004 dataset. BIBREF10 proposed copy mechanism in summarization, demonstrating its effectiveness by combining the advantages of extractive and abstractive approach. BIBREF5 applied pointing BIBREF11 as copy mechanism and use coverage mechanism BIBREF12 to discourage repetition.",
        "type": "Document"
      },
      {
        "id": "a0bab460-b264-4101-b443-57d7dacff658",
        "metadata": {
          "vector_store_key": "1910.00825-1",
          "chunk_id": 33,
          "document_id": "1910.00825",
          "start_idx": 20339,
          "end_idx": 21121
        },
        "page_content": "However, the semantic slot scaffold can be relaxed to any critical entities in the corpus, such as team name in sports news or professional terminology in a technical meeting. We adapt a dialog generation dataset, MultiWOZ to an abstractive dialog summarization dataset. We propose SPNet, an end-to-end model that incorporates the speaker role, semantic slot and dialog domain as the semantic scaffolds to improve abstractive summary quality. We also propose an automatic evaluation metric CIC that considers semantic slot relevance to serve as a complementary metric to ROUGE. SPNet outperforms baseline methods in both automatic and human evaluation metrics. It suggests that involving semantic scaffolds efficiently improves abstractive summarization quality in the dialog scene.",
        "type": "Document"
      },
      {
        "id": "2cc15add-ff37-4674-a5a6-25ea6e03d8e0",
        "metadata": {
          "vector_store_key": "1910.00825-1",
          "chunk_id": 26,
          "document_id": "1910.00825",
          "start_idx": 15970,
          "end_idx": 16706
        },
        "page_content": "We observe that SPNet reaches the highest score in both ROUGE and CIC. Both Pointer-Generator and Transformer achieve high ROUGE scores, but a relative low CIC scores. It suggests that the baselines have more room for improvement on preserving critical slot information. All the scaffolds we propose can be applied to different neural network models. In this work we select Pointer-Generator as our base model in SPNet because we observe that Transformer only has a small improvement over Pointer-Generator but is having a higher cost on training time and computing resources. We observe that SPNet outperforms other methods in all the automatic evaluation metrics with a big margin, as it incorporates all the three semantic scaffolds.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is it expected to have speaker role, semantic slot and dialog domain annotations in real world datasets?",
    "llm_answer": "The context mentions that speaker role and dialog domain annotations are often available in datasets, but semantic slot annotations are less common and can be relaxed to important entities.\n",
    "context": [
      {
        "id": "cf0d8b7e-2e4d-4c0d-b529-6239a90cc02e",
        "metadata": {
          "vector_store_key": "1910.00825-1",
          "chunk_id": 32,
          "document_id": "1910.00825",
          "start_idx": 19635,
          "end_idx": 20339
        },
        "page_content": "Furthermore, although our SPNet achieves a much-improved performance, the application of SPNet still needs extra annotations for semantic scaffolds. For a dialog dataset, speaker role scaffold is a natural pattern for modeling. Most multi-domain dialog corpus has the domain annotation. While for texts, for example news, its topic categorization such as sports or entertainment can be used as domain annotation. We find that semantic slot scaffold brings the most significant improvement, but it is seldom explicitly annotated. However, the semantic slot scaffold can be relaxed to any critical entities in the corpus, such as team name in sports news or professional terminology in a technical meeting.",
        "type": "Document"
      },
      {
        "id": "24f8dfec-c722-47c3-89b3-12384c534e78",
        "metadata": {
          "vector_store_key": "2002.11893-9",
          "chunk_id": 14,
          "document_id": "2002.11893",
          "start_idx": 8370,
          "end_idx": 9067
        },
        "page_content": "Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states. Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three experts were employed to manually annotate dialogue acts and states for 50 dialogues. The results show that our annotations are of high quality.",
        "type": "Document"
      },
      {
        "id": "a0bab460-b264-4101-b443-57d7dacff658",
        "metadata": {
          "vector_store_key": "1910.00825-1",
          "chunk_id": 33,
          "document_id": "1910.00825",
          "start_idx": 20339,
          "end_idx": 21121
        },
        "page_content": "However, the semantic slot scaffold can be relaxed to any critical entities in the corpus, such as team name in sports news or professional terminology in a technical meeting. We adapt a dialog generation dataset, MultiWOZ to an abstractive dialog summarization dataset. We propose SPNet, an end-to-end model that incorporates the speaker role, semantic slot and dialog domain as the semantic scaffolds to improve abstractive summary quality. We also propose an automatic evaluation metric CIC that considers semantic slot relevance to serve as a complementary metric to ROUGE. SPNet outperforms baseline methods in both automatic and human evaluation metrics. It suggests that involving semantic scaffolds efficiently improves abstractive summarization quality in the dialog scene.",
        "type": "Document"
      },
      {
        "id": "eee2f80d-e680-4708-9c00-a608592966ea",
        "metadata": {
          "vector_store_key": "1910.00825-1",
          "chunk_id": 3,
          "document_id": "1910.00825",
          "start_idx": 2078,
          "end_idx": 2902
        },
        "page_content": "However, directly applying a news summarizer to dialog results in two drawbacks: informative entities such as place name are difficult to capture precisely and contents in different domains are summarized unequally. To address these problems, we propose Scaffold Pointer Network (SPNet). SPNet incorporates three types of semantic scaffolds in dialog: speaker role, semantic slot, and dialog domain. Firstly, SPNet adapts separate encoder to attentional Seq2Seq framework, producing distinct semantic representations for different speaker roles. Then, our method inputs delexicalized utterances for producing delexicalized summary, and fills in slot values to generate complete summary. Finally, we incorporate dialog domain scaffold by jointly optimizing dialog domain classification task along with the summarization task.",
        "type": "Document"
      },
      {
        "id": "b72b6824-2157-4377-8af7-f812f214d7a9",
        "metadata": {
          "vector_store_key": "2002.11893-9",
          "chunk_id": 31,
          "document_id": "2002.11893",
          "start_idx": 18005,
          "end_idx": 18636
        },
        "page_content": "We also obtained a binary label for each semantic tuple in the user state, which indicates whether this semantic tuple has been selected to be expressed by the user. This annotation directly illustrates the progress of the conversation. To evaluate the quality of the annotation of dialogue acts and states (both user and system states), three experts were employed to manually annotate dialogue acts and states for the same 50 dialogues (806 utterances), 10 for each goal type (see Section SECREF4). Since dialogue act annotation is not a classification problem, we didn't use Fleiss' kappa to measure the agreement among experts.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does new evaluation metric considers critical informative entities?",
    "llm_answer": "CIC (Critical Information Completeness) is a recall metric of semantic slot information between a candidate and reference summary.\n",
    "context": [
      {
        "id": "fb08d1ed-9fd0-4bfd-aa35-649e100f6eac",
        "metadata": {
          "vector_store_key": "1910.00825-2",
          "chunk_id": 21,
          "document_id": "1910.00825",
          "start_idx": 13254,
          "end_idx": 13895
        },
        "page_content": "The semantic slot values (e.g. [restaurant_name], [time]) are more essential compared to other words in the summary. However, ROUGE did not take this into consideration. To address this drawback in ROUGE, we propose a new evaluation metric: Critical Information Completeness (CIC). Formally, CIC is a recall of semantic slot information between a candidate summary and a reference summary. CIC is defined as follows: where $V$ stands for a set of delexicalized values in the reference summary, $Count_{match}(v)$ is the number of values co-occurring in the candidate summary and reference summary, and $m$ is the number of values in set $V$.",
        "type": "Document"
      },
      {
        "id": "8276c14e-e296-4c98-be0c-ca1c9654bd57",
        "metadata": {
          "vector_store_key": "1905.08949-6",
          "chunk_id": 19,
          "document_id": "1905.08949",
          "start_idx": 11074,
          "end_idx": 11806
        },
        "page_content": "As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE BIBREF43 , are also widely used. However, some studies BIBREF44 , BIBREF45 have shown that these metrics do not correlate well with fluency, adequacy, coherence, as they essentially compute the $n$ -gram similarity between the source sentence and the generated question. To overcome this, BIBREF46 proposed a new metric to evaluate the \u201canswerability\u201d of a question by calculating the scores for several question-specific factors, including question type, content words, function words, and named entities. However, as it is newly proposed, it has not been applied to evaluate any NQG system yet.",
        "type": "Document"
      },
      {
        "id": "8c81c383-8e40-4094-8e18-ab0a2d32126a",
        "metadata": {
          "vector_store_key": "1905.08949-6",
          "chunk_id": 18,
          "document_id": "1905.08949",
          "start_idx": 10340,
          "end_idx": 11074
        },
        "page_content": "Although the datasets are commonly shared between QG and QA, it is not the case for evaluation: it is challenging to define a gold standard of proper questions to ask. Meaningful, syntactically correct, semantically sound and natural are all useful criteria, yet they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks. As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE BIBREF43 , are also widely used.",
        "type": "Document"
      },
      {
        "id": "8c0b614b-bf22-42bc-9131-ea972e221afa",
        "metadata": {
          "vector_store_key": "1709.10217-1",
          "chunk_id": 7,
          "document_id": "1709.10217",
          "start_idx": 4048,
          "end_idx": 4813
        },
        "page_content": "It is worth noting that besides the released data for training and developing, we also allow to collect external data for training and developing. To considering that, the task 1 is indeed includes two sub tasks. One is a closed evaluation, in which only the released data can be used for training and developing. The other is an open evaluation that allow to collect external data for training and developing. For task 1, we use F1-score as evaluation metric. For the task-oriented dialogue systems, the best way for evaluation is to use the online human-computer dialogue. After finishing an online human-computer dialogue with a dialogue system, the human then manually evaluate the system by using the metrics of user satisfaction degree, dialogue fluency, etc.",
        "type": "Document"
      },
      {
        "id": "f221ac3d-8ea6-49e8-8ce8-1528ccd79f99",
        "metadata": {
          "vector_store_key": "1909.00578-2",
          "chunk_id": 2,
          "document_id": "1909.00578",
          "start_idx": 1185,
          "end_idx": 1928
        },
        "page_content": "Sum-QE achieves very high correlations with human ratings, showing the ability of BERT to model linguistic qualities that relate to both text content and form. Summarization evaluation metrics like Pyramid BIBREF5 and ROUGE BIBREF3, BIBREF2 are recall-oriented; they basically measure the content from a model (reference) summary that is preserved in peer (system generated) summaries. Pyramid requires substantial human effort, even in its more recent versions that involve the use of word embeddings BIBREF8 and a lightweight crowdsourcing scheme BIBREF9. ROUGE is the most commonly used evaluation metric BIBREF10, BIBREF11, BIBREF12. Inspired by BLEU BIBREF4, it relies on common $n$-grams or subsequences between peer and model summaries.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are state of the art methods MMM is compared to?",
    "llm_answer": "MTMSN, NABERT+, MLP, MCB, NMN\n",
    "context": [
      {
        "id": "db7fa7e7-329d-4626-8f94-4473f228496d",
        "metadata": {
          "vector_store_key": "1910.00458-0",
          "chunk_id": 4,
          "document_id": "1910.00458",
          "start_idx": 2239,
          "end_idx": 3031
        },
        "page_content": "For example, the MCTest dataset has two variants: MC160 and MC500, which are curated in a similar way, and MC160 is considered easier than MC500 BIBREF9. However, BERT-based models perform much worse on MC160 compared with MC500 (8\u201310% gap) since the data size of the former is about three times smaller. To tackle this issue, we investigate how to improve the generalization of BERT-based MCQA models with the constraint of limited training data using four representative MCQA datasets: DREAM, MCTest, TOEFL, and SemEval-2018 Task 11. We proposed MMM, a Multi-stage Multi-task learning framework for Multi-choice question answering. Our framework involves two sequential stages: coarse-tuning stage using out-of-domain datasets and multi-task learning stage using a larger in-domain dataset.",
        "type": "Document"
      },
      {
        "id": "1858edb9-546d-4edc-9fc6-41fbc071abbd",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 39,
          "document_id": "1909.13375",
          "start_idx": 20678,
          "end_idx": 21407
        },
        "page_content": "The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could be found in Section (SECREF36). Notice that different BERTlarge models were used, so the comparison is less direct. Overall, our large models exhibits similar results to those of MTMSNlarge. For multi-span questions we achieve a significantly better performance. While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1.",
        "type": "Document"
      },
      {
        "id": "8a97ca05-3bf6-452d-a996-768305af6f53",
        "metadata": {
          "vector_store_key": "1703.09684-2",
          "chunk_id": 43,
          "document_id": "1703.09684",
          "start_idx": 24094,
          "end_idx": 24785
        },
        "page_content": "We next investigate how much of the good performance is due to bias in the answer distribution, which N-MPT compensates for. One of our major aims was to compensate for the fact that algorithms can achieve high scores by simply learning to answer more populated and easier question-types. For existing datasets, earlier work has shown that simple baseline methods routinely exceed more complex methods using simple accuracy BIBREF7 , BIBREF8 , BIBREF19 . On TDIUC, MLP surpasses MCB and NMN in terms of simple accuracy, but a closer inspection reveals that MLP's score is highly determined by performance on categories with a large number of examples, such as `absurd' and `object presence.'",
        "type": "Document"
      },
      {
        "id": "2e7cfe41-b41c-475c-83f6-e7d39903180d",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 40,
          "document_id": "1909.13375",
          "start_idx": 21052,
          "end_idx": 21792
        },
        "page_content": "While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1. All that, while keeping in mind we compare a base model to a large model (for reference, note the 8 point improvement between MTMSNbase and MTMSNlarge in both EM and F1). Our best model, large-squad, exhibits a huge improvement of 29.7 in EM and 15.1 in F1 compared to MTMSNlarge. When comparing single-span performance, our best model exhibits slightly better results, but it should be noted that it retains the single-span heads from NABERT+, while in MTMSN they have one head to predict both single-span and multi-span answers.",
        "type": "Document"
      },
      {
        "id": "d96990f0-760d-4401-b868-e98fb5836455",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 10,
          "document_id": "1910.04269",
          "start_idx": 6004,
          "end_idx": 6789
        },
        "page_content": "GMM combined with RPLP features gave the most promising results and achieved an accuracy of 88.8% on ten languages. They designed their own dataset comprising of ten languages being Dutch, English, French, German, Italian, Russian, Spanish, Hindi, Telegu, and Bengali. Montavon BIBREF7 generated Mel spectrogram as features for a time-delay neural network (TDNN). This network had two-dimensional convolutional layers for feature extraction. An elaborate analysis of how deep architectures outperform their shallow counterparts is presented in this reseacrch. The difficulties in classifying perceptually similar languages like German and English were also put forward in this work. It is mentioned that the proposed approach is less robust to new speakers present in the test dataset.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the problems related to ambiguity in PICO sentence prediction tasks?",
    "llm_answer": "Mapping emotions to facial expressions, personal taste, and lack of consensus on terms like \"many people\" or \"crowded\".\n",
    "context": [
      {
        "id": "519efef9-9bc1-4fef-8903-647112ff38ff",
        "metadata": {
          "vector_store_key": "2001.11268-2",
          "chunk_id": 18,
          "document_id": "2001.11268",
          "start_idx": 10222,
          "end_idx": 10978
        },
        "page_content": "As we show later, the automated annotation technique causes noise in the evaluation because as the network learns, it can assign correct tags to wrongly labelled data. We also show that sentence labels are often ambiguous, and that the assignment of a single label limits the quality of the predictions for their use in real-world reviewing tasks. We acknowledge that the assignment of classes such as `Results' or `Conclusions' to sentences is potentially valuable for many use-cases. However, those sentences can contain additional information related to the PICO classes of interest. In the original LSTM-based model the A, M, R, and C data classes in Table TABREF4 are utilized for sequence optimization, which leads to increased classification scores.",
        "type": "Document"
      },
      {
        "id": "a89052f4-c9ef-44d5-b1d0-bcbec9282c12",
        "metadata": {
          "vector_store_key": "2004.03744-3",
          "chunk_id": 11,
          "document_id": "2004.03744",
          "start_idx": 6333,
          "end_idx": 7116
        },
        "page_content": "We show some examples in Figure FIGREF3 and in Appendix SECREF43. In order to have a better sense of this ambiguity, three authors of this paper independently annotated 100 random examples. All three authors agreed on 54% of the examples, exactly two authors agreed on 45%, and there was only one example on which all three authors disagreed. We identified the following three major sources of ambiguity: mapping an emotion in the hypothesis to a facial expression in the image premise, e.g., \u201cpeople enjoy talking\u201d, \u201cangry people\u201d, \u201csad woman\u201d. Even when the face is seen, it may be subjective to infer an emotion from a static image (see Figure FIGREF44 in Appendix SECREF43). personal taste, e.g., \u201cthe sign is ugly\u201d. lack of consensus on terms such as \u201cmany people\u201d or \u201ccrowded\u201d.",
        "type": "Document"
      },
      {
        "id": "023458b4-cbfa-4948-b87c-eb5bc387424f",
        "metadata": {
          "vector_store_key": "2001.11268-7",
          "chunk_id": 45,
          "document_id": "2001.11268",
          "start_idx": 25764,
          "end_idx": 26518
        },
        "page_content": "In this evaluation, the F1 scores represent the overlap of labelled and predicted answer spans on token level. We also obtained scores for the subgroups of sentences that did not contain an answer versus the ones that actually included PICO elements. These results are shown in Table TABREF30. For the P class, only 30% of all sentences included an entity, whereas its sub-classes age, gender, condition and size averaged 10% each. In the remaining classes, these percentages were higher. F1 scores for correctly detecting that a sentence includes no PICO element exceeded 0.92 in all classes. This indicates that the addition of impossible answer elements was successful, and that the model learned a representation of how to discriminate PICO contexts.",
        "type": "Document"
      },
      {
        "id": "2847d7cd-7409-4acb-8dc5-24f2ab40950d",
        "metadata": {
          "vector_store_key": "2001.11268-7",
          "chunk_id": 46,
          "document_id": "2001.11268",
          "start_idx": 26518,
          "end_idx": 27305
        },
        "page_content": "This indicates that the addition of impossible answer elements was successful, and that the model learned a representation of how to discriminate PICO contexts. The scores for correctly predicting PICOs in positive scenarios are lower. These results are presented in Table TABREF30. Here, two factors could influence this score in a negative way. First, labelled spans can be noisy. Training spans were annotated by crowd workers and the authors of the original dataset noted inter-annotator disagreement. Often, these spans include full stops, other punctuation or different levels of detail describing a PICO. The F1 score decreases if the model predicts a PICO, but the predicted span includes marginal differences that were not marked up by the experts who annotated the testing set.",
        "type": "Document"
      },
      {
        "id": "b2661a29-e2ef-467a-bb53-7b3bfe6a392f",
        "metadata": {
          "vector_store_key": "2001.11268-7",
          "chunk_id": 47,
          "document_id": "2001.11268",
          "start_idx": 26970,
          "end_idx": 27675
        },
        "page_content": "The F1 score decreases if the model predicts a PICO, but the predicted span includes marginal differences that were not marked up by the experts who annotated the testing set. Second, some spans include multiple PICOs, sometimes across sentence boundaries. Other spans mark up single PICOS in succession. In these cases the model might find multiple PICOs in a row, and annotate them as one or vice versa. In this work, we have shown possibilities for sentence classification and data extraction of PICO characteristics from abstracts of RCTs. For sentence classification, models based on transformers can predict multiple labels per sentence, even if trained on a corpus that assigns a single label only.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is knowledge stored in the memory?",
    "llm_answer": "Knowledge is stored in a memory matrix, $\\mathbf{E} \\in \\mathbb{R}^{d_E \\times K}$, where each row represents an entity (ingredient) and is updated after each recipe step.  Entity representations ($\\mathbf{e}_i$) are initialized with CharCNN and pre-trained GloVe embeddings.\n",
    "context": [
      {
        "id": "6cbf55fd-b2e2-436b-89df-7bb7f3d5e34e",
        "metadata": {
          "vector_store_key": "1909.00754-6",
          "chunk_id": 25,
          "document_id": "1909.00754",
          "start_idx": 13526,
          "end_idx": 14450
        },
        "page_content": "We can interpret this hierarchical structure as the internal order for the memory processing, since from the daily life experience, people tend to attend to the most contemporary memories (system/user utterance) first and then attend to the older history (belief states). All of the parameters are shared between the attention modules. The concatenated working memory, $\\mathbf {r}_0$ , is then fed into a Multi-Layer Perceptron (MLP) with four layers, $\n\\mathbf {r}_1 & =\\sigma (W_1^T\\mathbf {r}_0+\\mathbf {b}_1),\\\\\n\\mathbf {r}_2 & =\\sigma (W_2^T\\mathbf {r}_1+\\mathbf {b}_2),\\\\\n\\mathbf {r}_3 & = \\sigma (W_3^T\\mathbf {r}_2+\\mathbf {b}_3),\\\\\n\\mathbf {h}_s & = \\sigma (W_4^T\\mathbf {r}_3+\\mathbf {b}_4),\n$   where $\\sigma $ is a non-linear activation, and the weights $W_1 \\in R^{4d_m \\times d_m}$ , $W_i \\in R^{d_m \\times d_m}$ and the bias $b_1 \\in R^{d_m}$ , $b_i \\in R^{d_m}$ are learnable parameters, and $2\\le i\\le 4$ .",
        "type": "Document"
      },
      {
        "id": "89cc740d-3eb7-47da-ae3c-31290471bc5f",
        "metadata": {
          "vector_store_key": "1707.00110-0",
          "chunk_id": 12,
          "document_id": "1707.00110",
          "start_idx": 6438,
          "end_idx": 7132
        },
        "page_content": "Our memory-based attention model can be understood intuitively in two ways. We can interpret it as \"predicting\" the set of attention contexts produced by a standard attention mechanism during encoding. To see this, assume we set $K \\approx |T|$ . In this case, we predict all $|T|$ attention contexts during the encoding stage and learn to choose the right one during decoding. This is cheaper than computing contexts one-by-one based on the decoder and encoder content. In fact, we could enforce this objective by first training a regular attention model and adding a regularization term to force the memory matrix $C$ to be close to the $T\\times D$ vectors computed by the standard attention.",
        "type": "Document"
      },
      {
        "id": "5b30074a-73ad-40f8-a6c8-53e03ef5b085",
        "metadata": {
          "vector_store_key": "1706.07179-1",
          "chunk_id": 2,
          "document_id": "1706.07179",
          "start_idx": 1130,
          "end_idx": 1842
        },
        "page_content": "For the task of question-answering, we instead make an attempt at an end-to-end approach which directly models the entities and relations in the text as memory slots. While incorporating existing knowledge (from curated knowledge bases) for the purpose of question-answering BIBREF11 , BIBREF8 , BIBREF15 is an important area of research, we consider the simpler setting where all the information is contained within the text itself \u2013 which is the approach taken by many recent memory based neural network models BIBREF16 , BIBREF17 , BIBREF18 , BIBREF19 . Recently, BIBREF17 proposed a dynamic memory based neural network for implicitly modeling the state of entities present in the text for question answering.",
        "type": "Document"
      },
      {
        "id": "62c91aee-6ab9-47cb-a33e-8f03f68a1206",
        "metadata": {
          "vector_store_key": "1909.08859-0",
          "chunk_id": 17,
          "document_id": "1909.08859",
          "start_idx": 9394,
          "end_idx": 10076
        },
        "page_content": "This helps us to keep track of the entities, their state changes and their relations in relation to each other over the course of the recipe (see Fig. FIGREF14). As we will examine in more detail in Section SECREF4, it also greatly improves the interpretability of model outputs. Specifically, we set up the memory with a memory matrix $\\mathbf {E} \\in \\mathbb {R}^{d_E \\times K}$ by extracting $K$ entities (ingredients) from the first step of the recipe. We initialize each memory cell $\\mathbf {e}_i$ representing a specific entity by its CharCNN and pre-trained GloVe embeddings. From now on, we will use the terms memory cells and entities interchangeably throughout the paper.",
        "type": "Document"
      },
      {
        "id": "d60620db-539b-4f7a-82ad-fd3f6096386f",
        "metadata": {
          "vector_store_key": "1909.08859-0",
          "chunk_id": 18,
          "document_id": "1909.08859",
          "start_idx": 9817,
          "end_idx": 10854
        },
        "page_content": "From now on, we will use the terms memory cells and entities interchangeably throughout the paper. Since the input recipe is given in the form of a procedural text decomposed into a number of steps, we update the memory cells after each step, reflecting the state changes happened on the entities. This update procedure is modelled via a relational recurrent neural network (R-RNN), recently proposed by BIBREF5. It is built on a 2-dimensional LSTM model whose matrix of cell states represent our memory matrix $\\mathbf {E}$. Here, each row $i$ of the matrix $\\mathbf {E}$ refers to a specific entity $\\mathbf {e}_i$ and is updated after each recipe step $t$ as follows: where $\\mathbf {s}_{t}$ denotes the embedding of recipe step $t$ and $\\mathbf {\\phi }_{i,t}=(\\mathbf {h}_{i,t},\\mathbf {e}_{i,t})$ is the cell state of the R-RNN at step $t$ with $\\mathbf {h}_{i,t}$ and $\\mathbf {e}_{i,t}$ being the $i$-th row of the hidden state of the R-RNN and the dynamic representation of entity $\\mathbf {e}_{i}$ at the step $t$, respectively.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they measure the diversity of inferences?",
    "llm_answer": "The provided text does not describe how diversity of inferences is measured.  It discusses diversity of entities and corpora, not inferences.\n",
    "context": [
      {
        "id": "e89d1313-4d99-4724-ba48-34585f7294c2",
        "metadata": {
          "vector_store_key": "1701.02877-6",
          "chunk_id": 58,
          "document_id": "1701.02877",
          "start_idx": 29900,
          "end_idx": 30536
        },
        "page_content": "The goal is to gauge how much diversity is due to new entities appearing over time. To do this, we used just the surface lexicalisations of entities as the entity representation. The overlap of surface forms was measured across different corpora of the same genre and language. We used an additional corpus based on recent data \u2013 that from the W-NUT 2015 challenge BIBREF25 . This is measured in terms of occurrences, rather than distinct surface forms, so that the magnitude of the drift is shown instead of having skew in results from the the noisy long tail. Results are given in Table 7 for newswire and Table 8 for Twitter corpora.",
        "type": "Document"
      },
      {
        "id": "1c9ea9c4-986b-4fd2-825c-92a8ae6427d9",
        "metadata": {
          "vector_store_key": "1701.02877-7",
          "chunk_id": 22,
          "document_id": "1701.02877",
          "start_idx": 11359,
          "end_idx": 12146
        },
        "page_content": "In order to compare corpus diversity across genres, we measure NE and token/type diversity (following e.g. BIBREF2 ). Note that types are the unique tokens, so the ratio can be understood as ratio of total tokens to unique ones. Table 4 shows the ratios between the number of NEs and the number of unique NEs per corpus, while Table 5 reports the token/type ratios. The lower those ratios are, the more diverse a corpus is. While token/type ratios also include tokens which are NEs, they are a good measure of broader linguistic diversity. Aside from these metrics, there are other factors which contribute to corpus diversity, including how big a corpus is and how well sampled it is, e.g. if a corpus is only about one story, it should not be surprising to see a high token/type ratio.",
        "type": "Document"
      },
      {
        "id": "a83016e8-cd42-4140-8227-73248ddcfbb1",
        "metadata": {
          "vector_store_key": "1809.08731-3",
          "chunk_id": 1,
          "document_id": "1809.08731",
          "start_idx": 755,
          "end_idx": 1497
        },
        "page_content": "Hence, it is not guaranteed that a correct output will match any of a finite number of given references. This results in difficulties for current reference-based evaluation, especially of fluency, causing word-overlap metrics like ROUGE BIBREF1 to correlate only weakly with human judgments BIBREF2 . As a result, fluency evaluation of NLG is often done manually, which is costly and time-consuming. Evaluating sentences on their fluency, on the other hand, is a linguistic ability of humans which has been the subject of a decade-long debate in cognitive science. In particular, the question has been raised whether the grammatical knowledge that underlies this ability is probabilistic or categorical in nature BIBREF3 , BIBREF4 , BIBREF5 .",
        "type": "Document"
      },
      {
        "id": "844b6a32-aadf-4ef8-bbfc-0621972e3359",
        "metadata": {
          "vector_store_key": "1911.03243-4",
          "chunk_id": 13,
          "document_id": "1911.03243",
          "start_idx": 7636,
          "end_idx": 8501
        },
        "page_content": "We later manually estimate the rate of correct equivalences missed by this conservative method. As we will see, our evaluation heuristics, adapted from those in BIBREF5, significantly underestimate agreement between annotations, hence reflecting performance lower bounds. Devising more tight evaluation measures remains a challenge for future research. We extend our metric for evaluating manual or automatic redundant annotations, like the Dense dataset or the parser in BIBREF5, which predicts argument spans independently of each other. To that end, we ignore predicted arguments that match ground-truth but are not selected by the bipartite matching due to redundancy. After connecting unmatched predicted arguments that overlap, we count one false positive for every connected component to avoid penalizing precision too harshly when predictions are redundant.",
        "type": "Document"
      },
      {
        "id": "a521ec55-8888-471b-b041-19e128173cd5",
        "metadata": {
          "vector_store_key": "1703.05260-9",
          "chunk_id": 50,
          "document_id": "1703.05260",
          "start_idx": 27555,
          "end_idx": 28223
        },
        "page_content": "However, this measure is known to be sensitive to text length (see e.g. Tweedie1998), which would result in very small values for InScript and relatively large ones for DeScript, given the large average difference of text lengths between the corpora. Instead, we decided to use the Measure of Textual Lexical Diversity (MTLD) (McCarthy2010, McCarthy2005), which is familiar in corpus linguistics. This metric measures the average number of tokens in a text that are needed to retain a type-token ratio above a certain threshold. If the MTLD for a text is high, many tokens are needed to lower the type-token ratio under the threshold, so the text is lexically diverse.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much do they improve the accuracy of inferences over state-of-the-art methods?",
    "llm_answer": "The answer is not found in the context.  While improvements in accuracy for various tasks are discussed, no comparison to \"state-of-the-art methods\" is made in terms of inference improvement.\n",
    "context": [
      {
        "id": "e34ec430-93a0-4067-875a-f939ddf574fd",
        "metadata": {
          "vector_store_key": "1703.09684-9",
          "chunk_id": 9,
          "document_id": "1703.09684",
          "start_idx": 5278,
          "end_idx": 5981
        },
        "page_content": "For example, on COCO-VQA, improving accuracy on `Is/Are' questions by 15% will increase overall accuracy by over 5%, but answering all `Why/Where' questions correctly will increase accuracy by only 4.1% BIBREF10 . Due to the inability of the existing evaluation metrics to properly address these biases, algorithms trained on these datasets learn to exploit these biases, resulting in systems that work poorly when deployed in the real-world. For related reasons, major benchmarks released in the last decade do not use simple accuracy for evaluating image recognition and related computer vision tasks, but instead use metrics such as mean-per-class accuracy that compensates for unbalanced categories.",
        "type": "Document"
      },
      {
        "id": "dc6680bd-96f0-4828-8454-39d0afffd8b1",
        "metadata": {
          "vector_store_key": "1703.09684-2",
          "chunk_id": 42,
          "document_id": "1703.09684",
          "start_idx": 23464,
          "end_idx": 24094
        },
        "page_content": "High accuracy is also achieved on absurd, which we discuss in greater detail in Sec. \"Effects of Including Absurd Questions\" . Subordinate object recognition is moderately high ( $>80$ %), despite having a large number of unique answers. Accuracy on counting is low across all methods, despite a large number of training data. For the remaining question-types, more analysis is needed to pinpoint whether the weaker performance is due to lower amounts of training data, bias, or limitations of the models. We next investigate how much of the good performance is due to bias in the answer distribution, which N-MPT compensates for.",
        "type": "Document"
      },
      {
        "id": "8a97ca05-3bf6-452d-a996-768305af6f53",
        "metadata": {
          "vector_store_key": "1703.09684-2",
          "chunk_id": 43,
          "document_id": "1703.09684",
          "start_idx": 24094,
          "end_idx": 24785
        },
        "page_content": "We next investigate how much of the good performance is due to bias in the answer distribution, which N-MPT compensates for. One of our major aims was to compensate for the fact that algorithms can achieve high scores by simply learning to answer more populated and easier question-types. For existing datasets, earlier work has shown that simple baseline methods routinely exceed more complex methods using simple accuracy BIBREF7 , BIBREF8 , BIBREF19 . On TDIUC, MLP surpasses MCB and NMN in terms of simple accuracy, but a closer inspection reveals that MLP's score is highly determined by performance on categories with a large number of examples, such as `absurd' and `object presence.'",
        "type": "Document"
      },
      {
        "id": "4f72307a-0a25-4c37-9a12-004e692be9a5",
        "metadata": {
          "vector_store_key": "2003.06279-2",
          "chunk_id": 33,
          "document_id": "2003.06279",
          "start_idx": 19861,
          "end_idx": 20566
        },
        "page_content": "The results obtained for GloVe show that the highest relative improvements in performance occur for decision trees. This is apparent specially for the shortest samples. For $w=1,000$ words, the decision tree accuracy is enhanced by a factor of almost 50% when $p=20\\%$. An excellent gain in performance is also observed for both Naive Bayes and SVM classifiers, when $p=18\\%$ and $p=12\\%$, respectively. When $w=2,500$ words, the highest improvements was observed for the decision tree algorithm. A minor improvement was observed for the kNN method. A similar behavior occurred for $w=5,000$ words. Interestingly, SVM seems to benefit from the use of additional edges when larger documents are considered.",
        "type": "Document"
      },
      {
        "id": "d15b20c4-756e-4cff-bc08-b5520068bc45",
        "metadata": {
          "vector_store_key": "1906.06448-3",
          "chunk_id": 38,
          "document_id": "1906.06448",
          "start_idx": 22138,
          "end_idx": 22802
        },
        "page_content": "Table 9 shows the evaluation results by type of linguistic phenomenon. While accuracy on problems involving NPIs and conditionals was improved on both upward and downward inferences, accuracy on problems involving conjunction and disjunction was improved on only one direction. In addition, it is interesting to see that the change in accuracy on conjunction was opposite to that on disjunction. Downward inference examples involving disjunction are similar to upward inference ones; that is, inferences from a sentence to a shorter sentence are valid (e.g., Not many campers have had a sunburn or caught a cold $\\Rightarrow $ Not many campers have caught a cold).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does the context-aware variational autoencoder learn event background information?",
    "llm_answer": "In the pretraining stage, the model is trained on an auxiliary dataset with rich event context information.  The context-acquiring latent variable ($z_c$) is directly conditioned on the context, allowing it to learn background knowledge.  This knowledge is then transferred to a context-aware latent variable ($z_{c\u2032}$) by minimizing the distance between them.\n",
    "context": [
      {
        "id": "35d0120e-d82a-42f9-baeb-ebfd0fbd26c1",
        "metadata": {
          "vector_store_key": "1909.08824-1",
          "chunk_id": 12,
          "document_id": "1909.08824",
          "start_idx": 7058,
          "end_idx": 7879
        },
        "page_content": "To this end, we propose a context-aware variational autoencoder (CWVAE), with two additional latent variables: a context-acquiring latent variable $z_c$ to directly acquire context information, and a context-aware latent variable $z_{c^{\\prime }}$ to learn background knowledge from $z_c$, as shown in Figure FIGREF6 (a). However, the event context information is absent in the Event2Mind and Atomic dataset. To learn from the external event context information, we design the following two-stage training procedure for CWVAE. Pretrain: Learning Event Background Knowledge from Auxiliary Dataset In the pretrain stage, CWVAE is trained on three narrative story corpora with rich event context information. As shown in Figure FIGREF6 (a), context-acquiring latent variable $z_c$ is directly conditioned on the context $c$.",
        "type": "Document"
      },
      {
        "id": "6e29cbc5-77c6-425e-9c64-aabfc2713397",
        "metadata": {
          "vector_store_key": "1909.08824-3",
          "chunk_id": 3,
          "document_id": "1909.08824",
          "start_idx": 1951,
          "end_idx": 2849
        },
        "page_content": "To better solve these problems, we propose a context-aware variational autoencoder (CWVAE) together with a two-stage training procedure. Variational Autoencoder (VAE) based models have shown great potential in modeling the one-to-many problem and generate diversified inferences BIBREF8, BIBREF9. In addition to the traditional VAE structure, we introduces an extra context-aware latent variable in CWVAE to learn the event background knowledge. In the pretrain stage, CWVAE is trained on an auxiliary dataset (consists of three narrative story corpora and contains rich event background knowledge), to learn the event background information by using the context-aware latent variable. Subsequently, in the finetune stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target (e.g., intents, reactions, etc.).",
        "type": "Document"
      },
      {
        "id": "319ea78a-0e02-419a-a040-f975a853fad1",
        "metadata": {
          "vector_store_key": "1909.08824-1",
          "chunk_id": 13,
          "document_id": "1909.08824",
          "start_idx": 7879,
          "end_idx": 8593
        },
        "page_content": "As shown in Figure FIGREF6 (a), context-acquiring latent variable $z_c$ is directly conditioned on the context $c$. Hence, $z_c$ could be employed for acquiring background knowledge from event contexts. Then, we minimize the distance between $z_c$ and the context-aware latent variable $z_{c^{\\prime }}$, by which the event background knowledge is transferred from $z_c$ to $z_{c^{\\prime }}$. Finetune: Adapt Event Background Knowledge to Each Inference Dimension In the finetune stage, as shown in Figure FIGREF6 (b), CWVAE is trained on the Event2Mind and Atomic dataset without the event context information. Pretrained CWVAE is finetuned to learn the specific inferential knowledge of each inference dimension.",
        "type": "Document"
      },
      {
        "id": "2a789cdf-54eb-4c9c-99cb-ad60d230b669",
        "metadata": {
          "vector_store_key": "1909.08824-3",
          "chunk_id": 11,
          "document_id": "1909.08824",
          "start_idx": 6640,
          "end_idx": 7520
        },
        "page_content": "We refer to $p_{\\theta }(z|x)$ as a prior network, $q_{\\phi }(z|x,y)$ as a recognition network, and $p_{\\theta }(y|x,z)$ as a neural decoder. Traditional CVAE can model the event-target relation. In other words, given an observed event, CVAE can generate its corresponding targets. While in this paper we model the If-Then reasoning as a [(background), event]-target process. It means that in addition to the observed event, we also want to involve the event background knowledge (which can be learned from event contexts) to generate the reasonable targets. To this end, we propose a context-aware variational autoencoder (CWVAE), with two additional latent variables: a context-acquiring latent variable $z_c$ to directly acquire context information, and a context-aware latent variable $z_{c^{\\prime }}$ to learn background knowledge from $z_c$, as shown in Figure FIGREF6 (a).",
        "type": "Document"
      },
      {
        "id": "5479972c-14a1-49c6-a1c0-a039c5e20c04",
        "metadata": {
          "vector_store_key": "1910.00825-0",
          "chunk_id": 9,
          "document_id": "1910.00825",
          "start_idx": 6057,
          "end_idx": 6785
        },
        "page_content": "The input sequence is fed into the encoder token by token, producing the encoder hidden states $h_i$ in each encoding step. The decoder receives word embedding of the previous word and generates a distribution to decide the target element in this step, retaining decoder hidden states $s_t$. In Pointer-Generator, attention distribution $a^t$ is computed as in BIBREF9: where $W_h$, $W_s$, $v$ and $b_{attn}$ are all learnable parameters. With the attention distribution $a^t$, context vector $h_t^*$ is computed as the weighted sum of encoder's hidden states. Context vector is regarded as the attentional information in the source text: Pointer-Generator differs from typical Seq2Seq attention model in the generation process.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much improvement does their method get over the fine tuning baseline?",
    "llm_answer": "The context states that the proposed iterative learning approach significantly outperforms non-iterative settings, and that calibrating confidence using binary classification loss improves both reranking and assertion generation compared to the base model.  It also mentions performance augmentation is over 9% for BERT-Base and BERT-Large models compared to baselines.  However, the exact improvement over *fine-tuning* baselines is not specified.\n",
    "context": [
      {
        "id": "572cf0cb-57bc-4fef-94b1-18b3f7be4e4a",
        "metadata": {
          "vector_store_key": "1905.13413-4",
          "chunk_id": 17,
          "document_id": "1905.13413",
          "start_idx": 10441,
          "end_idx": 11152
        },
        "page_content": "We show both the results of using the confidence (E.q. 7 ) of the fine-tuned model to rerank the extractions of the base model (Rerank Only), and the end-to-end performance of the fine-tuned model in assertion generation (Generate). We found both settings lead to improved performance compared to the base model, which demonstrates that calibrating confidence using binary classification loss can improve the performance of both reranking and assertion generation. Finally, our proposed iterative learning approach (alg:iter, sec:ours) significantly outperforms non-iterative settings. We also investigate the performance of our iterative learning algorithm with respect to the number of iterations in fig:iter.",
        "type": "Document"
      },
      {
        "id": "610f509b-ccf3-4224-aa65-99841dc98dde",
        "metadata": {
          "vector_store_key": "1905.06566-4",
          "chunk_id": 36,
          "document_id": "1905.06566",
          "start_idx": 19794,
          "end_idx": 20574
        },
        "page_content": "We optimized our models using Adam with learning rate of 1e-4, $H$4 , $H$5 , L2 norm of 0.01, learning rate warmup 10,000 steps and learning rate decay afterwards using the strategies in vaswani:2017:nips. The dropout rate in all layers are 0.1. In pre-training stages, we trained our models until validation perplexities do not decrease significantly (around 45 epochs on GIGA-CM dataset and 100 to 200 epochs on CNNDM and NYT50). Training $H$6 for one epoch on GIGA-CM dataset takes approximately 20 hours. Our models during fine-tuning stage can be trained on a single GPU. The hyper-parameters are almost identical to these in the pre-training stages except that the learning rate is 5e-5, the batch size is 32, the warmup steps are 4,000 and we train our models for 5 epochs.",
        "type": "Document"
      },
      {
        "id": "0c4bd793-a8e9-4861-8601-03f439261b66",
        "metadata": {
          "vector_store_key": "1910.00458-6",
          "chunk_id": 21,
          "document_id": "1910.00458",
          "start_idx": 12016,
          "end_idx": 12817
        },
        "page_content": "In the table, we first report the accuracy of the SOTA models in the leaderboard. We then report the performance of our re-implementation of fine-tuned models as another set of strong baselines, among which the RoBERTa-Large model has already surpassed the previous SOTA. For these baselines, the top-level classifier is a two-layer FCNN for BERT-based models and a one-layer FCNN for the RoBERTa-Large model. Lastly, we report model performances that use all our proposed method, MMM (MAN classifier + speaker normalization + two stage learning strategies). As direct comparisons, we also list the accuracy increment between MMM and the baseline with the same sentence encoder marked by the parentheses, from which we can see that the performance augmentation is over 9% for BERT-Base and BERT-Large.",
        "type": "Document"
      },
      {
        "id": "1de9457d-2102-43f6-af81-4aa7864c7093",
        "metadata": {
          "vector_store_key": "1905.11901-4",
          "chunk_id": 14,
          "document_id": "1905.11901",
          "start_idx": 7819,
          "end_idx": 8508
        },
        "page_content": "We subsequently add the methods described in section SECREF3 , namely the bideep RNN, label smoothing, dropout, tied embeddings, layer normalization, changes to the BPE vocabulary size, batch size, model depth, regularization parameters and learning rate. Detailed hyperparameters are reported in Appendix SECREF7 . Table TABREF18 shows the effect of adding different methods to the baseline NMT system, on the ultra-low data condition (100k words of training data) and the full IWSLT 14 training corpus (3.2M words). Our \"mainstream improvements\" add around 6\u20137 BLEU in both data conditions. In the ultra-low data condition, reducing the BPE vocabulary size is very effective (+4.9 BLEU).",
        "type": "Document"
      },
      {
        "id": "1b1f9986-e4da-44a7-a6cd-13931a730da6",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 38,
          "document_id": "1909.13375",
          "start_idx": 19979,
          "end_idx": 20678
        },
        "page_content": "For the large models we used a batch size of 3 with a learning rate of $5e^{-6}$ and trained for 5 epochs, except for the model without the single-span heads that was trained with a batch size of 2 for 7 epochs. F1 was used as our validation metric. All models were trained on a single GPU with 12-16GB of memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much do they outpeform previous results on the word discrimination task?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "d28d8f41-1fee-4410-beaf-00c419d7d01d",
        "metadata": {
          "vector_store_key": "1906.06448-1",
          "chunk_id": 37,
          "document_id": "1906.06448",
          "start_idx": 21420,
          "end_idx": 22138
        },
        "page_content": "Table 8 shows the evaluation results by genre. This result shows that inference problems collected from linguistics publications are more challenging than crowdsourced inference problems, even if we add HELP to training sets. As shown in Figure 2 , the change in performance on problems from linguistics publications is milder than that on problems from crowdsourcing. This result also indicates the difficulty of problems from linguistics publications. Regarding non-monotone problems collected via crowdsourcing, there are very few non-monotone problems, so accuracy is 100%. Adding non-monotone problems to our test set is left for future work. Table 9 shows the evaluation results by type of linguistic phenomenon.",
        "type": "Document"
      },
      {
        "id": "3bb417b4-cec9-4c38-9627-1d4f81fa7c9a",
        "metadata": {
          "vector_store_key": "1904.08386-1",
          "chunk_id": 11,
          "document_id": "1904.08386",
          "start_idx": 6428,
          "end_idx": 7215
        },
        "page_content": "Both ELMo and BERT outperform GloVe, which intuitively makes sense because the latter do not model the order or structure of the words in each description. While the purity of our methods is higher than that of a random clustering, it is still far below 1. To provide additional context to these results, we now switch to our \u201codd-one-out\u201d task and compare directly to human performance. For each triplet of cities, we identify the intruder as the city with the maximum Euclidean distance from the other two. Interestingly, crowd workers achieve only slightly higher accuracy than ELMo city representations; their interannotator agreement is also low, which indicates that close reading to analyze literary coherence between multiple texts is a difficult task, even for human annotators.",
        "type": "Document"
      },
      {
        "id": "066146fb-6819-4cc7-9410-79a909a010c9",
        "metadata": {
          "vector_store_key": "1910.14497-2",
          "chunk_id": 21,
          "document_id": "1910.14497",
          "start_idx": 11743,
          "end_idx": 19554
        },
        "page_content": "Future work should include considering a more rigorous definition and non-binary of bias and experimenting with various embedding algorithms and network architectures. The authors would like to thank Tommi Jaakkola for stimulating discussions during the initial stages of this work. For Equation 4, as described in the original work, in regards to the k sample words $w_i$ is drawn from the corpus using the Unigram distribution raised to the 3/4 power. For reference, the most male socially-biased words include words such as:\u2019john\u2019, \u2019jr\u2019, \u2019mlb\u2019, \u2019dick\u2019, \u2019nfl\u2019, \u2019cfl\u2019, \u2019sgt\u2019, \u2019abbot\u2019, \u2019halfback\u2019, \u2019jock\u2019, \u2019mike\u2019, \u2019joseph\u2019,while the most female socially-biased words include words such as:\u2019feminine\u2019, \u2019marital\u2019, \u2019tatiana\u2019, \u2019pregnancy\u2019, \u2019eva\u2019, \u2019pageant\u2019, \u2019distress\u2019, \u2019cristina\u2019, \u2019ida\u2019, \u2019beauty\u2019, \u2019sexuality\u2019,\u2019fertility\u2019 'accountant', 'acquaintance', 'actor', 'actress', 'administrator', 'adventurer', 'advocate', 'aide', 'alderman', 'ambassador', 'analyst', 'anthropologist', 'archaeologist', 'archbishop', 'architect', 'artist', 'assassin', 'astronaut', 'astronomer', 'athlete', 'attorney', 'author', 'baker', 'banker', 'barber', 'baron', 'barrister', 'bartender', 'biologist', 'bishop', 'bodyguard', 'boss', 'boxer', 'broadcaster', 'broker', 'businessman', 'butcher', 'butler', 'captain', 'caretaker', 'carpenter', 'cartoonist', 'cellist', 'chancellor', 'chaplain', 'character', 'chef', 'chemist', 'choreographer', 'cinematographer', 'citizen', 'cleric', 'clerk', 'coach', 'collector', 'colonel', 'columnist', 'comedian', 'comic', 'commander', 'commentator', 'commissioner', 'composer', 'conductor', 'confesses', 'congressman', 'constable', 'consultant', 'cop', 'correspondent', 'counselor', 'critic', 'crusader', 'curator', 'dad', 'dancer', 'dean', 'dentist', 'deputy', 'detective', 'diplomat', 'director', 'doctor', 'drummer', 'economist', 'editor', 'educator', 'employee', 'entertainer', 'entrepreneur', 'envoy', 'evangelist', 'farmer', 'filmmaker', 'financier', 'fisherman', 'footballer', 'foreman', 'gangster', 'gardener', 'geologist', 'goalkeeper', 'guitarist', 'headmaster', 'historian', 'hooker', 'illustrator', 'industrialist', 'inspector', 'instructor', 'inventor', 'investigator', 'journalist', 'judge', 'jurist', 'landlord', 'lawyer', 'lecturer', 'legislator', 'librarian', 'lieutenant', 'lyricist', 'maestro', 'magician', 'magistrate', 'maid', 'manager', 'marshal', 'mathematician', 'mechanic', 'midfielder', 'minister', 'missionary', 'monk', 'musician', 'nanny', 'narrator', 'naturalist', 'novelist', 'nun', 'nurse', 'observer', 'officer', 'organist', 'painter', 'pastor', 'performer', 'philanthropist', 'philosopher', 'photographer', 'physician', 'physicist', 'pianist', 'planner', 'playwright', 'poet', 'policeman', 'politician', 'preacher', 'president', 'priest', 'principal', 'prisoner', 'professor', 'programmer', 'promoter', 'proprietor', 'prosecutor', 'protagonist', 'provost', 'psychiatrist', 'psychologist', 'rabbi', 'ranger', 'researcher', 'sailor', 'saint', 'salesman', 'saxophonist', 'scholar', 'scientist', 'screenwriter', 'sculptor', 'secretary', 'senator', 'sergeant', 'servant', 'singer', 'skipper', 'sociologist', 'soldier', 'solicitor', 'soloist', 'sportsman', 'statesman', 'steward', 'student', 'substitute', 'superintendent', 'surgeon', 'surveyor', 'swimmer', 'teacher', 'technician', 'teenager', 'therapist', 'trader', 'treasurer', 'trooper', 'trumpeter', 'tutor', 'tycoon', 'violinist', 'vocalist', 'waiter', 'waitress', 'warden', 'warrior', 'worker', 'wrestler', 'writer' Words used for WEAT statistic, consisting of baseline bias tests and gender bias tests in the format X vs Y / A vs B Flowers vs Insects / Pleasant vs Unpleasant X: \"aster\", \"clover\", \"hyacinth\", \"marigold\", \"poppy\", \"azalea\", \"crocus\", \"iris\", \"orchid\", \"rose\", \"bluebell\", \"daffodil\", \"lilac\", \"pansy\", \"tulip\", \"buttercup\", \"daisy\", \"lily\", \"peony\", \"violet\", \"carnation\", \"gladiola\", \"magnolia\", \"petunia\", \"zinnia\" Y: \"ant\", \"caterpillar\", \"flea\", \"locust\", \"spider\", \"bedbug\", \"centipede\", \"fly\", \"maggot\", \"tarantula\", \"bee\", \"cockroach\", \"gnat\", \"mosquito\", \"termite\", \"beetle\", \"cricket\", \"hornet\", \"moth\", \"wasp\", \"blackfly\", \"dragonfly\", \"horsefly\", \"roach\", \"weevil\" A: \"caress\", \"freedom\", \"health\", \"love\", \"peace\", \"cheer\", \"friend\", \"heaven\", \"loyal\", \"pleasure\", \"diamond\", \"gentle\", \"honest\", \"lucky\", \"rainbow\", \"diploma\", \"gift\", \"honor\", \"miracle\", \"sunrise\", \"family\", \"happy\", \"laughter\", \"paradise\", \"vacation\" B: \"abuse\", \"crash\", \"filth\", \"murder\", \"sickness\", \"accident\", \"death\", \"grief\", \"poison\", \"stink\", \"assault\", \"disaster\", \"hatred\", \"pollute\", \"tragedy\", \"divorce\", \"jail\", \"poverty\", \"ugly\", \"cancer\", \"kill\", \"rotten\", \"vomit\", \"agony\", \"prison\" Instruments vs Weapons / Pleasant vs Unpleasant: X: \"bagpipe\", \"cello\", \"guitar\", \"lute\", \"trombone\", \"banjo\", \"clarinet\", \"harmonica\", \"mandolin\", \"trumpet\", \"bassoon\", \"drum\", \"harp\", \"oboe\", \"tuba\", \"bell\", \"fiddle\", \"harpsichord\", \"piano\", \"viola\", \"bongo\", \"flute\", \"horn\", \"saxophone\", \"violin\" Y: \"arrow\", \"club\", \"gun\", \"missile\", \"spear\", \"ax\", \"dagger\", \"harpoon\", \"pistol\", \"sword\", \"blade\", \"dynamite\", \"hatchet\", \"rifle\", \"tank\", \"bomb\", \"firearm\", \"knife\", \"shotgun\", \"teargas\", \"cannon\", \"grenade\", \"mace\", \"slingshot\", \"whip\" A: \"caress\", \"freedom\", \"health\", \"love\", \"peace\", \"cheer\", \"friend\", \"heaven\", \"loyal\", \"pleasure\", \"diamond\", \"gentle\", \"honest\", \"lucky\", \"rainbow\", \"diploma\", \"gift\", \"honor\", \"miracle\", \"sunrise\", \"family\", \"happy\", \"laughter\", \"paradise\", \"vacation\" B: \"abuse\", \"crash\", \"filth\", \"murder\", \"sickness\", \"accident\", \"death\", \"grief\", \"poison\", \"stink\", \"assault\", \"disaster\", \"hatred\", \"pollute\", \"tragedy\", \"divorce\", \"jail\", \"poverty\", \"ugly\", \"cancer\", \"kill\", \"rotten\", \"vomit\", \"agony\", \"prison\" Male vs Female / Career vs Family: X: \"brother\", \"father\", \"uncle\", \"grandfather\", \"son\", \"he\", \"his\", \"him\", \"man\", \"himself\", \"men\", \"husband\", \"boy\", \"uncle\", \"nephew\", \"boyfriend\", \"king\", \"actor\" Y: \"sister\", \"mother\", \"aunt\", \"grandmother\", \"daughter\", \"she\", \"hers\", \"her\", \"woman\", \"herself\", \"women\", \"wife\", \"aunt\", \"niece\", \"girlfriend\", \"queen\", \"actress\" A: \"executive\", \"management\", \"professional\", \"corporation\", \"salary\", \"office\", \"business\", \"career\", \"industry\", \"company\", \"promotion\", \"profession\", \"CEO\", \"manager\", \"coworker\", \"entrepreneur\" B: \"home\", \"parents\", \"children\", \"family\", \"cousins\", \"marriage\", \"wedding\", \"relatives\", \"grandparents\", \"grandchildren\", \"nurture\", \"child\", \"toddler\", \"infant\", \"teenager\" Math vs Art / Male vs Female: X: \"math\", \"algebra\", \"geometry\", \"calculus\", \"equations\", \"computation\", \"numbers\", \"addition\", \"trigonometry\", \"arithmetic\", \"logic\", \"proofs\", \"multiplication\", \"mathematics\" Y: \"poetry\", \"art\", \"Shakespeare\", \"dance\", \"literature\", \"novel\", \"symphony\", \"drama\", \"orchestra\", \"music\", \"ballet\", \"arts\", \"creative\", \"sculpture\" A: \"brother\", \"father\", \"uncle\", \"grandfather\", \"son\", \"he\", \"his\", \"him\", \"man\", \"himself\", \"men\", \"husband\", \"boy\", \"uncle\", \"nephew\", \"boyfriend\", \"king\", \"actor\" B: \"sister\", \"mother\", \"aunt\", \"grandmother\", \"daughter\", \"she\", \"hers\", \"her\", \"woman\", \"herself\", \"women\", \"wife\", \"aunt\", \"niece\", \"girlfriend\", \"queen\", \"actress\" Science vs Art / Male8 vs Female8: X:\"science\", \"technology\", \"physics\", \"chemistry\", \"Einstein\", \"NASA\", \"experiment\", \"astronomy\", \"biology\", \"aeronautics\", \"mechanics\", \"thermodynamics\" Y: \"poetry\", \"art\", \"Shakespeare\", \"dance\", \"literature\", \"novel\", \"symphony\", \"drama\", \"orchestra\", \"music\", \"ballet\", \"arts\", \"creative\", \"sculpture\" A: \"brother\", \"father\", \"uncle\", \"grandfather\", \"son\", \"he\", \"his\", \"him\", \"man\", \"himself\", \"men\", \"husband\", \"boy\", \"uncle\", \"nephew\", \"boyfriend\" B: \"sister\", \"mother\", \"aunt\", \"grandmother\", \"daughter\", \"she\", \"hers\", \"her\", \"woman\", \"herself\", \"women\", \"wife\", \"aunt\", \"niece\", \"girlfriend\"",
        "type": "Document"
      },
      {
        "id": "eb50f458-36f8-4576-966b-fe75b44e31c8",
        "metadata": {
          "vector_store_key": "1809.08731-0",
          "chunk_id": 24,
          "document_id": "1809.08731",
          "start_idx": 13599,
          "end_idx": 14284
        },
        "page_content": "This is interesting, since they, in contrast to traditional metrics, do not require reference compressions. However, their correlation with human fluency judgments is strictly lower than that of their respective SLOR counterparts. The difference between WordSLOR and WordNCE is bigger than that between WPSLOR and WPNCE. This might be due to accounting for differences in frequencies being more important for words than for WordPieces. Both WordPPL and WPPPL clearly underperform as compared to all other metrics in our experiments. The traditional word-overlap metrics all perform similarly. ROUGE-L-mult and LR2-F-mult are best and worst, respectively. Results are shown in Table 7 .",
        "type": "Document"
      },
      {
        "id": "050432d2-b325-4282-ae97-1513abbfedc7",
        "metadata": {
          "vector_store_key": "2003.06279-7",
          "chunk_id": 36,
          "document_id": "2003.06279",
          "start_idx": 21531,
          "end_idx": 22144
        },
        "page_content": "While Figures FIGREF14 \u2013 FIGREF16 show the relative behavior in the accuracy, it still interesting to observe the absolute accuracy rate obtained with the classifiers. In Table TABREF17, we show the best accuracy rate (i.e. $\\max \\Gamma _+ = \\max _p \\Gamma _+(p)$) for GloVe. We also show the average difference in performance ($\\langle \\Gamma _+ - \\Gamma _0 \\rangle $) and the total number of cases in which an improvement in performance was observed ($N_+$). $N_+$ ranges in the interval $0 \\le N_+ \\le 20$. Table TABREF17 summarizes the results obtained for $w = \\lbrace 1.0, 5.0, 10.0\\rbrace $ thousand words.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many paraphrases are generated per question?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "d0eaa2d2-3f13-4bba-b87f-450cda299dc3",
        "metadata": {
          "vector_store_key": "1601.06068-3",
          "chunk_id": 36,
          "document_id": "1601.06068",
          "start_idx": 19021,
          "end_idx": 19683
        },
        "page_content": "For our development experiments we tune the models on held-out data consisting of 30% training questions, while for final testing we use the complete training data. We use average precision (avg P.), average recall (avg R.) and average F $_1$ (avg F $_1$ ) proposed by berantsemantic2013 as evaluation metrics. We use GraphParser without paraphrases as our baseline. This gives an idea about the impact of using paraphrases. We compare our paraphrasing models with monolingual machine translation based model for paraphrase generation BIBREF24 , BIBREF36 . In particular, we use Moses BIBREF37 to train a monolingual phrase-based MT system on the Paralex corpus.",
        "type": "Document"
      },
      {
        "id": "c2a27e9c-3950-477d-a8bd-1ace2ba538db",
        "metadata": {
          "vector_store_key": "1601.06068-3",
          "chunk_id": 22,
          "document_id": "1601.06068",
          "start_idx": 11970,
          "end_idx": 12688
        },
        "page_content": "To improve its precision, we build a binary classifier to filter the generated paraphrases. We randomly select 100 distinct questions from the Paralex corpus and generate paraphrases using our generation algorithm with various lattice settings. We randomly select 1,000 pairs of input-sampled sentences and manually annotate them as \u201ccorrect\u201d or \u201cincorrect\u201d paraphrases. We train our classifier on this manually created training data. We follow madnani2012, who used MT metrics for paraphrase identification, and experiment with 8 MT metrics as features for our binary classifier. In addition, we experiment with a binary feature which checks if the sampled paraphrase preserves named entities from the input sentence.",
        "type": "Document"
      },
      {
        "id": "e94e8ac9-6edd-4f5a-bd8b-dadc3ce90eb8",
        "metadata": {
          "vector_store_key": "1601.06068-3",
          "chunk_id": 42,
          "document_id": "1601.06068",
          "start_idx": 22384,
          "end_idx": 23076
        },
        "page_content": "We described a grammar method to generate paraphrases for questions, and applied it to a question answering system based on semantic parsing. We showed that using paraphrases for a question answering system is a useful way to improve its performance. Our method is rather generic and can be applied to any question answering system. The authors would like to thank Nitin Madnani for his help with the implementation of the paraphrase classifier. We would like to thank our anonymous reviewers for their insightful comments. This research was supported by an EPSRC grant (EP/L02411X/1), the H2020 project SUMMA (under grant agreement 688139), and a Google PhD Fellowship for the second author.",
        "type": "Document"
      },
      {
        "id": "7f06fb04-f233-42da-acf9-d3b7b640603b",
        "metadata": {
          "vector_store_key": "1601.06068-3",
          "chunk_id": 7,
          "document_id": "1601.06068",
          "start_idx": 4118,
          "end_idx": 4724
        },
        "page_content": "The rest of this section describes our paraphrase generation algorithm. We define our paraphrase generation task as a sampling problem from an L-PCFG $G_{\\mathrm {syn}}$ , which is estimated from a large corpus of parsed questions. Once this grammar is estimated, our algorithm follows a pipeline with two major steps. We first build a word lattice $W_q$ for the input question $q$ . We use the lattice to constrain our paraphrases to a specific choice of words and phrases that can be used. Once this lattice is created, a grammar $G_{\\mathrm {syn}}^{\\prime }$ is then extracted from $G_{\\mathrm {syn}}$ .",
        "type": "Document"
      },
      {
        "id": "49b84072-3e99-447e-ab07-a4f8d6f14e2a",
        "metadata": {
          "vector_store_key": "1601.06068-3",
          "chunk_id": 9,
          "document_id": "1601.06068",
          "start_idx": 5257,
          "end_idx": 5915
        },
        "page_content": "Once $G_{\\mathrm {syn}}^{\\prime }$ is generated, we sample paraphrases of the input question $q$ . These paraphrases are further filtered with a classifier to improve the precision of the generated paraphrases. We train the L-PCFG $G_{\\mathrm {syn}}$ on the Paralex corpus BIBREF9 . Paralex is a large monolingual parallel corpus, containing 18 million pairs of question paraphrases with 2.4M distinct questions in the corpus. It is suitable for our task of generating paraphrases since its large scale makes our model robust for open-domain questions. We construct a treebank by parsing 2.4M distinct questions from Paralex using the BLLIP parser BIBREF25 .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How strong was the correlation between exercise and diabetes?",
    "llm_answer": "The context states that exercising is associated with controlling diabetes and assisting with meal planning.  However, it does not quantify the strength of the correlation.\n",
    "context": [
      {
        "id": "8196c101-09d6-4b02-807e-006c4bca7297",
        "metadata": {
          "vector_store_key": "1709.07916-3",
          "chunk_id": 1,
          "document_id": "1709.07916",
          "start_idx": 698,
          "end_idx": 1520
        },
        "page_content": "Excess energy intake and inadequate energy expenditure both contribute to weight gain and diabetes BIBREF3 , BIBREF4 . Obesity can be reduced through modifiable lifestyle behaviors such as diet and exercise BIBREF4 . There are several comorbidities associated with being overweight or obese, such as diabetes BIBREF5 . The prevalence of diabetes in adults has risen globally from 4.7% in 1980 to 8.5% in 2014. Current projections estimate that by 2050, 29 million Americans will be diagnosed with type 2 diabetes, which is a 165% increase from the 11 million diagnosed in 2002 BIBREF6 . Studies show that there are strong relations among diabetes, diet, exercise, and obesity (DDEO) BIBREF7 , BIBREF4 , BIBREF8 , BIBREF9 ; however, the general public's perception of DDEO remains limited to survey-based studies BIBREF10 .",
        "type": "Document"
      },
      {
        "id": "55e74231-d4d7-4551-98c9-f2c3c067a69d",
        "metadata": {
          "vector_store_key": "1709.07916-0",
          "chunk_id": 26,
          "document_id": "1709.07916",
          "start_idx": 15109,
          "end_idx": 15925
        },
        "page_content": "The recent mobile gaming phenomenon Pokeman-Go game BIBREF68 was highly associated with the exercise topic. Pokemon-Go allows users to operate in a virtual environment while simultaneously functioning in the real word. Capturing Pokemons, battling characters, and finding physical locations for meeting other users required physically activity to reach predefined locations. These themes reflect on the potential of augmented reality in increasing patients' physical activity levels BIBREF69 . Obesity had the lowest number of subtopics in our study. Three of the subtopics were related to other diseases such as diabetes (Tables TABREF7 and TABREF8 ). The scholarly literature has well documented the possible linkages between obesity and chronic diseases such as diabetes BIBREF1 as supported by the study results.",
        "type": "Document"
      },
      {
        "id": "62eed83f-8107-44da-b8f9-e1647b72dde2",
        "metadata": {
          "vector_store_key": "1709.07916-3",
          "chunk_id": 25,
          "document_id": "1709.07916",
          "start_idx": 14645,
          "end_idx": 15348
        },
        "page_content": "The diet plans of celebrities were also considered influential to explaining and informing diet opinions of Twitter users BIBREF64 . Exercise themes show the Twitter users' association of exercise with \u201cbrain\" benefits such as increased memory and cognitive performance (Tables TABREF7 and TABREF8 ) BIBREF65 . The topics also confirm that exercising is associated with controlling diabetes and assisting with meal planning BIBREF66 , BIBREF9 , and obesity BIBREF67 . Additionally, we found the Twitter users mentioned exercise topics about the use of computer games that assist with exercising. The recent mobile gaming phenomenon Pokeman-Go game BIBREF68 was highly associated with the exercise topic.",
        "type": "Document"
      },
      {
        "id": "66fd8119-25c9-4780-8747-3904e69deb8a",
        "metadata": {
          "vector_store_key": "1709.07916-3",
          "chunk_id": 28,
          "document_id": "1709.07916",
          "start_idx": 16246,
          "end_idx": 17127
        },
        "page_content": "Although considered a perplexing finding, recent studies have been conducted to identify possible correlation between obesity and Alzheimer's disease BIBREF71 , BIBREF72 , BIBREF73 . Indeed, Twitter users have expressed opinions about the study of Alzheimer's disease and the linkage between these two topics. This paper addresses a need for clinical providers, public health experts, and social scientists to utilize a large conversational dataset to collect and utilize population level opinions and information needs. Although our framework is applied to Twitter, the applications from this study can be used in patient communication devices monitored by physicians or weight management interventions with social media accounts, and support large scale population-wide initiatives to promote healthy behaviors and preventative measures for diabetes, diet, exercise, and obesity.",
        "type": "Document"
      },
      {
        "id": "4c8626db-d027-400d-a794-72f26fc58165",
        "metadata": {
          "vector_store_key": "1709.07916-0",
          "chunk_id": 23,
          "document_id": "1709.07916",
          "start_idx": 13334,
          "end_idx": 14115
        },
        "page_content": "The subtopic Alzheimer is also shown in the obesity subtopics. This overlap between categories prompts the discussion of research and linkages among obesity, diabetes, and Alzheimer's disease. Type 2 diabetes was another subtopic that was also expressed by users and scientifically documented in the literature. The inclusion of Yoga in posts about diabetes is interesting. While yoga would certainly be labeled as a form of fitness, when considering the post, it was insightful to see discussion on the mental health benefits that yoga offers to those living with diabetes BIBREF63 . Diet had the highest number of subtopics. For example, religious diet activities such as fasting during the month of Ramadan for Muslims incorporated two subtopics categorized under the diet topic",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How were topics of interest about DDEO identified?",
    "llm_answer": "Tweets using the words \"diabetes, diet, exercise, and obesity\" were collected using Twitter's API.  A limited number of queries were used to select the initial pool of tweets.\n",
    "context": [
      {
        "id": "3a5364a6-4ecb-4bcf-8a57-1b7788449fc1",
        "metadata": {
          "vector_store_key": "1709.07916-1",
          "chunk_id": 7,
          "document_id": "1709.07916",
          "start_idx": 4149,
          "end_idx": 4869
        },
        "page_content": "These common public opinions/topics and perceptions can be used by providers and public health agencies to better understand the common opinions of their population denominators in regard to DDEO, and reflect upon those opinions accordingly. Our approach uses semantic and linguistics analyses for disclosing health characteristics of opinions in tweets containing DDEO words. The present study included three phases: data collection, topic discovery, and topic-content analysis. This phase collected tweets using Twitter's Application Programming Interfaces (API) BIBREF43 . Within the Twitter API, diabetes, diet, exercise, and obesity were selected as the related words BIBREF4 and the related health areas BIBREF19 .",
        "type": "Document"
      },
      {
        "id": "3e1385d9-2c27-4449-8372-f72feac0122c",
        "metadata": {
          "vector_store_key": "1709.07916-1",
          "chunk_id": 30,
          "document_id": "1709.07916",
          "start_idx": 17334,
          "end_idx": 18000
        },
        "page_content": "Second, we used a limited number of queries to select the initial pool of tweets, thus perhaps missing tweets that may have been relevant to DDEO but have used unusual terms referenced. Third, our analysis only included tweets generated in one month; however, as our previous work has demonstrated BIBREF42 , public opinion can change during a year. Additionally, we did not track individuals across time to detect changes in common themes discussed. Our future research plans includes introducing a dynamic framework to collect and analyze DDEO related tweets during extended time periods (multiple months) and incorporating spatial analysis of DDEO-related tweets.",
        "type": "Document"
      },
      {
        "id": "357f7b56-3a2c-498b-8071-7cc8a3e6d7b5",
        "metadata": {
          "vector_store_key": "1709.07916-1",
          "chunk_id": 20,
          "document_id": "1709.07916",
          "start_idx": 11392,
          "end_idx": 12169
        },
        "page_content": "The words with italic and underline styles in Table 2 demonstrate the relation among the four DDEO areas. Our results show users' interest about posting their opinions, sharing information, and conversing about exercise & diabetes, exercise & diet, diabetes & diet, diabetes & obesity, and diet & obesity (Figure FIGREF9 ). The strongest correlation among the topics was determined to be between exercise and obesity ( INLINEFORM0 ). Other notable correlations were: diabetes and obesity ( INLINEFORM1 ), and diet and obesity ( INLINEFORM2 ). Diabetes, diet, exercise, and obesity are common public health related opinions. Analyzing individual- level opinions by automated algorithmic techniques can be a useful approach to better characterize health opinions of a population.",
        "type": "Document"
      },
      {
        "id": "afbc38a0-77f1-4ff3-af3d-2254af5c9a79",
        "metadata": {
          "vector_store_key": "1709.07916-1",
          "chunk_id": 31,
          "document_id": "1709.07916",
          "start_idx": 18000,
          "end_idx": 18751
        },
        "page_content": "Our future research plans includes introducing a dynamic framework to collect and analyze DDEO related tweets during extended time periods (multiple months) and incorporating spatial analysis of DDEO-related tweets. This study represents the first step in developing routine processes to collect, analyze, and interpret DDEO-related posts to social media around health-related topics and presents a transdisciplinary approach to analyzing public discussions around health topics. With 2.34 billion social media users in 2016, the ability to collect and synthesize social media data will continue to grow. Developing methods to make this process more streamlined and robust will allow for more rapid identification of public health trends in real time.",
        "type": "Document"
      },
      {
        "id": "5ced13a5-2476-41e2-8d47-5bfb109a2673",
        "metadata": {
          "vector_store_key": "1709.07916-1",
          "chunk_id": 29,
          "document_id": "1709.07916",
          "start_idx": 17127,
          "end_idx": 17879
        },
        "page_content": "Although our framework is applied to Twitter, the applications from this study can be used in patient communication devices monitored by physicians or weight management interventions with social media accounts, and support large scale population-wide initiatives to promote healthy behaviors and preventative measures for diabetes, diet, exercise, and obesity. This research has some limitations. First, our DDEO analysis does not take geographical location of the Twitter users into consideration and thus does not reveal if certain geographical differences exists. Second, we used a limited number of queries to select the initial pool of tweets, thus perhaps missing tweets that may have been relevant to DDEO but have used unusual terms referenced.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do their train their embeddings?",
    "llm_answer": "The context describes various embedding training methods: CBOW embeddings trained on comments (with different tokenization methods), sentence embeddings trained using the RoBERTa architecture, and pre-trained embeddings like GloVe and contextual embeddings (BERT).  Some teams trained their own embeddings.\n",
    "context": [
      {
        "id": "22b913b3-266c-40fe-adc1-813047a68289",
        "metadata": {
          "vector_store_key": "1910.12618-6",
          "chunk_id": 30,
          "document_id": "1910.12618",
          "start_idx": 16308,
          "end_idx": 16981
        },
        "page_content": "In order to obtain the embedding, the texts are first converted into a sequence of integers: each word is given a number ranging from 1 to $V$, where $V$ is the vocabulary size (0 is used for padding or unknown words in the test set). One must then calculate the maximum sequence length $S$, and sentences of length shorter than $S$ are then padded by zeros. During the training process of the network, for each word a $q$ dimensional real-valued vector representation is calculated simultaneously to the rest of the weights of the network. Ergo a sentence of $S$ words is translated into a sequence of $S$ $q$-sized vectors, which is then fed into a recurrent neural unit.",
        "type": "Document"
      },
      {
        "id": "412f0ac4-c554-4507-98d1-390918d910ae",
        "metadata": {
          "vector_store_key": "1910.11493-0",
          "chunk_id": 31,
          "document_id": "1910.11493",
          "start_idx": 18060,
          "end_idx": 18722
        },
        "page_content": "In the submitted version of the system, the input is split into short chunks corresponding to the target word plus one word of context on either side, and the system is trained to output the corresponding lemmas and tags for each three-word chunk. Several teams relied on external resources to improve their lemmatization and feature analysis. Several teams made use of pre-trained embeddings. CHARLES-SAARLAND-2 and UFALPRAGUE-1 used pretrained contextual embeddings (BERT) provided by Google BIBREF22. CBNU-1 used a mix of pre-trained embeddings from the CoNLL 2017 shared task and fastText. Further, some teams trained their own embeddings to aid performance.",
        "type": "Document"
      },
      {
        "id": "5603f527-cf97-47b9-8799-eb636cb66ef2",
        "metadata": {
          "vector_store_key": "1703.04617-0",
          "chunk_id": 8,
          "document_id": "1703.04617",
          "start_idx": 4608,
          "end_idx": 5396
        },
        "page_content": "And we use the pre-trained 300-D GloVe vectors BIBREF20 (see the experiment section for details) to initialize our word-level embedding. Each word is therefore represented as the concatenation of the character-composition vector and word-level embedding. This is performed on both questions and documents, resulting in two matrices: the $\\mathbf {Q}^e \\in \\mathbb {R} ^{N\\times d_w}$ for a question and the $\\mathbf {D}^e \\in \\mathbb {R} ^{M\\times d_w}$ for a document, where $N$ is the question length (number of word tokens), $M$ is the document length, and $d_w$ is the embedding dimensionality. The above word representation focuses on representing individual words, and an input encoder here employs recurrent neural networks to obtain the representation of a word under its context.",
        "type": "Document"
      },
      {
        "id": "ddcf9218-124f-4751-801a-d5493a7de604",
        "metadata": {
          "vector_store_key": "1910.05608-2",
          "chunk_id": 17,
          "document_id": "1910.05608",
          "start_idx": 9441,
          "end_idx": 10142
        },
        "page_content": "If the class weight is not set, we find that model cannot adjust parameters. The model tends to output all clean classes. We experiment 8 types of embedding in total: comment: CBOW embedding training in all dataset comment, each word is splited by space. Embedding size is 200. comment_bpe: CBOW embedding training in all dataset comment, each word is splited by subword bpe. Embedding size is 200. comment_tokenize: CBOW embedding training in all dataset comment, each word is splited by space. Before split by space, word is concatenated by using BIBREF9, BIBREF13, BIBREF10. Embedding size is 200. roberta: sentence embedding training in all dataset comment, training by using RoBERTa architecture.",
        "type": "Document"
      },
      {
        "id": "28662374-887d-4024-b8ea-4d0583f8576b",
        "metadata": {
          "vector_store_key": "1909.00154-2",
          "chunk_id": 24,
          "document_id": "1909.00154",
          "start_idx": 13237,
          "end_idx": 14055
        },
        "page_content": "This formalization is the simplest version, without loss of generality. In practice, as seen below, we will model multiple embeddings matrices simultaneously, and will add regularization terms to the loss function, so the models tested in this paper consist of compositions of the above. So these so called embeddings are in fact a relatively shallow data representation in a simple neural network. What is their added value? Obviously, the first practical benefit is dimensionality reduction, because now there is a mapping between each of the $C$ words to a unique vector of size $K$. The second aspect is that this new representation is the one that maximizes the performance towards a specific task (in our example, prediction of the next word), therefore it is a supervised process, as opposed for example to PCA.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they model travel behavior?",
    "llm_answer": "The model uses a mapping that preserves semantic distance relative to a choice problem, analogous to word embeddings.  It also uses attention mechanisms to correlate navigation instructions with a topological map of the environment to predict navigation plans.\n",
    "context": [
      {
        "id": "9807f8e2-43e3-453f-95e2-8fbcb20126b7",
        "metadata": {
          "vector_store_key": "1807.03367-5",
          "chunk_id": 21,
          "document_id": "1807.03367",
          "start_idx": 11520,
          "end_idx": 12701
        },
        "page_content": "Specifically, we let trained tourist models undertake random walks, using the following protocol: at each step, the tourist communicates its observations and actions to the guide, who predicts the tourist's location. If the guide predicts that the tourist is at target, we evaluate its location. If successful, the task ends, otherwise we continue until there have been three wrong evaluations. The protocol is given as pseudo-code in Appendix SECREF12 . The designed navigation protocol relies on a trained localization model that predicts the tourist's location from a communicated message. Before we formalize this localization sub-task in Section UID21 , we further introduce two simplifying assumptions\u2014perfect perception and orientation-agnosticism\u2014so as to overcome some of the difficulties we encountered in preliminary experiments. paragraph4 0.1ex plus0.1ex minus.1ex-1em Perfect Perception Early experiments revealed that perceptual grounding of landmarks is difficult: we set up a landmark classification problem, on which models with extracted CNN BIBREF7 or text recognition features BIBREF8 barely outperform a random baseline\u2014see Appendix SECREF13 for full details.",
        "type": "Document"
      },
      {
        "id": "a00e51a6-f5f1-4fa6-8830-bed50685019e",
        "metadata": {
          "vector_store_key": "1909.00154-1",
          "chunk_id": 35,
          "document_id": "1909.00154",
          "start_idx": 19307,
          "end_idx": 19967
        },
        "page_content": "This is particularly relevant if one considers the complicated challenges for opening or sharing travel survey datasets in our field. Of course, a major question arises: are behaviors that consistent across the world? There are certainly nuances across the world, but we believe that general patterns would emerge (e.g. a \u201cbusiness\" trip purpose will be closer to \u201cwork\" than \u201cleisure\", in a departure time choice model; \u201cstudent\" will be closer to \u201cunemployed\" than to \u201cretired\" in a car ownership model). We believe that, as with word embeddings, a mapping that preserves semantic distance relative to a certain choice problem, should be useful for modeling.",
        "type": "Document"
      },
      {
        "id": "78f6379a-3da8-4cb5-bdb1-2cfee0e7794f",
        "metadata": {
          "vector_store_key": "1807.03367-3",
          "chunk_id": 3,
          "document_id": "1807.03367",
          "start_idx": 1999,
          "end_idx": 2553
        },
        "page_content": "We introduce the Talk the Walk dataset, where the aim is for two agents, a \u201cguide\u201d and a \u201ctourist\u201d, to interact with each other via natural language in order to achieve a common goal: having the tourist navigate towards the correct location. The guide has access to a map and knows the target location, but does not know where the tourist is; the tourist has a 360-degree view of the world, but knows neither the target location on the map nor the way to it. The agents need to work together through communication in order to successfully solve the task.",
        "type": "Document"
      },
      {
        "id": "db08d4cf-4c1e-4b90-b4fd-36f8c389bccb",
        "metadata": {
          "vector_store_key": "1810.00663-9",
          "chunk_id": 6,
          "document_id": "1810.00663",
          "start_idx": 3263,
          "end_idx": 3960
        },
        "page_content": "More specifically, the proposed model takes as input user directions in text form, the behavioral graph of the environment encoded as INLINEFORM0 node; edge; node INLINEFORM1 triplets, and the initial location of the robot in the graph. The model then predicts a set of behaviors to reach the desired destination according to the instructions and the map (Fig. FIGREF4 (c)). Our main insight is that using attention mechanisms to correlate navigation instructions with the topological map of the environment can facilitate predicting correct navigation plans. This work also contributes a new dataset of INLINEFORM0 pairs of free-form natural language instructions and high-level navigation plans.",
        "type": "Document"
      },
      {
        "id": "94d3f835-dc2c-40a0-a575-9ea9711e572f",
        "metadata": {
          "vector_store_key": "1807.03367-3",
          "chunk_id": 14,
          "document_id": "1807.03367",
          "start_idx": 7446,
          "end_idx": 8060
        },
        "page_content": "That is, the dialogue is driven by uncertainty in the tourist's current location and the properties of the target location: if the exact location and orientation of the tourist were known, it would suffice to communicate a sequence of actions. For the Talk The Walk task, we randomly choose one of the five neighborhoods, and subsample a 4x4 grid (one block with four complete intersections) from the entire grid. We specify the boundaries of the grid by the top-left and bottom-right corners INLINEFORM0 . Next, we construct the overhead map of the environment, i.e. INLINEFORM1 with INLINEFORM2 and INLINEFORM3 .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do their interpret the coefficients?",
    "llm_answer": "The sign of the coefficient $\\beta_w$ indicates the impact on the time series.  A positive coefficient ($\\beta_w > 0$) indicates a rise in the time series, and a negative coefficient ($\\beta_w < 0$) indicates a decline.\n",
    "context": [
      {
        "id": "dd646278-2540-4dcd-b31b-1407b0a281e8",
        "metadata": {
          "vector_store_key": "2001.07209-4",
          "chunk_id": 34,
          "document_id": "2001.07209",
          "start_idx": 18438,
          "end_idx": 19308
        },
        "page_content": "$\\beta _f$, $\\beta _l$, $\\beta _c$, and $\\beta _0$ are the corresponding factor weights and intercept, respectively; and $\\epsilon \\sim \\mathcal {N}(0, \\sigma )$ is the regression error term. Table TABREF27 shows the results of multiple linear regression. We observe that concreteness is a significant negative predictor of change toward moral relevance, suggesting that abstract concepts are more strongly associated with increasing moral relevance over time than concrete concepts. This significance persists under partial correlation test against the control factors ($p < 0.01$). We further verified the diachronic component of this effect in a random permutation analysis. We generated 1,000 control time courses by randomly shuffling the 20 decades in our data, and repeated the regression analysis to obtain a control distribution for each regression coefficient.",
        "type": "Document"
      },
      {
        "id": "a201475f-4ea8-4787-9eaf-012108f9dc68",
        "metadata": {
          "vector_store_key": "1909.00154-0",
          "chunk_id": 15,
          "document_id": "1909.00154",
          "start_idx": 8673,
          "end_idx": 9472
        },
        "page_content": "If there is a lot of variance in the associated behavior, the variance of the coefficient will also be large, and the coefficient will be considered statistically insignificant. The benefit of representations that map into a latent space, like embeddings and PCA, is that such a space is inevitably shared, and thus every observation contributes indirectly to all category variables. This comes with no interpretability cost, because one can always map to the \u201cdummy\" space and analyse the individual coefficients, as will be shown in our experiments. The idea of text embeddings comes from a simple re-representation necessity. A natural-language processing system is itself also a numeric machine, therefore it requires each individual word in a dictionary to match its own numeric representation.",
        "type": "Document"
      },
      {
        "id": "0b028722-e621-4383-b20f-24c7f2e106e8",
        "metadata": {
          "vector_store_key": "2001.07209-4",
          "chunk_id": 33,
          "document_id": "2001.07209",
          "start_idx": 18199,
          "end_idx": 18863
        },
        "page_content": "Since we were interested in moral relevance change within this large set of words, we restricted our analysis to those words whose model predictions indicate change in moral relevance, in either direction, from the 1800s to the 1990s. We performed a multiple linear regression under the following model: Here $\\rho (w)$ is the slope of moral relevance change for word $w$; $f(w$) is its average frequency; $l(w)$ is its character length; $c(w)$ is its concreteness rating; $\\beta _f$, $\\beta _l$, $\\beta _c$, and $\\beta _0$ are the corresponding factor weights and intercept, respectively; and $\\epsilon \\sim \\mathcal {N}(0, \\sigma )$ is the regression error term.",
        "type": "Document"
      },
      {
        "id": "00f151b3-1ed8-4d5d-99bc-fb7ad58a472b",
        "metadata": {
          "vector_store_key": "1910.12618-9",
          "chunk_id": 56,
          "document_id": "1910.12618",
          "start_idx": 29981,
          "end_idx": 30543
        },
        "page_content": "We also performed the analysis of the relevant words for the LASSO. In order to do that, we examined the words $w$ with the largest associated coefficients $\\beta _w$ (in absolute value) in the regression. Since the TF-IDF matrix has positive coefficients, it is possible to interpret the sign of the coefficient $\\beta _w$ as its impact on the time series. For instance if $\\beta _w > 0$ then the presence of the word $w$ causes a rise the time series (respectively if $\\beta _w < 0$, it entails a decline). The results are plotted fig. FIGREF35 for the the UK.",
        "type": "Document"
      },
      {
        "id": "e09fa85e-e0c3-400a-98dd-9548f6632a71",
        "metadata": {
          "vector_store_key": "1909.00154-0",
          "chunk_id": 27,
          "document_id": "1909.00154",
          "start_idx": 15084,
          "end_idx": 15749
        },
        "page_content": "Essentially, the vector $King-Man$ corresponds to the concept of \u201ccrowning\" (therefore $Woman+crowning=Queen$). The same could be done with many other concept pairs. Figure FIGREF11 show also an alternative interpretation of \u201cman-female\", as well as examples with cities and verb tense. Finally, another relevant note on the embeddings representation is that, just like the PCA encoding, one can always project back into the original space and use this for interpretability. In other words, since there is a 1-to-1 mapping from each category to its encoding, there is also a 1-to-1 mapping between a model that uses dummy variables and a model using such encodings.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much do they outperform previous state-of-the-art models?",
    "llm_answer": "Our best model improves CoNLL F1 score by 2.93% and 3.01% over Stanford, 1.41% and 1.77% over Multigraph, and 2.62% and 3.02% over MIR.\n",
    "context": [
      {
        "id": "60552214-8a5b-4305-aa4b-bfbdc35d8792",
        "metadata": {
          "vector_store_key": "2002.11402-4",
          "chunk_id": 16,
          "document_id": "2002.11402",
          "start_idx": 9020,
          "end_idx": 10286
        },
        "page_content": "Similarly, Stanford did well in Precision but could not catch up with Spacy and our model in terms of Recall. Flair overall performed poorly, but as said before these open-source models are not trained for our particular use-case. Lets check some examples for detailed analysis of the models and their results. Following is the economy related news. Example 1 : around $1\u20131.5 trillion or around two percent of global gdp, are lost to corruption every year, president of the natural resource governance institute nrgi has said. speaking at a panel on integrity in public governance during the world bank group and international monetary fund annual meeting on sunday, daniel kaufmann, president of nrgi, presented the statistic, result of a study by the nrgi, an independent, non-profit organisation based in new york. however, according to kaufmann, the figure is only the direct costs of corruption as it does not factor in the opportunities lost on innovation and productivity, xinhua news agency reported. a country that addresses corruption and significantly improves rule of law can expect a huge increase in per capita income in the long run, the study showed. it will also see similar gains in reducing infant mortality and improving education, said kaufmann.",
        "type": "Document"
      },
      {
        "id": "2e7cfe41-b41c-475c-83f6-e7d39903180d",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 40,
          "document_id": "1909.13375",
          "start_idx": 21052,
          "end_idx": 21792
        },
        "page_content": "While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1. All that, while keeping in mind we compare a base model to a large model (for reference, note the 8 point improvement between MTMSNbase and MTMSNlarge in both EM and F1). Our best model, large-squad, exhibits a huge improvement of 29.7 in EM and 15.1 in F1 compared to MTMSNlarge. When comparing single-span performance, our best model exhibits slightly better results, but it should be noted that it retains the single-span heads from NABERT+, while in MTMSN they have one head to predict both single-span and multi-span answers.",
        "type": "Document"
      },
      {
        "id": "59630f23-3c9c-47bb-b331-4027472a3a6b",
        "metadata": {
          "vector_store_key": "1603.04553-1",
          "chunk_id": 20,
          "document_id": "1603.04553",
          "start_idx": 10753,
          "end_idx": 11450
        },
        "page_content": "Specifically, our model achieves improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford system, the winner of the CoNLL 2011 shared task, on the CoNLL 2012 development and test sets, respectively. The improvements on CoNLL F1 score over the Multigraph model are 1.41% and 1.77% on the development and test sets, respectively. Comparing with the MIR model, we obtain significant improvements of 2.62% and 3.02% on CoNLL F1 score. To make a thorough empirical comparison with previous studies, Table 3 (below the dashed line) also shows the results of some state-of-the-art supervised coreference resolution systems \u2014 IMS: the second best system in the CoNLL 2012 shared task BIBREF28 ;",
        "type": "Document"
      },
      {
        "id": "ef18c881-3bf1-4e36-94ad-61037dd58977",
        "metadata": {
          "vector_store_key": "1706.07179-0",
          "chunk_id": 14,
          "document_id": "1706.07179",
          "start_idx": 7436,
          "end_idx": 7670
        },
        "page_content": "Future work will investigate the performance of these models on more real world datasets, interpreting what the models learn, and scaling these models to answer questions about entities and relations from reading massive text corpora.",
        "type": "Document"
      },
      {
        "id": "245d9a70-772c-4b81-b29d-8987a0ef16f8",
        "metadata": {
          "vector_store_key": "1707.03904-0",
          "chunk_id": 37,
          "document_id": "1707.03904",
          "start_idx": 20485,
          "end_idx": 21217
        },
        "page_content": "The GA model achieves $48.3\\%$ accuracy on the set of instances for which the answer is in context, however, a search accuracy of only $65\\%$ means its overall performance is lower. This can improve with improved retrieval. For Quasar-T, both the neural models significantly outperform the heuristic models, with BiDAF getting the highest F1 score of $28.5\\%$ . The best performing baselines, however, lag behind human performance by $16.4\\%$ and $32.1\\%$ for Quasar-S and Quasar-T respectively, indicating the strong potential for improvement. Interestingly, for human performance we observe that non-experts are able to match or beat the performance of experts when given access to the background corpus for searching the answers.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance difference between proposed method and state-of-the-arts on these datasets?",
    "llm_answer": "The proposed model outperformed other state-of-the-art methods overall, and in two categories (Person and Miscellaneous).  TRADE, a state-of-the-art model, performed poorly on the dataset.\n",
    "context": [
      {
        "id": "52a5c7b9-197c-4c52-9a22-93a7f3f1bb7a",
        "metadata": {
          "vector_store_key": "1910.12618-2",
          "chunk_id": 45,
          "document_id": "1910.12618",
          "start_idx": 24114,
          "end_idx": 24750
        },
        "page_content": "In order to tone it down and to increase the consistency of our results, the different models are run $B=10$ times. The results presented hereafter correspond to the average and standard-deviation on those runs. The RF model denoted as \"sel\" is the one with the reduced number of features, whereas the other RF uses the full vocabulary. We also considered an aggregated forecaster (abridged Agg), consisting of the average of the two best individual ones in terms of RMSE. All the neural network methods have a reduced vocabulary size $V^*$. The results for the French and UK data are respectively given by tables TABREF26 and TABREF27.",
        "type": "Document"
      },
      {
        "id": "a911fe4f-be82-423f-b7ac-93bbcd15b2f3",
        "metadata": {
          "vector_store_key": "1910.12618-2",
          "chunk_id": 49,
          "document_id": "1910.12618",
          "start_idx": 26517,
          "end_idx": 27217
        },
        "page_content": "This inversion of performance of the algorithms is possibly due to a change in the way the reports were written by the Met Office after August 2017, since the results of the MLP and RNN on the validation set (not shown here) were satisfactory and better than both RFs. For the two languages both the CNN and the LASSO yielded poor results. For the former, it is because despite grid search no satisfactory architecture was found, whereas the latter is a linear approach and was used more for interpretation purposes than strong performance. Finally the naive aggregation of the two best experts always yields improvement, especially for the French case where the two different encodings are combined.",
        "type": "Document"
      },
      {
        "id": "92dd0ad6-8e89-4d29-9317-3cf4eca34bfd",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 37,
          "document_id": "1909.13375",
          "start_idx": 19698,
          "end_idx": 20343
        },
        "page_content": "We preferred to focus our efforts on the ablation studies, in hope to gain further insights on the effect of the components that we ourselves introduced. For ease of performance comparison, we followed NABERT+'s training settings: we used the BERT Adam optimizer from huggingface with default settings and a learning rate of $1e^{-5}$. The only difference was that we used a batch size of 12. We trained our base model for 20 epochs. For the large models we used a batch size of 3 with a learning rate of $5e^{-6}$ and trained for 5 epochs, except for the model without the single-span heads that was trained with a batch size of 2 for 7 epochs.",
        "type": "Document"
      },
      {
        "id": "11bbd522-28ef-4580-9084-84720bfb9758",
        "metadata": {
          "vector_store_key": "2001.06888-0",
          "chunk_id": 42,
          "document_id": "2001.06888",
          "start_idx": 22556,
          "end_idx": 22770
        },
        "page_content": "Evaluation results of our proposed model compared to other state of the art methods show its superiority to these methods in overall while in two categories (Person and Miscellaneous) our model outperformed others.",
        "type": "Document"
      },
      {
        "id": "23292eed-ceb7-4e1d-8484-45160c25f296",
        "metadata": {
          "vector_store_key": "2002.11893-1",
          "chunk_id": 47,
          "document_id": "2002.11893",
          "start_idx": 27189,
          "end_idx": 27978
        },
        "page_content": "We used the first query for these models and left state tracking with recommendation for future work. Result Analysis: We evaluated the joint state accuracy (percentage of exact matching) of these two models (Table TABREF31). TRADE, the state-of-the-art model on MultiWOZ, performs poorly on our dataset, indicating that more powerful state trackers are necessary. At the test stage, RuleDST can access the previous gold system state and user dialogue acts, which leads to higher joint state accuracy than TRADE. Both models perform worse on cross multi-domain dialogues (CM and CM+T). To evaluate the ability of modeling cross-domain transition, we further calculated joint state accuracy for those turns that receive \"Select\" intent from users (e.g., \"Find a hotel near the attraction\").",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What benchmarks are created?",
    "llm_answer": "SQuAD, UMBC corpus for Twitter NER, Twitter NER corpus (BIBREF21), Making Sense of Microposts (MSM) initiative, OntoNotes, ACE, MUC 7, Ritter NER corpus, MSM 2013 corpus, and UMBC Twitter corpus.\n",
    "context": [
      {
        "id": "895b9934-11e7-438a-aedf-2435c41cdf73",
        "metadata": {
          "vector_store_key": "1905.08949-8",
          "chunk_id": 17,
          "document_id": "1905.08949",
          "start_idx": 9885,
          "end_idx": 10631
        },
        "page_content": "Table 1 presents a listing of the NQG corpora grouped by their cognitive level and answer type, along with their statistics. Among them, SQuAD was used by most groups as the benchmark to evaluate their NQG models. This provides a fair comparison between different techniques. However, it raises the issue that most NQG models work on factoid questions with answer as text span, leaving other types of QG problems less investigated, such as generating deep multi-choice questions. To overcome this, a wider variety of corpora should be benchmarked against in future NQG research. Although the datasets are commonly shared between QG and QA, it is not the case for evaluation: it is challenging to define a gold standard of proper questions to ask.",
        "type": "Document"
      },
      {
        "id": "45469f61-1e7c-4f8e-8bfe-a44402cac3bb",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 38,
          "document_id": "1910.12618",
          "start_idx": 20318,
          "end_idx": 20978
        },
        "page_content": "The hyperparameters are tuned per method and per country: ergo the hyperparameters of a given algorithm will be the same for the different time series of a country (e.g. the RNN architecture for temperature and load for France will be the same, but different from the UK one). Finally before application on the testing set, all the methods are re-trained from scratch using both the training and validation data. The goal of our experiments is to quantify how close one can get using textual data only when compared to numerical data. However the inputs of the numerical benchmark should be hence comparable to the information contained in the weather reports.",
        "type": "Document"
      },
      {
        "id": "7541e4a5-40b5-4746-827a-020fdc016b15",
        "metadata": {
          "vector_store_key": "1701.02877-1",
          "chunk_id": 15,
          "document_id": "1701.02877",
          "start_idx": 7875,
          "end_idx": 8631
        },
        "page_content": "So far, there have been no big evaluation efforts, such as ACE and OntoNotes, resulting in substantial amounts of gold standard data. Instead, benchmark corpora were created as part of smaller challenges or individual projects. The first such corpus is the UMBC corpus for Twitter NER BIBREF33 , where researchers used crowdsourcing to obtain annotations for persons, locations and organisations. A further Twitter NER corpus was created by BIBREF21 , which, in contrast to other corpora, contains more fine-grained classes defined by the Freebase schema BIBREF41 . Next, the Making Sense of Microposts initiative BIBREF32 (MSM) provides single annotated data for named entity recognition on Twitter for persons, locations, organisations and miscellaneous.",
        "type": "Document"
      },
      {
        "id": "3be44322-a327-4b65-b36a-be0068af802a",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 36,
          "document_id": "1910.12618",
          "start_idx": 19551,
          "end_idx": 20124
        },
        "page_content": "The years 2014 and 2015 serve as validation set (730 days). It is used to tune the hyperparameters of the different approaches. All the data from January the 1st 2016 (974 days for France and 1,096 for the UK) is used as test set, on which the final results are presented. Grid search is applied to find the best combination of values: for each hyperparameter, a range of values is defined, and all the possible combinations are successively tested. The one yielding the lowest RMSE (see section SECREF4) on the validation set is used for the final results on the test one.",
        "type": "Document"
      },
      {
        "id": "fafef8a0-39be-4007-b0e6-a36081a57786",
        "metadata": {
          "vector_store_key": "1701.02877-1",
          "chunk_id": 8,
          "document_id": "1701.02877",
          "start_idx": 3965,
          "end_idx": 4697
        },
        "page_content": "As detailed in Section \"Datasets\" , the corpora studied are OntoNotes BIBREF29 , ACE BIBREF30 , MUC 7 BIBREF31 , the Ritter NER corpus BIBREF21 , the MSM 2013 corpus BIBREF32 , and the UMBC Twitter corpus BIBREF33 . To eliminate potential bias from the choice of statistical NER approach, experiments are carried out with three differently-principled NER approaches, namely Stanford NER BIBREF34 , SENNA BIBREF35 and CRFSuite BIBREF36 (see Section \"NER Models and Features\" for details). Since the goal of this study is to compare NER performance on corpora from diverse domains and genres, seven benchmark NER corpora are included, spanning newswire, broadcast conversation, Web content, and social media (see Table 1 for details).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What percentage fewer errors did professional translations make?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "3699de29-b56f-4a68-a4e9-78e9f0e57a0d",
        "metadata": {
          "vector_store_key": "2004.01694-1",
          "chunk_id": 60,
          "document_id": "2004.01694",
          "start_idx": 34112,
          "end_idx": 34975
        },
        "page_content": "As for the choice of raters, professional translators showed a significant preference for human translation, while non-expert raters did not. In terms of linguistic context, raters found human translation significantly more accurate than machine translation when evaluating full documents, but not when evaluating single sentences out of context. They also found human translation significantly more fluent than machine translation, both when evaluating full documents and single sentences. Moreover, we showed that aggressive editing of human reference translations for target language fluency can decrease adequacy to the point that they become indistinguishable from machine translation, and that raters found human translations significantly better than machine translations of original source texts, but not of source texts that were translations themselves.",
        "type": "Document"
      },
      {
        "id": "6c59aa70-03ad-4887-9def-10b79328542d",
        "metadata": {
          "vector_store_key": "2004.01694-1",
          "chunk_id": 37,
          "document_id": "2004.01694",
          "start_idx": 21445,
          "end_idx": 22255
        },
        "page_content": "From this, we can see that the human translation H$_B$, which was aggressively edited to ensure target fluency, resulted in lower adequacy (Table TABREF30). With more fluent and less accurate translations, raters do not prefer human over machine translation in terms of adequacy (Table TABREF30), but have a stronger preference for human translation in terms of fluency (compare Tables TABREF30 and TABREF21). In a direct comparison of the two human translations (Table TABREF30), we also find that H$_A$ is considered significantly more adequate than H$_B$, while there is no significant difference in fluency. To achieve a finer-grained understanding of what errors the evaluated translations exhibit, we perform a categorisation of 150 randomly sampled sentences based on the classification used by BIBREF3.",
        "type": "Document"
      },
      {
        "id": "daad7eee-8b96-4f14-b87a-f87cd0b2960b",
        "metadata": {
          "vector_store_key": "2004.01694-1",
          "chunk_id": 59,
          "document_id": "2004.01694",
          "start_idx": 33683,
          "end_idx": 34437
        },
        "page_content": "Our blind qualitative analysis, however, showed that the machine translation output contained significantly more incorrect words, omissions, mistranslated names, and word order errors. Our experiments show that recent findings of human\u2013machine parity in language translation are owed to weaknesses in the design of human evaluation campaigns. We empirically tested alternatives to what is currently considered best practice in the field, and found that the choice of raters, the availability of linguistic context, and the creation of reference translations have a strong impact on perceived translation quality. As for the choice of raters, professional translators showed a significant preference for human translation, while non-expert raters did not.",
        "type": "Document"
      },
      {
        "id": "0b87a1f3-0d1c-4e6d-a0cd-649cdb03d7c7",
        "metadata": {
          "vector_store_key": "2004.01694-2",
          "chunk_id": 39,
          "document_id": "2004.01694",
          "start_idx": 22663,
          "end_idx": 23508
        },
        "page_content": "Our analysis was performed by one of the co-authors who is a bi-lingual native Chinese/English speaker. Sentences were shown in the context of the document, to make it easier to determine whether the translations were correct based on the context. The analysis was performed on one machine translation (MT$_1$) and two human translation outputs (H$_A$, H$_B$), using the same 150 sentences, but blinding their origin by randomising the order in which the documents were presented. We show the results of this analysis in Table TABREF32. From these results, we can glean a few interesting insights. First, we find significantly larger numbers of errors of the categories of Incorrect Word and Named Entity in MT$_1$, indicating that the MT system is less effective at choosing correct translations for individual words than the human translators.",
        "type": "Document"
      },
      {
        "id": "6b15acb6-b791-4062-8e79-51e3c3676c45",
        "metadata": {
          "vector_store_key": "2004.01694-1",
          "chunk_id": 54,
          "document_id": "2004.01694",
          "start_idx": 30451,
          "end_idx": 31194
        },
        "page_content": "Raters who judge target language fluency without access to the source texts show a stronger preference for human translation than raters with access to the source texts (Sections SECREF4 and SECREF24). In all of our experiments, raters prefer human translation in terms of fluency while, just as in BIBREF3's BIBREF3 evaluation, they find no significant difference between human and machine translation in sentence-level adequacy (Tables TABREF21 and TABREF30). Our error analysis in Table TABREF34 also indicates that MT still lags behind human translation in fluency, specifically in grammaticality. In professional translation workflows, texts are typically revised with a focus on target language fluency after an initial translation step.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was the weakness in Hassan et al's evaluation design?",
    "llm_answer": "Non-experts disregarded translation nuances, leading to a more tolerant judgment of MT systems and lower inter-annotator agreement.  Human rater performance may vary over time due to learning or fatigue, and the unit of evaluation was a concern.\n",
    "context": [
      {
        "id": "edd8a897-e57d-44b4-bba9-4bf6cbf30547",
        "metadata": {
          "vector_store_key": "2004.01694-3",
          "chunk_id": 21,
          "document_id": "2004.01694",
          "start_idx": 12650,
          "end_idx": 13333
        },
        "page_content": "This indicates that non-experts disregard translation nuances in the evaluation, which leads to a more tolerant judgement of MT systems and a lower inter-annotator agreement ($\\kappa =0.13$ for non-experts versus $\\kappa =0.254$ for experts). It is worth noticing that, regardless of their expertise, the performance of human raters may vary over time. For example, performance may improve or decrease due to learning effects or fatigue, respectively BIBREF23. It is likely that such longitudinal effects are present in our data. They should be accounted for in future work, e. g., by using trial number as an additional predictor BIBREF24. Another concern is the unit of evaluation.",
        "type": "Document"
      },
      {
        "id": "4be44773-b407-4314-abd0-d9c32e99174b",
        "metadata": {
          "vector_store_key": "1908.07816-1",
          "chunk_id": 7,
          "document_id": "1908.07816",
          "start_idx": 4025,
          "end_idx": 4656
        },
        "page_content": "We consider factors such as the balance of positive and negative sentiments in test dialogs, a well-chosen range of topics, and dialogs that our human evaluators can relate. It is the first time such an approach is designed with consideration for the human judges. Our main goal is to increase the objectivity of the results and reduce judges' mistakes due to out-of-context dialogs they have to evaluate. The rest of the paper unfolds as follows. Section SECREF2 discusses some related work. In Section SECREF3, we give detailed description of the methodology. We present experimental results and some analysis in Section SECREF4.",
        "type": "Document"
      },
      {
        "id": "7e5fdc0f-cb40-4269-bb3f-304094c2c5c8",
        "metadata": {
          "vector_store_key": "1910.10408-4",
          "chunk_id": 40,
          "document_id": "1910.10408",
          "start_idx": 21178,
          "end_idx": 21879
        },
        "page_content": "After manually inspecting the outputs of the best performing models under the large data condition, we decided to run a human evaluation only for the En-It Len-Tok model. As our ultimate goal is to be able to generate shorter translations and as close as possible to the length of the source sentences, we focused the manual evaluation on the Short output class and aimed to verify possible losses in quality with respect to the baseline system. We ran a head-to-head evaluation on the first 10 sentences of each test talk, for a total of 270 sentences, by asking annotators to blindly rank the two system outputs (ties were also permitted) in terms of quality with respect to a reference translation.",
        "type": "Document"
      },
      {
        "id": "76713430-261b-4bb1-aa63-76ff4a6088ca",
        "metadata": {
          "vector_store_key": "1605.08675-4",
          "chunk_id": 96,
          "document_id": "1605.08675",
          "start_idx": 53142,
          "end_idx": 53894
        },
        "page_content": "It could be explained by different compositions of the question sets (table TABREF85 ) \u2013 the development one contains much more questions beginning with ambiguous pronouns, followed by a question focus, e.g. Kt\u00f3ry poeta... (which poet), thus providing a precise synset (a poet) for deep entity recognition. Members of the evaluation set much more frequently begin with pronouns like Kto ...(who), where a synset corresponds to a general NE type (a person). As RAFAEL is the first Polish QA system, able to answer by entities instead of documents, we can not compare it directly to any other solution. However, the evaluation set has been created based on questions published by BIBREF56 and used for evaluation of a document retrieval system BIBREF18 .",
        "type": "Document"
      },
      {
        "id": "316d151a-06ef-4dc6-bf85-da0eb956ffd7",
        "metadata": {
          "vector_store_key": "1904.08386-4",
          "chunk_id": 17,
          "document_id": "1904.08386",
          "start_idx": 10140,
          "end_idx": 10411
        },
        "page_content": "Additionally, we thank Nader Akoury, Garrett Bernstein, Chenghao Lv, Ari Kobren, Kalpesh Krishna, Saumya Lal, Tu Vu, Zhichao Yang, Mengxue Zhang and the UMass NLP group for suggestions that improved the paper's clarity, coverage of related work, and analysis experiments.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What evidence do they present that the model attends to shallow context clues?",
    "llm_answer": "Figure FIGREF37 shows that the model's most important inputs are verbs possibly associated with the entity butter, in addition to the entity's mentions themselves.  It further shows that the model learns to extract shallow clues of identifying actions exerted upon only the entity being tracked.\n",
    "context": [
      {
        "id": "68d14e79-5da7-476e-9e15-7f11237c4355",
        "metadata": {
          "vector_store_key": "2002.00652-1",
          "chunk_id": 40,
          "document_id": "2002.00652",
          "start_idx": 24451,
          "end_idx": 24829
        },
        "page_content": "Furthermore, by performing a fine-grained analysis, we summarize two potential directions as our future work: incorporating common sense for better pronouns inference, and modeling contextual clues in a more explicit manner. By open-sourcing our code and materials, we believe our work can facilitate the community to debug models in a fine-grained level and make more progress.",
        "type": "Document"
      },
      {
        "id": "616b928e-485e-4f1a-be43-7cc22f9cc48c",
        "metadata": {
          "vector_store_key": "2002.00652-1",
          "chunk_id": 39,
          "document_id": "2002.00652",
          "start_idx": 23980,
          "end_idx": 24850
        },
        "page_content": "Under the same task, BIBREF1 summarized contextual phenomena in a coarse-grained level, while BIBREF0 performed a wizard-of-oz experiment to study the most frequent phenomena. What makes our work different from them is that we not only summarize contextual phenomena by fine-grained types, but also perform an analysis on context modeling methods. This work conducts an exploratory study on semantic parsing in context, to realize how far we are from effective context modeling. Through a thorough comparison, we find that existing context modeling methods are not as effective as expected. A simple concatenation method can be much competitive. Furthermore, by performing a fine-grained analysis, we summarize two potential directions as our future work: incorporating common sense for better pronouns inference, and modeling contextual clues in a more explicit manner.",
        "type": "Document"
      },
      {
        "id": "f9166c68-259b-4897-a77d-ca61d0108502",
        "metadata": {
          "vector_store_key": "2004.02393-1",
          "chunk_id": 2,
          "document_id": "2004.02393",
          "start_idx": 1212,
          "end_idx": 1871
        },
        "page_content": "As a result, the supervision from these datasets can still be insufficient for training accurate models. Taking question answering with multi-hop reasoning as an example, annotating only supporting passages is not sufficient to show the reasoning processes due to the lack of necessary structural information (Figure FIGREF1). One example is the order of annotated evidence, which is crucial in logic reasoning and the importance of which has also been demonstrated in text-based QA BIBREF8. The other example is how the annotated evidence pieces are connected, which requires at least the definition of arguments, such as a linking entity, concept, or event.",
        "type": "Document"
      },
      {
        "id": "4b14f5c7-7ded-475f-8dec-89fec2df5c16",
        "metadata": {
          "vector_store_key": "1910.07181-9",
          "chunk_id": 23,
          "document_id": "1910.07181",
          "start_idx": 12763,
          "end_idx": 13504
        },
        "page_content": "As in many cases, not just one, but a handful of contexts is known for a rare word, we follow the approach of BIBREF19 to deal with multiple contexts: We add an Attentive Mimicking head on top of our model, as can be seen in Figure FIGREF14 (right). That is, given a set of contexts $\\mathcal {C} = \\lbrace C_1, \\ldots , C_m\\rbrace $ and the corresponding embeddings $v_{(w,C_1)}, \\ldots , v_{(w,C_m)}$, we apply a self-attention mechanism to all embeddings, allowing the model to distinguish informative contexts from uninformative ones. The final embedding $v_{(w, \\mathcal {C})}$ is then a linear combination of the embeddings obtained from each context, where the weight of each embedding is determined based on the self-attention layer.",
        "type": "Document"
      },
      {
        "id": "56dc5a3f-8db3-44db-83c2-8fd2a9c4a6ad",
        "metadata": {
          "vector_store_key": "1909.02635-5",
          "chunk_id": 44,
          "document_id": "1909.02635",
          "start_idx": 25189,
          "end_idx": 25838
        },
        "page_content": "To understand what the model actually does, we perform analysis of the model's behavior with respect to the input to understand what cues it is picking up on. One way to analyze the model is to compute model gradients with respect to input features BIBREF26, BIBREF25. Figure FIGREF37 shows that in this particular example, the most important model inputs are verbs possibly associated with the entity butter, in addition to the entity's mentions themselves. It further shows that the model learns to extract shallow clues of identifying actions exerted upon only the entity being tracked, regardless of other entities, by leveraging verb semantics.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "In what way is the input restructured?",
    "llm_answer": "The input sentence is recursively transformed into structurally simplified sentences and a semantic hierarchy is established between them using a set of hand-crafted grammar rules.\n",
    "context": [
      {
        "id": "c630b6be-cc52-4297-bad8-134c4951c3a8",
        "metadata": {
          "vector_store_key": "1909.12140-3",
          "chunk_id": 3,
          "document_id": "1909.12140",
          "start_idx": 2151,
          "end_idx": 2767
        },
        "page_content": "It takes a sentence as input and performs a recursive transformation process that is based upon a small set of 35 hand-crafted grammar rules for the English version and 29 rules for the German approach. These patterns were heuristically determined in a comprehensive linguistic analysis and encode syntactic and lexical features that can be derived from a sentence's parse tree. Each rule specifies (1) how to split up and rephrase the input into structurally simplified sentences and (2) how to set up a semantic hierarchy between them. They are recursively applied on a given source sentence in a top-down fashion.",
        "type": "Document"
      },
      {
        "id": "2559b0f2-f74f-428e-8742-ee0eb0a62ca3",
        "metadata": {
          "vector_store_key": "2001.00137-2",
          "chunk_id": 11,
          "document_id": "2001.00137",
          "start_idx": 7001,
          "end_idx": 7717
        },
        "page_content": "It also prefixes the sequence of tokens with a special character `[CLS]' and sufixes each sentence with a `[SEP]' character. It is followed by an embedding layer used for input representation, with the final input embedding being a sum of token embedddings, segmentation embeddings and position embeddings. The first one, token embedding layer, uses a vocabulary dictionary to convert each token into a more representative embedding. The segmentation embedding layer indicates which tokens constitute a sentence by signaling either 1 or 0. In our case, since our data are formed of single sentences, the segment is 1 until the first `[SEP]' character appears (indicating segment A) and then it becomes 0 (segment B).",
        "type": "Document"
      },
      {
        "id": "86156354-f981-49fa-94ec-cb742e2ac4e8",
        "metadata": {
          "vector_store_key": "1910.11471-2",
          "chunk_id": 15,
          "document_id": "1910.11471",
          "start_idx": 8505,
          "end_idx": 9124
        },
        "page_content": "In Fig. FIGREF13, the neural model architecture is demonstrated. The diagram shows how it takes the source and target text as input and uses it for training. Vector representation of tokenized source and target text are fed into the model. Each token of the source text is passed into an encoder cell. Target text tokens are passed into a decoder cell. Encoder cells are part of the encoder RNN layer and decoder cells are part of the decoder RNN layer. End of the input sequence is marked by a $<$eos$>$ token. Upon getting the $<$eos$>$ token, the final cell state of encoder layer initiate the output layer sequence.",
        "type": "Document"
      },
      {
        "id": "6bbd7a5c-e154-44ef-aa8d-26d57ff0fc1a",
        "metadata": {
          "vector_store_key": "1706.07179-3",
          "chunk_id": 5,
          "document_id": "1706.07179",
          "start_idx": 2979,
          "end_idx": 3608
        },
        "page_content": "The model is sequential in nature, consisting of the following steps: read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer. We model the dynamic memory in a fashion similar to Recurrent Entity Networks BIBREF17 and then equip it with an additional relational memory. There are three main components to the model: 1) input encoder 2) dynamic memory, and 3) output module. We will describe these three modules in details. The input encoder and output module implementations are similar to the Entity Network BIBREF17 and main novelty lies in the dynamic memory.",
        "type": "Document"
      },
      {
        "id": "3603fcf3-dc08-4944-b443-356dfbd8546e",
        "metadata": {
          "vector_store_key": "2001.00137-2",
          "chunk_id": 15,
          "document_id": "2001.00137",
          "start_idx": 9449,
          "end_idx": 10523
        },
        "page_content": "The first set is responsible for compressing the $h_{inc}$ into a latent-space representation, extracting more abstract features into lower dimension vectors $z_1$, $z_2$ and $\\mathbf {z}$ with shape $(N_{bs}, 128, 128)$, $(N_{bs}, 32, 128)$, and $(N_{bs}, 12, 128)$, respectively. This process is shown in Eq. (DISPLAY_FORM5): where $f(\\cdot )$ is the parameterized function mapping $h_{inc}$ to the hidden state $\\mathbf {z}$. The second set then respectively reconstructs $z_1$, $z_2$ and $\\mathbf {z}$ into $h_{rec_1}$, $h_{rec_2}$ and $h_{rec}$. This process is shown in Eq. (DISPLAY_FORM6): where $g(\\cdot )$ is the parameterized function that reconstructs $\\mathbf {z}$ as $h_{rec}$. The reconstructed hidden sentence embedding $h_{rec}$ is compared with the complete hidden sentence embedding $h_{comp}$ through a mean square error loss function, as shown in Eq. (DISPLAY_FORM7): After reconstructing the correct hidden embeddings from the incomplete sentences, the correct hidden embeddings are given to bidirectional transformers to generate input representations.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What language is the Twitter content in?",
    "llm_answer": "English, Russian\n",
    "context": [
      {
        "id": "b2f91af3-6dd9-4ef4-b16b-4685b735cfa2",
        "metadata": {
          "vector_store_key": "1604.05781-2",
          "chunk_id": 8,
          "document_id": "1604.05781",
          "start_idx": 4059,
          "end_idx": 4740
        },
        "page_content": "To avoid cross-language effects, we kept only tweets with a user-reported language of `English' and, as a second constraint, individual tweets needed to match more English stopwords than any other language's set of stopwords. Stopwords considered for each language were determined using NLTK's database BIBREF29 . A tweet will be referred to as a `document' for the rest of this work. All document text was processed the same way. Punctuation, XML characters, and hyperlinks were removed, as were Twitter-specific \u201cat-mentions\u201d and \u201chashtags\u201d (see also the Appendix). There is useful information here, but it is either not natural language text, or it is Twitter-specific, or both.",
        "type": "Document"
      },
      {
        "id": "2a976745-5b4c-462f-bcc8-cc5993986f7c",
        "metadata": {
          "vector_store_key": "1912.13109-1",
          "chunk_id": 11,
          "document_id": "1912.13109",
          "start_idx": 5441,
          "end_idx": 6231
        },
        "page_content": "Stop words: Stop words corpus obtained from NLTK was used to eliminate most unproductive words which provide little information about individual tweets. Transliteration: Followed by above two processes, we translated Hinglish tweets into English words using a two phase process Transliteration: In phase I, we used translation API's provided by Google translation services and exposed via a SDK, to transliteration the Hinglish messages to English messages. Translation: After transliteration, words that were specific to Hinglish were translated to English using an Hinglish-English dictionary. By doing this we converted the Hinglish message to and assortment of isolated words being presented in the message in a sequence that can also be represented using word to vector representation.",
        "type": "Document"
      },
      {
        "id": "5c4b1458-497e-4f2e-a4e7-c5e739d9cad0",
        "metadata": {
          "vector_store_key": "1912.13109-1",
          "chunk_id": 9,
          "document_id": "1912.13109",
          "start_idx": 4561,
          "end_idx": 5222
        },
        "page_content": "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al. This dataset was important to employ Transfer Learning to our task since the number of labeled dataset was very small. Basic summary and examples of the data from the dataset are below: The obtained data set had many challenges and thus a data preparation task was employed to clean the data and make it ready for the deep learning pipeline.",
        "type": "Document"
      },
      {
        "id": "59dbbb96-7ec6-444b-b42f-cdfacda0f1a3",
        "metadata": {
          "vector_store_key": "1909.02764-2",
          "chunk_id": 32,
          "document_id": "1909.02764",
          "start_idx": 17674,
          "end_idx": 18385
        },
        "page_content": "We train models on a variety of corpora, namely the common format published by BIBREF27 of the FigureEight (formally known as Crowdflower) data set of social media, the ISEAR data BIBREF40 (self-reported emotional events), and, the Twitter Emotion Corpus (TEC, weakly annotated Tweets with #anger, #disgust, #fear, #happy, #sadness, and #surprise, Mohammad2012). From all corpora, we use instances with labels fear, anger, or joy. These corpora are English, however, we do predictions on German utterances. Therefore, each corpus is preprocessed to German with Google Translate. We remove URLs, user tags (\u201c@Username\u201d), punctuation and hash signs. The distributions of the data sets are shown in Table TABREF12.",
        "type": "Document"
      },
      {
        "id": "1ccebfd0-e53f-4893-9fc0-da94bb8662b4",
        "metadata": {
          "vector_store_key": "1602.08741-4",
          "chunk_id": 4,
          "document_id": "1602.08741",
          "start_idx": 2178,
          "end_idx": 2986
        },
        "page_content": "In this paper we concentrate on usage of Russian Twitter stream as training corpus for Word2Vec model in semantic similarity task, and show results comparable with current (trained on a single corpus). This research is part of molva.spb.ru project, which is a trending topic detection engine for Russian Twitter. Thus the choice of language of interest is narrowed down to only Russian, although there is strong intuition that one can achieve similar results with other languages. The primary goal of this paper is to prove usefulness of Russian Twitter stream as word semantic similarity resource. Twitter is a popular social network, or also called \"microblogging service\", which enables users to share and interact with short messages instantly and publicly (although private accounts are also available).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What evaluations did the authors use on their system?",
    "llm_answer": "Human evaluation of translations, head-to-head ranking of system outputs, and human evaluation of Chinese human-computer dialogue.\n",
    "context": [
      {
        "id": "7e5fdc0f-cb40-4269-bb3f-304094c2c5c8",
        "metadata": {
          "vector_store_key": "1910.10408-4",
          "chunk_id": 40,
          "document_id": "1910.10408",
          "start_idx": 21178,
          "end_idx": 21879
        },
        "page_content": "After manually inspecting the outputs of the best performing models under the large data condition, we decided to run a human evaluation only for the En-It Len-Tok model. As our ultimate goal is to be able to generate shorter translations and as close as possible to the length of the source sentences, we focused the manual evaluation on the Short output class and aimed to verify possible losses in quality with respect to the baseline system. We ran a head-to-head evaluation on the first 10 sentences of each test talk, for a total of 270 sentences, by asking annotators to blindly rank the two system outputs (ties were also permitted) in terms of quality with respect to a reference translation.",
        "type": "Document"
      },
      {
        "id": "e2de1ca0-a155-45ff-aab2-4caac13662f7",
        "metadata": {
          "vector_store_key": "1709.10217-0",
          "chunk_id": 16,
          "document_id": "1709.10217",
          "start_idx": 8888,
          "end_idx": 9658
        },
        "page_content": "Note that for task 2, there are 7 submitted systems. However, only 4 systems can provide correct results or be connected in a right way at the test phase. Therefore, Table TABREF16 shows the complete results of the task 2. In this paper, we introduce the first evaluation of Chinese human-computer dialogue technology. In detail, we first present the two tasks of the evaluation as well as the evaluation metrics. We then describe the released data for evaluation. Finally, we also show the evaluation results of the two tasks. As the evaluation data is provided by the iFLYTEK Corporation from their real online applications, we believe that the released data will further promote the research of human-computer dialogue and fill the blank of the data on the two tasks.",
        "type": "Document"
      },
      {
        "id": "74a8f350-1cdc-4b68-87aa-e719da16ac2c",
        "metadata": {
          "vector_store_key": "1709.10217-3",
          "chunk_id": 4,
          "document_id": "1709.10217",
          "start_idx": 2049,
          "end_idx": 2735
        },
        "page_content": "For the task-oriented system, although there are some objective evaluation metrics, such as the number of turns in a dialogue, the ratio of task completion, etc., there is no gold standard for automatically evaluating two (or more) dialogue systems when considering the satisfaction of the human and the fluency of the generated dialogue. To promote the development of the evaluation technology for dialogue systems, especially considering the language characteristics of Chinese, we organize the first evaluation of Chinese human-computer dialogue technology. In this paper, we will present the evaluation scheme and the released corpus in detail. The rest of this paper is as follows.",
        "type": "Document"
      },
      {
        "id": "b8ca75dd-84d9-4514-9a0b-ed8771d04dfc",
        "metadata": {
          "vector_store_key": "1605.08675-1",
          "chunk_id": 90,
          "document_id": "1605.08675",
          "start_idx": 49692,
          "end_idx": 50451
        },
        "page_content": "However, because of the small difference between the techniques including title, for the sake of simplicity, the single sentence is used in the final evaluation. To impose a realistic challenge to the system, the evaluation set, used at this stage, substantially differs from the one used during the development (see section SECREF80 ). A configuration for the final evaluation has been prepared based on results of the experiments. All of the tested versions share the following features: no question analysis corrections, question classification and query generation solutions which proved best in the previous experiments (see section SECREF18 ), a retrieved set of documents including 20 articles, no minimal confidence, singe sentence context with title.",
        "type": "Document"
      },
      {
        "id": "4be44773-b407-4314-abd0-d9c32e99174b",
        "metadata": {
          "vector_store_key": "1908.07816-1",
          "chunk_id": 7,
          "document_id": "1908.07816",
          "start_idx": 4025,
          "end_idx": 4656
        },
        "page_content": "We consider factors such as the balance of positive and negative sentiments in test dialogs, a well-chosen range of topics, and dialogs that our human evaluators can relate. It is the first time such an approach is designed with consideration for the human judges. Our main goal is to increase the objectivity of the results and reduce judges' mistakes due to out-of-context dialogs they have to evaluate. The rest of the paper unfolds as follows. Section SECREF2 discusses some related work. In Section SECREF3, we give detailed description of the methodology. We present experimental results and some analysis in Section SECREF4.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What accuracy does CNN model achieve?",
    "llm_answer": "The context states that CNN accuracy drops from around 0.8 to 0.5 when noise rate increases from 0 to 0.5, but NetAb outperforms CNN.  No specific CNN accuracy is given.\n",
    "context": [
      {
        "id": "993cd19c-93b3-4820-be72-e55dcc52919c",
        "metadata": {
          "vector_store_key": "1911.07228-1",
          "chunk_id": 16,
          "document_id": "1911.07228",
          "start_idx": 7814,
          "end_idx": 8542
        },
        "page_content": "Before we decide to analysis on this result, we have run some other methods, but this one with Vietnamese pre-trained word embeddings provided by Kyubyong Park obtains the best result. Other results are shown in the Table 2. Table 2 shows our experiments on two models with and without different pre-trained word embedding \u2013 KP means the Kyubyong Park\u2019s pre-trained word embeddings and EG means Edouard Grave\u2019s pre-trained word embeddings. We compare the outputs of BLSTM-CNN-CRF model (predicted) to the annotated data (gold) and analyzed the errors. Table 3 shows perfomance of the BLSTM-CNN-CRF model. In our experiments, we use three evaluation parameters (precision, recall, and F1 score) to access our experimental result.",
        "type": "Document"
      },
      {
        "id": "456b24a0-2d89-4a89-88bc-f3adfe9eed46",
        "metadata": {
          "vector_store_key": "1603.04553-1",
          "chunk_id": 23,
          "document_id": "1603.04553",
          "start_idx": 12556,
          "end_idx": 13350
        },
        "page_content": "Experimental results on the data from CoNLL-2012 shared task show that our system significantly improves the accuracy on different evaluation metrics over the baseline systems. One possible direction for future work is to differentiate more resolution modes. Another one is to add more precise or even event-based features to improve the model's performance. This research was supported in part by DARPA grant FA8750-12-2-0342 funded under the DEFT program. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA. Appendix A. Derivation of Model Learning Formally, we iteratively estimate the model parameters $\\theta $ , employing the following EM algorithm: For simplicity, we denote: $",
        "type": "Document"
      },
      {
        "id": "e6f9963b-692b-48f2-85f6-77a0547a3648",
        "metadata": {
          "vector_store_key": "1909.00124-4",
          "chunk_id": 23,
          "document_id": "1909.00124",
          "start_idx": 13279,
          "end_idx": 13837
        },
        "page_content": "We corrupt the clean training data by switching the labels of some random instances based on a noise rate parameter. Then we use the corrupted data to train NetAb and CNN BIBREF25. The test accuracy curves with the noise rates [0, $0.1$, $0.2$, $0.3$, $0.4$, $0.5$] are shown in Figure FIGREF13. From the figure, we can see that the test accuracy drops from around 0.8 to 0.5 when the noise rate increases from 0 to 0.5, but our NetAb outperforms CNN. The results clearly show that the performance of the CNN drops quite a lot with the noise rate increasing.",
        "type": "Document"
      },
      {
        "id": "1367ebec-9d29-4557-a0d2-81083aa7eb9e",
        "metadata": {
          "vector_store_key": "1810.09774-1",
          "chunk_id": 28,
          "document_id": "1810.09774",
          "start_idx": 13908,
          "end_idx": 14612
        },
        "page_content": "In contrast to the findings of BIBREF1 , utilizing external knowledge did not improve the model's generalization capability, as KIM performed equally poorly across all dataset combinations. Also including a pretrained ELMo language model did not improve the results significantly. The overall performance of BERT was significantly better than the other models, having the lowest average difference in accuracy of 22.5 points. Our baselines for SNLI (90.4%) and SNLI + MultiNLI (90.6%) outperform the previous state-of-the-art accuracy for SNLI (90.1%) by BIBREF24 . To understand better the types of errors made by neural network models in NLI we looked at some example failure-pairs for selected models.",
        "type": "Document"
      },
      {
        "id": "c38775a4-9a95-4d17-ad61-ec8d23737f63",
        "metadata": {
          "vector_store_key": "1910.04269-3",
          "chunk_id": 19,
          "document_id": "1910.04269",
          "start_idx": 11225,
          "end_idx": 11896
        },
        "page_content": "Kernel Size: We varied the receptive fields of convolutional layers by choosing the kernel size from among the set of {3, 5, 7, 9}. We observe that a kernel size of 9 gives better accuracy at the cost of increased computation time and larger number of parameters. A large kernel size is able to capture longer patterns in its input due to bigger receptive power which results in an improved accuracy. Dropout: Dropout randomly turns-off (sets to 0) various individual nodes during training of the network. In a deep CNN it is important that nodes do not develop a co-dependency amongst each other during training in order to prevent overfitting on training data BIBREF25.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many documents are in the Indiscapes dataset?",
    "llm_answer": "508\n",
    "context": [
      {
        "id": "cdcd424c-45b3-4216-b36f-c24b9237a08a",
        "metadata": {
          "vector_store_key": "1912.07025-2",
          "chunk_id": 6,
          "document_id": "1912.07025",
          "start_idx": 4022,
          "end_idx": 4827
        },
        "page_content": "No publicly available large-scale, annotated dataset of historical Indic manuscripts exists to the best of our knowledge. In contrast with existing collections, our proposed dataset contains a much larger diversity in terms of document type (palm-leaf and early paper), scripts and annotated layout elements (see Tables TABREF5,TABREF8). An additional level of complexity arises from the presence of multiple manuscript pages within a single image (see Fig. FIGREF1). A number of contributions can also be found for the task of historical document layout parsing BIBREF21, BIBREF22, BIBREF23, BIBREF24. Wei et al. BIBREF22 explore the effect of using a hybrid feature selection method while using autoencoders for semantic segmentation in five historical English and Medieval European manuscript datasets.",
        "type": "Document"
      },
      {
        "id": "6941e3c3-ed76-4d5f-86b8-66718faf1417",
        "metadata": {
          "vector_store_key": "1912.07025-2",
          "chunk_id": 13,
          "document_id": "1912.07025",
          "start_idx": 8701,
          "end_idx": 9444
        },
        "page_content": "As our results show, our approach can successfully handle such multi-page documents, thereby making it truly an end-to-end system. Overall, our dataset contains 508 annotated Indic manuscripts. Some salient aspects of the dataset can be viewed in Table TABREF5 and a pictorial illustration of layout regions can be viewed in Figure FIGREF13. Note that multiple regions can overlap, unlike existing historical document datasets which typically contain disjoint region annotations. For the rest of the section, we discuss the challenges associated with annotating Indic manuscripts (Section SECREF9) and our web-based annotation tool (Section SECREF11). A variety of unique challenges exist in the context of annotating Indic manuscript layouts.",
        "type": "Document"
      },
      {
        "id": "bf4fa22f-827e-4d4b-bde3-75e9826d1e13",
        "metadata": {
          "vector_store_key": "1912.07025-2",
          "chunk_id": 40,
          "document_id": "1912.07025",
          "start_idx": 24257,
          "end_idx": 24787
        },
        "page_content": "This would enable large-scale data collection and automated analysis efforts for Indic as well as other historical Asian manuscripts. The repositories related to the systems presented in this paper and the Indiscapes dataset can be accessed at https://ihdia.iiit.ac.in. We would like to thank Dr. Sai Susarla for enabling access to the Bhoomi document collection. We also thank Poreddy Mourya Kumar Reddy, Gollapudi Sai Vamsi Krishna for their contributions related to dashboard and various annotators for their labelling efforts.",
        "type": "Document"
      },
      {
        "id": "cb848f74-8c49-4e0e-a0f9-b2b3b657a672",
        "metadata": {
          "vector_store_key": "1912.07025-2",
          "chunk_id": 11,
          "document_id": "1912.07025",
          "start_idx": 7366,
          "end_idx": 8122
        },
        "page_content": "From the $2{,}880$ Indic manuscript book-sets, we carefully curated 193 manuscript images for annotation. Our curated selection aims to maximize the diversity of the dataset in terms of various attributes such as the extent of document degradation, script language, presence of non-textual elements (e.g. pictures, tables) and number of lines. Some images contain multiple manuscript pages stacked vertically or horizontally (see bottom-left image in Figure FIGREF1). The second source for manuscript images in our dataset is Bhoomi, an assorted collection of 315 images sourced from multiple Oriental Research Institutes and libraries across India. As with the first collection, we chose a subset intended to maximize the overall diversity of the dataset.",
        "type": "Document"
      },
      {
        "id": "cf45e630-c733-430f-8ea2-fe32eb7867ea",
        "metadata": {
          "vector_store_key": "1912.07025-2",
          "chunk_id": 4,
          "document_id": "1912.07025",
          "start_idx": 2517,
          "end_idx": 3496
        },
        "page_content": "In addition, given the typical non-technical nature of domain experts who study manuscripts, it is also important to develop easy-to-use graphical interfaces for annotation, post-annotation visualization and analytics. We make the following contributions: We introduce Indiscapes, the first ever historical Indic manuscript dataset with detailed spatial layout annotations (Section SECREF3). We adapt a deep neural network architecture for instance-level spatial layout parsing of historical manuscript images (Section SECREF16). We also introduce a lightweight web-based GUI for annotation and dashboard-style analytics keeping in mind the non-technical domain experts and the unique layout-level challenges of Indic manuscripts (Section SECREF11). A number of research groups have invested significant efforts in the creation and maintenance of annotated, publicly available historical manuscript image datasets BIBREF10, BIBREF11, BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF12.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are simulated datasets collected?",
    "llm_answer": "Simulated datasets contain documents and their revisions, generated through operations like deletion, addition, and replacement of words, sentences, paragraphs, and section names.  They include corpora (Corpus 0-5) and revisions from specific time periods.  Also included are datasets mapping pictures and questions (MS COCO, Bing, Flickr), and an open-domain conversation dataset.\n",
    "context": [
      {
        "id": "6d7e1fda-02df-491f-93a1-de0c2f84c4b0",
        "metadata": {
          "vector_store_key": "1709.01256-4",
          "chunk_id": 36,
          "document_id": "1709.01256",
          "start_idx": 19769,
          "end_idx": 20455
        },
        "page_content": "The generation process of the simulated data sets is designed to mimic the real world. Users open some existing documents in a file system, make some changes (e.g. addition, deletion or replacement), and save them as separate documents. These documents become revisions of the original documents. We started from an initial corpus that did not have revisions, and kept adding new documents and revising existing documents. Similar to a file system, at any moment new documents could be added and/or some of the current documents could be revised. The revision operations we used were deletion, addition and replacement of words, sentences, paragraphs, section names and document titles.",
        "type": "Document"
      },
      {
        "id": "1e2bf238-b0a0-40af-9d8d-e6fea4ad79d0",
        "metadata": {
          "vector_store_key": "1910.09399-3",
          "chunk_id": 105,
          "document_id": "1910.09399",
          "start_idx": 56273,
          "end_idx": 57009
        },
        "page_content": "For example, image data captured from a variety of photo-ready devices, such as smart-phones, and online social media services opened the door to the analysis of large amounts of media datasets BIBREF70. The availability of large media datasets allow new frameworks and algorithms to be proposed and tested on real-world data. A summary of some reviewed methods and benchmark datasets used for validation is reported in Table TABREF43. In addition, the performance of different GANs with respect to the benchmark datasets and performance metrics is reported in Table TABREF48. In order to synthesize images from text descriptions, many frameworks have taken a minimalistic approach by creating small and background-less images BIBREF73.",
        "type": "Document"
      },
      {
        "id": "8ef64fd9-ae4b-416a-994a-f082e80e88e4",
        "metadata": {
          "vector_store_key": "1703.04617-2",
          "chunk_id": 2,
          "document_id": "1703.04617",
          "start_idx": 973,
          "end_idx": 1680
        },
        "page_content": "We then viewed and modelled different types of questions and the information shared among them as an adaptation problem and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results on our competitive baselines. Recent advance on reading comprehension and question answering has been closely associated with the availability of various datasets. BIBREF0 released the MCTest data consisting of 500 short, fictional open-domain stories and 2000 questions. The CNN/Daily Mail dataset BIBREF1 contains news articles for close style machine comprehension, in which only entities are removed and tested for comprehension.",
        "type": "Document"
      },
      {
        "id": "93a841ff-3b54-4752-90d9-794ea465a2e0",
        "metadata": {
          "vector_store_key": "1709.01256-4",
          "chunk_id": 38,
          "document_id": "1709.01256",
          "start_idx": 20976,
          "end_idx": 21630
        },
        "page_content": "We generated six data sets using different random seeds, and each data set contained six corpora (Corpus 0 - 5). Table TABREF48 summarizes the first data set. In each data set, we name the initial corpus Corpus 0, and define INLINEFORM0 as the timestamp when we started this simulation process. We set INLINEFORM1 , INLINEFORM2 . Corpus INLINEFORM3 corresponds to documents generated before timestamp INLINEFORM4 . We extracted document revisions from Corpus INLINEFORM5 and compared the revisions generated in (Corpus INLINEFORM6 - Corpus INLINEFORM7 ) with the ground truths in Table TABREF48 . Hence, we ran four experiments on this data set in total.",
        "type": "Document"
      },
      {
        "id": "947a5cf6-1656-4ca3-827b-105c8344c1d6",
        "metadata": {
          "vector_store_key": "1910.11949-3",
          "chunk_id": 16,
          "document_id": "1910.11949",
          "start_idx": 8700,
          "end_idx": 9318
        },
        "page_content": "In particular, we use two types of datasets to train our models: A dataset that maps pictures with questions, and an open-domain conversation dataset. The details of the two datasets are as follows. We use MS COCO, Bing and Flickr datasets from BIBREF26 to train the model that generates questions. These datasets contain natural questions about images with the purpose of knowing more about the picture. As can be seen in the Figure FIGREF8, questions cannot be answered by only looking at the image. Each source contains 5,000 images with 5 questions per image, adding a total of 15,000 images with 75,000 questions.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What human evaluation metrics were used in the paper?",
    "llm_answer": "Task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, partner collaboration, information ease, task ease, user expertise, and a 5-point Likert scale for question quality.\n",
    "context": [
      {
        "id": "55dbf52d-8f13-4098-bee7-188913301f41",
        "metadata": {
          "vector_store_key": "1908.07816-1",
          "chunk_id": 31,
          "document_id": "1908.07816",
          "start_idx": 17850,
          "end_idx": 18520
        },
        "page_content": "Recent work BIBREF25 has shown that the automatic evaluation metrics borrowed from machine translation such as BLEU score BIBREF26 tend to align poorly with human judgement. Therefore, in this paper, we mainly adopt human evaluation, along with perplexity, following the existing work. To develop a test set for human evaluation, we first selected the emotionally colored dialogs with exactly four turns from the DailyDialog dataset. In the dataset each dialog turn is annotated with a corresponding emotional category, including the neutral one. For our purposes we filtered out only those dialogs where more than a half of utterances have non-neutral emotional labels.",
        "type": "Document"
      },
      {
        "id": "ab676abe-1fd3-450a-a82d-153595442021",
        "metadata": {
          "vector_store_key": "1709.10217-1",
          "chunk_id": 11,
          "document_id": "1709.10217",
          "start_idx": 6267,
          "end_idx": 6922
        },
        "page_content": "For building the dialogue systems of participants, we release an example set of complete user intent and three data files of flight, train and hotel in JSON format. There are five evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the number of total tasks. User satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied, neutral, satisfied and very satisfied, respectively. Response fluency: There are three scores -1, 0, 1, which indicate nonfluency, neutral, fluency. Number of dialogue turns: The number of utterances in a task-completed dialogue.",
        "type": "Document"
      },
      {
        "id": "eab7e7bd-c8ba-4bad-955c-111fcdafe6ae",
        "metadata": {
          "vector_store_key": "2003.05995-3",
          "chunk_id": 43,
          "document_id": "2003.05995",
          "start_idx": 22319,
          "end_idx": 22996
        },
        "page_content": "The post-task questionnaire had four questions rated in 7-point rating scales that are loosely based on the PARADISE BIBREF27 questions for spoken dialogue systems: Partner collaboration: \u201cHow helpful was your partner?\u201d on a scale of 1 (not helpful at all) to 7 (very helpful). Information ease: \u201cIn this conversation, was it easy to get the information that I needed?\u201d on a scale of 1 (no, not at all) to 7 (yes, completely). Task ease: \u201cHow easy was the task?\u201d on a scale of 1 (very easy) to 7 (very difficult). User expertise: \u201cIn this conversation, did you know what you could say or do at each point of the dialog?\u201d on a scale of 1 (no, not at all) to 7 (yes, completely).",
        "type": "Document"
      },
      {
        "id": "f221ac3d-8ea6-49e8-8ce8-1528ccd79f99",
        "metadata": {
          "vector_store_key": "1909.00578-2",
          "chunk_id": 2,
          "document_id": "1909.00578",
          "start_idx": 1185,
          "end_idx": 1928
        },
        "page_content": "Sum-QE achieves very high correlations with human ratings, showing the ability of BERT to model linguistic qualities that relate to both text content and form. Summarization evaluation metrics like Pyramid BIBREF5 and ROUGE BIBREF3, BIBREF2 are recall-oriented; they basically measure the content from a model (reference) summary that is preserved in peer (system generated) summaries. Pyramid requires substantial human effort, even in its more recent versions that involve the use of word embeddings BIBREF8 and a lightweight crowdsourcing scheme BIBREF9. ROUGE is the most commonly used evaluation metric BIBREF10, BIBREF11, BIBREF12. Inspired by BLEU BIBREF4, it relies on common $n$-grams or subsequences between peer and model summaries.",
        "type": "Document"
      },
      {
        "id": "8c81c383-8e40-4094-8e18-ab0a2d32126a",
        "metadata": {
          "vector_store_key": "1905.08949-6",
          "chunk_id": 18,
          "document_id": "1905.08949",
          "start_idx": 10340,
          "end_idx": 11074
        },
        "page_content": "Although the datasets are commonly shared between QG and QA, it is not the case for evaluation: it is challenging to define a gold standard of proper questions to ask. Meaningful, syntactically correct, semantically sound and natural are all useful criteria, yet they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks. As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE BIBREF43 , are also widely used.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "For the purposes of this paper, how is something determined to be domain specific knowledge?",
    "llm_answer": "Domain-specific knowledge is described by the dominant subject matter of text, which might give specialized vocabulary or specific, unusual word senses.  Examples include \"financial text\" or \"popular culture.\"\n",
    "context": [
      {
        "id": "b61fbff6-9989-4be8-b9e3-0ded8355c234",
        "metadata": {
          "vector_store_key": "1605.08675-1",
          "chunk_id": 26,
          "document_id": "1605.08675",
          "start_idx": 14807,
          "end_idx": 15551
        },
        "page_content": "Secondly, it may be annotated using a set of tools (NLP), but this could also happen at an answering stage for selected documents only. After the system receives a question, it gets analysed (QUESTION ANALYSIS) and transformed into a data structure, called question model. One of its constituents, a search query, is used to find a set of documents, which are probably appropriate for the current problem (SEARCH). For each of the documents, all entity mentions compatible with an obtained question type (e.g. monarchs), are extracted (ENTITY RECOGNITION). For each of them, a context is generated (CONTEXT GENERATION). Finally, a distance between a question content and the entity context is computed to asses its relevance (DISTANCE MEASURE).",
        "type": "Document"
      },
      {
        "id": "fc3fff88-91b5-43eb-ba3d-90c6824c69c7",
        "metadata": {
          "vector_store_key": "1809.05752-2",
          "chunk_id": 16,
          "document_id": "1809.05752",
          "start_idx": 9507,
          "end_idx": 10175
        },
        "page_content": "After using the RPDR query tool to extract EHR paragraphs from the RPDR database, we created a training corpus by categorizing the extracted paragraphs according to their risk factor domain using a lexicon of 120 keywords that were identified by the clinicians involved in this project. Certain domains \u2013 particularly those involving thoughts and other abstract concepts \u2013 are often identifiable by MWEs rather than single words. The same clinicians who identified the keywords manually examined the bigrams and trigrams with the highest TF-IDF scores for each domain in the categorized paragraphs, identifying those which are conceptually related to the given domain.",
        "type": "Document"
      },
      {
        "id": "ed2b65a3-293a-4a8c-874e-949c480c257f",
        "metadata": {
          "vector_store_key": "2002.11893-4",
          "chunk_id": 13,
          "document_id": "2002.11893",
          "start_idx": 8009,
          "end_idx": 8625
        },
        "page_content": "Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context. To make workers understand the task more easily, we crafted templates to generate natural language descriptions for each structured goal. Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality.",
        "type": "Document"
      },
      {
        "id": "ec11c575-9cdc-40f2-b742-781c89922785",
        "metadata": {
          "vector_store_key": "1701.02877-9",
          "chunk_id": 11,
          "document_id": "1701.02877",
          "start_idx": 5586,
          "end_idx": 6230
        },
        "page_content": "Domain describes the dominant subject matter of text, which might give specialised vocabulary or specific, unusal word senses. For example, \u201cbroadcast news\" is a genre, describing the manner of use of language, whereas \u201cfinancial text\" or \u201cpopular culture\" are domains, describing the topic. One notable exception to this terminology is social media, which tends to be a blend of myriad domains and genres, with huge variation in both these dimensions BIBREF38 , BIBREF39 ; for simplicity, we also refer to this as a genre here. In chronological order, the first corpus included here is MUC 7, which is the last of the MUC challenges BIBREF31 .",
        "type": "Document"
      },
      {
        "id": "295852d4-7516-4995-a50b-123620a087ae",
        "metadata": {
          "vector_store_key": "1605.08675-1",
          "chunk_id": 27,
          "document_id": "1605.08675",
          "start_idx": 15551,
          "end_idx": 16267
        },
        "page_content": "Finally, a distance between a question content and the entity context is computed to asses its relevance (DISTANCE MEASURE). All the mentions and their distance scores are stored and, after no more documents are left, used to select the best match (BEST ENTITY SELECTION). The system returns the entity, supplied with information about a supporting sentence and a document, as an answer. Knowledge base (KB) processing consists of two elements: indexing and annotating. The objective of the first is to create an index for efficient searching using a search engine. In the system, Lucene 3.6 is used to build two separate full-text indices: regular and stemmed using a built-in stemmer for Polish, Stempel BIBREF24 .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What type of model are the ELMo representations used in?",
    "llm_answer": "CNNs and Bi-directional LSTMs.\n",
    "context": [
      {
        "id": "0b441b21-296d-49c9-83aa-8c2657bb99d1",
        "metadata": {
          "vector_store_key": "1809.09795-1",
          "chunk_id": 10,
          "document_id": "1809.09795",
          "start_idx": 5604,
          "end_idx": 6342
        },
        "page_content": "Using CNNs, each vector is built upon the characters that compose the underlying words. As ELMo also contains a deep bi-directional LSTM on top of this character-derived vectors, each word-level embedding contains contextual information from their surroundings. Concretely, we use a pre-trained ELMo model, obtained using the 1 Billion Word Benchmark which contains about 800M tokens of news crawl data from WMT 2011 BIBREF24 . Subsequently, the contextualized embeddings are passed on to a BiLSTM with 2,048 hidden units. We aggregate the LSTM hidden states using max-pooling, which in our preliminary experiments offered us better results, and feed the resulting vector to a 2-layer feed-forward network, where each layer has 512 units.",
        "type": "Document"
      },
      {
        "id": "395db516-5e5d-453b-b353-0926fa5a8225",
        "metadata": {
          "vector_store_key": "1809.09795-1",
          "chunk_id": 9,
          "document_id": "1809.09795",
          "start_idx": 5319,
          "end_idx": 5986
        },
        "page_content": "Despite this, all models in the literature rely on word-level representations, which keeps the models from being able to easily capture some of the lexical and morpho-syntactic cues known to denote irony, such as all caps, quotation marks and emoticons, and in Twitter, also emojis and hashtags. The usage of a purely character-based input would allow us to directly recover and model these features. Consequently, our architecture is based on Embeddings from Language Model or ELMo BIBREF10 . The ELMo layer allows to recover a rich 1,024-dimensional dense vector for each word. Using CNNs, each vector is built upon the characters that compose the underlying words.",
        "type": "Document"
      },
      {
        "id": "6ea9dcbb-5844-4b70-b5ae-d444569ccf52",
        "metadata": {
          "vector_store_key": "1910.11769-1",
          "chunk_id": 17,
          "document_id": "1910.11769",
          "start_idx": 10424,
          "end_idx": 11200
        },
        "page_content": "The unsupervised nature of the language model allows it to utilize a large amount of available unlabelled data in order to learn better representations of words. We used the pre-trained ELMo model (v2) available on Tensorhub for this benchmark. We fed the word embeddings of ELMo as input into a one layer Bi-directional RNN (16 units) with GRU cells (with dropout) and a dense layer. Cross-entropy was used as the cost function. Bidirectional Encoder Representations from Transformers (BERT) BIBREF11 has achieved state-of-the-art results on several NLP tasks, including sentence classification. We used the fine-tuning procedure outlined in the original work to adapt the pre-trained uncased BERT$_\\textrm {{\\scriptsize LARGE}}$ to a multi-class passage classification task.",
        "type": "Document"
      },
      {
        "id": "e43f81eb-a4af-4476-ab10-d1f5f4551018",
        "metadata": {
          "vector_store_key": "1911.03894-3",
          "chunk_id": 5,
          "document_id": "1911.03894",
          "start_idx": 3192,
          "end_idx": 3964
        },
        "page_content": "Contextualised word representations such as ELMo BIBREF5 and flair BIBREF6, improved the expressivity of word embeddings by taking context into account. They improved the performance of downstream tasks when they replaced traditional word representations. This paved the way towards larger contextualised models that replaced downstream architectures in most tasks. These approaches, trained with language modeling objectives, range from LSTM-based architectures such as ULMFiT BIBREF15 to the successful transformer-based architectures such as GPT2 BIBREF8, BERT BIBREF7, RoBERTa BIBREF9 and more recently ALBERT BIBREF16 and T5 BIBREF10. Since the introduction of word2vec BIBREF3, many attempts have been made to create monolingual models for a wide range of languages.",
        "type": "Document"
      },
      {
        "id": "592c7752-6b01-4685-aa48-a071601f120e",
        "metadata": {
          "vector_store_key": "1812.07023-4",
          "chunk_id": 19,
          "document_id": "1812.07023",
          "start_idx": 10986,
          "end_idx": 11756
        },
        "page_content": "For the official dataset, training, validation and test sets are drawn from the corresponding Charades sets. The Charades dataset also provides additional annotations for the videos such as action, scene, and object annotations, which are considered to be external data sources by the avsd challenge, for which there is a special sub-task in the challenge. The action annotations also include the start and end time of the action in the video. Our modelname model is based on the hred framework for modelling dialogue systems. In our model, an utterance-level recurrent lstm encoder encodes utterances and a dialogue-level recurrent lstm encoder encodes the final hidden states of the utterance-level encoders, thus maintaining the dialogue state and dialogue coherence.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does using phonetic feedback improve state-of-the-art systems?",
    "llm_answer": "The context states that the model using phonetic feedback outperforms the best character-based result in the literature (Deep Speech 2) on the WSJ benchmark.\n",
    "context": [
      {
        "id": "2dd6b3f7-d5b2-4a26-a547-40656e019b94",
        "metadata": {
          "vector_store_key": "2003.01769-3",
          "chunk_id": 1,
          "document_id": "2003.01769",
          "start_idx": 739,
          "end_idx": 1422
        },
        "page_content": "However, our hypothesis is that directly providing phonetic feedback can be a powerful additional signal for speech enhancement. For example, many local metrics will be more attuned to high-energy regions of speech, but not all phones of a language carry equal energy in production (compare /v/ to /ae/). Our proxy for phonetic intelligibility is a frozen automatic speech recognition (ASR) acoustic model trained on clean speech; the loss functions we incorporate into training encourage the speech enhancement system to produce output that is interpretable to a fixed acoustic model as clean speech, by making the output of the acoustic model mimic its behavior under clean speech.",
        "type": "Document"
      },
      {
        "id": "2584e746-a468-421a-b9b6-0cca72fdf8e9",
        "metadata": {
          "vector_store_key": "2003.01769-0",
          "chunk_id": 23,
          "document_id": "2003.01769",
          "start_idx": 12347,
          "end_idx": 13017
        },
        "page_content": "We find that joint training performs much better on the enhancement metrics in this setup, though still not quite as well as the mimic setup. Compared to the previous experiment without parallel data, the presence of the spectral magnitude and time-domain losses likely keep the enhancement output more stable when joint training, at the cost of requiring parallel training data. We have shown that phonetic feedback is valuable for speech enhancement systems. In addition, we show that our approach to this feedback, the mimic loss framework, is useful in many scenarios: with and without the presence of parallel data, in both the enhancement and robust ASR scenarios.",
        "type": "Document"
      },
      {
        "id": "69248264-66c4-455d-a487-5d6b02e20fd1",
        "metadata": {
          "vector_store_key": "1904.05862-4",
          "chunk_id": 3,
          "document_id": "1904.05862",
          "start_idx": 1955,
          "end_idx": 2891
        },
        "page_content": "Our model, , is a convolutional neural network that takes raw audio as input and computes a general representation that can be input to a speech recognition system. The objective is a contrastive loss that requires distinguishing a true future audio sample from negatives BIBREF22 , BIBREF23 , BIBREF15 . Different to previous work BIBREF15 , we move beyond frame-wise phoneme classification and apply the learned representations to improve strong supervised ASR systems. relies on a fully convolutional architecture which can be easily parallelized over time on modern hardware compared to recurrent autoregressive models used in previous work (\u00a7 SECREF2 ). Our experimental results on the WSJ benchmark demonstrate that pre-trained representations estimated on about 1,000 hours of unlabeled speech can substantially improve a character-based ASR system and outperform the best character-based result in the literature, Deep Speech 2.",
        "type": "Document"
      },
      {
        "id": "26b8a209-cebb-4537-922e-7b34529def1a",
        "metadata": {
          "vector_store_key": "2001.00137-1",
          "chunk_id": 2,
          "document_id": "2001.00137",
          "start_idx": 1208,
          "end_idx": 2001
        },
        "page_content": "This is an important issue to tackle because, in addition to more widespread user adoption, improving Speech-to-Text (STT) accuracy diminishes error propagation to modules using the recognized text. With that in mind, in order for current systems to improve the quality of their services, there is a need for development of robust intelligent systems that are able to understand a user even when faced with incomplete representation in language. The advancement of deep neural networks have immensely aided in the development of the Natural Language Processing (NLP) domain. Tasks such as text generation, sentence correction, image captioning and text classification, have been possible via models such as Convolutional Neural Networks and Recurrent Neural Networks BIBREF5, BIBREF6, BIBREF7.",
        "type": "Document"
      },
      {
        "id": "847cb598-6e5d-46a4-a67e-3589834994be",
        "metadata": {
          "vector_store_key": "2003.01769-3",
          "chunk_id": 3,
          "document_id": "2003.01769",
          "start_idx": 1608,
          "end_idx": 2321
        },
        "page_content": "When no parallel data is available, but transcripts are available, a loss is easily computed against hard senone labels and backpropagated to the enhancement model trained from scratch. Since the clean acoustic model is frozen, the only way for the enhancement model to improve the loss is to make a signal that is more recognizable to the acoustic model. The improvement by this model demonstrates the power of phonetic feedback; very few neural enhancement techniques until now have been able to achieve improvements without parallel data. When parallel data is available, mimic loss works by comparing the outputs of the acoustic model on clean speech with the outputs of the acoustic model on denoised speech.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what are the baselines?",
    "llm_answer": "i-vector system BIBREF3, x-vector system BIBREF10, Sliding Window (SW), Sliding Window + Distance (SW+D), Majority Class, Exact Match, First Occurrence, LSTM, ELMo$_{token}$, ELMo$_{sent}$, GPT, 3-gram, 4-gram, 5-gram, bidirectional RNN language model.\n",
    "context": [
      {
        "id": "02bef31f-a2b5-4b89-8063-62dc0b0f91eb",
        "metadata": {
          "vector_store_key": "1911.01799-4",
          "chunk_id": 19,
          "document_id": "1911.01799",
          "start_idx": 11188,
          "end_idx": 11946
        },
        "page_content": "Two state-of-the-art baseline systems were built following the Kaldi SITW recipe BIBREF22: an i-vector system BIBREF3 and an x-vector system BIBREF10. For the i-vector system, the acoustic feature involved 24-dimensional MFCCs plus the log energy, augmented by the first- and second-order derivatives. We also applied the cepstral mean normalization (CMN) and the energy-based voice active detection (VAD). The universal background model (UBM) consisted of $2,048$ Gaussian components, and the dimensionality of the i-vector space was 400. LDA was applied to reduce the dimensionality of the i-vectors to 150. The PLDA model was used for scoring BIBREF4. For the x-vector system, the feature-learning component was a 5-layer time-delay neural network (TDNN).",
        "type": "Document"
      },
      {
        "id": "ddbf653d-2b06-40f5-b102-1128cf41b68a",
        "metadata": {
          "vector_store_key": "1707.03904-0",
          "chunk_id": 31,
          "document_id": "1707.03904",
          "start_idx": 17195,
          "end_idx": 17895
        },
        "page_content": "For Quasar-T we also test the Sliding Window (SW) and Sliding Window + Distance (SW+D) baselines proposed in BIBREF13 . The scores were computed for the list of candidate solutions described in Section \"Context Retrieval\" . For Quasar-S, since the answers come from a fixed vocabulary of entities, we test language model baselines which predict the most likely entity to appear in a given context. We train three n-gram baselines using the SRILM toolkit BIBREF21 for $n=3,4,5$ on the entire corpus of all Stack Overflow posts. The output predictions are restricted to the output vocabulary of entities. We also train a bidirectional Recurrent Neural Network (RNN) language model (based on GRU units).",
        "type": "Document"
      },
      {
        "id": "5e76f24e-4c75-4a34-b69c-7fe1f8d12771",
        "metadata": {
          "vector_store_key": "1909.02635-2",
          "chunk_id": 15,
          "document_id": "1909.02635",
          "start_idx": 9069,
          "end_idx": 9719
        },
        "page_content": "This includes (i) Majority Class, (ii) Exact Match of an ingredient $e$ in recipe step $s_t$, and (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo$_{token}$ and ELMo$_{sent}$) BIBREF22 compared to GPT.",
        "type": "Document"
      },
      {
        "id": "de2465b8-bc43-4557-b11e-b5ad1b2f0d72",
        "metadata": {
          "vector_store_key": "1909.00279-3",
          "chunk_id": 23,
          "document_id": "1909.00279",
          "start_idx": 13381,
          "end_idx": 14083
        },
        "page_content": "(Section SECREF27) To this end, we built a dataset as described in Section SECREF18. Evaluation metrics and baselines are described in Section SECREF21 and SECREF22. For the implementation details of building the dataset and models, please refer to supplementary materials. Training and Validation Sets We collected a corpus of poems and a corpus of vernacular literature from online resources. The poem corpus contains 163K quatrain poems from Tang Poems and Song Poems, the vernacular literature corpus contains 337K short paragraphs from 281 famous books, the corpus covers various literary forms including prose, fiction and essay. Note that our poem corpus and a vernacular corpus are not aligned.",
        "type": "Document"
      },
      {
        "id": "c135d56d-1485-4494-bd43-1c96dff69c88",
        "metadata": {
          "vector_store_key": "1701.00185-3",
          "chunk_id": 48,
          "document_id": "1701.00185",
          "start_idx": 26209,
          "end_idx": 26890
        },
        "page_content": "The value of INLINEFORM0 for the top INLINEFORM1 -max pooling in Eqn. ( EQREF10 ) is 5. The number of feature maps at the first convolutional layer is 12, and 8 feature maps at the second convolutional layer. Both those two convolutional layers are followed by a folding layer. We further set the dimension of word embeddings INLINEFORM2 as 48. Finally, the dimension of the deep feature representation INLINEFORM3 is fixed to 480. Moreover, we set the learning rate INLINEFORM4 as 0.01 and the mini-batch training size as 200. The output size INLINEFORM5 in Eqn. ( EQREF19 ) is set same as the best dimensions of subspace in the baseline method, as described in Section SECREF37 .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the results of the experiment?",
    "llm_answer": "The provided text describes experiments, but does not summarize results in a concise, generalizable way.  Specific results are presented for different experiments, but not a unified \"result\" for all experiments.  For example, increasing documents retrieved in a search can increase recall but decrease precision.  The text also discusses participant evaluations and system performance, but again, no single, overarching result is given.  The answer is not found in the context.\n",
    "context": [
      {
        "id": "b360d511-f50b-49b0-adae-b9cdd19697b2",
        "metadata": {
          "vector_store_key": "1908.07816-1",
          "chunk_id": 33,
          "document_id": "1908.07816",
          "start_idx": 19008,
          "end_idx": 19804
        },
        "page_content": "Thus each person produced 25 dialogs, and in total we obtained 50 emotionally negative daily dialogs in addition to the 14 already available. To form the test set, we randomly selected 50 emotionally positive and 50 emotionally negative dialogs from the two pools of dialogs described above (78 positive dialogs from DailyDialog, 64 negative dialogs from DailyDialog and human-generated). For human evaluation of the models, we recruited another four English-speaking students from our university without any relationship to the authors' lab to rate the responses generated by the models. Specifically, we randomly shuffled the 100 dialogs in the test set, then we used the first three utterances of each dialog as the input to the three models being compared and let them generate the responses.",
        "type": "Document"
      },
      {
        "id": "711988d5-b6d5-455f-a849-15926d3a08fb",
        "metadata": {
          "vector_store_key": "1910.12618-2",
          "chunk_id": 12,
          "document_id": "1910.12618",
          "start_idx": 6819,
          "end_idx": 7418
        },
        "page_content": "The rest of this paper is organized as follows. The following section introduces the two data sets used to conduct our study. Section 3 presents the different machine learning approaches used and how they were tuned. Section 4 highlights the main results of our study, while section 5 concludes this paper and gives insight on future possible work. In order to prove the consistency of our work, experiments have been conducted on two data sets, one for France and the other for the UK. In this section details about the text and time series data are given, as well as the major preprocessing steps.",
        "type": "Document"
      },
      {
        "id": "f52b8f16-ad56-40ac-9993-378236de84c3",
        "metadata": {
          "vector_store_key": "1605.08675-3",
          "chunk_id": 85,
          "document_id": "1605.08675",
          "start_idx": 46989,
          "end_idx": 47743
        },
        "page_content": "The goal of the first experiment is to test how number a of documents retrieved from the search engine and analysed by the entity recognition techniques, influences the performance. Question classification errors have been bypassed as described in the previous paragraph. Additionally, two versions have been evaluated: with and without corrections of a retrieved set of documents. Figure FIGREF89 demonstrates results for different entity recognition techniques. As we can see, if a retrieved set contains the desired article, adding new documents slightly increases recall, while precision drops observably. That is because additional irrelevant documents usually introduce noise. However, in some cases they are useful, as increasing recall indicates.",
        "type": "Document"
      },
      {
        "id": "8d9f5c38-6bc0-4659-9333-e06a908e8875",
        "metadata": {
          "vector_store_key": "1709.10217-4",
          "chunk_id": 15,
          "document_id": "1709.10217",
          "start_idx": 8466,
          "end_idx": 9077
        },
        "page_content": "If the tester says \u201ctoday\u201d, the systems developed by the participants should understand that he/she indicates the date of April 18, 2017. There are 74 participants who are signing up the evaluation. The final number of participants is 28 and the number of submitted systems is 43. Table TABREF14 and TABREF15 show the evaluation results of the closed test and open test of the task 1 respectively. Due to the space limitation, we only present the top 5 results of task 1. We will add the complete lists of the evaluation results in the version of full paper. Note that for task 2, there are 7 submitted systems.",
        "type": "Document"
      },
      {
        "id": "93a841ff-3b54-4752-90d9-794ea465a2e0",
        "metadata": {
          "vector_store_key": "1709.01256-4",
          "chunk_id": 38,
          "document_id": "1709.01256",
          "start_idx": 20976,
          "end_idx": 21630
        },
        "page_content": "We generated six data sets using different random seeds, and each data set contained six corpora (Corpus 0 - 5). Table TABREF48 summarizes the first data set. In each data set, we name the initial corpus Corpus 0, and define INLINEFORM0 as the timestamp when we started this simulation process. We set INLINEFORM1 , INLINEFORM2 . Corpus INLINEFORM3 corresponds to documents generated before timestamp INLINEFORM4 . We extracted document revisions from Corpus INLINEFORM5 and compared the revisions generated in (Corpus INLINEFORM6 - Corpus INLINEFORM7 ) with the ground truths in Table TABREF48 . Hence, we ran four experiments on this data set in total.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How was the dataset collected?",
    "llm_answer": "The Polish quiz show dataset was downloaded as a single database dump.  Laptop and Restaurant datasets were collected from SemEval-2016.  The HEOT dataset was from a previous study by Mathur et al.  An English tweet dataset was from a study by Davidson et al.\n",
    "context": [
      {
        "id": "56e160ae-5061-4b16-89e6-3dcdc3fc7666",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 10,
          "document_id": "1912.13109",
          "start_idx": 4796,
          "end_idx": 5441
        },
        "page_content": "Basic summary and examples of the data from the dataset are below: The obtained data set had many challenges and thus a data preparation task was employed to clean the data and make it ready for the deep learning pipeline. The challenges and processes that were applied are stated below: Messy text messages: The tweets had urls, punctuations, username mentions, hastags, emoticons, numbers and lots of special characters. These were all cleaned up in a preprocessing cycle to clean the data. Stop words: Stop words corpus obtained from NLTK was used to eliminate most unproductive words which provide little information about individual tweets.",
        "type": "Document"
      },
      {
        "id": "4e4a9c72-348e-4ad9-99fa-d93ed3fe149d",
        "metadata": {
          "vector_store_key": "1605.08675-5",
          "chunk_id": 76,
          "document_id": "1605.08675",
          "start_idx": 42042,
          "end_idx": 42683
        },
        "page_content": "It has been downloaded from a project site as a single database dump at 03.03.2013, from which plain text files have been extracted using Wikipedia Extractor 2.2 script. It means that only plain text is taken into account \u2013 without lists, infoboxes, tables, etc. This procedure leads to a corpus with 895,486 documents, containing 168,982,550 segments, which undergo the annotation process, described in section SECREF12 . The questions that are to be answered with the knowledge base come from two separate sets: Development set bases on 1500 (1130 after filtering) questions from a Polish quiz TV show, called Jeden z dziesi\u0119ciu BIBREF55 .",
        "type": "Document"
      },
      {
        "id": "8c714d0f-b982-4837-aefc-3b59003e3a67",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 15,
          "document_id": "1912.13109",
          "start_idx": 7410,
          "end_idx": 8070
        },
        "page_content": "Train-test split: The labelled dataset that was available for this task was very limited in number of examples and thus as noted above few data augmentation techniques were applied to boost the learning of the network. Before applying augmentation, a train-test split of 78%-22% was done from the original, cleansed data set. Thus, 700 tweets/messages were held out for testing. All model evaluation were done in on the test set that got generated by this process. The results presented in this report are based on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data.",
        "type": "Document"
      },
      {
        "id": "1b887290-2c77-48b6-b1a2-fc513d707afc",
        "metadata": {
          "vector_store_key": "1909.00124-1",
          "chunk_id": 20,
          "document_id": "1909.00124",
          "start_idx": 11467,
          "end_idx": 12190
        },
        "page_content": "The other two datasets are laptop and restaurant datasets collected from SemEval-2016 . The former consists of laptop review sentences and the latter consists of restaurant review sentences. The original datasets (i.e., Laptop and Restaurant) were annotated with aspect polarity in each sentence. We used all sentences with only one polarity (positive or negative) for their aspects. That is, we only used sentences with aspects having the same sentiment label in each sentence. Thus, the sentiment of each aspect gives the ground-truth as the sentiments of all aspects are the same. For each clean-labeled dataset, the sentences are randomly partitioned into training set and test set with $80\\%$ and $20\\%$, respectively.",
        "type": "Document"
      },
      {
        "id": "5c4b1458-497e-4f2e-a4e7-c5e739d9cad0",
        "metadata": {
          "vector_store_key": "1912.13109-1",
          "chunk_id": 9,
          "document_id": "1912.13109",
          "start_idx": 4561,
          "end_idx": 5222
        },
        "page_content": "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al. This dataset was important to employ Transfer Learning to our task since the number of labeled dataset was very small. Basic summary and examples of the data from the dataset are below: The obtained data set had many challenges and thus a data preparation task was employed to clean the data and make it ready for the deep learning pipeline.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many annotators participated?",
    "llm_answer": "4\n",
    "context": [
      {
        "id": "188012af-4dd7-4f42-9775-be8dcdd871ca",
        "metadata": {
          "vector_store_key": "2004.03744-2",
          "chunk_id": 48,
          "document_id": "2004.03744",
          "start_idx": 26393,
          "end_idx": 27002
        },
        "page_content": "2,060 workers participated in the annotation effort, with an average of 1.98 assignments per worker and a standard deviation of 5.54. We required the workers to have a previous approval rate above 90%. No restriction was put on the workers' location. Each assignment consisted of a set of 10 image-sentence pairs. For each pair, the participant was asked to (a) choose a label, (b) highlight words in the sentence that led to their decision, and (c) explain their decision in a comprehensive and concise manner, using a subset of the words that they highlighted. The instructions are shown in Figure FIGREF42.",
        "type": "Document"
      },
      {
        "id": "c2650d93-c3c3-4583-b079-29db9f7fc689",
        "metadata": {
          "vector_store_key": "1703.05260-3",
          "chunk_id": 32,
          "document_id": "1703.05260",
          "start_idx": 17652,
          "end_idx": 18317
        },
        "page_content": "The annotation was rather time-consuming due to the complexity of the task, and thus we decided for single annotation mode. To assess annotation quality, a small sample of texts was annotated by all four annotators and their inter-annotator agreement was measured (see Section \"Inter-Annotator Agreement\" ). It was found to be sufficiently high. Annotation of the corpus together with some pre- and post-processing of the data required about 500 hours of work. All stories were annotated with event and participant types (a total of 12,188 and 43,946 instances, respectively). On average there were 7 coreference chains per story with an average length of 6 tokens.",
        "type": "Document"
      },
      {
        "id": "7e41f955-4622-4d3a-b465-fd3d8521a43d",
        "metadata": {
          "vector_store_key": "1912.01673-1",
          "chunk_id": 21,
          "document_id": "1912.01673",
          "start_idx": 12287,
          "end_idx": 13032
        },
        "page_content": "The time needed to carry out one piece of annotation (i.e. to provide one seed sentence with all 15 transformations) was on average almost 20 minutes but some annotators easily needed even half an hour. Out of the 4262 distinct sentences, only 188 was recorded more than once. In other words, the chance of two annotators producing the same output string is quite low. The most repeated transformations are by far past, future and ban. The least repeated is paraphrase with only single one repeated. multiple-annots documents this in another way. The 293 annotations are split into groups depending on how many annotators saw the same input sentence: 30 annotations were annotated by one person only, 30 annotations by two different persons etc.",
        "type": "Document"
      },
      {
        "id": "99ad02db-5ec2-4581-9917-d2c099b5f1b3",
        "metadata": {
          "vector_store_key": "1912.01673-1",
          "chunk_id": 22,
          "document_id": "1912.01673",
          "start_idx": 12632,
          "end_idx": 13376
        },
        "page_content": "The 293 annotations are split into groups depending on how many annotators saw the same input sentence: 30 annotations were annotated by one person only, 30 annotations by two different persons etc. The last column shows the number of unique outputs obtained in that group. Across all cases, 96.8% of produced strings were unique. In line with instructions, the annotators were using the IMPOSSIBLE option scarcely (95 times, i.e. only 2%). It was also a case of 7 annotators only; the remaining 5 annotators were capable of producing all requested transformations. The top three transformations considered unfeasible were different meaning (using the same set of words), past (esp. for sentences already in the past tense) and simple sentence.",
        "type": "Document"
      },
      {
        "id": "a7206ebd-d04d-47ec-b00f-0ae9d605cbde",
        "metadata": {
          "vector_store_key": "1703.05260-3",
          "chunk_id": 43,
          "document_id": "1703.05260",
          "start_idx": 23985,
          "end_idx": 24623
        },
        "page_content": "For coreference chain annotation, we calculated the percentage of pairs which were annotated by at least 3 annotators (qualified majority vote) compared to the set of those pairs annotated by at least one person (see Figure 4 ). We take the result of 90.5% between annotators to be a good agreement. Figure 5 gives an overview of the number of event and participant types provided in the templates. Taking a flight and getting a haircut stand out with a large number of both event and participant types, which is due to the inherent complexity of the scenarios. In contrast, planting a tree and going on a train contain the fewest labels.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they show their model discovers underlying syntactic structure?",
    "llm_answer": "The model's unsupervised parsing outperforms some strong baseline models, demonstrating that the structure found by their model is similar to the intrinsic structure provided by human experts.\n",
    "context": [
      {
        "id": "dd28628f-62c7-4f01-bf73-16148cfbf8f3",
        "metadata": {
          "vector_store_key": "1711.02013-1",
          "chunk_id": 5,
          "document_id": "1711.02013",
          "start_idx": 2877,
          "end_idx": 3616
        },
        "page_content": "With our model, we assume that language can be naturally represented as a tree-structured graph. The model is composed of three parts: We evaluate our model on three tasks: word-level language modeling, character-level language modeling, and unsupervised constituency parsing. The proposed model achieves (or is close to) the state-of-the-art on both word-level and character-level language modeling. The model's unsupervised parsing outperforms some strong baseline models, demonstrating that the structure found by our model is similar to the intrinsic structure provided by human experts. The idea of introducing some structures, especially trees, into language understanding to help a downstream task has been explored in various ways.",
        "type": "Document"
      },
      {
        "id": "e030d5c0-9c1a-41a6-b4b4-0a34c1f87798",
        "metadata": {
          "vector_store_key": "1711.02013-3",
          "chunk_id": 43,
          "document_id": "1711.02013",
          "start_idx": 24813,
          "end_idx": 25596
        },
        "page_content": "In this paper, we propose a novel neural language model that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model. We introduce a new neural parsing network: Parsing-Reading-Predict Network, that can make differentiable parsing decisions. We use a new structured attention mechanism to control skip connections in a recurrent neural network. Hence induced syntactic structure information can be used to improve the model's performance. Via this mechanism, the gradient can be directly back-propagated from the language model loss function into the neural Parsing Network. The proposed model achieve (or is close to) the state-of-the-art on both word/character-level language modeling tasks.",
        "type": "Document"
      },
      {
        "id": "84104c9f-f895-46f2-b0a6-d3f3451ac006",
        "metadata": {
          "vector_store_key": "1906.00180-3",
          "chunk_id": 3,
          "document_id": "1906.00180",
          "start_idx": 1879,
          "end_idx": 2547
        },
        "page_content": "First, we develop a protocol for automatically generating data that can be used in entailment recognition tasks. Second, we demonstrate that several deep learning architectures succeed at one such task. Third, we present and apply a number of experiments to test whether models are capable of compositional generalization. The data generation process is inspired by BIBREF13 : an artificial language is defined, sentences are generated according to its grammar and the entailment relation between pairs of such sentences is established according to a fixed background logic. However, our language is significantly more complex, and instead of natural logic we use FOL.",
        "type": "Document"
      },
      {
        "id": "05a73f89-afb8-42b1-ad95-08ddd224cc2f",
        "metadata": {
          "vector_store_key": "1906.00180-3",
          "chunk_id": 7,
          "document_id": "1906.00180",
          "start_idx": 4156,
          "end_idx": 4862
        },
        "page_content": "The consistently higher testing accuracy provides evidence that the recurrent networks are not only capable of recognizing FOL entailment relations between unseen sentences. They can also outperform the tree-shaped models on this task, although they do not use any of the symbolic structure that seemed to explain the success of their recursive predecessors. The recurrent classifiers have learned to apply their own strategies, which we will investigate in the remainder of this paper. Compositionality is the ability to interpret and generate a possibly infinite number of constructions from known constituents, and is commonly understood as one of the fundamental aspects of human learning and reasoning",
        "type": "Document"
      },
      {
        "id": "ee08930c-388f-47d7-bd79-24ee1d6ede02",
        "metadata": {
          "vector_store_key": "1711.02013-3",
          "chunk_id": 6,
          "document_id": "1711.02013",
          "start_idx": 3373,
          "end_idx": 3987
        },
        "page_content": "The idea of introducing some structures, especially trees, into language understanding to help a downstream task has been explored in various ways. For example, BIBREF5 , BIBREF4 learn a bottom-up encoder, taking as an input a parse tree supplied from an external parser. There are models that are able to infer a tree during test time, while still need supervised signal on tree structure during training. For example, BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 , etc. Moreover, BIBREF22 did an in-depth analysis of recursive models that are able to learn tree structure without being exposed to any grammar trees.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they measure performance of language model tasks?",
    "llm_answer": "Task finish rate (simulations), BLEU score, accuracy (NLI), human evaluation (ranking translations against a reference).\n",
    "context": [
      {
        "id": "a12d06d7-03ca-41a7-8544-9536a025b37c",
        "metadata": {
          "vector_store_key": "2002.11893-8",
          "chunk_id": 59,
          "document_id": "2002.11893",
          "start_idx": 33942,
          "end_idx": 34674
        },
        "page_content": "We calculated \"task finish rate\" on 1000 times simulations for each goal type (See Table TABREF31). Findings are summarized below: Cross multi-domain tasks (CM and CM+T) are much harder to finish. Comparing M and M+T, although each module performs well in traffic domains, additional sub-goals in these domains are still difficult to accomplish. The system-level performance is largely limited by RuleDST and SL policy. Although the corpus-based performance of NLU and NLG modules is high, the two modules still harm the performance. Thus more powerful models are needed for all components of a pipelined dialogue system. TemplateNLG has a much lower BLEU score but performs better than SC-LSTM in natural language level simulation.",
        "type": "Document"
      },
      {
        "id": "24e7a576-ab3d-4f0a-8326-f14fe7d7f139",
        "metadata": {
          "vector_store_key": "1911.03894-4",
          "chunk_id": 26,
          "document_id": "1911.03894",
          "start_idx": 15178,
          "end_idx": 16033
        },
        "page_content": "We also evaluate our model on the Natural Language Inference (NLI) task, using the French part of the XNLI dataset BIBREF50. NLI consists in predicting whether a hypothesis sentence is entailed, neutral or contradicts a premise sentence. The XNLI dataset is the extension of the Multi-Genre NLI (MultiNLI) corpus BIBREF51 to 15 languages by translating the validation and test sets manually into each of those languages. The English training set is also machine translated for all languages. The dataset is composed of 122k train, 2490 valid and 5010 test examples. As usual, NLI performance is evaluated using accuracy. To evaluate a model on a language other than English (such as French), we consider the two following settings: TRANSLATE-TEST: The French test set is machine translated into English, and then used with an English classification model.",
        "type": "Document"
      },
      {
        "id": "7e5fdc0f-cb40-4269-bb3f-304094c2c5c8",
        "metadata": {
          "vector_store_key": "1910.10408-4",
          "chunk_id": 40,
          "document_id": "1910.10408",
          "start_idx": 21178,
          "end_idx": 21879
        },
        "page_content": "After manually inspecting the outputs of the best performing models under the large data condition, we decided to run a human evaluation only for the En-It Len-Tok model. As our ultimate goal is to be able to generate shorter translations and as close as possible to the length of the source sentences, we focused the manual evaluation on the Short output class and aimed to verify possible losses in quality with respect to the baseline system. We ran a head-to-head evaluation on the first 10 sentences of each test talk, for a total of 270 sentences, by asking annotators to blindly rank the two system outputs (ties were also permitted) in terms of quality with respect to a reference translation.",
        "type": "Document"
      },
      {
        "id": "8eadeabb-5370-4958-bea8-48a63ba10138",
        "metadata": {
          "vector_store_key": "2001.11268-5",
          "chunk_id": 40,
          "document_id": "2001.11268",
          "start_idx": 22945,
          "end_idx": 23764
        },
        "page_content": "In practice, the detachment between model predictions and assignment of labels means that a reviewer who wishes to switch between high recall and high precision results can do so very quickly, without obtaining new predictions from the model itself. More visualizations can be found in this project's GitHub repository , including true class labels and a detailed breakdown of true and false predictions for each class. The highest proportion of false classification appears between the results and conclusion classes. The fine-tuned multilingual model showed marginally inferior classification scores on the exclusively English testing data. However, this model's contribution is not limited to the English language because its interior weights embed a shared vocabulary of 100 languages, including German and Chinese.",
        "type": "Document"
      },
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are content clusters used to improve the prediction of incident severity?",
    "llm_answer": "Content clusters, combined with externally imposed classifications, improve the performance of a supervised classifier predicting incident harm severity.  The clusters provide complementary information, especially in cases with highly homogeneous medical content, and mitigate human error in category selection.\n",
    "context": [
      {
        "id": "a5e419f0-c4e1-410d-a6ec-af7296695e8d",
        "metadata": {
          "vector_store_key": "1909.00183-9",
          "chunk_id": 97,
          "document_id": "1909.00183",
          "start_idx": 54275,
          "end_idx": 55055
        },
        "page_content": "The use of content-driven classification of reports could also be important within current efforts by the World Health Organisation (WHO) under the framework for the International Classification for Patient Safety (ICPS) BIBREF9 to establish a set of conceptual categories to monitor, analyse and interpret information to improve patient care. We have used our clusters within a supervised classifier to predict the degree of harm of an incident based only on free-text descriptions. The degree of harm is an important measure in hospital evaluation and has been shown to depend on the reporting culture of the particular organisation. Overall, our method shows that text description complemented by the topic labels extracted by our method show improved performance in this task.",
        "type": "Document"
      },
      {
        "id": "90f10078-bdd5-4cb7-9da3-ea7702c1c1a7",
        "metadata": {
          "vector_store_key": "1909.00183-4",
          "chunk_id": 96,
          "document_id": "1909.00183",
          "start_idx": 53345,
          "end_idx": 54275
        },
        "page_content": "The clusters have high medical content, thus providing complementary information to the externally imposed classification categories. Indeed, some of the most relevant and persistent communities emerge because of their highly homogeneous medical content, even if they cannot be mapped to standardised external categories. An area of future research will be to confirm if the finer unsupervised cluster found by our analysis are consistent with a second level in the hierarchy of external categories (Level 2, around 100 categories), which is used less consistently in hospital settings. The use of content-driven classification of reports could also be important within current efforts by the World Health Organisation (WHO) under the framework for the International Classification for Patient Safety (ICPS) BIBREF9 to establish a set of conceptual categories to monitor, analyse and interpret information to improve patient care.",
        "type": "Document"
      },
      {
        "id": "e62fc46d-f6fc-4df9-b4db-e82fa7b23d80",
        "metadata": {
          "vector_store_key": "1909.00183-9",
          "chunk_id": 7,
          "document_id": "1909.00183",
          "start_idx": 4090,
          "end_idx": 4770
        },
        "page_content": "The obtained results can help mitigate human error or effort in finding the right category in complex classification trees. We illustrate in our analysis the insight gained from this unsupervised, multi-resolution approach in this specialised corpus of medical records. As an additional application, we use machine learning methods for the prediction of the degree of harm of incidents directly from the text in the NRLS incident reports. Although the degree of harm is recorded by the reporting person for every event, this information can be unreliable as reporters have been known to game the system, or to give different answers depending on their professional status BIBREF6.",
        "type": "Document"
      },
      {
        "id": "94cdbbf1-7739-4076-ab56-9804c246b768",
        "metadata": {
          "vector_store_key": "1909.00183-9",
          "chunk_id": 78,
          "document_id": "1909.00183",
          "start_idx": 43583,
          "end_idx": 44253
        },
        "page_content": "Here we approach the task of training a supervised classifier that predicts the degree of harm of an incident based on other features of the record (such as location, external category, and medical specialty) and on the textual component of the report. To this end, we use the embedded text vectors and MS cluster labels of the records as features to predict the degree of harm to the patient. Each NRLS record has more than 170 features filled manually by healthcare staff, including the degree of harm (DoH) to the patient, a crucial assessment of the reported incident. The incident is classified into five levels: 'No harm', 'Low', 'Moderate', 'Severe', and 'Death'.",
        "type": "Document"
      },
      {
        "id": "32e5f167-fee6-4134-b725-8d989e2f4755",
        "metadata": {
          "vector_store_key": "1909.00183-4",
          "chunk_id": 62,
          "document_id": "1909.00183",
          "start_idx": 34309,
          "end_idx": 35149
        },
        "page_content": "The partition into 44 communities presents content clusters with well-defined characterisations, as shown by the Sankey diagram and the highly clustered structure of the contingency table (Figure FIGREF22). Compared to the 15 hand-coded categories, this 44-community partition provides finer groupings corresponding to specific sub-themes within the generic hand-coded categories. This is apparent in the hand-coded classes `Accidents', `Medication', `Clinical assessment', `Documentation' and `Infrastructure', where a variety of meaningful subtopics are identified (see Fig. FIGREF23 for details). In other cases, however, the content clusters cut across the external categories, e.g., the clusters on labour ward, chemotherapy, radiotherapy and infection control are coherent in content but can belong to several of the external classes.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What cluster identification method is used in this paper?",
    "llm_answer": "Markov Stability (MS), a multiscale community detection algorithm.\n",
    "context": [
      {
        "id": "4d537926-6f08-4819-843c-19dd0927e96b",
        "metadata": {
          "vector_store_key": "1904.08386-3",
          "chunk_id": 8,
          "document_id": "1904.08386",
          "start_idx": 4645,
          "end_idx": 5516
        },
        "page_content": "We devise a simple clustering algorithm to approximate this process. First, we initialize with random cluster assignments and define \u201ccluster strength\u201d to be the relative difference between \u201cintra-group\u201d Euclidean distance and \u201cinter-group\u201d Euclidean distance. Then, we iteratively propose random exchanges of memberships, only accepting these proposals when the cluster strength increases, until convergence. To evaluate the quality of the computationally-derived clusters against those of Calvino, we measure cluster purity BIBREF21 : given a set of predicted clusters INLINEFORM1 and ground-truth clusters INLINEFORM2 that both partition a set of INLINEFORM3 data points, INLINEFORM4  While the results from the above section allow us to compare our three computational methods against each other, we additionally collect human judgments to further ground our results.",
        "type": "Document"
      },
      {
        "id": "a1a49f21-1db7-40db-8150-067a721c065b",
        "metadata": {
          "vector_store_key": "1909.00183-2",
          "chunk_id": 15,
          "document_id": "1909.00183",
          "start_idx": 8376,
          "end_idx": 9177
        },
        "page_content": "We sparsify this graph to the union of a minimum spanning tree and a k-Nearest Neighbors (MST-kNN) graph BIBREF14, a geometric construction that removes less important similarities but preserves global connectivity for the graph and, hence, for the dataset. The MST-kNN graph is then analysed with Markov Stability BIBREF15, BIBREF16, BIBREF17, BIBREF18, a multi-resolution graph partitioning method that identifies relevant subgraphs (i.e., clusters of documents) at different levels of granularity. MS uses a diffusive process on the graph to reveal the multiscale organisation at different resolutions without the need to choose a priori the number or type of clusters. The partitions found by MS across levels of resolution are analysed a posteriori through visualisations and quantitative scores.",
        "type": "Document"
      },
      {
        "id": "98d737c7-5b77-4594-8cbe-d3a7307ae9ac",
        "metadata": {
          "vector_store_key": "2003.06279-3",
          "chunk_id": 26,
          "document_id": "2003.06279",
          "start_idx": 15495,
          "end_idx": 16268
        },
        "page_content": "In summary, the methodology used in this paper encompasses the following steps: Network construction: here texts are mapped into a co-occurrence networks. Some variations exists in the literature, however here we focused in the most usual variation, i.e. the possibility of considering or disregarding stopwords. A network with co-occurrence links is obtained after this step. Network enrichment: in this step, the network is enriched with virtual edges established via similarity of word embeddings. After this step, we are given a complete network with weighted links. Virtually, any embedding technique could be used to gauge the similarity between nodes. Network filtering: in order to eliminate spurious links included in the last step, the weakest edges are filtered.",
        "type": "Document"
      },
      {
        "id": "d3ddab90-27a1-4b5b-b0e4-ab709c2828ef",
        "metadata": {
          "vector_store_key": "1909.00183-6",
          "chunk_id": 4,
          "document_id": "1909.00183",
          "start_idx": 2253,
          "end_idx": 3360
        },
        "page_content": "Such tools can also offer unbiased insight into the root cause analysis of incidents that could improve the safety and quality of care and efficiency of healthcare services. In this work, we showcase an algorithmic methodology that detects content-based groups of records in an unsupervised manner, based only on the free (unstructured) textual descriptions of the incidents. To do so, we combine deep neural-network high-dimensional text-embedding algorithms with graph-theoretical methods for multiscale clustering. Specifically, we apply the framework of Markov Stability (MS), a multiscale community detection algorithm, to sparsified graphs of documents obtained from text vector similarities. Our method departs both from traditional natural language processing tools, which have generally used bag-of-words (BoW) representation of documents and statistical methods based on Latent Dirichlet Allocation (LDA) to cluster documents BIBREF2, and from more recent approaches that have used deep neural network based language models, but have used k-means clustering without a graph-based analysis BIBREF3.",
        "type": "Document"
      },
      {
        "id": "845f0b87-0a09-4c39-b672-eb118242ff38",
        "metadata": {
          "vector_store_key": "1703.04617-1",
          "chunk_id": 30,
          "document_id": "1703.04617",
          "start_idx": 16730,
          "end_idx": 17713
        },
        "page_content": "In order to keep calculation differentiable, we compute the weight of all the clusters based on the distances of $\\mathbf {x}$ and each cluster center vector, in stead of just choosing the closest cluster. Then the discriminative vector $\\mathbf {\\delta _x}$ with regard to these most relevant clusters are computed. All this information is combined to obtain the discriminative information. In order to keep the full information of input, we also copy the input question $\\mathbf {x}$ , together with the acquired discriminative information, to a feed-forward layer to obtain a new representation $\\mathbf {x^\\prime }$ for the question. More specifically, the adaptation algorithm contains two steps: adapting and updating, which is detailed as follows: Adapting In the adapting step, we first compute the similarity score between an input question vector $\\mathbf {x}\\in \\mathbb {R} ^{h}$ and each centroid vector of $K$ clusters $~\\mathbf {\\bar{x}}\\in \\mathbb {R} ^{K \\times h}$ .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Why did they think this was a good idea?",
    "llm_answer": "This approach allowed for guided dialogues, which led to better data for training machine learning models, reduced the amount of data needed, and improved the accuracy of the model's performance.  It also saved computing resources.\n",
    "context": [
      {
        "id": "01c98c39-bc6f-4de2-afa3-4d94f9ea0dcc",
        "metadata": {
          "vector_store_key": "2003.05995-5",
          "chunk_id": 17,
          "document_id": "2003.05995",
          "start_idx": 8791,
          "end_idx": 9497
        },
        "page_content": "This is a valuable solution for collecting dialogues in domains where specific expertise is required and the cost of training capable Wizards is high. We required fine-grained control over the Wizard interface so as to be able to generate more directed dialogues for specialised domains, such as emergency response for offshore facilities. By providing the Wizard with several dialogue options (aside from free text), we guided the conversation and could introduce actions that change an internal system state. This proposes several advantages: A guided dialogue allows for set procedures to be learned and reduces the amount of data needed for a machine learning model for dialogue management to converge.",
        "type": "Document"
      },
      {
        "id": "ed9d07b9-fb91-4da9-8d82-6ee0e1648a2f",
        "metadata": {
          "vector_store_key": "2001.00137-0",
          "chunk_id": 39,
          "document_id": "2001.00137",
          "start_idx": 22875,
          "end_idx": 23770
        },
        "page_content": "The idea was to improve the accuracy performance by improving the representation ability of the model with the implementation of novel denoising transformers. More specifically, our model was able to reconstruct hidden embeddings from their respective incomplete hidden embeddings. Stacked DeBERT was compared against three NLU service platforms and two other machine learning methods, namely BERT and Semantic Hashing with neural classifier. Our model showed better performance when evaluated on F1 scores in both Twitter sentiment and intent text with STT error classification tasks. The per-class F1 score was also evaluated in the form of normalized confusion matrices, showing that our model was able to improve the overall performance by better balancing the accuracy of each class, trading-off small decreases in high achieving class for significant improvements in lower performing ones.",
        "type": "Document"
      },
      {
        "id": "7ba6bef8-9db8-473f-877a-6cd151b4774c",
        "metadata": {
          "vector_store_key": "2003.05995-5",
          "chunk_id": 53,
          "document_id": "2003.05995",
          "start_idx": 27314,
          "end_idx": 28045
        },
        "page_content": "It is important to consider the number of available participants ready and willing to perform the task at any one time. This type of crowdsourcing requires two participants to connect within a few minutes of each other to be partnered together. As mentioned above, there were some issues with participants not collaborating and these dialogues had to be discarded as they were not of use. In future work, we want to expand and improve the platform. Dialogue system development can greatly benefit from better ways of obtaining data for rich task-oriented domains such as ours. Part of fully exploiting the potential of crowdsourcing services lies in having readily available tools that help in the generation and gathering of data.",
        "type": "Document"
      },
      {
        "id": "1f3dba72-b97a-4aaa-afd1-13d66851466b",
        "metadata": {
          "vector_store_key": "1906.06448-4",
          "chunk_id": 12,
          "document_id": "1906.06448",
          "start_idx": 6897,
          "end_idx": 7661
        },
        "page_content": "The motivation for using crowdsourcing is to collect naturally alike monotonicity inference problems that include various expressions. One problem here is that it is unclear how to instruct workers to create monotonicity inference problems without knowledge of natural language syntax and semantics. We must make tasks simple for workers to comprehend and provide sound judgements. Moreover, recent studies BIBREF12 , BIBREF3 , BIBREF13 point out that previous crowdsourced datasets, such as SNLI BIBREF14 and MultiNLI BIBREF10 , include hidden biases. As these previous datasets are motivated by approximated entailments, workers are asked to freely write hypotheses given a premise, which does not strictly restrict them to creating logically complex inferences.",
        "type": "Document"
      },
      {
        "id": "11d9962a-2577-495d-917c-943ce90262c9",
        "metadata": {
          "vector_store_key": "2001.11268-0",
          "chunk_id": 51,
          "document_id": "2001.11268",
          "start_idx": 29476,
          "end_idx": 30207
        },
        "page_content": "This was done in order to save computing resources, as an addition of 100 SQuAD domains resulted in training time increases of two hours, depending on various other parameter settings. Adjusted parameters include increased batch size, and decreased maximal context length in order to reduce training time. With this paper we aimed to explore state-of-the-art NLP methods to advance systematic review (semi)automation. Both of the presented fine-tuning approaches for transformers demonstrated flexibility and high performance. We contributed an approach to deal with ambiguity in whole-sentence predictions, and proposed the usage of a completely different approach to entity recognition in settings where training data are sparse.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What QA models were used?",
    "llm_answer": "Pointer Networks, Self-Matching Networks, QA-BERT, and QA-DQN.\n",
    "context": [
      {
        "id": "11333e9e-fb41-439f-b73b-6d153753f7b4",
        "metadata": {
          "vector_store_key": "1804.03396-4",
          "chunk_id": 16,
          "document_id": "1804.03396",
          "start_idx": 8818,
          "end_idx": 9580
        },
        "page_content": "Therefore, in our model, we incorporate Pointer Networks BIBREF39 to adapt to the answers formed by any words within the document in any order as well as Self-Matching Networks BIBREF29 to enhance the ability on modeling longer input documents. The contributions of this paper are as follows: We propose a novel IE framework named QA4IE to overcome the weaknesses of existing IE systems. As we discussed above, the problem of step 1, 2 and 4 can be solved by existing work and we propose to solve the problem of step 3 with QA models. To train a high quality neural network QA model, we build a large IE benchmark in QA style named QA4IE benchmark which consists of 293K Wikipedia articles and 2 million golden relation triples with 636 different relation types.",
        "type": "Document"
      },
      {
        "id": "ac7f0e7d-9b23-4c72-84d1-f98752db2b18",
        "metadata": {
          "vector_store_key": "2001.11268-8",
          "chunk_id": 20,
          "document_id": "2001.11268",
          "start_idx": 11319,
          "end_idx": 12154
        },
        "page_content": "However, the data obtained through this method are not fine-grained enough for usage in data extraction, or for the use in pipelines for automated evidence synthesis. Therefore, we expand our experiments to include QA-BERT, a question-answering model that predicts the locations of PICO entities within sentences. In this work we investigate state-of-the-art methods for language modelling and sentence classification. Our contributions are centred around developing transformer-based fine-tuning approaches tailored to SR tasks. We compare our sentence classification with the LSTM baseline and evaluate the biggest set of PICO sentence data available at this point BIBREF13. We demonstrate that models based on the BERT architecture solve problems related to ambiguous sentence labels by learning to predict multiple labels reliably.",
        "type": "Document"
      },
      {
        "id": "abde311f-db3e-44fd-96aa-882a0a2aeeb0",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 17,
          "document_id": "1912.13109",
          "start_idx": 8552,
          "end_idx": 9307
        },
        "page_content": "These processes and its implementation details will be discussed later but they were learning rate decay, early stopping, model checkpointing and reducing learning rate on plateau. For the loss function we chose categorical cross entropy loss in finding the most optimal weights/parameters of the model. Formally this loss function for the model is defined as below: The double sum is over the number of observations and the categories respectively. While the model probability is the probability that the observation i belongs to category c. Among the model architectures we experimented with and without data augmentation were: Fully Connected dense networks: Model hyperparameters were inspired from the previous work done by Vo et al and Mathur et al.",
        "type": "Document"
      },
      {
        "id": "e7c1ce5e-2cb2-4999-8cf9-87c1af3ba05a",
        "metadata": {
          "vector_store_key": "1908.10449-3",
          "chunk_id": 21,
          "document_id": "1908.10449",
          "start_idx": 11449,
          "end_idx": 12176
        },
        "page_content": "During test, we follow common practice in supervised learning tasks where we report the agent's test performance corresponding to its best validation performance . As a baseline, we propose QA-DQN, an agent that adopts components from QANet BIBREF18 and adds an extra command generation module inspired by LSTM-DQN BIBREF19. As illustrated in Figure FIGREF6, the agent consists of three components: an encoder, an action generator, and a question answerer. More precisely, at a game step $t$, the encoder reads observation string $o_t$ and question string $q$ to generate attention aggregated hidden representations $M_t$. Using $M_t$, the action generator outputs commands (defined in previous sections) to interact with iMRC.",
        "type": "Document"
      },
      {
        "id": "a7fe717f-04bc-4708-96e2-e0d3744977b2",
        "metadata": {
          "vector_store_key": "1707.03904-4",
          "chunk_id": 9,
          "document_id": "1707.03904",
          "start_idx": 5317,
          "end_idx": 6034
        },
        "page_content": "We also collect annotations on a subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short. We plan to release these annotations along with the datasets, and our retrieved documents for each question. Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated for each Quasar variant. The software question set was built from the definitional \u201cexcerpt\u201d entry for each tag (entity) on StackOverflow.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do their results compare against other competitors in the PAN 2017 shared task on Author Profiling?",
    "llm_answer": "N-GrAM was the best-performing system out of 22 submitted.\n",
    "context": [
      {
        "id": "2cfbd5a1-3b0e-4773-bba4-1d7269cafa11",
        "metadata": {
          "vector_store_key": "1707.03764-4",
          "chunk_id": 30,
          "document_id": "1707.03764",
          "start_idx": 15369,
          "end_idx": 16187
        },
        "page_content": "Our system significantly out-performed the baseline on the joint task, as the baseline scored significantly lower for the gender task than for the variety task. We conclude that, for the current author profiling task, a seemingly simple system using word and character n-grams and an SVM classifier proves very hard to beat. Indeed, N-GrAM turned out to be the best-performing out of the 22 systems submitted in this shared task. Using additional training data, `smart' features, and hand-crafted resources hurts rather than helps performance. A possible lesson to take from this would be that manually crafting features serves only to hinder a machine learning algorithm's ability to find patterns in a dataset, and perhaps it is better to focus one's efforts on parameter optimisation instead of feature engineering.",
        "type": "Document"
      },
      {
        "id": "bfb452f9-a05f-42a0-803e-7641dc452965",
        "metadata": {
          "vector_store_key": "1707.03764-4",
          "chunk_id": 5,
          "document_id": "1707.03764",
          "start_idx": 3151,
          "end_idx": 3949
        },
        "page_content": "In this paper we report how our final submitted system works, and provide some general data analysis, but we also devote substantial space to describing what we tried (under which motivations), as we believe this is very informative towards future developments of author profiling systems. After an extensive grid-search we submitted as our final run, a simple SVM system (using the scikit-learn LinearSVM implementation) that uses character 3- to 5-grams and word 1- to 2-grams with tf-idf weighting with sublinear term frequency scaling, where instead of the standard term frequency the following is used:  INLINEFORM0  We ran the grid search over both tasks and all languages on a 64-core machine with 1 TB RAM (see Table TABREF2 for the list of values over which the grid search was performed).",
        "type": "Document"
      },
      {
        "id": "53ebca3d-fe75-4ab2-998f-88fdac0e9135",
        "metadata": {
          "vector_store_key": "1707.03764-3",
          "chunk_id": 12,
          "document_id": "1707.03764",
          "start_idx": 6175,
          "end_idx": 6859
        },
        "page_content": "We divide our attempts according to the different ways we attempted to enhance performance: manipulating the data itself (adding more, and changing preprocessing), using a large variety of features, and changing strategies in modelling the problem by using different algorithms and paradigms. All reported results are on the PAN 2017 training data using five-fold cross-validation, unless otherwise specified. We extended the training dataset by adding data and gender labels from the PAN 16 Author Profiling shared task BIBREF5 . However, the additional data consistently resulted in lower cross-validation scores than when using only the training data provided with the PAN 17 task.",
        "type": "Document"
      },
      {
        "id": "3bb417b4-cec9-4c38-9627-1d4f81fa7c9a",
        "metadata": {
          "vector_store_key": "1904.08386-1",
          "chunk_id": 11,
          "document_id": "1904.08386",
          "start_idx": 6428,
          "end_idx": 7215
        },
        "page_content": "Both ELMo and BERT outperform GloVe, which intuitively makes sense because the latter do not model the order or structure of the words in each description. While the purity of our methods is higher than that of a random clustering, it is still far below 1. To provide additional context to these results, we now switch to our \u201codd-one-out\u201d task and compare directly to human performance. For each triplet of cities, we identify the intruder as the city with the maximum Euclidean distance from the other two. Interestingly, crowd workers achieve only slightly higher accuracy than ELMo city representations; their interannotator agreement is also low, which indicates that close reading to analyze literary coherence between multiple texts is a difficult task, even for human annotators.",
        "type": "Document"
      },
      {
        "id": "34a7e346-21b8-4577-ac47-50e05c3055c4",
        "metadata": {
          "vector_store_key": "1904.08386-1",
          "chunk_id": 12,
          "document_id": "1904.08386",
          "start_idx": 6782,
          "end_idx": 7543
        },
        "page_content": "Interestingly, crowd workers achieve only slightly higher accuracy than ELMo city representations; their interannotator agreement is also low, which indicates that close reading to analyze literary coherence between multiple texts is a difficult task, even for human annotators. Overall, results from both computational and human approaches suggests that the author-assigned labels are not entirely arbitrary, as we can reliably recover some of the thematic groups. Our quantitative results suggest that while vector-based city representations capture some thematic similarities, there is much room for improvement. In this section, we first investigate whether the learned clusters provide evidence for any arguments put forth by literary critics on the novel.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "On which task does do model do worst?",
    "llm_answer": "\"ZUL\"\n",
    "context": [
      {
        "id": "61f37a83-a12e-476e-bbd0-2fedf87d5f9d",
        "metadata": {
          "vector_store_key": "1910.05456-4",
          "chunk_id": 24,
          "document_id": "1910.05456",
          "start_idx": 13270,
          "end_idx": 13952
        },
        "page_content": "For SPA, again HUN performs best, followed closely by ITA. While the good performance of HUN as a source language is still unexpected, ITA is closely related to SPA, which could explain the high accuracy of the final model. As for ENG, pretraining on EUS and NAV yields the worst final models \u2013 importantly, accuracy is over $15\\%$ lower than for QVH, which is also an unrelated language. This again suggests that the prefixing morphology of EUS and NAV might play a role. Lastly, for ZUL, all models perform rather poorly, with a minimum accuracy of 10.7 and 10.8 for the source languages QVH and EUS, respectively, and a maximum accuracy of 24.9 for a model pretrained on Turkish.",
        "type": "Document"
      },
      {
        "id": "8b681439-d4eb-401b-8199-4cc0d2c2d0ed",
        "metadata": {
          "vector_store_key": "1909.02764-4",
          "chunk_id": 37,
          "document_id": "1909.02764",
          "start_idx": 20568,
          "end_idx": 21193
        },
        "page_content": "This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set. These results are mostly in line with Bostan2018. Now we analyze how well the models trained in Experiment 1 perform when applied to our data set. The results are shown in column \u201cSimple\u201d in Table TABREF19. We observe a clear drop in performance, with an average of F$_1$=48 %. The best performing model is again the one trained on TEC, en par with the one trained on the Figure8 data. The model trained on ISEAR performs second best in Experiment 1, it performs worst in Experiment 2.",
        "type": "Document"
      },
      {
        "id": "5c04f951-3c42-49a8-8e5e-c7b57f013114",
        "metadata": {
          "vector_store_key": "1909.02635-5",
          "chunk_id": 43,
          "document_id": "1909.02635",
          "start_idx": 24858,
          "end_idx": 25492
        },
        "page_content": "Also, a majority of errors leading to false negatives are due to the the formation of new sub-entities which are then mentioned with other names. For example, when talking about weak acid in \u201cthe water becomes a weak acid. the water dissolves limestone\u201d the weak acid is also considered to move to the limestone. The model's performance on these challenging task cases suggests that even though it outperforms baselines, it may not be capturing deep reasoning about entities. To understand what the model actually does, we perform analysis of the model's behavior with respect to the input to understand what cues it is picking up on.",
        "type": "Document"
      },
      {
        "id": "5567ec52-3c38-41b4-b624-2bfe13262a85",
        "metadata": {
          "vector_store_key": "1703.04617-3",
          "chunk_id": 43,
          "document_id": "1703.04617",
          "start_idx": 23973,
          "end_idx": 24558
        },
        "page_content": "In Figure UID61 we can see that the average EM/F1 of the \"when\" question is highest and those of the \"why\" question is the lowest. From Figure UID62 we can see the \"what\" question is the major class. Figure 5 shows the composition of F1 score. Take our best model as an example, we observed a 78.38% F1 score on the whole development set, which can be separated into two parts: one is where F1 score equals to 100%, which means an exact match. This part accounts for 69.10% of the entire development set. And the other part accounts for 30.90%, of which the average F1 score is 30.03%.",
        "type": "Document"
      },
      {
        "id": "a2585c2d-9180-46a9-b604-79b46839bab2",
        "metadata": {
          "vector_store_key": "1603.07044-4",
          "chunk_id": 24,
          "document_id": "1603.07044",
          "start_idx": 12983,
          "end_idx": 13641
        },
        "page_content": "We set $0.8$ for the main task (task C) and $0.1$ for the other auxiliary tasks. The MAP score did not improve, but F1 increases to $0.1617$ . We believe this is because other tasks have more balanced labels, which improves the shared parameters for task C. There are many sources of external question-answer pairs that could be used in our tasks. For example: WebQuestion (was introduced by the authors of SEMPRE system BIBREF18 ) and The SimpleQuestions dataset . All of them are positive examples for our task and we can easily create negative examples from it. Initial experiments indicate that it is very easy to overfit these obvious negative examples.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "On which task does do model do best?",
    "llm_answer": "The context does not specify a task on which the model performs best.  It describes various models and analyses, but no comparison of performance across tasks is made.\n",
    "context": [
      {
        "id": "ef18c881-3bf1-4e36-94ad-61037dd58977",
        "metadata": {
          "vector_store_key": "1706.07179-0",
          "chunk_id": 14,
          "document_id": "1706.07179",
          "start_idx": 7436,
          "end_idx": 7670
        },
        "page_content": "Future work will investigate the performance of these models on more real world datasets, interpreting what the models learn, and scaling these models to answer questions about entities and relations from reading massive text corpora.",
        "type": "Document"
      },
      {
        "id": "56dc5a3f-8db3-44db-83c2-8fd2a9c4a6ad",
        "metadata": {
          "vector_store_key": "1909.02635-5",
          "chunk_id": 44,
          "document_id": "1909.02635",
          "start_idx": 25189,
          "end_idx": 25838
        },
        "page_content": "To understand what the model actually does, we perform analysis of the model's behavior with respect to the input to understand what cues it is picking up on. One way to analyze the model is to compute model gradients with respect to input features BIBREF26, BIBREF25. Figure FIGREF37 shows that in this particular example, the most important model inputs are verbs possibly associated with the entity butter, in addition to the entity's mentions themselves. It further shows that the model learns to extract shallow clues of identifying actions exerted upon only the entity being tracked, regardless of other entities, by leveraging verb semantics.",
        "type": "Document"
      },
      {
        "id": "0ab61107-67bf-493c-aa5f-199d370caaa3",
        "metadata": {
          "vector_store_key": "1703.04617-2",
          "chunk_id": 26,
          "document_id": "1703.04617",
          "start_idx": 14460,
          "end_idx": 15206
        },
        "page_content": "The previous models are often trained for all questions without explicitly discriminating different question types; however, for a target question, both the common features shared by all questions and the specific features for a specific type of question are further considered in this paper, as they could potentially obey different distributions. In this paper we further explicitly model different types of questions in the end-to-end training. We start from a simple way to first analyze the word frequency of all questions, and obtain top-10 most frequent question types: what, how, who, when, which, where, why, be, whose, and whom, in which be stands for the questions beginning with different forms of the word be such as is, am, and are.",
        "type": "Document"
      },
      {
        "id": "c339f618-d827-414a-8c1a-a2ec44007ae4",
        "metadata": {
          "vector_store_key": "1908.06606-0",
          "chunk_id": 8,
          "document_id": "1908.06606",
          "start_idx": 5104,
          "end_idx": 5876
        },
        "page_content": "Task-specific end-to-end methods BIBREF3, BIBREF4 use large amount of data to automatically model the specific task. Topaz et al. BIBREF3 constructed an automated wound information identification model with five output. Tan et al. BIBREF4 identified patients undergoing radical cystectomy for bladder cancer. Although they achieved good performance, none of their models could be used to another task due to output format difference. This makes building a new model for a new task a costly job. Pipeline methods BIBREF7, BIBREF8, BIBREF9 break down the entire task into several basic natural language processing tasks. Bill et al. BIBREF7 focused on attributes extraction which mainly relied on dependency parsing and named entity recognition BIBREF10, BIBREF11, BIBREF12.",
        "type": "Document"
      },
      {
        "id": "7f096aba-b41f-4722-9d3e-87bcdbbf13c5",
        "metadata": {
          "vector_store_key": "1909.00183-9",
          "chunk_id": 55,
          "document_id": "1909.00183",
          "start_idx": 30782,
          "end_idx": 31490
        },
        "page_content": "As a measure of performance of the classifiers and models, we use the weighted average of the F1 score for all levels of DoH, which takes into account both precision and recall, i.e., both the exactness and completeness of the model. We showcase our methodology through the analysis of the text from NRLS patient incident reports. In addition to textual descriptions, the reports are hand-coded upon reporting with up to 170 features per case, including a two-level manual classification of the incidents. Here, we only use the text component and apply our graph-based text clustering to a set of 3229 reports from St Mary's Hospital, London (Imperial College Healthcare NHS Trust) over three months in 2014.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does counterfactual data augmentation aim to tackle bias?",
    "llm_answer": "By swapping gendered words.\n",
    "context": [
      {
        "id": "420db740-8052-429f-a0c9-0a135057516c",
        "metadata": {
          "vector_store_key": "1906.04571-1",
          "chunk_id": 3,
          "document_id": "1906.04571",
          "start_idx": 1719,
          "end_idx": 2483
        },
        "page_content": "Consider the Spanish phrase el ingeniero experto (the skilled engineer). Replacing ingeniero with ingeniera is insufficient\u2014el must also be replaced with la and experto with experta. In this paper, we present a new approach to counterfactual data augmentation BIBREF10 for mitigating gender stereotypes associated with animate nouns (i.e., nouns that represent people) for morphologically rich languages. We introduce a Markov random field with an optional neural parameterization that infers the manner in which a sentence must change when altering the grammatical gender of particular nouns. We use this model as part of a four-step process, depicted in fig:pipeline, to reinflect entire sentences following an intervention on the grammatical gender of one word.",
        "type": "Document"
      },
      {
        "id": "3881bca3-c8f7-4419-a022-a99a1793213f",
        "metadata": {
          "vector_store_key": "1911.03842-2",
          "chunk_id": 5,
          "document_id": "1911.03842",
          "start_idx": 2769,
          "end_idx": 3555
        },
        "page_content": "To offset this, we collect additional in-domain personas and dialogues to balance gender and increase the diversity of personas in the dataset. Next, we combine this approach with Counterfactual Data Augmentation and methods for controllable text generation to mitigate the bias in dialogue generation. Our proposed techniques create models that produce engaging responses with less gender bias. Recent work in dialogue incorporates personas, or personality descriptions that ground speaker's chat, such as I love fishing BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF16. Personas have been shown to increase engagingness and improve consistency. However, they can be a starting point for bias BIBREF17, BIBREF18, BIBREF9, as bias in the personas propagates to subsequent conversations.",
        "type": "Document"
      },
      {
        "id": "5e836b96-dbdf-4583-b9f0-4664ecca0624",
        "metadata": {
          "vector_store_key": "1911.03842-3",
          "chunk_id": 13,
          "document_id": "1911.03842",
          "start_idx": 7315,
          "end_idx": 8274
        },
        "page_content": "We explore both data augmentation and algorithmic methods to mitigate bias in generative Transformer dialogue models. We describe first our modeling setting and then the three proposed techniques for mitigating bias. Using (i) counterfactual data augmentation BIBREF25 to swap gendered words and (ii) additional data collection with crowdworkers, we create a gender-balanced dataset. Further, (iii) we describe a controllable generation method which moderates the male and female gendered words it produces. Following BIBREF0, in all of our experiments we fine-tune a large, pre-trained Transformer encoder-decoder neural network on the dialogues in the LIGHT dataset. The model was pre-trained on Reddit conversations, using a previously existing Reddit dataset extracted and obtained by a third party and made available on pushshift.io. During pre-training, models were trained to generate a comment conditioned on the full thread leading up to the comment.",
        "type": "Document"
      },
      {
        "id": "6d547f0e-ca37-499d-9089-ca4ff9d66ce7",
        "metadata": {
          "vector_store_key": "1911.03842-3",
          "chunk_id": 32,
          "document_id": "1911.03842",
          "start_idx": 17610,
          "end_idx": 18060
        },
        "page_content": "We present techniques using data augmentation and controllable generation to reduce gender bias in neural language generation for dialogue. We use the dataset LIGHT as a testbed for this work. By integrating these methods together, our models provide control over how gendered dialogue is and decrease the offensiveness of the generated utterances. Overall, our proposed methodology reduces the effect of bias while maintaining dialogue engagingness.",
        "type": "Document"
      },
      {
        "id": "6667ae8d-b630-43a0-9e0f-07ccbf573ffe",
        "metadata": {
          "vector_store_key": "1812.06705-1",
          "chunk_id": 38,
          "document_id": "1812.06705",
          "start_idx": 21903,
          "end_idx": 22181
        },
        "page_content": "In the future, (1)We will explore how to perform text data augmentation on imbalanced datasets with pre-trained language model, (2) we believe the idea of conditional BERT contextual augmentation is universal and will be applied to paragraph or document level data augmentation.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "In the targeted data collection approach, what type of data is targetted?",
    "llm_answer": "Sentiment polarities of a sentence over \"opinion targets\" that explicitly appear in the sentence.\n",
    "context": [
      {
        "id": "4b089285-b3b9-42b5-8910-f089dc14d3d3",
        "metadata": {
          "vector_store_key": "1605.08675-9",
          "chunk_id": 31,
          "document_id": "1605.08675",
          "start_idx": 17446,
          "end_idx": 18317
        },
        "page_content": "The goal of question analysis is to examine a question and extract all the information that suffices for answer finding. A resulting data structure, called question model, contains the following elements: Question type \u2013 a description of expected answer type, instructing the system, what type of data could be returned as an answer. It has three levels of specificity: General question type \u2013 one of the types of factoid questions, enumerated at the beginning of this chapter, Named entity type \u2013 applicable only in case general type equals named entity. Possible values are the following: place, continent, river, lake, mountain, mountain range, island, archipelago, sea, celestial body, country, state, city, nationality, person, first name, last name, band, dynasty, organisation, company, event, date, century, year, period, number, quantity, vehicle, animal, title.",
        "type": "Document"
      },
      {
        "id": "0ab61107-67bf-493c-aa5f-199d370caaa3",
        "metadata": {
          "vector_store_key": "1703.04617-2",
          "chunk_id": 26,
          "document_id": "1703.04617",
          "start_idx": 14460,
          "end_idx": 15206
        },
        "page_content": "The previous models are often trained for all questions without explicitly discriminating different question types; however, for a target question, both the common features shared by all questions and the specific features for a specific type of question are further considered in this paper, as they could potentially obey different distributions. In this paper we further explicitly model different types of questions in the end-to-end training. We start from a simple way to first analyze the word frequency of all questions, and obtain top-10 most frequent question types: what, how, who, when, which, where, why, be, whose, and whom, in which be stands for the questions beginning with different forms of the word be such as is, am, and are.",
        "type": "Document"
      },
      {
        "id": "ed2b65a3-293a-4a8c-874e-949c480c257f",
        "metadata": {
          "vector_store_key": "2002.11893-4",
          "chunk_id": 13,
          "document_id": "2002.11893",
          "start_idx": 8009,
          "end_idx": 8625
        },
        "page_content": "Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context. To make workers understand the task more easily, we crafted templates to generate natural language descriptions for each structured goal. Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality.",
        "type": "Document"
      },
      {
        "id": "e49a121e-cef0-4730-8399-cabf32add1b4",
        "metadata": {
          "vector_store_key": "1902.09314-2",
          "chunk_id": 0,
          "document_id": "1902.09314",
          "start_idx": 0,
          "end_idx": 793
        },
        "page_content": "Targeted sentiment classification is a fine-grained sentiment analysis task, which aims at determining the sentiment polarities (e.g., negative, neutral, or positive) of a sentence over \u201copinion targets\u201d that explicitly appear in the sentence. For example, given a sentence \u201cI hated their service, but their food was great\u201d, the sentiment polarities for the target \u201cservice\u201d and \u201cfood\u201d are negative and positive respectively. A target is usually an entity or an entity aspect. In recent years, neural network models are designed to automatically learn useful low-dimensional representations from targets and contexts and obtain promising results BIBREF0 , BIBREF1 . However, these neural network models are still in infancy to deal with the fine-grained targeted sentiment classification task.",
        "type": "Document"
      },
      {
        "id": "b7e7e7c9-ac8d-4682-84ce-d358ef5a3286",
        "metadata": {
          "vector_store_key": "1909.00542-0",
          "chunk_id": 4,
          "document_id": "1909.00542",
          "start_idx": 2338,
          "end_idx": 3061
        },
        "page_content": "Phase A provides a biomedical question as an input, and participants are expected to find relevant concepts from designated terminologies and ontologies, relevant articles from PubMed, relevant snippets from the relevant articles, and relevant RDF triples from designated ontologies. Phase B provides a biomedical question and a list of relevant articles and snippets, and participant systems are expected to return the exact answers and the ideal answers. The training data is composed of the test data from all previous years, and amounts to 2,747 samples. There has been considerable research on the use of machine learning approaches for tasks related to text summarisation, especially on single-document summarisation.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they determine which words are informative?",
    "llm_answer": "Informative words are nouns and verbs that are matched if their raw text, lemmatizations, WordNet synsets, or aliases are the same.\n",
    "context": [
      {
        "id": "1ff5bf3c-ab7b-41b1-b097-d65f35d06d5e",
        "metadata": {
          "vector_store_key": "1808.00265-1",
          "chunk_id": 27,
          "document_id": "1808.00265",
          "start_idx": 15762,
          "end_idx": 16499
        },
        "page_content": "Informative words are all nouns and verbs, where two informative words are matched if at least one of the following conditions is met: (1) Their raw text as they appear in INLINEFORM5 or INLINEFORM6 are the same; (2) Their lemmatizations (using NLTK BIBREF22 ) are the same; (3) Their synsets in WordNet BIBREF23 are the same; (4) Their aliases (provided from VG) are the same. We refer to the resulting labels as region-level groundings. Figure FIGREF3 (a) illustrates an example of a region-level grounding. In terms of object annotations, for each image in a INLINEFORM0 triplet we select the bounding box of an object as a valid grounding label, if the object name matches one of the informative nouns in INLINEFORM1 or INLINEFORM2 .",
        "type": "Document"
      },
      {
        "id": "ab674a67-5069-4697-b94d-f23217a9a6f1",
        "metadata": {
          "vector_store_key": "1908.08419-2",
          "chunk_id": 5,
          "document_id": "1908.08419",
          "start_idx": 3007,
          "end_idx": 3728
        },
        "page_content": "Specifically, we combine information branch and gated neural network to determine if the segment is a legal word, i.e., word score. Meanwhile, we use the hidden layer output of the long short-term memory (LSTM) BIBREF11 to find out how the word is linked to its surroundings, i.e., link score. The final decision on the selection of labeling samples is made by calculating the average of word and link scores on the whole segmented sentence, i.e., sequence score. Besides, to capture coherence over characters, we additionally add K-means clustering features to the input of CRF-based word segmenter. To sum up, the main contributions of our work are summarized as follows: The rest of this paper is organized as follows.",
        "type": "Document"
      },
      {
        "id": "3424c51e-512e-4527-a993-3595feee1d4a",
        "metadata": {
          "vector_store_key": "1910.12618-6",
          "chunk_id": 28,
          "document_id": "1910.12618",
          "start_idx": 15445,
          "end_idx": 16193
        },
        "page_content": "Indeed, considering the novelty of this work, the understanding of the impact of the words on the forecast is of paramount importance, and as opposed to embeddings, TF-IDF has a natural interpretation. Furthermore the RF and LASSO methods give the possibility to interpret marginal effects and analyze the importance of features, and thus to find the words which affect the time series the most. As for the word embedding, recurrent or convolutional neural networks (respectively RNN and CNN) were used with them. MLPs are not used, for they would require to concatenate all the vector representations of a sentence together beforehand and result in a network with too many parameters to be trained correctly with our number of available documents.",
        "type": "Document"
      },
      {
        "id": "f1d2ea89-a5f9-48e1-9646-477e08134cb3",
        "metadata": {
          "vector_store_key": "1908.08419-1",
          "chunk_id": 4,
          "document_id": "1908.08419",
          "start_idx": 2278,
          "end_idx": 3007
        },
        "page_content": "Active learning methods are favored by the researchers in many natural language processing (NLP) tasks, such as text classification BIBREF9 and named entity recognition (NER) BIBREF10 . However, only a handful of works are conducted on CWS BIBREF2 , and few focuses on medical domain tasks. Given the aforementioned challenges and current researches, we propose a word segmentation method based on active learning. To model the segmentation history, we incorporate a sampling strategy consisting of word score, link score and sequence score, which effectively evaluates the segmentation decisions. Specifically, we combine information branch and gated neural network to determine if the segment is a legal word, i.e., word score.",
        "type": "Document"
      },
      {
        "id": "3cefb9d9-38cd-43b2-b236-88be0e68364c",
        "metadata": {
          "vector_store_key": "2001.07209-2",
          "chunk_id": 28,
          "document_id": "2001.07209",
          "start_idx": 15311,
          "end_idx": 16128
        },
        "page_content": "We used 1990s word embeddings, and obtained predictions for two-word topics by querying the model with their averaged embeddings. Figure FIGREF23 shows plots of relevance and polarity predictions against survey proportions, and we observe a visible correspondence between model predictions and human judgments despite the difficulty of this task and limited number of topics. Beyond analyzing selected concepts, we applied our framework predictively on a large repertoire of words to automatically discover the concepts that have exhibited the greatest change in moral sentiment at two tiers, moral relevance and moral polarity. We selected the 10,000 nouns with highest total frequency in the 1800\u20131999 period according to data from BIBREF30, restricted to words labelled as nouns in WordNet BIBREF34 for validation.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What improvement does the MOE model make over the SOTA on language modelling?",
    "llm_answer": "The MoE model achieves lower test perplexity and better BLEU scores on 11 of 12 language pairs than the multilingual GNMT model, and even beats monolingual GNMT models on 8 of 12.  It also has a lower test perplexity than the best published model by a specific amount (INLINEFORM2).\n",
    "context": [
      {
        "id": "f70832a6-4a72-4af8-a21e-2c285c6f053a",
        "metadata": {
          "vector_store_key": "1701.06538-6",
          "chunk_id": 59,
          "document_id": "1701.06538",
          "start_idx": 31843,
          "end_idx": 32485
        },
        "page_content": "For MoE-34M, the LSTM layers have 1024 units. For MoE-143M, the LSTM layers have 4096 units and an output projection of size 1024 BIBREF41 . MoE-34M uses a hierarchical MoE layer with 1024 experts, each with a hidden layer of size 2048. MoE-143M uses a hierarchical MoE layer with 256 experts, each with a hidden layer of size 8192. Both models have 4B parameters in the MoE layers. We searched for the best INLINEFORM0 for each model, and trained each model for 10 epochs. The two models achieved test perplexity of INLINEFORM0 and INLINEFORM1 respectively, showing that even in the presence of a large MoE, more computation is still useful.",
        "type": "Document"
      },
      {
        "id": "a1fbcd6d-2f5b-470f-9363-9532e53e4072",
        "metadata": {
          "vector_store_key": "1701.06538-9",
          "chunk_id": 42,
          "document_id": "1701.06538",
          "start_idx": 22788,
          "end_idx": 23507
        },
        "page_content": "Our training time was shorter due to the lower computational budget of our model. Results for the single-pair GNMT models, the multilingual GNMT model and the multilingual MoE model are given in Table TABREF50 . The MoE model achieves 19% lower perplexity on the dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English INLINEFORM0 Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number of real examples were highly oversampled in the training corpus.",
        "type": "Document"
      },
      {
        "id": "65b13eaa-8842-4674-bace-5cc835bbc84d",
        "metadata": {
          "vector_store_key": "1701.06538-9",
          "chunk_id": 60,
          "document_id": "1701.06538",
          "start_idx": 32272,
          "end_idx": 33014
        },
        "page_content": "The two models achieved test perplexity of INLINEFORM0 and INLINEFORM1 respectively, showing that even in the presence of a large MoE, more computation is still useful. Results are reported at the bottom of Table TABREF76 . The larger of the two models has a similar computational budget to the best published model from the literature, and training times are similar. Comparing after 10 epochs, our model has a lower test perplexity by INLINEFORM2 . The models are similar in structure to the 8-million-operations-per-timestep models described in the previous section. We vary the number of experts between models, using an ordinary MoE layer with 32 experts and hierarchical MoE layers with 256, 1024, 4096, 16384, 65536 and 131072 experts.",
        "type": "Document"
      },
      {
        "id": "4a40510b-7440-4b61-9d80-e6a084f59fa0",
        "metadata": {
          "vector_store_key": "1701.06538-6",
          "chunk_id": 58,
          "document_id": "1701.06538",
          "start_idx": 31215,
          "end_idx": 31843
        },
        "page_content": "For each model, we report the test perplexity, the computational budget, the parameter counts, the value of INLINEFORM0 , and the computational efficiency. We ran two additional models (MoE-34M and MoE-143M) to investigate the effects of adding more computation in the presence of a large MoE layer. These models have computation budgets of 34M and 143M ops/timestep. Similar to the models above, these models use a MoE layer between two LSTM layers. The dimensionality of the embedding layer, and the input and output dimensionality of the MoE layer are set to 1024 instead of 512. For MoE-34M, the LSTM layers have 1024 units.",
        "type": "Document"
      },
      {
        "id": "b14bd41f-b36a-4b9b-abf0-8de45a655fd4",
        "metadata": {
          "vector_store_key": "1701.06538-6",
          "chunk_id": 69,
          "document_id": "1701.06538",
          "start_idx": 37393,
          "end_idx": 38073
        },
        "page_content": "This doubles the amount of computation in the MoE layers, raising the computational budget of the entire model from 85M to 102M ops/timestep. We trained our networks using the Adam optimizer BIBREF39 . The base learning rate was increased linearly for the first 2000 training steps, held constant for an additional 8000 steps, and decreased after that so as to be proportional to the inverse square root of the step number. For the single-language-pair models, similarly to BIBREF3 , we applied dropout BIBREF43 to the output of all embedding, LSTM and MoE layers, using INLINEFORM0 . Training was done synchronously on a cluster of up to 64 GPUs as described in section SECREF3 .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the difference in performance between the interpretable system (e.g. vectors and cosine distance) and LSTM with ELMo system?",
    "llm_answer": "The context states that if interpretability and explainability are required, a system combining vector distance and edit distance might be better than the ELMo-based LSTM system.  It does not quantify the difference in performance.\n",
    "context": [
      {
        "id": "1f91c375-2f5f-492c-a2ef-292f591fc2ee",
        "metadata": {
          "vector_store_key": "1909.02635-2",
          "chunk_id": 27,
          "document_id": "1909.02635",
          "start_idx": 15978,
          "end_idx": 16754
        },
        "page_content": "Interestingly, the ELMo-based model under-performs the first-occurrence baseline, indicating that the LSTM model is not learning much in terms of recognizing complex entity semantics grounded in long term contexts. Comparing the four variants of structuring input in proposed architectures as discussed in Section SECREF4, we observe that the document-level, entity-first model is the best performing variant. Given the left-to-right unidirectional transformer architecture, this model notably forms target-specific representations for all process tokens, compared to using the transformer self-attention only to extract entity specific information at the end of the process. We perform ablations to evaluate the model's dependency on the context and on the target ingredient.",
        "type": "Document"
      },
      {
        "id": "c7e982e9-d89b-45c0-a9be-09d47027a1df",
        "metadata": {
          "vector_store_key": "1809.09795-1",
          "chunk_id": 17,
          "document_id": "1809.09795",
          "start_idx": 9558,
          "end_idx": 10175
        },
        "page_content": "We also experimented using a slanted triangular learning rate scheme, which was shown by BIBREF27 to deliver excellent results on several tasks, but in practice we did not obtain significant differences. We experimented with batch sizes of 16, 32 and 64, and dropouts ranging from 0.1 to 0.5. The size of the LSTM hidden layer was fixed to 1,024, based on our preliminary experiments. We do not train the ELMo embeddings, but allow their dropouts to be active during training. Table TABREF2 summarizes our results. For each dataset, the top row denotes our baseline and the second row shows our best comparable model.",
        "type": "Document"
      },
      {
        "id": "496e7041-9ab2-4956-9f1d-834ab5350187",
        "metadata": {
          "vector_store_key": "1905.10810-1",
          "chunk_id": 24,
          "document_id": "1905.10810",
          "start_idx": 13588,
          "end_idx": 14310
        },
        "page_content": "Among the methods tested the bidirectional LSTM, especially initialized by ELMo embeddings, offers the best accuracy and raw performance. Adding ELMo to a straightforward PyTorch implementation of LSTM may be easier now than at the time of performing our tests, as since then the authors of ELMoForManyLangs package BIBREF19 improved their programmatic interface. However, if a more interpretable and explainable output is required, some version of vector distance combined with edit distance may be the best direction. It should be noted that this method produces multiple candidate corrections with their similarity scores, as opposed to only one \u201cbest guess\u201c correction that can be obtained from a character-based LSTM.",
        "type": "Document"
      },
      {
        "id": "0b441b21-296d-49c9-83aa-8c2657bb99d1",
        "metadata": {
          "vector_store_key": "1809.09795-1",
          "chunk_id": 10,
          "document_id": "1809.09795",
          "start_idx": 5604,
          "end_idx": 6342
        },
        "page_content": "Using CNNs, each vector is built upon the characters that compose the underlying words. As ELMo also contains a deep bi-directional LSTM on top of this character-derived vectors, each word-level embedding contains contextual information from their surroundings. Concretely, we use a pre-trained ELMo model, obtained using the 1 Billion Word Benchmark which contains about 800M tokens of news crawl data from WMT 2011 BIBREF24 . Subsequently, the contextualized embeddings are passed on to a BiLSTM with 2,048 hidden units. We aggregate the LSTM hidden states using max-pooling, which in our preliminary experiments offered us better results, and feed the resulting vector to a 2-layer feed-forward network, where each layer has 512 units.",
        "type": "Document"
      },
      {
        "id": "fe0ae5ec-9d3c-41df-8493-198daf806818",
        "metadata": {
          "vector_store_key": "1909.02635-2",
          "chunk_id": 26,
          "document_id": "1909.02635",
          "start_idx": 15255,
          "end_idx": 15978
        },
        "page_content": "Comparing to the baselines (Majority through First) and post-conditioned models, we see that the early entity conditioning is critical to achieve high performance. Although the First model still achieves the highest CR, due to operating in a high-recall regime, we see that the ET$_{GPT}$ models all significantly outperform the post-conditioning models on this metric, indicating better modeling of these compositions. Both recall and precision are substantially increaesd compared to these baseline models. Interestingly, the ELMo-based model under-performs the first-occurrence baseline, indicating that the LSTM model is not learning much in terms of recognizing complex entity semantics grounded in long term contexts.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which language-pair had the better performance?",
    "llm_answer": "English-French\n",
    "context": [
      {
        "id": "eb2f2f8b-9bc1-4ba6-ac71-4681cb392670",
        "metadata": {
          "vector_store_key": "1911.00069-1",
          "chunk_id": 40,
          "document_id": "1911.00069",
          "start_idx": 23170,
          "end_idx": 23915
        },
        "page_content": "In the tables, we also provide the performance of the supervised RE model (Bi-LSTM) for each target language, which is trained with a few hundred thousand tokens of manually annotated RE data in the target-language, and may serve as an upper bound for the cross-lingual model transfer performance. Among the 2 neural network models, the Bi-LSTM model achieves a better cross-lingual RE performance than the CNN model for 6 out of the 7 target languages. In terms of absolute performance, the Bi-LSTM model achieves over $40.0$ $F_1$ scores for German, Spanish, Portuguese and Chinese. In terms of relative performance, it reaches over $75\\%$ of the accuracy of the supervised target-language RE model for German, Spanish, Italian and Portuguese.",
        "type": "Document"
      },
      {
        "id": "9545e7c5-f618-4e8b-bb92-d6862910d3a8",
        "metadata": {
          "vector_store_key": "2004.01694-7",
          "chunk_id": 43,
          "document_id": "2004.01694",
          "start_idx": 24807,
          "end_idx": 25697
        },
        "page_content": "Finally, while there was not a significant difference, likely due to the small number of examples overall, it is noticeable that MT$_1$ had a higher percentage of Collocation and Context errors, which indicate that the system has more trouble translating words that are dependent on longer-range context. Similarly, some Named Entity errors are also attributable to translation inconsistencies due to lack of longer-range context. Table TABREF34 shows an example where we see that the MT system was unable to maintain a consistently gendered or correct pronoun for the female Olympic shooter Zhang Binbin (\u5f20\u5f6c\u5f6c). Apart from showing qualitative differences between the three translations, the analysis also supports the finding of the pairwise ranking study: H$_A$ is both preferred over MT$_1$ in the pairwise ranking study, and exhibits fewer translation errors in our error classification.",
        "type": "Document"
      },
      {
        "id": "d2e2382c-8e5a-4f0c-92c5-2f119e6e1178",
        "metadata": {
          "vector_store_key": "1910.07481-0",
          "chunk_id": 20,
          "document_id": "1910.07481",
          "start_idx": 11787,
          "end_idx": 12518
        },
        "page_content": "It also surpassed the Baselinee by 0.18 points on the newstest2017 with strong statistical significance, and by 0.15 BLEU points on the newstest2018 but this time with no statistical evidence. These encouraging results prompted us to extend experiments to another language pair: English-French. En$\\rightarrow $Fr: The Document system obtained the best results considering all metrics on all test sets with strong statistical evidence. It surpassed the Baseline by 1.09 BLEU points and 0.85 TER points on tst2015, 0.75 BLEU points and 0.76 TER points on tst2014, and 0.48 BLEU points and 0.68 TER points on tst2013. Fr$\\rightarrow $En: Of all experiments, this language pair shows the most important improvements over the Baseline.",
        "type": "Document"
      },
      {
        "id": "08f96819-f760-4592-92f6-f7ab790500d2",
        "metadata": {
          "vector_store_key": "2004.01694-7",
          "chunk_id": 44,
          "document_id": "2004.01694",
          "start_idx": 25115,
          "end_idx": 25890
        },
        "page_content": "Apart from showing qualitative differences between the three translations, the analysis also supports the finding of the pairwise ranking study: H$_A$ is both preferred over MT$_1$ in the pairwise ranking study, and exhibits fewer translation errors in our error classification. H$_B$ has a substantially higher number of missing words than the other two translations, which agrees with the lower perceived adequacy in the pairwise ranking. However, the analysis not only supports the findings of the pairwise ranking study, but also adds nuance to it. Even though H$_B$ has the highest number of deletions, and does worse than the other two translations in a pairwise adequacy ranking, it is similar to H$_A$, and better than MT$_1$, in terms of most other error categories.",
        "type": "Document"
      },
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which psycholinguistic and basic linguistic features are used?",
    "llm_answer": "Emoticons, punctuation, modifiable inflectional features, fixed lexical features, part-of-speech (POS), POS ratios, discourse connectors, spelling errors, repeated characters, and sentiment-based features.\n",
    "context": [
      {
        "id": "0b88551e-4a78-42be-86ef-39b1e5a9896e",
        "metadata": {
          "vector_store_key": "2001.05493-2",
          "chunk_id": 7,
          "document_id": "2001.05493",
          "start_idx": 4448,
          "end_idx": 5330
        },
        "page_content": "Exploiting psycho-linguistic features with basic linguistic features as meta-data. The main aim is to minimize the direct dependencies on in-depth grammatical structure of the language (i.e., to support code-mixed data). We have also included emoticons, and punctuation features with it. We use the term \"NLP Features\" to represent it in the entire paper. Dual embedding based on FastText and Glove. This dual embedding helps in high vocabulary coverage and to capture the rare and partially incorrect words in the text (specially by FastText BIBREF20). Our \"Deep-text architecture\" uses model averaging strategy with three different deep learning architectures. Model averaging belongs to the family of ensemble learning techniques that uses multiple models for the same problem and combines their predictions to produce a more reliable and consistent prediction accuracy BIBREF21.",
        "type": "Document"
      },
      {
        "id": "feac3007-6c08-4c0f-9eb9-99b8e1bb0bd1",
        "metadata": {
          "vector_store_key": "1810.06743-1",
          "chunk_id": 6,
          "document_id": "1810.06743",
          "start_idx": 3477,
          "end_idx": 4086
        },
        "page_content": "Thus, to prepare for later discussion, we divide the morphological features of a word into two categories: the modifiable inflectional features and the fixed lexical features. A part of speech (POS) is a coarse syntactic category (like verb) that begets a word's particular menu of lexical and inflectional features. In English, verbs express no gender, and adjectives do not reflect person or number. The part of speech dictates a set of inflectional slots to be filled by the surface forms. Completing these slots for a given lemma and part of speech gives a paradigm: a mapping from slots to surface forms.",
        "type": "Document"
      },
      {
        "id": "b3924254-497e-47f3-bd41-1a624b234c0c",
        "metadata": {
          "vector_store_key": "2001.05493-1",
          "chunk_id": 17,
          "document_id": "2001.05493",
          "start_idx": 10541,
          "end_idx": 11308
        },
        "page_content": "Some approaches like:BIBREF12 has used emotions frequency as one of the features, while some others use sentiment emotion as featureBIBREF11. Also,BIBREF17, BIBREF19 have converted emoticons to their description. BIBREF9 have used TF-IDF of emoticons per-class as one of the features. Compared to all these approaches, we have concentrated to capture multiple linguistic/pattern based relations, key-terms and key-patters (with their association in text) through a combination of deep learning architectures with model averaging. We have also used NLP features as additional features with our deep learning architecture, obtained from psycho-linguistic and basic linguistic features. In this section, we describe our system architecture for aggressiveness classifier.",
        "type": "Document"
      },
      {
        "id": "82ab2606-8943-430b-be00-b232375b5200",
        "metadata": {
          "vector_store_key": "1610.00879-1",
          "chunk_id": 8,
          "document_id": "1610.00879",
          "start_idx": 4769,
          "end_idx": 5396
        },
        "page_content": "POS ratios are a set of features that record the proportion of each POS tag in the dataset (for example, the proportion of nouns/adjectives, etc.). The POS tags and named entity mentions are obtained from NLTK BIBREF9 . Discourse connectors are identified based on a manually created list. Spelling errors are identified using a spell checker by enchant. The repeated characters feature captures a situation in which a word contains a letter that is repeated three or more times, as in the case of happpy. Since drunk-texting is often associated with emotional expression, we also incorporate a set of sentiment-based features.",
        "type": "Document"
      },
      {
        "id": "32885c8a-31a5-44be-93ba-8d5e25b62c8a",
        "metadata": {
          "vector_store_key": "1910.04269-2",
          "chunk_id": 4,
          "document_id": "1910.04269",
          "start_idx": 2318,
          "end_idx": 2987
        },
        "page_content": "Our 1D-ConvNet architecture auto-extracts and classifies features from this raw audio input. The structure of the work is as follows. In Section 2 we discuss about the previous related studies in this field. The model architecture for both the raw waveforms and log-Mel spectrogram images is discussed in Section 3 along with the a discussion on hyperparameter space exploration. In Section 4 we present the experimental results. Finally, in Section 5 we discuss the conclusions drawn from the experiment and future work. Extraction of language dependent features like prosody and phonemes was a popular approach to classify spoken languages BIBREF8, BIBREF9, BIBREF10.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How have the differences in communication styles between Twitter and Facebook increase the complexity of the problem?",
    "llm_answer": "The context does not contain information about Facebook communication styles, so the answer is not found in the context.\n",
    "context": [
      {
        "id": "18b52094-a986-4981-81e5-40f449faef71",
        "metadata": {
          "vector_store_key": "1710.01492-0",
          "chunk_id": 5,
          "document_id": "1710.01492",
          "start_idx": 2968,
          "end_idx": 3584
        },
        "page_content": "In addition to the genre difference, there is also a difference in length: social media messages are generally short, often length-limited by design as in Twitter, i.e., a sentence or a headline rather than a full document. How to handle such challenges has only recently been the subject of thorough research BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 . Sentiment analysis has a wide number of applications in areas such as market research, political and social sciences, and for studying public opinion in general, and Twitter is one of the most commonly-used platforms for this.",
        "type": "Document"
      },
      {
        "id": "a790080e-89b5-4939-a556-c3893438a663",
        "metadata": {
          "vector_store_key": "1710.01492-0",
          "chunk_id": 4,
          "document_id": "1710.01492",
          "start_idx": 2206,
          "end_idx": 2968
        },
        "page_content": "Despite all these opportunities, the rise of social media has also presented new challenges for natural language processing (NLP) applications, which had largely relied on NLP tools tuned for formal text genres such as newswire, and thus were not readily applicable to the informal language and style of social media. That language proved to be quite challenging with its use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and #hashtags. In addition to the genre difference, there is also a difference in length: social media messages are generally short, often length-limited by design as in Twitter, i.e., a sentence or a headline rather than a full document.",
        "type": "Document"
      },
      {
        "id": "dacb2b0f-4703-4192-af93-a0fb095339f9",
        "metadata": {
          "vector_store_key": "2001.06888-2",
          "chunk_id": 1,
          "document_id": "2001.06888",
          "start_idx": 699,
          "end_idx": 1425
        },
        "page_content": "According to studies, news in twitter is propagated and reported faster than conventional news media BIBREF1. Thus, extracting first hand news and entities occurring in this fast and versatile online media gives valuable information. However, abridged and noisy content of Tweets makes it even more difficult and challenging for tasks such as named entity recognition and information retrieval BIBREF2. The task of tracking and recovering information from social media posts is a concise definition of information retrieval in social media BIBREF3, BIBREF4. However many challenges are blocking useful solutions to this issue, namely, the noisy nature of user generated content and the perplexity of words used in short posts.",
        "type": "Document"
      },
      {
        "id": "d6234e5e-cba1-4cd1-9f22-1f15d3220e47",
        "metadata": {
          "vector_store_key": "2001.00137-1",
          "chunk_id": 1,
          "document_id": "2001.00137",
          "start_idx": 799,
          "end_idx": 1576
        },
        "page_content": "This has been aggravated with the advent of internet and social networks, which allowed language and modern communication to be been rapidly transformed BIBREF1, BIBREF2. Take Twitter for instance, where information is expected to be readily communicated in short and concise sentences with little to no regard to correct sentence grammar or word spelling BIBREF3. Further motivation can be found in Automatic Speech Recognition (ASR) applications, where high error rates prevail and pose an enormous hurdle in the broad adoption of speech technology by users worldwide BIBREF4. This is an important issue to tackle because, in addition to more widespread user adoption, improving Speech-to-Text (STT) accuracy diminishes error propagation to modules using the recognized text.",
        "type": "Document"
      },
      {
        "id": "74d3ef9c-1e9c-450c-8107-b5ac379b6c2b",
        "metadata": {
          "vector_store_key": "1911.12569-4",
          "chunk_id": 0,
          "document_id": "1911.12569",
          "start_idx": 0,
          "end_idx": 730
        },
        "page_content": "The emergence of social media sites with limited character constraint has ushered in a new style of communication. Twitter users within 280 characters per tweet share meaningful and informative messages. These short messages have a powerful impact on how we perceive and interact with other human beings. Their compact nature allows them to be transmitted efficiently and assimilated easily. These short messages can shape people's thought and opinion. This makes them an interesting and important area of study. Tweets are not only important for an individual but also for the companies, political parties or any organization. Companies can use tweets to gauge the performance of their products and predict market trends BIBREF0.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What data/studies do the authors provide to support the assertion that the majority of aggressive conversations contain code-mixed languages?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "982bd649-3390-40e6-b89f-3556c7e1a9ec",
        "metadata": {
          "vector_store_key": "2001.05493-4",
          "chunk_id": 2,
          "document_id": "2001.05493",
          "start_idx": 1351,
          "end_idx": 2136
        },
        "page_content": "Non-Aggressive(NAG) - Generally these type of text lack any kind of aggression it is basically used to state facts, wishing on occasions and polite and supportive. The additional discussion on aggressiveness task can be found in Kaggle task , which just divided the task into two classes - i.e., presence or absence of aggression in tweets. The informal setting/environment of social media often encourage multilingual speakers to switch back and forth between languages when speaking or writing. These all resulted in code-mixing and code-switching. Code-mixing refers to the use of linguistic units from different languages in a single utterance or sentence, whereas code-switching refers to the co-occurrence of speech extracts belonging to two different grammatical systemsBIBREF3.",
        "type": "Document"
      },
      {
        "id": "da84a1f6-ee94-4ce6-b6ca-2e9684a711fa",
        "metadata": {
          "vector_store_key": "2001.05493-4",
          "chunk_id": 3,
          "document_id": "2001.05493",
          "start_idx": 2136,
          "end_idx": 2870
        },
        "page_content": "Code-mixing refers to the use of linguistic units from different languages in a single utterance or sentence, whereas code-switching refers to the co-occurrence of speech extracts belonging to two different grammatical systemsBIBREF3. This language interchange makes the grammar more complex and thus it becomes tough to handle it by traditional algorithms. Thus the presence of high percentage of code-mixed content in social media text has increased the complexity of the aggression detection task. For example, the dataset provided by the organizers of TRAC-2018 BIBREF0, BIBREF2 is actually a code-mixed dataset. The massive increase of the social media data rendered the manual methods of content moderation difficult and costly.",
        "type": "Document"
      },
      {
        "id": "14ee825f-d162-481b-beac-84ca6e96b11e",
        "metadata": {
          "vector_store_key": "2001.05493-1",
          "chunk_id": 6,
          "document_id": "2001.05493",
          "start_idx": 3737,
          "end_idx": 4448
        },
        "page_content": "Most of the above-discussed systems either shows high performance on (a) Twitter dataset or (b) Facebook dataset (given in the TRAC-2018), but not on both English code-mixed datasets. This may be due to the text style or level of complexities of both datasets. So, we concentrated to develop a robust system for English code-mixed texts, and uni-lingual texts, which can also handle different writing styles. Our approach is based on three main ideas: Deep-Text Learning. The goal is to learn long range associations, dependencies between regions of text, N-grams, key-patterns, topical information, and sequential dependencies. Exploiting psycho-linguistic features with basic linguistic features as meta-data.",
        "type": "Document"
      },
      {
        "id": "461772b1-783a-4117-a429-0e50269bca6c",
        "metadata": {
          "vector_store_key": "1912.13109-4",
          "chunk_id": 4,
          "document_id": "1912.13109",
          "start_idx": 2085,
          "end_idx": 2733
        },
        "page_content": "The use of Hindi and English in a code switched setting is highly influenced by these language. No grammar rules: Hinglish has no fixed set of grammar rules. The rules are inspired from both Hindi and English and when mixed with slur and slang produce large variation. Spelling variation: There is no agreement on the spellings of the words which are mixed with English. For example to express love, a code mixed spelling, specially when used social platforms might be pyaar, pyar or pyr. Dataset: Based on some earlier work, only available labelled dataset had 3189 rows of text messages of average length of 116 words and with a range of 1, 1295.",
        "type": "Document"
      },
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What datasets are used to evaluate the model?",
    "llm_answer": "AIDA-B and other datasets.\n",
    "context": [
      {
        "id": "458b51fc-c0d4-482d-90b3-d4cefed66ac3",
        "metadata": {
          "vector_store_key": "1902.00330-3",
          "chunk_id": 37,
          "document_id": "1902.00330",
          "start_idx": 20853,
          "end_idx": 21565
        },
        "page_content": "We implement our models in Tensorflow and run experiments on 4 Tesla V100 GPU. We conduct experiments on several different types of public datasets including news and encyclopedia corpus. The training set is AIDA-Train and Wikipedia datasets, where AIDA-Train contains 18448 mentions and Wikipedia contains 25995 mentions. In order to compare with the previous methods, we evaluate our model on AIDA-B and other datasets. These datasets are well-known and have been used for the evaluation of most entity linking systems. The statistics of the datasets are shown in Table 1. AIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.",
        "type": "Document"
      },
      {
        "id": "951de0ef-328c-4f84-83e7-f3d72ab2203d",
        "metadata": {
          "vector_store_key": "1909.00154-1",
          "chunk_id": 38,
          "document_id": "1909.00154",
          "start_idx": 20824,
          "end_idx": 21499
        },
        "page_content": "Encode choice models for train, development and test sets using the learned embeddings Estimate choice model accordingly using its train set Evaluate the new model using the test set Since there is stochasticity in the embeddings training model, we will repeat the above multiple times, for the different experiments in the paper (and report the respective mean and standard deviation statistics). Whenever we want to analyse a particular model (e.g. to check the coefficients of a choice model), we select the one with the highest likelihood at the development set (i.e. in practice, its out-of-sample generalization performance), and report its performance on the test set.",
        "type": "Document"
      },
      {
        "id": "d2a89e07-a3cc-434c-bec7-480f4902d920",
        "metadata": {
          "vector_store_key": "1909.00154-1",
          "chunk_id": 45,
          "document_id": "1909.00154",
          "start_idx": 24722,
          "end_idx": 25549
        },
        "page_content": "We split the dataset into 3 different parts: Embeddings train set: 60% of the dataset (6373 vectors) Development set: 20% of the dataset (2003 vectors) Test set: 20% of the dataset (2003 vectors) The PyLogit package BIBREF11 also uses Swissmetro as an example. Therefore, our model specifications will extend the default one from this package. We re-estimated this model with the train set and validated with testset. The results are shown in tables TABREF31 and TABREF32. Since we are comparing the models at the test set, the key indicators should be pseudo R-square and log-likelihood. Indicators that consider model complexity (robust r-square and AIC) are less important on the test set in our view because the overfitting effect (i.e. improving fit just by adding more variables) will no longer be verifiable in this way.",
        "type": "Document"
      },
      {
        "id": "e630aac0-599c-46aa-86a1-cd0ed38949d6",
        "metadata": {
          "vector_store_key": "1908.07816-1",
          "chunk_id": 34,
          "document_id": "1908.07816",
          "start_idx": 19456,
          "end_idx": 20127
        },
        "page_content": "Specifically, we randomly shuffled the 100 dialogs in the test set, then we used the first three utterances of each dialog as the input to the three models being compared and let them generate the responses. According to the context given, the raters were instructed to evaluate the quality of the responses based on three criteria: (1) grammatical correctness\u2014whether or not the response is fluent and free of grammatical mistakes; (2) contextual coherence\u2014whether or not the response is context sensitive to the previous dialog history; (3) emotional appropriateness\u2014whether or not the response conveys the right emotion and feels as if it had been produced by a human.",
        "type": "Document"
      },
      {
        "id": "947a5cf6-1656-4ca3-827b-105c8344c1d6",
        "metadata": {
          "vector_store_key": "1910.11949-3",
          "chunk_id": 16,
          "document_id": "1910.11949",
          "start_idx": 8700,
          "end_idx": 9318
        },
        "page_content": "In particular, we use two types of datasets to train our models: A dataset that maps pictures with questions, and an open-domain conversation dataset. The details of the two datasets are as follows. We use MS COCO, Bing and Flickr datasets from BIBREF26 to train the model that generates questions. These datasets contain natural questions about images with the purpose of knowing more about the picture. As can be seen in the Figure FIGREF8, questions cannot be answered by only looking at the image. Each source contains 5,000 images with 5 questions per image, adding a total of 15,000 images with 75,000 questions.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What baseline models do they compare against?",
    "llm_answer": "BIBREF11, BIBREF14, BIBREF21, and NABERT+\n",
    "context": [
      {
        "id": "dcbb3e93-d5ad-43bf-80ac-ec1ae660eda5",
        "metadata": {
          "vector_store_key": "1812.07023-3",
          "chunk_id": 31,
          "document_id": "1812.07023",
          "start_idx": 17294,
          "end_idx": 18075
        },
        "page_content": "Their approach, however, is not public as of yet. We observe the following for our models: Since the official test set has not been released publicly, results reported on the official test set have been provided by the challenge organizers. For the prototype test set and for the ablation study presented in Table TABREF24 , we use the same code for evaluation metrics as used by BIBREF11 for fairness and comparability. We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below: Our primary architectural differences over the baseline model are: not concatenating the question, answer pairs before encoding them, the auxiliary decoder module, and using the Time-Extended FiLM module for feature extraction.",
        "type": "Document"
      },
      {
        "id": "d0d327ba-3d19-42ba-901e-e49b07f97533",
        "metadata": {
          "vector_store_key": "1809.09795-4",
          "chunk_id": 19,
          "document_id": "1809.09795",
          "start_idx": 10585,
          "end_idx": 11463
        },
        "page_content": "As the winning system is a voting-based ensemble of 10 models, for comparison, we report results using an equivalent setting. For the Riloff, Pt\u00e1\u010dek, SC-V1 and SC-V2 datasets, our baseline models are taken directly from BIBREF14 . As their pre-processing includes truncating sentence lengths at 40 and 80 tokens for the Twitter and Dialog datasets respectively, while always removing examples with less than 5 tokens, we replicate those steps and report our results under these settings. Finally, for the Reddit datasets, our baselines are taken from BIBREF21 . Although their models are trained for binary classification, instead of reporting the performance in terms of standard classification evaluation metrics, their proposed evaluation task is predicting which of two given statements that share the same context is sarcastic, with performance measured solely by accuracy.",
        "type": "Document"
      },
      {
        "id": "14c77096-531d-4d71-9236-61c2ccdb8cfa",
        "metadata": {
          "vector_store_key": "1910.00458-6",
          "chunk_id": 44,
          "document_id": "1910.00458",
          "start_idx": 24199,
          "end_idx": 24865
        },
        "page_content": "Compared with the baselines, MMM leads to improvements ranging from 0.5% to 3.0% in accuracy. Our best result is obtained by the RoBERTa-Large encoder. In order to investigate how well our model performs for different types of questions, we did an error analysis by first randomly selecting 150 samples that had wrong predictions by the BERT-Base baseline model from the development set of DREAM dataset. We then manually classified them into several question types, as shown in Table TABREF34. The annotation criterion is described in the Section 3 of the Supplementary Material. We see that the BERT-Base baseline model still does not do well on matching problems.",
        "type": "Document"
      },
      {
        "id": "36dcf150-24e8-459c-a8c0-aa8fe1c0018f",
        "metadata": {
          "vector_store_key": "1909.02764-4",
          "chunk_id": 36,
          "document_id": "1909.02764",
          "start_idx": 19976,
          "end_idx": 20568
        },
        "page_content": "We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with 40 % of the data from the same domain (results shown in the column \u201cIn-Domain\u201d in Table TABREF19). Excluding AMMER, we achieve an average micro $\\text{F}_1$ of 68 %, with best results of F$_1$=73 % on TEC. The model trained on our AMMER corpus achieves an F1 score of 57%. This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set.",
        "type": "Document"
      },
      {
        "id": "1b1f9986-e4da-44a7-a6cd-13931a730da6",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 38,
          "document_id": "1909.13375",
          "start_idx": 19979,
          "end_idx": 20678
        },
        "page_content": "For the large models we used a batch size of 3 with a learning rate of $5e^{-6}$ and trained for 5 epochs, except for the model without the single-span heads that was trained with a batch size of 2 for 7 epochs. F1 was used as our validation metric. All models were trained on a single GPU with 12-16GB of memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the differences with previous applications of neural networks for this task?",
    "llm_answer": "The context describes newer applications using sequence-to-sequence transformer models to avoid overfitting and capture distant character information, subword information integrated with Lattice LSTM, and gated recursive neural networks with reset and update gates to handle complex character combinations.  Previous methods, like those using LSTM to capture previous important information or deep layers to learn character representations, are mentioned but not contrasted in detail.\n",
    "context": [
      {
        "id": "24e8685b-5155-4811-8b7e-daf4752f15cf",
        "metadata": {
          "vector_store_key": "1908.08419-1",
          "chunk_id": 8,
          "document_id": "1908.08419",
          "start_idx": 4695,
          "end_idx": 5419
        },
        "page_content": "Recently, neural networks have been applied in CWS tasks. To name a few, Zheng et al. BIBREF14 used deep layers of neural networks to learn feature representations of characters. Chen et al. BIBREF6 adopted LSTM to capture the previous important information. Chen et al. BIBREF18 proposed a gated recursive neural network (GRNN), which contains reset and update gates to incorporate the complicated combinations of characters. Jiang and Tang BIBREF19 proposed a sequence-to-sequence transformer model to avoid overfitting and capture character information at the distant site of a sentence. Yang et al. BIBREF20 investigated subword information for CWS and integrated subword embeddings into a Lattice LSTM (LaLSTM) network.",
        "type": "Document"
      },
      {
        "id": "3d39f4b0-edc5-4a42-9d37-50ba49c8dca7",
        "metadata": {
          "vector_store_key": "1810.09774-4",
          "chunk_id": 0,
          "document_id": "1810.09774",
          "start_idx": 0,
          "end_idx": 656
        },
        "page_content": "Natural Language Inference (NLI) has attracted considerable interest in the NLP community and, recently, a large number of neural network-based systems have been proposed to deal with the task. One can attempt a rough categorization of these systems into: a) sentence encoding systems, and b) other neural network systems. Both of them have been very successful, with the state of the art on the SNLI and MultiNLI datasets being 90.4%, which is our baseline with BERT BIBREF0 , and 86.7% BIBREF0 respectively. However, a big question with respect to these systems is their ability to generalize outside the specific datasets they are trained and tested on.",
        "type": "Document"
      },
      {
        "id": "26b8a209-cebb-4537-922e-7b34529def1a",
        "metadata": {
          "vector_store_key": "2001.00137-1",
          "chunk_id": 2,
          "document_id": "2001.00137",
          "start_idx": 1208,
          "end_idx": 2001
        },
        "page_content": "This is an important issue to tackle because, in addition to more widespread user adoption, improving Speech-to-Text (STT) accuracy diminishes error propagation to modules using the recognized text. With that in mind, in order for current systems to improve the quality of their services, there is a need for development of robust intelligent systems that are able to understand a user even when faced with incomplete representation in language. The advancement of deep neural networks have immensely aided in the development of the Natural Language Processing (NLP) domain. Tasks such as text generation, sentence correction, image captioning and text classification, have been possible via models such as Convolutional Neural Networks and Recurrent Neural Networks BIBREF5, BIBREF6, BIBREF7.",
        "type": "Document"
      },
      {
        "id": "df6b5c30-4ff1-431d-840e-26336afa5261",
        "metadata": {
          "vector_store_key": "1909.00154-4",
          "chunk_id": 3,
          "document_id": "1909.00154",
          "start_idx": 1906,
          "end_idx": 2623
        },
        "page_content": "The former relied on simple representations, such as vector frequencies, or dummy variables, to become practical, while the latter used domain knowledge such as grammars or logic. Until recently, neither had considerable success in making machines able to understand or generate human language, but developments in deep neural networks together with overwhelmingly massive amounts of data (i.e. the World Wide Web) brought them to a new area, where the two are approaching each other and achieving hitherto results considered extremely hard, such as question answering, translation, next word prediction. One of the key concepts in this revolution is that of embeddings, which will be further explained in this paper.",
        "type": "Document"
      },
      {
        "id": "7b22bd33-1dea-4038-b206-45cec0eb3454",
        "metadata": {
          "vector_store_key": "1701.00185-6",
          "chunk_id": 14,
          "document_id": "1701.00185",
          "start_idx": 7507,
          "end_idx": 8410
        },
        "page_content": "However, RecNN exhibits high time complexity to construct the textual tree, and RNN, using the layer computed at the last word to represent the text, is a biased model. Recently, Long Short-Term Memory (LSTM) BIBREF26 and Gated Recurrent Unit (GRU) BIBREF27 , as sophisticated recurrent hidden units of RNN, has presented its advantages in many sequence generation problem, such as machine translation BIBREF28 , speech recognition BIBREF29 , and text conversation BIBREF30 . While, CNN is better to learn non-biased implicit features which has been successfully exploited for many supervised NLP learning tasks as described in Section SECREF1 , and various CNN based variants are proposed in the recent works, such as Dynamic Convolutional Neural Network (DCNN) BIBREF10 , Gated Recursive Convolutional Neural Network (grConv) BIBREF31 and Self-Adaptive Hierarchical Sentence model (AdaSent) BIBREF32 .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much improvement is gained from the proposed approaches?",
    "llm_answer": "The provided text describes improvements in overall performance, outperformance in two categories, and  superiority to other methods in detecting PTSD users.  However, it does not quantify the *amount* of improvement.  Therefore, the answer is not found in the context.\n",
    "context": [
      {
        "id": "11bbd522-28ef-4580-9084-84720bfb9758",
        "metadata": {
          "vector_store_key": "2001.06888-0",
          "chunk_id": 42,
          "document_id": "2001.06888",
          "start_idx": 22556,
          "end_idx": 22770
        },
        "page_content": "Evaluation results of our proposed model compared to other state of the art methods show its superiority to these methods in overall while in two categories (Person and Miscellaneous) our model outperformed others.",
        "type": "Document"
      },
      {
        "id": "7405365c-0f26-4485-b300-7809b4a62db7",
        "metadata": {
          "vector_store_key": "2002.00652-3",
          "chunk_id": 30,
          "document_id": "2002.00652",
          "start_idx": 18286,
          "end_idx": 19043
        },
        "page_content": "In terms of the combinations of different context modeling methods, they do not significantly improve the performance as we expected. As mentioned in Section SECREF1, intuitively, methods which only use the precedent SQL enjoys better generalizability. To validate it, we further conduct an out-of-distribution experiment to assess the generalizability of different context modeling methods. Concretely, we select three representative methods and train them on questions at turn 1 and 2, whereas test them at turn 3, 4 and beyond. As shown in Figure FIGREF38, Action Copy has a consistently comparable or better performance, validating the intuition. Meanwhile, Concat appears to be strikingly competitive, demonstrating it also has a good generalizability.",
        "type": "Document"
      },
      {
        "id": "874d1312-9fdb-46fe-865b-ce75dcb445d6",
        "metadata": {
          "vector_store_key": "1812.07023-3",
          "chunk_id": 32,
          "document_id": "1812.07023",
          "start_idx": 17666,
          "end_idx": 18428
        },
        "page_content": "We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below: Our primary architectural differences over the baseline model are: not concatenating the question, answer pairs before encoding them, the auxiliary decoder module, and using the Time-Extended FiLM module for feature extraction. These, combined with using scheduled sampling and running hyperparameter optimization over the validation set to select hyperparameters, give us the observed performance boost. We observe that our models generate fairly relevant responses to questions in the dialogues, and models with audio-visual inputs respond to audio-visual questions (e.g. \u201cis there any voices or music ?\u201d) correctly more often.",
        "type": "Document"
      },
      {
        "id": "b2f8b59b-1092-4698-9fa3-c0a21594d3ff",
        "metadata": {
          "vector_store_key": "2003.07433-4",
          "chunk_id": 47,
          "document_id": "2003.07433",
          "start_idx": 25723,
          "end_idx": 26401
        },
        "page_content": "To compare the outperformance of our method, we also implemented Coppersmith et. al. proposed method and achieved an 86% overall accuracy of detecting PTSD users BIBREF11 following the same training-test dataset distribution. Fig FIGREF28 illustrates the comparisons between LAXARY and Coppersmith et. al. proposed method. Here we can see, the outperformance of our proposed method as well as the importance of $s-score$ estimation. We also illustrates the importance of $\\alpha -score$ and $S-score$ in Fig FIGREF30. Fig FIGREF30 illustrates that if we change the number of training samples (%), LAXARY models outperforms Coppersmith et. al. proposed model under any condition.",
        "type": "Document"
      },
      {
        "id": "6ac93a11-3bbd-439f-a9ee-157a9bd58381",
        "metadata": {
          "vector_store_key": "1912.01673-1",
          "chunk_id": 13,
          "document_id": "1912.01673",
          "start_idx": 7638,
          "end_idx": 8392
        },
        "page_content": "In the first one, we were looking for original and uncommon sentence change suggestions. In the second one, we collected sentence alternations using ideas from the first round. The first and second rounds of annotation could be broadly called as collecting ideas and collecting data, respectively. We manually selected 15 newspaper headlines. Eleven annotators were asked to modify each headline up to 20 times and describe the modification with a short name. They were given an example sentence and several of its possible alternations, see tab:firstroundexamples. Unfortunately, these examples turned out to be highly influential on the annotators' decisions and they correspond to almost two thirds of all of modifications gathered in the first round.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is infinite-length sequence generation a result of training with maximum likelihood?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "59181420-ee19-49af-b0f6-79c4cc202abc",
        "metadata": {
          "vector_store_key": "2002.02492-9",
          "chunk_id": 54,
          "document_id": "2002.02492",
          "start_idx": 30977,
          "end_idx": 31845
        },
        "page_content": "Maximum likelihood learning fits the model $p_{\\theta }$ using the data distribution, whereas a decoded sequence from the trained model follows the distribution $q_{\\mathcal {F}}$ induced by a decoding algorithm. Based on this discrepancy, we make a strong conjecture: we cannot be guaranteed to obtain a good consistent sequence generator using maximum likelihood learning and greedy decoding. Sequence-level learning, however, uses a decoding algorithm during training BIBREF25, BIBREF26. We hypothesize that sequence-level learning can result in a good sequence generator that is consistent with respect to incomplete decoding. We extended the notion of consistency of a recurrent language model put forward by BIBREF16 to incorporate a decoding algorithm, and used it to analyze the discrepancy between a model and the distribution induced by a decoding algorithm.",
        "type": "Document"
      },
      {
        "id": "52f8ceed-2f43-4c9d-8136-4a1fb5de4948",
        "metadata": {
          "vector_store_key": "2002.02492-9",
          "chunk_id": 0,
          "document_id": "2002.02492",
          "start_idx": 0,
          "end_idx": 827
        },
        "page_content": "Neural sequence models trained with maximum likelihood estimation (MLE) have become a standard approach to modeling sequences in a variety of natural language applications such as machine translation BIBREF0, dialogue modeling BIBREF1, and language modeling BIBREF2. Despite this success, MLE-trained neural sequence models have been shown to exhibit issues such as length bias BIBREF3, BIBREF4 and degenerate repetition BIBREF5. These issues are suspected to be related to the maximum likelihood objective's local normalization, which results in a discrepancy between the learned model's distribution and the distribution induced by the decoding algorithm used to generate sequences BIBREF6, BIBREF7. This has prompted the development of alternative decoding methods BIBREF8, BIBREF5 and training objectives BIBREF9, BIBREF10.",
        "type": "Document"
      },
      {
        "id": "2769210f-cd1c-471d-b7eb-9f9787f8dfa8",
        "metadata": {
          "vector_store_key": "2002.02492-3",
          "chunk_id": 42,
          "document_id": "2002.02492",
          "start_idx": 24147,
          "end_idx": 24859
        },
        "page_content": "Unless specified otherwise, we report metrics using the test context distribution, since the train, valid, and randomly generated context distributions had similar results. We train recurrent language models for sequence completion with maximum likelihood, using the following loss on each sequence $Y=(c_1,\\ldots ,c_k,y_1,\\ldots ,y_T)$: This amounts to running the full training sequence through a recurrent model and zeroing the loss for the first $k$ tokens, so that the first $k$ steps correspond to learning a $g_{\\theta }$ that encodes the context. Each model is trained on a single Nvidia P40 GPU for up to 100 epochs, stopping early when validation perplexity does not decrease for 10 consecutive epochs.",
        "type": "Document"
      },
      {
        "id": "ae53b6c4-15b8-4c8d-ba39-f52d77140134",
        "metadata": {
          "vector_store_key": "2002.11402-3",
          "chunk_id": 11,
          "document_id": "2002.11402",
          "start_idx": 6538,
          "end_idx": 7196
        },
        "page_content": "We trained two models, one with sequence length 512 to capture document level important n-grams and second with sequence length 64 to capture sentence/paragraph level important n-grams. Through experiments it was evident that, sequence length plays a vital role in deciding context and locally/globally important n-grams. Final output is a concatenation of both the model outputs. Trained the topic model on single 32gb NVidia-V100 and it took around 50 hours to train the model with sequence length 512. We had to take 256gb ram machine to accommodate all data in memory for faster read/write. Also, trained model with 64 sequence length in around 17 hours.",
        "type": "Document"
      },
      {
        "id": "4fe2006f-f86b-43ab-a207-f9762931bf52",
        "metadata": {
          "vector_store_key": "2002.02492-9",
          "chunk_id": 56,
          "document_id": "2002.02492",
          "start_idx": 32205,
          "end_idx": 32716
        },
        "page_content": "We suspect the absence of decoding in maximum likelihood estimation as a cause behind this inconsistency, and suggest investigating sequence-level learning as an alternative in the future. We thank Chris Dyer, Noah Smith and Kevin Knight for valuable discussions. This work was supported by Samsung Advanced Institute of Technology (Next Generation Deep Learning: from pattern recognition to AI) and Samsung Research (Improving Deep Learning using Latent Structure). KC thanks eBay and NVIDIA for their support.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is dataset for this challenge?",
    "llm_answer": "There are 26,700 question-comment pairs for task A, 2,670 question-question pairs for task B, and 26,700 question-comment pairs for task C.  The test dataset includes 50 questions, 500 related questions, and 5,000 comments.  Additionally, there are 20,000 paper titles, with a 10% development set.  There are 70,000 to 170,000 raw tweets,  12,350 labeled ads, and  5,000 balanced validation and 10,000 test tweets.\n",
    "context": [
      {
        "id": "5151eb5d-0f58-4ac6-8701-22b81cc2fc48",
        "metadata": {
          "vector_store_key": "1603.07044-0",
          "chunk_id": 18,
          "document_id": "1603.07044",
          "start_idx": 9722,
          "end_idx": 10344
        },
        "page_content": "The cQA data is organized as follows: there are 267 original questions, each question has 10 related question, and each related question has 10 comments. Therefore, for task A, there are a total number of 26,700 question-comment pairs. For task B, there are 2,670 question-question pairs. For task C, there are 26,700 question-comment pairs. The test dataset includes 50 questions, 500 related questions and 5,000 comments which do not overlap with the training set. To evaluate the performance, we use mean average precision (MAP) and F1 score. Table 2 shows the initial results using the RNN encoder for different tasks.",
        "type": "Document"
      },
      {
        "id": "413f45ab-c88d-48ed-95ac-641283f82d18",
        "metadata": {
          "vector_store_key": "1701.00185-7",
          "chunk_id": 35,
          "document_id": "1701.00185",
          "start_idx": 19061,
          "end_idx": 19668
        },
        "page_content": "We use the challenge data published in BioASQ's official website. In our experiments, we randomly select 20, 000 paper titles from 20 different MeSH major topics as in Table TABREF25 . As described in Table TABREF24 , the max length of selected paper titles is 53. For these datasets, we randomly select 10% of data as the development set. Since SearchSnippets has been pre-processed by Phan et al. BIBREF41 , we do not further process this dataset. In StackOverflow, texts contain lots of computer terminology, and symbols and capital letters are meaningful, thus we do not do any pre-processed procedures.",
        "type": "Document"
      },
      {
        "id": "981c0952-332a-4735-8174-f9b2309ffe8d",
        "metadata": {
          "vector_store_key": "1910.03814-1",
          "chunk_id": 22,
          "document_id": "1910.03814",
          "start_idx": 12251,
          "end_idx": 12958
        },
        "page_content": "We separate balanced validation ($5,000$) and test ($10,000$) sets. The remaining tweets are used for training. We also experimented using hate scores for each tweet computed given the different votes by the three annotators instead of binary labels. The results did not present significant differences to those shown in the experimental part of this work, but the raw annotations will be published nonetheless for further research. As far as we know, this dataset is the biggest hate speech dataset to date, and the first multimodal hate speech dataset. One of its challenges is to distinguish between tweets using the same key offensive words that constitute or not an attack to a community (hate speech).",
        "type": "Document"
      },
      {
        "id": "d87d327c-2858-4b5d-ae8a-860d401f63bd",
        "metadata": {
          "vector_store_key": "1908.05434-1",
          "chunk_id": 27,
          "document_id": "1908.05434",
          "start_idx": 14432,
          "end_idx": 15064
        },
        "page_content": "We have ensured that the raw dataset has no overlap with the labeled dataset to avoid bias in test accuracy. While it is possible to scrape more raw data, we did not observe significant improvements in model performances when the size of raw data increased from INLINEFORM0 70,000 to INLINEFORM1 170,000, hence we assume that the current raw dataset is sufficiently large. The labeled dataset is called Trafficking-10k. It consists of 12,350 ads from Backpage labeled by experts in human trafficking detection BIBREF9 . Each label is one of seven ordered levels of likelihood that the corresponding ad comes from a human trafficker.",
        "type": "Document"
      },
      {
        "id": "a7093661-2713-4940-973a-7151e0d98050",
        "metadata": {
          "vector_store_key": "1901.02262-4",
          "chunk_id": 32,
          "document_id": "1901.02262",
          "start_idx": 16551,
          "end_idx": 17146
        },
        "page_content": "The Q&A task also requires an abstractive answer but prefers a more concise answer than the NLG task, averaging 13.1 words, where many of the answers do not contain the context of the question. For instance, for the question \u201ctablespoon in cup\u201d, the answer in the Q&A task will be \u201c16\u201d, and the answer in the NLG task will be \u201cThere are 16 tablespoons in a cup.\u201d In addition to the ALL dataset, we prepared two subsets (Table 1 ). The ANS set consists of answerable questions, and the WFA set consists of the answerable questions and well-formed answers, where WFA $\\subset $ ANS $\\subset $ ALL.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How better is performance of proposed model compared to baselines?",
    "llm_answer": "The proposed model leads to improvements ranging from 0.5% to 3.0% in accuracy, and outperforms baselines in new environments by a clear margin.\n",
    "context": [
      {
        "id": "874d1312-9fdb-46fe-865b-ce75dcb445d6",
        "metadata": {
          "vector_store_key": "1812.07023-3",
          "chunk_id": 32,
          "document_id": "1812.07023",
          "start_idx": 17666,
          "end_idx": 18428
        },
        "page_content": "We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below: Our primary architectural differences over the baseline model are: not concatenating the question, answer pairs before encoding them, the auxiliary decoder module, and using the Time-Extended FiLM module for feature extraction. These, combined with using scheduled sampling and running hyperparameter optimization over the validation set to select hyperparameters, give us the observed performance boost. We observe that our models generate fairly relevant responses to questions in the dialogues, and models with audio-visual inputs respond to audio-visual questions (e.g. \u201cis there any voices or music ?\u201d) correctly more often.",
        "type": "Document"
      },
      {
        "id": "c938d47c-e132-4294-bd51-25295fb776d2",
        "metadata": {
          "vector_store_key": "1603.07044-3",
          "chunk_id": 28,
          "document_id": "1603.07044",
          "start_idx": 15079,
          "end_idx": 15744
        },
        "page_content": "Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited training data is given, both feature-rich based system and our system are worse than the IR system. For task C, our system also got comparable results with the feature-rich based system. If we do a simple system combination (average the rank score) between our system and the IR system, the combined system will give large gains on tasks B and C.",
        "type": "Document"
      },
      {
        "id": "6824b4f2-136e-4c26-a2a7-a10cf7d4c97e",
        "metadata": {
          "vector_store_key": "1810.00663-8",
          "chunk_id": 46,
          "document_id": "1810.00663",
          "start_idx": 25788,
          "end_idx": 26550
        },
        "page_content": "The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as can be seen in performance drops in Table TABREF28 for the new environments. Nonetheless, the insights from the previous section still hold: masking in the output layer and reordering the graph triplets tend to increase performance. Even though the results in Table TABREF28 suggest that there is room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM.",
        "type": "Document"
      },
      {
        "id": "14c77096-531d-4d71-9236-61c2ccdb8cfa",
        "metadata": {
          "vector_store_key": "1910.00458-6",
          "chunk_id": 44,
          "document_id": "1910.00458",
          "start_idx": 24199,
          "end_idx": 24865
        },
        "page_content": "Compared with the baselines, MMM leads to improvements ranging from 0.5% to 3.0% in accuracy. Our best result is obtained by the RoBERTa-Large encoder. In order to investigate how well our model performs for different types of questions, we did an error analysis by first randomly selecting 150 samples that had wrong predictions by the BERT-Base baseline model from the development set of DREAM dataset. We then manually classified them into several question types, as shown in Table TABREF34. The annotation criterion is described in the Section 3 of the Supplementary Material. We see that the BERT-Base baseline model still does not do well on matching problems.",
        "type": "Document"
      },
      {
        "id": "c3b4b0ea-1fb0-4db4-a1e3-cca1d982ef2e",
        "metadata": {
          "vector_store_key": "1809.04960-4",
          "chunk_id": 38,
          "document_id": "1809.04960",
          "start_idx": 21924,
          "end_idx": 22725
        },
        "page_content": "It shows that IR+Proposed can outperform the supervised IR model given the same paired dataset. It concludes that the proposed model can exploit the unpaired data to further improve the performance of the supervised model. Although our proposed model can achieve better performance than previous models, there are still remaining two questions: why our model can outperform them, and how to further improve the performance. To address these queries, we perform error analysis to analyze the error types of our model and the baseline models. We select TF-IDF, S2S, and IR as the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types of comments as described in the above retrieval evaluation setting: Correct, Plausible, Popular, and Random.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What DCGs are used?",
    "llm_answer": "DGC-ave, DGC-max, DGC-gated.\n",
    "context": [
      {
        "id": "60a93ce9-4eb8-4e98-8a3b-2a3ed996543a",
        "metadata": {
          "vector_store_key": "1902.09087-7",
          "chunk_id": 41,
          "document_id": "1902.09087",
          "start_idx": 22847,
          "end_idx": 23671
        },
        "page_content": "For DGCs with different kinds of pooling operations, average pooling (DGC-ave) performs the best, which delivers similar performance with LCN-ave. While DGC-max performs a little worse, because it ignores the importance of different edges and the maximum operation is more sensitive to noise than the average operation. The DGC-gated performs the worst. Compared with LCN-gated that learns the gate value adaptively from multiple n-gram context, it is harder for DGC to learn the importance of each edge via the node and the center node in the word lattice. It is not surprising that LCN-gated performs much better than GDC-gated, indicating again that n-grams in word lattice play an important role in context modeling, while DGCs are designed for general directed graphs which may not be perfect to work with word lattice.",
        "type": "Document"
      },
      {
        "id": "73932706-6bb0-4d90-ab8f-645898ddebd9",
        "metadata": {
          "vector_store_key": "1902.09087-7",
          "chunk_id": 52,
          "document_id": "1902.09087",
          "start_idx": 29160,
          "end_idx": 29917
        },
        "page_content": "Furthermore, our model has no conflicts with most of them except BIBREF3 and could gain further improvement. GCNs BIBREF25 , BIBREF26 and graph-RNNs BIBREF27 , BIBREF28 have extended CNNs and RNNs to model graph information, and DGCs generalize GCNs on directed graphs in the fields of semantic-role labeling BIBREF12 , document dating BIBREF18 , and SQL query embedding BIBREF29 . However, DGCs control information flowing from neighbor vertexes via edge types, while we focus on capturing different contexts for each word in word lattice via convolutional kernels and poolings. Previous works involved Chinese lattice into RNNs for Chinese-English translation BIBREF10 , Chinese named entity recognition BIBREF11 , and Chinese word segmentation BIBREF30 .",
        "type": "Document"
      },
      {
        "id": "9d853fa7-0c72-4c65-b29e-60b371a76d0e",
        "metadata": {
          "vector_store_key": "2001.06354-0",
          "chunk_id": 7,
          "document_id": "2001.06354",
          "start_idx": 3970,
          "end_idx": 4726
        },
        "page_content": "Since NDCG measures more of a model's generalization ability (because it allows multiple similar answers), while the other metrics measure a model's preciseness, we interpret the results of these above experiments to mean that a model with more history information tends to predict correct answers by memorizing keywords or patterns in the history while a model with less history information (i.e., the image-only model) is better at generalization by avoiding relying on such exact-match extracted information. We think that an ideal model should have more balanced behavior and scores over all the metrics rather than having higher scores only for a certain metric and such a model could be considered as the one with both preciseness and generalization.",
        "type": "Document"
      },
      {
        "id": "64ec7753-613f-466b-b6cb-9afdcdadb71d",
        "metadata": {
          "vector_store_key": "2001.05672-4",
          "chunk_id": 30,
          "document_id": "2001.05672",
          "start_idx": 16205,
          "end_idx": 16869
        },
        "page_content": "DCG rule for the negative form is implemented as follows. DCG rules for the negative form is almost similar to those of the positive form, except from pol/1 predicate. However, in the 2nd step for the negative form, it completely utilizes the rule for the positive form as follows. However, there is an exception of the 2nd step for group 1, it needs an extra rule like: As we can see above, the negative form of group 1 needs the extra rule lex(AUX_POL,pol,Tense ,Qs) because, in this negative form, an extra auxiliary verb is needed. For example, the positive sentence is \u201che buys an apple\", but the corresponding negative sentence is \u201che does not buy an apple\".",
        "type": "Document"
      },
      {
        "id": "c0d4f6e4-2cc4-4f3d-8af8-c2d5362b6f6c",
        "metadata": {
          "vector_store_key": "2001.06354-7",
          "chunk_id": 28,
          "document_id": "2001.06354",
          "start_idx": 14725,
          "end_idx": 15531
        },
        "page_content": "Unlike the images in the train and validation sets, the images in the test set have only one follow-up question and candidate answers and their corresponding conversational context. For evaluation, the Visual Dialog task employs four metrics. NDCG is the primary metric of the Visual Dialog Challenge which considers multiple similar answers as correct ones. The other three are MRR, recall@k, and mean rank where they only consider the rank of a single answer. Our experiments show the scores of NDCG and non-NDCG metrics from our image-only and joint models have a trade-off relationship due to their different ability (as shown in Sec.SECREF41) in completing Visual Dialog tasks: the image-only model has a high NDCG and low non-NDCG values while the joint model has a low NDCG and high non-NDCG values.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance difference of using a generated summary vs. a user-written one?",
    "llm_answer": "The models consistently perform better with a user-written (gold) summary than with a system-generated summary.\n",
    "context": [
      {
        "id": "cc3eede4-864a-4b24-99a2-7bc048e29df4",
        "metadata": {
          "vector_store_key": "1911.02711-1",
          "chunk_id": 31,
          "document_id": "1911.02711",
          "start_idx": 17352,
          "end_idx": 18036
        },
        "page_content": "A comparison between models that use summary information and those that do not use summary information shows that the review summary is useful for sentiment classification. In addition, the same models work consistently better when the user written gold summary is used compared to a system generated summary, which is intuitively reasonable since the current state-of-the-art abstractive summarization models are far from perfect. Interestingly, as shown in the second section of the table, the gold summary itself does not lead to better sentiment accuracy compared with the review itself, which shows that summaries better serve as auxiliary information sources to review contents.",
        "type": "Document"
      },
      {
        "id": "b21e5524-d973-414c-863a-66acef36ed67",
        "metadata": {
          "vector_store_key": "1911.02711-3",
          "chunk_id": 33,
          "document_id": "1911.02711",
          "start_idx": 18373,
          "end_idx": 19132
        },
        "page_content": "In contrast, our model integrates summary information into the review representation in each layer, thereby allowing the integrated representation to be hierarchically refined, leading to more abstract hidden states. Finally, the fact that with gold summary, our baseline and final models outperforms the state-of-the-art methods by jointly training shows the importance of making use of user written summaries when they are available. Even with system summary, out models still outperforms HSSC and SAHSSC, showing that our network is more effective than parameter sharing under the same setting without input summaries. Figure FIGREF37 consists of line graphs on the accuracy of BiLSTM+self-attention, BiLSTM+pooling and our model against the review length.",
        "type": "Document"
      },
      {
        "id": "6bf5fbfe-41fa-4e7d-9a38-eb526e4cfadf",
        "metadata": {
          "vector_store_key": "1909.00578-2",
          "chunk_id": 5,
          "document_id": "1909.00578",
          "start_idx": 3090,
          "end_idx": 3864
        },
        "page_content": "Since no post-edited datasets \u2013 like the ones used in MT \u2013 are available for summarization, we use instead the ratings assigned by human annotators with respect to a set of linguistic quality criteria. Our proposed models achieve high correlation with human judgments, showing that it is possible to estimate summary quality without human references. We use datasets from the NIST DUC-05, DUC-06 and DUC-07 shared tasks BIBREF7, BIBREF19, BIBREF20. Given a question and a cluster of newswire documents, the contestants were asked to generate a 250-word summary answering the question. DUC-05 contains 1,600 summaries (50 questions x 32 systems); in DUC-06, 1,750 summaries are included (50 questions x 35 systems); and DUC-07 has 1,440 summaries (45 questions x 32 systems).",
        "type": "Document"
      },
      {
        "id": "b63dca89-e845-447d-900c-f1dee155ba11",
        "metadata": {
          "vector_store_key": "1911.02711-3",
          "chunk_id": 24,
          "document_id": "1911.02711",
          "start_idx": 13819,
          "end_idx": 14438
        },
        "page_content": "It can also be used to compare the effectiveness of the review itself, the summary itself and the combination of both when used as inputs to the problem. This baseline uses a BiLSTM with hidden size of 256 in both directions. On the top of BiLSTM, self-attention is used to provide a set of summation weight vectors for the final representation. This method is conceptually simple yet gives the state-of-the-art results for many classification and text matching tasks. Its main difference to our model lies in the fact that attention is performed only in the top hidden layer in this method, yet in every layer in ours.",
        "type": "Document"
      },
      {
        "id": "da84b8b6-0536-4be7-8368-96be88ce0e06",
        "metadata": {
          "vector_store_key": "1910.00825-4",
          "chunk_id": 0,
          "document_id": "1910.00825",
          "start_idx": 0,
          "end_idx": 915
        },
        "page_content": "Summarization aims to condense a piece of text to a shorter version, retaining the critical information. On dialogs, summarization has various promising applications in the real world. For instance, the automatic doctor-patient interaction summary can save doctors' massive amount of time used for filling medical records. There is also a general demand for summarizing meetings in order to track project progress in the industry. Generally, multi-party conversations with interactive communication are more difficult to summarize than single-speaker documents. Hence, dialog summarization will be a potential field in summarization track. There are two types of summarization: extractive and abstractive. Extractive summarization selects sentences or phrases directly from the source text and merges them to a summary, while abstractive summarization attempts to generate novel expressions to condense information.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What evaluation metrics did they look at?",
    "llm_answer": "Partner collaboration, information ease, task ease, user expertise, task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, F<sub>1</sub> score, training curves, test performance.\n",
    "context": [
      {
        "id": "eab7e7bd-c8ba-4bad-955c-111fcdafe6ae",
        "metadata": {
          "vector_store_key": "2003.05995-3",
          "chunk_id": 43,
          "document_id": "2003.05995",
          "start_idx": 22319,
          "end_idx": 22996
        },
        "page_content": "The post-task questionnaire had four questions rated in 7-point rating scales that are loosely based on the PARADISE BIBREF27 questions for spoken dialogue systems: Partner collaboration: \u201cHow helpful was your partner?\u201d on a scale of 1 (not helpful at all) to 7 (very helpful). Information ease: \u201cIn this conversation, was it easy to get the information that I needed?\u201d on a scale of 1 (no, not at all) to 7 (yes, completely). Task ease: \u201cHow easy was the task?\u201d on a scale of 1 (very easy) to 7 (very difficult). User expertise: \u201cIn this conversation, did you know what you could say or do at each point of the dialog?\u201d on a scale of 1 (no, not at all) to 7 (yes, completely).",
        "type": "Document"
      },
      {
        "id": "8c81c383-8e40-4094-8e18-ab0a2d32126a",
        "metadata": {
          "vector_store_key": "1905.08949-6",
          "chunk_id": 18,
          "document_id": "1905.08949",
          "start_idx": 10340,
          "end_idx": 11074
        },
        "page_content": "Although the datasets are commonly shared between QG and QA, it is not the case for evaluation: it is challenging to define a gold standard of proper questions to ask. Meaningful, syntactically correct, semantically sound and natural are all useful criteria, yet they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks. As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE BIBREF43 , are also widely used.",
        "type": "Document"
      },
      {
        "id": "ab676abe-1fd3-450a-a82d-153595442021",
        "metadata": {
          "vector_store_key": "1709.10217-1",
          "chunk_id": 11,
          "document_id": "1709.10217",
          "start_idx": 6267,
          "end_idx": 6922
        },
        "page_content": "For building the dialogue systems of participants, we release an example set of complete user intent and three data files of flight, train and hotel in JSON format. There are five evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the number of total tasks. User satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied, neutral, satisfied and very satisfied, respectively. Response fluency: There are three scores -1, 0, 1, which indicate nonfluency, neutral, fluency. Number of dialogue turns: The number of utterances in a task-completed dialogue.",
        "type": "Document"
      },
      {
        "id": "6a443055-4b4a-46e1-a2dc-b3211ac62d38",
        "metadata": {
          "vector_store_key": "1908.10449-0",
          "chunk_id": 20,
          "document_id": "1908.10449",
          "start_idx": 10741,
          "end_idx": 11449
        },
        "page_content": "Since iMRC involves both MRC and RL, we adopt evaluation metrics from both settings. First, as a question answering task, we use $\\text{F}_1$ score to compare predicted answers against ground-truth, as in previous works. When there exist multiple ground-truth answers, we report the max $\\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we evaluate an agent's performance during both its training and testing phases. During training, we report training curves averaged over 3 random seeds. During test, we follow common practice in supervised learning tasks where we report the agent's test performance corresponding to its best validation performance .",
        "type": "Document"
      },
      {
        "id": "74a8f350-1cdc-4b68-87aa-e719da16ac2c",
        "metadata": {
          "vector_store_key": "1709.10217-3",
          "chunk_id": 4,
          "document_id": "1709.10217",
          "start_idx": 2049,
          "end_idx": 2735
        },
        "page_content": "For the task-oriented system, although there are some objective evaluation metrics, such as the number of turns in a dialogue, the ratio of task completion, etc., there is no gold standard for automatically evaluating two (or more) dialogue systems when considering the satisfaction of the human and the fluency of the generated dialogue. To promote the development of the evaluation technology for dialogue systems, especially considering the language characteristics of Chinese, we organize the first evaluation of Chinese human-computer dialogue technology. In this paper, we will present the evaluation scheme and the released corpus in detail. The rest of this paper is as follows.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the datasets used for the task?",
    "llm_answer": "HEOT (cleaned tweets), Davidson et al. (English tweets), MS COCO, Bing, Flickr, SST-2, Yelp, MRPC, STS-B, QQP (GLUE-Para), SQuAD 1.1, SQuAD 2.0, MRQA.\n",
    "context": [
      {
        "id": "5c4b1458-497e-4f2e-a4e7-c5e739d9cad0",
        "metadata": {
          "vector_store_key": "1912.13109-1",
          "chunk_id": 9,
          "document_id": "1912.13109",
          "start_idx": 4561,
          "end_idx": 5222
        },
        "page_content": "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al. This dataset was important to employ Transfer Learning to our task since the number of labeled dataset was very small. Basic summary and examples of the data from the dataset are below: The obtained data set had many challenges and thus a data preparation task was employed to clean the data and make it ready for the deep learning pipeline.",
        "type": "Document"
      },
      {
        "id": "947a5cf6-1656-4ca3-827b-105c8344c1d6",
        "metadata": {
          "vector_store_key": "1910.11949-3",
          "chunk_id": 16,
          "document_id": "1910.11949",
          "start_idx": 8700,
          "end_idx": 9318
        },
        "page_content": "In particular, we use two types of datasets to train our models: A dataset that maps pictures with questions, and an open-domain conversation dataset. The details of the two datasets are as follows. We use MS COCO, Bing and Flickr datasets from BIBREF26 to train the model that generates questions. These datasets contain natural questions about images with the purpose of knowing more about the picture. As can be seen in the Figure FIGREF8, questions cannot be answered by only looking at the image. Each source contains 5,000 images with 5 questions per image, adding a total of 15,000 images with 75,000 questions.",
        "type": "Document"
      },
      {
        "id": "731d76b9-6365-4a8c-872e-c14badc3b626",
        "metadata": {
          "vector_store_key": "1911.07228-0",
          "chunk_id": 12,
          "document_id": "1911.07228",
          "start_idx": 6115,
          "end_idx": 6702
        },
        "page_content": "The dataset contains four different types of label: Location (LOC), Person (PER), Organization (ORG) and Miscellaneous - Name of an entity that do not belong to 3 types above (Table TABREF15). Although the corpus has more information about the POS and chunks, but we do not use them as features in our model. There are two folders with 267 text files of training data and 45 text files of test data. They all have their own format. We take 21 first text files and 22 last text files and 22 sentences of the 22th text file and 55 sentences of the 245th text file to be a development data.",
        "type": "Document"
      },
      {
        "id": "2ded8a9e-d4bc-48fa-9555-3885b88eba42",
        "metadata": {
          "vector_store_key": "1910.00458-5",
          "chunk_id": 32,
          "document_id": "1910.00458",
          "start_idx": 17414,
          "end_idx": 18132
        },
        "page_content": "For the sentiment analysis category, we used the Stanford Sentiment Treebank (SST-2) dataset from the GLUE benchmark BIBREF22 (around 60k train examples) and the Yelp dataset (around 430k train examples). For the paraphrase category, three paraphrasing datasets are used from the GLUE benchmark: Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), and Quora Question Pairs (QQP), which are denoted as \u201cGLUE-Para.\u201d. For the span-based QA, we use the SQuAD 1.1, SQuAD 2.0 , and MRQA which is a joint dataset including six popular span-based QA datasets. Table TABREF23 summarizes the results. We see that sentiment analysis datasets do not help much with our target MCQA datasets.",
        "type": "Document"
      },
      {
        "id": "56e160ae-5061-4b16-89e6-3dcdc3fc7666",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 10,
          "document_id": "1912.13109",
          "start_idx": 4796,
          "end_idx": 5441
        },
        "page_content": "Basic summary and examples of the data from the dataset are below: The obtained data set had many challenges and thus a data preparation task was employed to clean the data and make it ready for the deep learning pipeline. The challenges and processes that were applied are stated below: Messy text messages: The tweets had urls, punctuations, username mentions, hastags, emoticons, numbers and lots of special characters. These were all cleaned up in a preprocessing cycle to clean the data. Stop words: Stop words corpus obtained from NLTK was used to eliminate most unproductive words which provide little information about individual tweets.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the accuracy of the model for the six languages tested?",
    "llm_answer": "91.2% for English, French, and German; 94.9 \u00b1 0.3% for Manipuri.  Other accuracies are mentioned for specific language combinations, but not a single \"six-language\" overall accuracy.\n",
    "context": [
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      },
      {
        "id": "343fa539-680b-4b43-9631-124e054c3bf9",
        "metadata": {
          "vector_store_key": "1909.13362-0",
          "chunk_id": 39,
          "document_id": "1909.13362",
          "start_idx": 23022,
          "end_idx": 23711
        },
        "page_content": "Our reported accuracy of $94.9 \\pm 0.3\\%$ on the Manipuri dataset is furthest from state of the art. We suspect this to be due to having limited amounts of training data; the $97.5\\%$ accurate system from BIBREF35 supplemented their data-driven approach with rules of syllabification. Examples from the outputs of the Base model can give us insight into what the model does well and what types of words it struggles with. The total number of sounds across languages is vast, but not infinite, as Ladefoged and Maddieson's The Sounds of the the World's Languages demonstrates BIBREF42. Different languages choose different inventories from the total producible by the human vocal apparatus.",
        "type": "Document"
      },
      {
        "id": "78370272-7c04-4f69-947d-7a47a39d9515",
        "metadata": {
          "vector_store_key": "1910.05456-4",
          "chunk_id": 25,
          "document_id": "1910.05456",
          "start_idx": 13952,
          "end_idx": 14606
        },
        "page_content": "Lastly, for ZUL, all models perform rather poorly, with a minimum accuracy of 10.7 and 10.8 for the source languages QVH and EUS, respectively, and a maximum accuracy of 24.9 for a model pretrained on Turkish. The latter result hints at the fact that a regular and agglutinative morphology might be beneficial in a source language \u2013 something which could also account for the performance of models pretrained on HUN. For our qualitative analysis, we make use of the validation set. Therefore, we show validation set accuracies in Table TABREF19 for comparison. As we can see, the results are similar to the test set results for all language combinations.",
        "type": "Document"
      },
      {
        "id": "8eadeabb-5370-4958-bea8-48a63ba10138",
        "metadata": {
          "vector_store_key": "2001.11268-5",
          "chunk_id": 40,
          "document_id": "2001.11268",
          "start_idx": 22945,
          "end_idx": 23764
        },
        "page_content": "In practice, the detachment between model predictions and assignment of labels means that a reviewer who wishes to switch between high recall and high precision results can do so very quickly, without obtaining new predictions from the model itself. More visualizations can be found in this project's GitHub repository , including true class labels and a detailed breakdown of true and false predictions for each class. The highest proportion of false classification appears between the results and conclusion classes. The fine-tuned multilingual model showed marginally inferior classification scores on the exclusively English testing data. However, this model's contribution is not limited to the English language because its interior weights embed a shared vocabulary of 100 languages, including German and Chinese.",
        "type": "Document"
      },
      {
        "id": "61f37a83-a12e-476e-bbd0-2fedf87d5f9d",
        "metadata": {
          "vector_store_key": "1910.05456-4",
          "chunk_id": 24,
          "document_id": "1910.05456",
          "start_idx": 13270,
          "end_idx": 13952
        },
        "page_content": "For SPA, again HUN performs best, followed closely by ITA. While the good performance of HUN as a source language is still unexpected, ITA is closely related to SPA, which could explain the high accuracy of the final model. As for ENG, pretraining on EUS and NAV yields the worst final models \u2013 importantly, accuracy is over $15\\%$ lower than for QVH, which is also an unrelated language. This again suggests that the prefixing morphology of EUS and NAV might play a role. Lastly, for ZUL, all models perform rather poorly, with a minimum accuracy of 10.7 and 10.8 for the source languages QVH and EUS, respectively, and a maximum accuracy of 24.9 for a model pretrained on Turkish.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which models achieve state-of-the-art performances?",
    "llm_answer": "KIM, and possibly HUN and ITA (for SPA).\n",
    "context": [
      {
        "id": "91c26d50-2f90-4366-8fc5-dffa0cf4cca8",
        "metadata": {
          "vector_store_key": "1810.09774-1",
          "chunk_id": 20,
          "document_id": "1810.09774",
          "start_idx": 9939,
          "end_idx": 10602
        },
        "page_content": "KIM is particularly interesting in this context as it performed significantly better than other models in the Breaking NLI experiment conducted by BIBREF1 . The success of pre-trained language models in multiple NLP tasks make ESIM + ELMo and BERT interesting additions to this experiment. Table 3 lists the different models used in the experiments. For BiLSTM-max we used the Adam optimizer BIBREF21 , a learning rate of 5e-4 and batch size of 64. The learning rate was decreased by the factor of 0.2 after each epoch if the model did not improve. Dropout of 0.1 was used between the layers of the multi-layer perceptron classifier, except before the last layer.",
        "type": "Document"
      },
      {
        "id": "60552214-8a5b-4305-aa4b-bfbdc35d8792",
        "metadata": {
          "vector_store_key": "2002.11402-4",
          "chunk_id": 16,
          "document_id": "2002.11402",
          "start_idx": 9020,
          "end_idx": 10286
        },
        "page_content": "Similarly, Stanford did well in Precision but could not catch up with Spacy and our model in terms of Recall. Flair overall performed poorly, but as said before these open-source models are not trained for our particular use-case. Lets check some examples for detailed analysis of the models and their results. Following is the economy related news. Example 1 : around $1\u20131.5 trillion or around two percent of global gdp, are lost to corruption every year, president of the natural resource governance institute nrgi has said. speaking at a panel on integrity in public governance during the world bank group and international monetary fund annual meeting on sunday, daniel kaufmann, president of nrgi, presented the statistic, result of a study by the nrgi, an independent, non-profit organisation based in new york. however, according to kaufmann, the figure is only the direct costs of corruption as it does not factor in the opportunities lost on innovation and productivity, xinhua news agency reported. a country that addresses corruption and significantly improves rule of law can expect a huge increase in per capita income in the long run, the study showed. it will also see similar gains in reducing infant mortality and improving education, said kaufmann.",
        "type": "Document"
      },
      {
        "id": "ad1d7753-3848-45ad-a7ec-2f9bff0e735c",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 18,
          "document_id": "1912.13109",
          "start_idx": 9307,
          "end_idx": 10125
        },
        "page_content": "While the model probability is the probability that the observation i belongs to category c. Among the model architectures we experimented with and without data augmentation were: Fully Connected dense networks: Model hyperparameters were inspired from the previous work done by Vo et al and Mathur et al. This was also used as a baseline model but we did not get appreciable performance on such architecture due to FC networks not being able to capture local and long term dependencies. Convolution based architectures: Architecture and hyperparameter choices were chosen from the past study Deon on the subject. We were able to boost the performance as compared to only FC based network but we noticed better performance from architectures that are suitable to sequences such as text messages or any timeseries data.",
        "type": "Document"
      },
      {
        "id": "61f37a83-a12e-476e-bbd0-2fedf87d5f9d",
        "metadata": {
          "vector_store_key": "1910.05456-4",
          "chunk_id": 24,
          "document_id": "1910.05456",
          "start_idx": 13270,
          "end_idx": 13952
        },
        "page_content": "For SPA, again HUN performs best, followed closely by ITA. While the good performance of HUN as a source language is still unexpected, ITA is closely related to SPA, which could explain the high accuracy of the final model. As for ENG, pretraining on EUS and NAV yields the worst final models \u2013 importantly, accuracy is over $15\\%$ lower than for QVH, which is also an unrelated language. This again suggests that the prefixing morphology of EUS and NAV might play a role. Lastly, for ZUL, all models perform rather poorly, with a minimum accuracy of 10.7 and 10.8 for the source languages QVH and EUS, respectively, and a maximum accuracy of 24.9 for a model pretrained on Turkish.",
        "type": "Document"
      },
      {
        "id": "818e90de-47ca-469c-9c94-6817ecccd8bd",
        "metadata": {
          "vector_store_key": "1811.00942-0",
          "chunk_id": 23,
          "document_id": "1811.00942",
          "start_idx": 13144,
          "end_idx": 13731
        },
        "page_content": "If a high-end commodity GPU is available, then the models are only twice as slow as KN-5 is. From row 5, even better results are noted with wt103-qrnn: On the CPU, the QRNN is only 60% slower than KN-5 is, while the model is faster by 11 $\\times $ on a GPU. These results suggest that, if only latency is considered under a commodity desktop environment, the QRNN model is humanly indistinguishable from the KN-5 model, even without using GPU acceleration. In the present work, we describe and examine the tradeoff space between quality and performance for the task of language modeling.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which competitive relational classification models do they test?",
    "llm_answer": "The context does not mention any competitive relational classification models.  It describes baseline models using Logistic Regression.\n",
    "context": [
      {
        "id": "5a72856b-91f8-460a-a634-ef0826946058",
        "metadata": {
          "vector_store_key": "1910.03891-7",
          "chunk_id": 43,
          "document_id": "1910.03891",
          "start_idx": 24539,
          "end_idx": 25258
        },
        "page_content": "The training time on both data sets is limited to at most 400 epochs. The best models are selected by a grid search and early stopping on validation sets. In entity classification, the aim is to predicate the type of entity. For all baseline models, we first get the entity embedding in different datasets through default parameter settings as in their original papers or implementations. Then, Logistic Regression is used as classifier, which regards the entity's embeddings as feature of classifier. In evaluation, we random selected 10% of training set as validation set and accuracy as evaluation metric. Experimental results of entity classification on the test sets of all the datasets is shown in Table TABREF25.",
        "type": "Document"
      },
      {
        "id": "895b9934-11e7-438a-aedf-2435c41cdf73",
        "metadata": {
          "vector_store_key": "1905.08949-8",
          "chunk_id": 17,
          "document_id": "1905.08949",
          "start_idx": 9885,
          "end_idx": 10631
        },
        "page_content": "Table 1 presents a listing of the NQG corpora grouped by their cognitive level and answer type, along with their statistics. Among them, SQuAD was used by most groups as the benchmark to evaluate their NQG models. This provides a fair comparison between different techniques. However, it raises the issue that most NQG models work on factoid questions with answer as text span, leaving other types of QG problems less investigated, such as generating deep multi-choice questions. To overcome this, a wider variety of corpora should be benchmarked against in future NQG research. Although the datasets are commonly shared between QG and QA, it is not the case for evaluation: it is challenging to define a gold standard of proper questions to ask.",
        "type": "Document"
      },
      {
        "id": "5654405b-1abb-4c07-b666-4dd8bba00ec0",
        "metadata": {
          "vector_store_key": "2001.09332-1",
          "chunk_id": 25,
          "document_id": "2001.09332",
          "start_idx": 12628,
          "end_idx": 13400
        },
        "page_content": "The first test (Table TABREF15) was performed considering all the analogies present, and therefore evaluating as an error any analogy that was not executable (as it related to one or more words absent from the vocabulary). As it can be seen, regardless of the metric used, our model has significantly better results than the other two models, both overall and within the two macro-areas. Furthermore, the other two models seem to be more subject to the metric used, perhaps due to a stabilization not yet reached for the few training epochs. For a complete comparison, both models were also tested considering only the subset of the analogies in common with our model (i.e. eliminating from the test all those analogies that were not executable by one or the other model).",
        "type": "Document"
      },
      {
        "id": "0ab61107-67bf-493c-aa5f-199d370caaa3",
        "metadata": {
          "vector_store_key": "1703.04617-2",
          "chunk_id": 26,
          "document_id": "1703.04617",
          "start_idx": 14460,
          "end_idx": 15206
        },
        "page_content": "The previous models are often trained for all questions without explicitly discriminating different question types; however, for a target question, both the common features shared by all questions and the specific features for a specific type of question are further considered in this paper, as they could potentially obey different distributions. In this paper we further explicitly model different types of questions in the end-to-end training. We start from a simple way to first analyze the word frequency of all questions, and obtain top-10 most frequent question types: what, how, who, when, which, where, why, be, whose, and whom, in which be stands for the questions beginning with different forms of the word be such as is, am, and are.",
        "type": "Document"
      },
      {
        "id": "2db15cf8-1c5d-41f7-afc9-e231338cf62c",
        "metadata": {
          "vector_store_key": "2002.11893-0",
          "chunk_id": 5,
          "document_id": "2002.11893",
          "start_idx": 3362,
          "end_idx": 4193
        },
        "page_content": "To facilitate model comparison, benchmark models are provided for different modules in pipelined task-oriented dialogue systems, including natural language understanding, dialogue state tracking, dialogue policy learning, and natural language generation. We also provide a user simulator, which will facilitate the development and evaluation of dialogue models on this corpus. The corpus and the benchmark models are publicly available at https://github.com/thu-coai/CrossWOZ. According to whether the dialogue agent is human or machine, we can group the collection methods of existing task-oriented dialogue datasets into three categories. The first one is human-to-human dialogues. One of the earliest and well-known ATIS dataset BIBREF6 used this setting, followed by BIBREF8, BIBREF9, BIBREF10, BIBREF15, BIBREF16 and BIBREF12.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they gather human judgements for similarity between relations?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "25a85af6-47db-4b68-b31a-3e38c648c999",
        "metadata": {
          "vector_store_key": "1907.08937-4",
          "chunk_id": 5,
          "document_id": "1907.08937",
          "start_idx": 2861,
          "end_idx": 3579
        },
        "page_content": "Therefore, we further provide a sampling-based method to approximate the similarity score over the entity pair space for computational efficiency. Besides developing a framework for assessing the similarity between relations, our second contribution is that we have done a survey of applications. We present experiments and analysis aimed at answering five questions: (1) How well does the computed similarity score correlate with human judgment about the similarity between relations? How does our approach compare to other possible approaches based on other kinds of relation embeddings to define a similarity? (sec:relationship and sec:human-judgment) (2) Open IE models inevitably extract many redundant relations.",
        "type": "Document"
      },
      {
        "id": "e550a34a-c3f5-4b71-b63b-3b391b5b5406",
        "metadata": {
          "vector_store_key": "1907.08937-4",
          "chunk_id": 45,
          "document_id": "1907.08937",
          "start_idx": 25789,
          "end_idx": 26773
        },
        "page_content": "However, the existing methods suffer from sparse data, which makes it difficult to achieve an effective and stable similarity metric. Motivated by this, we propose to measure relation similarity by leveraging their fact distribution so that we can identify nuances between similar relations, and merge those distant surface forms of the same relations, benefitting the tasks mentioned above. In this paper, we introduce an effective method to quantify the relation similarity and provide analysis and a survey of applications. We note that there are a wide range of future directions: (1) human prior knowledge could be incorporated into the similarity quantification; (2) similarity between relations could also be considered in multi-modal settings, e.g., extracting relations from images, videos, or even from audios; (3) by analyzing the distributions corresponding to different relations, one can also find some \u201cmeta-relations\u201d between relations, such as hypernymy and hyponymy.",
        "type": "Document"
      },
      {
        "id": "43be2718-0425-4540-800b-cdcae2bd119b",
        "metadata": {
          "vector_store_key": "1907.08937-3",
          "chunk_id": 44,
          "document_id": "1907.08937",
          "start_idx": 25111,
          "end_idx": 25789
        },
        "page_content": "Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 , BIBREF35 , BIBREF36 , BIBREF37 and relation extraction BIBREF38 , BIBREF39 , BIBREF40 , BIBREF41 , BIBREF42 , BIBREF43 , and relation prediction BIBREF3 , BIBREF44 , BIBREF45 , BIBREF46 , BIBREF47 . For both semantic relations and general relations, identifying them is a crucial problem, requiring systems to provide a fine-grained relation similarity metric. However, the existing methods suffer from sparse data, which makes it difficult to achieve an effective and stable similarity metric.",
        "type": "Document"
      },
      {
        "id": "91413bd5-f1d3-4fc0-8e9b-059f0bdfb1e6",
        "metadata": {
          "vector_store_key": "1907.08937-3",
          "chunk_id": 46,
          "document_id": "1907.08937",
          "start_idx": 26183,
          "end_idx": 26986
        },
        "page_content": "We note that there are a wide range of future directions: (1) human prior knowledge could be incorporated into the similarity quantification; (2) similarity between relations could also be considered in multi-modal settings, e.g., extracting relations from images, videos, or even from audios; (3) by analyzing the distributions corresponding to different relations, one can also find some \u201cmeta-relations\u201d between relations, such as hypernymy and hyponymy. This work is supported by the National Natural Science Foundation of China (NSFC No. 61572273, 61532010), the National Key Research and Development Program of China (No. 2018YFB1004503). Chen and Zhu is supported by Tsinghua University Initiative Scientific Research Program, and Chen is also supported by DCST Student Academic Training Program.",
        "type": "Document"
      },
      {
        "id": "f8edd7c6-d4c8-4b4f-b52e-039b9457751d",
        "metadata": {
          "vector_store_key": "1907.08937-3",
          "chunk_id": 0,
          "document_id": "1907.08937",
          "start_idx": 0,
          "end_idx": 900
        },
        "page_content": "Author contributions: Hao Zhu designed the research; Weize Chen prepared the data, and organized data annotation; Hao Zhu and Xu Han designed the experiments; Weize Chen performed the experiments; Hao Zhu, Weize Chen and Xu Han wrote the paper; Zhiyuan Liu and Maosong Sun proofread the paper. Zhiyuan Liu is the corresponding author. Relations, representing various types of connections between entities or arguments, are the core of expressing relational facts in most general knowledge bases (KBs) BIBREF0 , BIBREF1 . Hence, identifying relations is a crucial problem for several information extraction tasks. Although considerable effort has been devoted to these tasks, some nuances between similar relations are still overlooked, (tab:similarityexample shows an example); on the other hand, some distinct surface forms carrying the same relational semantics are mistaken as different relations.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What text classification task is considered?",
    "llm_answer": "Incomplete text classification.\n",
    "context": [
      {
        "id": "d1918f95-dc1f-47f0-842d-63afce009fd3",
        "metadata": {
          "vector_store_key": "1605.08675-1",
          "chunk_id": 32,
          "document_id": "1605.08675",
          "start_idx": 17882,
          "end_idx": 18604
        },
        "page_content": "Possible values are the following: place, continent, river, lake, mountain, mountain range, island, archipelago, sea, celestial body, country, state, city, nationality, person, first name, last name, band, dynasty, organisation, company, event, date, century, year, period, number, quantity, vehicle, animal, title. Focus synset \u2013 applicable in case of entity questions; a WordNet synset, to which a question focus belongs; necessary for DeepER. Search query \u2013 used to find possibly relevant documents, Question content \u2013 the words from question which are supposed to appear also in context of an answer. The task presented above, called question classification, is an example of text classification with very short texts.",
        "type": "Document"
      },
      {
        "id": "07c95130-d362-4bcc-89aa-6f66d690e36e",
        "metadata": {
          "vector_store_key": "1907.05664-2",
          "chunk_id": 3,
          "document_id": "1907.05664",
          "start_idx": 1820,
          "end_idx": 2435
        },
        "page_content": "Our goal in this paper is to test the limits of the use of such a technique for more complex tasks, where the notion of input importance might not be as simple as in topic classification or sentiment analysis. We changed from a classification task to a generative task and chose a more complex one than text translation (in which we can easily find a word to word correspondence/importance between input and output). We chose text summarization. We consider abstractive and informative text summarization, meaning that we write a summary \u201cin our own words\" and retain the important information of the original text.",
        "type": "Document"
      },
      {
        "id": "0ed71a6b-a1aa-4a60-a95d-09b3534a2593",
        "metadata": {
          "vector_store_key": "1707.02377-4",
          "chunk_id": 35,
          "document_id": "1707.02377",
          "start_idx": 20784,
          "end_idx": 21482
        },
        "page_content": "The 100 categories includes categories under sports, entertainment, literature, and politics etc. Examples of categories include American drama films, Directorial debut films, Major League Baseball pitchers and Sydney Swans players. Body texts (the second paragraph) were extracted for each page as a document. For each category, we select 1,000 documents with unique category label, and 100 documents were used for training and 900 documents for testing. The remaining documents are used as unlabeled data. The 100 classes are balanced in the training and testing sets. For this data set, we learn the word embedding and document representation for all the algorithms using all the available data.",
        "type": "Document"
      },
      {
        "id": "e0c5c1ea-3566-4bc4-9d2e-d9432320995c",
        "metadata": {
          "vector_store_key": "2001.00137-1",
          "chunk_id": 3,
          "document_id": "2001.00137",
          "start_idx": 2001,
          "end_idx": 2847
        },
        "page_content": "Tasks such as text generation, sentence correction, image captioning and text classification, have been possible via models such as Convolutional Neural Networks and Recurrent Neural Networks BIBREF5, BIBREF6, BIBREF7. More recently, state-of-the-art results have been achieved with attention models, more specifically Transformers BIBREF8. Surprisingly, however, there is currently no research on incomplete text classification in the NLP community. Realizing the need of research in that area, we make it the focus of this paper. In this novel task, the model aims to identify the user's intent or sentiment by analyzing a sentence with missing and/or incorrect words. In the sentiment classification task, the model aims to identify the user's sentiment given a tweet, written in informal language and without regards for sentence correctness.",
        "type": "Document"
      },
      {
        "id": "bf94617a-c59a-4da6-9917-c606a70fbddc",
        "metadata": {
          "vector_store_key": "1909.00542-3",
          "chunk_id": 7,
          "document_id": "1909.00542",
          "start_idx": 4311,
          "end_idx": 5132
        },
        "page_content": "Our past participating systems used regression approaches using the following framework: Train the regressor to predict the ROUGE-SU4 F1 score of the input sentence. Produce a summary by selecting the top $n$ input sentences. A novelty in the current participation is the introduction of classification approaches using the following framework. Train the classifier to predict the target label (\u201csummary\u201d or \u201cnot summary\u201d) of the input sentence. Produce a summary by selecting all sentences predicted as \u201csummary\u201d. If the total number of sentences selected is less than $n$, select $n$ sentences with higher probability of label \u201csummary\u201d. Introducing a classifier makes labelling the training data not trivial, since the target summaries are human-generated and they do not have a perfect mapping to the input sentences.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What novel class of recurrent-like networks is proposed?",
    "llm_answer": "A class of recurrent-like neural networks satisfying the differential equation DISPLAYFORM0.\n",
    "context": [
      {
        "id": "8211f53a-6eb1-46df-ab14-3ff38f9ef75a",
        "metadata": {
          "vector_store_key": "1909.13362-1",
          "chunk_id": 11,
          "document_id": "1909.13362",
          "start_idx": 6678,
          "end_idx": 7353
        },
        "page_content": "These neural-based approaches are taking the place of HMMs, maximum entropy Markov models (MEMM), and conditional random fields (CRF) BIBREF18. In the following section and in Fig. FIGREF1, we present a neural network architecture that leverages both recurrence and one-dimensional convolutions. Recurrence enables our model to read a sequence much like a human would; a sequence with elements $abcd$ would be read one element at a time, updating a latent understanding after reading each $a$, $b$, $c$, and finally $d$. One-dimensional convolutions extract a spatial relationship between sequential elements. The $abcd$ example sequence may then be read as $ab$, $bc$, $cd$.",
        "type": "Document"
      },
      {
        "id": "90c2ec7b-0ef6-4535-a552-d8a117d544d0",
        "metadata": {
          "vector_store_key": "1908.05434-7",
          "chunk_id": 16,
          "document_id": "1908.05434",
          "start_idx": 8445,
          "end_idx": 9093
        },
        "page_content": "Recurrent neural networks (RNNs) have recently seen great success at modeling sequential data, especially in natural language processing tasks BIBREF19 . On a high level, an RNN is a neural network that processes a sequence of inputs one at a time, taking the summary of the sequence seen so far from the previous time point as an additional input and producing a summary for the next time point. One of the most widely used variations of RNNs, a Long short-term memory network (LSTM), uses various gates to control the information flow and is able to better preserve long-term dependencies in the running summary compared to a basic RNN BIBREF20 .",
        "type": "Document"
      },
      {
        "id": "eb8f7583-83a8-44fe-a58c-d4713eaf8c18",
        "metadata": {
          "vector_store_key": "1710.01507-2",
          "chunk_id": 14,
          "document_id": "1710.01507",
          "start_idx": 8096,
          "end_idx": 8806
        },
        "page_content": "Recurrent Neural Network (RNN) is a class of artificial neural networks which utilizes sequential information and maintains history through its intermediate layers. A standard RNN has an internal state whose output at every time-step which can be expressed in terms of that of previous time-steps. However, it has been seen that standard RNNs suffer from a problem of vanishing gradients BIBREF15 . This means it will not be able to efficiently model dependencies and interactions between words that are a few steps apart. LSTMs are able to tackle this issue by their use of gating mechanisms. For each record in the dataset, the content of the post as well as the content of the related web page is available.",
        "type": "Document"
      },
      {
        "id": "26bfeab4-dc42-4a10-a750-ed51db8858eb",
        "metadata": {
          "vector_store_key": "1803.02839-0",
          "chunk_id": 10,
          "document_id": "1803.02839",
          "start_idx": 5741,
          "end_idx": 6390
        },
        "page_content": "Since sentences trace out a path on the word manifold, and parallel transport operators for representations of the word manifold take values in the group, the RNN must parallel transport hidden states either on the group itself or on a base space, INLINEFORM5 , equipped with some word field, INLINEFORM6 , that connects the path in the base space to the path on the word manifold. Leveraging these results, we propose two new technologies. First, we propose a class of recurrent-like neural networks for NLP tasks that satisfy the differential equation DISPLAYFORM0  where DISPLAYFORM0   and where INLINEFORM0 and INLINEFORM1 are learned functions.",
        "type": "Document"
      },
      {
        "id": "c10534c5-d73c-4cd1-8b3e-02a80234f34e",
        "metadata": {
          "vector_store_key": "1811.00942-3",
          "chunk_id": 7,
          "document_id": "1811.00942",
          "start_idx": 4717,
          "end_idx": 5503
        },
        "page_content": "Quasi-recurrent neural networks (QRNNs; BIBREF10 ) achieve current state of the art in word-level language modeling BIBREF11 . A quasi-recurrent layer comprises two separate parts: a convolution layer with three weights, and a recurrent pooling layer. Given an input $\\mathbf {X} \\in \\mathbb {R}^{k \\times n}$ , the convolution layer is $\n\\mathbf {Z} = \\tanh (\\mathbf {W}_z \\cdot \\mathbf {X})\\\\\n\\mathbf {F} = \\sigma (\\mathbf {W}_f \\cdot \\mathbf {X})\\\\\n\\mathbf {O} = \\sigma (\\mathbf {W}_o \\cdot \\mathbf {X})\n$  where $\\sigma $ denotes the sigmoid function, $\\cdot $ represents masked convolution across time, and $\\mathbf {W}_{\\lbrace z, f, o\\rbrace } \\in \\mathbb {R}^{m \\times k \\times r}$ are convolution weights with $k$ input channels, $m$ output channels, and a window size of $r$ .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much bigger is Switchboard-2000 than Switchboard-300 database?",
    "llm_answer": "The context states that Switchboard-2000 has 2000 hours of speech data, and Switchboard-300 has 300 hours.  Therefore, Switchboard-2000 is approximately 6.67 times larger.\n",
    "context": [
      {
        "id": "9dc90cf1-221c-476e-917f-7dba29e15ec5",
        "metadata": {
          "vector_store_key": "2001.07263-1",
          "chunk_id": 30,
          "document_id": "2001.07263",
          "start_idx": 16706,
          "end_idx": 17424
        },
        "page_content": "Our overall results on the Hub5'00 and other evaluation sets are summarized in Table TABREF14. The results in Fig. FIGREF12 and Table TABREF14 show that adding more training data greatly improves the system, by around 30% relative in some cases. For comparison with others, the 2000-hour system reaches 8.7% and 7.4% WER on rt02 and rt04. We observe that the regularization techniques, which are extremely important on the 300h setup, are still beneficial but have a significantly smaller effect. For comparison with results in the literature we refer to the Switchboard-300 results in BIBREF3, BIBREF7, BIBREF51, BIBREF52 and the Switchboard-2000 results in BIBREF50, BIBREF51, BIBREF53, BIBREF54, BIBREF55, BIBREF56.",
        "type": "Document"
      },
      {
        "id": "92a17c7e-ceea-4d63-87d3-8848e9c37d49",
        "metadata": {
          "vector_store_key": "2001.07263-1",
          "chunk_id": 31,
          "document_id": "2001.07263",
          "start_idx": 16706,
          "end_idx": 17535
        },
        "page_content": "For comparison with results in the literature we refer to the Switchboard-300 results in BIBREF3, BIBREF7, BIBREF51, BIBREF52 and the Switchboard-2000 results in BIBREF50, BIBREF51, BIBREF53, BIBREF54, BIBREF55, BIBREF56. Our 300-hour model not only outperforms the previous best attention based encoder-decoder model BIBREF3 by a large margin, it also surpasses the best hybrid systems with multiple LMs BIBREF7. Our result on Switchboard-2000 is also better than any single system results reported to date, and reaches the performance of the best system combinations. We presented an attention based encoder-decoder setup which achieves state-of-the-art performance on Switchboard-300. A rather simple model built from LSTM layers and a decoder with a single-headed attention mechanism outperforms the standard hybrid approach.",
        "type": "Document"
      },
      {
        "id": "c76de05b-e6b4-4956-bfe5-a528b200e4c5",
        "metadata": {
          "vector_store_key": "2001.07263-1",
          "chunk_id": 29,
          "document_id": "2001.07263",
          "start_idx": 16706,
          "end_idx": 17363
        },
        "page_content": "Lastly, if the model is trained on 2000 hours of speech data (see next section), the extremely fast greedy decoding gives remarkably good performance. Although the importance of beam search decreases with an increased amount of training data, we still measure 10% relative degradation compared to a system with a cross-utterance LM and wide (240) beam search. As a contrast to our best results on Switchboard-300, we also train a seq2seq model on the 2000-hour Switchboard+Fisher data. This model consists of 10 encoder layers, and is trained for only 50 epochs. Our overall results on the Hub5'00 and other evaluation sets are summarized in Table TABREF14.",
        "type": "Document"
      },
      {
        "id": "1460d9d9-e1cf-41c5-88a4-0c7f7d0baf5d",
        "metadata": {
          "vector_store_key": "1910.00458-6",
          "chunk_id": 24,
          "document_id": "1910.00458",
          "start_idx": 13565,
          "end_idx": 14218
        },
        "page_content": "The MC160 and MC500 datasets were curated in almost the same way BIBREF9 with only one difference that MC160 is around three times smaller than MC500. We can see from Table TABREF17 that both the BERT and RoBERTa baselines perform much worse on MC160 than MC500. We think the reason is that the data size of MC160 is not enough to well fine-tune the large models with a huge amount of trainable parameters. However, by leveraging the transfer learning techniques we proposed, we can significantly improve the generalization capability of BERT and RoBERTa models on the small datasets so that the best performance of MC160 can even surpass that of MC500.",
        "type": "Document"
      },
      {
        "id": "a3fe60d8-57d9-453a-b8b7-c6a050171dd8",
        "metadata": {
          "vector_store_key": "1701.06538-2",
          "chunk_id": 0,
          "document_id": "1701.06538",
          "start_idx": 0,
          "end_idx": 609
        },
        "page_content": "Exploiting scale in both training data and model size has been central to the success of deep learning. When datasets are sufficiently large, increasing the capacity (number of parameters) of neural networks can give much better prediction accuracy. This has been shown in domains such as text BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , images BIBREF4 , BIBREF5 , and audio BIBREF6 , BIBREF7 . For typical deep learning models, where the entire model is activated for every example, this leads to a roughly quadratic blow-up in training costs, as both the model size and the number of training examples increase.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What domains are detected in this paper?",
    "llm_answer": "The context mentions \"Lifestyle\" as an example domain, but does not list all detected domains.\n",
    "context": [
      {
        "id": "6d3c770c-4ae2-4b84-8b4b-6da41e938353",
        "metadata": {
          "vector_store_key": "1907.11499-3",
          "chunk_id": 9,
          "document_id": "1907.11499",
          "start_idx": 4901,
          "end_idx": 5612
        },
        "page_content": "Our contributions in this work are threefold; we propose domain detection, as a new fine-grained multilabel learning problem which we argue would benefit the development of domain aware NLP tools; we introduce a weakly supervised encoder-detector model within the context of multiple instance learning; and demonstrate that it can be applied across languages and text genres without modification. Our work lies at the intersection of multiple research areas, including domain adaptation, representation learning, multiple instance learning, and topic modeling. We review related work below. We formulate domain detection as a multilabel learning problem. Our model is trained on samples of document-label pairs.",
        "type": "Document"
      },
      {
        "id": "2e76af14-b064-4501-b27a-103fd6bbbeaf",
        "metadata": {
          "vector_store_key": "1907.11499-3",
          "chunk_id": 12,
          "document_id": "1907.11499",
          "start_idx": 6293,
          "end_idx": 7203
        },
        "page_content": "For example, the definition of the \u201cLifestyle\u201d domain is \u201cthe interests, opinions, behaviors, and behavioral orientations of an individual, group, or culture\u201d. Figure FIGREF5 provides an overview of our Domain Detection Network, which we call DetNet. The model comprises two modules; an encoder learns representations for words and sentences whilst incorporating prior domain information; a detector generates domain scores for words, sentences, and documents by selectively attending to previously encoded information. We describe the two modules in more detail below. We learn representations for words and sentences using identical encoders with separate learning parameters. Given a document, the two encoders implement the following steps: INLINEFORM0   For each sentence INLINEFORM0 , the word-level encoder yields contextualized word representations INLINEFORM1 and their attention weights INLINEFORM2 .",
        "type": "Document"
      },
      {
        "id": "667686e8-376a-4680-a974-5d16cd242a14",
        "metadata": {
          "vector_store_key": "1907.11499-3",
          "chunk_id": 7,
          "document_id": "1907.11499",
          "start_idx": 3647,
          "end_idx": 4428
        },
        "page_content": "Importantly, we do not require document-level domain annotations either since we obtain these via distant supervision by leveraging information drawn from Wikipedia. Our domain detection framework comprises two neural network modules; an encoder learns representations for words and sentences together with prior domain information if the latter is available (e.g., domain definitions), while a detector generates domain-specific scores for words, sentences, and documents. We obtain a segment-level domain predictor which is trained end-to-end on document-level labels using a hierarchical, attention-based neural architecture BIBREF15 . We conduct domain detection experiments on English and Chinese and measure system performance using both automatic and human-based evaluation.",
        "type": "Document"
      },
      {
        "id": "07f2f6d5-a96f-41a6-8b41-ebce920a5808",
        "metadata": {
          "vector_store_key": "1907.11499-3",
          "chunk_id": 8,
          "document_id": "1907.11499",
          "start_idx": 4121,
          "end_idx": 4901
        },
        "page_content": "We conduct domain detection experiments on English and Chinese and measure system performance using both automatic and human-based evaluation. Experimental results show that our model outperforms several strong baselines and is robust across languages and text genres, despite learning from weak supervision. We also showcase our model's application potential for text summarization. Our contributions in this work are threefold; we propose domain detection, as a new fine-grained multilabel learning problem which we argue would benefit the development of domain aware NLP tools; we introduce a weakly supervised encoder-detector model within the context of multiple instance learning; and demonstrate that it can be applied across languages and text genres without modification.",
        "type": "Document"
      },
      {
        "id": "a0e7fdb9-ae87-4700-a7f4-76bf89caa336",
        "metadata": {
          "vector_store_key": "1907.11499-3",
          "chunk_id": 2,
          "document_id": "1907.11499",
          "start_idx": 957,
          "end_idx": 1759
        },
        "page_content": "The ability to handle a wide variety of domains has become more pertinent with the rise of data-hungry machine learning techniques like neural networks and their application to a plethora of textual media ranging from news articles to twitter, blog posts, medical journals, Reddit comments, and parliamentary debates BIBREF0 , BIBREF3 , BIBREF4 , BIBREF5 . The question of how to best deal with multiple domains when training data is available for one or few of them has met with much interest in the literature. The field of domain adaptation BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 aims at improving the learning of a predictive function in a target domain where there is little or no labeled data, using knowledge transferred from a source domain where sufficient labeled data is available.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Why do they think this task is hard?  What is the baseline performance?",
    "llm_answer": "Users with mild cognitive impairment found the task challenging due to the effort required to remember answers to generated questions.  The baseline performance is not explicitly stated but initial policy training with additional complexity resulted in significantly worse performance.\n",
    "context": [
      {
        "id": "b1b5b777-261c-4aa4-80b1-ab705fce7f3f",
        "metadata": {
          "vector_store_key": "1910.08210-0",
          "chunk_id": 33,
          "document_id": "1910.08210",
          "start_idx": 18239,
          "end_idx": 18903
        },
        "page_content": "We repeatedly add more complexity until we obtain $10\\times 10$ worlds with moving monsters, many-to-one group assignments and natural language templated descriptions. The performance across the curriculum is shown in Table TABREF32 (see Figure FIGREF58 in appendix SECREF12 for training curves of each stage). We see that curriculum learning is crucial to making progress on , and that initial policy training (first row of Table TABREF32) with additional complexities in any of the dimensions result in significantly worse performance. We take each of the 5 runs after training through the whole curriculum and evaluate them on dynamics not seen during training.",
        "type": "Document"
      },
      {
        "id": "60ae0bac-9718-4081-bea1-f9dbe2921a21",
        "metadata": {
          "vector_store_key": "2003.05995-6",
          "chunk_id": 46,
          "document_id": "2003.05995",
          "start_idx": 23418,
          "end_idx": 24080
        },
        "page_content": "We predicted that only a small portion of the participants would be able to resolve the emergency in less than 6 minutes, thus it was framed as a bonus challenge rather than a requirement to get paid. The fastest time recorded to resolve the emergency was 4 minutes 13 seconds with a mean of 5 minutes 8 seconds. Table TABREF28 shows several interaction statistics for the data collected compared to the single lab-based WoZ study BIBREF4. Table TABREF33 gives the results from the post-task survey. We observe, that subjective and objective task success are similar in that the dialogues that resolved the emergency were rated consistently higher than the rest.",
        "type": "Document"
      },
      {
        "id": "eb1e9310-af06-425f-b1fb-fe1a6a3fc96e",
        "metadata": {
          "vector_store_key": "1910.11949-4",
          "chunk_id": 36,
          "document_id": "1910.11949",
          "start_idx": 18262,
          "end_idx": 18874
        },
        "page_content": "Responses were given on a five-point scale ranging from strongly disagree (1) to strongly agree (5) and very easy (1) to very difficult (5). The results were 4.6 for amusing and engaging and 2.6 for difficulty. Healthy users found it very easy to use (1/5) and even a bit silly, because of some of the generated questions and comments. Nevertheless, users with mild cognitive impairment found it engaging (5/5) and challenging (4/5), because of the effort they had to make to remember the answers for some of the generated questions. All the users had in common that they enjoyed doing the therapy with Elisabot.",
        "type": "Document"
      },
      {
        "id": "0d228039-17bf-423a-9d72-8145791ddf43",
        "metadata": {
          "vector_store_key": "1703.09684-9",
          "chunk_id": 46,
          "document_id": "1703.09684",
          "start_idx": 25450,
          "end_idx": 26300
        },
        "page_content": "We found that for MCB-A, the accuracy on subordinate object recognition drops from 85.54% with unnormalized to 23.22% with normalized, and for scene recognition it drops from 93.06% (unnormalized) to 38.53% (normalized). Both these categories have a heavily skewed answer distribution; the top-25 answers in subordinate object recognition and the top-5 answers in scene recognition cover over 80% of all questions in their respective question-types. This shows that question-types that appear to be easy may simply be due to the algorithms learning the answer statistics. A truly easy question-type will have similar performance for both unnormalized and normalized metrics. For example, sport recognition shows only 17.39% drop compared to a 30.21% drop for counting, despite counting having same number of unique answers and far more training data.",
        "type": "Document"
      },
      {
        "id": "0db8a56e-c4b1-4bc7-9cc9-da1cec054a59",
        "metadata": {
          "vector_store_key": "1703.09684-9",
          "chunk_id": 68,
          "document_id": "1703.09684",
          "start_idx": 37982,
          "end_idx": 38641
        },
        "page_content": "For example, while scene understanding has top-4 answers covering over 85% of all the questions, there are roughly as many `no' questions (most common answer) as there are `yes' questions (second most-common answer). Similar distributions can be seen for counting, where `two' (most-common answer) is repeated almost as many times as `one' (second most-common answer). By having at least the top-2 answers split almost equally, we remove the incentive for an algorithm to perform well using simple mode guessing, even when using the simple accuracy metric. In the paper, we mentioned that we split the entire collection into 70% train and 30% test/validation.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they generate the synthetic dataset?",
    "llm_answer": "The simulated data sets are generated by opening existing documents, making changes (addition, deletion, replacement), and saving them as revisions.  This process starts with an initial corpus without revisions, adding new documents and revising existing ones, mimicking a file system.  Revisions include changes to words, sentences, paragraphs, section names, and titles.\n",
    "context": [
      {
        "id": "6d7e1fda-02df-491f-93a1-de0c2f84c4b0",
        "metadata": {
          "vector_store_key": "1709.01256-4",
          "chunk_id": 36,
          "document_id": "1709.01256",
          "start_idx": 19769,
          "end_idx": 20455
        },
        "page_content": "The generation process of the simulated data sets is designed to mimic the real world. Users open some existing documents in a file system, make some changes (e.g. addition, deletion or replacement), and save them as separate documents. These documents become revisions of the original documents. We started from an initial corpus that did not have revisions, and kept adding new documents and revising existing documents. Similar to a file system, at any moment new documents could be added and/or some of the current documents could be revised. The revision operations we used were deletion, addition and replacement of words, sentences, paragraphs, section names and document titles.",
        "type": "Document"
      },
      {
        "id": "84104c9f-f895-46f2-b0a6-d3f3451ac006",
        "metadata": {
          "vector_store_key": "1906.00180-3",
          "chunk_id": 3,
          "document_id": "1906.00180",
          "start_idx": 1879,
          "end_idx": 2547
        },
        "page_content": "First, we develop a protocol for automatically generating data that can be used in entailment recognition tasks. Second, we demonstrate that several deep learning architectures succeed at one such task. Third, we present and apply a number of experiments to test whether models are capable of compositional generalization. The data generation process is inspired by BIBREF13 : an artificial language is defined, sentences are generated according to its grammar and the entailment relation between pairs of such sentences is established according to a fixed background logic. However, our language is significantly more complex, and instead of natural logic we use FOL.",
        "type": "Document"
      },
      {
        "id": "fffe3f80-9435-409d-bad1-e4512d1dea31",
        "metadata": {
          "vector_store_key": "1910.09399-2",
          "chunk_id": 5,
          "document_id": "1910.09399",
          "start_idx": 3128,
          "end_idx": 4145
        },
        "page_content": "Most of the contributions from these papers rely on multimodal learning approaches that include generative adversarial networks and deep convolutional decoder networks as their main drivers to generate entrancing images from text BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11. First introduced by Ian Goodfellow et al. BIBREF9, generative adversarial networks (GANs) consist of two neural networks paired with a discriminator and a generator. These two models compete with one another, with the generator attempting to produce synthetic/fake samples that will fool the discriminator and the discriminator attempting to differentiate between real (genuine) and synthetic samples. Because GANs' adversarial training aims to cause generators to produce images similar to the real (training) images, GANs can naturally be used to generate synthetic images (image synthesis), and this process can even be customized further by using text descriptions to specify the types of images to generate, as shown in Figure FIGREF6.",
        "type": "Document"
      },
      {
        "id": "a7fe717f-04bc-4708-96e2-e0d3744977b2",
        "metadata": {
          "vector_store_key": "1707.03904-4",
          "chunk_id": 9,
          "document_id": "1707.03904",
          "start_idx": 5317,
          "end_idx": 6034
        },
        "page_content": "We also collect annotations on a subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short. We plan to release these annotations along with the datasets, and our retrieved documents for each question. Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated for each Quasar variant. The software question set was built from the definitional \u201cexcerpt\u201d entry for each tag (entity) on StackOverflow.",
        "type": "Document"
      },
      {
        "id": "d66cc891-009f-4bc3-b1b8-5e32625faea7",
        "metadata": {
          "vector_store_key": "1910.09399-2",
          "chunk_id": 103,
          "document_id": "1910.09399",
          "start_idx": 55000,
          "end_idx": 55775
        },
        "page_content": "Algorithms and methods developed in the computer vision field have allowed researchers in recent years to create realistic images from plain sentences. Advances in the computer vision, deep convolutional nets, and semantic units have shined light and redirected focus to this research area of text-to-image synthesis, having as its prime directive: to aid in the generation of compelling images with as much fidelity to text descriptions as possible. To date, models for generating synthetic images from textual natural language in research laboratories at universities and private companies have yielded compelling images of flowers and birds BIBREF8. Though flowers and birds are the most common objects studied thus far, research has been applied to other classes as well.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the average length of the claims?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "80c551bb-b7d7-4d60-8d8d-9cd05f017c4c",
        "metadata": {
          "vector_store_key": "2004.03034-4",
          "chunk_id": 24,
          "document_id": "2004.03034",
          "start_idx": 12316,
          "end_idx": 12912
        },
        "page_content": "Therefore, we claim that it is not sufficient to only study the linguistic characteristic of a claim to determine its impact, but it is also necessary to consider its context in determining the impact. Context length ($\\text{C}_{l}$) for a particular claim C is defined by number of claims included in the argument path starting from the thesis until the claim C. For example, in Figure FIGREF1, the context length for O1 and S3 are 1 and 2 respectively. Table TABREF8 shows number of claims with the given range of context length for the claims with more than 5 votes and $60\\%$ agreement score.",
        "type": "Document"
      },
      {
        "id": "fe9534ab-1689-42de-808c-53a9981dc4dc",
        "metadata": {
          "vector_store_key": "1906.03538-3",
          "chunk_id": 21,
          "document_id": "1906.03538",
          "start_idx": 12299,
          "end_idx": 13085
        },
        "page_content": "We measure the accuracy of our adjudicated labels versus AMT labels, resulting in 87.7%. This indicates the high quality of the crowdsourced data. We now provide a brief summary of [wave]390P[wave]415e[wave]440r[wave]465s[wave]485p[wave]525e[wave]535c[wave]595t[wave]610r[wave]635u[wave]660m. The dataset contains about INLINEFORM0 claims with a significant length diversity (Table TABREF19 ). Additionally, the dataset comes with INLINEFORM1 perspectives, most of which were generated through paraphrasing (step 2b). The perspectives which convey the same point with respect to a claim are grouped into clusters. On average, each cluster has a size of INLINEFORM2 which shows that, on average, many perspectives have equivalents. More granular details are available in Table TABREF19 .",
        "type": "Document"
      },
      {
        "id": "62222161-c2c3-4a24-983c-32cab6f17553",
        "metadata": {
          "vector_store_key": "2004.03034-4",
          "chunk_id": 25,
          "document_id": "2004.03034",
          "start_idx": 12912,
          "end_idx": 13547
        },
        "page_content": "Table TABREF8 shows number of claims with the given range of context length for the claims with more than 5 votes and $60\\%$ agreement score. We observe that more than half of these claims have 3 or higher context length. Similar to prior work, our aim is to understand the characteristics of impactful claims in argumentation. However, we hypothesize that the qualitative characteristics of arguments is not independent of the context in which they are presented. To understand the relationship between argument context and the impact of a claim, we aim to incorporate the context along with the claim itself in our predictive models.",
        "type": "Document"
      },
      {
        "id": "e3ed7eb0-dafc-4fdf-b9c9-0c8b34e2afd0",
        "metadata": {
          "vector_store_key": "2004.03034-0",
          "chunk_id": 18,
          "document_id": "2004.03034",
          "start_idx": 9152,
          "end_idx": 9743
        },
        "page_content": "We limit our study to the claims with at least 5 votes to have a more reliable assignment for the accumulated impact label for each claim. Impact label statistics. Table TABREF7 shows the distribution of the number of votes for each of the impact categories. The claims have $241,884$ total votes. The majority of the impact votes belong to medium impact category. We observe that users assign more high impact and very high impact votes than low impact and no impact votes respectively. When we restrict the claims to the ones with at least 5 impact votes, we have $213,277$ votes in total.",
        "type": "Document"
      },
      {
        "id": "9580b547-af3a-4386-9d79-ab75474298dc",
        "metadata": {
          "vector_store_key": "2004.03034-4",
          "chunk_id": 35,
          "document_id": "2004.03034",
          "start_idx": 18098,
          "end_idx": 18702
        },
        "page_content": "In this setting, we use the parent claim's text, in addition to the target claim text, in order to classify the impact of the target claim. We treat this as a sequence pair classification task, and combine both the target claim and parent claim as a single sequence of tokens, separated by the special separator [SEP]. We then follow the same procedure above, for fine-tuning. In this setting, we consider incorporating a larger context from the discourse, in order to assess the impact of a claim. In particular, we consider up to four previous claims in the discourse (for a total context length of 5).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What debate topics are included in the dataset?",
    "llm_answer": "Politics, World, and Society.  The text also mentions 10+ other topics.\n",
    "context": [
      {
        "id": "c16e5203-7bf2-4c72-93f7-adfd4ce26304",
        "metadata": {
          "vector_store_key": "1708.05873-2",
          "chunk_id": 10,
          "document_id": "1708.05873",
          "start_idx": 5733,
          "end_idx": 6387
        },
        "page_content": "Figure FIGREF4 provides a list of the main topics (and the highest probability words associated these topics) that emerge from the STM of UN General Debate statements. In addition to the highest probability words, we use several other measures of key words (not presented here) to interpret the dimensions. This includes the FREX metric (which combines exclusivity and word frequency), the lift (which gives weight to words that appear less frequently in other topics), and the score (which divides the log frequency of the word in the topic by the log frequency of the word in other topics). We provide a brief description of each of the 16 topics here.",
        "type": "Document"
      },
      {
        "id": "0fd906a2-ab91-40d0-8bd1-0d9e08eb7d04",
        "metadata": {
          "vector_store_key": "1906.03538-2",
          "chunk_id": 22,
          "document_id": "1906.03538",
          "start_idx": 13085,
          "end_idx": 13832
        },
        "page_content": "More granular details are available in Table TABREF19 . To better understand the topical breakdown of claims in the dataset, we crowdsource the set of \u201ctopics\u201d associated with each claim (e.g., Law, Ethics, etc.) We observe that, as expected, the three topics of Politics, World, and Society have the biggest portions (Figure FIGREF21 ). Additionally, the included claims touch upon 10+ different topics. Figure FIGREF22 depicts a few popular categories and sampled questions from each. We perform a closer investigation of the abilities required to solve the stance classification task. One of the authors went through a random subset of claim-perspectives pairs and annotated each with the abilities required in determining their stances labels.",
        "type": "Document"
      },
      {
        "id": "9dc69e28-1cf2-4d3c-9f8f-18a6f0169737",
        "metadata": {
          "vector_store_key": "1708.05873-2",
          "chunk_id": 9,
          "document_id": "1708.05873",
          "start_idx": 5213,
          "end_idx": 5988
        },
        "page_content": "Following BIBREF8 we generate a set of candidate models ranging between 3 and 50 topics. We then plot the exclusivity and semantic coherence (numbers closer to 0 indicate higher coherence), with a linear regression overlaid (Figure FIGREF3 ). Models above the regression line have a \u201cbetter\u201d exclusivity-semantic coherence trade off. We select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence. The topic quality is usually evaluated by highest probability words, which is presented in Figure FIGREF4 . Figure FIGREF4 provides a list of the main topics (and the highest probability words associated these topics) that emerge from the STM of UN General Debate statements.",
        "type": "Document"
      },
      {
        "id": "7e223432-1b91-4065-b079-3c2cc4af2517",
        "metadata": {
          "vector_store_key": "1708.05873-2",
          "chunk_id": 7,
          "document_id": "1708.05873",
          "start_idx": 3967,
          "end_idx": 4674
        },
        "page_content": "This enables us to identify the key international development topics discussed in the GD. We model topic prevalence in the context of the structural covariates. In addition, we control for region fixed effects and time trend. The aim is to allow the observed metadata to affect the frequency with which a topic is discussed in General Debate speeches. This allows us to test the degree of association between covariates (and region/time effects) and the average proportion of a document discussing a topic. We assess the optimal number of topics that need to be specified for the STM analysis. We follow the recommendations of the original STM paper and focus on exclusivity and semantic coherence measures.",
        "type": "Document"
      },
      {
        "id": "d9e575cf-43f4-48a8-8180-5926cbb54aa2",
        "metadata": {
          "vector_store_key": "1906.03538-2",
          "chunk_id": 39,
          "document_id": "1906.03538",
          "start_idx": 22440,
          "end_idx": 23117
        },
        "page_content": "The dataset presented here is not intended to be exhaustive, nor does it attempt to reflect a true distribution of the important claims and perspectives in the world, or to associate any of the perspective and identified evidence with levels of expertise and trustworthiness. Moreover, it is important to note that when we ask crowd-workers to evaluate the validity of perspectives and evidence, their judgement process can potentially be influenced by their prior beliefs BIBREF42 . To avoid additional biases introduced in the process of dataset construction, we try to take the least restrictive approach in filtering dataset content beyond the necessary quality assurances.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much, the proposed method improves BiDAF and DCN on SQuAD dataset?",
    "llm_answer": "The context states that Hybrid and DCA models perform better than BiDAF and Co-Attention, but does not quantify the improvement.  Therefore, the answer is not found in the context.\n",
    "context": [
      {
        "id": "2728e40a-66bd-4fcd-95d8-fcf1f01e14cf",
        "metadata": {
          "vector_store_key": "2001.11268-0",
          "chunk_id": 44,
          "document_id": "2001.11268",
          "start_idx": 25385,
          "end_idx": 26008
        },
        "page_content": "We never used the full SQuAD data in order to reduce time for training but observed increased performance when adding additional data. For classifying I entities, an increase from 20 to 200 additional SQuAD domains resulted in an increase of 8% for the F1 score, whereas the increase for the O domain was less than 1%. After training a model with 200 additional SQuAD domains, we also evaluated it on the original SQuAD development set and obtained a F1 score of 0.72 for this general reading comprehension task. In this evaluation, the F1 scores represent the overlap of labelled and predicted answer spans on token level.",
        "type": "Document"
      },
      {
        "id": "25f5452d-87b9-4719-820a-075c37fe06ee",
        "metadata": {
          "vector_store_key": "1803.09230-0",
          "chunk_id": 21,
          "document_id": "1803.09230",
          "start_idx": 11770,
          "end_idx": 12436
        },
        "page_content": "To ensure the generality of our model, we used Dropout technique for regularizing neural networks. We start our experiments with default hyperparameters: embedding size of 100, batch size 100, hidden size 200, learning rate of 0.001 and a dropout rate of 0.15. For character level encoding, default character embedding size is 20, kernel size is 5 and number of filters are 100. For each architecture, we report the evaluation metrics F1 and EM (Exact match) computed on the dev set. The effect of character embedding on the BiDAF model is reported in Table 1 . We can notice that character embedding boosts up the performance by roughly 2% for both EM and F1 score.",
        "type": "Document"
      },
      {
        "id": "23cb61b1-1e29-4c81-b4e0-11caae3eb342",
        "metadata": {
          "vector_store_key": "1803.09230-0",
          "chunk_id": 24,
          "document_id": "1803.09230",
          "start_idx": 13242,
          "end_idx": 13986
        },
        "page_content": "Visualizations demonstrate that both hybrid and DCA models perform better than vanilla Co-Attention and BiDAF attention mechanisms and reduce the losses faster and increase the dev F1/EM scores faster as well. We made a brief attempt to do a bit of hyperparameter tuning on our proposed DCA model and we report the results in Table 3 . Ideally, hyperparameter tuning for neural network architectures should be done using bayesian hyperparameter optimization but due to the lack of time we tried to do a random search on a small set of hyperparameters that we guessed could be more suitable. While, we didn't find any significantly good set of parameters, we noticed that reducing the hidden size has a minor effect on improving the performance.",
        "type": "Document"
      },
      {
        "id": "11d9962a-2577-495d-917c-943ce90262c9",
        "metadata": {
          "vector_store_key": "2001.11268-0",
          "chunk_id": 51,
          "document_id": "2001.11268",
          "start_idx": 29476,
          "end_idx": 30207
        },
        "page_content": "This was done in order to save computing resources, as an addition of 100 SQuAD domains resulted in training time increases of two hours, depending on various other parameter settings. Adjusted parameters include increased batch size, and decreased maximal context length in order to reduce training time. With this paper we aimed to explore state-of-the-art NLP methods to advance systematic review (semi)automation. Both of the presented fine-tuning approaches for transformers demonstrated flexibility and high performance. We contributed an approach to deal with ambiguity in whole-sentence predictions, and proposed the usage of a completely different approach to entity recognition in settings where training data are sparse.",
        "type": "Document"
      },
      {
        "id": "580cdf3a-fa5e-4cf7-94a8-c62dc9453aa1",
        "metadata": {
          "vector_store_key": "1803.09230-0",
          "chunk_id": 23,
          "document_id": "1803.09230",
          "start_idx": 12863,
          "end_idx": 13545
        },
        "page_content": "Hybrid and DCA have a slight edge over plain BiDAF and Co-Attention module as per the results. Co-Attention with char embedding was giving us worse results so we put the best numbers we got for Co-Attention there. We would like to point out that the BiDAF model here doesn't include BiLSTM layer as present in original paper because the BiLSTM didn't give any advantage except for slowing down the training. Selected tensorboard visualizations are also shown in Figure 3 . Visualizations demonstrate that both hybrid and DCA models perform better than vanilla Co-Attention and BiDAF attention mechanisms and reduce the losses faster and increase the dev F1/EM scores faster as well.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the linguistic differences between each class?",
    "llm_answer": "English and Spanish are suffixing languages with Spanish having larger verbal paradigms and internal stem changes.  Zulu is a strongly prefixing language.  Basque is a language isolate with frequent use of prefixes and suffixes.  The other languages mentioned (French, German, Hungarian, Italian, Navajo, Turkish, and Quechua) are included in the study but their specific linguistic differences are not detailed.\n",
    "context": [
      {
        "id": "50580b33-3f37-4708-bb48-721919a2f597",
        "metadata": {
          "vector_store_key": "1910.05456-7",
          "chunk_id": 18,
          "document_id": "1910.05456",
          "start_idx": 9647,
          "end_idx": 10340
        },
        "page_content": "Spanish (SPA), in contrast, is morphologically rich, and disposes of much larger verbal paradigms than English. Like English, it is a suffixing language, and it additionally makes use of internal stem changes (e.g., o $\\rightarrow $ ue). Since English and Spanish are both Indo-European languages, and, thus, relatively similar, we further add a third, unrelated target language. We choose Zulu (ZUL), a Bantoid language. In contrast to the first two, it is strongly prefixing. For pretraining, we choose languages with different degrees of relatedness and varying morphological similarity to English, Spanish, and Zulu. We limit our experiments to languages which are written in Latin script.",
        "type": "Document"
      },
      {
        "id": "51d4c28c-8fc4-4ec7-8232-9dff738c46ad",
        "metadata": {
          "vector_store_key": "1910.05456-2",
          "chunk_id": 3,
          "document_id": "1910.05456",
          "start_idx": 1695,
          "end_idx": 2489
        },
        "page_content": "We aim at finding answers to two main questions: (i) Do errors systematically differ between source languages? (ii) Do these differences seem explainable, given the properties of the source and target languages? In other words, we are interested in exploring if and how L2 acquisition of morphological inflection depends on the L1, i.e., the \"native language\", in neural network models. To this goal, we select a diverse set of eight source languages from different language families \u2013 Basque, French, German, Hungarian, Italian, Navajo, Turkish, and Quechua \u2013 and three target languages \u2013 English, Spanish and Zulu. We pretrain a neural sequence-to-sequence architecture on each of the source languages and then fine-tune the resulting models on small datasets in each of the target languages.",
        "type": "Document"
      },
      {
        "id": "f4cfd74b-5e32-48c1-89df-fee20b25162a",
        "metadata": {
          "vector_store_key": "1910.05456-7",
          "chunk_id": 19,
          "document_id": "1910.05456",
          "start_idx": 10340,
          "end_idx": 11079
        },
        "page_content": "We limit our experiments to languages which are written in Latin script. As an estimate for morphological similarity we look at the features from the Morphology category mentioned in The World Atlas of Language Structures (WALS). An overview of the available features as well as the respective values for our set of languages is shown in Table TABREF13. We decide on Basque (EUS), French (FRA), German (DEU), Hungarian (HUN), Italian (ITA), Navajo (NAV), Turkish (TUR), and Quechua (QVH) as source languages. Basque is a language isolate. Its inflectional morphology makes similarly frequent use of prefixes and suffixes, with suffixes mostly being attached to nouns, while prefixes and suffixes can both be employed for verbal inflection.",
        "type": "Document"
      },
      {
        "id": "200cc95f-4e37-473d-a087-aa810472d010",
        "metadata": {
          "vector_store_key": "1906.04571-4",
          "chunk_id": 20,
          "document_id": "1906.04571",
          "start_idx": 11477,
          "end_idx": 12107
        },
        "page_content": "We provide corpus statistics for both languages in the top two rows of tab:data. We created a hard-coded INLINEFORM0 to serve as a baseline for each language. For Spanish, we only activated, i.e. set to a number greater than zero, values that relate adjectives and determiners to nouns; for Hebrew, we only activated values that relate adjectives and verbs to nouns. We created two separate baselines because gender agreement operates differently in each language. To evaluate our approach, we held all morpho-syntactic subtags fixed except for gender. For each annotated sentence, we intervened on the gender of the animate noun.",
        "type": "Document"
      },
      {
        "id": "e037cf97-f9dc-4932-9456-9cc893e52668",
        "metadata": {
          "vector_store_key": "1808.09409-1",
          "chunk_id": 29,
          "document_id": "1808.09409",
          "start_idx": 16535,
          "end_idx": 17235
        },
        "page_content": "To better understand the overall results, we further look deep into the output by addressing the questions: What types of error negatively impact both systems over learner texts? What types of error are more problematic for the neural syntax-agnostic one over the L2 data but can be solved by the syntax-based one to some extent? We first carry out a suite of empirical investigations by breaking down error types for more detailed evaluation. To compare two systems, we analyze results on ENG-L2 and JPN-L2 given that they reflect significant advantages of the syntax-based systems over the neural syntax-agnostic system. Note that the syntax-based system here refers to the neural-parser-based one.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what genres do they songs fall under?",
    "llm_answer": "The songs fall under 14 genres.\n",
    "context": [
      {
        "id": "1ea655c6-a511-4998-9de5-4415c5075b9c",
        "metadata": {
          "vector_store_key": "2003.05377-3",
          "chunk_id": 1,
          "document_id": "2003.05377",
          "start_idx": 751,
          "end_idx": 1479
        },
        "page_content": "Music genres remain a poorly defined concept, and boundaries between genres still remain fuzzy, which makes the automatic classification problem a nontrivial task BIBREF1. Traditional approaches in text classification have applied algorithms such as Support Vector Machine (SVM) and Na\u00efve Bayes, combined with handcraft features (POS and chunk tags) and word count-based representations, like bag-of-words. More recently, the usage of Deep Learning methods such as Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) has produced great results in text classification tasks. Some works like BIBREF2, BIBREF3 BIBREF4 focus on classification of mood or sentiment of music based on its lyrics or audio content.",
        "type": "Document"
      },
      {
        "id": "f59c1b42-c556-43c8-be41-aea9aeb53621",
        "metadata": {
          "vector_store_key": "2003.05377-1",
          "chunk_id": 20,
          "document_id": "2003.05377",
          "start_idx": 10800,
          "end_idx": 11392
        },
        "page_content": "The bossa-nova and jovem-guarda genres, which have few instances in the dataset, are among the most difficult ones to classify using the model. The pop genre, by contrast, has a small distribution between the number of songs and the number of artists, and could not be well classified by our model. This may indicate that our model was unable to identify a pattern due to the low number of songs per artist, or that the song lyrics of this genre cover several subjects that are confused with other genres. Figure FIGREF22 shows the confusion matrix of the results produced by our BLSTM model.",
        "type": "Document"
      },
      {
        "id": "75ba05ce-b089-44ad-a332-5e180e4f00a0",
        "metadata": {
          "vector_store_key": "2003.05377-3",
          "chunk_id": 0,
          "document_id": "2003.05377",
          "start_idx": 0,
          "end_idx": 751
        },
        "page_content": "Music is part of the day-to-day life of a huge number of people, and many works try to understand the best way to classify, recommend, and identify similarities between songs. Among the tasks that involve music classification, genre classification has been studied widely in recent years BIBREF0 since musical genres are the main top-level descriptors used by music dealers and librarians to organize their music collections BIBREF1. Automatic music genre classification based only on the lyrics is considered a challenging task in the field of Natural Language Processing (NLP). Music genres remain a poorly defined concept, and boundaries between genres still remain fuzzy, which makes the automatic classification problem a nontrivial task BIBREF1.",
        "type": "Document"
      },
      {
        "id": "181c2e21-3521-4422-8730-4b5e3c806e67",
        "metadata": {
          "vector_store_key": "2003.05377-1",
          "chunk_id": 21,
          "document_id": "2003.05377",
          "start_idx": 11392,
          "end_idx": 12168
        },
        "page_content": "Figure FIGREF22 shows the confusion matrix of the results produced by our BLSTM model. We can notice that many instances of class forr\u00f3 are often confused with class sertanejo. Indeed, these two genres are very close. Both Forr\u00f3 and sertanejo have as theme the cultural and daily aspects of the Northeast region of Brazil. Instances of class infantil are often confused with class gospel: in infantil we have music for children for both entertainment and education. In some of the songs, songwriters try to address religious education, which could explain the confusion between those genres. The MPB (Brazilian Popular Music) genre was the most confused of all, which may indicate that song lyrics of this genre cover a wide range of subjects that intersect with other genres.",
        "type": "Document"
      },
      {
        "id": "15e83a67-f7fe-4e5a-82ce-f3396e09f61b",
        "metadata": {
          "vector_store_key": "2003.05377-1",
          "chunk_id": 22,
          "document_id": "2003.05377",
          "start_idx": 11898,
          "end_idx": 12630
        },
        "page_content": "The MPB (Brazilian Popular Music) genre was the most confused of all, which may indicate that song lyrics of this genre cover a wide range of subjects that intersect with other genres. In this work we constructed a dataset of $138,368$ Brazilian song lyrics distributed in 14 genres. We applied SVM, Random Forest, and a Bidirectional Long Short-Term Memory (BLSTM) network combined with different word embeddings techniques to address the automatic genre classification task based only on the song lyrics. We compared the results between the different combinations of classifiers and word embedding techniques, concluding that our BLSTM combined with the Wang2Vec pre-trained model obtained the best F1-score classification result.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "To what other competitive baselines is this approach compared?",
    "llm_answer": "CNN baseline, and various BERT-based models (RoBERTa, DistillRoBERTa, and ensembles of these).  Also, a baseline model for the dialogue task is mentioned.\n",
    "context": [
      {
        "id": "dcbb3e93-d5ad-43bf-80ac-ec1ae660eda5",
        "metadata": {
          "vector_store_key": "1812.07023-3",
          "chunk_id": 31,
          "document_id": "1812.07023",
          "start_idx": 17294,
          "end_idx": 18075
        },
        "page_content": "Their approach, however, is not public as of yet. We observe the following for our models: Since the official test set has not been released publicly, results reported on the official test set have been provided by the challenge organizers. For the prototype test set and for the ablation study presented in Table TABREF24 , we use the same code for evaluation metrics as used by BIBREF11 for fairness and comparability. We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below: Our primary architectural differences over the baseline model are: not concatenating the question, answer pairs before encoding them, the auxiliary decoder module, and using the Time-Extended FiLM module for feature extraction.",
        "type": "Document"
      },
      {
        "id": "84311e92-1a2e-4e47-b34d-d3e1f63e5141",
        "metadata": {
          "vector_store_key": "1910.08210-5",
          "chunk_id": 31,
          "document_id": "1910.08210",
          "start_idx": 17119,
          "end_idx": 17791
        },
        "page_content": "Moreover, no ablated variant is able to solve the tasks\u2014it is the combination of ablated features that enables to win consistently. Qualitatively, the ablated variants converge to locally optimum policies in which the agent often picks up a random item and then attacks the correct monster, resulting in a $\\sim 50$% win rate. Table FIGREF29 shows that all models, with the exception of the CNN baseline, generalise to new evaluation environments with dynamics and world configurations not seen during training, with outperforming FiLM and the CNN model. We find similar results for , its ablated variants, and baselines on other tasks (see appendix SECREF11 for details).",
        "type": "Document"
      },
      {
        "id": "814822da-592c-40ae-bb7c-e78761735daf",
        "metadata": {
          "vector_store_key": "2002.06644-3",
          "chunk_id": 10,
          "document_id": "2002.06644",
          "start_idx": 6316,
          "end_idx": 7055
        },
        "page_content": "Our proposed methodology, the use of finetuned optimized BERT based models, and BERT-based ensemble models outperform the baselines for all the metrics. Among the optimized BERT based models, $RoBERTa_{large}$ outperforms all other non-ensemble models and the baselines for all metrics. It further achieves a maximum recall of $0.681$ for all the proposed models. We note that DistillRoBERTa, a distilled model, performs competitively, achieving $69.69\\%$ accuracy, and $0.672$ F1 score. This observation shows that distilled pretrained models can replace their undistilled counterparts in a low-computing environment. We further observe that ensemble models perform better than optimized BERT-based models and distilled pretrained models.",
        "type": "Document"
      },
      {
        "id": "296e6eef-e401-4ad7-9d3b-ae9068a8267c",
        "metadata": {
          "vector_store_key": "1910.00458-8",
          "chunk_id": 43,
          "document_id": "1910.00458",
          "start_idx": 23845,
          "end_idx": 24490
        },
        "page_content": "From this table, we see that all three techniques can bring in improvements over the baseline model for the source dataset RACE, among which NLI coarse-tuning stage can help elevate the scores most. Since we found all parts of MMM can work well for the source dataset, we tried to use them to improve the accuracy on RACE. The results are shown in Table TABREF32. We used four kinds of pre-trained sentence encoders: BERT-Base, BERT-Large, XLNet-Large, and RoBERTa-Large. For each encoder, we listed the official report of scores from the leaderboard. Compared with the baselines, MMM leads to improvements ranging from 0.5% to 3.0% in accuracy.",
        "type": "Document"
      },
      {
        "id": "874d1312-9fdb-46fe-865b-ce75dcb445d6",
        "metadata": {
          "vector_store_key": "1812.07023-3",
          "chunk_id": 32,
          "document_id": "1812.07023",
          "start_idx": 17666,
          "end_idx": 18428
        },
        "page_content": "We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below: Our primary architectural differences over the baseline model are: not concatenating the question, answer pairs before encoding them, the auxiliary decoder module, and using the Time-Extended FiLM module for feature extraction. These, combined with using scheduled sampling and running hyperparameter optimization over the validation set to select hyperparameters, give us the observed performance boost. We observe that our models generate fairly relevant responses to questions in the dialogues, and models with audio-visual inputs respond to audio-visual questions (e.g. \u201cis there any voices or music ?\u201d) correctly more often.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is human evaluation performed, what was the criteria?",
    "llm_answer": "Human evaluators graded generated poems from four perspectives: Fluency, Semantic coherence, Semantic preservability, and Poeticness.  For dialogs, evaluators rated generated responses based on criteria including emotional positivity/negativity, and the first three utterances of each dialog were used as input.  Another group of English-speaking students evaluated generated responses from dialogs.\n",
    "context": [
      {
        "id": "4be44773-b407-4314-abd0-d9c32e99174b",
        "metadata": {
          "vector_store_key": "1908.07816-1",
          "chunk_id": 7,
          "document_id": "1908.07816",
          "start_idx": 4025,
          "end_idx": 4656
        },
        "page_content": "We consider factors such as the balance of positive and negative sentiments in test dialogs, a well-chosen range of topics, and dialogs that our human evaluators can relate. It is the first time such an approach is designed with consideration for the human judges. Our main goal is to increase the objectivity of the results and reduce judges' mistakes due to out-of-context dialogs they have to evaluate. The rest of the paper unfolds as follows. Section SECREF2 discusses some related work. In Section SECREF3, we give detailed description of the methodology. We present experimental results and some analysis in Section SECREF4.",
        "type": "Document"
      },
      {
        "id": "f2ccbf11-92ca-4546-b7d7-d1243199c4a9",
        "metadata": {
          "vector_store_key": "1909.00279-3",
          "chunk_id": 26,
          "document_id": "1909.00279",
          "start_idx": 15254,
          "end_idx": 15992
        },
        "page_content": "The human evaluators were divided into two groups. The expert group contains 15 people who hold a bachelor degree in Chinese literature, and the amateur group contains 15 people who holds a bachelor degree in other fields. All 30 human evaluators are native Chinese speakers. We ask evaluators to grade each generated poem from four perspectives: 1) Fluency: Is the generated poem grammatically and rhythmically well formed, 2) Semantic coherence: Is the generated poem itself semantic coherent and meaningful, 3) Semantic preservability: Does the generated poem preserve the semantic of the modern Chinese translation, 4) Poeticness: Does the generated poem display the characteristic of a poem and does the poem build good poetic image.",
        "type": "Document"
      },
      {
        "id": "174627da-e53d-491f-84f7-65d4806ef65d",
        "metadata": {
          "vector_store_key": "2004.01694-4",
          "chunk_id": 62,
          "document_id": "2004.01694",
          "start_idx": 35148,
          "end_idx": 35678
        },
        "page_content": "To that end, we have offered a set of recommendations, supported by empirical data, which we believe are needed for assessing human\u2013machine parity, and will strengthen the human evaluation of MT in general. Our recommendations have the aim of increasing the validity of MT evaluation, but we are aware of the high cost of having MT evaluation done by professional translators, and on the level of full documents. We welcome future research into alternative evaluation protocols that can demonstrate their validity at a lower cost.",
        "type": "Document"
      },
      {
        "id": "8c81c383-8e40-4094-8e18-ab0a2d32126a",
        "metadata": {
          "vector_store_key": "1905.08949-6",
          "chunk_id": 18,
          "document_id": "1905.08949",
          "start_idx": 10340,
          "end_idx": 11074
        },
        "page_content": "Although the datasets are commonly shared between QG and QA, it is not the case for evaluation: it is challenging to define a gold standard of proper questions to ask. Meaningful, syntactically correct, semantically sound and natural are all useful criteria, yet they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks. As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE BIBREF43 , are also widely used.",
        "type": "Document"
      },
      {
        "id": "b360d511-f50b-49b0-adae-b9cdd19697b2",
        "metadata": {
          "vector_store_key": "1908.07816-1",
          "chunk_id": 33,
          "document_id": "1908.07816",
          "start_idx": 19008,
          "end_idx": 19804
        },
        "page_content": "Thus each person produced 25 dialogs, and in total we obtained 50 emotionally negative daily dialogs in addition to the 14 already available. To form the test set, we randomly selected 50 emotionally positive and 50 emotionally negative dialogs from the two pools of dialogs described above (78 positive dialogs from DailyDialog, 64 negative dialogs from DailyDialog and human-generated). For human evaluation of the models, we recruited another four English-speaking students from our university without any relationship to the authors' lab to rate the responses generated by the models. Specifically, we randomly shuffled the 100 dialogs in the test set, then we used the first three utterances of each dialog as the input to the three models being compared and let them generate the responses.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much better were results of the proposed models than base LSTM-RNN model?",
    "llm_answer": "The context states that the Base model performed the best, except on French and Manipuri datasets.  It also says the proposed LSTM models achieved similar test perplexity to previous work.  It does *not* state how much better the proposed models are than the base model.\n",
    "context": [
      {
        "id": "83adde3f-dc7d-4e4a-aee6-15eca11050a1",
        "metadata": {
          "vector_store_key": "1909.13362-1",
          "chunk_id": 36,
          "document_id": "1909.13362",
          "start_idx": 21332,
          "end_idx": 21946
        },
        "page_content": "Another model, Small, uses the same architecture as Base but reduces the number of convolutional layers to 1, the convolutional filters to 40, the LSTM dimension $l$ to 50, and the phone embedding size $d$ to 100. We also tested a Base-Softmax model, which replaces the CRF output of the Base model with a softmax. A comparison of the results of these three models can be seen in Table TABREF25. This comparison empirically motivates the CRF output because Base almost always outperforms Base-Softmax. Of these three models, the Base model performed the best with the exception of the French and Manipuri datasets.",
        "type": "Document"
      },
      {
        "id": "3e5aee15-2ccb-4886-bb59-5110e5ca8a15",
        "metadata": {
          "vector_store_key": "2002.02492-1",
          "chunk_id": 44,
          "document_id": "2002.02492",
          "start_idx": 25262,
          "end_idx": 25989
        },
        "page_content": "The choice of $\\tanh $ and LSTM RNNs implies that all of the recurrent language models that we train are consistent according to Lemma UNKREF23. Our LSTM models achieve similar test perplexity ($91.86 \\pm 0.4$) to those reported in previous work BIBREF24; see Appendix for further details. Additionally, we train self-terminating $\\tanh $-RNN and LSTM-RNN variants (Definition UNKREF33) at various values of $\\epsilon $, which controls a lower bound on the termination probability at each step. We use $\\sigma (x)=(1-\\epsilon )\\text{sigmoid}(x)$. We use the hyper-parameters selected in the preceding grid search. In this experiment, we demonstrate evidence of inconsistency with incomplete decoding methods (Theorem UNKREF27).",
        "type": "Document"
      },
      {
        "id": "f453b9b4-4706-4b66-b64e-a8b99ef5cdd2",
        "metadata": {
          "vector_store_key": "1905.11037-0",
          "chunk_id": 15,
          "document_id": "1905.11037",
          "start_idx": 8646,
          "end_idx": 9596
        },
        "page_content": "The latter performed better in general, although vanilla models achieved a higher performance for actions that occurred a few times in the training set. An analysis over the output of the lstm approach also revealed difficulties to discriminate among semantically related actions. The challenge here proposed corresponded to a fictional domain. A future line of work we are interested in is to test whether the knowledge learned with this dataset could be transferred to real-word actions (i.e. real-domain setups), or if such transfer is not possible and a model needs to be trained from scratch. This work has received support from the TELEPARES-UDC project (FFI2014-51978-C2-2-R) and the ANSWER-ASAP project (TIN2017-85160-C2-1-R) from MINECO, and from Xunta de Galicia (ED431B 2017/01), and from the European Research Council (ERC), under the European Union's Horizon 2020 research and innovation programme (FASTPARSE, grant agreement No 714150).",
        "type": "Document"
      },
      {
        "id": "5f712cdd-f7ca-421d-b6c6-81178c228e50",
        "metadata": {
          "vector_store_key": "1706.08032-2",
          "chunk_id": 28,
          "document_id": "1706.08032",
          "start_idx": 16200,
          "end_idx": 16915
        },
        "page_content": "We evaluate the efficiency of SR for the model in Table V of our full paper . We also conduct two experiments on two separate models: DeepCNN and Bi-LSTM in order to show the effectiveness of combination of DeepCNN and Bi-LSTM. In addition, the model using TwitterGlove outperform the model using GoogleW2V because TwitterGlove captures more information in Twitter than GoogleW2V. These results show that the character-level information and SR have a great impact on Twitter Data. The pre-train word vectors are good, universal feature extractors. The difference between our model and other approaches is the ability of our model to capture important features by using SR and combine these features at high benefit.",
        "type": "Document"
      },
      {
        "id": "98948535-0e36-49a4-a3ca-aabea669a302",
        "metadata": {
          "vector_store_key": "1811.00942-1",
          "chunk_id": 11,
          "document_id": "1811.00942",
          "start_idx": 6755,
          "end_idx": 7459
        },
        "page_content": "BIBREF13 ). Preprocessed by BIBREF14 , PTB contains 887K tokens for training, 70K for validation, and 78K for test, with a vocabulary size of 10,000. On the other hand, WT103 comprises 103 million tokens for training, 217K for validation, and 245K for test, spanning a vocabulary of 267K unique tokens. For the neural language model, we used a four-layer QRNN BIBREF10 , which achieves state-of-the-art results on a variety of datasets, such as WT103 BIBREF11 and PTB. To compare against more common LSTM architectures, we also evaluated AWD-LSTM BIBREF4 on PTB. For the non-neural approach, we used a standard five-gram model with modified Kneser-Ney smoothing BIBREF15 , as explored in BIBREF16 on PTB.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which one of the four proposed models performed best?",
    "llm_answer": "Hybrid model\n",
    "context": [
      {
        "id": "11bbd522-28ef-4580-9084-84720bfb9758",
        "metadata": {
          "vector_store_key": "2001.06888-0",
          "chunk_id": 42,
          "document_id": "2001.06888",
          "start_idx": 22556,
          "end_idx": 22770
        },
        "page_content": "Evaluation results of our proposed model compared to other state of the art methods show its superiority to these methods in overall while in two categories (Person and Miscellaneous) our model outperformed others.",
        "type": "Document"
      },
      {
        "id": "33263f7a-b742-485d-8662-a59178bf4ca3",
        "metadata": {
          "vector_store_key": "1909.13362-0",
          "chunk_id": 37,
          "document_id": "1909.13362",
          "start_idx": 21946,
          "end_idx": 22603
        },
        "page_content": "Of these three models, the Base model performed the best with the exception of the French and Manipuri datasets. The differences in the French results can be considered negligible because the accuracies are all near $100\\%$. The Small model performed best on Manipuri, which may suggest that reducing the number of parameters of the Base model leads to better accuracy on smaller datasets. When comparing our model with previous syllabifiers, we consider the Base model exclusively. In Table TABREF26, a side-by-side comparison of our Base model to a selection of published syllabifiers shows that Base is near state-of-the art performance on English CELEX.",
        "type": "Document"
      },
      {
        "id": "ad1d7753-3848-45ad-a7ec-2f9bff0e735c",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 18,
          "document_id": "1912.13109",
          "start_idx": 9307,
          "end_idx": 10125
        },
        "page_content": "While the model probability is the probability that the observation i belongs to category c. Among the model architectures we experimented with and without data augmentation were: Fully Connected dense networks: Model hyperparameters were inspired from the previous work done by Vo et al and Mathur et al. This was also used as a baseline model but we did not get appreciable performance on such architecture due to FC networks not being able to capture local and long term dependencies. Convolution based architectures: Architecture and hyperparameter choices were chosen from the past study Deon on the subject. We were able to boost the performance as compared to only FC based network but we noticed better performance from architectures that are suitable to sequences such as text messages or any timeseries data.",
        "type": "Document"
      },
      {
        "id": "52a5c7b9-197c-4c52-9a22-93a7f3f1bb7a",
        "metadata": {
          "vector_store_key": "1910.12618-2",
          "chunk_id": 45,
          "document_id": "1910.12618",
          "start_idx": 24114,
          "end_idx": 24750
        },
        "page_content": "In order to tone it down and to increase the consistency of our results, the different models are run $B=10$ times. The results presented hereafter correspond to the average and standard-deviation on those runs. The RF model denoted as \"sel\" is the one with the reduced number of features, whereas the other RF uses the full vocabulary. We also considered an aggregated forecaster (abridged Agg), consisting of the average of the two best individual ones in terms of RMSE. All the neural network methods have a reduced vocabulary size $V^*$. The results for the French and UK data are respectively given by tables TABREF26 and TABREF27.",
        "type": "Document"
      },
      {
        "id": "bbc12882-454b-4106-942a-a7df2d750084",
        "metadata": {
          "vector_store_key": "2001.05467-3",
          "chunk_id": 31,
          "document_id": "2001.05467",
          "start_idx": 17277,
          "end_idx": 17972
        },
        "page_content": "The F1 results are shown in Table TABREF24. Among all single models, LFT performs the best, followed by MinAvgOut. RL is also comparable with previous state-of-the-art models VHRED (attn) and Reranking-RL. We think that this is because LFT exerts no force in pulling the model predictions away from the ground-truth tokens, but rather just makes itself aware of how dull each response is. Consequently, its responses appear more relevant than the other two approaches. Moreover, the hybrid model (last row) outperforms all other models by a large margin. One might expect that minimizing AVGOUT causes the models to move further away from the ground-truth tokens, so that it will hurt relevance.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much is proposed model better than baselines in performed experiments?",
    "llm_answer": "The proposed model outperforms baselines by 0.5% to 3.0% in accuracy, 0.18 points on newstest2017 (statistically significant), 0.15 BLEU points on newstest2018 (no statistical evidence), 1.09 BLEU and 0.85 TER points on tst2015, 0.75 BLEU and 0.76 TER points on tst2014, 0.48 BLEU and 0.68 TER points on tst2013, and a clear margin in new environments.\n",
    "context": [
      {
        "id": "14c77096-531d-4d71-9236-61c2ccdb8cfa",
        "metadata": {
          "vector_store_key": "1910.00458-6",
          "chunk_id": 44,
          "document_id": "1910.00458",
          "start_idx": 24199,
          "end_idx": 24865
        },
        "page_content": "Compared with the baselines, MMM leads to improvements ranging from 0.5% to 3.0% in accuracy. Our best result is obtained by the RoBERTa-Large encoder. In order to investigate how well our model performs for different types of questions, we did an error analysis by first randomly selecting 150 samples that had wrong predictions by the BERT-Base baseline model from the development set of DREAM dataset. We then manually classified them into several question types, as shown in Table TABREF34. The annotation criterion is described in the Section 3 of the Supplementary Material. We see that the BERT-Base baseline model still does not do well on matching problems.",
        "type": "Document"
      },
      {
        "id": "c938d47c-e132-4294-bd51-25295fb776d2",
        "metadata": {
          "vector_store_key": "1603.07044-3",
          "chunk_id": 28,
          "document_id": "1603.07044",
          "start_idx": 15079,
          "end_idx": 15744
        },
        "page_content": "Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited training data is given, both feature-rich based system and our system are worse than the IR system. For task C, our system also got comparable results with the feature-rich based system. If we do a simple system combination (average the rank score) between our system and the IR system, the combined system will give large gains on tasks B and C.",
        "type": "Document"
      },
      {
        "id": "874d1312-9fdb-46fe-865b-ce75dcb445d6",
        "metadata": {
          "vector_store_key": "1812.07023-3",
          "chunk_id": 32,
          "document_id": "1812.07023",
          "start_idx": 17666,
          "end_idx": 18428
        },
        "page_content": "We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below: Our primary architectural differences over the baseline model are: not concatenating the question, answer pairs before encoding them, the auxiliary decoder module, and using the Time-Extended FiLM module for feature extraction. These, combined with using scheduled sampling and running hyperparameter optimization over the validation set to select hyperparameters, give us the observed performance boost. We observe that our models generate fairly relevant responses to questions in the dialogues, and models with audio-visual inputs respond to audio-visual questions (e.g. \u201cis there any voices or music ?\u201d) correctly more often.",
        "type": "Document"
      },
      {
        "id": "d2e2382c-8e5a-4f0c-92c5-2f119e6e1178",
        "metadata": {
          "vector_store_key": "1910.07481-0",
          "chunk_id": 20,
          "document_id": "1910.07481",
          "start_idx": 11787,
          "end_idx": 12518
        },
        "page_content": "It also surpassed the Baselinee by 0.18 points on the newstest2017 with strong statistical significance, and by 0.15 BLEU points on the newstest2018 but this time with no statistical evidence. These encouraging results prompted us to extend experiments to another language pair: English-French. En$\\rightarrow $Fr: The Document system obtained the best results considering all metrics on all test sets with strong statistical evidence. It surpassed the Baseline by 1.09 BLEU points and 0.85 TER points on tst2015, 0.75 BLEU points and 0.76 TER points on tst2014, and 0.48 BLEU points and 0.68 TER points on tst2013. Fr$\\rightarrow $En: Of all experiments, this language pair shows the most important improvements over the Baseline.",
        "type": "Document"
      },
      {
        "id": "6824b4f2-136e-4c26-a2a7-a10cf7d4c97e",
        "metadata": {
          "vector_store_key": "1810.00663-8",
          "chunk_id": 46,
          "document_id": "1810.00663",
          "start_idx": 25788,
          "end_idx": 26550
        },
        "page_content": "The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as can be seen in performance drops in Table TABREF28 for the new environments. Nonetheless, the insights from the previous section still hold: masking in the output layer and reordering the graph triplets tend to increase performance. Even though the results in Table TABREF28 suggest that there is room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much is precission increased?",
    "llm_answer": "Answer not found in the context.\n",
    "context": [
      {
        "id": "6e9156a1-1bd2-4904-9fe1-0957c0410a9d",
        "metadata": {
          "vector_store_key": "1909.13375-3",
          "chunk_id": 43,
          "document_id": "1909.13375",
          "start_idx": 22785,
          "end_idx": 23407
        },
        "page_content": "Not using the simple preprocessing from Section SECREF17 resulted in a 2.5 point decrease in both EM and F1. The numeric questions were the most affected, with their performance dropping by 3.5 points. Given that number questions make up about 61% of the dataset, we can deduce that our improved number handling is responsible for about a 2.1 point gain, while the rest could be be attributed to the improved Wikipedia parsing. Although NER span cleaning (Section SECREF23) affected only 3% of the multi-span questions, it provided a solid improvement of 5.4 EM in multi-span questions and 1.5 EM in single-span questions.",
        "type": "Document"
      },
      {
        "id": "c86113ee-192d-4181-b86f-2c801466130d",
        "metadata": {
          "vector_store_key": "1904.05862-2",
          "chunk_id": 25,
          "document_id": "1904.05862",
          "start_idx": 13422,
          "end_idx": 14093
        },
        "page_content": "Accuracy steadily increases with more data for pre-training and the best accuracy is achieved when we use the largest amount of data for pre-training. In this section we analyze some of the design choices we made for . We pre-train on the 80 hour subset of clean Librispeech and evaluate on TIMIT. Table shows that increasing the number of negative samples only helps up to ten samples. Thereafter, performance plateaus while training time increases. We suspect that this is because the training signal from the positive samples decreases as the number of negative samples increases. In this experiment, everything is kept equal except for the number of negative samples.",
        "type": "Document"
      },
      {
        "id": "aeb49bd5-6d5f-426e-b65a-13de0d015228",
        "metadata": {
          "vector_store_key": "1708.05873-1",
          "chunk_id": 25,
          "document_id": "1708.05873",
          "start_idx": 14555,
          "end_idx": 15151
        },
        "page_content": "However, with Topic 7 there is an increase in expected topic proportion until countries have a population of around 300 million, after which there is a decline in discussion of Topic 7. For countries with populations larger than 500 million there is no effect of population on discussion of Topic 7. It is only with the very largest populations that we see a positive effect on discussion of Topic 7. We would also expect the extent to which states discuss international development in their GD statements to be impacted by the amount of aid or official development assistance (ODA) they receive.",
        "type": "Document"
      },
      {
        "id": "98cf4de5-8e42-4028-8656-1fa707000b4e",
        "metadata": {
          "vector_store_key": "1904.05862-3",
          "chunk_id": 26,
          "document_id": "1904.05862",
          "start_idx": 13856,
          "end_idx": 14574
        },
        "page_content": "In this experiment, everything is kept equal except for the number of negative samples. Next, we analyze the effect of data augmentation through cropping audio sequences (\u00a7 SECREF11 ). When creating batches we crop sequences to a pre-defined maximum length. Table shows that a crop size of 150K frames results in the best performance. Not restricting the maximum length (None) gives an average sequence length of about 207K frames and results in the worst accuracy. This is most likely because the setting provides the least amount of data augmentation. Table shows that predicting more than 12 steps ahead in the future does not result in better performance and increasing the number of steps increases training time.",
        "type": "Document"
      },
      {
        "id": "097dcb3f-7129-4a69-8615-32f32df0ff09",
        "metadata": {
          "vector_store_key": "1707.03904-0",
          "chunk_id": 35,
          "document_id": "1707.03904",
          "start_idx": 19449,
          "end_idx": 20060
        },
        "page_content": "For these, we refer to the fraction of instances for which the correct answer is present in the context as Search Accuracy. The performance of the baseline among these instances is referred to as the Reading Accuracy, and the overall performance (which is a product of the two) is referred to as the Overall Accuracy. In Figure 4 we compare how these three vary as the number of context documents is varied. Naturally, the search accuracy increases as the context size increases, however at the same time reading performance decreases since the task of extracting the answer becomes harder for longer documents.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What labels are in the dataset?",
    "llm_answer": "Location (LOC), Person (PER), Organization (ORG), Miscellaneous (TABREF15), Entailment, Contradiction, Neutral.\n",
    "context": [
      {
        "id": "731d76b9-6365-4a8c-872e-c14badc3b626",
        "metadata": {
          "vector_store_key": "1911.07228-0",
          "chunk_id": 12,
          "document_id": "1911.07228",
          "start_idx": 6115,
          "end_idx": 6702
        },
        "page_content": "The dataset contains four different types of label: Location (LOC), Person (PER), Organization (ORG) and Miscellaneous - Name of an entity that do not belong to 3 types above (Table TABREF15). Although the corpus has more information about the POS and chunks, but we do not use them as features in our model. There are two folders with 267 text files of training data and 45 text files of test data. They all have their own format. We take 21 first text files and 22 last text files and 22 sentences of the 22th text file and 55 sentences of the 245th text file to be a development data.",
        "type": "Document"
      },
      {
        "id": "8a7ecb64-b63d-404c-8ff0-5e668dcbbc1e",
        "metadata": {
          "vector_store_key": "2004.03744-3",
          "chunk_id": 5,
          "document_id": "2004.03744",
          "start_idx": 3088,
          "end_idx": 3860
        },
        "page_content": "There are three possible labels: Entailment: if there is enough evidence in $P_{image}$ to conclude that $H_{text}$ is true. Contradiction: if there is enough evidence in $P_{image}$ to conclude that $H_{text}$ is false. Neutral: if neither of the earlier two are true. The SNLI-VE dataset proposed by Xie BIBREF1 is the combination of Flickr30k, a popular image dataset for image captioning BIBREF2 and SNLI, an influential dataset for natural language inference BIBREF0. Textual premises from SNLI are replaced with images from Flickr30k, which is possible, as these premises were originally collected as captions of these images (see Figure FIGREF3). However, in practice, a sensible proportion of labels are wrong due to the additional information contained in images.",
        "type": "Document"
      },
      {
        "id": "f870a55c-6138-4670-8d8e-7c4a62a3e961",
        "metadata": {
          "vector_store_key": "1909.00124-1",
          "chunk_id": 21,
          "document_id": "1909.00124",
          "start_idx": 12190,
          "end_idx": 12874
        },
        "page_content": "For each clean-labeled dataset, the sentences are randomly partitioned into training set and test set with $80\\%$ and $20\\%$, respectively. Following BIBREF25, We also randomly select $10\\%$ of the test data for validation to check the model during training. Summary statistics of the training, validation, and test data are shown in Table TABREF9. Noisy-labeled Training Datasets. For the above three domains (movie, laptop, and restaurant), we collected 2,000 reviews for each domain from the same review source. We extracted sentences from each review and assigned review's label to its sentences. Like previous work, we treat 4 or 5 stars as positive and 1 or 2 stars as negative.",
        "type": "Document"
      },
      {
        "id": "b793e1d3-56ff-490e-9245-8753da5bf16c",
        "metadata": {
          "vector_store_key": "1911.07228-2",
          "chunk_id": 17,
          "document_id": "1911.07228",
          "start_idx": 8542,
          "end_idx": 9180
        },
        "page_content": "In our experiments, we use three evaluation parameters (precision, recall, and F1 score) to access our experimental result. They will be described as follow in Table 3. The \"correctNE\", the number of correct label for entity that the model can found. The \"goldNE\", number of the real label annotated by annotator in the gold data. The \"foundNE\", number of the label the model find out (no matter if they are correct or not). In Table 3 above, we can see that recall score on ORG label is lowest. The reason is almost all the ORG label on test file is name of some brands that do not appear on training data and pre-trained word embedding.",
        "type": "Document"
      },
      {
        "id": "0ed71a6b-a1aa-4a60-a95d-09b3534a2593",
        "metadata": {
          "vector_store_key": "1707.02377-4",
          "chunk_id": 35,
          "document_id": "1707.02377",
          "start_idx": 20784,
          "end_idx": 21482
        },
        "page_content": "The 100 categories includes categories under sports, entertainment, literature, and politics etc. Examples of categories include American drama films, Directorial debut films, Major League Baseball pitchers and Sydney Swans players. Body texts (the second paragraph) were extracted for each page as a document. For each category, we select 1,000 documents with unique category label, and 100 documents were used for training and 900 documents for testing. The remaining documents are used as unlabeled data. The 100 classes are balanced in the training and testing sets. For this data set, we learn the word embedding and document representation for all the algorithms using all the available data.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are customer satisfaction, customer frustration and overall problem resolution data collected?",
    "llm_answer": "Conversations were labeled with majority consensus, excluding those with majority \"can't tell\" labels.  The inherent imbalance in the data was retained.\n",
    "context": [
      {
        "id": "ee3b1126-8e6e-4440-90ac-388f8582eb8f",
        "metadata": {
          "vector_store_key": "1709.05413-7",
          "chunk_id": 64,
          "document_id": "1709.05413",
          "start_idx": 34645,
          "end_idx": 35407
        },
        "page_content": "Each classification experiments centers around one of three problem outcomes: customer satisfaction, problem resolution, and customer frustration. For each outcome, we remove any conversation that did not receive majority consensus for a label, or received majority vote of \"can't tell\". Our final conversation sets consist of 216 satisfied and 500 unsatisfied customer conversations, 271 resolved and 425 unresolved problem conversations, and 534 frustrated and 229 not frustrated customer conversations. We retain the inherent imbalance in the data to match the natural distribution observed. The clear excess of consensus of responses that indicate negative outcomes further motivates us to understand what sorts of dialogic patterns results in such outcomes.",
        "type": "Document"
      },
      {
        "id": "07e99ede-25de-4826-8afa-741f20243e86",
        "metadata": {
          "vector_store_key": "1709.05413-7",
          "chunk_id": 72,
          "document_id": "1709.05413",
          "start_idx": 38816,
          "end_idx": 39583
        },
        "page_content": "To help guide our analysis, we divide the features into positions based on where they occur in the conversation: start (turns 1-3), middle (turns 4-6), and end (turns 7-10). Desirable outcomes (customers that are satisfied/not frustrated and resolved problems) are shown at the top rows of the table, and undesirable outcomes (unsatisfied/frustrated customers and unresolved problems) are shown at the bottom rows. Our analysis helps zone in on how the use of certain dialogue acts may be likely to result in different outcomes. The weights we observe vary in the amount of insight provided: for example, offering extra help at the end of a conversation, or thanking the customer yields more satisfied customers, and more resolved problems (with ratios of above 6:1).",
        "type": "Document"
      },
      {
        "id": "62107c36-6b58-4bf8-b35a-c26b5b3546d4",
        "metadata": {
          "vector_store_key": "1709.05413-7",
          "chunk_id": 73,
          "document_id": "1709.05413",
          "start_idx": 39583,
          "end_idx": 40308
        },
        "page_content": "The weights we observe vary in the amount of insight provided: for example, offering extra help at the end of a conversation, or thanking the customer yields more satisfied customers, and more resolved problems (with ratios of above 6:1). However, some outcomes are much more subtle: for example, asking yes-no questions early-on in a conversation is highly associated with problem resolution (ratio 3:1), but asking them at the end of a conversation has as similarly strong association with unsatisfied customers. Giving elaborate answers that are not a simple affirmative, negative, or response acknowledgement (i.e. Answer (Other)) towards the middle of a conversation leads to satisfied customers that are not frustrated.",
        "type": "Document"
      },
      {
        "id": "7206b3c3-54b1-46ff-b4db-0fe35d1d632d",
        "metadata": {
          "vector_store_key": "1709.05413-7",
          "chunk_id": 74,
          "document_id": "1709.05413",
          "start_idx": 39860,
          "end_idx": 40646
        },
        "page_content": "Giving elaborate answers that are not a simple affirmative, negative, or response acknowledgement (i.e. Answer (Other)) towards the middle of a conversation leads to satisfied customers that are not frustrated. Likewise, requesting information towards the end of a conversation (implying that more information is still necessary at the termination of the dialogue) leads to unsatisfied and unresolved customers, with ratios of at least 4:1. By using the feature weights we derive from using our predicted dialogue acts in our outcome classification experiments, we can thus derive data-driven patterns that offer useful insight into good/bad practices. Our goal is to then use these rules as guidelines, serving as a basis for automated response planning in the customer service domain.",
        "type": "Document"
      },
      {
        "id": "91702fda-46bb-47e0-9af8-8b7db8b245f2",
        "metadata": {
          "vector_store_key": "1709.05413-9",
          "chunk_id": 78,
          "document_id": "1709.05413",
          "start_idx": 42175,
          "end_idx": 43124
        },
        "page_content": "We conduct binary classification experiments to analyze how our predicted dialogue acts can be used to classify conversations as ending in customer satisfaction, customer frustration, and problem resolution. We observe interesting correlations between the dialogue acts agents use and the outcomes, offering insights into good/bad practices that are more useful for creating context-aware automated customer service systems than generating canned response templates. Future directions for this work revolve around the integration of the insights derived in the design of automated customer service systems. To this end, we aim to improve the taxonomy and annotation design by consulting domain-experts and using annotator feedback and agreement information, derive more powerful features for dialogue act prediction, and automate ranking and selection of best-practice rules based on domain requirements for automated customer service system design.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many improvements on the French-German translation benchmark?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "d2e2382c-8e5a-4f0c-92c5-2f119e6e1178",
        "metadata": {
          "vector_store_key": "1910.07481-0",
          "chunk_id": 20,
          "document_id": "1910.07481",
          "start_idx": 11787,
          "end_idx": 12518
        },
        "page_content": "It also surpassed the Baselinee by 0.18 points on the newstest2017 with strong statistical significance, and by 0.15 BLEU points on the newstest2018 but this time with no statistical evidence. These encouraging results prompted us to extend experiments to another language pair: English-French. En$\\rightarrow $Fr: The Document system obtained the best results considering all metrics on all test sets with strong statistical evidence. It surpassed the Baseline by 1.09 BLEU points and 0.85 TER points on tst2015, 0.75 BLEU points and 0.76 TER points on tst2014, and 0.48 BLEU points and 0.68 TER points on tst2013. Fr$\\rightarrow $En: Of all experiments, this language pair shows the most important improvements over the Baseline.",
        "type": "Document"
      },
      {
        "id": "31b6acb2-eae8-4f9b-8688-f96eeef1b74f",
        "metadata": {
          "vector_store_key": "1910.07481-0",
          "chunk_id": 18,
          "document_id": "1910.07481",
          "start_idx": 10571,
          "end_idx": 11391
        },
        "page_content": "We provide evidence of this phenomena with an additional system for the French-English language pair, noted Document+tuning (see Table TABREF7) that is identical to the Document model except that it adjusts its embeddings during training. The evaluated models are obtained by taking the average of their last 6 checkpoints, which were written at 5000 steps intervals. All experiments are run 8 times with different seeds to ensure the statistical robustness of our results. We provide p-values that indicate the probability of observing similar or more extreme results if the Document model is actually not superior to the Baseline. Table TABREF6 presents results associated to the experiments for the English to German translation task, models are evaluated on the newstest2017, neswtest2018 and newstest2019 test sets.",
        "type": "Document"
      },
      {
        "id": "6f8fdba5-d9ee-4056-9d5a-e7a7b5ddbdfe",
        "metadata": {
          "vector_store_key": "1809.04686-0",
          "chunk_id": 29,
          "document_id": "1809.04686",
          "start_idx": 16288,
          "end_idx": 16984
        },
        "page_content": "If we assume the translation to be perfect, we should expect this approach to perform as well as the English classifier. The Amazon Reviews and SNLI tasks have a French test set available, and we evaluate the performance of the bridged and zero-shot systems on each French set. However, the SST dataset does not have a French test set, hence the `pseudo French' test set described in Section UID14 is used to evaluate the zero-shot performance. We use the English accuracy scores from the SST column in Table 1 as a high-quality proxy for the SST bridged system. We do this since translating the `pseudo French' back to English will result in two distinct translation steps and hence more errors.",
        "type": "Document"
      },
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      },
      {
        "id": "eb2f2f8b-9bc1-4ba6-ac71-4681cb392670",
        "metadata": {
          "vector_store_key": "1911.00069-1",
          "chunk_id": 40,
          "document_id": "1911.00069",
          "start_idx": 23170,
          "end_idx": 23915
        },
        "page_content": "In the tables, we also provide the performance of the supervised RE model (Bi-LSTM) for each target language, which is trained with a few hundred thousand tokens of manually annotated RE data in the target-language, and may serve as an upper bound for the cross-lingual model transfer performance. Among the 2 neural network models, the Bi-LSTM model achieves a better cross-lingual RE performance than the CNN model for 6 out of the 7 target languages. In terms of absolute performance, the Bi-LSTM model achieves over $40.0$ $F_1$ scores for German, Spanish, Portuguese and Chinese. In terms of relative performance, it reaches over $75\\%$ of the accuracy of the supervised target-language RE model for German, Spanish, Italian and Portuguese.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they prevent the model complexity increasing with the increased number of slots?",
    "llm_answer": "The model avoids slot-specific parameters and incorporates better feature representation using syntactic information and CNNs, relying on semantic similarity between slots and utterances instead.\n",
    "context": [
      {
        "id": "597cc589-c06f-4244-b08d-edbb7f7d32fb",
        "metadata": {
          "vector_store_key": "1909.11833-0",
          "chunk_id": 4,
          "document_id": "1909.11833",
          "start_idx": 2466,
          "end_idx": 3207
        },
        "page_content": "Our model complexity does not increase when the number of slots in dialogue tasks go up. Thus, SIM has many fewer parameters than existing dialogue state tracking models. To compensate for the exclusion of slot-specific parameters, we incorporate better feature representation of user utterance and dialogue states using syntactic information and convolutional neural networks (CNN). The refined representation, in addition to cross and self-attention mechanisms, make our model achieve even better performance than slot-specific models. For instance, on Wizard-of-Oz (WOZ) 2.0 dataset BIBREF8, the SIM model obtains a joint-accuracy score of 89.5%, 1.4% higher than the previously best model GLAD, with only 22% of the number of parameters.",
        "type": "Document"
      },
      {
        "id": "abe85bde-9b7b-4bfb-9688-0c11e182137a",
        "metadata": {
          "vector_store_key": "1909.00754-7",
          "chunk_id": 30,
          "document_id": "1909.00754",
          "start_idx": 15367,
          "end_idx": 16012
        },
        "page_content": "Since the name slot rarely occurs in the dataset, it is not included in our experiments, following previous literature BIBREF3 , BIBREF20 . Our model is also tested on the multi-domain dataset, MultiWoZ BIBREF9 . It has a more complex ontology with 7 domains and 25 predefined slots. Since the combined slot-value pairs representation of the belief states has to be applied for the model with $O(n)$ ITC, the total number of slots is 35. The statistics of these two datsets are shown in Table 2 . Based on the statistics from these two datasets, we can calculate the theoretical Inference Time Multiplier (ITM), $K$ , as a metric of scalability.",
        "type": "Document"
      },
      {
        "id": "ac6fc58b-c6cd-42d6-8b98-ca8e807f036d",
        "metadata": {
          "vector_store_key": "1701.06538-8",
          "chunk_id": 62,
          "document_id": "1701.06538",
          "start_idx": 33391,
          "end_idx": 34060
        },
        "page_content": "Models are trained once-through over about 100 billion words. We implement several memory optimizations in order to fit up to 1 billion parameters per GPU. First, we do not store the activations of the hidden layers of the experts, but instead recompute them on the backwards pass. Secondly, we modify the optimizer on the expert parameters to require less auxiliary storage: The Adam optimizer BIBREF39 keeps first and second moment estimates of the per-parameter gradients. This triples the required memory. To avoid keeping a first-moment estimator, we set INLINEFORM0 . To reduce the size of the second moment estimator, we replace it with a factored approximation.",
        "type": "Document"
      },
      {
        "id": "31e2cd79-e775-44f2-93aa-b1a6836dcae2",
        "metadata": {
          "vector_store_key": "1909.11833-2",
          "chunk_id": 3,
          "document_id": "1909.11833",
          "start_idx": 1983,
          "end_idx": 2684
        },
        "page_content": "Furthermore, due to the sparsity of labels, there may not be enough data to thoroughly train each slot-specific network structure. BIBREF6, BIBREF7 both propose to remove the model's dependency on dialogue slots but there's no modification to the representation part, which could be crucial to textual understanding as we will show later. To solve this problem, we need a state tracking model independent of dialogue slots. In other words, the network should depend on the semantic similarity between slots and utterance instead of slot-specific modules. To this end, we propose the Slot-Independent Model (SIM). Our model complexity does not increase when the number of slots in dialogue tasks go up.",
        "type": "Document"
      },
      {
        "id": "4f33e83c-d199-47c4-a8c1-b9481642f476",
        "metadata": {
          "vector_store_key": "1909.00754-7",
          "chunk_id": 2,
          "document_id": "1909.00754",
          "start_idx": 1036,
          "end_idx": 1831
        },
        "page_content": "To generate a distribution over the candidate set, previous works often take each of the slot-value pairs as input for scoring. However, in real-world scenarios, it is often not practical to enumerate all possible slot value pairs and perform scoring from a large dynamically changing knowledge base BIBREF11 . To tackle this problem, a popular direction is to build a fixed-length candidate set that is dynamically updated throughout the dialogue development. cpt briefly summaries the inference time complexity of multiple state-of-the-art DST models following this direction. Since the inference complexity of all of previous model is at least proportional to the number of the slots, these models will struggle to scale to multi-domain datasets with much larger numbers of pre-defined slots.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they measure model size?",
    "llm_answer": "The context describes model size by the number of experts (32, 256, 1024, etc.) in the MoE layers and the number of parameters.\n",
    "context": [
      {
        "id": "266139a8-c68b-428f-9148-108176241a21",
        "metadata": {
          "vector_store_key": "1701.06538-8",
          "chunk_id": 61,
          "document_id": "1701.06538",
          "start_idx": 33014,
          "end_idx": 33624
        },
        "page_content": "We vary the number of experts between models, using an ordinary MoE layer with 32 experts and hierarchical MoE layers with 256, 1024, 4096, 16384, 65536 and 131072 experts. For the hierarchical MoE layers, the first level branching factors are 32, 32, 64, 128, 256 and 256, respectively. Models are trained on a cluster of 32 Tesla K40 GPUs, except for the last two models, which are trained on clusters of 64 and 128 GPUs so as to have enough memory for all the parameters. For all models, training batch sizes are approximately 2.5 million words. Models are trained once-through over about 100 billion words.",
        "type": "Document"
      },
      {
        "id": "d2ae848e-e3b9-4e1d-9c97-9dcca104e6f7",
        "metadata": {
          "vector_store_key": "1911.12579-9",
          "chunk_id": 54,
          "document_id": "1911.12579",
          "start_idx": 30817,
          "end_idx": 31591
        },
        "page_content": "Dimensions ($D$): We evaluate and compare the quality of $100-D$, $200-D$, and $300-D$ using WordSim353 on different $ws$, and the optimal $300-D$ are evaluated with cosine similarity matrix for querying nearest neighboring words and calculating the similarity between word pairs. The embedding dimensions have little affect on the quality of the intrinsic evaluation process. However, the selection of embedding dimensions might have more impact on the accuracy in certain downstream NLP applications. The lower embedding dimensions are faster to train and evaluate. Character n-grams: The selection of minimum (minn) and the maximum (maxn) length of character $n-grams$ is an important parameter for learning character-level representations of words in CBoW and SG models.",
        "type": "Document"
      },
      {
        "id": "f8151257-f5cc-4431-9c35-593f4c0ddabf",
        "metadata": {
          "vector_store_key": "1605.08675-1",
          "chunk_id": 105,
          "document_id": "1605.08675",
          "start_idx": 58501,
          "end_idx": 59124
        },
        "page_content": "To check whether a word denotes a unit of measurement, the plWordNet is searched for lexemes equal to its base. Then it suffices to check whether it belongs to a synset, having <jednostka miary 1> (unit of measurement) as one of (direct or indirect) hypernyms, e.g. pi\u0119tna\u015bcie kilogram\u00f3w (fifteen kilograms) or 5 000 wat\u00f3w (5 000 watts). Study was supported by research fellowship within \"Information technologies: research and their interdisciplinary applications\" agreement number POKL.04.01.01-00-051/10-00. Critical reading of the manuscript by Agnieszka Mykowiecka and Aleksandra Brzezi\u0144ska is gratefully acknowledged.",
        "type": "Document"
      },
      {
        "id": "b073c59c-f20a-409b-bdc7-43d07015143d",
        "metadata": {
          "vector_store_key": "1809.05752-4",
          "chunk_id": 24,
          "document_id": "1809.05752",
          "start_idx": 14238,
          "end_idx": 15006
        },
        "page_content": "The fourth column in Table TABREF6 , Mean Accuracy, was calculated by averaging the three annotator accuracies as evaluated against the gold standard. This provides us with an informative baseline of human parity on the domain classification task. [6]Rectified Linear Units, INLINEFORM0 BIBREF17 [7]Adaptive Moment Estimation BIBREF18  Figure FIGREF8 illustrates the data pipeline for generating our training and testing corpora, and applying them to our classification models. We use the TfidfVectorizer tool included in the scikit-learn machine learning toolkit BIBREF19 to generate our TF-IDF vector space models, stemming tokens with the Porter Stemmer tool provided by the NLTK library BIBREF20 , and calculating TF-IDF scores for unigrams, bigrams, and trigrams.",
        "type": "Document"
      },
      {
        "id": "9ca5340d-587b-4e94-bcbe-160570788673",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 35,
          "document_id": "1910.12618",
          "start_idx": 19294,
          "end_idx": 19871
        },
        "page_content": "For instance for random forests it can correspond to the maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014 and 2015 serve as validation set (730 days).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which model architecture do they for sentence encoding?",
    "llm_answer": "BiLSTM-max, ESIM, KIM, ESIM + ELMo, BERT\n",
    "context": [
      {
        "id": "06ec005a-cbec-4c13-b9a7-f571ddab4828",
        "metadata": {
          "vector_store_key": "1810.09774-0",
          "chunk_id": 19,
          "document_id": "1810.09774",
          "start_idx": 9644,
          "end_idx": 10299
        },
        "page_content": "For sentence encoding models, we chose a simple one-layer bidirectional LSTM with max pooling (BiLSTM-max) with the hidden size of 600D per direction, used e.g. in InferSent BIBREF17 , and HBMP BIBREF18 . For the other models, we have chosen ESIM BIBREF19 , which includes cross-sentence attention, and KIM BIBREF2 , which has cross-sentence attention and utilizes external knowledge. We also selected two model involving a pre-trained language model, namely ESIM + ELMo BIBREF20 and BERT BIBREF0 . KIM is particularly interesting in this context as it performed significantly better than other models in the Breaking NLI experiment conducted by BIBREF1 .",
        "type": "Document"
      },
      {
        "id": "23c3ddf4-7360-4852-a9d9-fffa8b5d3686",
        "metadata": {
          "vector_store_key": "1910.05608-2",
          "chunk_id": 13,
          "document_id": "1910.05608",
          "start_idx": 7214,
          "end_idx": 7851
        },
        "page_content": "In our system, we use five different model architectures combining many types of CNN, and RNN. Each model will use some types of word embedding or handle directly sentence embedding to achieve the best general result. Source code of five models is extended from the GitHub repository The first model is TextCNN (figure FIGREF2) proposed in BIBREF11. It only contains CNN blocks following by some Dense layers. The output of multiple CNN blocks with different kernel sizes is connected to each other. The second model is VDCNN (figure FIGREF5) inspired by the research in BIBREF12. Like the TextCNN model, it contains multiple CNN blocks.",
        "type": "Document"
      },
      {
        "id": "d832ef95-9f46-426a-8eb7-c035b6a72c67",
        "metadata": {
          "vector_store_key": "1905.06566-0",
          "chunk_id": 8,
          "document_id": "1905.06566",
          "start_idx": 4736,
          "end_idx": 5667
        },
        "page_content": "Both the sentence encoder and document encoder are based on the Transformer encoder described in vaswani:2017:nips. As shown in Figure 1 , they are nested in a hierarchical fashion. A transformer encoder usually has multiple layers and each layer is composed of a multi-head self attentive sub-layer followed by a feed-forward sub-layer with residual connections BIBREF30 and layer normalizations BIBREF31 . For more details of the Transformer encoder, we refer the interested readers to vaswani:2017:nips. To learn the representation of $S_i$ , $S_i= (w_1^i, w_2^i, \\dots , w_{|S_i|}^i)$ is first mapped into continuous space  $$\\begin{split}\n\\mathbf {E}_i = (\\mathbf {e}_1^i, \\mathbf {e}_2^i, \\dots , \\mathbf {e}_{|S_i|}^i) \\\\\n\\quad \\quad \\text{where} \\quad \\mathbf {e}_j^i = e(w_j^i) + \\mathbf {p}_j\n\\end{split}$$   (Eq. 6)   where $e(w_j^i)$ and $\\mathbf {p}_j$ are the word and positional embeddings of $w_j^i$ , respectively.",
        "type": "Document"
      },
      {
        "id": "e325926f-6952-4c23-ab25-af4eed197952",
        "metadata": {
          "vector_store_key": "1910.05608-2",
          "chunk_id": 7,
          "document_id": "1910.05608",
          "start_idx": 3964,
          "end_idx": 4626
        },
        "page_content": "Each sentence has a set of words corresponding to a set of word vectors, and that set of word vectors is a representation of a sentence. We also make a sentence embedding by using RoBERTa architecture BIBREF8. CBOW and RoBERTa models trained on text from some resources including VLSP 2016 Sentiment Analysis, VLSP 2018 Sentiment Analysis, VLSP 2019 HSD and text crawled from Facebook. After having sentence representation, we use some classification models to classify input sentences. Those models will be described in detail in the section SECREF13. With the multiply output results, we will use an ensemble method to combine them and output the final result.",
        "type": "Document"
      },
      {
        "id": "ae6bb1c2-02b5-4bc4-8ec9-c6db58b04dab",
        "metadata": {
          "vector_store_key": "1804.00079-0",
          "chunk_id": 32,
          "document_id": "1804.00079",
          "start_idx": 18596,
          "end_idx": 19340
        },
        "page_content": "In future work, we would like understand and interpret the inductive biases that our model learns and observe how it changes with the addition of different tasks beyond just our simple analysis of sentence characteristics and syntax. Having a rich, continuous sentence representation space could allow the application of state-of-the-art generative models of images such as that of BIBREF52 to language. One could also consider controllable text generation by directly manipulating the sentence representations and realizing it by decoding with a conditional language model. The authors would like to thank Chinnadhurai Sankar, Sebastian Ruder, Eric Yuan, Tong Wang, Alessandro Sordoni, Guillaume Lample and Varsha Embar for useful discussions.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which data sources do they use?",
    "llm_answer": "News and encyclopedia corpus, AIDA-Train and Wikipedia datasets, AIDA-CoNLL, Stanford Sentiment Treebank (SST-2), Yelp dataset, Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), Quora Question Pairs (QQP), SQuAD 1.1, SQuAD 2.0, MRQA.\n",
    "context": [
      {
        "id": "598753ae-f85a-4b8a-91f2-1a842414776d",
        "metadata": {
          "vector_store_key": "1906.03538-2",
          "chunk_id": 43,
          "document_id": "1906.03538",
          "start_idx": 24759,
          "end_idx": 25446
        },
        "page_content": "In addition, we ignore trustworthiness and credibility issues, important research issues that are addressed in other works. The authors would like to thank Jennifer Sheffield, Stephen Mayhew, Shyam Upadhyay, Nitish Gupta and the anonymous reviewers for insightful comments and suggestions. This work was supported in part by a gift from Google and by Contract HR0011-15-2-0025 with the US Defense Advanced Research Projects Agency (DARPA). The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. We provide brief statistics on the sources of different content in our dataset in Table TABREF46 .",
        "type": "Document"
      },
      {
        "id": "02e8c704-8405-466b-88ff-ff42a3075d4e",
        "metadata": {
          "vector_store_key": "1910.00458-9",
          "chunk_id": 47,
          "document_id": "1910.00458",
          "start_idx": 25974,
          "end_idx": 26708
        },
        "page_content": "The extractive QA tasks primarily focus on locating text spans from the given document/corpus to answer questions BIBREF2. Answers in abstractive datasets such as MS MARCO BIBREF24, SearchQA BIBREF25, and NarrativeQA BIBREF26 are human-generated and based on source documents or summaries in free text format. However, since annotators tend to copy spans as answers BIBREF27, the majority of answers are still extractive in these datasets. The multi-choice QA datasets are collected either via crowd sourcing, or collected from examinations designed by educational experts BIBREF7. In this type of QA datasets, besides token matching, a significant portion of questions require multi-sentence reasoning and external knowledge BIBREF5.",
        "type": "Document"
      },
      {
        "id": "458b51fc-c0d4-482d-90b3-d4cefed66ac3",
        "metadata": {
          "vector_store_key": "1902.00330-3",
          "chunk_id": 37,
          "document_id": "1902.00330",
          "start_idx": 20853,
          "end_idx": 21565
        },
        "page_content": "We implement our models in Tensorflow and run experiments on 4 Tesla V100 GPU. We conduct experiments on several different types of public datasets including news and encyclopedia corpus. The training set is AIDA-Train and Wikipedia datasets, where AIDA-Train contains 18448 mentions and Wikipedia contains 25995 mentions. In order to compare with the previous methods, we evaluate our model on AIDA-B and other datasets. These datasets are well-known and have been used for the evaluation of most entity linking systems. The statistics of the datasets are shown in Table 1. AIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.",
        "type": "Document"
      },
      {
        "id": "2ded8a9e-d4bc-48fa-9555-3885b88eba42",
        "metadata": {
          "vector_store_key": "1910.00458-5",
          "chunk_id": 32,
          "document_id": "1910.00458",
          "start_idx": 17414,
          "end_idx": 18132
        },
        "page_content": "For the sentiment analysis category, we used the Stanford Sentiment Treebank (SST-2) dataset from the GLUE benchmark BIBREF22 (around 60k train examples) and the Yelp dataset (around 430k train examples). For the paraphrase category, three paraphrasing datasets are used from the GLUE benchmark: Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), and Quora Question Pairs (QQP), which are denoted as \u201cGLUE-Para.\u201d. For the span-based QA, we use the SQuAD 1.1, SQuAD 2.0 , and MRQA which is a joint dataset including six popular span-based QA datasets. Table TABREF23 summarizes the results. We see that sentiment analysis datasets do not help much with our target MCQA datasets.",
        "type": "Document"
      },
      {
        "id": "0f32a044-7be0-45fd-bb81-df8c989b9dec",
        "metadata": {
          "vector_store_key": "1907.02636-2",
          "chunk_id": 2,
          "document_id": "1907.02636",
          "start_idx": 1150,
          "end_idx": 1874
        },
        "page_content": "A number of systems are proposed to help discover and gather malicious information and IOCs from various types of data sources BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 . However, most of those systems consist of several components that identify IOCs by using human-crafted features that heavily rely on specific language knowledge such as dependency structure, and they often have to be pre-defined by experts in the field of the cybersecurity. Furthermore, they need a large amount of annotated data used as the training data to train an IOC classifier. Those training data are frequently difficult to be crowed-sourced, because non-experts can hardly distinguish IOCs from those non-malicious IPs or URLs.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How were breast cancer related posts compiled from the Twitter streaming API?",
    "llm_answer": "Tweets mentioning \"breast\" AND \"cancer\" were collected from a primary Twitter endpoint.\n",
    "context": [
      {
        "id": "25e2e9e8-0eae-4314-8494-31606b6778ac",
        "metadata": {
          "vector_store_key": "1805.09959-1",
          "chunk_id": 8,
          "document_id": "1805.09959",
          "start_idx": 4772,
          "end_idx": 5442
        },
        "page_content": "These targeted feeds were able to collect a large sample of all tweets mentioning these terms; approximately 96% of tweets mentioning \u201cbreast,cancer\u201d and 65.2% of all tweets mentioning `cancer' while active. More information regarding the types of Twitter endpoints and calculating the sampling proportion of collected tweets is described in Appendix II. Our goal was to analyze content authored only by patients. To help ensure this outcome we removed posts containing a URL for classification, BIBREF19 . Twitter allows users to spread content from other users via `retweets'. We also removed these posts prior to classification to isolate tweets authored by patients.",
        "type": "Document"
      },
      {
        "id": "ff8664bd-8304-4598-8b0b-dfba979966e1",
        "metadata": {
          "vector_store_key": "1805.09959-6",
          "chunk_id": 1,
          "document_id": "1805.09959",
          "start_idx": 808,
          "end_idx": 1560
        },
        "page_content": "Our study aims to investigate tweets mentioning \u201cbreast\u201d and \u201ccancer\" to analyze patient populations and selectively obtain content relevant to patient treatment experiences. Our previous study, BIBREF0 , collected tweets mentioning \u201ccancer\u201d over several months to investigate the potential for monitoring self-reported patient treatment experiences. Non-relevant tweets (e.g. astrological and horoscope references) were removed and the study identified a sample of 660 tweets from patients who were describing their condition. These self-reported diagnostic indicators allowed for a sentiment analysis of tweets authored by patients. However, this process was tedious, since the samples were hand verified and sifted through multiple keyword searches.",
        "type": "Document"
      },
      {
        "id": "25a95d7e-f08c-457a-b4e4-c784060da861",
        "metadata": {
          "vector_store_key": "1805.09959-1",
          "chunk_id": 4,
          "document_id": "1805.09959",
          "start_idx": 2412,
          "end_idx": 3202
        },
        "page_content": "We collected tweets from two distinct Spritzer endpoints from September 15th, 2016 through December 9th, 2017. The primary feed for the analysis collected INLINEFORM0 million tweets containing the keywords `breast' AND `cancer'. See Figure FIGREF2 for detailed Twitter frequency statistics along with the user activity distribution. Our secondary feed searched just for the keyword `cancer' which served as a comparison ( INLINEFORM1 million tweets, see Appendix 1), and helped us collect additional tweets relevant to cancer from patients. The numeric account ID provided in tweets helps to distinguish high frequency tweeting entities. Sentence classification combines natural language processing (NLP) with machine learning to identify trends in sentence structure, BIBREF14 , BIBREF15 .",
        "type": "Document"
      },
      {
        "id": "eebef7aa-8a86-4494-8671-dae4365361d9",
        "metadata": {
          "vector_store_key": "1805.09959-1",
          "chunk_id": 7,
          "document_id": "1805.09959",
          "start_idx": 4449,
          "end_idx": 5179
        },
        "page_content": "High frequency tweeting accounts are present in the tail, with a single account producing over 12,000 tweets \u2014an automated account served as a support tool called `ClearScan' for patients in recovery. Approximately 98% of the 2.4 million users shared less than 10 posts, which accounted for 70% of all sampled tweets. The Twitter API also provided the number of tweets withheld from our sample, due to rate limiting. Using these overflow statistics, we estimated the sampled proportion of tweets mentioning these keywords. These targeted feeds were able to collect a large sample of all tweets mentioning these terms; approximately 96% of tweets mentioning \u201cbreast,cancer\u201d and 65.2% of all tweets mentioning `cancer' while active.",
        "type": "Document"
      },
      {
        "id": "abbc8f0c-942e-4dd4-8386-25fc88f28802",
        "metadata": {
          "vector_store_key": "1805.09959-6",
          "chunk_id": 0,
          "document_id": "1805.09959",
          "start_idx": 0,
          "end_idx": 808
        },
        "page_content": "Twitter has shown potential for monitoring public health trends, BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , disease surveillance, BIBREF6 , and providing a rich online forum for cancer patients, BIBREF7 . Social media has been validated as an effective educational and support tool for breast cancer patients, BIBREF8 , as well as for generating awareness, BIBREF9 . Successful supportive organizations use social media sites for patient interaction, public education, and donor outreach, BIBREF10 . The advantages, limitations, and future potential of using social media in healthcare has been thoroughly reviewed, BIBREF11 . Our study aims to investigate tweets mentioning \u201cbreast\u201d and \u201ccancer\" to analyze patient populations and selectively obtain content relevant to patient treatment experiences.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What machine learning and NLP methods were used to sift tweets relevant to breast cancer experiences?",
    "llm_answer": "A context classifier, a Convolutional Neural Network (CNN) with Google's Tensorflow interface, and natural language processing.\n",
    "context": [
      {
        "id": "ff8664bd-8304-4598-8b0b-dfba979966e1",
        "metadata": {
          "vector_store_key": "1805.09959-6",
          "chunk_id": 1,
          "document_id": "1805.09959",
          "start_idx": 808,
          "end_idx": 1560
        },
        "page_content": "Our study aims to investigate tweets mentioning \u201cbreast\u201d and \u201ccancer\" to analyze patient populations and selectively obtain content relevant to patient treatment experiences. Our previous study, BIBREF0 , collected tweets mentioning \u201ccancer\u201d over several months to investigate the potential for monitoring self-reported patient treatment experiences. Non-relevant tweets (e.g. astrological and horoscope references) were removed and the study identified a sample of 660 tweets from patients who were describing their condition. These self-reported diagnostic indicators allowed for a sentiment analysis of tweets authored by patients. However, this process was tedious, since the samples were hand verified and sifted through multiple keyword searches.",
        "type": "Document"
      },
      {
        "id": "6f2189ec-d927-474c-b106-04f6f8d5ad3e",
        "metadata": {
          "vector_store_key": "1805.09959-6",
          "chunk_id": 38,
          "document_id": "1805.09959",
          "start_idx": 22824,
          "end_idx": 23534
        },
        "page_content": "We have demonstrated the potential of using context classifiers for identifying diagnostic tweets related to the experience of breast cancer patients. Our framework provides a proof of concept for integrating machine learning with natural language processing as a tool to help connect healthcare providers with patient experiences. These methods can inform the medical community to provide more personalized treatment regimens by evaluating patient satisfaction using social listening. Twitter has also been shown as a useful medium for political support of healthcare policies as well as spreading awareness. Applying these analyses across other social media platforms could provide comparably rich data-sets.",
        "type": "Document"
      },
      {
        "id": "46360ab7-7c13-425e-abf8-64d206f8bc51",
        "metadata": {
          "vector_store_key": "1805.09959-5",
          "chunk_id": 16,
          "document_id": "1805.09959",
          "start_idx": 9693,
          "end_idx": 10374
        },
        "page_content": "We also found users posting diagnostic related inquiries (7.6%) about possible symptoms that could be linked to breast cancer, or were interested in receiving preventative check-ups. The rest (40.2%) were related to `cancer', but not to patients and include public service updates as well as non-patient authored content (e.g., support groups). We note that the classifier was trained on very limited validated data (N=660), which certainly impacted the results. We used this validated annotated set of tweets to train a more sophisticated classifier to uncover self-diagnostic tweets from users describing their personal breast cancer experiences as current patients or survivors.",
        "type": "Document"
      },
      {
        "id": "528e9bdd-5783-4404-bd28-0895e919267e",
        "metadata": {
          "vector_store_key": "1805.09959-5",
          "chunk_id": 17,
          "document_id": "1805.09959",
          "start_idx": 10374,
          "end_idx": 11109
        },
        "page_content": "We used this validated annotated set of tweets to train a more sophisticated classifier to uncover self-diagnostic tweets from users describing their personal breast cancer experiences as current patients or survivors. We implemented the Convolutional Neural Network (CNN) with Google's Tensorflow interface, BIBREF26 . We adapted our framework from, BIBREF18 , but instead trained the CNN on these 1000 labeled cancer related tweets. The trained CNN was applied to predict patient self-diagnostic tweets from our breast cancer dataset. The CNN outputs a binary value: positive for a predicted tweet relevant to patients or survivors and negative for these other described categories (patient connected, unrelated, diagnostic inquiry).",
        "type": "Document"
      },
      {
        "id": "abbc8f0c-942e-4dd4-8386-25fc88f28802",
        "metadata": {
          "vector_store_key": "1805.09959-6",
          "chunk_id": 0,
          "document_id": "1805.09959",
          "start_idx": 0,
          "end_idx": 808
        },
        "page_content": "Twitter has shown potential for monitoring public health trends, BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , disease surveillance, BIBREF6 , and providing a rich online forum for cancer patients, BIBREF7 . Social media has been validated as an effective educational and support tool for breast cancer patients, BIBREF8 , as well as for generating awareness, BIBREF9 . Successful supportive organizations use social media sites for patient interaction, public education, and donor outreach, BIBREF10 . The advantages, limitations, and future potential of using social media in healthcare has been thoroughly reviewed, BIBREF11 . Our study aims to investigate tweets mentioning \u201cbreast\u201d and \u201ccancer\" to analyze patient populations and selectively obtain content relevant to patient treatment experiences.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What approach performs better in experiments global latent or sequence of fine-grained latent variables?",
    "llm_answer": "Global latent variable.\n",
    "context": [
      {
        "id": "ae53b6c4-15b8-4c8d-ba39-f52d77140134",
        "metadata": {
          "vector_store_key": "2002.11402-3",
          "chunk_id": 11,
          "document_id": "2002.11402",
          "start_idx": 6538,
          "end_idx": 7196
        },
        "page_content": "We trained two models, one with sequence length 512 to capture document level important n-grams and second with sequence length 64 to capture sentence/paragraph level important n-grams. Through experiments it was evident that, sequence length plays a vital role in deciding context and locally/globally important n-grams. Final output is a concatenation of both the model outputs. Trained the topic model on single 32gb NVidia-V100 and it took around 50 hours to train the model with sequence length 512. We had to take 256gb ram machine to accommodate all data in memory for faster read/write. Also, trained model with 64 sequence length in around 17 hours.",
        "type": "Document"
      },
      {
        "id": "63343df2-b858-40e6-8200-a56fa26483bc",
        "metadata": {
          "vector_store_key": "2003.12738-3",
          "chunk_id": 38,
          "document_id": "2003.12738",
          "start_idx": 21539,
          "end_idx": 22235
        },
        "page_content": "This is because both Persona and ED are well designed and have lower entropy than MojiTalk which collected from Twitter. We hypothesize that the sequential latent variables have no advantage in term of similarity to single, fixed \"gold response\" when model low entropy response. Indeed, in open domain dialogue response generation, automatic metric is not always aligned with the human judgement BIBREF28. In contrast, human evaluation result reported in Table TABREF35 demonstrates the generations of SVT are closer to the human standard in terms of coherence, invoked emotion and engagedness. Table TABREF42 compares the generation of the proposed models with baselines given the same contexts.",
        "type": "Document"
      },
      {
        "id": "a9021639-165f-4366-ad10-23dbc3ce68c2",
        "metadata": {
          "vector_store_key": "1909.02480-4",
          "chunk_id": 18,
          "document_id": "1909.02480",
          "start_idx": 9812,
          "end_idx": 10492
        },
        "page_content": "This ensures that the posterior distribution as a simple normal distribution, which we found helps train very deep generative flows more stably. The motivation of introducing the latent variable $\\mathbf {z}$ into the model is to model the uncertainty in the generative process. Thus, it is preferable that $\\mathbf {z}$ capture contextual interdependence between tokens in $\\mathbf {y}$. However, there is an obvious local optimum where the posterior network generates a latent vector $\\mathbf {z}_t$ that only encodes the information about the corresponding target token $y_t$, and the decoder simply generates the \u201ccorrect\u201d token at each step $t$ with $\\mathbf {z}_t$ as input.",
        "type": "Document"
      },
      {
        "id": "e5137f62-f074-4121-8443-e5ab3c8bf5b4",
        "metadata": {
          "vector_store_key": "2003.06279-2",
          "chunk_id": 45,
          "document_id": "2003.06279",
          "start_idx": 27075,
          "end_idx": 27706
        },
        "page_content": "The performance of the global strategy is $12.2\\%$ higher than the one obtained with the global method. A minor difference in performance was found in texts comprising $w=1,000$ words, yet the global strategy is still more efficient than the global one. To summarize all results obtained in this study we show in Table TABREF22 the best results obtained for each text length. We also show the relative gain in performance with the proposed approach and the embedding technique yielding the best result. All optimized results were obtained with the use of stopwords, global thresholding strategy and SVM as classification algorithm.",
        "type": "Document"
      },
      {
        "id": "e3fbb9ef-41e3-4d30-b2c7-fb635289f92f",
        "metadata": {
          "vector_store_key": "2003.12738-3",
          "chunk_id": 37,
          "document_id": "2003.12738",
          "start_idx": 21097,
          "end_idx": 21859
        },
        "page_content": "Compare to baseline models, the GVT achieves relatively lower reconstruction PPL, which suggests that the global latent variable contains rich latent information (e.g., topic) for response generation. Meanwhile, the sequential latent variables of the SVT encode fine-grained latent information and further improve the reconstruction PPL. On the other hand, SVT achieves the highest score in terms of two semantic relevance-oriented metrics such as $\\textbf {EMB}_\\textbf {FT}$ and $\\textbf {EMB}_\\textbf {BERT}$ in MojiTalk dataset, while in the combined dataset of Persona and ED, we observe performance drop of SVT compare to other models. This is because both Persona and ED are well designed and have lower entropy than MojiTalk which collected from Twitter.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which translation system do they use to translate to English?",
    "llm_answer": "Machine translation.\n",
    "context": [
      {
        "id": "dd0a3096-9615-48ec-a2e0-80c08aa831d0",
        "metadata": {
          "vector_store_key": "1911.00069-1",
          "chunk_id": 43,
          "document_id": "1911.00069",
          "start_idx": 25020,
          "end_idx": 25681
        },
        "page_content": "The source language, English, belongs to the SVO (Subject, Verb, Object) language family where in a sentence the subject comes first, the verb second, and the object third. Spanish, Italian, Portuguese, German (in conventional typology) and Chinese also belong to the SVO language family, and our approach achieves over $70\\%$ relative accuracy for these languages. On the other hand, Japanese belongs to the SOV (Subject, Object, Verb) language family and Arabic belongs to the VSO (Verb, Subject, Object) language family, and our approach achieves lower relative accuracy for these two languages. There are a few weakly supervised cross-lingual RE approaches.",
        "type": "Document"
      },
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      },
      {
        "id": "56fe66b6-5e7f-4d68-a3e5-95871fe6ebb1",
        "metadata": {
          "vector_store_key": "1911.03894-1",
          "chunk_id": 27,
          "document_id": "1911.03894",
          "start_idx": 16033,
          "end_idx": 16717
        },
        "page_content": "To evaluate a model on a language other than English (such as French), we consider the two following settings: TRANSLATE-TEST: The French test set is machine translated into English, and then used with an English classification model. This setting provides a reasonable, although imperfect, way to circumvent the fact that no such data set exists for French, and results in very strong baseline scores. TRANSLATE-TRAIN: The French model is fine-tuned on the machine-translated English training set and then evaluated on the French test set. This is the setting that we used for CamemBERT. For the TRANSLATE-TEST setting, we report results of the English RoBERTa to act as a reference.",
        "type": "Document"
      },
      {
        "id": "a7a9af20-c43d-4e30-be76-d8232ea1c023",
        "metadata": {
          "vector_store_key": "1909.02480-3",
          "chunk_id": 53,
          "document_id": "1909.02480",
          "start_idx": 29498,
          "end_idx": 29565
        },
        "page_content": "Table TABREF59 shows the detailed results of translation deversity.",
        "type": "Document"
      },
      {
        "id": "08b5fc05-6d59-46cd-966b-f55f2de0c30a",
        "metadata": {
          "vector_store_key": "1910.07481-0",
          "chunk_id": 12,
          "document_id": "1910.07481",
          "start_idx": 7210,
          "end_idx": 7931
        },
        "page_content": "Translation tasks are English to German, proposed in the first document-level translation task at WMT 2019 BIBREF17, English to French and French to English, following the IWSLT translation task BIBREF18. Table TABREF4 describes the data used for the English-German language pair. These corpora correspond to the WMT 2019 document-level translation task. Table TABREF5 describes corpora for the English-French language pair, the same data is used for both translation directions. For the English-German pair, only 10.4% (3.638M lines) of training data contains document boundaries. For English-French pair, we restricted the total amount of training data in order to keep 16.1% (602K lines) of document delimited corpora.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which pre-trained English NER model do they use?",
    "llm_answer": "Flair's default NER model.\n",
    "context": [
      {
        "id": "80b1fe71-d8ae-4416-88ca-6394b5e64492",
        "metadata": {
          "vector_store_key": "1906.01183-3",
          "chunk_id": 12,
          "document_id": "1906.01183",
          "start_idx": 7278,
          "end_idx": 7854
        },
        "page_content": "For pre-trained English NER system, we use the default NER model of Flair. We train our NER model using vanilla SGD with no momentum for 150 epochs, with an initial learning rate of 0.1 and a learning rate annealing method in which the train loss does not fall in 3 consecutive epochs. The hidden size of BiLSTM model is set to 256 and mini-batch size is set to 16. Dropout is applied to word embeddings with a rate of 0.1 and to BiLSTM with a rate of 0.5. We repeat each experiment 5 times under different random seeds and report the average of test set as final performance.",
        "type": "Document"
      },
      {
        "id": "b795f0e1-0cb3-42a3-acda-91510c822409",
        "metadata": {
          "vector_store_key": "2002.11402-2",
          "chunk_id": 2,
          "document_id": "2002.11402",
          "start_idx": 1170,
          "end_idx": 1923
        },
        "page_content": "Approach of using these pretrained models like ElmoBIBREF13, FlairBIBREF14 and BERTBIBREF0 for word representations followed by variety of LSMT and CRF combinations were tested by authors in BIBREF15 and these approaches show state-of-the-art performance. There are very few approaches where caseless NER task is explored. In this recent paperBIBREF16 authors have explored effects of \"Cased\" entities and how variety of networks perform and they show that the most effective strategy is a concatenation of cased and lowercased training data, producing a single model with high performance on both cased and uncased text. In another paperBIBREF17, authors have proposed True-Case pre-training before using BiLSTM+CRF approach to detect NERs effectively.",
        "type": "Document"
      },
      {
        "id": "67cd0f9e-7441-448c-a190-eb8bba2649b2",
        "metadata": {
          "vector_store_key": "2002.11402-0",
          "chunk_id": 23,
          "document_id": "2002.11402",
          "start_idx": 12947,
          "end_idx": 13653
        },
        "page_content": "Building cased or caseless NERs for English was not the final goal and this has already been benchmarked and explored before in previous approaches explained in \"Related Work\" section. We didn't use traditional datasets for model performance comparisons & benchmarks. As mentioned before, all the comparisons are being done with open-source models and libraries from the productionization point of view. We used a english-news validation dataset which is important and relevant to our specific task and all validation datasets and raw output results can be found at our github link . Wikipedia titles for Indian languages are very very less and resulting tagged data is even less to run deep architectures.",
        "type": "Document"
      },
      {
        "id": "dab5a80d-768e-4e14-9300-382602d0c030",
        "metadata": {
          "vector_store_key": "1906.01183-0",
          "chunk_id": 3,
          "document_id": "1906.01183",
          "start_idx": 2066,
          "end_idx": 2772
        },
        "page_content": "After translating the low-resource language into English, we utilize the pre-trained English NER model to predict the sentences and record the output states of BiLSTM in this model. The states contain the semantic and task-specific information of the sentences. By using soft alignment attention weights as a transformation matrix, we manage to transfer the knowledge of high resource language \u2014 English to other languages. Finally, using both word vectors and the transfer knowledge, we obtain new state-of-the-art results on four datasets. In this section, we will introduce the BAN in three parts. Our model is based on the mainstream NER model BIBREF5 , using BiLSTM-CRF as the basic network structure.",
        "type": "Document"
      },
      {
        "id": "962f9418-ce52-4828-bac5-903c85179fad",
        "metadata": {
          "vector_store_key": "1911.03894-4",
          "chunk_id": 21,
          "document_id": "1911.03894",
          "start_idx": 12356,
          "end_idx": 13179
        },
        "page_content": "UDPipe Future provides us a strong baseline that does not make use of any pretrained contextual embedding. We will compare to the more recent cross-lingual language model XLM BIBREF12, as well as the state-of-the-art CoNLL 2018 shared task results with predicted tokenisation and segmentation in an updated version of the paper. Named Entity Recognition (NER) is a sequence labeling task that consists in predicting which words refer to real-world objects, such as people, locations, artifacts and organisations. We use the French Treebank (FTB) BIBREF39 in its 2008 version introduced by cc-clustering:09short and with NER annotations by sagot2012annotation. The NER-annotated FTB contains more than 12k sentences and more than 350k tokens extracted from articles of the newspaper Le Monde published between 1989 and 1995.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much of the ASR grapheme set is shared between languages?",
    "llm_answer": "The character sets of the 7 languages have little overlap, except for the common basic Latin alphabet and the use of Devanagari script by Hindi and Marathi.\n",
    "context": [
      {
        "id": "fdf582fb-3fd0-4e0a-9bba-39ff6f6c125d",
        "metadata": {
          "vector_store_key": "1909.06522-0",
          "chunk_id": 22,
          "document_id": "1909.06522",
          "start_idx": 11687,
          "end_idx": 12449
        },
        "page_content": "For each language, the train and test set size are described in Table TABREF10, and most training data were Pages. On each language we also had a small validation set for model parameter tuning. Each monolingual ASR baseline was trained on language-specific data only. The character sets of these 7 languages have little overlap except that (i) they all include common basic Latin alphabet, and (ii) both Hindi and Marathi use Devanagari script. We took the union of 7 character sets therein as the multilingual grapheme set (Section SECREF2), which contained 432 characters. In addition, we deliberately split 7 languages into two groups, such that the languages within each group were more closely related in terms of language family, orthography or phonology.",
        "type": "Document"
      },
      {
        "id": "630ab35b-fa88-44c8-b1e0-1bed90922e9a",
        "metadata": {
          "vector_store_key": "1909.06522-4",
          "chunk_id": 3,
          "document_id": "1909.06522",
          "start_idx": 1612,
          "end_idx": 2379
        },
        "page_content": "Note that the above works all assume the test language identity to be known at decoding time, and the language specific lexicon and language model applied. In the absence of a phonetic lexicon, building graphemic systems has shown comparable performance to phonetic lexicon-based approaches in extensive monolingual evaluations BIBREF11, BIBREF12, BIBREF13. Recent advances in end-to-end ASR models have attempted to take the union of multiple language-specific grapheme (i.e. orthographic character) sets, and use such union as a universal grapheme set for a single sequence-to-sequence ASR model BIBREF14, BIBREF15, BIBREF16. It allows for learning a grapheme-based model jointly on data from multiple languages, and performing ASR on within training set languages.",
        "type": "Document"
      },
      {
        "id": "444f5f1b-466a-41b4-b1c3-9b2e0648d127",
        "metadata": {
          "vector_store_key": "1909.06522-0",
          "chunk_id": 33,
          "document_id": "1909.06522",
          "start_idx": 17858,
          "end_idx": 18627
        },
        "page_content": "Overall, we demonstrated the multilingual ASR with massive data augmentation \u2013 via a single graphemic model even without the use of explicit language ID \u2013 allowed for relative WER reductions of 11.0% on Kannada and 18.4% on Hindi. We have presented a multilingual grapheme-based ASR model can effectively perform language-independent recognition on any within training set languages, and substantially outperform each monolingual ASR alternative. Various data augmentation techniques can yield further complementary improvements. Such single multilingual model can not only provide better ASR performance, but also serves as an alternative to the standard production deployment that typically includes extensive monolingual ASR systems and a separate language ID model.",
        "type": "Document"
      },
      {
        "id": "ac319dc4-6cec-42eb-a596-c15418d960c6",
        "metadata": {
          "vector_store_key": "1909.06522-4",
          "chunk_id": 4,
          "document_id": "1909.06522",
          "start_idx": 2085,
          "end_idx": 2847
        },
        "page_content": "It allows for learning a grapheme-based model jointly on data from multiple languages, and performing ASR on within training set languages. In various cases it can produce performance gains over monolingual modeling that uses in-language data only. In our work, we aim to examine the same approach of building a multilingual graphemic lexicon, while using a standard hybrid ASR system \u2013 based on Bidirectional Long Short-Term Memory (BLSTM) and HMM \u2013 learned with lattice-free maximum mutual information (MMI) objective BIBREF17. Our initial attempt is on building a single cascade of an acoustic model, a phonetic decision tree, a graphemic lexicon and a language model \u2013 for 7 geographically proximal languages that have little overlap in their character sets.",
        "type": "Document"
      },
      {
        "id": "a71dd990-1506-42f7-9a93-0dcc0032b3c3",
        "metadata": {
          "vector_store_key": "1909.06522-4",
          "chunk_id": 11,
          "document_id": "1909.06522",
          "start_idx": 6024,
          "end_idx": 6788
        },
        "page_content": "$G$ encodes either the transcript during training, or a language model during decoding. To build a single grapheme-based acoustic model for multiple languages, a multilingual graphemic set is obtained by taking a union of each grapheme set from each language considered, each of which can be either overlapping or non-overlapping. In the multilingual graphemic lexicon, each word in any language is mapped to a sequence of characters in that language. A context-dependent acoustic model is constructed using the decision tree clustering of tri-grapheme states, in the same fashion as the context dependent triphone state tying BIBREF27. The graphemic-context decision tree is constructed over all the multilingual acoustic data including each language of interest.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the languages used to test the model?",
    "llm_answer": "English, French, German, Spanish, Russian, Italian.\n",
    "context": [
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      },
      {
        "id": "24e7a576-ab3d-4f0a-8326-f14fe7d7f139",
        "metadata": {
          "vector_store_key": "1911.03894-4",
          "chunk_id": 26,
          "document_id": "1911.03894",
          "start_idx": 15178,
          "end_idx": 16033
        },
        "page_content": "We also evaluate our model on the Natural Language Inference (NLI) task, using the French part of the XNLI dataset BIBREF50. NLI consists in predicting whether a hypothesis sentence is entailed, neutral or contradicts a premise sentence. The XNLI dataset is the extension of the Multi-Genre NLI (MultiNLI) corpus BIBREF51 to 15 languages by translating the validation and test sets manually into each of those languages. The English training set is also machine translated for all languages. The dataset is composed of 122k train, 2490 valid and 5010 test examples. As usual, NLI performance is evaluated using accuracy. To evaluate a model on a language other than English (such as French), we consider the two following settings: TRANSLATE-TEST: The French test set is machine translated into English, and then used with an English classification model.",
        "type": "Document"
      },
      {
        "id": "56fe66b6-5e7f-4d68-a3e5-95871fe6ebb1",
        "metadata": {
          "vector_store_key": "1911.03894-1",
          "chunk_id": 27,
          "document_id": "1911.03894",
          "start_idx": 16033,
          "end_idx": 16717
        },
        "page_content": "To evaluate a model on a language other than English (such as French), we consider the two following settings: TRANSLATE-TEST: The French test set is machine translated into English, and then used with an English classification model. This setting provides a reasonable, although imperfect, way to circumvent the fact that no such data set exists for French, and results in very strong baseline scores. TRANSLATE-TRAIN: The French model is fine-tuned on the machine-translated English training set and then evaluated on the French test set. This is the setting that we used for CamemBERT. For the TRANSLATE-TEST setting, we report results of the English RoBERTa to act as a reference.",
        "type": "Document"
      },
      {
        "id": "c7d343d1-f033-46c4-b03f-17dac9615d98",
        "metadata": {
          "vector_store_key": "1910.07481-0",
          "chunk_id": 14,
          "document_id": "1910.07481",
          "start_idx": 8370,
          "end_idx": 9086
        },
        "page_content": "We evaluate the English-German systems on newstest2017, newstest2018 and newstest2019 where documents consist of newspaper articles to keep consistency with the training data. English to French and French to English systems are evaluated over IWSLT TED tst2013, tst2014 and tst2015 where documents are transcriptions of TED conferences (see Table TABREF5). Prior to experiments, corpora are tokenized using Moses tokenizer BIBREF19. To limit vocabulary size, we adopt the BPE subword unit approach BIBREF20, through the SentencePiece toolkit BIBREF21, with 32K rules. We use the OpenNMT framework BIBREF22 in its TensorFlow version to create and train our models. All experiments are run on a single NVIDIA V100 GPU.",
        "type": "Document"
      },
      {
        "id": "c835fe0c-f240-4e5f-96f5-494421121339",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 6,
          "document_id": "1910.04269",
          "start_idx": 3498,
          "end_idx": 4224
        },
        "page_content": "The model uses a cyclic learning rate where learning rate increases and then decreases linearly. Maximum learning rate for a cycle is set by finding the optimal learning rate using fastai BIBREF15 library. The model classified six languages \u2013 English, French, Spanish, Russian, Italian and German \u2013 and achieving an accuracy of 89.0%. Gazeau et al. BIBREF16 in his research showed how Neural Networks, Support Vector Machine and Hidden Markov Model (HMM) can be used to identify French, English, Spanish and German. Dataset was prepared using voice samples from Youtube News BIBREF17and VoxForge BIBREF6 datasets. Hidden Markov models convert speech into a sequence of vectors, was used to capture temporal features in speech.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which language has the lowest error rate reduction?",
    "llm_answer": "ParTUT (0.28% for tagging, 1.65% for parsing)\n",
    "context": [
      {
        "id": "49c5572c-938e-4575-b194-ae1ba2406671",
        "metadata": {
          "vector_store_key": "1911.03894-1",
          "chunk_id": 33,
          "document_id": "1911.03894",
          "start_idx": 19193,
          "end_idx": 19879
        },
        "page_content": "We observe a larger error reduction for parsing than for tagging. For POS tagging, we observe error reductions of respectively 0.71% for GSD, 0.81% for Sequoia, 0.7% for Spoken and 0.28% for ParTUT. For parsing, we observe error reductions in LAS of 2.96% for GSD, 3.33% for Sequoia, 1.70% for Spoken and 1.65% for ParTUT. On the XNLI benchmark, CamemBERT obtains improved performance over multilingual language models on the TRANSLATE-TRAIN setting (81.2 vs. 80.2 for XLM) while using less than half the parameters (110M vs. 250M). However, its performance still lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa.",
        "type": "Document"
      },
      {
        "id": "816615b3-7c5f-4f48-b48b-1101b03115aa",
        "metadata": {
          "vector_store_key": "1905.10810-1",
          "chunk_id": 23,
          "document_id": "1905.10810",
          "start_idx": 13256,
          "end_idx": 13913
        },
        "page_content": "(Table TABREF5 ). The first layer, and the one that is nearest to input of the network, is given relatively the least importance, while the middle one dominates both others taken together. This suggests that in error correction, at least for Polish, the middle level of morphemes and other characteristic character chunks is more important than phenomena that are low-level or tied to some specific words. This observation should be taken into account in further research on practical solutions for spelling correction. Among the methods tested the bidirectional LSTM, especially initialized by ELMo embeddings, offers the best accuracy and raw performance.",
        "type": "Document"
      },
      {
        "id": "dd0a3096-9615-48ec-a2e0-80c08aa831d0",
        "metadata": {
          "vector_store_key": "1911.00069-1",
          "chunk_id": 43,
          "document_id": "1911.00069",
          "start_idx": 25020,
          "end_idx": 25681
        },
        "page_content": "The source language, English, belongs to the SVO (Subject, Verb, Object) language family where in a sentence the subject comes first, the verb second, and the object third. Spanish, Italian, Portuguese, German (in conventional typology) and Chinese also belong to the SVO language family, and our approach achieves over $70\\%$ relative accuracy for these languages. On the other hand, Japanese belongs to the SOV (Subject, Object, Verb) language family and Arabic belongs to the VSO (Verb, Subject, Object) language family, and our approach achieves lower relative accuracy for these two languages. There are a few weakly supervised cross-lingual RE approaches.",
        "type": "Document"
      },
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      },
      {
        "id": "e6361237-dc68-4013-9cc2-87fe65a01b0e",
        "metadata": {
          "vector_store_key": "1909.06522-0",
          "chunk_id": 26,
          "document_id": "1909.06522",
          "start_idx": 14030,
          "end_idx": 14721
        },
        "page_content": "After lattice-free MMI training, the model with the best accuracy on validation set was used for evaluation on test set. ASR word error rate (WER%) results are shown in Table TABREF11. We found that, although not explicitly given any information on test language identities, multilingual ASR with language-independent decoding (Section SECREF3) - trained on 3, 4, or 7 languages - substantially outperformed each monolingual ASR in all cases, and on average led to relative WER reductions between 4.6% (Sinhala) and 10.3% (Hindi). Note that the word hypotheses from language-independent decoding could be language mismatched, e.g., part of a Kannada utterance was decoded into Marathi words.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is moral bias measured?",
    "llm_answer": "The Moral Choice Machine computes cosine similarity in a sentence embedding space of an arbitrary action embedded in question/answer pairs.  Several question/answer prompts are conflated into a template, the element of interest is inserted, and the resulting biases are averaged to an overall bias value.\n",
    "context": [
      {
        "id": "7a2ef75d-8818-4fa5-9378-7b446035b0d5",
        "metadata": {
          "vector_store_key": "1912.05238-1",
          "chunk_id": 29,
          "document_id": "1912.05238",
          "start_idx": 16699,
          "end_idx": 17359
        },
        "page_content": "The correlation was tested by means of Pearson's Correlation Coefficient: where $m_x$ and $m_y$ are the the means of $X$ and $Y$. Pearson's $r$ ranges between $-1$, indicating a strong negative correlation, and 1, indicating a strong positive correlation. Significance levels are defined as $5\\%$, $1\\%$ and $0.1\\%$, indicated by one, two or three starlets. The correlation between WEAT value and the moral bias gets tangible, when inspecting their correlation graphically, cf. Fig. FIGREF4. The concrete bias scores can be found in the Appendix, Fig. TABREF28 and TABREF29. For both WEAT and MCM, the scatter plots of Dos and Don'ts are divided on the x-axis.",
        "type": "Document"
      },
      {
        "id": "e2b077cf-55f7-4fec-a36f-bf57da22419a",
        "metadata": {
          "vector_store_key": "1912.05238-6",
          "chunk_id": 39,
          "document_id": "1912.05238",
          "start_idx": 22172,
          "end_idx": 22954
        },
        "page_content": "Our identification of a moral subspace in sentence embeddings lays the foundation for this. BIBREF0 (BIBREF0) developed Moral Choice Machine computes the cosine similarity in a sentence embedding space of an arbitrary action embedded in question/answer pairs. This is illustrated in Fig. FIGREF16 for the moral bias of the action murder. Since murdering is a quite destructive and generally refused behaviour, the questions are expected to lie closer to the denying response and thus to yield a negative bias. To create a more meaningful and comprehensive statistic, several question/answer prompts were conflated to a question/answer template (cf. Tab. TABREF15). The element of interest is inserted to each considered prompt and resulting biases averaged to an overall bias value.",
        "type": "Document"
      },
      {
        "id": "1a01ce65-031d-4bd3-8be5-885449152773",
        "metadata": {
          "vector_store_key": "1912.05238-1",
          "chunk_id": 28,
          "document_id": "1912.05238",
          "start_idx": 15981,
          "end_idx": 16699
        },
        "page_content": "Next, based on the verbs extractions and the question/answer templates, we show that social norms are present in text embeddings and a text embedding network known to achieve high score in unsupervised scenarios \u2014such as semantic textual similarity via cosine-similarity, clustering or semantic search\u2014 improves the scores of the extracted moral actions. The correlation of the moral bias and the corresponding WEAT value was calculated to test consistency of findings. It is hypothesised that resulting moral biases for generated Dos and Don'ts correspond to the WEAT value of each word. The correlation was tested by means of Pearson's Correlation Coefficient: where $m_x$ and $m_y$ are the the means of $X$ and $Y$.",
        "type": "Document"
      },
      {
        "id": "b7b6f45e-5bb3-4c8f-b222-7b593ba5ae97",
        "metadata": {
          "vector_store_key": "2001.07209-1",
          "chunk_id": 9,
          "document_id": "2001.07209",
          "start_idx": 5261,
          "end_idx": 6003
        },
        "page_content": "Our work leverages and complements existing research that finds implicit human biases from word embeddings BIBREF13, BIBREF14, BIBREF19 by developing a novel perspective on using NLP methodology to discover principles of moral sentiment change in human society. Our framework treats the moral sentiment toward a concept at three incremental levels, as illustrated in Figure FIGREF3. First, we consider moral relevance, distinguishing between morally irrelevant and morally relevant concepts. At the second tier, moral polarity, we further split morally relevant concepts into those that are positively or negatively perceived in the moral domain. Finally, a third tier classifies these concepts into fine-grained categories of human morality.",
        "type": "Document"
      },
      {
        "id": "51bfd2c9-1d58-49cc-8c7d-149993a4fadf",
        "metadata": {
          "vector_store_key": "1912.05238-6",
          "chunk_id": 19,
          "document_id": "1912.05238",
          "start_idx": 10924,
          "end_idx": 11596
        },
        "page_content": "Since murdering is a quite destructive and generally refused behaviour, the questions are expected to lie closer to the denying response and thus to yield a negative bias. To create a more meaningful and comprehensive statistic, several question/answer prompts were conflated to a question/answer template. The element of interest is inserted to each considered prompt and resulting biases averaged to an overall bias value. Specifically, we considered two different biases: gender and moral. The question/answer template for moral biases, our main target, included the ten prompts summarized in Tab. TABREF15(Appendix), where any action term can be inserted for the dots.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What metrics are used for evaluation?",
    "llm_answer": "BLEU, BIBREF41, METEOR, ROUGE, Pyramid, ROUGE, Pearson correlation, MSE, MAPE, RMSE, MAE, and $R^2$ coefficient.\n",
    "context": [
      {
        "id": "8c81c383-8e40-4094-8e18-ab0a2d32126a",
        "metadata": {
          "vector_store_key": "1905.08949-6",
          "chunk_id": 18,
          "document_id": "1905.08949",
          "start_idx": 10340,
          "end_idx": 11074
        },
        "page_content": "Although the datasets are commonly shared between QG and QA, it is not the case for evaluation: it is challenging to define a gold standard of proper questions to ask. Meaningful, syntactically correct, semantically sound and natural are all useful criteria, yet they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks. As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE BIBREF43 , are also widely used.",
        "type": "Document"
      },
      {
        "id": "74a8f350-1cdc-4b68-87aa-e719da16ac2c",
        "metadata": {
          "vector_store_key": "1709.10217-3",
          "chunk_id": 4,
          "document_id": "1709.10217",
          "start_idx": 2049,
          "end_idx": 2735
        },
        "page_content": "For the task-oriented system, although there are some objective evaluation metrics, such as the number of turns in a dialogue, the ratio of task completion, etc., there is no gold standard for automatically evaluating two (or more) dialogue systems when considering the satisfaction of the human and the fluency of the generated dialogue. To promote the development of the evaluation technology for dialogue systems, especially considering the language characteristics of Chinese, we organize the first evaluation of Chinese human-computer dialogue technology. In this paper, we will present the evaluation scheme and the released corpus in detail. The rest of this paper is as follows.",
        "type": "Document"
      },
      {
        "id": "f221ac3d-8ea6-49e8-8ce8-1528ccd79f99",
        "metadata": {
          "vector_store_key": "1909.00578-2",
          "chunk_id": 2,
          "document_id": "1909.00578",
          "start_idx": 1185,
          "end_idx": 1928
        },
        "page_content": "Sum-QE achieves very high correlations with human ratings, showing the ability of BERT to model linguistic qualities that relate to both text content and form. Summarization evaluation metrics like Pyramid BIBREF5 and ROUGE BIBREF3, BIBREF2 are recall-oriented; they basically measure the content from a model (reference) summary that is preserved in peer (system generated) summaries. Pyramid requires substantial human effort, even in its more recent versions that involve the use of word embeddings BIBREF8 and a lightweight crowdsourcing scheme BIBREF9. ROUGE is the most commonly used evaluation metric BIBREF10, BIBREF11, BIBREF12. Inspired by BLEU BIBREF4, it relies on common $n$-grams or subsequences between peer and model summaries.",
        "type": "Document"
      },
      {
        "id": "85620a36-4465-42bf-a217-83811397b9bc",
        "metadata": {
          "vector_store_key": "1809.08731-4",
          "chunk_id": 20,
          "document_id": "1809.08731",
          "start_idx": 11375,
          "end_idx": 12197
        },
        "page_content": "Following earlier work BIBREF2 , we evaluate our metrics using Pearson correlation with human judgments. It is defined as the covariance divided by the product of the standard deviations:  $$\\rho _{X,Y} = \\frac{\\text{cov}(X,Y)}{\\sigma _X \\sigma _Y}$$   (Eq. 28)  Pearson cannot accurately judge a metric's performance for sentences of very similar quality, i.e., in the extreme case of rating outputs of identical quality, the correlation is either not defined or 0, caused by noise of the evaluation model. Thus, we additionally evaluate using mean squared error (MSE), which is defined as the squares of residuals after a linear transformation, divided by the sample size:  $$\\text{MSE}_{X,Y} = \\underset{f}{\\min }\\frac{1}{|X|}\\sum \\limits _{i = 1}^{|X|}{(f(x_i) - y_i)^2}$$   (Eq. 30)  with $f$ being a linear function.",
        "type": "Document"
      },
      {
        "id": "416141bf-5fd8-4079-8749-1519922ad6a1",
        "metadata": {
          "vector_store_key": "1910.12618-1",
          "chunk_id": 40,
          "document_id": "1910.12618",
          "start_idx": 21360,
          "end_idx": 21946
        },
        "page_content": "The metrics of evaluation are the Mean Absolute Percentage Error (MAPE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and the $R^2$ coefficient given by: where $T$ is the number of test samples, $y_t$ and $\\hat{y}_t$ are respectively the ground truth and the prediction for the document of day $t$, and $\\overline{y}$ is the empirical average of the time series over the test sample. A known problem with MAPE is that it unreasonably increases the error score for values close to 0. While for the load it isn't an issue at all, it can be for the meteorological time series.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much training data is used?",
    "llm_answer": "Tens of thousands to several million per script.\n",
    "context": [
      {
        "id": "9ca5340d-587b-4e94-bcbe-160570788673",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 35,
          "document_id": "1910.12618",
          "start_idx": 19294,
          "end_idx": 19871
        },
        "page_content": "For instance for random forests it can correspond to the maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014 and 2015 serve as validation set (730 days).",
        "type": "Document"
      },
      {
        "id": "36adf7ba-6ece-4ad5-84b3-0a38b137a4fc",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 16,
          "document_id": "1912.13109",
          "start_idx": 8070,
          "end_idx": 8831
        },
        "page_content": "The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we employed various operational techniques for optimizing on the learning process. These processes and its implementation details will be discussed later but they were learning rate decay, early stopping, model checkpointing and reducing learning rate on plateau.",
        "type": "Document"
      },
      {
        "id": "04fbdeca-ebca-4011-baea-cb3dfb9e705e",
        "metadata": {
          "vector_store_key": "1902.10525-8",
          "chunk_id": 59,
          "document_id": "1902.10525",
          "start_idx": 31351,
          "end_idx": 32094
        },
        "page_content": "The number of training samples varies from tens of thousands to several million per script, depending on the complexity and usage. The best configuration for our production systems were identified by running multiple experiments over a range of layer depths and widths on our Latin script datasets. For the Latin script experiments shown in Figure FIGREF63 , the training set we used was a mixture of data from all the Latin-script languages we support and evaluation is done on an English validation dataset, also used for the English evaluation in Table TABREF68 . Similarly to experiments depicted in Figure FIGREF48 and Figure FIGREF52 , increasing the depth and width of the network architecture brings diminishing returns fairly quickly.",
        "type": "Document"
      },
      {
        "id": "8c714d0f-b982-4837-aefc-3b59003e3a67",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 15,
          "document_id": "1912.13109",
          "start_idx": 7410,
          "end_idx": 8070
        },
        "page_content": "Train-test split: The labelled dataset that was available for this task was very limited in number of examples and thus as noted above few data augmentation techniques were applied to boost the learning of the network. Before applying augmentation, a train-test split of 78%-22% was done from the original, cleansed data set. Thus, 700 tweets/messages were held out for testing. All model evaluation were done in on the test set that got generated by this process. The results presented in this report are based on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data.",
        "type": "Document"
      },
      {
        "id": "19f836cb-0773-4155-890b-786e114e040f",
        "metadata": {
          "vector_store_key": "1911.00133-4",
          "chunk_id": 27,
          "document_id": "1911.00133",
          "start_idx": 14439,
          "end_idx": 15163
        },
        "page_content": "This ensures that while there is a reasonable distribution of labels and domains in the train and test set, the two do not explicitly share any of the same content. This results in a total of 2,838 train data points (51.6% labeled stressful) and 715 test data points (52.4% labeled stressful). Because our data is relatively small, we train our traditional supervised models with 10-fold cross-validation; for our neural models, we break off a further random 10% of the training data for validation and average the predictions of 10 randomly-initialized trained models. In addition to the words of the posts (both as bag-of-n-grams and distributed word embeddings), we include features in three categories: Lexical features.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does their model differ from BERT?",
    "llm_answer": "Their model uses pre-trained [CLS] and [SEP] tokens, and is optimized using techniques from BIBREF6 and BIBREF7.  It also potentially uses distilled BERT models (BIBREF8).\n",
    "context": [
      {
        "id": "811bcb74-e89a-4f0f-b42f-60b757b8c7e6",
        "metadata": {
          "vector_store_key": "1909.02635-2",
          "chunk_id": 24,
          "document_id": "1909.02635",
          "start_idx": 14075,
          "end_idx": 14804
        },
        "page_content": "BERT mainly differs in that it is bidirectional, though we also use the pre-trained [CLS] and [SEP] tokens instead of introducing new tokens in the input vocabulary and training them from scratch during fine-tuning. Owing to the lengths of the processes, all our experiments are performed on BERT$_{BASE}$. The most significant prior work on this dataset is the work of BIBREF15. However, their data condition differs significantly from ours: they train on a large noisy training set and do not use any of the high-quality labeled data, instead treating it as dev and test data. Consequently, their model achieves low performance, roughly 56 $F_1 $ while ours achieves $82.5$ $F_1$ (though these are not the exact same test set).",
        "type": "Document"
      },
      {
        "id": "2de7e36b-9a99-411b-9e59-98d488461e99",
        "metadata": {
          "vector_store_key": "2002.06644-1",
          "chunk_id": 6,
          "document_id": "2002.06644",
          "start_idx": 3726,
          "end_idx": 4586
        },
        "page_content": "Optimized BERT-based models: We use BERT-based models optimized as in BIBREF6 and BIBREF7, pretrained on a dataset as large as twelve times as compared to $BERT_{large}$, with bigger batches, and longer sequences. ALBERT, introduced in BIBREF7, uses factorized embedding parameterization and cross-layer parameter sharing for parameter reduction. These optimizations have led both the models to outperform $BERT_{large}$ in various benchmarking tests, like GLUE for text classification and SQuAD for Question Answering. Distilled BERT-based models: Secondly, we propose to use distilled BERT-based models, as introduced in BIBREF8. They are smaller general-purpose language representation model, pre-trained by leveraging distillation knowledge. This results in significantly smaller and faster models with performance comparable to their undistilled versions.",
        "type": "Document"
      },
      {
        "id": "aa94fb66-c910-42df-b61d-4a5241934b5b",
        "metadata": {
          "vector_store_key": "1908.06606-0",
          "chunk_id": 29,
          "document_id": "1908.06606",
          "start_idx": 17099,
          "end_idx": 17817
        },
        "page_content": "Due to the high cost of pre-training BERT language model, we directly adopt parameters pre-trained by Google in Chinese general corpus. The named entity recognition is applied on both pathology report texts and query texts. Since BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large. Prediction layer is attached at the end of the original BERT-Base model and we fine tune it on our dataset.",
        "type": "Document"
      },
      {
        "id": "ef2aedc1-5f20-45fa-860b-0486314d05c4",
        "metadata": {
          "vector_store_key": "1905.13497-0",
          "chunk_id": 23,
          "document_id": "1905.13497",
          "start_idx": 13621,
          "end_idx": 13999
        },
        "page_content": "However, although BERT seems to implicitly establish complex relationships between entities facilitating tasks such as coreference resolution, the results also suggest that solving commonsense reasoning tasks might require more than leveraging a language model trained on huge text corpora. Future work will entail adaption of the attentions, to further improve the performance.",
        "type": "Document"
      },
      {
        "id": "b8556365-eae7-4052-a155-1bc382ee0d64",
        "metadata": {
          "vector_store_key": "1905.13497-0",
          "chunk_id": 0,
          "document_id": "1905.13497",
          "start_idx": 0,
          "end_idx": 787
        },
        "page_content": "Recently, neural models pre-trained on a language modeling task, such as ELMo BIBREF0 , OpenAI GPT BIBREF1 , and BERT BIBREF2 , have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference. The success of BERT can largely be associated to the notion of context-aware word embeddings, which differentiate it from common approaches such as word2vec BIBREF3 that establish a static semantic embedding. Since the introduction of BERT, the NLP community continues to be impressed by the amount of ideas produced on top of this powerful language representation model. However, despite its success, it remains unclear whether the representations produced by BERT can be utilized for tasks such as commonsense reasoning.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does explicit constraint on the KL divergence term that authors propose looks like?",
    "llm_answer": "$|D_{KL}\\big (q_\\phi ({z}|{x}) || p({z})\\big )-C|$  where $C \\ge 0$.\n",
    "context": [
      {
        "id": "d8c77d06-1b73-4ec9-a29d-83d6f19c72da",
        "metadata": {
          "vector_store_key": "1909.13668-5",
          "chunk_id": 46,
          "document_id": "1909.13668",
          "start_idx": 24597,
          "end_idx": 25449
        },
        "page_content": "The immediate impact of the explicit constraint is avoiding the collapse issue ($D_{KL}=0$) by setting a non-zero positive constraint ($C\\ge 0$) on the KL term ($|D_{KL}\\big (q_\\phi ({z}|{x}) || p({z})\\big )-C|$). We experimented with a range of constraints ($C$) on the KL term and various powerful and weak decoder architectures (LSTM, GRU, and CNN), and empirically confirmed that in all cases the constraint was satisfied. We showed that the higher value of KL encourages not only divergence from the prior distribution, but also a sharper and more concentrated approximated posteriors. It encourages the decoder to be more sensitive to the variations on the latent code, and makes the model with higher KL less suitable for generation as the latent variables observed during training are farther away from the prior samples used during generation.",
        "type": "Document"
      },
      {
        "id": "37a5f153-169c-4624-ac22-bb5d80597fbe",
        "metadata": {
          "vector_store_key": "1909.00430-3",
          "chunk_id": 14,
          "document_id": "1909.00430",
          "start_idx": 6795,
          "end_idx": 7631
        },
        "page_content": "KL-divergence is composed of two parts: INLINEFORM6 INLINEFORM7  Since INLINEFORM0 is constant, we only need to minimize INLINEFORM1 , therefore the loss function becomes: DISPLAYFORM0  Notice that computing INLINEFORM0 requires summation over INLINEFORM1 for the entire set INLINEFORM2 , which can be prohibitive. We present batched approximation (Section SECREF19 ) to overcome this. BIBREF0 find that XR might find a degenerate solution. For example, in a three class classification task, where INLINEFORM0 , it might find a solution such that INLINEFORM1 for every instance, as a result, every instance will be classified the same. To avoid this, BIBREF0 suggest to penalize flat distributions by using a temperature coefficient T likewise: DISPLAYFORM0  Where z is a feature vector and W and b are the linear classifier parameters.",
        "type": "Document"
      },
      {
        "id": "32d64543-8e18-489e-b4d4-3b12082d3a3d",
        "metadata": {
          "vector_store_key": "1909.13668-5",
          "chunk_id": 12,
          "document_id": "1909.13668",
          "start_idx": 6303,
          "end_idx": 7027
        },
        "page_content": "While we could apply constraint optimization to impose the explicit constraint of $\\text{KL}\\!\\!=\\!\\!C$, we found that the above objective function satisfies the constraint (experiment). Alternatively, it has been shown BIBREF21 the similar effect could be reached by replacing the second term in eqn. DISPLAY_FORM6 with $\\max \\big (C,D_{KL}\\big (q_\\phi ({z}|{x}) || p({z})\\big )\\big )$ at the risk of breaking the ELBO when $\\text{KL}\\!\\!<\\!\\!C$ BIBREF22. We conduct various experiments to illustrate the properties that are encouraged via different KL magnitudes. In particular, we revisit the interdependence between rate and distortion, and shed light on the impact of KL on the sharpness of the approximated posteriors.",
        "type": "Document"
      },
      {
        "id": "9a6d3537-faac-42af-be9f-ad037b2c6619",
        "metadata": {
          "vector_store_key": "1909.13668-5",
          "chunk_id": 45,
          "document_id": "1909.13668",
          "start_idx": 24325,
          "end_idx": 24971
        },
        "page_content": "In this paper we analysed the interdependence of the KL term in Evidence Lower Bound (ELBO) and the properties of the approximated posterior for text generation. To perform the analysis we used an information theoretic framework based on a variant of $\\beta $-VAE objective, which permits explicit control of the KL term, and treats KL as a mechanism to control the amount of information transmitted between the encoder and decoder. The immediate impact of the explicit constraint is avoiding the collapse issue ($D_{KL}=0$) by setting a non-zero positive constraint ($C\\ge 0$) on the KL term ($|D_{KL}\\big (q_\\phi ({z}|{x}) || p({z})\\big )-C|$).",
        "type": "Document"
      },
      {
        "id": "3025bb09-fa5a-4130-8b0e-e1c26ad8398a",
        "metadata": {
          "vector_store_key": "1902.09314-1",
          "chunk_id": 17,
          "document_id": "1902.09314",
          "start_idx": 10452,
          "end_idx": 11270
        },
        "page_content": "LSR is equivalent to the KL divergence between the prior label distribution INLINEFORM0 and the network's predicted distribution INLINEFORM1 . Formally, LSR term is defined as: DISPLAYFORM0  The objective function (loss function) to be optimized is the cross-entropy loss with INLINEFORM0 and INLINEFORM1 regularization, which is defined as: DISPLAYFORM0   where INLINEFORM0 is the ground truth represented as a one-hot vector, INLINEFORM1 is the predicted sentiment distribution vector given by the output layer, INLINEFORM2 is the coefficient for INLINEFORM3 regularization term, and INLINEFORM4 is the parameter set. We conduct experiments on three datasets: SemEval 2014 Task 4 BIBREF15 dataset composed of Restaurant reviews and Laptop reviews, and ACL 14 Twitter dataset gathered by Dong et al. dong2014adaptive.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what was the baseline?",
    "llm_answer": "A Kaldi i-vector system (BIBREF3) and an x-vector system (BIBREF10) were baselines.  A Majority Class baseline, an Exact Match baseline, and a First Occurrence baseline were also used.  Finally, a LSTM baseline was used to evaluate the performance of ELMo embeddings.  A \"Baseline\" system was also used for a machine translation task.\n",
    "context": [
      {
        "id": "02bef31f-a2b5-4b89-8063-62dc0b0f91eb",
        "metadata": {
          "vector_store_key": "1911.01799-4",
          "chunk_id": 19,
          "document_id": "1911.01799",
          "start_idx": 11188,
          "end_idx": 11946
        },
        "page_content": "Two state-of-the-art baseline systems were built following the Kaldi SITW recipe BIBREF22: an i-vector system BIBREF3 and an x-vector system BIBREF10. For the i-vector system, the acoustic feature involved 24-dimensional MFCCs plus the log energy, augmented by the first- and second-order derivatives. We also applied the cepstral mean normalization (CMN) and the energy-based voice active detection (VAD). The universal background model (UBM) consisted of $2,048$ Gaussian components, and the dimensionality of the i-vector space was 400. LDA was applied to reduce the dimensionality of the i-vectors to 150. The PLDA model was used for scoring BIBREF4. For the x-vector system, the feature-learning component was a 5-layer time-delay neural network (TDNN).",
        "type": "Document"
      },
      {
        "id": "763bf540-a941-47b1-bdeb-25182eab991e",
        "metadata": {
          "vector_store_key": "2001.11381-4",
          "chunk_id": 55,
          "document_id": "2001.11381",
          "start_idx": 31798,
          "end_idx": 32460
        },
        "page_content": "Tampoco consideramos la utilizaci\u00f3n de un baseline de tipo aleatorio, porque los resultados carecer\u00edan de la homosintaxis y ser\u00eda sumamente f\u00e1cil obtener mejores resultados. Dicho lo anterior, el Modelo 1 podr\u00eda ser considerado como nuestro propio baseline. A continuaci\u00f3n presentamos un protocolo de evaluaci\u00f3n manual de los resultados obtenidos. El experimento consisti\u00f3 en la generaci\u00f3n de 15 frases por cada uno de los tres modelos propuestos. Para cada modelo, se consideraron tres queries: $Q=$ {AMOR, GUERRA, SOL}, generando 5 frases con cada uno. Las 15 frases fueron mezcladas entre s\u00ed y reagrupadas por queries, antes de presentarlas a los evaluadores.",
        "type": "Document"
      },
      {
        "id": "5e76f24e-4c75-4a34-b69c-7fe1f8d12771",
        "metadata": {
          "vector_store_key": "1909.02635-2",
          "chunk_id": 15,
          "document_id": "1909.02635",
          "start_idx": 9069,
          "end_idx": 9719
        },
        "page_content": "This includes (i) Majority Class, (ii) Exact Match of an ingredient $e$ in recipe step $s_t$, and (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo$_{token}$ and ELMo$_{sent}$) BIBREF22 compared to GPT.",
        "type": "Document"
      },
      {
        "id": "d2e2382c-8e5a-4f0c-92c5-2f119e6e1178",
        "metadata": {
          "vector_store_key": "1910.07481-0",
          "chunk_id": 20,
          "document_id": "1910.07481",
          "start_idx": 11787,
          "end_idx": 12518
        },
        "page_content": "It also surpassed the Baselinee by 0.18 points on the newstest2017 with strong statistical significance, and by 0.15 BLEU points on the newstest2018 but this time with no statistical evidence. These encouraging results prompted us to extend experiments to another language pair: English-French. En$\\rightarrow $Fr: The Document system obtained the best results considering all metrics on all test sets with strong statistical evidence. It surpassed the Baseline by 1.09 BLEU points and 0.85 TER points on tst2015, 0.75 BLEU points and 0.76 TER points on tst2014, and 0.48 BLEU points and 0.68 TER points on tst2013. Fr$\\rightarrow $En: Of all experiments, this language pair shows the most important improvements over the Baseline.",
        "type": "Document"
      },
      {
        "id": "bd2880bf-e0e6-40e6-8e69-9ab6dc0db9cf",
        "metadata": {
          "vector_store_key": "1803.09230-2",
          "chunk_id": 19,
          "document_id": "1803.09230",
          "start_idx": 9812,
          "end_idx": 10446
        },
        "page_content": "[\\mathbf {c}_N; \\mathbf {a}_N; \\mathbf {d}_N] \\rbrace )$$   (Eq. 12)  Finally all attention layer outputs are concatenated and fed into a Softmax layer that computes the probability distributions for the start and end token independently, as it is done in the baseline implementation. Before we started the enhancements of the baseline model, we studied the SQuAD data set. Figure. 2 shows the distribution of the answer, question and context lengths as well as the relative position of the answer span inside a context. Furthermore, we counted the different question types. We found that most answers have a length less than 5 words.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is dataset used?",
    "llm_answer": "The context mentions datasets of various sizes:  70,000 to 170,000 raw data, 12,350 labeled ads, 15,000 images with 75,000 questions, and tens of thousands to several million training samples per script.\n",
    "context": [
      {
        "id": "9ca5340d-587b-4e94-bcbe-160570788673",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 35,
          "document_id": "1910.12618",
          "start_idx": 19294,
          "end_idx": 19871
        },
        "page_content": "For instance for random forests it can correspond to the maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014 and 2015 serve as validation set (730 days).",
        "type": "Document"
      },
      {
        "id": "d87d327c-2858-4b5d-ae8a-860d401f63bd",
        "metadata": {
          "vector_store_key": "1908.05434-1",
          "chunk_id": 27,
          "document_id": "1908.05434",
          "start_idx": 14432,
          "end_idx": 15064
        },
        "page_content": "We have ensured that the raw dataset has no overlap with the labeled dataset to avoid bias in test accuracy. While it is possible to scrape more raw data, we did not observe significant improvements in model performances when the size of raw data increased from INLINEFORM0 70,000 to INLINEFORM1 170,000, hence we assume that the current raw dataset is sufficiently large. The labeled dataset is called Trafficking-10k. It consists of 12,350 ads from Backpage labeled by experts in human trafficking detection BIBREF9 . Each label is one of seven ordered levels of likelihood that the corresponding ad comes from a human trafficker.",
        "type": "Document"
      },
      {
        "id": "0fcc020b-3016-4745-902c-d73226505bf1",
        "metadata": {
          "vector_store_key": "1910.11949-3",
          "chunk_id": 17,
          "document_id": "1910.11949",
          "start_idx": 9318,
          "end_idx": 10010
        },
        "page_content": "Each source contains 5,000 images with 5 questions per image, adding a total of 15,000 images with 75,000 questions. COCO dataset includes images of complex everyday scenes containing common objects in their natural context, but it is limited in terms of the concepts it covers. Bing dataset contains more event related questions and has a wider range of questions longitudes (between 3 and 20 words), while Flickr questions are shorter (less than 6 words) and the images appear to be more casual. We use two datasets to train our chatbot model. The first one is the Persona-chat BIBREF15 which contains dialogues between two people with different profiles that are trying to know each other.",
        "type": "Document"
      },
      {
        "id": "1e2bf238-b0a0-40af-9d8d-e6fea4ad79d0",
        "metadata": {
          "vector_store_key": "1910.09399-3",
          "chunk_id": 105,
          "document_id": "1910.09399",
          "start_idx": 56273,
          "end_idx": 57009
        },
        "page_content": "For example, image data captured from a variety of photo-ready devices, such as smart-phones, and online social media services opened the door to the analysis of large amounts of media datasets BIBREF70. The availability of large media datasets allow new frameworks and algorithms to be proposed and tested on real-world data. A summary of some reviewed methods and benchmark datasets used for validation is reported in Table TABREF43. In addition, the performance of different GANs with respect to the benchmark datasets and performance metrics is reported in Table TABREF48. In order to synthesize images from text descriptions, many frameworks have taken a minimalistic approach by creating small and background-less images BIBREF73.",
        "type": "Document"
      },
      {
        "id": "04fbdeca-ebca-4011-baea-cb3dfb9e705e",
        "metadata": {
          "vector_store_key": "1902.10525-8",
          "chunk_id": 59,
          "document_id": "1902.10525",
          "start_idx": 31351,
          "end_idx": 32094
        },
        "page_content": "The number of training samples varies from tens of thousands to several million per script, depending on the complexity and usage. The best configuration for our production systems were identified by running multiple experiments over a range of layer depths and widths on our Latin script datasets. For the Latin script experiments shown in Figure FIGREF63 , the training set we used was a mixture of data from all the Latin-script languages we support and evaluation is done on an English validation dataset, also used for the English evaluation in Table TABREF68 . Similarly to experiments depicted in Figure FIGREF48 and Figure FIGREF52 , increasing the depth and width of the network architecture brings diminishing returns fairly quickly.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much better does this baseline neural model do?",
    "llm_answer": "The RelNet model achieves a mean error of 0.285%, better than the EntNet model.  MMM leads to improvements ranging from 0.5% to 3.0% in accuracy.\n",
    "context": [
      {
        "id": "8c0e86ed-567f-4b03-88bc-7a372957a5e9",
        "metadata": {
          "vector_store_key": "1706.07179-4",
          "chunk_id": 13,
          "document_id": "1706.07179",
          "start_idx": 6969,
          "end_idx": 7741
        },
        "page_content": "The baseline EntNet model was run for 10 times for each task BIBREF17 . The results are shown in Table 1 . The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the EntNet model BIBREF17 . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the EntNet model achieves 0% error on 7 of the tasks. We demonstrated an end-to-end trained neural network augmented with a structured memory representation which can reason about entities and relations for question answering. Future work will investigate the performance of these models on more real world datasets, interpreting what the models learn, and scaling these models to answer questions about entities and relations from reading massive text corpora.",
        "type": "Document"
      },
      {
        "id": "14c77096-531d-4d71-9236-61c2ccdb8cfa",
        "metadata": {
          "vector_store_key": "1910.00458-6",
          "chunk_id": 44,
          "document_id": "1910.00458",
          "start_idx": 24199,
          "end_idx": 24865
        },
        "page_content": "Compared with the baselines, MMM leads to improvements ranging from 0.5% to 3.0% in accuracy. Our best result is obtained by the RoBERTa-Large encoder. In order to investigate how well our model performs for different types of questions, we did an error analysis by first randomly selecting 150 samples that had wrong predictions by the BERT-Base baseline model from the development set of DREAM dataset. We then manually classified them into several question types, as shown in Table TABREF34. The annotation criterion is described in the Section 3 of the Supplementary Material. We see that the BERT-Base baseline model still does not do well on matching problems.",
        "type": "Document"
      },
      {
        "id": "874d1312-9fdb-46fe-865b-ce75dcb445d6",
        "metadata": {
          "vector_store_key": "1812.07023-3",
          "chunk_id": 32,
          "document_id": "1812.07023",
          "start_idx": 17666,
          "end_idx": 18428
        },
        "page_content": "We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below: Our primary architectural differences over the baseline model are: not concatenating the question, answer pairs before encoding them, the auxiliary decoder module, and using the Time-Extended FiLM module for feature extraction. These, combined with using scheduled sampling and running hyperparameter optimization over the validation set to select hyperparameters, give us the observed performance boost. We observe that our models generate fairly relevant responses to questions in the dialogues, and models with audio-visual inputs respond to audio-visual questions (e.g. \u201cis there any voices or music ?\u201d) correctly more often.",
        "type": "Document"
      },
      {
        "id": "c938d47c-e132-4294-bd51-25295fb776d2",
        "metadata": {
          "vector_store_key": "1603.07044-3",
          "chunk_id": 28,
          "document_id": "1603.07044",
          "start_idx": 15079,
          "end_idx": 15744
        },
        "page_content": "Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited training data is given, both feature-rich based system and our system are worse than the IR system. For task C, our system also got comparable results with the feature-rich based system. If we do a simple system combination (average the rank score) between our system and the IR system, the combined system will give large gains on tasks B and C.",
        "type": "Document"
      },
      {
        "id": "22a4966b-fb5b-4fbf-bc0d-5fa53cc012ce",
        "metadata": {
          "vector_store_key": "1909.00124-4",
          "chunk_id": 25,
          "document_id": "1909.00124",
          "start_idx": 14255,
          "end_idx": 14886
        },
        "page_content": "The first DNN baseline is CNN BIBREF25, which does not handle noisy labels. The other five were designed to handle noisy labels. The comparison results are shown in Table TABREF12. From the results, we can make the following observations. (1) Our NetAb model achieves the best ACC and F1 on all datasets except for F1 of negative class on Laptop. The results demonstrate the superiority of NetAb. (2) NetAb outperforms the baselines designed for learning with noisy labels. These baselines are inferior to ours as they were tailored for image classification. Note that we found no existing method to deal with noisy labels for SSC.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the baseline models?",
    "llm_answer": "Baseline models:  Random, Modelo 1, TF-IDF, LDA, Majority Class, Exact Match, First Occurrence, LSTM, i-vector, x-vector.\n",
    "context": [
      {
        "id": "763bf540-a941-47b1-bdeb-25182eab991e",
        "metadata": {
          "vector_store_key": "2001.11381-4",
          "chunk_id": 55,
          "document_id": "2001.11381",
          "start_idx": 31798,
          "end_idx": 32460
        },
        "page_content": "Tampoco consideramos la utilizaci\u00f3n de un baseline de tipo aleatorio, porque los resultados carecer\u00edan de la homosintaxis y ser\u00eda sumamente f\u00e1cil obtener mejores resultados. Dicho lo anterior, el Modelo 1 podr\u00eda ser considerado como nuestro propio baseline. A continuaci\u00f3n presentamos un protocolo de evaluaci\u00f3n manual de los resultados obtenidos. El experimento consisti\u00f3 en la generaci\u00f3n de 15 frases por cada uno de los tres modelos propuestos. Para cada modelo, se consideraron tres queries: $Q=$ {AMOR, GUERRA, SOL}, generando 5 frases con cada uno. Las 15 frases fueron mezcladas entre s\u00ed y reagrupadas por queries, antes de presentarlas a los evaluadores.",
        "type": "Document"
      },
      {
        "id": "fcbb237d-302f-45b0-886a-31ec58fabb9f",
        "metadata": {
          "vector_store_key": "1909.09484-4",
          "chunk_id": 35,
          "document_id": "1909.09484",
          "start_idx": 19125,
          "end_idx": 19742
        },
        "page_content": "An Adam optimizer BIBREF22 is used for training our models and baselines, with a learning rate of 0.001 for supervised training and 0.0001 for reinforcement learning. In reinforcement learning, the decay parameter $\\lambda $ is set to 0.8. The weight decay is set to 0.001. And early stopping is performed on developing set. The experimental results of the proposed model and baselines will be analyzed from the following aspects. BPRA Results: As shown in Table TABREF35, most of the models have similar performance on BPRA on these two datasets, which can guarantee a consistent impact on the dialogue policy maker.",
        "type": "Document"
      },
      {
        "id": "96576214-2c10-4049-ba11-db649d59677b",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 26,
          "document_id": "1809.04960",
          "start_idx": 15085,
          "end_idx": 15736
        },
        "page_content": "Unsupervised baseline models are as follows: TF-IDF (Lexical, Non-Neural) is an important unsupervised baseline. We use the concatenation of the title and the body as the query to retrieve the candidate comment set by means of the similarity of the tf-idf value. The model is trained on unpaired articles and comments, which is the same as our proposed model. LDA (Topic, Non-Neural) is a popular unsupervised topic model, which discovers the abstract \"topics\" that occur in a collection of documents. We train the LDA with the articles and comments in the training set. The model retrieves the comments by the similarity of the topic representations.",
        "type": "Document"
      },
      {
        "id": "5e76f24e-4c75-4a34-b69c-7fe1f8d12771",
        "metadata": {
          "vector_store_key": "1909.02635-2",
          "chunk_id": 15,
          "document_id": "1909.02635",
          "start_idx": 9069,
          "end_idx": 9719
        },
        "page_content": "This includes (i) Majority Class, (ii) Exact Match of an ingredient $e$ in recipe step $s_t$, and (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo$_{token}$ and ELMo$_{sent}$) BIBREF22 compared to GPT.",
        "type": "Document"
      },
      {
        "id": "02bef31f-a2b5-4b89-8063-62dc0b0f91eb",
        "metadata": {
          "vector_store_key": "1911.01799-4",
          "chunk_id": 19,
          "document_id": "1911.01799",
          "start_idx": 11188,
          "end_idx": 11946
        },
        "page_content": "Two state-of-the-art baseline systems were built following the Kaldi SITW recipe BIBREF22: an i-vector system BIBREF3 and an x-vector system BIBREF10. For the i-vector system, the acoustic feature involved 24-dimensional MFCCs plus the log energy, augmented by the first- and second-order derivatives. We also applied the cepstral mean normalization (CMN) and the energy-based voice active detection (VAD). The universal background model (UBM) consisted of $2,048$ Gaussian components, and the dimensionality of the i-vector space was 400. LDA was applied to reduce the dimensionality of the i-vectors to 150. The PLDA model was used for scoring BIBREF4. For the x-vector system, the feature-learning component was a 5-layer time-delay neural network (TDNN).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much of MGNC-CNN out perform the baselines?",
    "llm_answer": "The context states that MGNC-CNN and MG-CNN always outperform baseline methods (including C-CNN).  No specific performance difference is given.\n",
    "context": [
      {
        "id": "86077cf4-e678-4e38-884f-8efbf84a734c",
        "metadata": {
          "vector_store_key": "1603.00968-2",
          "chunk_id": 19,
          "document_id": "1603.00968",
          "start_idx": 11251,
          "end_idx": 11973
        },
        "page_content": "For instantiations of MGNC-CNN in which we exploited two embeddings, we tuned both INLINEFORM2 , and INLINEFORM3 ; where we used three embedding sets, we tuned INLINEFORM4 and INLINEFORM5 . We used standard train/test splits for those datasets that had them. Otherwise, we performed 10-fold cross validation, creating nested development sets with which to tune hyperparameters. For all experiments we used filters sizes of 3, 4 and 5 and we created 100 feature maps for each filter size. We applied 1 max-pooling and dropout (rate: 0.5) at the classification layer. For training we used back-propagation in mini-batches and used AdaDelta as the stochastic gradient descent (SGD) update rule, and set mini-batch size as 50.",
        "type": "Document"
      },
      {
        "id": "294fd721-d865-404f-97b3-bbe898989ec7",
        "metadata": {
          "vector_store_key": "1603.00968-3",
          "chunk_id": 21,
          "document_id": "1603.00968",
          "start_idx": 12392,
          "end_idx": 13197
        },
        "page_content": "Results are shown in Table TABREF2 , and the corresponding best norm constraint value is shown in Table TABREF2 . We also show results on Subj, SST-1 and SST-2 achieved by the more complex model of BIBREF11 for comparison; this represents the state-of-the-art on the three datasets other than TREC. We can see that MGNC-CNN and MG-CNN always outperform baseline methods (including C-CNN), and MGNC-CNN is usually better than MG-CNN. And on the Subj dataset, MG-CNN actually achieves slightly better results than BIBREF11 , with far less complexity and required training time (MGNC-CNN performs comparably, although no better, here). On the TREC dataset, the best-ever accuracy we are aware of is 96.0% BIBREF21 , which falls within the range of the result of our MGNC-CNN model with three word embeddings.",
        "type": "Document"
      },
      {
        "id": "c2f943a3-eef8-4ec2-a234-054ebcb91e0d",
        "metadata": {
          "vector_store_key": "1909.00124-4",
          "chunk_id": 24,
          "document_id": "1909.00124",
          "start_idx": 13615,
          "end_idx": 14255
        },
        "page_content": "The results clearly show that the performance of the CNN drops quite a lot with the noise rate increasing. Experiment 2: Here we use the real noisy-labeled training data to train our model and the baselines, and then test on the test data in Table TABREF9. Our goal is two fold. First, we want to evaluate NetAb using real noisy data. Second, we want to see whether sentences with review level labels can be used to build effective SSC models. Baselines. We use one strong non-DNN baseline, NBSVM (with unigrams or bigrams features) BIBREF23 and six DNN baselines. The first DNN baseline is CNN BIBREF25, which does not handle noisy labels.",
        "type": "Document"
      },
      {
        "id": "adfc0dbe-f54c-4945-8f3a-7a59f2f3a029",
        "metadata": {
          "vector_store_key": "1603.00968-3",
          "chunk_id": 22,
          "document_id": "1603.00968",
          "start_idx": 13197,
          "end_idx": 13796
        },
        "page_content": "On the TREC dataset, the best-ever accuracy we are aware of is 96.0% BIBREF21 , which falls within the range of the result of our MGNC-CNN model with three word embeddings. On the irony dataset, our model with three embeddings achieves 4% improvement (in terms of AUC) compared to the baseline model. On SST-1 and SST-2, our model performs slightly worse than BIBREF11 . However, we again note that their performance is achieved using a much more complex model which involves pre-training and mutual-learning steps. This model takes days to train, whereas our model requires on the order of an hour.",
        "type": "Document"
      },
      {
        "id": "c9f58c8f-f07e-4688-a8ca-5b17e0fe4786",
        "metadata": {
          "vector_store_key": "2002.06644-4",
          "chunk_id": 9,
          "document_id": "2002.06644",
          "start_idx": 5847,
          "end_idx": 6623
        },
        "page_content": "For all BERT-based models, we use a learning rate of $2*10^{-5}$, a maximum sequence length of 50, and a weight decay of $0.01$ while finetuning the model. We use FastText's recently open-sourced automatic hyperparameter optimization functionality while training the model. For the BiLSTM baseline, we use a dropout of $0.05$ along with a recurrent dropout of $0.2$ in two 64 unit sized stacked BiLSTMs, using softmax activation layer as the final dense layer. tab:experimental-results shows the performance of different models on the WNC corpus evaluated on the following four metrics: Precision, Recall, F1, and Accuracy. Our proposed methodology, the use of finetuned optimized BERT based models, and BERT-based ensemble models outperform the baselines for all the metrics.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is increase in percentage of humor contained in headlines generated with TitleStylist method (w.r.t. baselines)?",
    "llm_answer": "Answer not found in the context.\n",
    "context": [
      {
        "id": "ae04e835-50ec-47dd-994b-e6e09f2b9f90",
        "metadata": {
          "vector_store_key": "2004.01980-3",
          "chunk_id": 5,
          "document_id": "2004.01980",
          "start_idx": 3058,
          "end_idx": 3799
        },
        "page_content": "The main contributions of our paper are listed below: To the best of our knowledge, it is the first research on the generation of attractive news headlines with styles without any supervised style-specific article-headline paired data. Through both automatic and human evaluation, we demonstrated that our proposed TitleStylist can generate relevant, fluent headlines with three styles (humor, romance, and clickbait), and they are even more attractive than human-written ones. Our model can flexibly incorporate multiple styles, thus efficiently and automatically providing humans with various creative headline options for references and inspiring them to think out of the box. Our work is related to summarization and text style transfer.",
        "type": "Document"
      },
      {
        "id": "2f024db4-b544-4cf7-9060-e1e31a8a933a",
        "metadata": {
          "vector_store_key": "2004.01980-3",
          "chunk_id": 35,
          "document_id": "2004.01980",
          "start_idx": 20102,
          "end_idx": 21015
        },
        "page_content": "It is noteworthy that the generated headlines of our TitleStylist for all three styles are close to the original human-written headlines in terms of relevance, validating that our generation results are qualified in this aspect. Another finding is that more attractive or more stylistic headlines would lose some relevance since they need to use more words outside the news body for improved creativity. In terms of attraction scores in Table TABREF51, we have three findings: (1) The human-written headlines are more attractive than those from NHG, which agrees with our observation in Section SECREF1. (2) Our TitleStylist can generate more attractive headlines over the NHG and Multitask baselines for all three styles, demonstrating that adapting the model to these styles could improve the attraction and specialization of some parameters in the model for different styles can further enhance the attraction.",
        "type": "Document"
      },
      {
        "id": "34e32211-33e9-454d-836f-5879aade562e",
        "metadata": {
          "vector_store_key": "2004.01980-3",
          "chunk_id": 4,
          "document_id": "2004.01980",
          "start_idx": 2370,
          "end_idx": 3058
        },
        "page_content": "This design enables us to use the shared content to generate headlines that are more relevant to the articles, as well as to control the style by plugging in a set of style-specific parameters. We validate the model on three tasks: humorous, romantic, and click-baity headline generation. Both automatic and human evaluations show that TitleStylist can generate headlines with the desired styles that appeal more to human readers, as in Figure FIGREF2. The main contributions of our paper are listed below: To the best of our knowledge, it is the first research on the generation of attractive news headlines with styles without any supervised style-specific article-headline paired data.",
        "type": "Document"
      },
      {
        "id": "0b2058fa-859d-4fd9-b649-78bb479e44ff",
        "metadata": {
          "vector_store_key": "2004.01980-3",
          "chunk_id": 36,
          "document_id": "2004.01980",
          "start_idx": 20478,
          "end_idx": 21240
        },
        "page_content": "(2) Our TitleStylist can generate more attractive headlines over the NHG and Multitask baselines for all three styles, demonstrating that adapting the model to these styles could improve the attraction and specialization of some parameters in the model for different styles can further enhance the attraction. (3) Adapting the model to the \u201cClickbait\u201d style could create the most attractive headlines, even out-weighting the original ones, which agrees with the fact that click-baity headlines are better at drawing readers' attention. To be noted, although we learned the \u201cClickbait\u201d style into our summarization system, we still made sure that we are generating relevant headlines instead of too exaggerated ones, which can be verified by our relevance scores.",
        "type": "Document"
      },
      {
        "id": "ff325abd-dcbe-4134-8252-87c569e0fb06",
        "metadata": {
          "vector_store_key": "2004.01980-2",
          "chunk_id": 37,
          "document_id": "2004.01980",
          "start_idx": 21240,
          "end_idx": 22054
        },
        "page_content": "To be noted, although we learned the \u201cClickbait\u201d style into our summarization system, we still made sure that we are generating relevant headlines instead of too exaggerated ones, which can be verified by our relevance scores. The human-annotated fluency scores in Table TABREF51 verified that our TitleStylist generated headlines are comparable or superior to the human-written headlines in terms of readability. We also validated that our TitleStylist can carry more styles compared with the Multitask and NHG baselines by summarizing the percentage of choices by humans for the most humorous or romantic headlines in Table TABREF57. Apart from the human evaluation of the overall generation quality on four criteria, we also conducted a conventional automatic assessment to gauge only the summarization quality.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the languages they consider in this paper?",
    "llm_answer": "English, French, German, Spanish, Russian, Italian, Bengali, Hindi, Telegu, Kannada, Malayalam, Sinhala, Tamil, and Marathi.\n",
    "context": [
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      },
      {
        "id": "9210eab3-d084-4395-859a-51482ae8da57",
        "metadata": {
          "vector_store_key": "1910.04269-1",
          "chunk_id": 34,
          "document_id": "1910.04269",
          "start_idx": 19227,
          "end_idx": 19993
        },
        "page_content": "2D attention models focused on the important features extracted by convolutional layers and bi-directional GRU captured the temporal features. Several of the spoken languages in Europe belong to the Indo-European family. Within this family, the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Katze in German and Ciao in Italian becomes Chao in Spanish.",
        "type": "Document"
      },
      {
        "id": "cb4078b8-00d7-4e01-a91b-7292c78d6351",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 12,
          "document_id": "1910.04269",
          "start_idx": 7197,
          "end_idx": 7983
        },
        "page_content": "The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel). Several state-of-the-art results on various audio classification tasks have been obtained by using log-Mel spectrograms of raw audio, as features BIBREF19. Convolutional Neural Networks have demonstrated an excellent performance gain in classification of these features BIBREF20, BIBREF21 against other machine learning techniques. It has been shown that using attention layers with ConvNets further enhanced their performance BIBREF22. This motivated us to develop a CNN-based architecture with attention since this approach hasn\u2019t been applied to the task of language identification before.",
        "type": "Document"
      },
      {
        "id": "c835fe0c-f240-4e5f-96f5-494421121339",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 6,
          "document_id": "1910.04269",
          "start_idx": 3498,
          "end_idx": 4224
        },
        "page_content": "The model uses a cyclic learning rate where learning rate increases and then decreases linearly. Maximum learning rate for a cycle is set by finding the optimal learning rate using fastai BIBREF15 library. The model classified six languages \u2013 English, French, Spanish, Russian, Italian and German \u2013 and achieving an accuracy of 89.0%. Gazeau et al. BIBREF16 in his research showed how Neural Networks, Support Vector Machine and Hidden Markov Model (HMM) can be used to identify French, English, Spanish and German. Dataset was prepared using voice samples from Youtube News BIBREF17and VoxForge BIBREF6 datasets. Hidden Markov models convert speech into a sequence of vectors, was used to capture temporal features in speech.",
        "type": "Document"
      },
      {
        "id": "7571d95c-6c8b-4418-8614-397954840b36",
        "metadata": {
          "vector_store_key": "1909.06522-0",
          "chunk_id": 23,
          "document_id": "1909.06522",
          "start_idx": 12449,
          "end_idx": 13169
        },
        "page_content": "In addition, we deliberately split 7 languages into two groups, such that the languages within each group were more closely related in terms of language family, orthography or phonology. We thus built 3 multilingual ASR models trained on: all 7 languages, for 1059 training hours in total, 4 languages \u2013 Kannada, Malayalam, Sinhala and Tamil \u2013 for 590 training hours, 3 languages \u2013 Bengali, Hindi and Marathi \u2013 for 469 training hours, which are referred to as 7lang, 4lang, and 3lang respectively. Note that Kannada, Malayalam and Tamil are Dravidian languages, which have rich agglutinative inflectional morphology BIBREF2 and resulted in around 10% OOV token rates on test sets (Hindi had the lowest OOV rate as 2-3%).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Did they experiment with tasks other than word problems in math?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "2b0855e5-b36f-4314-b3e0-1f2da26facc8",
        "metadata": {
          "vector_store_key": "1909.00578-4",
          "chunk_id": 23,
          "document_id": "1909.00578",
          "start_idx": 12727,
          "end_idx": 13118
        },
        "page_content": "It could thus be applied to other text generation tasks, such as natural language generation and sentence compression. We would like to thank the anonymous reviewers for their helpful feedback on this work. The work has been partly supported by the Research Center of the Athens University of Economics and Business, and by the French National Research Agency under project ANR-16-CE33-0013.",
        "type": "Document"
      },
      {
        "id": "80911f24-68f0-45cb-a65a-c54370ab3292",
        "metadata": {
          "vector_store_key": "1707.03764-3",
          "chunk_id": 11,
          "document_id": "1707.03764",
          "start_idx": 5958,
          "end_idx": 6709
        },
        "page_content": "This also shows that distinguishing words include both time-specific ones, like \u201cgilmore\u201d and \u201cimacelebrityau\u201d, and general words from everyday life, which are less likely to be subject to time-specific trends, like \u201cplayer\u201d, and \u201cchocolate\u201d. This section is meant to highlight all of the potential contributions to the systems which turned out to be detrimental to performance, when compared to the simpler system that we have described in Section SECREF2 . We divide our attempts according to the different ways we attempted to enhance performance: manipulating the data itself (adding more, and changing preprocessing), using a large variety of features, and changing strategies in modelling the problem by using different algorithms and paradigms.",
        "type": "Document"
      },
      {
        "id": "bf6402a0-fc04-4733-a70b-5f0b918a0326",
        "metadata": {
          "vector_store_key": "1605.08675-8",
          "chunk_id": 12,
          "document_id": "1605.08675",
          "start_idx": 6788,
          "end_idx": 7454
        },
        "page_content": "More information about the task, including its motivation, difficulties and a feasibility study for Polish could be found in BIBREF5 . The problem of Question Answering is not new to the Polish NLP community (nor working on other morphologically rich languages), but none of studies presented so far coincides with the notion of plain text-based QA presented above. First Polish QA attempts date back to 1985, when BIBREF6 presented a Polish interface to ORBIS database, containing information about the solar system. The database consisted of a set of PROLOG rules and the role of the system (called POLINT) was to translate Polish questions to appropriate queries.",
        "type": "Document"
      },
      {
        "id": "85a3f639-f1b5-45d9-9437-05e8dcf24304",
        "metadata": {
          "vector_store_key": "1910.00458-2",
          "chunk_id": 46,
          "document_id": "1910.00458",
          "start_idx": 25301,
          "end_idx": 25974
        },
        "page_content": "To investigate this question, we sampled some arithmetic questions that are correctly predicted by our model, made small alterations to the passage or question, and then checked whether our model can still make correct choices. We found our model is very fragile to these minor alterations, implicating that the model is actually not that good at arithmetic problems. We provided one interesting example in the Section 3 of the Supplementary Material. There are increasing interests in machine reading comprehension (MRC) for question answering (QA). The extractive QA tasks primarily focus on locating text spans from the given document/corpus to answer questions BIBREF2.",
        "type": "Document"
      },
      {
        "id": "a48c2a61-14a8-4538-8b46-146fb87ab447",
        "metadata": {
          "vector_store_key": "1703.04617-2",
          "chunk_id": 0,
          "document_id": "1703.04617",
          "start_idx": 0,
          "end_idx": 722
        },
        "page_content": "Enabling computers to understand given documents and answer questions about their content has recently attracted intensive interest, including but not limited to the efforts as in BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 . Many specific problems such as machine comprehension and question answering often involve modeling such question-document pairs. The recent availability of relatively large training datasets (see Section \"Related Work\" for more details) has made it more feasible to train and estimate rather complex models in an end-to-end fashion for these problems, in which a whole model is fit directly with given question-answer tuples and the resulting model has shown to be rather effective.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What evaluation metrics are used?",
    "llm_answer": "Sum-QE, Pyramid, ROUGE, F<sub>1</sub> score, BPRA, number of turns, task completion ratio.\n",
    "context": [
      {
        "id": "8c81c383-8e40-4094-8e18-ab0a2d32126a",
        "metadata": {
          "vector_store_key": "1905.08949-6",
          "chunk_id": 18,
          "document_id": "1905.08949",
          "start_idx": 10340,
          "end_idx": 11074
        },
        "page_content": "Although the datasets are commonly shared between QG and QA, it is not the case for evaluation: it is challenging to define a gold standard of proper questions to ask. Meaningful, syntactically correct, semantically sound and natural are all useful criteria, yet they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks. As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE BIBREF43 , are also widely used.",
        "type": "Document"
      },
      {
        "id": "f221ac3d-8ea6-49e8-8ce8-1528ccd79f99",
        "metadata": {
          "vector_store_key": "1909.00578-2",
          "chunk_id": 2,
          "document_id": "1909.00578",
          "start_idx": 1185,
          "end_idx": 1928
        },
        "page_content": "Sum-QE achieves very high correlations with human ratings, showing the ability of BERT to model linguistic qualities that relate to both text content and form. Summarization evaluation metrics like Pyramid BIBREF5 and ROUGE BIBREF3, BIBREF2 are recall-oriented; they basically measure the content from a model (reference) summary that is preserved in peer (system generated) summaries. Pyramid requires substantial human effort, even in its more recent versions that involve the use of word embeddings BIBREF8 and a lightweight crowdsourcing scheme BIBREF9. ROUGE is the most commonly used evaluation metric BIBREF10, BIBREF11, BIBREF12. Inspired by BLEU BIBREF4, it relies on common $n$-grams or subsequences between peer and model summaries.",
        "type": "Document"
      },
      {
        "id": "6a443055-4b4a-46e1-a2dc-b3211ac62d38",
        "metadata": {
          "vector_store_key": "1908.10449-0",
          "chunk_id": 20,
          "document_id": "1908.10449",
          "start_idx": 10741,
          "end_idx": 11449
        },
        "page_content": "Since iMRC involves both MRC and RL, we adopt evaluation metrics from both settings. First, as a question answering task, we use $\\text{F}_1$ score to compare predicted answers against ground-truth, as in previous works. When there exist multiple ground-truth answers, we report the max $\\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we evaluate an agent's performance during both its training and testing phases. During training, we report training curves averaged over 3 random seeds. During test, we follow common practice in supervised learning tasks where we report the agent's test performance corresponding to its best validation performance .",
        "type": "Document"
      },
      {
        "id": "74a8f350-1cdc-4b68-87aa-e719da16ac2c",
        "metadata": {
          "vector_store_key": "1709.10217-3",
          "chunk_id": 4,
          "document_id": "1709.10217",
          "start_idx": 2049,
          "end_idx": 2735
        },
        "page_content": "For the task-oriented system, although there are some objective evaluation metrics, such as the number of turns in a dialogue, the ratio of task completion, etc., there is no gold standard for automatically evaluating two (or more) dialogue systems when considering the satisfaction of the human and the fluency of the generated dialogue. To promote the development of the evaluation technology for dialogue systems, especially considering the language characteristics of Chinese, we organize the first evaluation of Chinese human-computer dialogue technology. In this paper, we will present the evaluation scheme and the released corpus in detail. The rest of this paper is as follows.",
        "type": "Document"
      },
      {
        "id": "fa230c6a-9ae2-4313-8be2-bde2493abce6",
        "metadata": {
          "vector_store_key": "1909.09484-4",
          "chunk_id": 29,
          "document_id": "1909.09484",
          "start_idx": 15790,
          "end_idx": 16412
        },
        "page_content": "We evaluate the performance of the proposed model in three aspects: (1) the accuracy of the dialogue state tracker, it aims to show the impact of the dialogue state tracker on the dialogue policy maker; (2) the accuracy of dialogue policy maker, it aims to explain the performance of different methods of constructing dialogue policy; (3) the quality of the final response, it aims to explain the impact of the dialogue policy on the final dialogue response. The evaluation metrics are listed as follows: BPRA: Belief Per-Response Accuracy (BPRA) tests the ability to generate the correct user intents during the dialogue.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What kind of Youtube video transcripts did they use?",
    "llm_answer": "Manual transcriptions of newscasts, interviews, reports, and round tables.\n",
    "context": [
      {
        "id": "463438ef-caf2-4802-8966-9430a0960731",
        "metadata": {
          "vector_store_key": "1808.08850-3",
          "chunk_id": 23,
          "document_id": "1808.08850",
          "start_idx": 13023,
          "end_idx": 13771
        },
        "page_content": "The selected videos cover different topics like technology, human rights, terrorism and politics with a length variation between 2 and 10 minutes. To encourage the diversity of content format we included newscasts, interviews, reports and round tables. During the transcription phase we opted for a manual transcription process because we observed that using transcripts from an ASR system will difficult in a large degree the manual segmentation process. The number of words per transcript oscilate between 271 and 1,602 with a total number of 8,080. We gave clear instructions to three evaluators ( INLINEFORM0 ) of how segmentation was needed to be perform, including the SU concept and how punctuation marks were going to be taken into account.",
        "type": "Document"
      },
      {
        "id": "af229424-afeb-42f5-a2e4-a305bd4b228a",
        "metadata": {
          "vector_store_key": "1812.07023-0",
          "chunk_id": 13,
          "document_id": "1812.07023",
          "start_idx": 7755,
          "end_idx": 8419
        },
        "page_content": "They divide each video into multiple equal-duration segments and, from each of them, extract video features using an I3D BIBREF21 model, and audio features using a VGGish BIBREF22 model. The I3D model was pre-trained on Kinetics BIBREF23 dataset and the VGGish model was pre-trained on Audio Set BIBREF24 . The baseline encodes the current utterance's question with a lstm BIBREF25 and uses the encoding to attend to the audio and video features from all the video segments and to fuse them together. The dialogue history is modelled with a hierarchical recurrent lstm encoder where the input to the lower level encoder is a concatenation of question-answer pairs.",
        "type": "Document"
      },
      {
        "id": "1e7f981f-f1fe-4217-8374-dfac0a145d49",
        "metadata": {
          "vector_store_key": "1812.07023-1",
          "chunk_id": 21,
          "document_id": "1812.07023",
          "start_idx": 12176,
          "end_idx": 12809
        },
        "page_content": "We also add an auxiliary decoding module which is similar to the response decoder except that it tries to generate the caption and/or the summary of the video. We present our model in Figure FIGREF2 and describe the individual components in detail below. The utterance-level encoder is a recurrent neural network consisting of a single layer of lstm cells. The input to the lstm are word embeddings for each word in the utterance. The utterance is concatenated with a special symbol <eos> marking the end of the sequence. We initialize our word embeddings using 300-dimensional GloVe BIBREF30 and then fine-tune them during training.",
        "type": "Document"
      },
      {
        "id": "15f6efbb-cc0c-4b85-a505-c7c3687a4b69",
        "metadata": {
          "vector_store_key": "1910.09399-5",
          "chunk_id": 94,
          "document_id": "1910.09399",
          "start_idx": 50132,
          "end_idx": 51042
        },
        "page_content": "In this context, the synthesised videos are often useful resources for automated assistance or story telling. black One early/interesting work of motion enhancement GANs is to generate spoofed speech and lip-sync videos (or talking face) of Barack Obama (i.e. ObamaNet) based on text input BIBREF62. This framework is consisted of three parts, i.e. text to speech using \u201cChar2Wav\u201d, mouth shape representation synced to the audio using a time-delayed LSTM and \u201cvideo generation\u201d conditioned on the mouth shape using \u201cU-Net\u201d architecture. Although the results seem promising, ObamaNet only models the mouth region and the videos are not generated from noise which can be regarded as video prediction other than video generation. black Another meaningful trial of using synthesised videos for automated assistance is to translate spoken language (e.g. text) into sign language video sequences (i.e. T2S) BIBREF63.",
        "type": "Document"
      },
      {
        "id": "78b95723-5cab-4b82-ba72-8c15fb12eff3",
        "metadata": {
          "vector_store_key": "1812.07023-1",
          "chunk_id": 23,
          "document_id": "1812.07023",
          "start_idx": 13252,
          "end_idx": 13865
        },
        "page_content": "The description encoder also has access to the last hidden state of the utterance-level encoder, which it uses to generate an attention map over the hidden states of its lstm. The final output of this module is the attention-weighted sum of the lstm hidden states. For the video encoder, we use an I3D model pre-trained on the Kinetics dataset BIBREF23 and extract the output of its Mixed_7c layer for INLINEFORM0 (30 for our models) equi-distant segments of the video. Over these features, we add INLINEFORM1 (2 for our models) FiLM BIBREF31 blocks which have been highly successful in visual reasoning problems.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What makes it a more reliable metric?",
    "llm_answer": "Pearson correlation, along with mean squared error, is used because simple accuracy metrics can be unreliable, particularly when evaluating outputs of similar quality.  Mean per-class accuracy is also used in image recognition tasks to compensate for unbalanced categories.\n",
    "context": [
      {
        "id": "e34ec430-93a0-4067-875a-f939ddf574fd",
        "metadata": {
          "vector_store_key": "1703.09684-9",
          "chunk_id": 9,
          "document_id": "1703.09684",
          "start_idx": 5278,
          "end_idx": 5981
        },
        "page_content": "For example, on COCO-VQA, improving accuracy on `Is/Are' questions by 15% will increase overall accuracy by over 5%, but answering all `Why/Where' questions correctly will increase accuracy by only 4.1% BIBREF10 . Due to the inability of the existing evaluation metrics to properly address these biases, algorithms trained on these datasets learn to exploit these biases, resulting in systems that work poorly when deployed in the real-world. For related reasons, major benchmarks released in the last decade do not use simple accuracy for evaluating image recognition and related computer vision tasks, but instead use metrics such as mean-per-class accuracy that compensates for unbalanced categories.",
        "type": "Document"
      },
      {
        "id": "85620a36-4465-42bf-a217-83811397b9bc",
        "metadata": {
          "vector_store_key": "1809.08731-4",
          "chunk_id": 20,
          "document_id": "1809.08731",
          "start_idx": 11375,
          "end_idx": 12197
        },
        "page_content": "Following earlier work BIBREF2 , we evaluate our metrics using Pearson correlation with human judgments. It is defined as the covariance divided by the product of the standard deviations:  $$\\rho _{X,Y} = \\frac{\\text{cov}(X,Y)}{\\sigma _X \\sigma _Y}$$   (Eq. 28)  Pearson cannot accurately judge a metric's performance for sentences of very similar quality, i.e., in the extreme case of rating outputs of identical quality, the correlation is either not defined or 0, caused by noise of the evaluation model. Thus, we additionally evaluate using mean squared error (MSE), which is defined as the squares of residuals after a linear transformation, divided by the sample size:  $$\\text{MSE}_{X,Y} = \\underset{f}{\\min }\\frac{1}{|X|}\\sum \\limits _{i = 1}^{|X|}{(f(x_i) - y_i)^2}$$   (Eq. 30)  with $f$ being a linear function.",
        "type": "Document"
      },
      {
        "id": "9d853fa7-0c72-4c65-b29e-60b371a76d0e",
        "metadata": {
          "vector_store_key": "2001.06354-0",
          "chunk_id": 7,
          "document_id": "2001.06354",
          "start_idx": 3970,
          "end_idx": 4726
        },
        "page_content": "Since NDCG measures more of a model's generalization ability (because it allows multiple similar answers), while the other metrics measure a model's preciseness, we interpret the results of these above experiments to mean that a model with more history information tends to predict correct answers by memorizing keywords or patterns in the history while a model with less history information (i.e., the image-only model) is better at generalization by avoiding relying on such exact-match extracted information. We think that an ideal model should have more balanced behavior and scores over all the metrics rather than having higher scores only for a certain metric and such a model could be considered as the one with both preciseness and generalization.",
        "type": "Document"
      },
      {
        "id": "594fe7e8-5314-4485-a855-1108b86ee8e8",
        "metadata": {
          "vector_store_key": "1703.09684-9",
          "chunk_id": 10,
          "document_id": "1703.09684",
          "start_idx": 5508,
          "end_idx": 6196
        },
        "page_content": "For related reasons, major benchmarks released in the last decade do not use simple accuracy for evaluating image recognition and related computer vision tasks, but instead use metrics such as mean-per-class accuracy that compensates for unbalanced categories. For example, on Caltech-101 BIBREF13 , even with balanced training data, simple accuracy fails to address the fact that some categories were much easier to classify than others (e.g., faces and planes were easy and also had the largest number of test images). Mean per-class accuracy compensates for this by requiring a system to do well on each category, even when the amount of test instances in categories vary considerably.",
        "type": "Document"
      },
      {
        "id": "0c070008-5c07-4c13-ab5e-76daec86eb90",
        "metadata": {
          "vector_store_key": "1909.09484-4",
          "chunk_id": 31,
          "document_id": "1909.09484",
          "start_idx": 16716,
          "end_idx": 17421
        },
        "page_content": "But our model actually generates each individual token of actions, and we consider a prediction to be correct only if every token of the model output matches the corresponding token in the ground truth. BLEU BIBREF19: The metric evaluates the quality of the final response generated by natural language generator. The metric is usually used to measure the performance of the task-oriented dialogue system. We also choose the following metrics to evaluate the efficiency of training the model: $\\mathbf {Time_{full}}$: The time for training the whole model, which is important for industry settings. $\\mathbf {Time_{DP}}$: The time for training the dialogue policy maker in a task-oriented dialogue system.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much in experiments is performance improved for models trained with generated adversarial examples?",
    "llm_answer": "The context states that performance of target models *raises significantly* after adversarial training.\n",
    "context": [
      {
        "id": "2bdfae0f-3a46-497e-aed6-09835abf4ed2",
        "metadata": {
          "vector_store_key": "1909.02560-2",
          "chunk_id": 42,
          "document_id": "1909.02560",
          "start_idx": 24601,
          "end_idx": 25378
        },
        "page_content": "After adversarial training, the performance of all the target models raises significantly, while that on the original examples remain comparable. Note that since the focus of this paper is on model robustness which can hardly be reflected in original data, we do not expect performance improvement on original data. The results demonstrate that adversarial training with our adversarial examples can significantly improve the robustness we focus on without remarkably hurting the performance on original data. Moreover, although the adversarial example generation is constrained by a BERT language model, BiMPM and DIIN which do not use the BERT language model can also significantly benefit from the adversarial examples, further demonstrating the effectiveness of our method.",
        "type": "Document"
      },
      {
        "id": "8d4666ba-135a-4caa-9028-8079110d4240",
        "metadata": {
          "vector_store_key": "1909.02560-2",
          "chunk_id": 37,
          "document_id": "1909.02560",
          "start_idx": 21759,
          "end_idx": 22668
        },
        "page_content": "We notice that the original models are more vulnerable to unmatched adversarial examples, because there are generally more replaceable position choices during the generation. Nevertheless, the results of the matched case are also sufficiently strong to reveal the robustness issues. We do not quantitatively compare the performance drop of the target models on the adversarial examples with previous work, because we generate a new type of adversarial examples that previous methods are not capable of. We have different experiment settings, including original example sampling and constraints on adversarial modifications, which are tailored to the robustness issues we study. Performance drop on different kinds of adversarial examples with little overlap is not comparable, and thus surpassing other adversarial examples on model performance drop is unnecessary and irrelevant to support our contributions.",
        "type": "Document"
      },
      {
        "id": "04c3d0aa-c381-4018-9744-61e04d2fd0fd",
        "metadata": {
          "vector_store_key": "1909.02560-2",
          "chunk_id": 38,
          "document_id": "1909.02560",
          "start_idx": 22263,
          "end_idx": 22960
        },
        "page_content": "Performance drop on different kinds of adversarial examples with little overlap is not comparable, and thus surpassing other adversarial examples on model performance drop is unnecessary and irrelevant to support our contributions. Therefore, such comparisons are not included in this paper. To verify the validity our generated adversarial examples, we further perform a manual evaluation. For each dataset, using BERT as the target model, we randomly sample 100 successful adversarial examples on which the target model fails, with balanced labels. We blend these adversarial examples with the corresponding original examples, and present each example to three workers on Amazon Mechanical Turk.",
        "type": "Document"
      },
      {
        "id": "15f3c328-7faa-483d-a607-746ff033f482",
        "metadata": {
          "vector_store_key": "1909.02560-5",
          "chunk_id": 50,
          "document_id": "1909.02560",
          "start_idx": 28552,
          "end_idx": 29200
        },
        "page_content": "We generate labeled adversarial examples that can be semantically different from original sentences and the BOW similarity between each pair of sentences is generally low. Such examples reveal robustness issues that previous methods are not able for. The accuracies of the target models drop dramatically on our adversarial examples, while human annotators are much less affected and the modified sentences retain a good grammarticality. We also show that model robustness can be improved using adversarial training with our adversarial examples. Moreover, our adversarial examples can foster future research for further improving model robustness.",
        "type": "Document"
      },
      {
        "id": "7daca693-121d-4398-8687-4dbb273ebe12",
        "metadata": {
          "vector_store_key": "1909.02560-2",
          "chunk_id": 36,
          "document_id": "1909.02560",
          "start_idx": 20941,
          "end_idx": 21759
        },
        "page_content": "After adversarial modifications, the performance of the original target models (those without the \u201c-adv\u201d suffix) drops dramatically (e.g. the overall accuracy of BERT on Quora drops from 94.6% to 24.1%), revealing that the target models are vulnerable to our adversarial examples. Particularly, even though our generation is constrained by a BERT language model, BERT is still vulnerable to our adversarial examples. These results demonstrate the effectiveness of our algorithm for generating adversarial examples and also revealing the corresponding robustness issues. Moreover, we present some generated adversarial examples in the appendix. We notice that the original models are more vulnerable to unmatched adversarial examples, because there are generally more replaceable position choices during the generation.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the delta-softmax calculated?",
    "llm_answer": "The delta-softmax metric ${\\Delta }_s$ is calculated by comparing the ${\\Delta }_s$ for a particular token to the maximum ${\\Delta }_s$ achieved by any token in the current pair and the maximum ${\\Delta }_s$ achieved by any token in the current document.  It depends on the true relation, tokens in the sequence, and the input sequence with a masked position.\n",
    "context": [
      {
        "id": "22ee3e62-8bac-48f5-ae1f-48a7bc882840",
        "metadata": {
          "vector_store_key": "1910.03891-6",
          "chunk_id": 31,
          "document_id": "1910.03891",
          "start_idx": 17858,
          "end_idx": 18596
        },
        "page_content": "The softmax function can be formulated as follows: Hereafter, we implement the attention coefficients $\\pi (h,r,t)$ through a single-layer feedforward neural network, which is formulated as follows: where the leakyRelu is selected as activation function. As shown in Equation DISPLAY_FORM13, the attention coefficient score is depend on the distance head entity $h$ and the tail entity $t$ plus the relation $r$, which follows the idea behind TransE that the embedding $\\textbf {t}$ of head entity should be close to the tail entity's embedding $\\textbf {r}$ plus the relation vector $\\textbf {t}$ if $(h, r, t)$ holds. Embedding Aggregation. To stabilize the learning process of attention, we perform multi-head attention on final layer.",
        "type": "Document"
      },
      {
        "id": "4f8d45fa-8878-4a4b-a11f-e26b8409e169",
        "metadata": {
          "vector_store_key": "2001.02380-6",
          "chunk_id": 81,
          "document_id": "2001.02380",
          "start_idx": 43195,
          "end_idx": 43942
        },
        "page_content": "We call this metric ${\\Delta }_s$ (for delta-softmax), which can be written as: where $rel$ is the true relation of the EDU pair, $t_i$ represents the token at index $i$ of $N$ tokens, and $X_{mask=i}$ represents the input sequence with the masked position $i$ (for $i \\in 1 \\ldots N$ ignoring separators, or $\\phi $, the empty set). To visualize the model's predictions, we compare ${\\Delta }_s$ for a particular token to two numbers: the maximum ${\\Delta }_s$ achieved by any token in the current pair (a measure of relative importance for the current classification) and the maximum ${\\Delta }_s$ achieved by any token in the current document (a measure of how strongly the current relation is signaled compared to other relations in the text).",
        "type": "Document"
      },
      {
        "id": "4f5b19aa-c361-4a70-a43d-334917574552",
        "metadata": {
          "vector_store_key": "2001.00137-1",
          "chunk_id": 17,
          "document_id": "2001.00137",
          "start_idx": 10606,
          "end_idx": 11353
        },
        "page_content": "Softmax $\\sigma $ is a discrete probability distribution function for $N_C$ classes, with the sum of the classes probability being 1 and the maximum value being the predicted class. The predicted class can be mathematically calculated as in Eq. (DISPLAY_FORM8): where $o = W t + b$, the output of the feedforward layer used for classification. In order to evaluate the performance of our model, we need access to a naturally noisy dataset with real human errors. Poor quality texts obtained from Twitter, called tweets, are then ideal for our task. For this reason, we choose Kaggle's two-class Sentiment140 dataset BIBREF18, which consists of spoken text being used in writing and without strong consideration for grammar or sentence correctness.",
        "type": "Document"
      },
      {
        "id": "9ff7a777-eefc-417e-9b77-8ab58fa35df8",
        "metadata": {
          "vector_store_key": "1902.00330-7",
          "chunk_id": 32,
          "document_id": "1902.00330",
          "start_idx": 18107,
          "end_idx": 18972
        },
        "page_content": "The probability distribution is generated as follows:  $$\\pi (a|s) = Softmax(W * h_l(S) + b)$$   (Eq. 18)  Where the $W$ and $b$ are the parameters of the softmax layer. For each mention in the sequence, we will take action to select the target entity from its candidate set. After completing all decisions in the episode, each action will get an expected reward and our goal is to maximize the expected total rewards. Formally, the objective function is defined as:  $$\\begin{split}\nJ(\\Theta ) &= \\mathbb {E}_{(s_t, a_t){\\sim }P_\\Theta {(s_t, a_t)}}R(s_1{a_1}...s_L{a_L}) \\\\\n&=\\sum _{t}\\sum _{a}\\pi _{\\Theta }(a|s)R(a_t)\n\\end{split}$$   (Eq. 19)  Where $P_\\Theta {(s_t, a_t)}$ is the state transfer function, $\\pi _{\\Theta }(a|s)$ indicates the probability of taking action $a$ under the state $s$ , $R(a_t)$ is the expected reward of action $a$ at time step $t$ .",
        "type": "Document"
      },
      {
        "id": "b861d885-7679-4283-a212-5efc30e8b1df",
        "metadata": {
          "vector_store_key": "2002.00876-0",
          "chunk_id": 13,
          "document_id": "2002.00876",
          "start_idx": 7307,
          "end_idx": 8062
        },
        "page_content": "While each is internally implemented using different specialized algorithms and optimizations, from the user's perspective they all utilize the same external distributional API, and pass a generic set of distributional tests. This approach hides the internal complexity of the inference procedure, while giving the user full access to the model. We now describe the technical approach underlying the library. To establish notation first consider the implementation of a categorical distribution, Cat($\\ell $), with one-hot categories $z$ with $z_i = 1$ from a set $\\cal Z$ and probabilities given by the softmax, Define the log-partition as $A(\\ell ) = \\mathrm {LSE}(\\ell )$, i.e. log of the denominator, where $\\mathrm {LSE}$ is the log-sum-exp operator.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which two datasets does the resource come from?",
    "llm_answer": "English Wiktionary and Wikipedia.\n",
    "context": [
      {
        "id": "c2cd706c-2366-42c5-a6db-bdf116dc6d79",
        "metadata": {
          "vector_store_key": "1902.00330-3",
          "chunk_id": 38,
          "document_id": "1902.00330",
          "start_idx": 21409,
          "end_idx": 22127
        },
        "page_content": "It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets. ACE2004 BIBREF15 is a subset of the ACE2004 Coreference documents. MSNBC BIBREF16 contains top two stories in the ten news categories(Politics, Business, Sports etc.) AQUAINT BIBREF17 is a news corpus from the Xinhua News Service, the New York Times, and the Associated Press. WNED-CWEB BIBREF18 is randomly picked from the FACC1 annotated ClueWeb 2012 dataset. WNED-WIKI BIBREF18 is crawled from Wikipedia pages with its original hyperlink annotation. OURSELF-WIKI is crawled by ourselves from Wikipedia pages. During the training of our RLEL model, we select top K candidate entities for each mention to optimize the memory and run time.",
        "type": "Document"
      },
      {
        "id": "2f8e9d5b-32b1-4b15-bfdb-c136c904ee9c",
        "metadata": {
          "vector_store_key": "1910.11493-2",
          "chunk_id": 9,
          "document_id": "1910.11493",
          "start_idx": 5320,
          "end_idx": 6006
        },
        "page_content": "Data for all but four languages (Basque, Kurmanji, Murrinhpatha, and Sorani) are extracted from English Wiktionary, a large multi-lingual crowd-sourced dictionary with morphological paradigms for many lemmata. 20 of the 100 language pairs are either distantly related or unrelated; this allows speculation into the relative importance of data quantity and linguistic relatedness. For each language, the basic data consists of triples of the form (lemma, feature bundle, inflected form), as in tab:sub1data. The first feature in the bundle always specifies the core part of speech (e.g., verb). For each language pair, separate files contain the high- and low-resource training examples.",
        "type": "Document"
      },
      {
        "id": "34df9a0f-974b-460c-9178-c3de80bac6ac",
        "metadata": {
          "vector_store_key": "2002.02224-3",
          "chunk_id": 25,
          "document_id": "2002.02224",
          "start_idx": 14910,
          "end_idx": 15763
        },
        "page_content": "These datasets consist of the individual pairs containing the identification of the decision from which the reference was retrieved, and the identification of the referred documents. As we only extracted references to other judicial decisions, we obtained 471,319 references from Supreme Court decisions, 167,237 references from Supreme Administrative Court decisions and 264,463 references from Constitutional Court Decisions. These are numbers of text spans identified as references prior the further processing described in Section SECREF3. These references include all identifiers extracted from the court decisions contained in the CzCDC 1.0. Therefore, this number includes all other court decisions, including lower courts, the Court of Justice of the European Union, the European Court of Human Rights, decisions of other public authorities etc.",
        "type": "Document"
      },
      {
        "id": "02e8c704-8405-466b-88ff-ff42a3075d4e",
        "metadata": {
          "vector_store_key": "1910.00458-9",
          "chunk_id": 47,
          "document_id": "1910.00458",
          "start_idx": 25974,
          "end_idx": 26708
        },
        "page_content": "The extractive QA tasks primarily focus on locating text spans from the given document/corpus to answer questions BIBREF2. Answers in abstractive datasets such as MS MARCO BIBREF24, SearchQA BIBREF25, and NarrativeQA BIBREF26 are human-generated and based on source documents or summaries in free text format. However, since annotators tend to copy spans as answers BIBREF27, the majority of answers are still extractive in these datasets. The multi-choice QA datasets are collected either via crowd sourcing, or collected from examinations designed by educational experts BIBREF7. In this type of QA datasets, besides token matching, a significant portion of questions require multi-sentence reasoning and external knowledge BIBREF5.",
        "type": "Document"
      },
      {
        "id": "598753ae-f85a-4b8a-91f2-1a842414776d",
        "metadata": {
          "vector_store_key": "1906.03538-2",
          "chunk_id": 43,
          "document_id": "1906.03538",
          "start_idx": 24759,
          "end_idx": 25446
        },
        "page_content": "In addition, we ignore trustworthiness and credibility issues, important research issues that are addressed in other works. The authors would like to thank Jennifer Sheffield, Stephen Mayhew, Shyam Upadhyay, Nitish Gupta and the anonymous reviewers for insightful comments and suggestions. This work was supported in part by a gift from Google and by Contract HR0011-15-2-0025 with the US Defense Advanced Research Projects Agency (DARPA). The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. We provide brief statistics on the sources of different content in our dataset in Table TABREF46 .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the size of the second dataset?",
    "llm_answer": "141 symptom-prescription pairs\n",
    "context": [
      {
        "id": "77d22a71-6f4d-472b-8473-563b5d9ec67b",
        "metadata": {
          "vector_store_key": "1801.09030-2",
          "chunk_id": 33,
          "document_id": "1801.09030",
          "start_idx": 17171,
          "end_idx": 17794
        },
        "page_content": "The second test dataset has been proofread and all of the prescriptions are the most classical and influential ones in the history. So the quality is much better than the first one. However, the number of the cases is limited. There are 141 symptom-prescription pairs in the second dataset. Thus we use two test sets to do evaluation to take the advantages of both data magnitude and quality. In our experiments, we implement our models with the PyTorch toolkit . We set the embedding size of both Chinese characters in the symptoms and the herb tokens to 100. We set the hidden state size to 300, and the batch size to 20.",
        "type": "Document"
      },
      {
        "id": "f2791272-1042-4774-b6d5-46dd96b7e65b",
        "metadata": {
          "vector_store_key": "1704.00253-2",
          "chunk_id": 24,
          "document_id": "1704.00253",
          "start_idx": 13943,
          "end_idx": 14614
        },
        "page_content": "For a fair comparison, all cleaned synthetic parallel data have equal sizes. The summary of the final parallel corpora is presented in Table 1 . All networks have 1024 hidden units and 500 dimensional embeddings. The vocabulary size is limited to 30K for each language. Each model is trained for 10 epochs using stochastic gradient descent with Adam BIBREF22 . The Minibatch size is 80, and the training set is reshuffled between every epoch. The norm of the gradient is clipped not to exceed 1.0 BIBREF23 . The learning rate is $2 \\cdot 10^{-4}$ in every case. We use the newstest 2012 set for a development set and the newstest 2011 and newstest 2013 sets as test sets.",
        "type": "Document"
      },
      {
        "id": "0fcc020b-3016-4745-902c-d73226505bf1",
        "metadata": {
          "vector_store_key": "1910.11949-3",
          "chunk_id": 17,
          "document_id": "1910.11949",
          "start_idx": 9318,
          "end_idx": 10010
        },
        "page_content": "Each source contains 5,000 images with 5 questions per image, adding a total of 15,000 images with 75,000 questions. COCO dataset includes images of complex everyday scenes containing common objects in their natural context, but it is limited in terms of the concepts it covers. Bing dataset contains more event related questions and has a wider range of questions longitudes (between 3 and 20 words), while Flickr questions are shorter (less than 6 words) and the images appear to be more casual. We use two datasets to train our chatbot model. The first one is the Persona-chat BIBREF15 which contains dialogues between two people with different profiles that are trying to know each other.",
        "type": "Document"
      },
      {
        "id": "4a420f91-0ffd-440c-b176-e1aa838223ea",
        "metadata": {
          "vector_store_key": "1709.05404-4",
          "chunk_id": 33,
          "document_id": "1709.05404",
          "start_idx": 17489,
          "end_idx": 18127
        },
        "page_content": "It is interesting to note that the overall results of the Hyp data are lower than those for Gen and RQs, likely due to the smaller size of the Hyp dataset. To examine the effect of dataset size, we compare F-measure (using the same 10-fold cross-validation setup) for each dataset while holding the number of posts per class constant. Figure FIGREF26 shows the performance of each of the Gen, RQ, and Hyp datasets at intervals of 100 posts per class (up to the maximum size of 582 posts per class for Hyp, and 851 posts per class for RQ). From the graph, we can see that as a general trend, the datasets benefit from larger dataset sizes.",
        "type": "Document"
      },
      {
        "id": "52a5c7b9-197c-4c52-9a22-93a7f3f1bb7a",
        "metadata": {
          "vector_store_key": "1910.12618-2",
          "chunk_id": 45,
          "document_id": "1910.12618",
          "start_idx": 24114,
          "end_idx": 24750
        },
        "page_content": "In order to tone it down and to increase the consistency of our results, the different models are run $B=10$ times. The results presented hereafter correspond to the average and standard-deviation on those runs. The RF model denoted as \"sel\" is the one with the reduced number of features, whereas the other RF uses the full vocabulary. We also considered an aggregated forecaster (abridged Agg), consisting of the average of the two best individual ones in terms of RMSE. All the neural network methods have a reduced vocabulary size $V^*$. The results for the French and UK data are respectively given by tables TABREF26 and TABREF27.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How large is the first dataset?",
    "llm_answer": "12,350 ads\n",
    "context": [
      {
        "id": "d87d327c-2858-4b5d-ae8a-860d401f63bd",
        "metadata": {
          "vector_store_key": "1908.05434-1",
          "chunk_id": 27,
          "document_id": "1908.05434",
          "start_idx": 14432,
          "end_idx": 15064
        },
        "page_content": "We have ensured that the raw dataset has no overlap with the labeled dataset to avoid bias in test accuracy. While it is possible to scrape more raw data, we did not observe significant improvements in model performances when the size of raw data increased from INLINEFORM0 70,000 to INLINEFORM1 170,000, hence we assume that the current raw dataset is sufficiently large. The labeled dataset is called Trafficking-10k. It consists of 12,350 ads from Backpage labeled by experts in human trafficking detection BIBREF9 . Each label is one of seven ordered levels of likelihood that the corresponding ad comes from a human trafficker.",
        "type": "Document"
      },
      {
        "id": "0fcc020b-3016-4745-902c-d73226505bf1",
        "metadata": {
          "vector_store_key": "1910.11949-3",
          "chunk_id": 17,
          "document_id": "1910.11949",
          "start_idx": 9318,
          "end_idx": 10010
        },
        "page_content": "Each source contains 5,000 images with 5 questions per image, adding a total of 15,000 images with 75,000 questions. COCO dataset includes images of complex everyday scenes containing common objects in their natural context, but it is limited in terms of the concepts it covers. Bing dataset contains more event related questions and has a wider range of questions longitudes (between 3 and 20 words), while Flickr questions are shorter (less than 6 words) and the images appear to be more casual. We use two datasets to train our chatbot model. The first one is the Persona-chat BIBREF15 which contains dialogues between two people with different profiles that are trying to know each other.",
        "type": "Document"
      },
      {
        "id": "711988d5-b6d5-455f-a849-15926d3a08fb",
        "metadata": {
          "vector_store_key": "1910.12618-2",
          "chunk_id": 12,
          "document_id": "1910.12618",
          "start_idx": 6819,
          "end_idx": 7418
        },
        "page_content": "The rest of this paper is organized as follows. The following section introduces the two data sets used to conduct our study. Section 3 presents the different machine learning approaches used and how they were tuned. Section 4 highlights the main results of our study, while section 5 concludes this paper and gives insight on future possible work. In order to prove the consistency of our work, experiments have been conducted on two data sets, one for France and the other for the UK. In this section details about the text and time series data are given, as well as the major preprocessing steps.",
        "type": "Document"
      },
      {
        "id": "5ddc0045-f341-48fb-a602-1e299ee68078",
        "metadata": {
          "vector_store_key": "1808.08850-2",
          "chunk_id": 26,
          "document_id": "1808.08850",
          "start_idx": 14603,
          "end_idx": 15207
        },
        "page_content": "S1 was better in 2 occasions only for INLINEFORM1 ; S1 overperformed S2 in 3 occasions concerning INLINEFORM2 and in 4 occasions for INLINEFORM3 (bold). Also from Table TABREF30 we can observe that INLINEFORM0 has a bigger similarity to S1 in 5 occasions compared to other two references, while INLINEFORM1 is more similar to S2 in 7 transcripts (underline). After computing the mean F1 scores over the transcripts, it can be concluded that in average S2 had a better performance segmenting the dataset compared to S1, obtaining a F1 score equal to 0.510. But... What about the complexity of the dataset?",
        "type": "Document"
      },
      {
        "id": "f2791272-1042-4774-b6d5-46dd96b7e65b",
        "metadata": {
          "vector_store_key": "1704.00253-2",
          "chunk_id": 24,
          "document_id": "1704.00253",
          "start_idx": 13943,
          "end_idx": 14614
        },
        "page_content": "For a fair comparison, all cleaned synthetic parallel data have equal sizes. The summary of the final parallel corpora is presented in Table 1 . All networks have 1024 hidden units and 500 dimensional embeddings. The vocabulary size is limited to 30K for each language. Each model is trained for 10 epochs using stochastic gradient descent with Adam BIBREF22 . The Minibatch size is 80, and the training set is reshuffled between every epoch. The norm of the gradient is clipped not to exceed 1.0 BIBREF23 . The learning rate is $2 \\cdot 10^{-4}$ in every case. We use the newstest 2012 set for a development set and the newstest 2011 and newstest 2013 sets as test sets.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Who was the top-scoring team?",
    "llm_answer": "N-GrAM\n",
    "context": [
      {
        "id": "881327b3-fadf-4da1-b31b-7c5f8db473b2",
        "metadata": {
          "vector_store_key": "1707.03764-3",
          "chunk_id": 28,
          "document_id": "1707.03764",
          "start_idx": 14236,
          "end_idx": 14932
        },
        "page_content": "Overall, N-GrAM came first in the shared task, with a score of 0.8253 for gender 0.9184 for variety, a joint score of 0.8361 and an average score of 0.8599 (final rankings were taken from this average score BIBREF0 ). For the global scores, all languages are combined. We present finer-grained scores showing the breakdown per language in Table TABREF24 . We compare our gender and variety accuracies against the LDR-baseline BIBREF10 , a low dimensionality representation especially tailored to language variety identification, provided by the organisers. The final column, + 2nd shows the difference between N-GrAM and that achieved by the second-highest ranked system (excluding the baseline).",
        "type": "Document"
      },
      {
        "id": "b07216f6-894d-447e-bbfa-69052ba50e92",
        "metadata": {
          "vector_store_key": "1804.03396-0",
          "chunk_id": 39,
          "document_id": "1804.03396",
          "start_idx": 21263,
          "end_idx": 21989
        },
        "page_content": "Two metrics are introduced in the SQuAD dataset: Exact Match (EM) and F1-score. EM measures the percentage that the model prediction matches one of the ground truth answers exactly while F1-score measures the overlap between the prediction and ground truth answers. Our QA4IE benchmark also adopts these two metrics. Table 3 presents the results of our QA model on SQuAD dataset. Our model outperforms the previous sequence model but is not competitive with span models because it is designed to produce sequence answers in IE settings while baseline span models are designed to produce span answers for SQuAD dataset. The comparison between our QA model and two baseline QA models on our QA4IE benchmark is shown in Table 4 .",
        "type": "Document"
      },
      {
        "id": "2d960f20-764f-4b61-a4d4-bfe76e6d30f8",
        "metadata": {
          "vector_store_key": "1707.03764-3",
          "chunk_id": 29,
          "document_id": "1707.03764",
          "start_idx": 14932,
          "end_idx": 15668
        },
        "page_content": "The final column, + 2nd shows the difference between N-GrAM and that achieved by the second-highest ranked system (excluding the baseline). Results are broken down per language, and are summarised as both joint and average scores. The joint score is is the percentage of texts for which both gender and variety were predicted correctly at the same time. The average is calculated as the mean over all languages. N-GrAM ranked first in all cases except for the language variety task. In this case, the baseline was the top-ranked system, and ours was second by a small margin. Our system significantly out-performed the baseline on the joint task, as the baseline scored significantly lower for the gender task than for the variety task.",
        "type": "Document"
      },
      {
        "id": "f8e7499a-4f36-4e69-af31-27bce7c31517",
        "metadata": {
          "vector_store_key": "1909.07734-4",
          "chunk_id": 18,
          "document_id": "1909.07734",
          "start_idx": 10355,
          "end_idx": 10968
        },
        "page_content": "The submissions and the final results are summarized in Tables and . Two of the submissions did not follow up with technical papers and thus they do not appear in this summary. We note that the top-performing models used BERT, reflecting the recent state-of-the-art performance of this model in many NLP tasks. For Friends and EmotionPush the top micro-F1 scores were 81.5% and 88.5% respectively. An evaluation summary of the submissions is available in Tables and . We only present the teams that submitted technical reports. A full leaderboard that includes all the teams is available on the challenge website.",
        "type": "Document"
      },
      {
        "id": "1ae126c1-8673-4f37-aa7f-e26b562233b7",
        "metadata": {
          "vector_store_key": "1704.05572-3",
          "chunk_id": 8,
          "document_id": "1704.05572",
          "start_idx": 4120,
          "end_idx": 4712
        },
        "page_content": "Given a multiple-choice question $qa$ with question text $q$ and answer choices A= $\\lbrace a_i\\rbrace $ , we select the most relevant tuples from $T$ and $S$ as follows. Selecting from Tuple KB: We use an inverted index to find the 1,000 tuples that have the most overlapping tokens with question tokens $tok(qa).$ . We also filter out any tuples that overlap only with $tok(q)$ as they do not support any answer. We compute the normalized TF-IDF score treating the question, $q$ as a query and each tuple, $t$ as a document: $\n&\\textit {tf}(x, q)=1\\; \\textmd {if x} \\in q ; \\textit {idf}(x)",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Why is big data not appropriate for this task?",
    "llm_answer": "\"Big\" datasets are not always available, particularly in computational social science NLP applications, where the data of interest are often from sources like press releases, academic journals, books, and transcripts of recorded speech, rather than large-scale internet or social media sources.\n",
    "context": [
      {
        "id": "b3cdca23-cd1a-4e37-80f1-a79c8db93ffa",
        "metadata": {
          "vector_store_key": "1705.07368-0",
          "chunk_id": 3,
          "document_id": "1705.07368",
          "start_idx": 1892,
          "end_idx": 2593
        },
        "page_content": "In this work, I offer a somewhat contrarian perspective to the currently prevailing trend of big data optimism, as exemplified by the work of BIBREF0 , BIBREF1 , BIBREF3 , and others, who argue that massive datasets are sufficient to allow language models to automatically resolve many challenging NLP tasks. Note that \u201cbig\u201d datasets are not always available, particularly in computational social science NLP applications, where the data of interest are often not obtained from large scale sources such as the internet and social media, but from sources such as press releases BIBREF11 , academic journals BIBREF10 , books BIBREF12 , and transcripts of recorded speech BIBREF13 , BIBREF14 , BIBREF15 .",
        "type": "Document"
      },
      {
        "id": "9dd55468-5bca-4d24-8c18-3cc32bc2ea74",
        "metadata": {
          "vector_store_key": "1705.07368-0",
          "chunk_id": 2,
          "document_id": "1705.07368",
          "start_idx": 1181,
          "end_idx": 1892
        },
        "page_content": "Word embeddings have risen in popularity for NLP applications due to the success of models designed specifically for the big data setting. In particular, BIBREF0 , BIBREF1 showed that very simple word embedding models with high-dimensional representations can scale up to massive datasets, allowing them to outperform more sophisticated neural network language models which can process fewer documents. In this work, I offer a somewhat contrarian perspective to the currently prevailing trend of big data optimism, as exemplified by the work of BIBREF0 , BIBREF1 , BIBREF3 , and others, who argue that massive datasets are sufficient to allow language models to automatically resolve many challenging NLP tasks.",
        "type": "Document"
      },
      {
        "id": "2142e3ea-785b-4a8d-a58c-f289a860a3c1",
        "metadata": {
          "vector_store_key": "1705.07368-0",
          "chunk_id": 4,
          "document_id": "1705.07368",
          "start_idx": 1893,
          "end_idx": 2508
        },
        "page_content": "Note that \u201cbig\u201d datasets are not always available, particularly in computational social science NLP applications, where the data of interest are often not obtained from large scale sources such as the internet and social media, but from sources such as press releases BIBREF11 , academic journals BIBREF10 , books BIBREF12 , and transcripts of recorded speech BIBREF13 , BIBREF14 , BIBREF15 . A standard practice in the literature is to train word embedding models on a generic large corpus such as Wikipedia, and use the embeddings for NLP tasks on the target dataset, cf. BIBREF3 , BIBREF0 , BIBREF16 , BIBREF17 .",
        "type": "Document"
      },
      {
        "id": "a3fe60d8-57d9-453a-b8b7-c6a050171dd8",
        "metadata": {
          "vector_store_key": "1701.06538-2",
          "chunk_id": 0,
          "document_id": "1701.06538",
          "start_idx": 0,
          "end_idx": 609
        },
        "page_content": "Exploiting scale in both training data and model size has been central to the success of deep learning. When datasets are sufficiently large, increasing the capacity (number of parameters) of neural networks can give much better prediction accuracy. This has been shown in domains such as text BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , images BIBREF4 , BIBREF5 , and audio BIBREF6 , BIBREF7 . For typical deep learning models, where the entire model is activated for every example, this leads to a roughly quadratic blow-up in training costs, as both the model size and the number of training examples increase.",
        "type": "Document"
      },
      {
        "id": "d97c491a-0bc5-4b2a-9b3d-3bc9d49d6eeb",
        "metadata": {
          "vector_store_key": "1602.08741-4",
          "chunk_id": 6,
          "document_id": "1602.08741",
          "start_idx": 3369,
          "end_idx": 4002
        },
        "page_content": "Twitter is not usually considered, and intuition behind this is that probably every-day language is too simple and too occasional to produce good results. On the other hand, the real-time nature of this user message stream seems promising, as it may reveal what certain word means in this given moment. The other counter-argument against Twitter-as-Dataset is the policy of Twitter, which disallows publication of any dump of Twitter messages larger than 50K . However, this policy permits publication of Twitter IDs in any amount. Thus the secondary goal of this paper is to describe how to create this kind of dataset from scratch.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is an example of a computational social science NLP task?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "66a6e3ab-e5b0-40ff-b855-d461a70e27a8",
        "metadata": {
          "vector_store_key": "1807.03367-3",
          "chunk_id": 58,
          "document_id": "1807.03367",
          "start_idx": 33697,
          "end_idx": 34454
        },
        "page_content": "The Talk the Walk task and dataset facilitate future research on various important subfields of artificial intelligence, including grounded language learning, goal-oriented dialogue research and situated navigation. Here, we describe related previous work in these areas. paragraph4 0.1ex plus0.1ex minus.1ex-1em Related tasks There has been a long line of work involving related tasks. Early work on task-oriented dialogue dates back to the early 90s with the introduction of the Map Task BIBREF11 and Maze Game BIBREF25 corpora. Recent efforts have led to larger-scale goal-oriented dialogue datasets, for instance to aid research on visually-grounded dialogue BIBREF2 , BIBREF1 , knowledge-base-grounded discourse BIBREF29 or negotiation tasks BIBREF36 .",
        "type": "Document"
      },
      {
        "id": "ecff7bfa-6797-4658-a750-40ec5f395b03",
        "metadata": {
          "vector_store_key": "1910.02789-9",
          "chunk_id": 4,
          "document_id": "1910.02789",
          "start_idx": 2345,
          "end_idx": 3167
        },
        "page_content": "Over the past few years, Natural Language Processing (NLP) has shown an acceleration in progress on a wide range of downstream applications ranging from Question Answering BIBREF7, BIBREF8, to Natural Language Inference BIBREF9, BIBREF10, BIBREF11 through Syntactic Parsing BIBREF12, BIBREF13, BIBREF14. Recent work has shown the ability to learn flexible, hierarchical, contextualized representations, obtaining state-of-the-art results on various natural language processing tasks BIBREF15. A basic observation of our work is that natural language representations are also beneficial for solving problems in which natural language is not the underlying source of input. Moreover, our results indicate that natural language is a strong alternative to current complementary methods for semantic representations of a state.",
        "type": "Document"
      },
      {
        "id": "941c76ce-e3f3-4306-a654-b2cea20bf28a",
        "metadata": {
          "vector_store_key": "1804.00079-1",
          "chunk_id": 12,
          "document_id": "1804.00079",
          "start_idx": 6307,
          "end_idx": 7067
        },
        "page_content": "Second, their work aims for improvements on the same tasks on which the model is trained, as opposed to learning re-usable sentence representations that transfer elsewhere. We further present a fine-grained analysis of how different tasks contribute to the encoding of different information signals in our representations following work by BIBREF14 and BIBREF17 . BIBREF23 similarly present a multi-task framework for textual entailment with task supervision at different levels of learning. \u201cUniversal\" multi-task models have also been successfully explored in the context of computer vision problems BIBREF24 , BIBREF25 . Five out of the six tasks that we consider for multi-task learning are formulated as sequence-to-sequence problems BIBREF26 , BIBREF27 .",
        "type": "Document"
      },
      {
        "id": "5b971679-1c33-44e3-aa78-464bf8f2f5bd",
        "metadata": {
          "vector_store_key": "1910.05456-0",
          "chunk_id": 1,
          "document_id": "1910.05456",
          "start_idx": 704,
          "end_idx": 1399
        },
        "page_content": "Within the area of natural language processing (NLP) research, experimenting on neural network models just as if they were human subjects has recently been gaining popularity BIBREF3, BIBREF4, BIBREF5. Often, so-called probing tasks are used, which require a specific subset of linguistic knowledge and can, thus, be leveraged for qualitative evaluation. The goal is to answer the question: What do neural networks learn that helps them to succeed in a given task? Neural network models, and specifically sequence-to-sequence models, have pushed the state of the art for morphological inflection \u2013 the task of learning a mapping from lemmata to their inflected forms \u2013 in the last years BIBREF6.",
        "type": "Document"
      },
      {
        "id": "63f3e389-c681-4ed1-ad95-50520b8a7526",
        "metadata": {
          "vector_store_key": "1905.11037-1",
          "chunk_id": 1,
          "document_id": "1905.11037",
          "start_idx": 690,
          "end_idx": 1481
        },
        "page_content": "2017arXiv171207040K introduce NarrativeQA, where answering the questions requires to process entire stories. In a related line, 2017arXiv171011601F use fictional crime scene investigation data, from the CSI series, to define a task where the models try to answer the question: \u2018who committed the crime?\u2019. In an alternative line of work, script induction BIBREF10 has been also a useful approach to evaluate inference and semantic capabilities of nlp systems. Here, a model processes a document to infer new sequences that reflect events that are statistically probable (e.g. go to a restaurant, be seated, check the menu, ...). For example, chambers2008unsupervised introduce narrative event chains, a representation of structured knowledge of a set of events occurring around a protagonist.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which major geographical regions are studied?",
    "llm_answer": "Latin America and the Caribbean, South Asia, Sub-Saharan Africa, Europe and Central Asia, Middle East and North Africa, East Asia and the Pacific, North America.\n",
    "context": [
      {
        "id": "146edbe3-80de-4947-acc1-3a0e77ff5b01",
        "metadata": {
          "vector_store_key": "1809.02494-3",
          "chunk_id": 15,
          "document_id": "1809.02494",
          "start_idx": 8253,
          "end_idx": 9022
        },
        "page_content": "Among them, we can highlight the creation of models of geographical descriptors, comparing models between both subject groups, studying combinations of models of cardinal directions, and researching on geographical referring expression generation. Furthermore, insights about the semantics of geographical concepts could be inferred under a more thorough analysis. One of the inconveniences that our data sets present is the appearance of the issues described in Sec. SECREF10 . It could be necessary to filter some of the answers according to different criteria (e.g., deviation of the centroid location, deviation of size, etc.). For more applied cases, manually filtering can also be an option, but this would require a certain knowledge of the geography of Galicia.",
        "type": "Document"
      },
      {
        "id": "d82d6c29-8b8c-4059-95cf-3d56c77fbaa8",
        "metadata": {
          "vector_store_key": "1708.05873-1",
          "chunk_id": 31,
          "document_id": "1708.05873",
          "start_idx": 17790,
          "end_idx": 18388
        },
        "page_content": "The fact that there is no effect of conflict on Topic 2 is interesting in this regard. Finally, we consider regional effects in Figure FIGREF14 . We use the World Bank's classifications of regions: Latin America and the Caribbean (LCN), South Asia (SAS), Sub-Saharan Africa (SSA), Europe and Central Asia (ECS), Middle East and North Africa (MEA), East Asia and the Pacific (EAS), North America (NAC). The figure shows that states in South Asia, and Latin America and the Caribbean are likely to discuss Topic 2 the most. States in South Asia and East Asia and the Pacific discuss Topic 7 the most.",
        "type": "Document"
      },
      {
        "id": "8263997d-1d98-4963-95fb-37f021b05a82",
        "metadata": {
          "vector_store_key": "1809.02494-3",
          "chunk_id": 17,
          "document_id": "1809.02494",
          "start_idx": 9333,
          "end_idx": 10148
        },
        "page_content": "These should provide a variety of different shapes (both regular and irregular), so that it can be feasible to generalize (e.g., through data-driven approaches) the semantics of some of the more common descriptors, such as cardinal points, coastal areas, etc. The proposal of a shared task could help achieve this objective. This research was supported by the Spanish Ministry of Economy and Competitiveness (grants TIN2014-56633-C3-1-R and TIN2017-84796-C2-1-R) and the Galician Ministry of Education (grants GRC2014/030 and \"accreditation 2016-2019, ED431G/08\"). All grants were co-funded by the European Regional Development Fund (ERDF/FEDER program). A. Ramos-Soto is funded by the \u201cConseller\u00eda de Cultura, Educaci\u00f3n e Ordenaci\u00f3n Universitaria\u201d (under the Postdoctoral Fellowship accreditation ED481B 2017/030).",
        "type": "Document"
      },
      {
        "id": "6cef21e1-1539-410e-8d6c-10636b485f75",
        "metadata": {
          "vector_store_key": "1809.02494-2",
          "chunk_id": 4,
          "document_id": "1809.02494",
          "start_idx": 2227,
          "end_idx": 2833
        },
        "page_content": "It was answered by 15/16 year old students in a high school in Pontevedra (located in Western Galicia). 99 students provided answers for a list of 7 descriptors (including cardinal points, coast, inland, and a proper name). Figure FIGREF2 shows a representation of the answers given by the students for \u201cNorthern Galicia\u201d and a contour map that illustrates the percentages of overlapping answers. The second survey was addressed to meteorologists in the Galician Weather Agency BIBREF12 . Its purpose was to gather data to create fuzzy models that will be used in a future NLG system in the weather domain.",
        "type": "Document"
      },
      {
        "id": "23d79c6b-1661-4410-a728-1ba1d6f10fca",
        "metadata": {
          "vector_store_key": "1809.02494-2",
          "chunk_id": 9,
          "document_id": "1809.02494",
          "start_idx": 4919,
          "end_idx": 5681
        },
        "page_content": "If we focus on single descriptors, one interesting outcome is that some of the answers for \u201cNorthern Galicia\u201d and \u201cSouthern Galicia\u201d overlap for both subject groups. Thus, although `north' and `south' are natural antonyms, if we take into account the opinion of each group as a whole, there exists a small area where points can be considered as belonging to both descriptors at the same time (see Fig. FIGREF9 ). In the case of \u201cwest\u201d and \u201ceast\u201d, the drawings made by the experts were almost divergent and showed no overlapping between those two descriptors. Regarding \u201cInland Galicia\u201d, the unions of the answers for each group occupy approximately the same area with a similar shape, but there is a very high overlapping among the answers of the meteorologists.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How strong is the correlation between the prevalence of the #MeToo movement and official reports [of sexual harassment]?",
    "llm_answer": "The correlation is statistically significant ($p-value<0.05$).  A change in reported cases corresponds to a substantial change in #MeToo users.\n",
    "context": [
      {
        "id": "dee09d57-2eab-4e61-a542-7023d22c2d4d",
        "metadata": {
          "vector_store_key": "2001.05970-0",
          "chunk_id": 25,
          "document_id": "2001.05970",
          "start_idx": 13749,
          "end_idx": 14433
        },
        "page_content": "Observing the results of the linear regression in Table 2, we find the normalized governmental reported cases count and regional feature to be statistically significant on the sexual harassment rate in the Twitter data ($p-value<0.05$). Specifically, the change in the number of reported cases constitutes a considerable change in the number of #MeToo users on Twitter as p-value is extremely small at $5.7e-13$. This corresponds to the research by Napolitano (2014) regarding the \"Yes means yes\" movement in higher education institutes in recent years, as even with some limitations and inconsistency, the sexual assault reporting system is gradually becoming more rigorous BIBREF17.",
        "type": "Document"
      },
      {
        "id": "d0191782-aaff-4cd1-9c30-a0329698533e",
        "metadata": {
          "vector_store_key": "2001.05970-0",
          "chunk_id": 26,
          "document_id": "2001.05970",
          "start_idx": 13926,
          "end_idx": 14700
        },
        "page_content": "This corresponds to the research by Napolitano (2014) regarding the \"Yes means yes\" movement in higher education institutes in recent years, as even with some limitations and inconsistency, the sexual assault reporting system is gradually becoming more rigorous BIBREF17. Meanwhile, attending colleges in the Northeast, West and South regions increases the possibility of posting about sexual harassment (positive coefficients), over the Midwest region. This finding is interesting and warrants further scrutiny. We discover that approximately half of users who detailed their sexual harassment experiences with the #MeToo hashtag suffered from physical aggression. Also, more than half of them claimed to encounter the perpetrators outside the college and work environment.",
        "type": "Document"
      },
      {
        "id": "f9fb169c-67da-4bf2-83ac-8f9f6b9061fe",
        "metadata": {
          "vector_store_key": "2001.05970-2",
          "chunk_id": 24,
          "document_id": "2001.05970",
          "start_idx": 12987,
          "end_idx": 13749
        },
        "page_content": "In fact, in our top 5 topics, Topics 1 and 5 mainly depict gruesome stories and childhood or college time experience. This finding seems to support the validity of the Twitter sample of Modrek and Chakalov (2019), where 11% discloses personal sexual harassment memories and 5.8% of them was in formative years BIBREF10. These users also shows multiple emotions toward this movement, such as compassion (topic 2), determination (topic 3), and hope (topic 4). We will further examine the emotion features in the latter results. Observing the results of the linear regression in Table 2, we find the normalized governmental reported cases count and regional feature to be statistically significant on the sexual harassment rate in the Twitter data ($p-value<0.05$).",
        "type": "Document"
      },
      {
        "id": "d4dd0032-34ce-445f-946b-de348ebf8c32",
        "metadata": {
          "vector_store_key": "2001.05970-2",
          "chunk_id": 2,
          "document_id": "2001.05970",
          "start_idx": 1095,
          "end_idx": 1808
        },
        "page_content": "On October 15 of 2017, prominent Hollywood actress Alyssa Milano, by accusing Oscar-winning film producer, Harvey Weinstein, for multiple sexual impropriety attempts on herself and many other women in the film industry, ignited the \"MeToo\" trend on social media that called for women and men to share their own sexual harassment experience. According to CNN, over 1.7 million users had used the hash-tag in 85 countries. Benefiting from the tremendous amount of data supplied by this trend and the existing state-of-the-art semantic parser and generative statistical models, we propose a new approach to characterizing sexual harassment by mining the tweets from college users with the hash-tag #metoo on Twitter.",
        "type": "Document"
      },
      {
        "id": "b627131f-4b9f-4a82-991a-e13cd60ad197",
        "metadata": {
          "vector_store_key": "2001.05970-2",
          "chunk_id": 1,
          "document_id": "2001.05970",
          "start_idx": 899,
          "end_idx": 1703
        },
        "page_content": "However, these studies on college sexual misconduct usually collect data based on questionnaires from a small sample of the college population, which might not be sufficiently substantial to capture the big picture of sexual harassment risk of the entire student body. Alternatively, social media opens up new opportunities to gather a larger and more comprehensive amount of data and mitigate the risk of false or inaccurate narratives from the studied subjects. On October 15 of 2017, prominent Hollywood actress Alyssa Milano, by accusing Oscar-winning film producer, Harvey Weinstein, for multiple sexual impropriety attempts on herself and many other women in the film industry, ignited the \"MeToo\" trend on social media that called for women and men to share their own sexual harassment experience.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are the topics embedded in the #MeToo tweets extracted?",
    "llm_answer": "Latent Dirichlet Allocation (LDA) is used to label universal topics demonstrated by the users.  The optimal topic number is determined by selecting the one with the highest coherence score.\n",
    "context": [
      {
        "id": "33898516-18d5-412c-a858-11ee45bf88f3",
        "metadata": {
          "vector_store_key": "2001.05970-4",
          "chunk_id": 14,
          "document_id": "2001.05970",
          "start_idx": 7605,
          "end_idx": 8446
        },
        "page_content": "We examine 6,760 tweets from the most relevant topic of our LDA model, and categorize them based on the following metrics: harassment types (verbal, physical, and visual abuse) and context (peer-to-peer, school employee or work employer, and third-parties). These labels are based on definitions by the U.S. Dept. of Education BIBREF14. In order to understand the latent topics of those #MeToo tweets for college followers, we first utilize Latent Dirichlet Allocation (LDA) to label universal topics demonstrated by the users. We determine the optimal topic number by selecting the one with the highest coherence score. Since certain words frequently appear in those #MeToo tweets (e.g., sexual harassment, men, women, story, etc.), we transform our corpus using TF-IDF, a term-weighting scheme that discounts the influence of common terms.",
        "type": "Document"
      },
      {
        "id": "89153ac8-a389-46ab-a4af-edaf0d9c05ed",
        "metadata": {
          "vector_store_key": "2001.05970-2",
          "chunk_id": 8,
          "document_id": "2001.05970",
          "start_idx": 4325,
          "end_idx": 5007
        },
        "page_content": "We utilize the Jefferson-Henrique script, a web scraper designed for Twitter to retrieve a total of over 300,000 #MeToo tweets from October 15th, when Alyssa Milano posted the inceptive #MeToo tweet, to November 15th of 2017 to cover a period of a month when the trend was on the rise and attracting mass concerns. Since the lists of the followers of the studied colleges might overlap and many Twitter users tend to reiterate other's tweets, simply putting all the data collected together could create a major redundancy problem. We extract unique users and tweets from the combined result set to generate a dataset of about 60,000 unique tweets, pertaining to 51,104 unique users.",
        "type": "Document"
      },
      {
        "id": "4c8af9d2-98e8-4980-9515-5c49c3a50a2f",
        "metadata": {
          "vector_store_key": "2001.05970-4",
          "chunk_id": 23,
          "document_id": "2001.05970",
          "start_idx": 12597,
          "end_idx": 13266
        },
        "page_content": "Because our predictive model computes event-entity sentiment scores and generates verb predicate knowledge simultaneously, it is sensitive to data initialization. Therefore, we train the model iteratively on a number of random initialization to achieve the best results. The results of LDA on #MeToo tweets of college users (Table 1) fall into the same pattern as the research of Modrek and Chakalov (2019), which suggests that a large portion of #MeToo tweets on Twitter focuses on sharing personal traumatic stories about sexual harassment BIBREF10. In fact, in our top 5 topics, Topics 1 and 5 mainly depict gruesome stories and childhood or college time experience.",
        "type": "Document"
      },
      {
        "id": "2640e469-17b2-4b44-8b53-7e59fbef48fa",
        "metadata": {
          "vector_store_key": "2001.05970-2",
          "chunk_id": 3,
          "document_id": "2001.05970",
          "start_idx": 1808,
          "end_idx": 2567
        },
        "page_content": "Benefiting from the tremendous amount of data supplied by this trend and the existing state-of-the-art semantic parser and generative statistical models, we propose a new approach to characterizing sexual harassment by mining the tweets from college users with the hash-tag #metoo on Twitter. Our main contributions are several folds. We investigate campus sexual harassment using a big-data approach by collecting data from Twitter. We employ traditional topic modeling and linear regression methods on a new dataset to highlight patterns of the ongoing troubling social behaviors at both institutional and individual levels. We propose a novel approach to combining domain-general deep semantic parsing and sentiment analysis to dissect personal narratives.",
        "type": "Document"
      },
      {
        "id": "745b31f1-9f9a-4ceb-a799-4449ea66540b",
        "metadata": {
          "vector_store_key": "1904.00648-2",
          "chunk_id": 19,
          "document_id": "1904.00648",
          "start_idx": 10653,
          "end_idx": 11294
        },
        "page_content": "In the second, together with the word embeddings we use the POS and chunk tag. In the third, all the features previously defined are included, in addition to the word embeddings. For every experiment, we use both the pre-trained embeddings and the ones that we created with our Twitter corpora. In section 4, results obtained from the several experiments are reported. The bot-generated tweets present a predefined structure and a formal language, which facilitates the entities detection. In this dataset, our goal is to assign to each track played on the radio, represented by a tweet, a list of entities extracted from the tweet raw text.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which geographical regions correlate to the trend?",
    "llm_answer": "South Asia, Latin America and the Caribbean, East Asia and the Pacific, North America.\n",
    "context": [
      {
        "id": "d82d6c29-8b8c-4059-95cf-3d56c77fbaa8",
        "metadata": {
          "vector_store_key": "1708.05873-1",
          "chunk_id": 31,
          "document_id": "1708.05873",
          "start_idx": 17790,
          "end_idx": 18388
        },
        "page_content": "The fact that there is no effect of conflict on Topic 2 is interesting in this regard. Finally, we consider regional effects in Figure FIGREF14 . We use the World Bank's classifications of regions: Latin America and the Caribbean (LCN), South Asia (SAS), Sub-Saharan Africa (SSA), Europe and Central Asia (ECS), Middle East and North Africa (MEA), East Asia and the Pacific (EAS), North America (NAC). The figure shows that states in South Asia, and Latin America and the Caribbean are likely to discuss Topic 2 the most. States in South Asia and East Asia and the Pacific discuss Topic 7 the most.",
        "type": "Document"
      },
      {
        "id": "a8799de9-f1e5-464f-b4b7-c9dc7b5af303",
        "metadata": {
          "vector_store_key": "1904.07342-1",
          "chunk_id": 13,
          "document_id": "1904.07342",
          "start_idx": 7276,
          "end_idx": 7980
        },
        "page_content": "The clusters correspond to four major regions of the U.S.: the Northeast (green), Southeast (yellow), Midwest (blue), and West Coast (purple); centroids are designated by crosses. Average sentiments within each cluster confirm prior knowledge BIBREF1 : the Southeast and Midwest have lower average sentiments ( INLINEFORM0 and INLINEFORM1 , respectively) than the West Coast and Northeast (0.22 and 0.09, respectively). In Figure FIGREF5 , we plot predicted sentiment averaged by U.S. city of event-related tweeters. The majority of positive tweets emanate from traditionally liberal hubs (e.g. San Francisco, Los Angeles, Austin), while most negative tweets come from the Philadelphia metropolitan area.",
        "type": "Document"
      },
      {
        "id": "aaa064c9-4357-4358-925b-a1f6a5e45dd8",
        "metadata": {
          "vector_store_key": "1904.07342-1",
          "chunk_id": 14,
          "document_id": "1904.07342",
          "start_idx": 7614,
          "end_idx": 8364
        },
        "page_content": "The majority of positive tweets emanate from traditionally liberal hubs (e.g. San Francisco, Los Angeles, Austin), while most negative tweets come from the Philadelphia metropolitan area. These regions aside, rural areas tended to see more negative sentiment tweeters post-event, whereas urban regions saw more positive sentiment tweeters; however, overall average climate change sentiment pre- and post-event was relatively stable geographically. This map further confirms findings that coastal cities tend to be more aware of climate change BIBREF8 . From these mapping exercises, we claim that our \u201cinfluential tweet\" labeling is reasonable. We now discuss our final method on outcomes: comparing average Twitter sentiment pre-event to post-event.",
        "type": "Document"
      },
      {
        "id": "01873a32-5b00-4e50-b0b6-96cdcdae17e3",
        "metadata": {
          "vector_store_key": "1708.05873-1",
          "chunk_id": 29,
          "document_id": "1708.05873",
          "start_idx": 16707,
          "end_idx": 17442
        },
        "page_content": "There is then an increase in the expected topic proportion for Topic 7 as levels of democracy increase reaching a peak at around 5 on the Polity scale, after this there is a gradual decline in discussion of Topic 7. This would suggest that democratizing or semi-democratic countries (which are more likely to be developing countries with democratic institutions) discuss sustainable development more than established democracies (that are more likely to be developed countries). We also plot the results of the analysis as the difference in topic proportions for two different values of the effect of conflict. Our measure of whether a country is experiencing a civil conflict comes from the UCDP/PRIO Armed Conflict Dataset BIBREF11 .",
        "type": "Document"
      },
      {
        "id": "746cc846-0181-4e8d-9bbd-08c8638e658e",
        "metadata": {
          "vector_store_key": "1708.05873-1",
          "chunk_id": 32,
          "document_id": "1708.05873",
          "start_idx": 18226,
          "end_idx": 18983
        },
        "page_content": "States in South Asia and East Asia and the Pacific discuss Topic 7 the most. The figure shows that countries in North America are likely to speak about Topic 7 least. The analysis of discussion of international development in annual UN General Debate statements therefore uncovers two principle development topics: economic development and sustainable development. We find that discussion of Topic 2 is not significantly impacted by country-specific factors, such as wealth, population, democracy, levels of ODA, and conflict (although there are regional effects). However, we find that the extent to which countries discuss sustainable development (Topic 7) in their annual GD statements varies considerably according to these different structural factors.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What two components are included in their proposed framework?",
    "llm_answer": "Deep learning classifier and a tweet processor.\n",
    "context": [
      {
        "id": "1f7e4710-1099-4b7c-bb94-2b1ee2d1b0d4",
        "metadata": {
          "vector_store_key": "1706.08032-2",
          "chunk_id": 4,
          "document_id": "1706.08032",
          "start_idx": 2399,
          "end_idx": 3090
        },
        "page_content": "The paper makes the following contributions: The organization of the present paper is as follows: In section 2, we describe the model architecture which introduces the structure of the model. We explain the basic idea of model and the way of constructing the model. Section 3 show results and analysis and section 4 summarize this paper. Our proposed model consists of a deep learning classifier and a tweet processor. The deep learning classifier is a combination of DeepCNN and Bi-LSTM. The tweet processor standardizes tweets and then applies semantic rules on datasets. We construct a framework that treats the deep learning classifier and the tweet processor as two distinct components.",
        "type": "Document"
      },
      {
        "id": "283f0f76-4b6e-44fa-9151-69f69aa90fbc",
        "metadata": {
          "vector_store_key": "1710.01507-2",
          "chunk_id": 9,
          "document_id": "1710.01507",
          "start_idx": 5189,
          "end_idx": 5946
        },
        "page_content": "These components are (1) BiLSTM with attention, (2) Siamese Network on Text Embeddings, and (3) Siamese Network on Visual Embeddings. An overview of the architecture can be seen in Figure 1. We start with an explanation of the features used in the first component of the model. Distributed Word Embeddings Considering the effectiveness of distributional semantics in modeling language data, we use a pre-trained 300 dimensional Word2Vec BIBREF9 model trained over 100 billion words in the Google News corpus using the Continuous Bag of Words architecture. These map the words in a language to a high dimensional real-valued vectors to capture hidden semantic and syntactic properties of words, and are typically learned from large, unannotated text corpora.",
        "type": "Document"
      },
      {
        "id": "e7d0a447-e90c-4fde-a453-20ac8576450a",
        "metadata": {
          "vector_store_key": "2001.00137-1",
          "chunk_id": 8,
          "document_id": "2001.00137",
          "start_idx": 5052,
          "end_idx": 5740
        },
        "page_content": "To summarize, our contribution is two-fold: Novel model architecture that is more robust to incomplete data, including missing or incorrect words in text. Proposal of the novel tasks of incomplete intent and sentiment classification from incorrect sentences, and release of corpora related with these tasks. The remainder of this paper is organized in four sections, with Section SECREF2 explaining the proposed model. This is followed by Section SECREF3 which includes a detailed description of the dataset used for training and evaluation purposes and how it was obtained. Section SECREF4 covers the baseline models used for comparison, training specifications and experimental results.",
        "type": "Document"
      },
      {
        "id": "3cf1728c-0711-4400-a389-d0f3ab12da96",
        "metadata": {
          "vector_store_key": "1909.08859-0",
          "chunk_id": 8,
          "document_id": "1909.08859",
          "start_idx": 4401,
          "end_idx": 5098
        },
        "page_content": "Its architecture is based on a bi-directional attention flow (BiDAF) model BIBREF6, but also equipped with an explicit reasoning module that acts on entity-specific relational memory units. Fig. FIGREF4 shows an overview of the network architecture. It consists of five main modules: An input module, an attention module, a reasoning module, a modeling module, and an output module. Note that the question answering tasks we consider here are multimodal in that while the context is a procedural text, the question and the multiple choice answers are composed of images. Input Module extracts vector representations of inputs at different levels of granularity by using several different encoders.",
        "type": "Document"
      },
      {
        "id": "2e13a23c-d578-41c2-9fb1-e75e9dfa80e1",
        "metadata": {
          "vector_store_key": "2002.00876-1",
          "chunk_id": 29,
          "document_id": "2002.00876",
          "start_idx": 16646,
          "end_idx": 17170
        },
        "page_content": "We also believe the library provides a strong foundation for building generic tools for interpretablity, control, and visualization through its probabilistic API. Finally, we hope to explore further optimizations to make core algorithms competitive with highly-optimized neural network components. We thank Yoon Kim, Xiang Lisa Li, Sebastian Gehrmann, Yuntian Deng, and Justin Chiu for discussion and feedback on the project. The project was supported by NSF CAREER 1845664, NSF 1901030, and research awards by Sony and AWS.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which modifications do they make to well-established Seq2seq architectures?",
    "llm_answer": "1. Using a soft loss function.\n2. Modifying the Seq2Seq attention mechanism to be mostly monotonic, suitable for TTS.\n",
    "context": [
      {
        "id": "409a4ee5-6cca-4310-8a41-609786281348",
        "metadata": {
          "vector_store_key": "1909.02480-0",
          "chunk_id": 1,
          "document_id": "1909.02480",
          "start_idx": 821,
          "end_idx": 1610
        },
        "page_content": "(parameterized by $\\theta $). Most seq2seq models are autoregressive, meaning that they factorize the joint probability of the output sequence given the input sequence $P_\\theta (\\mathbf {y}|\\mathbf {x})$ into the product of probabilities over the next token in the sequence given the input sequence and previously generated tokens: Each factor, $P_\\theta (y_{t} | y_{<t}, \\mathbf {x})$, can be implemented by function approximators such as RNNs BIBREF0 and Transformers BIBREF3. This factorization takes the complicated problem of joint estimation over an exponentially large output space of outputs $\\mathbf {y}$, and turns it into a sequence of tractable multi-class classification problems predicting $y_t$ given the previous words, allowing for simple maximum log-likelihood training.",
        "type": "Document"
      },
      {
        "id": "dce121d0-2a6b-4acb-8b25-0cea5470722e",
        "metadata": {
          "vector_store_key": "1801.09030-9",
          "chunk_id": 45,
          "document_id": "1801.09030",
          "start_idx": 23043,
          "end_idx": 23733
        },
        "page_content": "The most significant improvement comes from applying the soft loss function. The soft loss function can relieve the strong assumption of order made by seq2seq model. Because predicting a correct token in the wrong position is not as harmful as predicting a completely wrong token. This simple modification gives a big improvement on both test sets for all the three evaluation metrics. In this subsection, we show an example generated by various models in Table 6 in test set 2 because the quality of test set 2 is much more satisfactory. The multi-label model produces too many herbs that lower the precision, we do not go deep into its results, already we report its results in the table.",
        "type": "Document"
      },
      {
        "id": "4ca05e67-d35a-4c2d-9735-3a4fc9fa1ac5",
        "metadata": {
          "vector_store_key": "1903.07398-4",
          "chunk_id": 14,
          "document_id": "1903.07398",
          "start_idx": 7681,
          "end_idx": 8336
        },
        "page_content": "Our proposed improvements come from the observation that employing generic Seq2seq models for TTS application misses out on further optimization that can be achieved when we consider the specific problem of TTS. Specifically, we notice that in TTS, unlike in applications like machine translation, the Seq2Seq attention mechanism should be mostly monotonic. In other words, when one reads a sequence of text, it is natural to assume that the text position progress nearly linearly in time with the sequence of output mel spectrogram. With this insight, we can make 3 modifications to the model that allows us to train faster while using a a smaller model.",
        "type": "Document"
      },
      {
        "id": "f1ceec66-d00a-4b37-8018-212e3b8b4f6a",
        "metadata": {
          "vector_store_key": "1905.08949-7",
          "chunk_id": 8,
          "document_id": "1905.08949",
          "start_idx": 4558,
          "end_idx": 5491
        },
        "page_content": "The majority of current NQG models follow the sequence-to-sequence (Seq2Seq) framework that use a unified representation and joint learning of content selection (via the encoder) and question construction (via the decoder). In this framework, traditional parsing-based content selection has been replaced by more flexible approaches such as attention BIBREF22 and copying mechanism BIBREF23 . Question construction has become completely data-driven, requiring far less labor compared to transformation rules, enabling better language flexibility compared to question templates. However, unlike other Seq2Seq learning NLG tasks, such as Machine Translation, Image Captioning, and Abstractive Summarization, which can be loosely regarded as learning a one-to-one mapping, generated questions can differ significantly when the intent of asking differs (e.g., the target answer, the target aspect to ask about, and the question's depth).",
        "type": "Document"
      },
      {
        "id": "64cdf403-0684-4656-8497-5bb2c4add2d1",
        "metadata": {
          "vector_store_key": "1801.09030-9",
          "chunk_id": 9,
          "document_id": "1801.09030",
          "start_idx": 5196,
          "end_idx": 5940
        },
        "page_content": "In the experiment results, our proposed model beats all the baselines in professional evaluations, and we observe a large increase in both the recall rate and the F1 score compared with the basic seq2seq model. The main contributions of this paper lie in the following three folds: There has not been much work concerning computational TCM. zhou2010development attempted to build a TCM clinical data warehouse so that the TCM knowledge can be analyzed and used. This is a typical way of collecting data, since the number of prescriptions given by the practitioners in the clinics is very large. However, in reality, most of the TCM doctors do not refer to the constructed digital systems, because the quality of the input data tends to be poor.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How was speed measured?",
    "llm_answer": "A navigation system with voice commands and information on the screen kept participants on a predefined track.  The time needed for the task would be compared to other participants.\n",
    "context": [
      {
        "id": "59c0f923-f0e2-4368-b5fa-048069bbb974",
        "metadata": {
          "vector_store_key": "1910.12618-1",
          "chunk_id": 39,
          "document_id": "1910.12618",
          "start_idx": 20978,
          "end_idx": 21881
        },
        "page_content": "However the inputs of the numerical benchmark should be hence comparable to the information contained in the weather reports. Considering they mainly contain calendar (day of the week and month) as well as temperature and wind information, the benchmark of comparison is a random forest trained on four features only: the time of the year (whose value is 0 on January the 1st and 1 on December the 31st with a linear growth in between), the day of the week, the national average temperature and wind speed. The metrics of evaluation are the Mean Absolute Percentage Error (MAPE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and the $R^2$ coefficient given by: where $T$ is the number of test samples, $y_t$ and $\\hat{y}_t$ are respectively the ground truth and the prediction for the document of day $t$, and $\\overline{y}$ is the empirical average of the time series over the test sample.",
        "type": "Document"
      },
      {
        "id": "b2818aee-b754-49e3-973a-9ea3999732e2",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 21,
          "document_id": "1909.02764",
          "start_idx": 11888,
          "end_idx": 12581
        },
        "page_content": "They are informed that the time needed for the task would be compared to other participants. The route comprises highways, rural roads, and city streets. A navigation system with voice commands and information on the screen keeps the participants on the predefined track. To trigger emotion changes in the participant, we use the following events: (i) a car on the right lane cutting off to the left lane when participants try to overtake followed by trucks blocking both lanes with a slow overtaking maneuver (ii) a skateboarder who appears unexpectedly on the street and (iii) participants are praised for reaching the destination unexpectedly quickly in comparison to previous participants.",
        "type": "Document"
      },
      {
        "id": "f8151257-f5cc-4431-9c35-593f4c0ddabf",
        "metadata": {
          "vector_store_key": "1605.08675-1",
          "chunk_id": 105,
          "document_id": "1605.08675",
          "start_idx": 58501,
          "end_idx": 59124
        },
        "page_content": "To check whether a word denotes a unit of measurement, the plWordNet is searched for lexemes equal to its base. Then it suffices to check whether it belongs to a synset, having <jednostka miary 1> (unit of measurement) as one of (direct or indirect) hypernyms, e.g. pi\u0119tna\u015bcie kilogram\u00f3w (fifteen kilograms) or 5 000 wat\u00f3w (5 000 watts). Study was supported by research fellowship within \"Information technologies: research and their interdisciplinary applications\" agreement number POKL.04.01.01-00-051/10-00. Critical reading of the manuscript by Agnieszka Mykowiecka and Aleksandra Brzezi\u0144ska is gratefully acknowledged.",
        "type": "Document"
      },
      {
        "id": "80911f24-68f0-45cb-a65a-c54370ab3292",
        "metadata": {
          "vector_store_key": "1707.03764-3",
          "chunk_id": 11,
          "document_id": "1707.03764",
          "start_idx": 5958,
          "end_idx": 6709
        },
        "page_content": "This also shows that distinguishing words include both time-specific ones, like \u201cgilmore\u201d and \u201cimacelebrityau\u201d, and general words from everyday life, which are less likely to be subject to time-specific trends, like \u201cplayer\u201d, and \u201cchocolate\u201d. This section is meant to highlight all of the potential contributions to the systems which turned out to be detrimental to performance, when compared to the simpler system that we have described in Section SECREF2 . We divide our attempts according to the different ways we attempted to enhance performance: manipulating the data itself (adding more, and changing preprocessing), using a large variety of features, and changing strategies in modelling the problem by using different algorithms and paradigms.",
        "type": "Document"
      },
      {
        "id": "802cf092-35a9-4ac9-be41-5fbe805c7317",
        "metadata": {
          "vector_store_key": "1709.01256-2",
          "chunk_id": 23,
          "document_id": "1709.01256",
          "start_idx": 12824,
          "end_idx": 13810
        },
        "page_content": "Algorithm SECREF21 and Figure FIGREF22 describe our wDTW measure. wDTW observes semantic information from word vectors, which is fundamentally different from the word distance calculated from hierarchies among words in the algorithm proposed by BIBREF27 liu2007sentence. The shortcomings of their work are that it is difficult to learn semantic taxonomy of all words and that their DTW algorithm can only be applied to sentences not documents. wDTW [h] Represent documents INLINEFORM0 and INLINEFORM1 with vectors of paragraphs: INLINEFORM2 and INLINEFORM3 Input: INLINEFORM4 and INLINEFORM5 Initialize the first row and the first column of INLINEFORM6 matrix INLINEFORM7 INLINEFORM8 INLINEFORM9 INLINEFORM10 in range INLINEFORM11 INLINEFORM12 in range INLINEFORM13 INLINEFORM14 DistPara INLINEFORM15 calculate INLINEFORM16 Return: INLINEFORM17  TED is reasonable for measuring document distances as documents can be easily transformed to tree structures visualized in Figure FIGREF24 .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What were their accuracy results on the task?",
    "llm_answer": "Recall and precision measures were used for evaluating the algorithm's performance on questions from the development and evaluation sets.  High accuracy was achieved on \"absurd\" questions, moderate accuracy on subordinate object recognition, and low accuracy on counting questions.  More analysis is needed for other question types.  The best accuracy rate for GloVe is shown in Table TABREF17.\n",
    "context": [
      {
        "id": "8dce665d-9665-4bef-8d03-212f5e982d92",
        "metadata": {
          "vector_store_key": "1605.08675-3",
          "chunk_id": 82,
          "document_id": "1605.08675",
          "start_idx": 45319,
          "end_idx": 46017
        },
        "page_content": "Of course, the final evaluation needs to be checked manually. As mentioned in previous section, the results consist of two groups: experiments, showing an influence of some aspects of algorithm on performance, and a final assessment. Both use the Polish Wikipedia as a knowledge base, whereas the questions asked belong to development and evaluation sets, respectively. In this section, recall measures percentage of questions, to which RAFAEL gave any answer, whereas precision denotes percentage of question answered correctly. When analysing results of different entity recognition techniques, we need to remember that they strongly rely on output of the question analysis, which is not perfect.",
        "type": "Document"
      },
      {
        "id": "46af3281-0f4a-4599-92f9-4383aab62197",
        "metadata": {
          "vector_store_key": "2003.05995-3",
          "chunk_id": 47,
          "document_id": "2003.05995",
          "start_idx": 24080,
          "end_idx": 24997
        },
        "page_content": "We observe, that subjective and objective task success are similar in that the dialogues that resolved the emergency were rated consistently higher than the rest. Mann-Whitney-U one-tailed tests show that the scores of the Emergency Resolved Dialogues for Q1 and Q2 were significantly higher than the scores of the Emergency Not Resolved Dialogues at the 95% confidence level (Q1: $U = 1654.5$, $p < 0.0001$; Q2: $U = 2195$, $p = 0.009$, both $p < 0.05$). This indicates that effective collaboration and information ease are key to task completion in this setting. Regarding the qualitative data, one of the objectives of the Wizard-of-Oz technique was to make the participant believe that they are interacting with an automated agent and the qualitative feedback seemed to reflect this: \u201cThe AI in the game was not helpful at all [...]\u201d or \u201cI was talking to Fred a bot assistant, I had no other partner in the game\u201c.",
        "type": "Document"
      },
      {
        "id": "050432d2-b325-4282-ae97-1513abbfedc7",
        "metadata": {
          "vector_store_key": "2003.06279-7",
          "chunk_id": 36,
          "document_id": "2003.06279",
          "start_idx": 21531,
          "end_idx": 22144
        },
        "page_content": "While Figures FIGREF14 \u2013 FIGREF16 show the relative behavior in the accuracy, it still interesting to observe the absolute accuracy rate obtained with the classifiers. In Table TABREF17, we show the best accuracy rate (i.e. $\\max \\Gamma _+ = \\max _p \\Gamma _+(p)$) for GloVe. We also show the average difference in performance ($\\langle \\Gamma _+ - \\Gamma _0 \\rangle $) and the total number of cases in which an improvement in performance was observed ($N_+$). $N_+$ ranges in the interval $0 \\le N_+ \\le 20$. Table TABREF17 summarizes the results obtained for $w = \\lbrace 1.0, 5.0, 10.0\\rbrace $ thousand words.",
        "type": "Document"
      },
      {
        "id": "799aee48-83fb-428f-ba63-cf2bf1a9ec07",
        "metadata": {
          "vector_store_key": "1804.03396-5",
          "chunk_id": 58,
          "document_id": "1804.03396",
          "start_idx": 30874,
          "end_idx": 31510
        },
        "page_content": "We randomly sample 25 articles respectively from the 6 datasets (in total of 1002 ground truth triples with 2691 labeled answer locations) and let two human annotators label the Triple Accuracy for each ground truth triple and the Contextual Consistency for each answer location. The two human annotators reached an agreement on 131 of 132 randomly selected Triple Accuracy cases and on 229 of 234 randomly selected Contextual Consistency cases. The human evaluation results are shown in Table 8 . We can find that the Triple Accuracy and the Triple Consistency is acceptable while the Contextual Consistency still needs to be improved.",
        "type": "Document"
      },
      {
        "id": "dc6680bd-96f0-4828-8454-39d0afffd8b1",
        "metadata": {
          "vector_store_key": "1703.09684-2",
          "chunk_id": 42,
          "document_id": "1703.09684",
          "start_idx": 23464,
          "end_idx": 24094
        },
        "page_content": "High accuracy is also achieved on absurd, which we discuss in greater detail in Sec. \"Effects of Including Absurd Questions\" . Subordinate object recognition is moderately high ( $>80$ %), despite having a large number of unique answers. Accuracy on counting is low across all methods, despite a large number of training data. For the remaining question-types, more analysis is needed to pinpoint whether the weaker performance is due to lower amounts of training data, bias, or limitations of the models. We next investigate how much of the good performance is due to bias in the answer distribution, which N-MPT compensates for.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What two types the Chinese reading comprehension dataset consists of?",
    "llm_answer": "People Daily news datasets and Children's Fairy Tale datasets.\n",
    "context": [
      {
        "id": "6223e71d-fc1d-4442-9c7f-0db7d4d7435a",
        "metadata": {
          "vector_store_key": "1709.08299-1",
          "chunk_id": 7,
          "document_id": "1709.08299",
          "start_idx": 4134,
          "end_idx": 4932
        },
        "page_content": "The previously mentioned datasets are all in English. To add diversities to the reading comprehension datasets, Cui et al. cui-etal-2016 proposed the first Chinese cloze-style reading comprehension dataset: People Daily & Children's Fairy Tale, including People Daily news datasets and Children's Fairy Tale datasets. They also generate the data in an automatic manner, which is similar to the previous datasets. They choose short articles (several hundreds of words) as Document and remove a word from it, whose type is mostly named entities and common nouns. Then the sentence that contains the removed word will be regarded as Query. To add difficulties to the dataset, along with the automatically generated evaluation sets (validation/test), they also release a human-annotated evaluation set.",
        "type": "Document"
      },
      {
        "id": "4ffec73f-7491-41cc-9397-ef79a8583875",
        "metadata": {
          "vector_store_key": "1806.09103-3",
          "chunk_id": 22,
          "document_id": "1806.09103",
          "start_idx": 12657,
          "end_idx": 13327
        },
        "page_content": "In these datasets, a story containing consecutive sentences is formed as the Document and one of the sentences is either automatically or manually selected as the Query where one token is replaced by a placeholder to indicate the answer to fill in. Table TABREF8 gives data statistics. Different from the current cloze-style datasets for English reading comprehension, such as CBT, Daily Mail and CNN BIBREF0 , the three Chinese datasets do not provide candidate answers. Thus, the model has to find the correct answer from the entire document. Besides, we also use the Children's Book Test (CBT) dataset BIBREF1 to test the generalization ability in multi-lingual case.",
        "type": "Document"
      },
      {
        "id": "f28e32bd-4bea-45af-bf35-49be7d518a61",
        "metadata": {
          "vector_store_key": "1709.08299-1",
          "chunk_id": 1,
          "document_id": "1709.08299",
          "start_idx": 1080,
          "end_idx": 1885
        },
        "page_content": "In this paper, we provide a new Chinese reading comprehension dataset, which has the following features We also host the 1st Evaluation on Chinese Machine Reading Comprehension (CMRC2017), which has attracted over 30 participants and finally there were 17 participants submitted their evaluation systems for testing their reading comprehension models on our newly developed dataset, suggesting its potential impact. We hope the release of the dataset to the public will accelerate the progress of Chinese research community on machine reading comprehension field. We also provide four official baselines for the evaluations, including two traditional baselines and two neural baselines. In this paper, we adopt two widely used neural reading comprehension model: AS Reader BIBREF3 and AoA Reader BIBREF4 .",
        "type": "Document"
      },
      {
        "id": "951347ed-9d66-4d0c-813f-8d01fd83f8fd",
        "metadata": {
          "vector_store_key": "1709.08299-1",
          "chunk_id": 0,
          "document_id": "1709.08299",
          "start_idx": 0,
          "end_idx": 1080
        },
        "page_content": "Machine Reading Comprehension (MRC) has become enormously popular in recent research, which aims to teach the machine to comprehend human languages and answer the questions based on the reading materials. Among various reading comprehension tasks, the cloze-style reaing comprehension is relatively easy to follow due to its simplicity in definition, which requires the model to fill an exact word into the query to form a coherent sentence according to the document material. Several cloze-style reading comprehension datasets are publicly available, such as CNN/Daily Mail BIBREF0 , Children's Book Test BIBREF1 , People Daily and Children's Fairy Tale BIBREF2 . In this paper, we provide a new Chinese reading comprehension dataset, which has the following features We also host the 1st Evaluation on Chinese Machine Reading Comprehension (CMRC2017), which has attracted over 30 participants and finally there were 17 participants submitted their evaluation systems for testing their reading comprehension models on our newly developed dataset, suggesting its potential impact.",
        "type": "Document"
      },
      {
        "id": "e01a5883-53fd-4319-9281-c693f6334799",
        "metadata": {
          "vector_store_key": "1709.08299-0",
          "chunk_id": 2,
          "document_id": "1709.08299",
          "start_idx": 1352,
          "end_idx": 2008
        },
        "page_content": "In this paper, we adopt two widely used neural reading comprehension model: AS Reader BIBREF3 and AoA Reader BIBREF4 . The rest of the paper will be organized as follows. In Section 2, we will introduce the related works on the reading comprehension dataset, and then the proposed dataset as well as our competitions will be illustrated in Section 3. The baseline and participant system results will be given in Section 4 and we will made a brief conclusion at the end of this paper. In this section, we will introduce several public cloze-style reading comprehension dataset. Some news articles often come along with a short summary or brief introduction.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "For which languages most of the existing MRC datasets are created?",
    "llm_answer": "English.\n",
    "context": [
      {
        "id": "2f8e9d5b-32b1-4b15-bfdb-c136c904ee9c",
        "metadata": {
          "vector_store_key": "1910.11493-2",
          "chunk_id": 9,
          "document_id": "1910.11493",
          "start_idx": 5320,
          "end_idx": 6006
        },
        "page_content": "Data for all but four languages (Basque, Kurmanji, Murrinhpatha, and Sorani) are extracted from English Wiktionary, a large multi-lingual crowd-sourced dictionary with morphological paradigms for many lemmata. 20 of the 100 language pairs are either distantly related or unrelated; this allows speculation into the relative importance of data quantity and linguistic relatedness. For each language, the basic data consists of triples of the form (lemma, feature bundle, inflected form), as in tab:sub1data. The first feature in the bundle always specifies the core part of speech (e.g., verb). For each language pair, separate files contain the high- and low-resource training examples.",
        "type": "Document"
      },
      {
        "id": "fdf582fb-3fd0-4e0a-9bba-39ff6f6c125d",
        "metadata": {
          "vector_store_key": "1909.06522-0",
          "chunk_id": 22,
          "document_id": "1909.06522",
          "start_idx": 11687,
          "end_idx": 12449
        },
        "page_content": "For each language, the train and test set size are described in Table TABREF10, and most training data were Pages. On each language we also had a small validation set for model parameter tuning. Each monolingual ASR baseline was trained on language-specific data only. The character sets of these 7 languages have little overlap except that (i) they all include common basic Latin alphabet, and (ii) both Hindi and Marathi use Devanagari script. We took the union of 7 character sets therein as the multilingual grapheme set (Section SECREF2), which contained 432 characters. In addition, we deliberately split 7 languages into two groups, such that the languages within each group were more closely related in terms of language family, orthography or phonology.",
        "type": "Document"
      },
      {
        "id": "2ded8a9e-d4bc-48fa-9555-3885b88eba42",
        "metadata": {
          "vector_store_key": "1910.00458-5",
          "chunk_id": 32,
          "document_id": "1910.00458",
          "start_idx": 17414,
          "end_idx": 18132
        },
        "page_content": "For the sentiment analysis category, we used the Stanford Sentiment Treebank (SST-2) dataset from the GLUE benchmark BIBREF22 (around 60k train examples) and the Yelp dataset (around 430k train examples). For the paraphrase category, three paraphrasing datasets are used from the GLUE benchmark: Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), and Quora Question Pairs (QQP), which are denoted as \u201cGLUE-Para.\u201d. For the span-based QA, we use the SQuAD 1.1, SQuAD 2.0 , and MRQA which is a joint dataset including six popular span-based QA datasets. Table TABREF23 summarizes the results. We see that sentiment analysis datasets do not help much with our target MCQA datasets.",
        "type": "Document"
      },
      {
        "id": "4a837fd9-26e7-4896-9f56-2a2f146a050e",
        "metadata": {
          "vector_store_key": "1809.04686-1",
          "chunk_id": 11,
          "document_id": "1809.04686",
          "start_idx": 5990,
          "end_idx": 6766
        },
        "page_content": "For the MT task, we use the WMT 2014 En $\\leftrightarrow $ Fr parallel corpus. The dataset contains 36 million En $\\rightarrow $ Fr sentence pairs. We swapped the source and target sentences to obtain parallel data for the Fr $\\rightarrow $ En translation task. We use these two datasets (72 million sentence pairs) to train a single multilingual NMT model to learn both these translation directions simultaneously. We generated a shared sub-word vocabulary BIBREF37 , BIBREF38 of 32K units from all source and target training data. We use this sub-word vocabulary for all of our experiments below. The Amazon reviews dataset BIBREF39 is a multilingual sentiment classification dataset, providing data for four languages - English (En), French (Fr), German (De), and Japanese.",
        "type": "Document"
      },
      {
        "id": "a8483ed8-51f4-4cd5-8d41-c8ea591c078b",
        "metadata": {
          "vector_store_key": "1902.10525-2",
          "chunk_id": 48,
          "document_id": "1902.10525",
          "start_idx": 25261,
          "end_idx": 25971
        },
        "page_content": "We use the CTC forward-backward training algorithm as described in BIBREF24 , and implemented in TensorFlow. The training hyperparameters are described in Section SECREF44 . The CTC decoding algorithm incorporates feature functions similarly to how the dictionary is incorporated in the previous state-of-the-art system. However, we use more feature functions, our language models are trained on a different corpus, and the combination weights are optimized separately as described in Sec SECREF45 . Another publicly-accessible English-language dataset is the IBM-UB-1 dataset BIBREF25 . From the available datasets therein, we use the English query dataset, which consists of 63 268 handwritten English words.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which sentiment analysis tasks are addressed?",
    "llm_answer": "Tweet-level sentiment, aspect-based sentiment analysis, sentiment analysis of figurative language, implicit event polarity, stance in tweets, out-of-context sentiment intensity, emotion detection, and clinical sentiment analysis.\n",
    "context": [
      {
        "id": "4b30d508-00ff-4992-90a7-4353796274b1",
        "metadata": {
          "vector_store_key": "1710.01492-0",
          "chunk_id": 17,
          "document_id": "1710.01492",
          "start_idx": 9066,
          "end_idx": 9794
        },
        "page_content": "Other related tasks have explored aspect-based sentiment analysis BIBREF32 , BIBREF33 , BIBREF21 , sentiment analysis of figurative language on Twitter BIBREF34 , implicit event polarity BIBREF35 , stance in tweets BIBREF36 , out-of-context sentiment intensity of phrases BIBREF37 , and emotion detection BIBREF38 . Some of these tasks featured languages other than English. Tweet-level sentiment. The simplest and also the most popular task of sentiment analysis on Twitter is to determine the overall sentiment expressed by the author of a tweet BIBREF30 , BIBREF28 , BIBREF26 , BIBREF29 , BIBREF27 . Typically, this means choosing one of the following three classes to describe the sentiment: Positive, Negative, and Neutral.",
        "type": "Document"
      },
      {
        "id": "58889ab1-5874-4cfd-afd7-731ea5861b4f",
        "metadata": {
          "vector_store_key": "1911.12569-3",
          "chunk_id": 6,
          "document_id": "1911.12569",
          "start_idx": 3626,
          "end_idx": 4482
        },
        "page_content": "The main contributions of the current work are two-fold: a) We propose a novel two-layered multi-task attention based system for joint sentiment and emotion analysis. This system has two levels of attention which builds a hierarchical representation. This provides an intuitive explanation of its working; b) We empirically show that emotion analysis is relevant and useful in sentiment analysis. The multi-task system utilizing fine-grained information of emotion analysis performs better than the single task system of sentiment analysis. A survey of related literature reveals the use of both classical and deep-learning approaches for sentiment and emotion analysis. The system proposed in BIBREF8 relied on supervised statistical text classification which leveraged a variety of surface form, semantic, and sentiment features for short informal texts.",
        "type": "Document"
      },
      {
        "id": "7f3b76ae-dc5a-4287-a5a9-3423e42298b3",
        "metadata": {
          "vector_store_key": "1911.12569-3",
          "chunk_id": 34,
          "document_id": "1911.12569",
          "start_idx": 20615,
          "end_idx": 21303
        },
        "page_content": "Similarly, the system M2 is made up of S2 and E2 where S2 performs the main task (sentiment analysis) and E2 commits to the auxiliary task (emotion analysis). We observe that in both the situations, the auxiliary task, i.e. emotional information increases the performance of the main task, i.e. sentiment analysis when these two are jointly performed. Experimental results help us to establish the fact that emotion analysis benefits sentiment analysis. The implicit sentiment attached to the emotion words assists the multi-task system. Emotion such as joy and trust are inherently associated with a positive sentiment whereas, anger, disgust, fear and sadness bear a negative sentiment.",
        "type": "Document"
      },
      {
        "id": "342ac496-c6a3-4b91-9a93-f412b806c89d",
        "metadata": {
          "vector_store_key": "1809.05752-2",
          "chunk_id": 41,
          "document_id": "1809.05752",
          "start_idx": 23602,
          "end_idx": 24509
        },
        "page_content": "These include a deeper analysis of clinical narratives in EHRs: our next task will be to extend our EHR data pipeline by distinguishing between clinically positive and negative phenomena within each risk factor domain. This will involve a series of annotation tasks that will allow us to generate lexicon-based and corpus-based sentiment analysis tools. We can then use these clinical sentiment scores to generate a gradient of patient improvement or deterioration over time. We will also take into account structured data that have been collected on the target cohort throughout the course of this study such as brain based electrophysiological (EEG) biomarkers, structural brain anatomy from MRI scans (gray matter volume, cortical thickness, cortical surface-area), social and role functioning assessments, personality assessment (NEO-FFI[9]), and various symptom scales (PANSS[10], MADRS[11], YMRS[12]).",
        "type": "Document"
      },
      {
        "id": "89ad953b-29df-4d57-8717-f96f683aca90",
        "metadata": {
          "vector_store_key": "1710.01492-6",
          "chunk_id": 26,
          "document_id": "1710.01492",
          "start_idx": 13646,
          "end_idx": 14235
        },
        "page_content": "A task related to, but arguably different in some respect from sentiment analysis, is that of stance detection. The goal here is to determine whether the author of a piece of text is in favor of, against, or neutral toward a proposition or a target BIBREF36 . For example, in (8) the author has a negative stance toward the proposition w\u200bomen have the right to abortion, even though the target is not mentioned at all. Similarly, in (9\u00a7) the author expresses a negative sentiment toward Mitt Romney, from which one can imply that s/he has a positive stance toward the target \u200bBarack Obama.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which 3 NLP areas are cited the most?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "14f90cca-16ee-4a4f-92e2-25610d63a5bb",
        "metadata": {
          "vector_store_key": "1911.03562-0",
          "chunk_id": 2,
          "document_id": "1911.03562",
          "start_idx": 842,
          "end_idx": 1445
        },
        "page_content": "This might be particularly the case for those that are new to the field and wish to get a broad overview of the NLP publishing landscape. On the other hand, even seasoned NLP'ers have likely wondered about the questions raised here and might be interested in the empirical evidence. Data: The analyses presented below are based on information about the papers taken directly from AA (as of June 2019) and citation information extracted from Google Scholar (as of June 2019). Thus, all subsequent papers and citations are not included in the analysis. A fresh data collection is planned for January 2020.",
        "type": "Document"
      },
      {
        "id": "ba5b7a45-4409-4a45-ab8d-7221fcdf2c7d",
        "metadata": {
          "vector_store_key": "1911.03562-0",
          "chunk_id": 102,
          "document_id": "1911.03562",
          "start_idx": 54159,
          "end_idx": 54785
        },
        "page_content": "(Of course, note that some of the 59 areas, as estimated using title term bigrams, are overlapping. Also, we did not include large scale in the list above because the difference in averages is very small and it is not really an area of research.) Thus, the citation gap is common across a majority of the high-citations areas within NLP. This work examined the ACL Anthology to identify broad trends in productivity, focus, and impact. We examined several questions such as: who and how many of us are publishing? what are we publishing on? where and in what form are we publishing? and what is the impact of our publications?",
        "type": "Document"
      },
      {
        "id": "99a1c7d3-abfc-4701-90f6-362fac542ba8",
        "metadata": {
          "vector_store_key": "1909.02776-8",
          "chunk_id": 13,
          "document_id": "1909.02776",
          "start_idx": 7643,
          "end_idx": 8450
        },
        "page_content": "They showed that some features work well only in some specific portion of text, for example, on the abstract section, while others perform better on the methodology section. This could be considered to be a consequence of differences in the structure and context of each section. All the above studies imply the significance of document context in ranking. Nevertheless, it has not been given enough attention in the NLP community, and even sometimes is neglected. For instance, authors in BIBREF30 suggest the use of a wide range of various features. Among these, seventeen part-of-speech based sentences features have been introduced, all of which are sentence-normalized, but not document-normalized, i.e. they count the ratio of a syntactic unit e.g. verbs, divided by the number of words in a sentence.",
        "type": "Document"
      },
      {
        "id": "38fa0068-2efa-45a5-b3c7-5eeb8eeb038b",
        "metadata": {
          "vector_store_key": "1911.03562-9",
          "chunk_id": 61,
          "document_id": "1911.03562",
          "start_idx": 32202,
          "end_idx": 32898
        },
        "page_content": "Discussion: Machine translation papers are well-represented in many of these lists, but especially in the system demo papers list. Toolkits such as MT evaluation ones, NLTK, Stanford Core NLP, WordNet Similarity, and OpenNMT have highly cited demo or workshop papers. The shared task papers list is dominated by task description papers (papers by task organizers describing the data and task), especially for sentiment analysis tasks. However, the list also includes papers by top-performing systems in these shared tasks, such as the NRC-Canada, HidelTime, and UKP papers. Q. What are the most cited AA' papers in the last decade? A. Figure FIGREF39 shows the most cited AA' papers in the 2010s.",
        "type": "Document"
      },
      {
        "id": "2a2dbaaf-82e2-44a4-977b-e1d2c82b614c",
        "metadata": {
          "vector_store_key": "1911.03562-0",
          "chunk_id": 1,
          "document_id": "1911.03562",
          "start_idx": 842,
          "end_idx": 1567
        },
        "page_content": "Our broader goal here is simply to record the state of the AA literature: who and how many of us are publishing? what are we publishing on? where and in what form are we publishing? and what is the impact of our publications? The answers are usually in the form of numbers, graphs, and inter-connected visualizations. We focus on the following aspects of NLP research: size, demographics, areas of research, impact, and correlation of citations with demographic attributes (age and gender). Target Audience: The analyses presented here are likely to be of interest to any NLP researcher. This might be particularly the case for those that are new to the field and wish to get a broad overview of the NLP publishing landscape.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which journal and conference are cited the most in recent years?",
    "llm_answer": "CL journal.\n",
    "context": [
      {
        "id": "bde7261c-73f4-419a-9282-79b094d9e8d5",
        "metadata": {
          "vector_store_key": "1911.03562-2",
          "chunk_id": 76,
          "document_id": "1911.03562",
          "start_idx": 40283,
          "end_idx": 41126
        },
        "page_content": "The somewhat high percentage of papers that are never cited (4.3%) are likely because some of the book reviews from earlier years are not explicitly marked in CL journal, and thus they were not removed from analysis. Also, letters to editors, which are more common in CL journal, tend to often obtain 0 citations. CL, EMNLP, and ACL have the best track record for accepting papers that have gone on to receive 1000 or more citations. *Sem, the semantics conference, seems to have notably lower percentage of high-citation papers, even though it has fairly competitive acceptance rates. Instead of percentage, if one considers raw numbers of papers that have at least ten citations (i-10 index), then LREC is particularly notable in terms of the large number of papers it accepts that have gone on to obtain ten or more citations ($\\sim $1600).",
        "type": "Document"
      },
      {
        "id": "12c0ffa0-9972-4e21-804e-436628254a1d",
        "metadata": {
          "vector_store_key": "1911.03562-2",
          "chunk_id": 70,
          "document_id": "1911.03562",
          "start_idx": 37227,
          "end_idx": 38023
        },
        "page_content": "A. CL journal has the highest average citations per paper. Figure FIGREF49 shows the average citations for AA\u2019 papers published 1965\u20132016 and 2010\u20132016, respectively, grouped by venue and paper type. (Figure with median citations is available online.) Discussion: In terms of citations, TACL papers have not been as successful as EMNLP and ACL; however, CL journal (the more traditional journal paper venue) has the highest average and median paper citations (by a large margin). This gap has reduced in papers published since 2010. When considering papers published between 2010 and 2016, the system demonstration papers, the SemEval shared task papers, and non-SemEval shared task papers have notably high average (surpassing those of EACL and COLING); however their median citations are lower.",
        "type": "Document"
      },
      {
        "id": "96a051b3-90f1-431f-b34e-9885e3b640e8",
        "metadata": {
          "vector_store_key": "1911.03562-2",
          "chunk_id": 65,
          "document_id": "1911.03562",
          "start_idx": 34520,
          "end_idx": 35228
        },
        "page_content": "A. In this analysis, we include only those AA\u2019 papers that were published in 2016 or earlier (to allow for at least 2.5 years to collect citations). There are 26,949 such papers. Figures FIGREF42 and FIGREF43 show the average citations by paper type when considering papers published 1965\u20132016 and 2010\u20132016, respectively. Figures FIGREF45 and FIGREF46 show the medians. Discussion: Journal papers have much higher average and median citations than other papers, but the gap between them and top-tier conferences is markedly reduced when considering papers published since 2010. System demo papers have the third highest average citations; however, shared task papers have the third highest median citations.",
        "type": "Document"
      },
      {
        "id": "44b83a29-7a0f-46bf-8a56-dad2c22028bb",
        "metadata": {
          "vector_store_key": "1911.03562-9",
          "chunk_id": 71,
          "document_id": "1911.03562",
          "start_idx": 38023,
          "end_idx": 38679
        },
        "page_content": "When considering papers published between 2010 and 2016, the system demonstration papers, the SemEval shared task papers, and non-SemEval shared task papers have notably high average (surpassing those of EACL and COLING); however their median citations are lower. This is likely because some heavily cited papers have pushed the average up. Nonetheless, it is interesting to note how, in terms of citations, demo and shared task papers have surpassed many conferences and even become competitive with some top-tier conferences such as EACL and COLING. Q. What percent of the AA\u2019 papers that were published in 2016 or earlier are cited more than 1000 times?",
        "type": "Document"
      },
      {
        "id": "5ec02485-ef09-4ce9-a60a-170319dc1fbd",
        "metadata": {
          "vector_store_key": "1911.03562-9",
          "chunk_id": 58,
          "document_id": "1911.03562",
          "start_idx": 30368,
          "end_idx": 31046
        },
        "page_content": "Hovering over individual papers reveals an information box showing the paper title, authors, year of publication, publication venue, and #citations. Discussion: With time, not only have the number of papers grown, but also the number of high-citation papers. We see a marked jump in the 1990s over the previous decades, but the 2000s are the most notable in terms of the high number of citations. The 2010s papers will likely surpass the 2000s papers in the years to come. Q. What are the most cited papers in AA'? A. Figure FIGREF37 shoes the most cited papers in the AA'. Discussion: We see that the top-tier conference papers (green) are some of the most cited papers in AA\u2019.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which 5 languages appear most frequently in AA paper titles?",
    "llm_answer": "The provided text states that Chinese, Arabic, Korean, Japanese, and Hindi are among the most widely spoken Asian languages well-represented in AA papers, and French, German, Swedish, Spanish, Portuguese, and Italian are well-represented European languages.  However, it does not provide a ranked list of the *five* most frequent.  The answer is not found in the context.\n",
    "context": [
      {
        "id": "9bae1401-c973-4257-8ce6-9f77bbdb402e",
        "metadata": {
          "vector_store_key": "1911.03562-4",
          "chunk_id": 30,
          "document_id": "1911.03562",
          "start_idx": 16304,
          "end_idx": 16896
        },
        "page_content": "Figure FIGREF27 is a treemap of the 122 languages arranged alphabetically and shaded such that languages that appear more often in AA paper titles have a darker shade of green. Discussion: Even though the amount of work done on English is much larger than that on any other language, often the word English does not appear in the title, and this explains why English is not the first (but the second-most) common language name to appear in the titles. This is likely due to the fact that many papers fail to mention the language of study or the language of the datasets used if it is English.",
        "type": "Document"
      },
      {
        "id": "ebd07c6a-d298-4e6d-a1d3-507d4f54c70d",
        "metadata": {
          "vector_store_key": "1911.03562-4",
          "chunk_id": 29,
          "document_id": "1911.03562",
          "start_idx": 15864,
          "end_idx": 16569
        },
        "page_content": "Instead, we will focus on estimating how much research pertains to non-English languages. We will make use of the idea that often when work is done focusing on a non-English language, then the language is mentioned in the title. We collected a list of 122 languages indexed by Wiktionary and looked for the presence of these words in the titles of AA papers. (Of course there are hundreds of other lesser known languages as well, but here we wanted to see the representation of these more prominent languages in NLP literature.) Figure FIGREF27 is a treemap of the 122 languages arranged alphabetically and shaded such that languages that appear more often in AA paper titles have a darker shade of green.",
        "type": "Document"
      },
      {
        "id": "614c930a-a646-4855-b862-cc80d671dfae",
        "metadata": {
          "vector_store_key": "1911.03562-4",
          "chunk_id": 31,
          "document_id": "1911.03562",
          "start_idx": 16896,
          "end_idx": 17600
        },
        "page_content": "This is likely due to the fact that many papers fail to mention the language of study or the language of the datasets used if it is English. There is growing realization in the community that this is not quite right. However, the language of study can be named in other less prominent places than the title, for example the abstract, introduction, or when the datasets are introduced, depending on how central it is to the paper. We can see from the treemap that the most widely spoken Asian and Western European languages enjoy good representation in AA. These include: Chinese, Arabic, Korean, Japanese, and Hindi (Asian) as well as French, German, Swedish, Spanish, Portuguese, and Italian (European).",
        "type": "Document"
      },
      {
        "id": "cd3ce833-0516-4d3b-acb9-b9c51d7871b6",
        "metadata": {
          "vector_store_key": "1911.03562-6",
          "chunk_id": 40,
          "document_id": "1911.03562",
          "start_idx": 21141,
          "end_idx": 21786
        },
        "page_content": "However, even if one does not believe in that hypothesis, it is worth examining the terms in the titles of tens of thousands of papers in the ACL Anthology\u2014spread across many decades. Q. What terms are used most commonly in the titles of the AA papers? How has that changed with time? A. Figure FIGREF28 shows the most common unigrams (single word) and bigrams (two-word sequences) in the titles of papers published from 1980 to 2019. (Ignoring function words.) The timeline graph at the bottom shows the percentage of occurrences of the unigrams over the years (the colors of the unigrams in the Timeline match those in the Title Unigram list).",
        "type": "Document"
      },
      {
        "id": "f30e4a96-82ff-4293-bc09-560dd6c20763",
        "metadata": {
          "vector_store_key": "1911.03562-4",
          "chunk_id": 33,
          "document_id": "1911.03562",
          "start_idx": 18029,
          "end_idx": 18659
        },
        "page_content": "Notable among these is the extremely low representation of languages from Africa, languages from non-Indo-European language families, and Indigenous languages from around the world. Natural Language Processing addresses a wide range of research questions and tasks pertaining to language and computing. It encompasses many areas of research that have seen an ebb and flow of interest over the years. In this section, we examine the terms that have been used in the titles of ACL Anthology (AA) papers. The terms in a title are particularly informative because they are used to clearly and precisely convey what the paper is about.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much F1 was improved after adding skip connections?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "2728e40a-66bd-4fcd-95d8-fcf1f01e14cf",
        "metadata": {
          "vector_store_key": "2001.11268-0",
          "chunk_id": 44,
          "document_id": "2001.11268",
          "start_idx": 25385,
          "end_idx": 26008
        },
        "page_content": "We never used the full SQuAD data in order to reduce time for training but observed increased performance when adding additional data. For classifying I entities, an increase from 20 to 200 additional SQuAD domains resulted in an increase of 8% for the F1 score, whereas the increase for the O domain was less than 1%. After training a model with 200 additional SQuAD domains, we also evaluated it on the original SQuAD development set and obtained a F1 score of 0.72 for this general reading comprehension task. In this evaluation, the F1 scores represent the overlap of labelled and predicted answer spans on token level.",
        "type": "Document"
      },
      {
        "id": "69f3e1d8-4a10-46d9-afc6-686b92fe5b9e",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 44,
          "document_id": "1810.00663",
          "start_idx": 24620,
          "end_idx": 25272
        },
        "page_content": "Note that the impact of the masking function is less evident in terms of the F1 score because this metric considers if a predicted behavior exists in the ground truth navigation plan, irrespective of its specific position in the output sequence. The results in the last four rows of Table TABREF28 suggest that ordering the graph triplets can facilitate predicting correct navigation plans in previously seen environments. Providing the triplets that surround the starting location of the robot first to the model leads to a boost of INLINEFORM0 in EM and GM performance. The rearrangement of the graph triplets also helps to reduce ED and increase F1.",
        "type": "Document"
      },
      {
        "id": "10c49bf8-f431-41f3-8955-86d929d274f3",
        "metadata": {
          "vector_store_key": "1912.10435-3",
          "chunk_id": 18,
          "document_id": "1912.10435",
          "start_idx": 10144,
          "end_idx": 10875
        },
        "page_content": "After adding the separate start and end logic, we found that the localized feature extraction did not allow an improvement in the network's learning via an ablation study where we ran the network without the convolutional layers. We speculate that the convolutions prevented improvement beyond a certain F1 score because they are lossy compressors and the information lost by the convolutions might be essential to downstream learning. As shown in Figure FIGREF2, we have a skip connection from the BERT embedding layer combined with the convolved directed co-attention output (C2Q and Q2C). We experimented with 3 skip connection configurations: Simple ResNet inspired Skip, Self-Attention Transformer Skip, and a Highway Network.",
        "type": "Document"
      },
      {
        "id": "4797d131-8519-42ef-98fb-1738864ef144",
        "metadata": {
          "vector_store_key": "1910.04269-3",
          "chunk_id": 23,
          "document_id": "1910.04269",
          "start_idx": 13320,
          "end_idx": 14070
        },
        "page_content": "Residual connections in a way makes the model selective of the contributing layers, determines the optimal number of layers required for training and solves the problem of vanishing gradients. Residual connections or skip connections skip training of those layers that do not contribute much in the overall outcome of model. We added spatial attention BIBREF27 networks to help the model in focusing on specific regions or areas in an image. Spatial attention aids learning irrespective of transformations, scaling and rotation done on the input images making the model more robust and helping it to achieve better results. We added Channel Attention networks so as to help the model to find interdependencies among color channels of log-Mel spectra.",
        "type": "Document"
      },
      {
        "id": "eae874dc-a852-44af-a50c-e15d33f015e7",
        "metadata": {
          "vector_store_key": "1912.10435-3",
          "chunk_id": 30,
          "document_id": "1912.10435",
          "start_idx": 16487,
          "end_idx": 17202
        },
        "page_content": "The skip connection containing the transformer provides minimal gains while adding a significant overhead to runtime. Therefore, we built the final convolutional experiments on the Simple Skip architecture. The localized feature extraction within the coattention network produced the best results in the base model, but prevented an improvement in our modified BERT large model. Table TABREF21 shows the F1 and EM scores obtained for the experiments on the large model. The models labeled 1, 2, and 3 are described in detail in Section 3.6. Each of the models built on BERT large used our augmented dataset in addition to the coattention architecture, simple skip connection, and separate start and end logit logic.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much gain does the model achieve with pretraining MVCNN?",
    "llm_answer": "The context does not contain a specific number for the gain achieved by pretraining MVCNN.  It only states that pretraining is important for words without initialization and that MVCNN uses pretraining.\n",
    "context": [
      {
        "id": "96da0864-e672-4200-9e9b-b4561f126e6f",
        "metadata": {
          "vector_store_key": "1603.04513-5",
          "chunk_id": 46,
          "document_id": "1603.04513",
          "start_idx": 25957,
          "end_idx": 26667
        },
        "page_content": "Pretraining is expected to learn good representations for all words, but pretraining is especially important for words without initialization (\u201cno hit\u201d); a particularly clear example for this is the Senti140 task: 236,484 of 387,877 words or 61% are in the \u201cno hit\u201d category. Table 3 compares results on test of MVCNN and its variants with other baselines in the four sentence classification tasks. Row 34, \u201cMVCNN (overall)\u201d, shows performance of the best configuration of MVCNN, optimized on dev. This version uses five versions of word embeddings, four filter sizes (3, 5, 7, 9), both mutual-learning and pretraining, three convolution layers for Senti140 task and two convolution layers for the other tasks.",
        "type": "Document"
      },
      {
        "id": "19f970d2-4423-4e69-8b7b-5e6ddd9e6f54",
        "metadata": {
          "vector_store_key": "1603.04513-5",
          "chunk_id": 37,
          "document_id": "1603.04513",
          "start_idx": 21088,
          "end_idx": 21912
        },
        "page_content": "During pretraining, all the model parameters, including mutichannel input, convolution parameters and fully connected layer, will be updated until they are mature to extract the sentence features. Subsequently, the same sets of parameters will be fine-tuned for supervised classification tasks. In sum, this pretraining is designed to produce good initial values for both model parameters and word embeddings. It is especially helpful for pretraining the embeddings of unknown words. We test the network on four classification tasks. We begin by specifying aspects of the implementation and the training of the network. We then report the results of the experiments. In each of the experiments, the top of the network is a logistic regression that predicts the probability distribution over classes given the input sentence.",
        "type": "Document"
      },
      {
        "id": "36adf7ba-6ece-4ad5-84b3-0a38b137a4fc",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 16,
          "document_id": "1912.13109",
          "start_idx": 8070,
          "end_idx": 8831
        },
        "page_content": "The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we employed various operational techniques for optimizing on the learning process. These processes and its implementation details will be discussed later but they were learning rate decay, early stopping, model checkpointing and reducing learning rate on plateau.",
        "type": "Document"
      },
      {
        "id": "0c4bd793-a8e9-4861-8601-03f439261b66",
        "metadata": {
          "vector_store_key": "1910.00458-6",
          "chunk_id": 21,
          "document_id": "1910.00458",
          "start_idx": 12016,
          "end_idx": 12817
        },
        "page_content": "In the table, we first report the accuracy of the SOTA models in the leaderboard. We then report the performance of our re-implementation of fine-tuned models as another set of strong baselines, among which the RoBERTa-Large model has already surpassed the previous SOTA. For these baselines, the top-level classifier is a two-layer FCNN for BERT-based models and a one-layer FCNN for the RoBERTa-Large model. Lastly, we report model performances that use all our proposed method, MMM (MAN classifier + speaker normalization + two stage learning strategies). As direct comparisons, we also list the accuracy increment between MMM and the baseline with the same sentence encoder marked by the parentheses, from which we can see that the performance augmentation is over 9% for BERT-Base and BERT-Large.",
        "type": "Document"
      },
      {
        "id": "874d1312-9fdb-46fe-865b-ce75dcb445d6",
        "metadata": {
          "vector_store_key": "1812.07023-3",
          "chunk_id": 32,
          "document_id": "1812.07023",
          "start_idx": 17666,
          "end_idx": 18428
        },
        "page_content": "We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below: Our primary architectural differences over the baseline model are: not concatenating the question, answer pairs before encoding them, the auxiliary decoder module, and using the Time-Extended FiLM module for feature extraction. These, combined with using scheduled sampling and running hyperparameter optimization over the validation set to select hyperparameters, give us the observed performance boost. We observe that our models generate fairly relevant responses to questions in the dialogues, and models with audio-visual inputs respond to audio-visual questions (e.g. \u201cis there any voices or music ?\u201d) correctly more often.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the highest accuracy score achieved?",
    "llm_answer": "84.84%\n",
    "context": [
      {
        "id": "050432d2-b325-4282-ae97-1513abbfedc7",
        "metadata": {
          "vector_store_key": "2003.06279-7",
          "chunk_id": 36,
          "document_id": "2003.06279",
          "start_idx": 21531,
          "end_idx": 22144
        },
        "page_content": "While Figures FIGREF14 \u2013 FIGREF16 show the relative behavior in the accuracy, it still interesting to observe the absolute accuracy rate obtained with the classifiers. In Table TABREF17, we show the best accuracy rate (i.e. $\\max \\Gamma _+ = \\max _p \\Gamma _+(p)$) for GloVe. We also show the average difference in performance ($\\langle \\Gamma _+ - \\Gamma _0 \\rangle $) and the total number of cases in which an improvement in performance was observed ($N_+$). $N_+$ ranges in the interval $0 \\le N_+ \\le 20$. Table TABREF17 summarizes the results obtained for $w = \\lbrace 1.0, 5.0, 10.0\\rbrace $ thousand words.",
        "type": "Document"
      },
      {
        "id": "320f4c0c-143a-4d2c-872f-0a480e50c134",
        "metadata": {
          "vector_store_key": "1901.02257-0",
          "chunk_id": 31,
          "document_id": "1901.02257",
          "start_idx": 18545,
          "end_idx": 19174
        },
        "page_content": "The TriAN achieves 81.94% in terms of test accuracy, which is the best result of the single model. The best performing ensemble result is 84.13%, provided by HMA, which is the voting results of 7 single systems. Our single MPFN model achieves 83.52% in terms of accuracy, outperforming all the previous models. The model exceeds the HMA and TriAN by approximately 2.58% and 1.58% absolute respectively. Our ensemble model surpasses the current state-of-the-art model with an accuracy of 84.84%. We got the final ensemble result by voting on 4 single models. Every single model uses the same architecture but different parameters.",
        "type": "Document"
      },
      {
        "id": "c6bf5650-c4e0-4678-9752-ec427fb3980e",
        "metadata": {
          "vector_store_key": "2001.11268-5",
          "chunk_id": 39,
          "document_id": "2001.11268",
          "start_idx": 22675,
          "end_idx": 23309
        },
        "page_content": "After obtaining the model's predictions, a simple threshold parameter can be used to obtain the final class labels. On our labelled testing data, we tested 50 evenly spaced thresholds between 0 and 1 in order to obtain these graphs. Here, recall and precision scores in ranges between 0.92 and 0.97 are possible with F1 scores not dropping below 0.84 for the main classes of interest. In practice, the detachment between model predictions and assignment of labels means that a reviewer who wishes to switch between high recall and high precision results can do so very quickly, without obtaining new predictions from the model itself.",
        "type": "Document"
      },
      {
        "id": "b793e1d3-56ff-490e-9245-8753da5bf16c",
        "metadata": {
          "vector_store_key": "1911.07228-2",
          "chunk_id": 17,
          "document_id": "1911.07228",
          "start_idx": 8542,
          "end_idx": 9180
        },
        "page_content": "In our experiments, we use three evaluation parameters (precision, recall, and F1 score) to access our experimental result. They will be described as follow in Table 3. The \"correctNE\", the number of correct label for entity that the model can found. The \"goldNE\", number of the real label annotated by annotator in the gold data. The \"foundNE\", number of the label the model find out (no matter if they are correct or not). In Table 3 above, we can see that recall score on ORG label is lowest. The reason is almost all the ORG label on test file is name of some brands that do not appear on training data and pre-trained word embedding.",
        "type": "Document"
      },
      {
        "id": "cd8b8886-f028-4670-a1c2-713b6ff3c7b0",
        "metadata": {
          "vector_store_key": "1804.03396-0",
          "chunk_id": 48,
          "document_id": "1804.03396",
          "start_idx": 25921,
          "end_idx": 26660
        },
        "page_content": "/ L ~,\n\\end{split}$$   (Eq. 34)  where $(a^1_{i_1}, ... , a^L_{i_L})$ is the answer sequence and ${\\rm P}(a^t_i)$ is defined in Eq. ( 22 ). ${\\rm Score_{mul}}$ is equivalent to the training loss in Eq. ( 24 ) and ${\\rm Score_{avg}}$ takes the answer length into account. The precision-recall curves of our framework based on the two confidence scores are plotted in Figure 3 . We can observe that the EM rate we achieve in QA settings is actually the best recall (91.87) in this curve (by setting $\\delta = 0$ ). The best F1-scores of the two curves are 29.97 (precision $= 21.61$ , recall $= 48.85$ , $\\delta = 0.91$ ) for ${\\rm Score_{mul}}$ and 31.05 (precision $= 23.93$ , recall $= 44.21$ , $\\delta = 0.97$ ) for ${\\rm Score_{avg}}$ .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the three datasets used in the paper?",
    "llm_answer": "France, UK, and laptop/restaurant datasets.\n",
    "context": [
      {
        "id": "711988d5-b6d5-455f-a849-15926d3a08fb",
        "metadata": {
          "vector_store_key": "1910.12618-2",
          "chunk_id": 12,
          "document_id": "1910.12618",
          "start_idx": 6819,
          "end_idx": 7418
        },
        "page_content": "The rest of this paper is organized as follows. The following section introduces the two data sets used to conduct our study. Section 3 presents the different machine learning approaches used and how they were tuned. Section 4 highlights the main results of our study, while section 5 concludes this paper and gives insight on future possible work. In order to prove the consistency of our work, experiments have been conducted on two data sets, one for France and the other for the UK. In this section details about the text and time series data are given, as well as the major preprocessing steps.",
        "type": "Document"
      },
      {
        "id": "e01a5883-53fd-4319-9281-c693f6334799",
        "metadata": {
          "vector_store_key": "1709.08299-0",
          "chunk_id": 2,
          "document_id": "1709.08299",
          "start_idx": 1352,
          "end_idx": 2008
        },
        "page_content": "In this paper, we adopt two widely used neural reading comprehension model: AS Reader BIBREF3 and AoA Reader BIBREF4 . The rest of the paper will be organized as follows. In Section 2, we will introduce the related works on the reading comprehension dataset, and then the proposed dataset as well as our competitions will be illustrated in Section 3. The baseline and participant system results will be given in Section 4 and we will made a brief conclusion at the end of this paper. In this section, we will introduce several public cloze-style reading comprehension dataset. Some news articles often come along with a short summary or brief introduction.",
        "type": "Document"
      },
      {
        "id": "1b887290-2c77-48b6-b1a2-fc513d707afc",
        "metadata": {
          "vector_store_key": "1909.00124-1",
          "chunk_id": 20,
          "document_id": "1909.00124",
          "start_idx": 11467,
          "end_idx": 12190
        },
        "page_content": "The other two datasets are laptop and restaurant datasets collected from SemEval-2016 . The former consists of laptop review sentences and the latter consists of restaurant review sentences. The original datasets (i.e., Laptop and Restaurant) were annotated with aspect polarity in each sentence. We used all sentences with only one polarity (positive or negative) for their aspects. That is, we only used sentences with aspects having the same sentiment label in each sentence. Thus, the sentiment of each aspect gives the ground-truth as the sentiments of all aspects are the same. For each clean-labeled dataset, the sentences are randomly partitioned into training set and test set with $80\\%$ and $20\\%$, respectively.",
        "type": "Document"
      },
      {
        "id": "2ded8a9e-d4bc-48fa-9555-3885b88eba42",
        "metadata": {
          "vector_store_key": "1910.00458-5",
          "chunk_id": 32,
          "document_id": "1910.00458",
          "start_idx": 17414,
          "end_idx": 18132
        },
        "page_content": "For the sentiment analysis category, we used the Stanford Sentiment Treebank (SST-2) dataset from the GLUE benchmark BIBREF22 (around 60k train examples) and the Yelp dataset (around 430k train examples). For the paraphrase category, three paraphrasing datasets are used from the GLUE benchmark: Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), and Quora Question Pairs (QQP), which are denoted as \u201cGLUE-Para.\u201d. For the span-based QA, we use the SQuAD 1.1, SQuAD 2.0 , and MRQA which is a joint dataset including six popular span-based QA datasets. Table TABREF23 summarizes the results. We see that sentiment analysis datasets do not help much with our target MCQA datasets.",
        "type": "Document"
      },
      {
        "id": "598753ae-f85a-4b8a-91f2-1a842414776d",
        "metadata": {
          "vector_store_key": "1906.03538-2",
          "chunk_id": 43,
          "document_id": "1906.03538",
          "start_idx": 24759,
          "end_idx": 25446
        },
        "page_content": "In addition, we ignore trustworthiness and credibility issues, important research issues that are addressed in other works. The authors would like to thank Jennifer Sheffield, Stephen Mayhew, Shyam Upadhyay, Nitish Gupta and the anonymous reviewers for insightful comments and suggestions. This work was supported in part by a gift from Google and by Contract HR0011-15-2-0025 with the US Defense Advanced Research Projects Agency (DARPA). The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. We provide brief statistics on the sources of different content in our dataset in Table TABREF46 .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is improvement in accuracy for short Jokes in relation other types of jokes?",
    "llm_answer": "A jump of 8 percent from the most recent CNN work.\n",
    "context": [
      {
        "id": "f64d833c-38eb-47da-9ead-e53d9a74ff18",
        "metadata": {
          "vector_store_key": "1909.00252-1",
          "chunk_id": 17,
          "document_id": "1909.00252",
          "start_idx": 9216,
          "end_idx": 9948
        },
        "page_content": "We hypothesize that this is due to the variations found in the different joke bodies: some take paragraphs to set up the joke, while others are less than a sentence. Our experiment with the Short Jokes dataset found the Transformer model's accuracy and F1 score to be 0.986. This was a jump of 8 percent from the most recent work done with CNNs (Table 4). The results on the Pun of the Day dataset are shown in Table 3 above. It shows an accuracy of 93 percent, close to 4 percent greater accuracy than the best CNN model proposed. Although the CNN model used a variety of techniques to extract the best features from the dataset, we see that the self-attention layers found even greater success in pulling out the crucial features.",
        "type": "Document"
      },
      {
        "id": "997769ec-9688-45bf-855e-b9f0959925a7",
        "metadata": {
          "vector_store_key": "1909.00252-1",
          "chunk_id": 16,
          "document_id": "1909.00252",
          "start_idx": 8627,
          "end_idx": 9216
        },
        "page_content": "In order to understand what may be happening in the model, we used the body and punchline only datasets to see what part of the joke was most important for humor. We found that all of the models, including humans, relied more on the punchline of the joke in their predictions (Table 2). Thus, it seems that although both parts of the joke are needed for it to be humorous, the punchline carries higher weight than the body. We hypothesize that this is due to the variations found in the different joke bodies: some take paragraphs to set up the joke, while others are less than a sentence.",
        "type": "Document"
      },
      {
        "id": "4f975c27-5cd5-49bc-be62-289d94e348fa",
        "metadata": {
          "vector_store_key": "1909.00252-2",
          "chunk_id": 21,
          "document_id": "1909.00252",
          "start_idx": 11364,
          "end_idx": 11790
        },
        "page_content": "This proposed model does not require any human interaction to determine, aside from the text of the joke itself, which jokes are humorous. This architecture can predict the level of humor for a specific audience to a higher degree than a general audience consensus. We also showed that this model has increased capability in joke identification as a result, with higher accuracy and F1 scores than previous work on this topic.",
        "type": "Document"
      },
      {
        "id": "5485a87a-3b1a-4053-b0c3-843041c5f4bf",
        "metadata": {
          "vector_store_key": "1909.00252-2",
          "chunk_id": 2,
          "document_id": "1909.00252",
          "start_idx": 1177,
          "end_idx": 1973
        },
        "page_content": "Although this type of humor may only be most enjoyable to a subset of the population, it is an effective way to measure responses to jokes in a large group setting. What enables us to perform such an analysis are the recent improvements in neural network architecture for natural language processing. These breakthroughs started with the Convolutional Neural Network BIBREF8 and have recently included the inception BIBREF9 and progress of the Attention mechanism BIBREF10, BIBREF11, and the Transformer architecture BIBREF12. In the related work of joke identification, we find a myriad of methods employed over the years: statistical and N-gram analysis BIBREF13, Regression Trees BIBREF14, Word2Vec combined with K-NN Human Centric Features BIBREF15, and Convolutional Neural Networks BIBREF4.",
        "type": "Document"
      },
      {
        "id": "9125dd85-9ff9-498a-befd-47c70601809f",
        "metadata": {
          "vector_store_key": "1909.00252-2",
          "chunk_id": 20,
          "document_id": "1909.00252",
          "start_idx": 10700,
          "end_idx": 11364
        },
        "page_content": "Although these results have high potential, defining the absolute truth value for a joke's humor is a challenging, if not impossible task. However, these results indicate that, at least for a subset of the population, we can find and identify jokes that will be most humorous to them. In this paper, we showed a method to define the measure of a joke's humor. We explored the idea of using machine learning tools, specifically a Transformer neural network architecture, to discern what jokes are funny and what jokes are not. This proposed model does not require any human interaction to determine, aside from the text of the joke itself, which jokes are humorous.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What baseline did they compare Entity-GCN to?",
    "llm_answer": "BiDAF and FastQA.\n",
    "context": [
      {
        "id": "fdc2320f-e084-4709-b3e5-bff97ddabec1",
        "metadata": {
          "vector_store_key": "1808.09920-8",
          "chunk_id": 45,
          "document_id": "1808.09920",
          "start_idx": 24145,
          "end_idx": 24894
        },
        "page_content": "Therefore, as we rely mostly on exact matching when constructing our graph for the masked case, we are more effective in recovering coreference links on the masked rather than unmasked version. In Figure 3 , we show how the model performance goes when the input graph is large. In particular, how Entity-GCN performs as the number of candidate answers or the number of nodes increases. In this section we provide an error analysis for our best single model predictions. First of all, we look at which type of questions our model performs well or poorly. There are more than 150 query types in the validation set but we filtered the three with the best and with the worst accuracy that have at least 50 supporting documents and at least 5 candidates.",
        "type": "Document"
      },
      {
        "id": "d1f3b989-46cf-4ae2-8e2d-12283c38fabc",
        "metadata": {
          "vector_store_key": "1808.09920-8",
          "chunk_id": 8,
          "document_id": "1808.09920",
          "start_idx": 4443,
          "end_idx": 5130
        },
        "page_content": "As our model is efficient, we also reported results of an ensemble which brings further 3.6% of improvement and only 3% below the human performance reported by BIBREF0 . Our contributions can be summarized as follows: In this section we explain our method. We first introduce the dataset we focus on, WikiHop by BIBREF0 , as well as the task abstraction. We then present the building blocks that make up our Entity-GCN model, namely, an entity graph used to relate mentions to entities within and across documents, a document encoder used to obtain representations of mentions in context, and a relational graph convolutional network that propagates information through the entity graph.",
        "type": "Document"
      },
      {
        "id": "c21fa98c-9cce-4219-a09f-1bcfa31be913",
        "metadata": {
          "vector_store_key": "1808.09920-3",
          "chunk_id": 7,
          "document_id": "1808.09920",
          "start_idx": 4099,
          "end_idx": 4769
        },
        "page_content": "Even in the somewhat contrived WikiHop setting, where fairly small sets of candidates are provided, the model is at least 5 times faster to train than BiDAF. Interestingly, when we substitute ELMo with simple pre-trained word embeddings, Entity-GCN still performs on par with many techniques that use expensive question-aware recurrent document encoders. Despite not using recurrent document encoders, the full Entity-GCN model achieves over 2% improvement over the best previously-published results. As our model is efficient, we also reported results of an ensemble which brings further 3.6% of improvement and only 3% below the human performance reported by BIBREF0 .",
        "type": "Document"
      },
      {
        "id": "2514ca3b-6c8e-4409-a3de-225b883e4ff7",
        "metadata": {
          "vector_store_key": "1909.02635-2",
          "chunk_id": 25,
          "document_id": "1909.02635",
          "start_idx": 14804,
          "end_idx": 15568
        },
        "page_content": "Consequently, their model achieves low performance, roughly 56 $F_1 $ while ours achieves $82.5$ $F_1$ (though these are not the exact same test set). Moreover, theirs underperforms the first occurrence baseline, which calls into question the value of that training data. Therefore, we do not compare to this model directly. We use the small set of human-annotated data for our probing task. Our train/dev/test split consists of $600/100/175$ recipes, respectively. Table TABREF20 compares the overall performances of our proposed models. Our best ET$_{GPT}$ model achieves an $F_1$ score of $82.50$. Comparing to the baselines (Majority through First) and post-conditioned models, we see that the early entity conditioning is critical to achieve high performance.",
        "type": "Document"
      },
      {
        "id": "40382650-93cd-4337-9925-a376cf25e696",
        "metadata": {
          "vector_store_key": "1808.09920-3",
          "chunk_id": 30,
          "document_id": "1808.09920",
          "start_idx": 15870,
          "end_idx": 16513
        },
        "page_content": "In this section, we compare our method against recent work as well as preforming an ablation study using the WikiHop dataset BIBREF0 . See Appendix \"Implementation and experiments details\" in the supplementary material for a description of the hyper-parameters of our model and training details. In this experiment, we compare our Enitity-GCN against recent prior work on the same task. We present test and development results (when present) for both versions of the dataset in Table 2 . From BIBREF0 , we list an oracle based on human performance as well as two standard reading comprehension models, namely BiDAF BIBREF3 and FastQA BIBREF6 .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How did they get relations between mentions?",
    "llm_answer": "They used i) co-occurrence within the same document (DOC-BASED), ii) exact matching (MATCH), and iii) coreference predicted by an external system (COREF).\n",
    "context": [
      {
        "id": "555562ef-e184-43df-ab93-838bd4372b9d",
        "metadata": {
          "vector_store_key": "1808.09920-1",
          "chunk_id": 14,
          "document_id": "1808.09920",
          "start_idx": 7377,
          "end_idx": 8578
        },
        "page_content": "This process is based on the following heuristic: we consider mentions spans in $S_q$ exactly matching an element of $C_q \\cup \\lbrace s\\rbrace $ . Admittedly, this is a rather simple strategy which may suffer from low recall. we use predictions from a coreference resolution system to add mentions of elements in $C_q \\cup \\lbrace s\\rbrace $ beyond exact matching (including both noun phrases and anaphoric pronouns). In particular, we use the end-to-end coreference resolution by BIBREF16 . we discard mentions which are ambiguously resolved to multiple coreference chains; this may sacrifice recall, but avoids propagating ambiguity. To each node $v_i$ , we associate a continuous annotation $\\mathbf {x}_i \\in \\mathbb {R}^D$ which represents an entity in the context where it was mentioned (details in Section \"Node annotations\" ). We then proceed to connect these mentions i) if they co-occur within the same document (we will refer to this as DOC-BASED edges), ii) if the pair of named entity mentions is identical (MATCH edges\u2014these may connect nodes across and within documents), or iii) if they are in the same coreference chain, as predicted by the external coreference system (COREF edges).",
        "type": "Document"
      },
      {
        "id": "9c6ca735-45dc-4fa7-9c2a-6065b34c7928",
        "metadata": {
          "vector_store_key": "1902.00330-5",
          "chunk_id": 60,
          "document_id": "1902.00330",
          "start_idx": 33205,
          "end_idx": 33845
        },
        "page_content": "Unfortunately, choosing the number of attention mentions is not easy in practice. Two recent studies BIBREF8 , BIBREF36 finish linking all mentions by scanning the pairs of mentions at most once, they assume each mention only needs to be consistent with one another mention in the document. The limitation of their method is that the consistency information is too sparse, resulting in low confidence. Similar to us, Guo et al. BIBREF18 also sort mentions according to the difficulty of disambiguation, but they did not make full use of the information of previously referred entities for the subsequent entity disambiguation. Nguyen et al.",
        "type": "Document"
      },
      {
        "id": "2249a35b-d97e-40b4-be9d-4d8912bfde4b",
        "metadata": {
          "vector_store_key": "1808.09920-1",
          "chunk_id": 40,
          "document_id": "1808.09920",
          "start_idx": 21154,
          "end_idx": 21895
        },
        "page_content": "Next, we ablate each type of relations independently, that is, we either remove connections of mentions that co-occur in the same document (DOC-BASED), connections between mentions matching exactly (MATCH), or edges predicted by the coreference system (COREF). The first thing to note is that the model makes better use of DOC-BASED connections than MATCH or COREF connections. This is mostly because i) the majority of the connections are indeed between mentions in the same document, and ii) without connecting mentions within the same document we remove important information since the model is unaware they appear closely in the document. Secondly, we notice that coreference links and complement edges seem to play a more marginal role.",
        "type": "Document"
      },
      {
        "id": "a8c5492c-1adf-464c-b4c5-69192031444e",
        "metadata": {
          "vector_store_key": "1902.00330-5",
          "chunk_id": 20,
          "document_id": "1902.00330",
          "start_idx": 11422,
          "end_idx": 12057
        },
        "page_content": "On this basis, we define $\\Psi _{max}(m_i, e_i^a)$ as the the maximum local similarity between the $m_i$ and its candidate set $C_{m_i} = \\lbrace e_i^1, e_i^2,..., e_i^n\\rbrace $ . We use $\\Psi _{max}(m_i, e_i^a)$ as criterion when sorting mentions. For instance, if $\\Psi _{max}(m_i, e_i^a) > \\Psi _{max}(m_j, e_j^b)$ then we place $m_i$ before $m_j$ . Under this circumstances, the mentions in the front positions may not be able to make better use of global consistency, but their target entities have a high degree of similarity to the context words, which allows them to be disambiguated without relying on additional information.",
        "type": "Document"
      },
      {
        "id": "3348402b-d30d-4267-9bbb-70d3dac230c9",
        "metadata": {
          "vector_store_key": "1603.04553-4",
          "chunk_id": 12,
          "document_id": "1603.04553",
          "start_idx": 6735,
          "end_idx": 7745
        },
        "page_content": "It makes sense because the position information is important for coreference resolved by matching attributes of two mentions such as resolving pronoun coreference, but not that important for those resolved by matching text or special relations like two mentions referring the same person and matching by the name. [t] Learning Model with EM Initialization: Initialize $\\theta _0 = \\lbrace t_0, q_0\\rbrace $   $t=0$ to $T$ set all counts $c(\\ldots ) = 0$  each document $D$ $j=1$ to $n$ $k=0$ to $j - 1$ $L_{jk} = \\frac{t(m_j|m_k,\\pi _j)q(k|\\pi _j, j)}{\\sum \\limits _{i = 0}^{j-1} t(m_j|m_i,\\pi _j)q(i|\\pi _j, j)}$   $c(m_j, m_k, \\pi _j) \\mathrel {+}= L_{jk}$   $c(m_k, \\pi _j) \\mathrel {+}= L_{jk}$   $c(k, j, \\pi _j) \\mathrel {+}= L_{jk}$   $c(j, \\pi _j) \\mathrel {+}= L_{jk}$ Recalculate the parameters $t(m|m^{\\prime }, \\pi ) = \\frac{c(m, m^{\\prime }, \\pi )}{c(m^{\\prime }, \\pi )}$   $q(k, j, \\pi ) = \\frac{c(k, j, \\pi )}{c(j, \\pi )}$  In this section, we describe the features we use to represent mentions.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How did they detect entity mentions?",
    "llm_answer": "The basic rules used to detect mentions are similar to those of Lee:2013:CL, except that their system uses filtering rules.  The described system keeps partitives, quantified noun phrases, and bare NP mentions, but discards pleonastic it and other spurious mentions.  Mentions are also selected by taking surrounding words and encoding them using a pre-trained lookup table.\n",
    "context": [
      {
        "id": "c270b503-2117-4cca-916a-5409b4814bfa",
        "metadata": {
          "vector_store_key": "1603.04553-4",
          "chunk_id": 16,
          "document_id": "1603.04553",
          "start_idx": 8974,
          "end_idx": 9640
        },
        "page_content": "The basic rules we used to detect mentions are similar to those of Lee:2013:CL, except that their system uses a set of filtering rules designed to discard instances of pleonastic it, partitives, certain quantified noun phrases and other spurious mentions. Our system keeps partitives, quantified noun phrases and bare NP mentions, but discards pleonastic it and other spurious mentions. Datasets. Due to the availability of readily parsed data, we select the APW and NYT sections of Gigaword Corpus (years 1994-2010) BIBREF20 to train the model. Following previous work BIBREF3 , we remove duplicated documents and the documents which include fewer than 3 sentences.",
        "type": "Document"
      },
      {
        "id": "b467a6d1-5646-4bfc-a73a-636b10060a88",
        "metadata": {
          "vector_store_key": "1603.04553-4",
          "chunk_id": 15,
          "document_id": "1603.04553",
          "start_idx": 8356,
          "end_idx": 9315
        },
        "page_content": "The best parameters appear at around the 5th iteration, according to our experiments. The detailed derivation of the learning algorithm is shown in Appendix A. The pseudo-code is shown is Algorithm \"Resolution Mode Variables\" . We use uniform initialization for all the parameters in our model. Several previous work has attempted to use EM for entity coreference resolution. cherry-bergsma:2005 and charniak-elsner:2009 applied EM for pronoun anaphora resolution. ng:2008:EMNLP probabilistically induced coreference partitions via EM clustering. Recently, moosavi2014 proposed an unsupervised model utilizing the most informative relations and achieved competitive performance with the Stanford system. The basic rules we used to detect mentions are similar to those of Lee:2013:CL, except that their system uses a set of filtering rules designed to discard instances of pleonastic it, partitives, certain quantified noun phrases and other spurious mentions.",
        "type": "Document"
      },
      {
        "id": "5a472ed6-bfb0-41be-9752-93ac10249a21",
        "metadata": {
          "vector_store_key": "1902.00330-5",
          "chunk_id": 64,
          "document_id": "1902.00330",
          "start_idx": 35603,
          "end_idx": 36562
        },
        "page_content": "By utilizing the information of previously referred entities, we can take advantage of global consistency to disambiguate mentions. For each selection result in the current state, it also has a long-term impact on subsequent decisions, which allows learned policy strategy has a global view. In experiments, we evaluate our method on AIDA-B and other well-known datasets, the results show that our system outperforms state-of-the-art solutions. In the future, we would like to use reinforcement learning to detect mentions and determine which mention should be firstly disambiguated in the document. This research is supported by the GS501100001809National Key Research and Development Program of China (No. GS5011000018092018YFB1004703), GS501100001809the Beijing Municipal Science and Technology Project under grant (No. GS501100001809 Z181100002718004), and GS501100001809the National Natural Science Foundation of China grants(No. GS50110000180961602466).",
        "type": "Document"
      },
      {
        "id": "ca42069a-d356-4d08-a7f9-24b993fb8093",
        "metadata": {
          "vector_store_key": "1902.00330-1",
          "chunk_id": 14,
          "document_id": "1902.00330",
          "start_idx": 7942,
          "end_idx": 8751
        },
        "page_content": "For each mention, we firstly select its $n$ surrounding words, and represent them as word embedding using a pre-trained lookup table BIBREF11 . Then, we use Long Short-Term Memory (LSTM) networks to encode the contextual word sequence $\\lbrace w_c^1, w_c^2,..., w_c^n\\rbrace $ as a fixed-size vector $V_{m_t}$ . The description of entity is encoded as $D_{e_t^i}$ in the same way. Apart from the description of entity, there are many other valuable information in the knowledge base. To make full use of these information, many researchers trained entity embeddings by combining the description, category, and relationship of entities. As shown in BIBREF0 , entity embeddings compress the semantic meaning of entities and drastically reduce the need for manually designed features or co-occurrence statistics.",
        "type": "Document"
      },
      {
        "id": "7cefa421-7611-4d7f-a9a5-2940853c44a9",
        "metadata": {
          "vector_store_key": "1603.04553-3",
          "chunk_id": 0,
          "document_id": "1603.04553",
          "start_idx": 0,
          "end_idx": 863
        },
        "page_content": "Entity coreference resolution has become a critical component for many Natural Language Processing (NLP) tasks. Systems requiring deep language understanding, such as information extraction BIBREF2 , semantic event learning BIBREF3 , BIBREF4 , and named entity linking BIBREF5 , BIBREF6 all benefit from entity coreference information. Entity coreference resolution is the task of identifying mentions (i.e., noun phrases) in a text or dialogue that refer to the same real-world entities. In recent years, several supervised entity coreference resolution systems have been proposed, which, according to ng:2010:ACL, can be categorized into three classes \u2014 mention-pair models BIBREF7 , entity-mention models BIBREF8 , BIBREF9 , BIBREF10 and ranking models BIBREF11 , BIBREF12 , BIBREF13 \u2014 among which ranking models recently obtained state-of-the-art performance.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What performance does the Entity-GCN get on WIKIHOP?",
    "llm_answer": "Entity-GCN achieves over 2% improvement over the best previously-published results on WikiHop, and an ensemble improves this further by 3.6% and only 3% below human performance.\n",
    "context": [
      {
        "id": "c21fa98c-9cce-4219-a09f-1bcfa31be913",
        "metadata": {
          "vector_store_key": "1808.09920-3",
          "chunk_id": 7,
          "document_id": "1808.09920",
          "start_idx": 4099,
          "end_idx": 4769
        },
        "page_content": "Even in the somewhat contrived WikiHop setting, where fairly small sets of candidates are provided, the model is at least 5 times faster to train than BiDAF. Interestingly, when we substitute ELMo with simple pre-trained word embeddings, Entity-GCN still performs on par with many techniques that use expensive question-aware recurrent document encoders. Despite not using recurrent document encoders, the full Entity-GCN model achieves over 2% improvement over the best previously-published results. As our model is efficient, we also reported results of an ensemble which brings further 3.6% of improvement and only 3% below the human performance reported by BIBREF0 .",
        "type": "Document"
      },
      {
        "id": "d1f3b989-46cf-4ae2-8e2d-12283c38fabc",
        "metadata": {
          "vector_store_key": "1808.09920-8",
          "chunk_id": 8,
          "document_id": "1808.09920",
          "start_idx": 4443,
          "end_idx": 5130
        },
        "page_content": "As our model is efficient, we also reported results of an ensemble which brings further 3.6% of improvement and only 3% below the human performance reported by BIBREF0 . Our contributions can be summarized as follows: In this section we explain our method. We first introduce the dataset we focus on, WikiHop by BIBREF0 , as well as the task abstraction. We then present the building blocks that make up our Entity-GCN model, namely, an entity graph used to relate mentions to entities within and across documents, a document encoder used to obtain representations of mentions in context, and a relational graph convolutional network that propagates information through the entity graph.",
        "type": "Document"
      },
      {
        "id": "56dec742-f01c-4f43-9bec-0b13edfe097c",
        "metadata": {
          "vector_store_key": "1808.09920-8",
          "chunk_id": 37,
          "document_id": "1808.09920",
          "start_idx": 19785,
          "end_idx": 20544
        },
        "page_content": "That is, the R-GCN component pushes the model to perform above Coref-GRU still without accessing context, but rather by updating mention representations based on their relation to other ones. These results highlight the impact of our R-GCN component. In this experiment we investigate the effect of the different relations available in the entity graph and processed by the R-GCN module. We start off by testing our stronger encoder (i.e., ELMo) in absence of edges connecting mentions in the supporting documents (i.e., using only self-loops \u2013 No R-GCN in Table 3 ). The results suggest that WikipHop genuinely requires multihop inference, as our best model is 6.1% and 8.4% more accurate than this local model, in unmasked and masked settings, respectively.",
        "type": "Document"
      },
      {
        "id": "dba06160-9931-4ebf-b6fd-f9c74e3ae92c",
        "metadata": {
          "vector_store_key": "1808.09920-8",
          "chunk_id": 9,
          "document_id": "1808.09920",
          "start_idx": 5130,
          "end_idx": 5772
        },
        "page_content": "We then present the building blocks that make up our Entity-GCN model, namely, an entity graph used to relate mentions to entities within and across documents, a document encoder used to obtain representations of mentions in context, and a relational graph convolutional network that propagates information through the entity graph. The WikiHop dataset comprises of tuples $\\langle q, S_q, C_q, a^\\star \\rangle $ where: $q$ is a query/question, $S_q$ is a set of supporting documents, $C_q$ is a set of candidate answers (all of which are entities mentioned in $S_q$ ), and $a^\\star \\in C_q$ is the entity that correctly answers the question.",
        "type": "Document"
      },
      {
        "id": "28175839-e1d2-4454-91a1-023bc366951a",
        "metadata": {
          "vector_store_key": "1808.09920-8",
          "chunk_id": 49,
          "document_id": "1808.09920",
          "start_idx": 25993,
          "end_idx": 26685
        },
        "page_content": "This is important as document sets can be large in practical applications. See Figure 3 in the supplemental material for plots. In Table 6 , we report three samples from WikiHop development set where out Entity-GCN fails. In particular, we show two instances where our model presents high confidence on the answer, and one where is not. We commented these samples explaining why our model might fail in these cases. In previous work, BiDAF BIBREF3 , FastQA BIBREF6 , Coref-GRU BIBREF12 , MHPGM BIBREF11 , and Weaver / Jenga BIBREF10 have been applied to multi-document question answering. The first two mainly focus on single document QA and BIBREF0 adapted both of them to work with WikiHop.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What document context was added?",
    "llm_answer": "A preceding and following sentence, forming a document context of a sentence pair.\n",
    "context": [
      {
        "id": "e521a3b1-76f8-41e4-aedb-5d8efc9f7695",
        "metadata": {
          "vector_store_key": "1906.03338-0",
          "chunk_id": 30,
          "document_id": "1906.03338",
          "start_idx": 16243,
          "end_idx": 16930
        },
        "page_content": "In other words, in this setting we want to examine effects when porting a context-sensitive system to a multi-document setting. For example, as seen in Figure FIGREF42 , the context of an argumentative unit may change from \u201cHowever\u201d to \u201cMoreover\u201d \u2013 which can happen naturally in open debates. The results are displayed in Figure FIGREF43 . In the standard setting (Figure FIGREF43 ), the models that have access to the context besides the content ( INLINEFORM0 ) and the models that are only allowed to access the context ( INLINEFORM1 ), always perform better than the content-based models ( INLINEFORM2 ) (bars above zero). However, when we randomly flip contexts of the test instances",
        "type": "Document"
      },
      {
        "id": "76d8c0e2-1364-47e9-b728-1ef8f2511451",
        "metadata": {
          "vector_store_key": "1906.03338-0",
          "chunk_id": 29,
          "document_id": "1906.03338",
          "start_idx": 16070,
          "end_idx": 16854
        },
        "page_content": "In what follows we are simulating the effects that an overly context-sensitive classifier could have in a cross-document setting, by modifying our experimental setting, and study the effects on the different model types: In one setup \u2013 we call it randomized-context \u2013 we systematically distort the context of our testing instances by exchanging the context in a randomized manner; in the other setting \u2013 called no-context, we are deleting the context around the ADUs to be classified. Randomized-context simulates an open world debate where argumentative units may occur in different contexts, sometimes with discourse markers indicating an opposite class. In other words, in this setting we want to examine effects when porting a context-sensitive system to a multi-document setting.",
        "type": "Document"
      },
      {
        "id": "774acb7e-4df2-4bcd-9b60-e66273d20303",
        "metadata": {
          "vector_store_key": "1809.01060-2",
          "chunk_id": 10,
          "document_id": "1809.01060",
          "start_idx": 5439,
          "end_idx": 6243
        },
        "page_content": "We extracted 200 sentence pairs from BIBREF3 's dataset and provided each pair with a document context consisting of a preceding and a following sentence, as in the following example. One of the authors constructed most of these contexts by hand. In some cases, it was possible to locate the original metaphor in an existing document. This was the case for For these cases, a variant of the existing context was added to both the metaphorical and the literal sentences. We introduced small modifications to keep the context short and clear, and to avoid copyright issues. We lightly modified the contexts of metaphors extracted from corpora when the original context was too long, ie. when the contextual sentences of the selected metaphor were longer than the maximum length we specified for our corpus.",
        "type": "Document"
      },
      {
        "id": "2c808064-97ad-4e66-bc1d-93651b33777d",
        "metadata": {
          "vector_store_key": "1910.07481-3",
          "chunk_id": 10,
          "document_id": "1910.07481",
          "start_idx": 6135,
          "end_idx": 6776
        },
        "page_content": "This tag takes the form of an additional token placed at the first position in the sentence and corresponds to the belonging document of the sentence (see Table TABREF1). The model considers the tag as an additional word and replace it with the corresponding document embedding. The Baseline model is trained on a standard corpus that does not contain document tags, while the Document model is trained on corpus that contains document tags. The proposed approach requires strong hypotheses about train and test data. The first downfall is the need for well defined document boundaries that allow to mark each sentence with its document tag.",
        "type": "Document"
      },
      {
        "id": "72657b2c-fa20-4e5c-b74c-831afbb94091",
        "metadata": {
          "vector_store_key": "1906.03338-0",
          "chunk_id": 28,
          "document_id": "1906.03338",
          "start_idx": 15297,
          "end_idx": 16070
        },
        "page_content": "For example, for the topic of legalizing marijuana, EAUs may be mined from millions of documents and thus their relations may naturally extend across document boundaries. If a system learns to over-proportionally attend to the EAUs' surrounding contexts it is prone to making many errors. In what follows we are simulating the effects that an overly context-sensitive classifier could have in a cross-document setting, by modifying our experimental setting, and study the effects on the different model types: In one setup \u2013 we call it randomized-context \u2013 we systematically distort the context of our testing instances by exchanging the context in a randomized manner; in the other setting \u2013 called no-context, we are deleting the context around the ADUs to be classified.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What were the results of the first experiment?",
    "llm_answer": "The context describes the setup for six data sets and multiple experiments, but does not contain results for a specific \"first experiment\".\n",
    "context": [
      {
        "id": "93a841ff-3b54-4752-90d9-794ea465a2e0",
        "metadata": {
          "vector_store_key": "1709.01256-4",
          "chunk_id": 38,
          "document_id": "1709.01256",
          "start_idx": 20976,
          "end_idx": 21630
        },
        "page_content": "We generated six data sets using different random seeds, and each data set contained six corpora (Corpus 0 - 5). Table TABREF48 summarizes the first data set. In each data set, we name the initial corpus Corpus 0, and define INLINEFORM0 as the timestamp when we started this simulation process. We set INLINEFORM1 , INLINEFORM2 . Corpus INLINEFORM3 corresponds to documents generated before timestamp INLINEFORM4 . We extracted document revisions from Corpus INLINEFORM5 and compared the revisions generated in (Corpus INLINEFORM6 - Corpus INLINEFORM7 ) with the ground truths in Table TABREF48 . Hence, we ran four experiments on this data set in total.",
        "type": "Document"
      },
      {
        "id": "b360d511-f50b-49b0-adae-b9cdd19697b2",
        "metadata": {
          "vector_store_key": "1908.07816-1",
          "chunk_id": 33,
          "document_id": "1908.07816",
          "start_idx": 19008,
          "end_idx": 19804
        },
        "page_content": "Thus each person produced 25 dialogs, and in total we obtained 50 emotionally negative daily dialogs in addition to the 14 already available. To form the test set, we randomly selected 50 emotionally positive and 50 emotionally negative dialogs from the two pools of dialogs described above (78 positive dialogs from DailyDialog, 64 negative dialogs from DailyDialog and human-generated). For human evaluation of the models, we recruited another four English-speaking students from our university without any relationship to the authors' lab to rate the responses generated by the models. Specifically, we randomly shuffled the 100 dialogs in the test set, then we used the first three utterances of each dialog as the input to the three models being compared and let them generate the responses.",
        "type": "Document"
      },
      {
        "id": "1f608efa-b23e-430e-9d58-a2f60a828f34",
        "metadata": {
          "vector_store_key": "1607.06025-3",
          "chunk_id": 53,
          "document_id": "1607.06025",
          "start_idx": 31067,
          "end_idx": 31718
        },
        "page_content": "One with hypotheses that had high probability that they come from the original distribution and the other one with low probability. However, the accuracies of classifiers trained on these datasets were very similar to the accuracy of the classifier on the unfiltered dataset. Similar test was also done with the log-likelihood metric. The examples with higher log-likelihood had similar performance than the ones with lower log-likelihood. This also lead us to set the size of the beam to 1. Also, the run time of generating hypothesis is INLINEFORM0 , where INLINEFORM1 is beam size. Thus, with lower beam sizes much more hypotheses can be generated.",
        "type": "Document"
      },
      {
        "id": "e2de1ca0-a155-45ff-aab2-4caac13662f7",
        "metadata": {
          "vector_store_key": "1709.10217-0",
          "chunk_id": 16,
          "document_id": "1709.10217",
          "start_idx": 8888,
          "end_idx": 9658
        },
        "page_content": "Note that for task 2, there are 7 submitted systems. However, only 4 systems can provide correct results or be connected in a right way at the test phase. Therefore, Table TABREF16 shows the complete results of the task 2. In this paper, we introduce the first evaluation of Chinese human-computer dialogue technology. In detail, we first present the two tasks of the evaluation as well as the evaluation metrics. We then describe the released data for evaluation. Finally, we also show the evaluation results of the two tasks. As the evaluation data is provided by the iFLYTEK Corporation from their real online applications, we believe that the released data will further promote the research of human-computer dialogue and fill the blank of the data on the two tasks.",
        "type": "Document"
      },
      {
        "id": "6102bd2d-6188-4492-aa1c-76c00ba26805",
        "metadata": {
          "vector_store_key": "1801.09030-1",
          "chunk_id": 31,
          "document_id": "1801.09030",
          "start_idx": 16224,
          "end_idx": 16843
        },
        "page_content": "Because the names of the herbs have evolved a lot, we conclude heuristic rules as well as specific projection rules to project some rarely seen herbs to their similar forms that are normally referred to. There are also prescriptions that refer to the name of other prescriptions. We simply substitute these names with their constituents. To make the experiment result more robust, we conduct our experiments on two separate test datasets. The first one is a subset of the data described above. We randomly split the whole data into three parts, the training data (90%), the development data (5%) and the test data (5%).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does their model outperform existing methods?",
    "llm_answer": "The model outperforms existing methods by 1.4% in joint goal accuracy and 0.2% in turn request accuracy on the WoZ dataset.  It also shows significant improvements in EM and F1 scores on other datasets.\n",
    "context": [
      {
        "id": "11bbd522-28ef-4580-9084-84720bfb9758",
        "metadata": {
          "vector_store_key": "2001.06888-0",
          "chunk_id": 42,
          "document_id": "2001.06888",
          "start_idx": 22556,
          "end_idx": 22770
        },
        "page_content": "Evaluation results of our proposed model compared to other state of the art methods show its superiority to these methods in overall while in two categories (Person and Miscellaneous) our model outperformed others.",
        "type": "Document"
      },
      {
        "id": "acfbe9ea-af02-42f7-84b2-1c4b30db2f68",
        "metadata": {
          "vector_store_key": "1909.11833-0",
          "chunk_id": 21,
          "document_id": "1909.11833",
          "start_idx": 12031,
          "end_idx": 12806
        },
        "page_content": "We compare our model SIM with a number of baseline systems: delexicalization model BIBREF8, BIBREF1, the neural belief tracker model (NBT) BIBREF0, global-locally self-attentive model GLAD BIBREF5, large-scale belief tracking model LSBT BIBREF7 and scalable multi-domain dialogue state tracking model SMDST BIBREF6. Table TABREF17 shows that, on WoZ dataset, SIM achieves a new state-of-the-art joint goal accuracy of 89.5%, a significant improvement of 1.4% over GLAD, and turn request accuracy of 97.3%, 0.2% above GLAD. On DSTC2 dataset, where noisy ASR results are used as user utterance during test, SIM obtains comparable results with GLAD. Furthermore, the better representation in SIM makes it significantly outperform previous slot-independent models LSBT and SMDST.",
        "type": "Document"
      },
      {
        "id": "2e7cfe41-b41c-475c-83f6-e7d39903180d",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 40,
          "document_id": "1909.13375",
          "start_idx": 21052,
          "end_idx": 21792
        },
        "page_content": "While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1. All that, while keeping in mind we compare a base model to a large model (for reference, note the 8 point improvement between MTMSNbase and MTMSNlarge in both EM and F1). Our best model, large-squad, exhibits a huge improvement of 29.7 in EM and 15.1 in F1 compared to MTMSNlarge. When comparing single-span performance, our best model exhibits slightly better results, but it should be noted that it retains the single-span heads from NABERT+, while in MTMSN they have one head to predict both single-span and multi-span answers.",
        "type": "Document"
      },
      {
        "id": "814822da-592c-40ae-bb7c-e78761735daf",
        "metadata": {
          "vector_store_key": "2002.06644-3",
          "chunk_id": 10,
          "document_id": "2002.06644",
          "start_idx": 6316,
          "end_idx": 7055
        },
        "page_content": "Our proposed methodology, the use of finetuned optimized BERT based models, and BERT-based ensemble models outperform the baselines for all the metrics. Among the optimized BERT based models, $RoBERTa_{large}$ outperforms all other non-ensemble models and the baselines for all metrics. It further achieves a maximum recall of $0.681$ for all the proposed models. We note that DistillRoBERTa, a distilled model, performs competitively, achieving $69.69\\%$ accuracy, and $0.672$ F1 score. This observation shows that distilled pretrained models can replace their undistilled counterparts in a low-computing environment. We further observe that ensemble models perform better than optimized BERT-based models and distilled pretrained models.",
        "type": "Document"
      },
      {
        "id": "874d1312-9fdb-46fe-865b-ce75dcb445d6",
        "metadata": {
          "vector_store_key": "1812.07023-3",
          "chunk_id": 32,
          "document_id": "1812.07023",
          "start_idx": 17666,
          "end_idx": 18428
        },
        "page_content": "We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below: Our primary architectural differences over the baseline model are: not concatenating the question, answer pairs before encoding them, the auxiliary decoder module, and using the Time-Extended FiLM module for feature extraction. These, combined with using scheduled sampling and running hyperparameter optimization over the validation set to select hyperparameters, give us the observed performance boost. We observe that our models generate fairly relevant responses to questions in the dialogues, and models with audio-visual inputs respond to audio-visual questions (e.g. \u201cis there any voices or music ?\u201d) correctly more often.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance of their model?",
    "llm_answer": "Their large models substantially improved across all metrics compared to base models, surpassing the NABERT+ baseline.  Their best model (large-squad) showed a huge improvement of 29.7 in EM and 15.1 in F1 compared to MTMSNlarge.\n",
    "context": [
      {
        "id": "1b1f9986-e4da-44a7-a6cd-13931a730da6",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 38,
          "document_id": "1909.13375",
          "start_idx": 19979,
          "end_idx": 20678
        },
        "page_content": "For the large models we used a batch size of 3 with a learning rate of $5e^{-6}$ and trained for 5 epochs, except for the model without the single-span heads that was trained with a batch size of 2 for 7 epochs. F1 was used as our validation metric. All models were trained on a single GPU with 12-16GB of memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions.",
        "type": "Document"
      },
      {
        "id": "874d1312-9fdb-46fe-865b-ce75dcb445d6",
        "metadata": {
          "vector_store_key": "1812.07023-3",
          "chunk_id": 32,
          "document_id": "1812.07023",
          "start_idx": 17666,
          "end_idx": 18428
        },
        "page_content": "We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below: Our primary architectural differences over the baseline model are: not concatenating the question, answer pairs before encoding them, the auxiliary decoder module, and using the Time-Extended FiLM module for feature extraction. These, combined with using scheduled sampling and running hyperparameter optimization over the validation set to select hyperparameters, give us the observed performance boost. We observe that our models generate fairly relevant responses to questions in the dialogues, and models with audio-visual inputs respond to audio-visual questions (e.g. \u201cis there any voices or music ?\u201d) correctly more often.",
        "type": "Document"
      },
      {
        "id": "1570de45-12ea-4fdd-91bb-8aff4f7a2cb4",
        "metadata": {
          "vector_store_key": "1706.07179-4",
          "chunk_id": 11,
          "document_id": "1706.07179",
          "start_idx": 6089,
          "end_idx": 6711
        },
        "page_content": "We compare the performance with the Recurrent Entity Networks model (EntNet) BIBREF17 . Performance is measured in terms of mean percentage error on the tasks. Training Details: We used Adam and did a grid search for the learning rate in {0.01, 0.005, 0.001} and choose a fixed learning rate of 0.005 based on performance on the validation set, and clip the gradient norm at 2. We keep all other details similar to BIBREF17 for a fair comparison. embedding dimensions were fixed to be 100, models were trained for a maximum of 250 epochs with mini-batches size of 32 for all tasks except 3 for which the batch size was 16.",
        "type": "Document"
      },
      {
        "id": "68a5e6f2-17dd-46e5-8764-888724e87ab9",
        "metadata": {
          "vector_store_key": "1701.06538-3",
          "chunk_id": 32,
          "document_id": "1701.06538",
          "start_idx": 17371,
          "end_idx": 18235
        },
        "page_content": "Table TABREF33 compares the results of these models to the best previously-published result on this dataset . Even the fastest of these models beats the best published result (when controlling for the number of training epochs), despite requiring only 6% of the computation. We trained our models using TensorFlow BIBREF30 on clusters containing 16-32 Tesla K40 GPUs. For each of our models, we determine computational efficiency in TFLOPS/GPU by dividing the number of floating point operations required to process one training batch by the observed step time and the number of GPUs in the cluster. The operation counts used here are higher than the ones we report in our ops/timestep numbers in that we include the backwards pass, we include the importance-sampling-based training of the softmax layer, and we count a multiply-and-add as two separate operations.",
        "type": "Document"
      },
      {
        "id": "2e7cfe41-b41c-475c-83f6-e7d39903180d",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 40,
          "document_id": "1909.13375",
          "start_idx": 21052,
          "end_idx": 21792
        },
        "page_content": "While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1. All that, while keeping in mind we compare a base model to a large model (for reference, note the 8 point improvement between MTMSNbase and MTMSNlarge in both EM and F1). Our best model, large-squad, exhibits a huge improvement of 29.7 in EM and 15.1 in F1 compared to MTMSNlarge. When comparing single-span performance, our best model exhibits slightly better results, but it should be noted that it retains the single-span heads from NABERT+, while in MTMSN they have one head to predict both single-span and multi-span answers.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they damage different neural modules?",
    "llm_answer": "The context describes techniques like weight decay, dropout, dropconnect, and zoneout for regularization, but doesn't detail how they damage different neural modules.  It explains *how* these techniques are used to *improve* generalization and prevent overfitting.  No information about damaging neural modules is provided.\n",
    "context": [
      {
        "id": "0faa6f94-26e4-4942-a217-7bb84e1923ca",
        "metadata": {
          "vector_store_key": "1909.00754-8",
          "chunk_id": 26,
          "document_id": "1909.00754",
          "start_idx": 13591,
          "end_idx": 14243
        },
        "page_content": "The concatenated working memory, $\\mathbf {r}_0$ , is then fed into a Multi-Layer Perceptron (MLP) with four layers, $\n\\mathbf {r}_1 & =\\sigma (W_1^T\\mathbf {r}_0+\\mathbf {b}_1),\\\\\n\\mathbf {r}_2 & =\\sigma (W_2^T\\mathbf {r}_1+\\mathbf {b}_2),\\\\\n\\mathbf {r}_3 & = \\sigma (W_3^T\\mathbf {r}_2+\\mathbf {b}_3),\\\\\n\\mathbf {h}_s & = \\sigma (W_4^T\\mathbf {r}_3+\\mathbf {b}_4),\n$   where $\\sigma $ is a non-linear activation, and the weights $W_1 \\in R^{4d_m \\times d_m}$ , $W_i \\in R^{d_m \\times d_m}$ and the bias $b_1 \\in R^{d_m}$ , $b_i \\in R^{d_m}$ are learnable parameters, and $2\\le i\\le 4$ . The number of layers for the MLP is decided by the grid search.",
        "type": "Document"
      },
      {
        "id": "2f88ee54-702a-40e7-80ea-9577ea1af853",
        "metadata": {
          "vector_store_key": "1810.09774-4",
          "chunk_id": 29,
          "document_id": "1810.09774",
          "start_idx": 14612,
          "end_idx": 15240
        },
        "page_content": "To understand better the types of errors made by neural network models in NLI we looked at some example failure-pairs for selected models. Tables 5 and 6 contain some randomly selected failure-pairs for two models: BERT and HBMP, and for three set-ups: SNLI $\\rightarrow $ SICK, SNLI $\\rightarrow $ MultiNLI and MultiNLI $\\rightarrow $ SICK. We chose BERT as the current the state of the art NLI model. HBMP was selected as a high performing model in the sentence encoding model type. Although the listed sentence pairs represent just a small sample of the errors made by these models, they do include some interesting examples.",
        "type": "Document"
      },
      {
        "id": "74585f93-3a5c-475e-b596-ae8d41fe930d",
        "metadata": {
          "vector_store_key": "2001.07263-2",
          "chunk_id": 4,
          "document_id": "2001.07263",
          "start_idx": 2375,
          "end_idx": 3184
        },
        "page_content": "Weight decay adds the $l_2$ norm of the trainable parameters to the loss function, which encourages the weights to stay small unless necessary, and is one of the oldest techniques to improve neural network generalization. As shown in BIBREF12, weight decay can improve generalization by suppressing some of the effects of static noise on the targets. Dropout randomly deactivates neurons with a predefined probability in every training step BIBREF13 to reduce co-adaptation of neurons. DropConnect, which is similar in spirit to dropout, randomly deactivates connections between neurons by temporarily zeroing out weights BIBREF14. Zoneout, which is also inspired by dropout and was especially developed for recurrent models BIBREF15, stochastically forces some hidden units to maintain their previous values.",
        "type": "Document"
      },
      {
        "id": "f43ec371-5806-4650-b6b1-f14ab4635c0a",
        "metadata": {
          "vector_store_key": "1907.02636-4",
          "chunk_id": 27,
          "document_id": "1907.02636",
          "start_idx": 14971,
          "end_idx": 15796
        },
        "page_content": "The ANN model is trained with the stochastic gradient descent to update all parameters, i.e., token embedding, character embedding, parameters of Bi-LSTM, weights of sentence attention, weights of multi-head self-attention, token features, and transition probabilities of CRF layers at each gradient step. For regularization, the dropout is applied to the output of each sub layer of the ANN model. Further training details are given below: (a) For attention-based Bi-LSTM module, dimensions of character embedding, hidden states of character-based token embedding LSTM, hidden states of Bi-LSTM, and sentence attention are set to 25, 25, 100 and 100, respectively. For multi-head self-attention module, we employ a stack of 6 multi-head self attention layer, each of which has 4 head and dimension of each head is set to 64.",
        "type": "Document"
      },
      {
        "id": "94cdbbf1-7739-4076-ab56-9804c246b768",
        "metadata": {
          "vector_store_key": "1909.00183-9",
          "chunk_id": 78,
          "document_id": "1909.00183",
          "start_idx": 43583,
          "end_idx": 44253
        },
        "page_content": "Here we approach the task of training a supervised classifier that predicts the degree of harm of an incident based on other features of the record (such as location, external category, and medical specialty) and on the textual component of the report. To this end, we use the embedded text vectors and MS cluster labels of the records as features to predict the degree of harm to the patient. Each NRLS record has more than 170 features filled manually by healthcare staff, including the degree of harm (DoH) to the patient, a crucial assessment of the reported incident. The incident is classified into five levels: 'No harm', 'Low', 'Moderate', 'Severe', and 'Death'.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How long are the datasets?",
    "llm_answer": "The answer is not found in the context.  While the context describes various dataset *splits* and *sizes* related to specific parameters (like article length or sequence length), it does not provide a single answer for the *overall* length of any datasets.\n",
    "context": [
      {
        "id": "9ca5340d-587b-4e94-bcbe-160570788673",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 35,
          "document_id": "1910.12618",
          "start_idx": 19294,
          "end_idx": 19871
        },
        "page_content": "For instance for random forests it can correspond to the maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014 and 2015 serve as validation set (730 days).",
        "type": "Document"
      },
      {
        "id": "39218650-a63d-4aad-96f4-87a0526dc751",
        "metadata": {
          "vector_store_key": "1804.03396-9",
          "chunk_id": 26,
          "document_id": "1804.03396",
          "start_idx": 14271,
          "end_idx": 14870
        },
        "page_content": "For studying the influence of the article length as longer articles are normally more difficult to model by LSTMs, we split the articles according to the article length. We name the set of articles with lengths shorter than 400 as S, lengths between 400 and 700 as M, lengths greater than 700 as L. Therefore we obtain 6 different datasets named QA4IE-SPAN-S/M/L and QA4IE-SEQ-S/M/L. A 5/1/5 splitting of train/dev/test sets is performed. The detailed statistics of QA4IE benchmark are provided in Table 1 . We further compare our QA4IE benchmark with some existing IE and QA benchmarks in Table 2 .",
        "type": "Document"
      },
      {
        "id": "73a83f60-b2c9-406d-b4cd-5720639e5f06",
        "metadata": {
          "vector_store_key": "1812.07023-4",
          "chunk_id": 18,
          "document_id": "1812.07023",
          "start_idx": 10358,
          "end_idx": 10986
        },
        "page_content": "After 10 such qa turns, the questioner wraps up by writing a summary of the video based on the conversation. Dataset statistics such as the number of dialogues, turns, and words for the avsd dataset are presented in Table TABREF5 . For the initially released prototype dataset, the training set of the avsd dataset corresponds to videos taken from the training set of the Charades dataset while the validation and test sets of the avsd dataset correspond to videos taken from the validation set of the Charades dataset. For the official dataset, training, validation and test sets are drawn from the corresponding Charades sets.",
        "type": "Document"
      },
      {
        "id": "ae53b6c4-15b8-4c8d-ba39-f52d77140134",
        "metadata": {
          "vector_store_key": "2002.11402-3",
          "chunk_id": 11,
          "document_id": "2002.11402",
          "start_idx": 6538,
          "end_idx": 7196
        },
        "page_content": "We trained two models, one with sequence length 512 to capture document level important n-grams and second with sequence length 64 to capture sentence/paragraph level important n-grams. Through experiments it was evident that, sequence length plays a vital role in deciding context and locally/globally important n-grams. Final output is a concatenation of both the model outputs. Trained the topic model on single 32gb NVidia-V100 and it took around 50 hours to train the model with sequence length 512. We had to take 256gb ram machine to accommodate all data in memory for faster read/write. Also, trained model with 64 sequence length in around 17 hours.",
        "type": "Document"
      },
      {
        "id": "8be62763-27f4-46ff-b2bb-edb6a6b2fa04",
        "metadata": {
          "vector_store_key": "1607.06025-3",
          "chunk_id": 42,
          "document_id": "1607.06025",
          "start_idx": 24463,
          "end_idx": 25198
        },
        "page_content": "The new dataset consists of training set, which is generated using examples from the original training set, and a development set, which is generated from the original development set. The beam size for beam search was set to 1. The details of the decision are presented in Section SECREF35 . Some datasets were constructed by filtering the generated datasets according to various thresholds. Thus, the generated datasets were constructed to contain enough examples, so that the filtered datasets had at least the number of examples as the original dataset. In the end, all the datasets were trimmed down to the size of the original dataset by selecting the samples sequentially from the beginning until the dataset had the right size.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the sources of the data?",
    "llm_answer": "Twitter.\n",
    "context": [
      {
        "id": "598753ae-f85a-4b8a-91f2-1a842414776d",
        "metadata": {
          "vector_store_key": "1906.03538-2",
          "chunk_id": 43,
          "document_id": "1906.03538",
          "start_idx": 24759,
          "end_idx": 25446
        },
        "page_content": "In addition, we ignore trustworthiness and credibility issues, important research issues that are addressed in other works. The authors would like to thank Jennifer Sheffield, Stephen Mayhew, Shyam Upadhyay, Nitish Gupta and the anonymous reviewers for insightful comments and suggestions. This work was supported in part by a gift from Google and by Contract HR0011-15-2-0025 with the US Defense Advanced Research Projects Agency (DARPA). The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. We provide brief statistics on the sources of different content in our dataset in Table TABREF46 .",
        "type": "Document"
      },
      {
        "id": "02af4b8c-52ae-42f3-9382-a1980571f7d9",
        "metadata": {
          "vector_store_key": "1709.07916-1",
          "chunk_id": 5,
          "document_id": "1709.07916",
          "start_idx": 3279,
          "end_idx": 4076
        },
        "page_content": "Some examples of Twitter data analysis for health-related topics include: flu BIBREF25 , BIBREF26 , BIBREF27 , BIBREF28 , BIBREF29 , BIBREF30 , mental health BIBREF31 , Ebola BIBREF32 , BIBREF33 , Zika BIBREF34 , medication use BIBREF35 , BIBREF36 , BIBREF37 , diabetes BIBREF38 , and weight loss and obesity BIBREF39 , BIBREF40 , BIBREF41 , BIBREF42 , BIBREF21 . The previous Twitter studies have dealt with extracting common topics of one health issue discussed by the users to better understand common themes; however, this study utilizes an innovative approach to computationally analyze unstructured health related text data exchanged via Twitter to characterize health opinions regarding four common health issues, including diabetes, diet, exercise and obesity (DDEO) on a population level.",
        "type": "Document"
      },
      {
        "id": "702bcde5-40fa-4bb1-a097-5290bb0bcd84",
        "metadata": {
          "vector_store_key": "1911.03562-8",
          "chunk_id": 3,
          "document_id": "1911.03562",
          "start_idx": 842,
          "end_idx": 1736
        },
        "page_content": "A fresh data collection is planned for January 2020. Interactive Visualizations: The visualizations we are developing for this work (using Tableau) are interactive\u2014so one can hover, click to select and filter, move sliders, etc. Since this work is high in the number of visualizations, the main visualizations are presented as figures in the paper and some sets of visualizations are pointed to online. The interactive visualizations and data will be made available through the first author's website after peer review. Related Work: This work builds on past research, including that on Google Scholar BIBREF0, BIBREF1, BIBREF2, BIBREF3, on the analysis of NLP papers BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, on citation intent BIBREF10, BIBREF11, BIBREF12, BIBREF13, BIBREF14, BIBREF15, and on measuring scholarly impact BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIBREF21.",
        "type": "Document"
      },
      {
        "id": "20b2b98c-675c-4f41-b553-6b0887f12f86",
        "metadata": {
          "vector_store_key": "1910.12618-0",
          "chunk_id": 71,
          "document_id": "1910.12618",
          "start_idx": 38448,
          "end_idx": 39210
        },
        "page_content": "However due to the redundancy of the information of the considered weather reports with meteorological features, it may be necessary to consider alternative textual sources. The use of social media such as Facebook, Twitter or Instagram may give interesting insight and will therefore be investigated in future work. Additional results for the prediction tasks on temperature and wind speed can be found in tables TABREF47 to TABREF50. An example of forecast for the French temperature is given in figure FIGREF51. While not strictly normally distributed, the residuals for the French electricity demand display an acceptable behavior. This holds also true for the British consumption, and both temperature time series, but is of lesser quality for the wind one.",
        "type": "Document"
      },
      {
        "id": "d9e575cf-43f4-48a8-8180-5926cbb54aa2",
        "metadata": {
          "vector_store_key": "1906.03538-2",
          "chunk_id": 39,
          "document_id": "1906.03538",
          "start_idx": 22440,
          "end_idx": 23117
        },
        "page_content": "The dataset presented here is not intended to be exhaustive, nor does it attempt to reflect a true distribution of the important claims and perspectives in the world, or to associate any of the perspective and identified evidence with levels of expertise and trustworthiness. Moreover, it is important to note that when we ask crowd-workers to evaluate the validity of perspectives and evidence, their judgement process can potentially be influenced by their prior beliefs BIBREF42 . To avoid additional biases introduced in the process of dataset construction, we try to take the least restrictive approach in filtering dataset content beyond the necessary quality assurances.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the new labeling strategy?",
    "llm_answer": "A two-level network utilizing two labeled data sets (pure and mixed sentiment orientations) is introduced.  Lexical cues and a new encoding strategy (INLINEFORM0-hot) are used.\n",
    "context": [
      {
        "id": "837c1d96-f509-4b5c-8385-746b9e14fac6",
        "metadata": {
          "vector_store_key": "1803.07771-5",
          "chunk_id": 56,
          "document_id": "1803.07771",
          "start_idx": 32780,
          "end_idx": 33557
        },
        "page_content": "In this study, a new labeling strategy is introduced to partition texts into those with pure and mixed sentiment orientations. These two categories of texts are labeled using different processes. A two-level network is accordingly proposed to utilize the two labeled data in our two-stage labeling strategy. Lexical cues (e.g., polar words, POS, conjunction words) are particularly useful in sentiment analysis. These lexical cues are used in our two-level network, and a new encoding strategy, that is, INLINEFORM0 -hot encoding, is introduced. INLINEFORM1 -hot encoding is motivated by one-hot encoding. However, the former alleviates the drawbacks of the latter. Three Chinese sentiment text data corpora are compiled to verify the effectiveness of the proposed methodology.",
        "type": "Document"
      },
      {
        "id": "f8cbf74f-b0a0-46be-b84d-79c9219c38a1",
        "metadata": {
          "vector_store_key": "2001.02380-9",
          "chunk_id": 79,
          "document_id": "2001.02380",
          "start_idx": 42002,
          "end_idx": 42662
        },
        "page_content": "Original: <$s$>$ To\\quad \\ p\u0304rovide \u012bnformation .. <$sep$>$ .. <$n$>$ Original: \\: $<$s$>$ \\: \\ To \\: provide \\: information \\: ... \\: $<$sep$>$ \\: ... \\: $<$n$>$ \\\\ Masked1: \\: $<$s$>$ \\: $<$X$>$ \\: provide \\: information \\: ... \\: $<$sep$>$ \\: ... \\: $<$n$>$ \\\\ Masked2: \\: $<$s$>$ \\: \\ To \\: \\ $<$X$>$ \\: information \\: ... \\: $<$sep$>$ \\: ... \\: $<$n$>$ \\\\ Masked3: \\: $<$s$>$ \\: \\ To \\: provide \\: \\ $<$X$>$ \\: ... \\: $<$sep$>$ \\: ... \\: $<$n$>$ $ Label: purpose We reason that, if a token is important for predicting the correct label, masking it will degrade the model's classification accuracy, or at least reduce its reported classification certainty.",
        "type": "Document"
      },
      {
        "id": "87845fa8-b3b4-4927-91c6-fda0a86db267",
        "metadata": {
          "vector_store_key": "1803.07771-8",
          "chunk_id": 39,
          "document_id": "1803.07771",
          "start_idx": 23123,
          "end_idx": 23708
        },
        "page_content": "In the second stage, five users are invited to label each text sample in the three raw data sets. The average score of the five users on each sample is calculated. Samples with average scores located in [0.6, 1] are labeled as \u201cpositive\". Samples with average scores located in [0, 0.4] are labeled as \u201cnegative\". Others are labeled as \u201cneutral\". The details of the labeling results are shown in Table 1. All the training and test data and the labels are available online. In our experiments, the five types of key lexical words introduced in Subsection 3.3.2 are manually constructed.",
        "type": "Document"
      },
      {
        "id": "87226686-6688-41b0-887a-9110ad49e395",
        "metadata": {
          "vector_store_key": "1803.07771-8",
          "chunk_id": 12,
          "document_id": "1803.07771",
          "start_idx": 7720,
          "end_idx": 8439
        },
        "page_content": "If a user is concerned about service, then the label of S1 may be \u201cnegative\". By contrast, for another user who does not care about service, the label may be \u201cpositive\". Similarly, a user may label S2 as \u201cpositive\" if he cares about quality. Another user may label it as \u201cnegative\" if the conjunction \u201cbut\" attracts the user\u00a1\u00afs attention more. Another user may label it as \u201cneutral\" if they are concerned about quality and appearance. The underlying reason is that sentiment is more subjective than semantics. In related research on subjective categorization, such as visual aesthetics, each sample is usually repeatedly annotated by multiple annotators, and the average label is taken as the final label of the sample.",
        "type": "Document"
      },
      {
        "id": "96b6d7d0-95ab-4b53-b472-c494d087b9fc",
        "metadata": {
          "vector_store_key": "1909.13362-2",
          "chunk_id": 10,
          "document_id": "1909.13362",
          "start_idx": 5932,
          "end_idx": 6678
        },
        "page_content": "We show that this novel approach to syllabification is language-agnostic by evaluating it on datasets of six languages, five from two major language families, and one that appears to be unrelated to any existing language. Syllabification can be considered a sequence labeling task where each label delineates the existence or absence of a syllable boundary. As such, syllabification has much in common with well-researched topics such as part-of-speech tagging, named-entity recognition, and chunking BIBREF17. Neural networks have recently outpaced more traditional methods in sequence labeling tasks. These neural-based approaches are taking the place of HMMs, maximum entropy Markov models (MEMM), and conditional random fields (CRF) BIBREF18.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are their changes evaluated?",
    "llm_answer": "The context mentions evaluating changes in prediction scores if an observed fact is absent during embedding learning, but doesn't describe a specific evaluation method for other modifications.\n",
    "context": [
      {
        "id": "e244cdf7-f116-4211-a8c7-4a880bb1f702",
        "metadata": {
          "vector_store_key": "1905.00563-6",
          "chunk_id": 12,
          "document_id": "1905.00563",
          "start_idx": 6344,
          "end_idx": 7053
        },
        "page_content": "We analyze other forms of modifications such as $\\langle s, r^{\\prime }, o^{\\prime }\\rangle $ and $\\langle s, r^{\\prime }, o\\rangle $ in appendices \"Modifications of the Form \u2329s,r ' ,o ' \u232a\\langle s, r^{\\prime }, o^{\\prime } \\rangle \" and \"Modifications of the Form \u2329s,r ' ,o\u232a\\langle s, r^{\\prime }, o \\rangle \" , and leave empirical evaluation of these modifications for future work. For explaining a target prediction, we are interested in identifying the observed fact that has the most influence (according to the model) on the prediction. We define influence of an observed fact on the prediction as the change in the prediction score if the observed fact was not present when the embeddings were learned.",
        "type": "Document"
      },
      {
        "id": "6d7e1fda-02df-491f-93a1-de0c2f84c4b0",
        "metadata": {
          "vector_store_key": "1709.01256-4",
          "chunk_id": 36,
          "document_id": "1709.01256",
          "start_idx": 19769,
          "end_idx": 20455
        },
        "page_content": "The generation process of the simulated data sets is designed to mimic the real world. Users open some existing documents in a file system, make some changes (e.g. addition, deletion or replacement), and save them as separate documents. These documents become revisions of the original documents. We started from an initial corpus that did not have revisions, and kept adding new documents and revising existing documents. Similar to a file system, at any moment new documents could be added and/or some of the current documents could be revised. The revision operations we used were deletion, addition and replacement of words, sentences, paragraphs, section names and document titles.",
        "type": "Document"
      },
      {
        "id": "989a119e-c2a1-4df8-8222-282290b958b5",
        "metadata": {
          "vector_store_key": "1611.02550-1",
          "chunk_id": 14,
          "document_id": "1611.02550",
          "start_idx": 7600,
          "end_idx": 8396
        },
        "page_content": "The updates are modulated by the values of several gating vectors, which control the degree to which the cell memory and hidden state are updated in light of new information in the current frame. For a single-layer LSTM network, the updates are as follows:  INLINEFORM0  where INLINEFORM0 , and INLINEFORM1 are all vectors of the same dimensionality, INLINEFORM2 , and INLINEFORM3 are learned weight matrices of the appropriate sizes, INLINEFORM4 and INLINEFORM5 are learned bias vectors, INLINEFORM6 is a componentwise logistic activation, and INLINEFORM7 refers to the Hadamard (componentwise) product. Similarly, in a GRU network, at each time step a GRU cell determines what components of old information are retained, overwritten, or modified in light of the next step in the input sequence.",
        "type": "Document"
      },
      {
        "id": "6fbf28cc-b593-44d8-a382-1b3b32ae482c",
        "metadata": {
          "vector_store_key": "1810.06743-0",
          "chunk_id": 26,
          "document_id": "1810.06743",
          "start_idx": 15228,
          "end_idx": 15949
        },
        "page_content": "Our post-editing procedure operates on the initial MSD hypothesis as follows: We evaluate our tool on two tasks: To be clear, our scope is limited to the schema conversion. Future work will explore NLP tasks that exploit both the created token-level UniMorph data and the existing type-level UniMorph data. We transform all UD data to the UniMorph. We compare the simple lookup-based transformation to the one with linguistically informed post-edits on all languages with both UD and UniMorph data. We then evaluate the recall of MSDs without partial credit. Because the UniMorph tables only possess annotations for verbs, nouns, adjectives, or some combination, we can only examine performance for these parts of speech.",
        "type": "Document"
      },
      {
        "id": "098d3883-1a90-40ca-93f0-c2145a6ba86d",
        "metadata": {
          "vector_store_key": "1709.01256-4",
          "chunk_id": 37,
          "document_id": "1709.01256",
          "start_idx": 20230,
          "end_idx": 20976
        },
        "page_content": "The revision operations we used were deletion, addition and replacement of words, sentences, paragraphs, section names and document titles. The addition of words, ..., section names, and new documents were pulled from the Wikipedia abstracts. This corpus generation process had five time periods INLINEFORM0 . Figure FIGREF42 illustrates this simulation. We set a Poisson distribution with rate INLINEFORM1 (the number of documents in the initial corpus) to control the number of new documents added in each time period, and a Poisson distribution with rate INLINEFORM2 to control the number of documents revised in each time period. We generated six data sets using different random seeds, and each data set contained six corpora (Corpus 0 - 5).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the six target languages?",
    "llm_answer": "English, Spanish, French, German, Russian, and Italian.\n",
    "context": [
      {
        "id": "50580b33-3f37-4708-bb48-721919a2f597",
        "metadata": {
          "vector_store_key": "1910.05456-7",
          "chunk_id": 18,
          "document_id": "1910.05456",
          "start_idx": 9647,
          "end_idx": 10340
        },
        "page_content": "Spanish (SPA), in contrast, is morphologically rich, and disposes of much larger verbal paradigms than English. Like English, it is a suffixing language, and it additionally makes use of internal stem changes (e.g., o $\\rightarrow $ ue). Since English and Spanish are both Indo-European languages, and, thus, relatively similar, we further add a third, unrelated target language. We choose Zulu (ZUL), a Bantoid language. In contrast to the first two, it is strongly prefixing. For pretraining, we choose languages with different degrees of relatedness and varying morphological similarity to English, Spanish, and Zulu. We limit our experiments to languages which are written in Latin script.",
        "type": "Document"
      },
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      },
      {
        "id": "71ab2f60-065f-4932-8b5f-6103425e923c",
        "metadata": {
          "vector_store_key": "1911.03562-4",
          "chunk_id": 32,
          "document_id": "1911.03562",
          "start_idx": 17312,
          "end_idx": 18029
        },
        "page_content": "These include: Chinese, Arabic, Korean, Japanese, and Hindi (Asian) as well as French, German, Swedish, Spanish, Portuguese, and Italian (European). This is followed by the relatively less widely spoken European languages (such as Russian, Polish, Norwegian, Romanian, Dutch, and Czech) and Asian languages (such as Turkish, Thai, and Urdu). Most of the well-represented languages are from the Indo-European language family. Yet, even in the limited landscape of the most common 122 languages, vast swathes are barren with inattention. Notable among these is the extremely low representation of languages from Africa, languages from non-Indo-European language families, and Indigenous languages from around the world.",
        "type": "Document"
      },
      {
        "id": "52730fa9-4bf0-4d20-9167-b503ca6945eb",
        "metadata": {
          "vector_store_key": "1909.12642-2",
          "chunk_id": 9,
          "document_id": "1909.12642",
          "start_idx": 4957,
          "end_idx": 5647
        },
        "page_content": "Each of the three languages had this subtask. Sub-task B consists of building a multi-class classification model which can predict the three different classes in the data points annotated as HOF: Hate speech (HATE), Offensive language (OFFN), and Profane (PRFN). Again all three languages have this sub-task. Sub-task C consists of building a binary classification model which can predict the type of offense: Targeted (TIN) and Untargeted (UNT). Sub-task C was not conducted for the German dataset. In this section, we will explain the details about our system, which comprises of two sub-parts- feature generation and model selection. Figure FIGREF15 shows the architecture of our system.",
        "type": "Document"
      },
      {
        "id": "9210eab3-d084-4395-859a-51482ae8da57",
        "metadata": {
          "vector_store_key": "1910.04269-1",
          "chunk_id": 34,
          "document_id": "1910.04269",
          "start_idx": 19227,
          "end_idx": 19993
        },
        "page_content": "2D attention models focused on the important features extracted by convolutional layers and bi-directional GRU captured the temporal features. Several of the spoken languages in Europe belong to the Indo-European family. Within this family, the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Katze in German and Ciao in Italian becomes Chao in Spanish.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the size of the released dataset?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "8be62763-27f4-46ff-b2bb-edb6a6b2fa04",
        "metadata": {
          "vector_store_key": "1607.06025-3",
          "chunk_id": 42,
          "document_id": "1607.06025",
          "start_idx": 24463,
          "end_idx": 25198
        },
        "page_content": "The new dataset consists of training set, which is generated using examples from the original training set, and a development set, which is generated from the original development set. The beam size for beam search was set to 1. The details of the decision are presented in Section SECREF35 . Some datasets were constructed by filtering the generated datasets according to various thresholds. Thus, the generated datasets were constructed to contain enough examples, so that the filtered datasets had at least the number of examples as the original dataset. In the end, all the datasets were trimmed down to the size of the original dataset by selecting the samples sequentially from the beginning until the dataset had the right size.",
        "type": "Document"
      },
      {
        "id": "d87d327c-2858-4b5d-ae8a-860d401f63bd",
        "metadata": {
          "vector_store_key": "1908.05434-1",
          "chunk_id": 27,
          "document_id": "1908.05434",
          "start_idx": 14432,
          "end_idx": 15064
        },
        "page_content": "We have ensured that the raw dataset has no overlap with the labeled dataset to avoid bias in test accuracy. While it is possible to scrape more raw data, we did not observe significant improvements in model performances when the size of raw data increased from INLINEFORM0 70,000 to INLINEFORM1 170,000, hence we assume that the current raw dataset is sufficiently large. The labeled dataset is called Trafficking-10k. It consists of 12,350 ads from Backpage labeled by experts in human trafficking detection BIBREF9 . Each label is one of seven ordered levels of likelihood that the corresponding ad comes from a human trafficker.",
        "type": "Document"
      },
      {
        "id": "9ca5340d-587b-4e94-bcbe-160570788673",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 35,
          "document_id": "1910.12618",
          "start_idx": 19294,
          "end_idx": 19871
        },
        "page_content": "For instance for random forests it can correspond to the maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014 and 2015 serve as validation set (730 days).",
        "type": "Document"
      },
      {
        "id": "73a83f60-b2c9-406d-b4cd-5720639e5f06",
        "metadata": {
          "vector_store_key": "1812.07023-4",
          "chunk_id": 18,
          "document_id": "1812.07023",
          "start_idx": 10358,
          "end_idx": 10986
        },
        "page_content": "After 10 such qa turns, the questioner wraps up by writing a summary of the video based on the conversation. Dataset statistics such as the number of dialogues, turns, and words for the avsd dataset are presented in Table TABREF5 . For the initially released prototype dataset, the training set of the avsd dataset corresponds to videos taken from the training set of the Charades dataset while the validation and test sets of the avsd dataset correspond to videos taken from the validation set of the Charades dataset. For the official dataset, training, validation and test sets are drawn from the corresponding Charades sets.",
        "type": "Document"
      },
      {
        "id": "b7e2d393-7a9c-415c-8ed4-e25103a1870e",
        "metadata": {
          "vector_store_key": "1709.08299-2",
          "chunk_id": 23,
          "document_id": "1709.08299",
          "start_idx": 13308,
          "end_idx": 13983
        },
        "page_content": "We hope the release of the full dataset (including hidden test set) could help the participants have a better knowledge of their systems and encourage more researchers to do experiments on. We would like to thank the anonymous reviewers for their thorough reviewing and providing thoughtful comments to improve our paper. We thank the Sixteenth China National Conference on Computational Linguistics (CCL 2017) and Nanjing Normal University for providing space for evaluation workshop. Also we want to thank our resource team for annotating and verifying evaluation data. This work was supported by the National 863 Leading Technology Research Project via grant 2015AA015409.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which OpenIE systems were used?",
    "llm_answer": "OpenIE-4, OpenIE 5, MiniIE.\n",
    "context": [
      {
        "id": "1a1bc848-ffb7-4acc-8c05-d2a88a00a74e",
        "metadata": {
          "vector_store_key": "1802.05574-1",
          "chunk_id": 29,
          "document_id": "1802.05574",
          "start_idx": 16414,
          "end_idx": 17144
        },
        "page_content": "Focusing on scientific and medical text (SCI), again where the triples are majority annotated as being correct, Open IE has higher inter-annotator agreement (Open IE: 0.83 - SD 0.24 vs MiniIE: 0.76 - SD 0.25). In both cases, the difference is significant with p-values $<$ 0.01 using Welch's t-test. This leads us to conclude that Open IE produces triples that annotators are more likely to agree as being correct. MinIE provides many more correct extractions than OpenIE 4 (935 more across both datasets). The true recall numbers of the two systems can not be calculated with the data available, but the 40% difference in the numbers of correct extractions is strong evidence that the two systems do not have equivalent behavior.",
        "type": "Document"
      },
      {
        "id": "00e7d051-781c-4892-adbb-8a912e39c442",
        "metadata": {
          "vector_store_key": "1802.05574-2",
          "chunk_id": 9,
          "document_id": "1802.05574",
          "start_idx": 5464,
          "end_idx": 6138
        },
        "page_content": "In our experience using OIE on scientific text, we have found that these systems often produce overly specific extractions that do not provide the redundancy useful for downstream tasks. Hence, we thought this was a useful package to explore. We note that both OpenIE 4 and MiniIE support relation extractions that go beyond binary tuples, supporting the extraction of n-ary relations. We note that the most recent version of Open IE (version 5) is focused on n-ary relations. For ease of judgement, we focused on binary relations. Additionally, both systems support the detection of negative relations. In terms of settings, we used the off the shelf settings for OpenIE 4.",
        "type": "Document"
      },
      {
        "id": "4eb7e215-4922-460f-9076-a470af2ad147",
        "metadata": {
          "vector_store_key": "1905.07471-0",
          "chunk_id": 0,
          "document_id": "1905.07471",
          "start_idx": 0,
          "end_idx": 730
        },
        "page_content": "Open Information Extraction (OpenIE) is the NLP task of generating (subject, relation, object) tuples from unstructured text e.g. \u201cFed chair Powell indicates rate hike\u201d outputs (Powell, indicates, rate hike). The modifier open is used to contrast IE research in which the relation belongs to a fixed set. OpenIE has been shown to be useful for several downstream applications such as knowledge base construction BIBREF0 , textual entailment BIBREF1 , and other natural language understanding tasks BIBREF2 . In our previous example an extraction was missing: (Powell, works for, Fed). Implicit extractions are our term for this type of tuple where the relation (\u201cworks for\u201d in this example) is not contained in the input sentence.",
        "type": "Document"
      },
      {
        "id": "794236e6-27b2-4b53-b979-a3767e14b0da",
        "metadata": {
          "vector_store_key": "1905.07471-3",
          "chunk_id": 6,
          "document_id": "1905.07471",
          "start_idx": 3302,
          "end_idx": 3951
        },
        "page_content": "In this work we compare to OpenIE-4 , ClausIE BIBREF8 , ReVerb BIBREF9 , OLLIE BIBREF10 , Stanford OpenIE BIBREF11 , and PropS BIBREF12 . Stanovsky et al. SupervisedOIE frame OpenIE as a BIO-tagging problem and train an LSTM to tag an input sentence. Tuples can be derived from the tagger, input, and BIO CFG parser. This method outperforms traditional systems, though the tagging scheme inherently constrains the relations to be part of the input sentence, prohibiting implicit relation extraction. Cui et al. NeuralOpenIE bootstrap (sentence, tuple) pairs from OpenIE-4 and train a standard seq2seq with attention model using OpenNMT-py BIBREF13 .",
        "type": "Document"
      },
      {
        "id": "db0c5ade-dca8-4e4e-b54d-69b4dc70d28c",
        "metadata": {
          "vector_store_key": "1802.05574-2",
          "chunk_id": 2,
          "document_id": "1802.05574",
          "start_idx": 1275,
          "end_idx": 1891
        },
        "page_content": "Indeed, as BIBREF8 noted, there is little work on the evaluation of OIE systems. Thus, the goal of this paper is to evaluate the performance of the state of the art in OIE systems on scientific text. Specifically, we aim to test two hypotheses: Additionally, we seek to gain insight into the value of unsupervised approaches to information extraction and also provide information useful to implementors of these systems. We note that our evaluation differs from existing OIE evaluations in that we use crowd-sourcing annotations instead of expert annotators. This allows for a larger number of annotators to be used.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "how are the bidirectional lms obtained?",
    "llm_answer": "A bidirectional LSTM (BiLSTM) is formed by concatenating a forward LSTM and a backward LSTM.\n",
    "context": [
      {
        "id": "87576d10-9bde-4c22-be00-04e4b896cad6",
        "metadata": {
          "vector_store_key": "1812.06705-3",
          "chunk_id": 14,
          "document_id": "1812.06705",
          "start_idx": 8549,
          "end_idx": 9356
        },
        "page_content": "In general, the language model(LM) models the probability of generating natural language sentences or documents. Given a sequence $\\textbf {\\textit {S}}$ of N tokens, $<t_1,t_2,...,t_N>$ , a forward language model allows us to predict the probability of the sequence as:  $$p(t_1,t_2,...,t_N) = \\prod _{i=1}^{N}p(t_i|t_1, t_2,..., t_{i-1}).$$   (Eq. 8)  Similarly, a backward language model allows us to predict the probability of the sentence as:  $$p(t_1,t_2,...,t_N) = \\prod _{i=1}^{N}p(t_i|t_{i+1}, t_{i+2},..., t_N).$$   (Eq. 9)  Traditionally, a bidirectional language model a shallow concatenation of independently trained forward and backward LMs. In order to train a deep bidirectional language model, BERT proposed Masked Language Model (MLM) task, which was also referred to Cloze Task BIBREF23 .",
        "type": "Document"
      },
      {
        "id": "95817dea-ccd5-4fcb-999e-ba04bd2a2001",
        "metadata": {
          "vector_store_key": "2001.02380-6",
          "chunk_id": 62,
          "document_id": "2001.02380",
          "start_idx": 33150,
          "end_idx": 34177
        },
        "page_content": "Formally, the input to our system is formed of EDU pairs which are the head units within the respective blocks of discourse units that they belong to, which are in turn connected by an instance of a discourse relation. This means that every discourse relation in the corpus is expressed as exactly one EDU pair. Each EDU is encoded as a (possibly padded) sequence of $n$-dimensional vector representations of each word ${x_1,..,x_T}$, with some added separators which are encoded in the same way and described below. The bidirectional LSTM composes representations and context for the input, and a fully connected softmax layer gives the probability of each relation: where the probability of each relation $rel_i$ is derived from the composed output of the function $h$ across time steps $0 \\ldots t$, $\\delta \\in \\lbrace b,f\\rbrace $ is the direction of the respective LSTMs, $c_t^\\delta $ is the recurrent context in each direction and $\\theta = {W,b}$ gives the model weights and bias parameters (see BIBREF46 for details).",
        "type": "Document"
      },
      {
        "id": "bf3e2739-4d8c-4d09-b5e5-ec00c971e957",
        "metadata": {
          "vector_store_key": "2001.02380-6",
          "chunk_id": 63,
          "document_id": "2001.02380",
          "start_idx": 34177,
          "end_idx": 35021
        },
        "page_content": "The bidirectional LSTM composes representations and context for the input, and a fully connected softmax layer gives the probability of each relation: where the probability of each relation $rel_i$ is derived from the composed output of the function $h$ across time steps $0 \\ldots t$, $\\delta \\in \\lbrace b,f\\rbrace $ is the direction of the respective LSTMs, $c_t^\\delta $ is the recurrent context in each direction and $\\theta = {W,b}$ gives the model weights and bias parameters (see BIBREF46 for details). Note that although the output of the system is ostensibly a probability distribution over relation types, we will not be directly interested in the most probable relation as outputted by the classifier, but rather in analyzing the model's behavior with respect to the input word representations as potential signals of each relation.",
        "type": "Document"
      },
      {
        "id": "6a3014da-1fc1-4c61-82c4-14b1938ca338",
        "metadata": {
          "vector_store_key": "2003.05377-4",
          "chunk_id": 13,
          "document_id": "2003.05377",
          "start_idx": 7559,
          "end_idx": 8447
        },
        "page_content": "The following equations are used to update $C_t$ and $h_t$ values. where $W_f$, $W_i$, $W_C$, $W_o$ are the weight matrices for $h_{t-1}$ input; $U_f$, $U_i$, $U_C$, $U_o$ are the weight matrices for $x_t$ input; and $b_f$, $b_i$, $b_C$, $b_o$ are the bias vectors. Basically, a Bidirectional LSTM network consists of using two LSTM networks: a forward LSTM and a backward LSTM. The intuition behind it is that, in some types of problems, past and future information captured by forward and backward LSTM layers are useful to predict the current data. Our proposed approach consists of three main steps. Firstly, we concatenate the title of the song with its lyrics, put all words in lower case and then we clean up the text by removing line breaks, multiple spaces, and some punctuation (,!.?). Secondly, we represent the text as a vector provided by a pre-trained word embeddings model.",
        "type": "Document"
      },
      {
        "id": "d26c9689-e90c-46a8-8a81-1dcebe54d910",
        "metadata": {
          "vector_store_key": "1909.13362-3",
          "chunk_id": 16,
          "document_id": "1909.13362",
          "start_idx": 9407,
          "end_idx": 10095
        },
        "page_content": "This can be a limitation when a time step has both past dependency and future dependency. For example, a consonant sound may be the coda of a syllable earlier in the sequence or the onset of a syllable later in the sequence. Thus, processing a phonetic sequence in both the forward and backwards directions provides an improved context for assigning syllable boundaries. A bidirectional LSTM (BiLSTM) is formed when an LSTM moving forward through time is concatenated with an LSTM moving backward through time BIBREF23. We use the LSTM network as follows. The $x$ vector is fed through the LSTM network which outputs a vector $\\overrightarrow{h_i}$ for each time step $i$ from 0 to $n-1$.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what metrics are used in evaluation?",
    "llm_answer": "ROUGE, BLEU, METEOR, F<sub>1</sub> score, Pyramid,  average rank, percentage of best-ranked questions, number of turns, ratio of task completion.\n",
    "context": [
      {
        "id": "61e54692-2da3-41fb-b1b0-c3469ffa76bf",
        "metadata": {
          "vector_store_key": "1607.06025-0",
          "chunk_id": 17,
          "document_id": "1607.06025",
          "start_idx": 10067,
          "end_idx": 10777
        },
        "page_content": "These attributes are conditional information that is fed to the models, like the discrete label is in our models. As recognized by BIBREF37 , the evaluation metrics of text-generating models fall into three categories: manual evaluation, automatic evaluation metrics, task-based evaluation. In evaluation based on human judgment each generated textual example is inspected manually. The automatic evaluation metrics, like ROUGE, BLEU and METEOR, compare human texts and generated texts. BIBREF38 shows METEOR has the strongest correlation with human judgments in image description evaluation. The last category is task-based evaluation, where the impact of the generated texts on a particular task is measured.",
        "type": "Document"
      },
      {
        "id": "8c81c383-8e40-4094-8e18-ab0a2d32126a",
        "metadata": {
          "vector_store_key": "1905.08949-6",
          "chunk_id": 18,
          "document_id": "1905.08949",
          "start_idx": 10340,
          "end_idx": 11074
        },
        "page_content": "Although the datasets are commonly shared between QG and QA, it is not the case for evaluation: it is challenging to define a gold standard of proper questions to ask. Meaningful, syntactically correct, semantically sound and natural are all useful criteria, yet they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks. As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE BIBREF43 , are also widely used.",
        "type": "Document"
      },
      {
        "id": "74a8f350-1cdc-4b68-87aa-e719da16ac2c",
        "metadata": {
          "vector_store_key": "1709.10217-3",
          "chunk_id": 4,
          "document_id": "1709.10217",
          "start_idx": 2049,
          "end_idx": 2735
        },
        "page_content": "For the task-oriented system, although there are some objective evaluation metrics, such as the number of turns in a dialogue, the ratio of task completion, etc., there is no gold standard for automatically evaluating two (or more) dialogue systems when considering the satisfaction of the human and the fluency of the generated dialogue. To promote the development of the evaluation technology for dialogue systems, especially considering the language characteristics of Chinese, we organize the first evaluation of Chinese human-computer dialogue technology. In this paper, we will present the evaluation scheme and the released corpus in detail. The rest of this paper is as follows.",
        "type": "Document"
      },
      {
        "id": "f221ac3d-8ea6-49e8-8ce8-1528ccd79f99",
        "metadata": {
          "vector_store_key": "1909.00578-2",
          "chunk_id": 2,
          "document_id": "1909.00578",
          "start_idx": 1185,
          "end_idx": 1928
        },
        "page_content": "Sum-QE achieves very high correlations with human ratings, showing the ability of BERT to model linguistic qualities that relate to both text content and form. Summarization evaluation metrics like Pyramid BIBREF5 and ROUGE BIBREF3, BIBREF2 are recall-oriented; they basically measure the content from a model (reference) summary that is preserved in peer (system generated) summaries. Pyramid requires substantial human effort, even in its more recent versions that involve the use of word embeddings BIBREF8 and a lightweight crowdsourcing scheme BIBREF9. ROUGE is the most commonly used evaluation metric BIBREF10, BIBREF11, BIBREF12. Inspired by BLEU BIBREF4, it relies on common $n$-grams or subsequences between peer and model summaries.",
        "type": "Document"
      },
      {
        "id": "6a443055-4b4a-46e1-a2dc-b3211ac62d38",
        "metadata": {
          "vector_store_key": "1908.10449-0",
          "chunk_id": 20,
          "document_id": "1908.10449",
          "start_idx": 10741,
          "end_idx": 11449
        },
        "page_content": "Since iMRC involves both MRC and RL, we adopt evaluation metrics from both settings. First, as a question answering task, we use $\\text{F}_1$ score to compare predicted answers against ground-truth, as in previous works. When there exist multiple ground-truth answers, we report the max $\\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we evaluate an agent's performance during both its training and testing phases. During training, we report training curves averaged over 3 random seeds. During test, we follow common practice in supervised learning tasks where we report the agent's test performance corresponding to its best validation performance .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what results do they achieve?",
    "llm_answer": "The provided text describes experiments, but does not state specific achieved results.  It mentions that classification approaches outperformed regression approaches, and that the \"first $n$\" baseline produced the same results as the neural regressor, but does not give numerical results.\n",
    "context": [
      {
        "id": "3563cf3d-eec8-415a-8c0f-6c546c874f63",
        "metadata": {
          "vector_store_key": "1803.02839-0",
          "chunk_id": 2,
          "document_id": "1803.02839",
          "start_idx": 1275,
          "end_idx": 1931
        },
        "page_content": "RNNs nominally aim to solve a general problem involving sequential inputs. For various more specified tasks, specialized and constrained implementations tend to perform better BIBREF12 , BIBREF13 , BIBREF14 , BIBREF7 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF10 , BIBREF11 , BIBREF8 , BIBREF9 . Often, the improvement simply mitigates the exploding/vanishing gradient problem BIBREF18 , BIBREF19 , but, for many tasks, the improvement is more capable of generalizing the network's training for that task. Understanding better how and why certain networks excel at certain NLP tasks can lead to more performant networks, and networks that solve new problems.",
        "type": "Document"
      },
      {
        "id": "ef18c881-3bf1-4e36-94ad-61037dd58977",
        "metadata": {
          "vector_store_key": "1706.07179-0",
          "chunk_id": 14,
          "document_id": "1706.07179",
          "start_idx": 7436,
          "end_idx": 7670
        },
        "page_content": "Future work will investigate the performance of these models on more real world datasets, interpreting what the models learn, and scaling these models to answer questions about entities and relations from reading massive text corpora.",
        "type": "Document"
      },
      {
        "id": "beee7b9e-00bc-4ef2-8232-722a694c8ed8",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 28,
          "document_id": "1909.00542",
          "start_idx": 15261,
          "end_idx": 15995
        },
        "page_content": "Therefore we do not comment on the results, other than observing that the \u201cfirst $n$\u201d baseline produces the same results as the neural regressor. As mentioned in Section SECREF3, the labels used for the classification experiments are the 5 sentences with highest ROUGE-SU4 F1 score. Macquarie University's participation in BioASQ 7 focused on the task of generating the ideal answers. The runs use query-based extractive techniques and we experiment with classification, regression, and reinforcement learning approaches. At the time of writing there were no human evaluation results, and based on ROUGE-F1 scores under cross-validation on the training data we observed that classification approaches outperform regression approaches.",
        "type": "Document"
      },
      {
        "id": "0d762234-52a5-4d16-bff7-4bc4f6021322",
        "metadata": {
          "vector_store_key": "1603.04513-7",
          "chunk_id": 48,
          "document_id": "1603.04513",
          "start_idx": 26860,
          "end_idx": 27553
        },
        "page_content": "All results in the five blocks are with respect to row 34, \u201cMVCNN (overall)\u201d; e.g., row 19 shows what happens when HLBL is removed from row 34, row 28 shows what happens when mutual learning is removed from row 34 etc. The block \u201cbaselines\u201d (1\u201318) lists some systems representative of previous work on the corresponding datasets, including the state-of-the-art systems (marked as italic). The block \u201cversions\u201d (19\u201323) shows the results of our system when one of the embedding versions was not used during training. We want to explore to what extend different embedding versions contribute to performance. The block \u201cfilters\u201d (24\u201327) gives the results when individual filter width is discarded.",
        "type": "Document"
      },
      {
        "id": "4c32b91e-41de-4214-8907-1d3f59805d18",
        "metadata": {
          "vector_store_key": "1801.09030-4",
          "chunk_id": 38,
          "document_id": "1801.09030",
          "start_idx": 19834,
          "end_idx": 20571
        },
        "page_content": "Both of the professors enjoy over five years of practicing traditional Chinese medical treatment. The evaluators are asked to evaluate the prescriptions with scores between 0 and 10. Both the textual symptoms and the standard reference are given, which is similar to the form of evaluation in a normal TCM examination. Different from the automatic evaluation method, the human evaluators focus on the potential curative effect of the candidate answers, rather than merely the literal similarity. We believe this way of evaluation is much more reasonable and close to reality. Because the evaluation procedure is very time consuming (each item requires more than 1 minute), we only ask the evaluators to judge the results from test set 2.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what previous systems were compared to?",
    "llm_answer": "IMS, Latent-Tree, Berkeley, LaSO, Latent-Strc, Model-Stack, Non-Linear, and Human.\n",
    "context": [
      {
        "id": "80911f24-68f0-45cb-a65a-c54370ab3292",
        "metadata": {
          "vector_store_key": "1707.03764-3",
          "chunk_id": 11,
          "document_id": "1707.03764",
          "start_idx": 5958,
          "end_idx": 6709
        },
        "page_content": "This also shows that distinguishing words include both time-specific ones, like \u201cgilmore\u201d and \u201cimacelebrityau\u201d, and general words from everyday life, which are less likely to be subject to time-specific trends, like \u201cplayer\u201d, and \u201cchocolate\u201d. This section is meant to highlight all of the potential contributions to the systems which turned out to be detrimental to performance, when compared to the simpler system that we have described in Section SECREF2 . We divide our attempts according to the different ways we attempted to enhance performance: manipulating the data itself (adding more, and changing preprocessing), using a large variety of features, and changing strategies in modelling the problem by using different algorithms and paradigms.",
        "type": "Document"
      },
      {
        "id": "904a361a-2d3c-41fa-8807-2303b37ca27b",
        "metadata": {
          "vector_store_key": "1603.04553-3",
          "chunk_id": 21,
          "document_id": "1603.04553",
          "start_idx": 11450,
          "end_idx": 12153
        },
        "page_content": "To make a thorough empirical comparison with previous studies, Table 3 (below the dashed line) also shows the results of some state-of-the-art supervised coreference resolution systems \u2014 IMS: the second best system in the CoNLL 2012 shared task BIBREF28 ; Latent-Tree: the latent tree model BIBREF29 obtaining the best results in the shared task; Berkeley: the Berkeley system with the final feature set BIBREF12 ; LaSO: the structured perceptron system with non-local features BIBREF30 ; Latent-Strc: the latent structure system BIBREF31 ; Model-Stack: the entity-centric system with model stacking BIBREF32 ; and Non-Linear: the non-linear mention-ranking model with feature representations BIBREF33 .",
        "type": "Document"
      },
      {
        "id": "7e3794a8-4c49-4ef6-b670-6e8d7e4dae71",
        "metadata": {
          "vector_store_key": "1905.06566-4",
          "chunk_id": 45,
          "document_id": "1905.06566",
          "start_idx": 25128,
          "end_idx": 25731
        },
        "page_content": "We asked the subjects to rank the outputs of these systems from best to worst. As shown in Table 4 , the output of $\\text{\\sc Hibert}_M$ is selected as the best in 30% of cases and we obtained lower mean rank than all systems except for Human. We also converted the rank numbers into ratings (rank $i$ to $7-i$ ) and applied student $t$ -test on the ratings. $\\text{\\sc Hibert}_M$ is significantly different from all systems in comparison ( $p < 0.05$ ), which indicates our model still lags behind Human, but is better than all other systems. As mentioned earlier, our pre-training includes two stages.",
        "type": "Document"
      },
      {
        "id": "51694777-d4a8-499c-a3f9-28691252fe65",
        "metadata": {
          "vector_store_key": "1902.10525-1",
          "chunk_id": 57,
          "document_id": "1902.10525",
          "start_idx": 30215,
          "end_idx": 30871
        },
        "page_content": "The first system has an explicit segmentation component along with a feed-forward network for recognizing character hypotheses, similar in formulation to our previous system BIBREF14 . In addition, they also make use of a bidirectional LSTM system trained end-to-end with the CTC loss. They do not provide additional details on which system is which. We note that the modeling stacks of the systems out-performing ours in this competition are not fundamentally different (to the best of our knowledge, according to released descriptions). We therefore believe that our system might perform comparably if trained on the competition training dataset as well.",
        "type": "Document"
      },
      {
        "id": "d2e2382c-8e5a-4f0c-92c5-2f119e6e1178",
        "metadata": {
          "vector_store_key": "1910.07481-0",
          "chunk_id": 20,
          "document_id": "1910.07481",
          "start_idx": 11787,
          "end_idx": 12518
        },
        "page_content": "It also surpassed the Baselinee by 0.18 points on the newstest2017 with strong statistical significance, and by 0.15 BLEU points on the newstest2018 but this time with no statistical evidence. These encouraging results prompted us to extend experiments to another language pair: English-French. En$\\rightarrow $Fr: The Document system obtained the best results considering all metrics on all test sets with strong statistical evidence. It surpassed the Baseline by 1.09 BLEU points and 0.85 TER points on tst2015, 0.75 BLEU points and 0.76 TER points on tst2014, and 0.48 BLEU points and 0.68 TER points on tst2013. Fr$\\rightarrow $En: Of all experiments, this language pair shows the most important improvements over the Baseline.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Are this models usually semi/supervised or unsupervised?",
    "llm_answer": "Semi-supervised.\n",
    "context": [
      {
        "id": "759081e2-9e3d-4b21-adda-fbc158eefa68",
        "metadata": {
          "vector_store_key": "1710.01492-5",
          "chunk_id": 36,
          "document_id": "1710.01492",
          "start_idx": 19782,
          "end_idx": 20629
        },
        "page_content": "All these models are examples of supervised learning as they need labeled training data. Semi-supervised learning. We should note two things about the use of deep neural networks. First they can often do quite well without the need for explicit feature modeling, as they can learn the relevant features in their hidden layers starting from the raw text. Second, they have too many parameters, and thus they require a lot of training data, orders of magnitude more than it is realistic to have manually annotated. A popular way to solve this latter problem is to use self training, a form of semi-supervised learning, where first a system is trained on the available training data only, then this system is applied to make predictions on a large unannotated set of tweets, and finally it is trained for a few more iterations on its own predictions.",
        "type": "Document"
      },
      {
        "id": "0755d351-fa2d-439e-8036-3e55704b4d44",
        "metadata": {
          "vector_store_key": "1808.00265-0",
          "chunk_id": 4,
          "document_id": "1808.00265",
          "start_idx": 2049,
          "end_idx": 2870
        },
        "page_content": "Generally, these mechanisms have either been considered as latent variables for which there is no supervision, or have been treated as output variables that receive direct supervision from human annotations. Unfortunately, both of these approaches have disadvantages. First, unsupervised training of attention tends to lead to models that cannot ground their decision in the image in a human interpretable manner. Second, supervised training of attention is difficult and expensive: human annotators may consider different regions to be relevant for the question at hand, which entails ambiguity and increased annotation cost. Our goal is to leverage the best of both worlds by providing VQA algorithms with interpretable grounding of their answers, without the need of direct and explicit manual annotation of attention.",
        "type": "Document"
      },
      {
        "id": "40ab126d-396e-493f-8c79-5fc8114974f0",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 22,
          "document_id": "1809.04960",
          "start_idx": 12891,
          "end_idx": 13537
        },
        "page_content": "The supervised model is trained on INLINEFORM2 so that we can learn the matching or mapping between articles and comments. By sharing the encoder of the supervised model and the unsupervised model, we can jointly train both the models with a joint objective function: DISPLAYFORM0  where INLINEFORM0 is the loss function of the unsupervised learning (Eq. refloss), INLINEFORM1 is the loss function of the supervised learning (e.g. the cross-entropy loss of Seq2Seq model), and INLINEFORM2 is a hyper-parameter to balance two parts of the loss function. Hence, the model is trained on both unpaired data INLINEFORM3 , and paired data INLINEFORM4 .",
        "type": "Document"
      },
      {
        "id": "96576214-2c10-4049-ba11-db649d59677b",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 26,
          "document_id": "1809.04960",
          "start_idx": 15085,
          "end_idx": 15736
        },
        "page_content": "Unsupervised baseline models are as follows: TF-IDF (Lexical, Non-Neural) is an important unsupervised baseline. We use the concatenation of the title and the body as the query to retrieve the candidate comment set by means of the similarity of the tf-idf value. The model is trained on unpaired articles and comments, which is the same as our proposed model. LDA (Topic, Non-Neural) is a popular unsupervised topic model, which discovers the abstract \"topics\" that occur in a collection of documents. We train the LDA with the articles and comments in the training set. The model retrieves the comments by the similarity of the topic representations.",
        "type": "Document"
      },
      {
        "id": "208c8ce5-bc34-4e8d-a91d-339f50fe35b3",
        "metadata": {
          "vector_store_key": "1905.11901-3",
          "chunk_id": 20,
          "document_id": "1905.11901",
          "start_idx": 11281,
          "end_idx": 12012
        },
        "page_content": "Supervised systems serve as an important baseline to judge the effectiveness of semisupervised or unsupervised approaches, and the quality of supervised systems trained on little data can directly impact semi-supervised workflows, for instance for the back-translation of monolingual data. Rico Sennrich has received funding from the Swiss National Science Foundation in the project CoNTra (grant number 105212_169888). Biao Zhang acknowledges the support of the Baidu Scholarship. Table TABREF23 lists hyperparameters used for the different experiments in the ablation study (Table 2). Hyperparameters were kept constant across different data settings, except for the validation interval and subword vocabulary size (see Table 1).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "When they say \"comparable performance\", how much of a performance drop do these new embeddings result in?",
    "llm_answer": "The context states the new embeddings achieve \"comparable performance\" on KG tasks, but does not quantify the performance drop.  Answer not found in the context.\n",
    "context": [
      {
        "id": "12d0a592-674a-4826-b5e6-8b8062a3b72b",
        "metadata": {
          "vector_store_key": "1911.12579-9",
          "chunk_id": 53,
          "document_id": "1911.12579",
          "start_idx": 30540,
          "end_idx": 31329
        },
        "page_content": "We tuned and evaluated the hyperparameters of three algorithms individually which are discussed as follows: Number of Epochs: Generally, more epochs on the corpus often produce better results but more epochs take long training time. Therefore, we evaluate 10, 20, 30 and 40 epochs for each word embedding model, and 40 epochs constantly produce good results. Learning rate (lr): We tried lr of $0.05$, $0.1$, and $0.25$, the optimal lr $(0.25)$ gives the better results for training all the embedding models. Dimensions ($D$): We evaluate and compare the quality of $100-D$, $200-D$, and $300-D$ using WordSim353 on different $ws$, and the optimal $300-D$ are evaluated with cosine similarity matrix for querying nearest neighboring words and calculating the similarity between word pairs.",
        "type": "Document"
      },
      {
        "id": "34560b01-4da0-4895-ae23-1cc40f23fb59",
        "metadata": {
          "vector_store_key": "1712.03547-2",
          "chunk_id": 23,
          "document_id": "1712.03547",
          "start_idx": 12678,
          "end_idx": 13085
        },
        "page_content": "We evaluated the proposed and the baseline method on the interpretability of the learned embeddings. We also evaluated the methods on different KG tasks and compared their performance. We found that the proposed method achieves better interpretability while maintaining comparable performance on KG tasks. As next steps, we plan to evaluate the generalizability of the method with more recent KG embeddings.",
        "type": "Document"
      },
      {
        "id": "9917ffb4-aa82-4b8b-bdf4-49f644272e16",
        "metadata": {
          "vector_store_key": "1603.04513-4",
          "chunk_id": 4,
          "document_id": "1603.04513",
          "start_idx": 2396,
          "end_idx": 3130
        },
        "page_content": "Many papers study the comparative performance of different versions of word embeddings, usually learned by different neural network (NN) architectures. For example, chen2013expressive compared HLBL BIBREF9 , SENNA BIBREF2 , Turian BIBREF13 and Huang BIBREF14 , showing great variance in quality and characteristics of the semantics captured by the tested embedding versions. hill2014not showed that embeddings learned by neural machine translation models outperform three representative monolingual embedding versions: skip-gram BIBREF15 , GloVe BIBREF16 and C&W BIBREF3 in some cases. These prior studies motivate us to explore combining multiple versions of word embeddings, treating each of them as a distinct description of words.",
        "type": "Document"
      },
      {
        "id": "94008079-b618-430d-8277-ace475cd6990",
        "metadata": {
          "vector_store_key": "1611.02550-3",
          "chunk_id": 36,
          "document_id": "1611.02550",
          "start_idx": 19704,
          "end_idx": 20403
        },
        "page_content": "Both classifier and Siamese LSTM embedding models outperform all prior results on this task of which we are aware. We next analyze the effects of model design choices, as well as the learned embeddings themselves. Table TABREF10 shows the effect on development set performance of the number of stacked layers INLINEFORM0 , the number of fully connected layers INLINEFORM1 , and LSTM vs. GRU cells, for classifier-based embeddings. The best performance in this experiment is achieved by the LSTM network with INLINEFORM2 . However, performance still seems to be improving with additional layers, suggesting that we may be able to further improve performance by adding even more layers of either type.",
        "type": "Document"
      },
      {
        "id": "1a2d01d9-329f-4f97-bafc-2f7aea69cf85",
        "metadata": {
          "vector_store_key": "1603.04513-4",
          "chunk_id": 51,
          "document_id": "1603.04513",
          "start_idx": 28841,
          "end_idx": 29587
        },
        "page_content": "In the block \u201cversions\u201d, we see that each embedding version is crucial for good performance: performance drops in every single case. Though it is not easy to compare fairly different embedding versions in NLP tasks, especially when those embeddings were trained on different corpora of different sizes using different algorithms, our results are potentially instructive for researchers making decision on which embeddings to use for their own tasks. This work presented MVCNN, a novel CNN architecture for sentence classification. It combines multichannel initialization \u2013 diverse versions of pretrained word embeddings are used \u2013 and variable-size filters \u2013 features of multigranular phrases are extracted with variable-size convolution filters.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What types of word representations are they evaluating?",
    "llm_answer": "Word embeddings.\n",
    "context": [
      {
        "id": "3d44a941-1ef4-468f-8360-179164039c15",
        "metadata": {
          "vector_store_key": "1804.00079-3",
          "chunk_id": 42,
          "document_id": "1804.00079",
          "start_idx": 23694,
          "end_idx": 24517
        },
        "page_content": "BIBREF6 and BIBREF9 provide a detailed description of tasks that are typically used to evaluate sentence representations. We provide a condensed summary and refer readers to their work for a more thorough description. We evaluate on text classification benchmarks - sentiment classification on movie reviews (MR), product reviews (CR) and Stanford sentiment (SST), question type classification (TREC), subjectivity/objectivity classification (SUBJ) and opinion polarity (MPQA). Representations are used to train a logistic regression classifier with 10-fold cross validation to tune the L2 weight penalty. The evaluation metric for all these tasks is classification accuracy. We also evaluate on pairwise text classification tasks such as paraphrase identification on the Microsoft Research Paraphrase Corpus (MRPC) corpus.",
        "type": "Document"
      },
      {
        "id": "f96bf9f5-d471-4d54-a0c5-712b86363001",
        "metadata": {
          "vector_store_key": "2001.09332-2",
          "chunk_id": 2,
          "document_id": "2001.09332",
          "start_idx": 1100,
          "end_idx": 1958
        },
        "page_content": "These word representations are called Word Embeddings since the words (points in a space of vocabulary size) are mapped in an embedding space of lower dimension. Supported by the distributional hypothesis BIBREF1 BIBREF2, which states that a word can be semantically characterized based on its context (i.e. the words that surround it in the sentence), in recent years many word embedding representations have been proposed (a fairly complete and updated review can be found in BIBREF3 and BIBREF4). These methods can be roughly categorized into two main classes: prediction-based models and count-based models. The former is generally linked to work on Neural Network Language Models (NNLM) and use a training algorithm that predicts the word given its local context, the latter leverage word-context statistics and co-occurrence counts in an entire corpus.",
        "type": "Document"
      },
      {
        "id": "e033fcf1-2977-4b36-a537-de74c22a51c1",
        "metadata": {
          "vector_store_key": "1910.12618-6",
          "chunk_id": 24,
          "document_id": "1910.12618",
          "start_idx": 13337,
          "end_idx": 13999
        },
        "page_content": "The second representation is a neural word embedding. It consists in representing every word in the corpus by a real-valued vector of dimension $q$. Such models are usually obtained by learning a vector representation from word co-occurrences in a very large corpus (typically hundred thousands of documents, such as Wikipedia articles for example). The two most popular embeddings are probably Google's Word2Vec BIBREF19 and Standford's GloVe BIBREF20. In the former, a neural network is trained to predict a word given its context (continuous bag of word model), whereas in the latter a matrix factorization scheme on the log co-occurences of words is applied.",
        "type": "Document"
      },
      {
        "id": "c578e611-8e7c-4422-be4d-cad6be398fa5",
        "metadata": {
          "vector_store_key": "1603.04513-4",
          "chunk_id": 10,
          "document_id": "1603.04513",
          "start_idx": 5948,
          "end_idx": 6678
        },
        "page_content": "Additionally, their idea of variable-size filters is further developed. le2014distributed initialized the representation of a sentence as a parameter vector, treating it as a global feature and combining this vector with the representations of context words to do word prediction. Finally, this fine-tuned vector is used as representation of this sentence. Apparently, this method can only produce generic sentence representations which encode no task-specific features. Our work is also inspired by studies that compared the performance of different word embedding versions or investigated the combination of them. For example, turian2010word compared Brown clusters, C&W embeddings and HLBL embeddings in NER and chunking tasks.",
        "type": "Document"
      },
      {
        "id": "07f483fe-45dc-4a88-b72f-d26d5f098e81",
        "metadata": {
          "vector_store_key": "1911.12579-5",
          "chunk_id": 77,
          "document_id": "1911.12579",
          "start_idx": 43727,
          "end_idx": 44543
        },
        "page_content": "However, in algorithmic perspective, the character-level learning approach in SG and CBoW improves the quality of representation learning, and overall window size, learning rate, number of epochs are the core parameters that largely influence the performance of word embeddings models. Ultimately, the new corpus of low-resourced Sindhi language, list of stop words and pretrained word embeddings along with empirical evaluation, will be a good supplement for future research in SSLP applications. In the future, we aim to use the corpus for annotation projects such as parts-of-speech tagging, named entity recognition. The proposed word embeddings will be refined further by creating custom benchmarks and the extrinsic evaluation approach will be employed for the performance analysis of proposed word embeddings.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What type of recurrent layers does the model use?",
    "llm_answer": "Quasi-recurrent neural networks (QRNNs).\n",
    "context": [
      {
        "id": "6356b79c-b02c-4045-923d-10c84efa9c62",
        "metadata": {
          "vector_store_key": "1710.01507-2",
          "chunk_id": 13,
          "document_id": "1710.01507",
          "start_idx": 7711,
          "end_idx": 8430
        },
        "page_content": "The implicit learning of spatial layout and object semantics in the later layers of the network from very large datasets has contributed to the success of these features. We use a pre-trained network of VGG-19 architecture BIBREF14 trained over the ImageNet database (ILSVRC-2012) and extract CNN features. We use the output of the fully-connected layer (FC7), which has 4096 dimensions, as feature representations for our architecture. We now go into detail about the components of the model, individual and combined, and how the parameters are learned. Recurrent Neural Network (RNN) is a class of artificial neural networks which utilizes sequential information and maintains history through its intermediate layers.",
        "type": "Document"
      },
      {
        "id": "6bbd7a5c-e154-44ef-aa8d-26d57ff0fc1a",
        "metadata": {
          "vector_store_key": "1706.07179-3",
          "chunk_id": 5,
          "document_id": "1706.07179",
          "start_idx": 2979,
          "end_idx": 3608
        },
        "page_content": "The model is sequential in nature, consisting of the following steps: read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer. We model the dynamic memory in a fashion similar to Recurrent Entity Networks BIBREF17 and then equip it with an additional relational memory. There are three main components to the model: 1) input encoder 2) dynamic memory, and 3) output module. We will describe these three modules in details. The input encoder and output module implementations are similar to the Entity Network BIBREF17 and main novelty lies in the dynamic memory.",
        "type": "Document"
      },
      {
        "id": "c37ff81b-ef62-42dd-bf83-07dc675c372d",
        "metadata": {
          "vector_store_key": "1811.00942-3",
          "chunk_id": 8,
          "document_id": "1811.00942",
          "start_idx": 4843,
          "end_idx": 5855
        },
        "page_content": "Given an input $\\mathbf {X} \\in \\mathbb {R}^{k \\times n}$ , the convolution layer is $\n\\mathbf {Z} = \\tanh (\\mathbf {W}_z \\cdot \\mathbf {X})\\\\\n\\mathbf {F} = \\sigma (\\mathbf {W}_f \\cdot \\mathbf {X})\\\\\n\\mathbf {O} = \\sigma (\\mathbf {W}_o \\cdot \\mathbf {X})\n$  where $\\sigma $ denotes the sigmoid function, $\\cdot $ represents masked convolution across time, and $\\mathbf {W}_{\\lbrace z, f, o\\rbrace } \\in \\mathbb {R}^{m \\times k \\times r}$ are convolution weights with $k$ input channels, $m$ output channels, and a window size of $r$ . In the recurrent pooling layer, the convolution outputs are combined sequentially: $\n\\mathbf {c}_t &= \\mathbf {f}_t \\odot \\mathbf {c}_{t-1} + (1 -\n\\mathbf {f}_t) \\odot \\mathbf {z}_t\\\\\n\\mathbf {h}_t &= \\mathbf {o}_t \\odot \\mathbf {c}_t\n$  Multiple QRNN layers can be stacked for deeper hierarchical representation, with the output $\\mathbf {h}_{1:t}$ being fed as the input into the subsequent layer: In language modeling, a four-layer QRNN is a standard architecture BIBREF11 .",
        "type": "Document"
      },
      {
        "id": "c10534c5-d73c-4cd1-8b3e-02a80234f34e",
        "metadata": {
          "vector_store_key": "1811.00942-3",
          "chunk_id": 7,
          "document_id": "1811.00942",
          "start_idx": 4717,
          "end_idx": 5503
        },
        "page_content": "Quasi-recurrent neural networks (QRNNs; BIBREF10 ) achieve current state of the art in word-level language modeling BIBREF11 . A quasi-recurrent layer comprises two separate parts: a convolution layer with three weights, and a recurrent pooling layer. Given an input $\\mathbf {X} \\in \\mathbb {R}^{k \\times n}$ , the convolution layer is $\n\\mathbf {Z} = \\tanh (\\mathbf {W}_z \\cdot \\mathbf {X})\\\\\n\\mathbf {F} = \\sigma (\\mathbf {W}_f \\cdot \\mathbf {X})\\\\\n\\mathbf {O} = \\sigma (\\mathbf {W}_o \\cdot \\mathbf {X})\n$  where $\\sigma $ denotes the sigmoid function, $\\cdot $ represents masked convolution across time, and $\\mathbf {W}_{\\lbrace z, f, o\\rbrace } \\in \\mathbb {R}^{m \\times k \\times r}$ are convolution weights with $k$ input channels, $m$ output channels, and a window size of $r$ .",
        "type": "Document"
      },
      {
        "id": "9db995e9-7e44-49c8-8611-2c8b24572a69",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 32,
          "document_id": "1910.12618",
          "start_idx": 17455,
          "end_idx": 18092
        },
        "page_content": "The output of the recurrent unit is afterwards linked to a fully connected (also referred as dense) layer, leading to the final forecast as output. The rectified linear unit (ReLU) activation in dense layers systematically gave the best results, except on the output layer where we used a sigmoid one considering the time series' normalization. In order to tone down overfitting, dropout layers BIBREF23 with probabilities of 0.25 or 0.33 are set in between the layers. Batch normalization BIBREF24 is also used before the GRU since it stabilized training and improved performance. Figure FIGREF14 represents the architecture of our RNN.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is a word confusion network?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "cc25a03f-a35a-4baf-991b-ab6c91def258",
        "metadata": {
          "vector_store_key": "1905.10810-0",
          "chunk_id": 1,
          "document_id": "1905.10810",
          "start_idx": 726,
          "end_idx": 1417
        },
        "page_content": "Non-word errors are here incorrect word forms that not only differ from what was intended, but also do not constitute another, existing word themselves. Much of the initial research on error correction focused on this simple task, tackled without means of taking the context of the nearest words into account. It is true that, especially in the case of neural networks, it is often possible and desirable to combine problems of error detection, correction and context awareness into one task trained with a supervised training procedure. In language correction research for English language also grammatical and regular spelling errors have been treated uniformly with much success BIBREF1 .",
        "type": "Document"
      },
      {
        "id": "78193a5b-5821-42df-8019-77ad863cc58e",
        "metadata": {
          "vector_store_key": "1909.00124-3",
          "chunk_id": 4,
          "document_id": "1909.00124",
          "start_idx": 2200,
          "end_idx": 3019
        },
        "page_content": "Basically, NetAb consists of two convolutional neural networks (CNNs) (see Figure FIGREF2), one for learning sentiment scores to predict `clean' labels and the other for learning a noise transition matrix to handle input noisy labels. We call the two CNNs A-network and Ab-network, respectively. The fundamental here is that (1) DNNs memorize easy instances first and gradually adapt to hard instances as training epochs increase BIBREF3, BIBREF13; and (2) noisy labels are theoretically flipped from the clean/true labels by a noise transition matrix BIBREF14, BIBREF15, BIBREF16, BIBREF17. We motivate and propose a CNN model with a transition layer to estimate the noise transition matrix for the input noisy labels, while exploiting another CNN to predict `clean' labels for the input training (and test) sentences.",
        "type": "Document"
      },
      {
        "id": "f96bf9f5-d471-4d54-a0c5-712b86363001",
        "metadata": {
          "vector_store_key": "2001.09332-2",
          "chunk_id": 2,
          "document_id": "2001.09332",
          "start_idx": 1100,
          "end_idx": 1958
        },
        "page_content": "These word representations are called Word Embeddings since the words (points in a space of vocabulary size) are mapped in an embedding space of lower dimension. Supported by the distributional hypothesis BIBREF1 BIBREF2, which states that a word can be semantically characterized based on its context (i.e. the words that surround it in the sentence), in recent years many word embedding representations have been proposed (a fairly complete and updated review can be found in BIBREF3 and BIBREF4). These methods can be roughly categorized into two main classes: prediction-based models and count-based models. The former is generally linked to work on Neural Network Language Models (NNLM) and use a training algorithm that predicts the word given its local context, the latter leverage word-context statistics and co-occurrence counts in an entire corpus.",
        "type": "Document"
      },
      {
        "id": "dee3e67c-b0b8-4580-aca8-e6b17aaab015",
        "metadata": {
          "vector_store_key": "1902.09087-0",
          "chunk_id": 20,
          "document_id": "1902.09087",
          "start_idx": 11042,
          "end_idx": 11923
        },
        "page_content": "The output vector of the center word is their weighted sum, where noisy contexts are expected to have lower weights to be smoothed. This pooling over different contexts allows LCNs to work over word lattice input. Word lattice can be seen as directed graphs and modeled by Directed Graph Convolutional networks (DGCs) BIBREF12 , which use poolings on neighboring vertexes that ignore the semantic structure of n-grams. But to some situations, their formulations can be very similar to ours (See Appendix for derivation). For example, if we set the kernel size in LCNs to 3, use linear activations and suppose the pooling mode is average in both LCNs and DGCs, at each word in each layer, the DGCs compute the average of the first order neighbors together with the center word, while the LCNs compute the average of the pre and post words separately and add them to the center word.",
        "type": "Document"
      },
      {
        "id": "c0b7f863-49a0-4ade-bb2d-33b9053fabb2",
        "metadata": {
          "vector_store_key": "1911.12569-2",
          "chunk_id": 20,
          "document_id": "1911.12569",
          "start_idx": 12291,
          "end_idx": 12976
        },
        "page_content": "For our task, the feed forward network for sentiment analysis has two output units, whereas the feed forward network for emotion analysis has eight output nodes performing multi-label classification. Distributional Thesaurus (DT) BIBREF31 ranks words according to their semantic similarity. It is a resource which produces a list of words in decreasing order of their similarity for each word. We use the DT to expand each word of the sentence. The top-4 words serve as the candidate terms for each word. For example, the candidate terms for the word good are: great, nice awesome and superb. DT offers the primary attention mechanism external knowledge in the form of candidate terms.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What evaluation metrics were used in the experiment?",
    "llm_answer": "Triple Accuracy, Contextual Consistency, MSE, R2, ROUGE, human evaluation (manual inspection), ROUGE, BLEU, METEOR, perplexity.\n",
    "context": [
      {
        "id": "1aa58b63-7d45-4c30-8497-358be6b0b602",
        "metadata": {
          "vector_store_key": "1804.03396-5",
          "chunk_id": 56,
          "document_id": "1804.03396",
          "start_idx": 29719,
          "end_idx": 30662
        },
        "page_content": "Note that our framework takes the whole document as the input while the baseline systems take the individual sentence as the input, which means the experiment setting is much more difficult for our framework. Finally, we perform a human evaluation on our QA4IE benchmark to verify the reliability of former experiments. The evaluation metrics are as follows: Triple Accuracy is to check whether each ground truth triple is accurate (one cannot find conflicts between the ground truth triple and the corresponding article) because the ground truth triples from WikiData and DBpedia may be incorrect or incomplete. Contextual Consistency is to check whether the context of each answer location is consistent with the corresponding ground truth triple (one can infer from the context to obtain the ground truth triple) because we keep all matched answer locations as ground truths but some of them may be irrelevant with the corresponding triple.",
        "type": "Document"
      },
      {
        "id": "b4147fd6-440a-48e7-8aba-605508e4eed1",
        "metadata": {
          "vector_store_key": "1909.02776-5",
          "chunk_id": 49,
          "document_id": "1909.02776",
          "start_idx": 27461,
          "end_idx": 28087
        },
        "page_content": "The same process was repeated with a random regressor that needed no training, and which simply assigns a random number between zero and one to any given sample. Apart from measuring the performance of this regressor on the test set, the quality of summaries produced is evaluated and reported as a baseline. The juxtaposition of this baseline and our measured results will demonstrate how effective our feature set was and how intelligent our whole system worked. In section (SECREF22) MSE, R2 and ROUGE scores are remarked as evaluation measures. The results of our experiments are reported below in terms of these measures.",
        "type": "Document"
      },
      {
        "id": "b360d511-f50b-49b0-adae-b9cdd19697b2",
        "metadata": {
          "vector_store_key": "1908.07816-1",
          "chunk_id": 33,
          "document_id": "1908.07816",
          "start_idx": 19008,
          "end_idx": 19804
        },
        "page_content": "Thus each person produced 25 dialogs, and in total we obtained 50 emotionally negative daily dialogs in addition to the 14 already available. To form the test set, we randomly selected 50 emotionally positive and 50 emotionally negative dialogs from the two pools of dialogs described above (78 positive dialogs from DailyDialog, 64 negative dialogs from DailyDialog and human-generated). For human evaluation of the models, we recruited another four English-speaking students from our university without any relationship to the authors' lab to rate the responses generated by the models. Specifically, we randomly shuffled the 100 dialogs in the test set, then we used the first three utterances of each dialog as the input to the three models being compared and let them generate the responses.",
        "type": "Document"
      },
      {
        "id": "61e54692-2da3-41fb-b1b0-c3469ffa76bf",
        "metadata": {
          "vector_store_key": "1607.06025-0",
          "chunk_id": 17,
          "document_id": "1607.06025",
          "start_idx": 10067,
          "end_idx": 10777
        },
        "page_content": "These attributes are conditional information that is fed to the models, like the discrete label is in our models. As recognized by BIBREF37 , the evaluation metrics of text-generating models fall into three categories: manual evaluation, automatic evaluation metrics, task-based evaluation. In evaluation based on human judgment each generated textual example is inspected manually. The automatic evaluation metrics, like ROUGE, BLEU and METEOR, compare human texts and generated texts. BIBREF38 shows METEOR has the strongest correlation with human judgments in image description evaluation. The last category is task-based evaluation, where the impact of the generated texts on a particular task is measured.",
        "type": "Document"
      },
      {
        "id": "55dbf52d-8f13-4098-bee7-188913301f41",
        "metadata": {
          "vector_store_key": "1908.07816-1",
          "chunk_id": 31,
          "document_id": "1908.07816",
          "start_idx": 17850,
          "end_idx": 18520
        },
        "page_content": "Recent work BIBREF25 has shown that the automatic evaluation metrics borrowed from machine translation such as BLEU score BIBREF26 tend to align poorly with human judgement. Therefore, in this paper, we mainly adopt human evaluation, along with perplexity, following the existing work. To develop a test set for human evaluation, we first selected the emotionally colored dialogs with exactly four turns from the DailyDialog dataset. In the dataset each dialog turn is annotated with a corresponding emotional category, including the neutral one. For our purposes we filtered out only those dialogs where more than a half of utterances have non-neutral emotional labels.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What kind of instructional videos are in the dataset?",
    "llm_answer": "Tutorial videos about image editing software.\n",
    "context": [
      {
        "id": "3bf43b61-90a8-4ada-8b08-07ee03879596",
        "metadata": {
          "vector_store_key": "1912.01046-0",
          "chunk_id": 9,
          "document_id": "1912.01046",
          "start_idx": 5679,
          "end_idx": 6399
        },
        "page_content": "Our dataset consists of 76 tutorial videos pertaining to an image editing software. All of the videos include spoken instructions which are transcribed and manually segmented into multiple segments. Specifically, we asked the annotators to manually divide each video into multiple segments such that each of the segments can serve as an answer to any question. For example, Fig. FIGREF1 shows example segments marked in red (each which are a complete unit as an answer span). Each sentence is associated with the starting and ending time-stamps, which can be used to access the relevant visual information. The dataset contains 6,195 non-factoid QA pairs, where the answers are the segments that were manually annotated.",
        "type": "Document"
      },
      {
        "id": "73a83f60-b2c9-406d-b4cd-5720639e5f06",
        "metadata": {
          "vector_store_key": "1812.07023-4",
          "chunk_id": 18,
          "document_id": "1812.07023",
          "start_idx": 10358,
          "end_idx": 10986
        },
        "page_content": "After 10 such qa turns, the questioner wraps up by writing a summary of the video based on the conversation. Dataset statistics such as the number of dialogues, turns, and words for the avsd dataset are presented in Table TABREF5 . For the initially released prototype dataset, the training set of the avsd dataset corresponds to videos taken from the training set of the Charades dataset while the validation and test sets of the avsd dataset correspond to videos taken from the validation set of the Charades dataset. For the official dataset, training, validation and test sets are drawn from the corresponding Charades sets.",
        "type": "Document"
      },
      {
        "id": "7c403dbf-bb17-4aeb-9fde-4d4b9b04d156",
        "metadata": {
          "vector_store_key": "1912.01046-0",
          "chunk_id": 12,
          "document_id": "1912.01046",
          "start_idx": 7310,
          "end_idx": 8032
        },
        "page_content": "Each segment can be seen as a subtask within a larger video dictating an example. We thus chose these videos because of the amount of procedural information stored in each video for which the user may ask. Though there is only one domain, each video corresponds to a different overall goal. We downloaded 76 videos from a tutorial website about an image editing program . Each video is pre-processed to provide the transcripts and the time-stamp information for each sentence in the transcript. We then used Amazon Mechanical Turk to collect the question-answer pairs . One naive way of collecting the data is to prepare a question list and then, for each question, ask the workers to find the relevant parts in the video.",
        "type": "Document"
      },
      {
        "id": "65fef37e-310d-4e21-ae35-5091c00fd191",
        "metadata": {
          "vector_store_key": "1912.01046-2",
          "chunk_id": 36,
          "document_id": "1912.01046",
          "start_idx": 20317,
          "end_idx": 21218
        },
        "page_content": "In future work, we also plan to use multi-modal information. While our baselines only used the transcript, complementing the narratives with the visual information may improve the performance, similarly to the text alignment task in BIBREF11. We have described the collection, analysis, and baseline results of TutorialVQA, a new type of dataset used to find answer spans in tutorial videos. Our data collection method for question-answer pairs on instructional video can be further adopted to other domains where the answers involve multiple steps and are part of an overall goal, such as cooking or educational videos. We have shown that current baseline models for finding the answer spans are not sufficient for achieving high accuracy and hope that by releasing this new dataset and task, more appropriate question answering models can be developed for question answering on instructional videos.",
        "type": "Document"
      },
      {
        "id": "b4247055-5c2b-4249-89ae-776301c5584c",
        "metadata": {
          "vector_store_key": "1912.01046-0",
          "chunk_id": 10,
          "document_id": "1912.01046",
          "start_idx": 6203,
          "end_idx": 6873
        },
        "page_content": "The dataset contains 6,195 non-factoid QA pairs, where the answers are the segments that were manually annotated. Fig. FIGREF5 shows an example of the annotations. video_id can be used to retrieve the video information such as meta information and the transcripts. answer_start and answer_end denote the starting and ending sentence indexes of the answer span. Table. TABREF4 shows the statistics of our dataset, with each answer segment having on average about 6 sentences, showing that our answers are more verbose than those in previous factoid QA tasks. We chose videos pertaining to an image editing software because of the complexity and variety of tasks involved.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What baseline algorithms were presented?",
    "llm_answer": "Fully Connected dense networks,  K-means, Skip-thought Vectors, Recursive Neural Network, Paragraph Vector, Majority Class, Exact Match, First Occurrence, LSTM, bidirectional RNN, MLP, Random Forest, Linear Support Vector Classifier, Regularized linear classifier, Stochastic Gradient Descent, Nearest Centroid.\n",
    "context": [
      {
        "id": "abde311f-db3e-44fd-96aa-882a0a2aeeb0",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 17,
          "document_id": "1912.13109",
          "start_idx": 8552,
          "end_idx": 9307
        },
        "page_content": "These processes and its implementation details will be discussed later but they were learning rate decay, early stopping, model checkpointing and reducing learning rate on plateau. For the loss function we chose categorical cross entropy loss in finding the most optimal weights/parameters of the model. Formally this loss function for the model is defined as below: The double sum is over the number of observations and the categories respectively. While the model probability is the probability that the observation i belongs to category c. Among the model architectures we experimented with and without data augmentation were: Fully Connected dense networks: Model hyperparameters were inspired from the previous work done by Vo et al and Mathur et al.",
        "type": "Document"
      },
      {
        "id": "fa88df39-033d-4e0f-891b-162e0f9b0424",
        "metadata": {
          "vector_store_key": "1709.08299-4",
          "chunk_id": 19,
          "document_id": "1709.08299",
          "start_idx": 11042,
          "end_idx": 11836
        },
        "page_content": "The neural baselines are built by the following steps. We first use the shared training data to build a general systems, and choose the best performing model (in terms of cloze validation set) as baseline. Use User Query validation data for further tuning the systems with 10-fold cross-validations. Increase dropout rate BIBREF9 to 0.5 for preventing over-fitting issue. All baseline systems are chosen according to the performance of the validation set. The participant system results are given in Table 2 and 3 . As we can see that two neural baselines are competitive among participant systems and AoA Reader successfully outperform AS Reader and all participant systems in single model condition, which proves that it is a strong baseline system even without further fine-tuning procedure.",
        "type": "Document"
      },
      {
        "id": "46e563f1-f3ad-4871-9d28-8409ceeb53b8",
        "metadata": {
          "vector_store_key": "1701.00185-0",
          "chunk_id": 38,
          "document_id": "1701.00185",
          "start_idx": 20509,
          "end_idx": 21535
        },
        "page_content": "Besides K-means, Skip-thought Vectors, Recursive Neural Network and Paragraph Vector based clustering methods, four baseline clustering methods are directly based on the popular unsupervised dimensionality reduction methods as described in Section SECREF11 . We further compare our approach with some other non-biased neural networks, such as bidirectional RNN. More details are listed as follows: K-means K-means BIBREF42 on original keyword features which are respectively weighted with term frequency (TF) and term frequency-inverse document frequency (TF-IDF). Skip-thought Vectors (SkipVec) This baseline BIBREF35 gives an off-the-shelf encoder to produce highly generic sentence representations. The encoder is trained using a large collection of novels and provides three encoder modes, that are unidirectional encoder (SkipVec (Uni)) with 2,400 dimensions, bidirectional encoder (SkipVec (Bi)) with 2,400 dimensions and combined encoder (SkipVec (Combine)) with SkipVec (Uni) and SkipVec (Bi) of 2,400 dimensions each.",
        "type": "Document"
      },
      {
        "id": "5e76f24e-4c75-4a34-b69c-7fe1f8d12771",
        "metadata": {
          "vector_store_key": "1909.02635-2",
          "chunk_id": 15,
          "document_id": "1909.02635",
          "start_idx": 9069,
          "end_idx": 9719
        },
        "page_content": "This includes (i) Majority Class, (ii) Exact Match of an ingredient $e$ in recipe step $s_t$, and (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo$_{token}$ and ELMo$_{sent}$) BIBREF22 compared to GPT.",
        "type": "Document"
      },
      {
        "id": "c93f2257-9006-466a-af1f-9340e6b4ab7d",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 28,
          "document_id": "2001.00137",
          "start_idx": 16720,
          "end_idx": 17443
        },
        "page_content": "Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector Classifier with L1 and L2 penalty and tolerance of $10^{-3}$; Regularized linear classifier with Stochastic Gradient Descent (SGD) learning with regularization term $alpha=10^{-4}$ and L1, L2 and Elastic-Net penalty; Nearest Centroid with Euclidian metric, where classification is done by representing each class with a centroid;",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does TP-N2F compare to LSTM-based Seq2Seq in terms of training and inference speed?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "14ea7791-dff4-4cc2-a119-4cbe7c5fdefe",
        "metadata": {
          "vector_store_key": "1910.02339-6",
          "chunk_id": 46,
          "document_id": "1910.02339",
          "start_idx": 23725,
          "end_idx": 24449
        },
        "page_content": "TP-N2F is compared with an LSTM seq2seq with attention model, the Seq2Tree model in BIBREF17, and a seq2seq model with a pre-trained tree decoder from the Tree2Tree autoencoder (SAPS) reported in BIBREF18. As shown in Table TABREF18, TP-N2F outperforms all existing models on both the full test set and the cleaned test set. Ablation experiments with TP2LSTM and LSTM2TP show that, for this task, the TP-N2F Decoder is more helpful than TP-N2F Encoder. This may be because lisp codes rely more heavily on structure representations. To interpret the structure learned by the model, we extract the trained unbinding relation vectors from the TP-N2F Decoder and reduce the dimension of vectors via Principal Component Analysis.",
        "type": "Document"
      },
      {
        "id": "4ca05e67-d35a-4c2d-9735-3a4fc9fa1ac5",
        "metadata": {
          "vector_store_key": "1903.07398-4",
          "chunk_id": 14,
          "document_id": "1903.07398",
          "start_idx": 7681,
          "end_idx": 8336
        },
        "page_content": "Our proposed improvements come from the observation that employing generic Seq2seq models for TTS application misses out on further optimization that can be achieved when we consider the specific problem of TTS. Specifically, we notice that in TTS, unlike in applications like machine translation, the Seq2Seq attention mechanism should be mostly monotonic. In other words, when one reads a sequence of text, it is natural to assume that the text position progress nearly linearly in time with the sequence of output mel spectrogram. With this insight, we can make 3 modifications to the model that allows us to train faster while using a a smaller model.",
        "type": "Document"
      },
      {
        "id": "10ddd0e6-7d6e-4234-b1f7-ac4a60556bfe",
        "metadata": {
          "vector_store_key": "1803.07771-2",
          "chunk_id": 47,
          "document_id": "1803.07771",
          "start_idx": 27756,
          "end_idx": 28405
        },
        "page_content": "The results in Table 4 indicate that the performances of Tl-LSTM on the mixed training and test data (R+C) are better than those of Bi-LSTM. This comparison indicates that the proposed two-level LSTM is effective. In addition, for the involved algorithms, most results achieved on \u201cR+C\" are better than the best results only achieved on `R'listed in Table 3. This comparison suggests that the introduced two-stage labeling is useful. The results also show that in the two-level LSTM, character embedding is more effective than word embedding. In this experimental run, lexicon embedding is used in the proposed two-level LSTM or INLINEFORM0 Tl-LSTM.",
        "type": "Document"
      },
      {
        "id": "852b99a8-bc1a-4c77-a299-e20733fe5af2",
        "metadata": {
          "vector_store_key": "1903.07398-0",
          "chunk_id": 23,
          "document_id": "1903.07398",
          "start_idx": 12628,
          "end_idx": 13299
        },
        "page_content": "For all other models the training time is not comparable as they either don't apply (e.g parametric model) or are not reported (Tacotron griffin lim, Deepvoice 3). Direct comparison of model parameters between ours and the open-source tacotron 2, our model contains 4.5 million parameters, whereas the Tacotron 2 contains around 13 million parameters with default setting. By helping our model learn attention alignment faster, we can afford to use a smaller overall model to achieve similar quality speech quality. We introduce a new architecture for end-to-end neural text-to-speech system. Our model relies on RNN-based Seq2seq architecture with a query-key attention.",
        "type": "Document"
      },
      {
        "id": "98948535-0e36-49a4-a3ca-aabea669a302",
        "metadata": {
          "vector_store_key": "1811.00942-1",
          "chunk_id": 11,
          "document_id": "1811.00942",
          "start_idx": 6755,
          "end_idx": 7459
        },
        "page_content": "BIBREF13 ). Preprocessed by BIBREF14 , PTB contains 887K tokens for training, 70K for validation, and 78K for test, with a vocabulary size of 10,000. On the other hand, WT103 comprises 103 million tokens for training, 217K for validation, and 245K for test, spanning a vocabulary of 267K unique tokens. For the neural language model, we used a four-layer QRNN BIBREF10 , which achieves state-of-the-art results on a variety of datasets, such as WT103 BIBREF11 and PTB. To compare against more common LSTM architectures, we also evaluated AWD-LSTM BIBREF4 on PTB. For the non-neural approach, we used a standard five-gram model with modified Kneser-Ney smoothing BIBREF15 , as explored in BIBREF16 on PTB.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance proposed model achieved on AlgoList benchmark?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "68a5e6f2-17dd-46e5-8764-888724e87ab9",
        "metadata": {
          "vector_store_key": "1701.06538-3",
          "chunk_id": 32,
          "document_id": "1701.06538",
          "start_idx": 17371,
          "end_idx": 18235
        },
        "page_content": "Table TABREF33 compares the results of these models to the best previously-published result on this dataset . Even the fastest of these models beats the best published result (when controlling for the number of training epochs), despite requiring only 6% of the computation. We trained our models using TensorFlow BIBREF30 on clusters containing 16-32 Tesla K40 GPUs. For each of our models, we determine computational efficiency in TFLOPS/GPU by dividing the number of floating point operations required to process one training batch by the observed step time and the number of GPUs in the cluster. The operation counts used here are higher than the ones we report in our ops/timestep numbers in that we include the backwards pass, we include the importance-sampling-based training of the softmax layer, and we count a multiply-and-add as two separate operations.",
        "type": "Document"
      },
      {
        "id": "31d590cc-f828-4a88-a077-fb680eb013eb",
        "metadata": {
          "vector_store_key": "1804.03396-8",
          "chunk_id": 38,
          "document_id": "1804.03396",
          "start_idx": 20650,
          "end_idx": 21263
        },
        "page_content": "Our model is trained on a GTX 1080 Ti GPU and it takes about 14 hours on small sized QA4IE datasets. We implement our model with TensorFlow BIBREF47 and optimize the computational expensive LSTM layers with LSTMBlockFusedCell. We first perform experiments in QA settings to evaluate our QA model on both SQuAD dataset and QA4IE benchmark. Since our goal is to solve IE, not QA, the motivation of this part of experiments is to evaluate the performance of our model and make a comparison between QA4IE benchmark and existing datasets. Two metrics are introduced in the SQuAD dataset: Exact Match (EM) and F1-score.",
        "type": "Document"
      },
      {
        "id": "6824b4f2-136e-4c26-a2a7-a10cf7d4c97e",
        "metadata": {
          "vector_store_key": "1810.00663-8",
          "chunk_id": 46,
          "document_id": "1810.00663",
          "start_idx": 25788,
          "end_idx": 26550
        },
        "page_content": "The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as can be seen in performance drops in Table TABREF28 for the new environments. Nonetheless, the insights from the previous section still hold: masking in the output layer and reordering the graph triplets tend to increase performance. Even though the results in Table TABREF28 suggest that there is room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM.",
        "type": "Document"
      },
      {
        "id": "a814ca02-aee8-4d15-8e77-8c0c407b21d5",
        "metadata": {
          "vector_store_key": "1910.02339-3",
          "chunk_id": 45,
          "document_id": "1910.02339",
          "start_idx": 23541,
          "end_idx": 24197
        },
        "page_content": "AlgoLisp provides an execution script to run the generated program and has three evaluation metrics: the accuracy of passing all test cases (Acc), the accuracy of passing 50% of test cases (50p-Acc), and the accuracy of generating an exactly matching program (M-Acc). AlgoLisp has about 10% noisy data (details in the Appendix), so we report results both on the full test set and the cleaned test set (in which all noisy testing samples are removed). TP-N2F is compared with an LSTM seq2seq with attention model, the Seq2Tree model in BIBREF17, and a seq2seq model with a pre-trained tree decoder from the Tree2Tree autoencoder (SAPS) reported in BIBREF18.",
        "type": "Document"
      },
      {
        "id": "e173d047-a768-42d6-8e8a-9658acdffb5a",
        "metadata": {
          "vector_store_key": "1808.09920-3",
          "chunk_id": 32,
          "document_id": "1808.09920",
          "start_idx": 16776,
          "end_idx": 17482
        },
        "page_content": "We report single runs of our two best single models and an ensemble one on the unmasked test set (recall that the test set is not publicly available and the task organizers only report unmasked results) as well as both versions of the validation set. Entity-GCN (best single model without coreference edges) outperforms all previous work by over 2% points. We additionally re-ran BiDAF baseline to compare training time: when using a single Titan X GPU, BiDAF and Entity-GCN process 12.5 and 57.8 document sets per second, respectively. Note that BIBREF0 had to use BiDAF with very small state dimensionalities (20), and smaller batch size due to the scalability issues (both memory and computation costs).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance proposed model achieved on MathQA?",
    "llm_answer": "The proposed model achieved execution accuracy and operation sequence accuracy on the MathQA dataset, but specific numbers are not provided.\n",
    "context": [
      {
        "id": "ae83288f-1276-4e59-bcfa-3af73d99eb03",
        "metadata": {
          "vector_store_key": "1910.02339-3",
          "chunk_id": 52,
          "document_id": "1910.02339",
          "start_idx": 27475,
          "end_idx": 28254
        },
        "page_content": "The MathQA dataset consists of about 37k math word problems ((80/12/8)% training/dev/testing problems), each with a corresponding list of multi-choice options and an straight-line operation sequence program to solve the problem. An example from the dataset is presented in the Appendix A.4. In this task, TP-N2F is deployed to generate the operation sequence given the question. The generated operations are executed to generate the solution for the given math problem. We use the execution script from BIBREF16 to execute the generated operation sequence and compute the multi-choice accuracy for each problem. During our experiments we observed that there are about 30% noisy examples (on which the execution script fails to get the correct answer on the ground truth program).",
        "type": "Document"
      },
      {
        "id": "a43f272e-38ae-4708-9702-e92f17b80a07",
        "metadata": {
          "vector_store_key": "1910.02339-3",
          "chunk_id": 41,
          "document_id": "1910.02339",
          "start_idx": 21293,
          "end_idx": 22032
        },
        "page_content": "The MathQA dataset consists of about 37k math word problems, each with a corresponding list of multi-choice options and the corresponding operation sequence. In this task, TP-N2F is deployed to generate the operation sequence given the question. The generated operations are executed with the execution script from BIBREF16 to select a multi-choice answer. As there are about 30% noisy data (where the execution script returns the wrong answer when given the ground-truth program; see Sec. SECREF20 of the Appendix), we report both execution accuracy (of the final multi-choice answer after running the execution engine) and operation sequence accuracy (where the generated operation sequence must match the ground truth sequence exactly).",
        "type": "Document"
      },
      {
        "id": "7bd99408-11ce-4476-9ae6-abfc66eb50a4",
        "metadata": {
          "vector_store_key": "1706.04815-2",
          "chunk_id": 39,
          "document_id": "1706.04815",
          "start_idx": 21864,
          "end_idx": 22575
        },
        "page_content": "For the evidence extraction part, our proposed multi-task learning framework achieves 42.23 and 44.11 for the single and ensemble model in terms of ROUGE-L. For the answer synthesis, the single and ensemble models improve 3.72% and 3.65% respectively in terms of ROUGE-L. We observe the consistent improvement when applying our answer synthesis model to other answer span prediction models, such as BiDAF and Prediction. We analyze the result of incorporating passage ranking as an additional task. We compare our multi-task framework with two baselines as shown in Table 4 . For passage selection, our multi-task model achieves the accuracy of 38.9, which outperforms the pure answer prediction model with 4.3.",
        "type": "Document"
      },
      {
        "id": "ac72441b-d421-4398-887b-1a131abd79cd",
        "metadata": {
          "vector_store_key": "1808.00265-0",
          "chunk_id": 39,
          "document_id": "1808.00265",
          "start_idx": 22641,
          "end_idx": 23411
        },
        "page_content": "Our models are built on top of prior works with the additional Attention Supervision Module as described in Section SECREF3 . Specifically, we denote by Attn-* our adaptation of the respective model by including our Attention Supervision Module. We highlight that MCB model is the winner of VQA challenge 2016 and MFH model is the best single model in VQA challenge 2017. In Table TABREF10 , we can observe that our proposed model achieves a significantly boost on rank-correlation with respect to human attention. Furthermore, our model outperforms alternative state-of-art techniques in terms of accuracy in answer prediction. Specifically, the rank-correlation for MFH model increases by 36.4% when is evaluated in VQA-HAT dataset and 7.7% when is evaluated in VQA-X.",
        "type": "Document"
      },
      {
        "id": "071b8e44-d470-4ee3-a8b6-10d72955ab53",
        "metadata": {
          "vector_store_key": "1909.08859-2",
          "chunk_id": 43,
          "document_id": "1909.08859",
          "start_idx": 24516,
          "end_idx": 25252
        },
        "page_content": "Our experimental analysis on visual reasoning tasks in the RecipeQA dataset shows that the model significantly improves the results of the previous models, indicating that it better understands the procedural text and the accompanying images. Additionally, we carefully analyze our results and find that our approach learns meaningful dynamic representations of entities without any entity-level supervision. Although we achieve state-of-the-art results on RecipeQA, clearly there is still room for improvement compared to human performance. We also believe that the PRN architecture will be of value to other visual and textual sequential reasoning tasks. We thank the anonymous reviewers and area chairs for their invaluable feedback.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What previous methods is the proposed method compared against?",
    "llm_answer": "State-of-the-art methods, baseline methods, Coppersmith et al.'s method, Shallow Fusion.\n",
    "context": [
      {
        "id": "11bbd522-28ef-4580-9084-84720bfb9758",
        "metadata": {
          "vector_store_key": "2001.06888-0",
          "chunk_id": 42,
          "document_id": "2001.06888",
          "start_idx": 22556,
          "end_idx": 22770
        },
        "page_content": "Evaluation results of our proposed model compared to other state of the art methods show its superiority to these methods in overall while in two categories (Person and Miscellaneous) our model outperformed others.",
        "type": "Document"
      },
      {
        "id": "34560b01-4da0-4895-ae23-1cc40f23fb59",
        "metadata": {
          "vector_store_key": "1712.03547-2",
          "chunk_id": 23,
          "document_id": "1712.03547",
          "start_idx": 12678,
          "end_idx": 13085
        },
        "page_content": "We evaluated the proposed and the baseline method on the interpretability of the learned embeddings. We also evaluated the methods on different KG tasks and compared their performance. We found that the proposed method achieves better interpretability while maintaining comparable performance on KG tasks. As next steps, we plan to evaluate the generalizability of the method with more recent KG embeddings.",
        "type": "Document"
      },
      {
        "id": "b2f8b59b-1092-4698-9fa3-c0a21594d3ff",
        "metadata": {
          "vector_store_key": "2003.07433-4",
          "chunk_id": 47,
          "document_id": "2003.07433",
          "start_idx": 25723,
          "end_idx": 26401
        },
        "page_content": "To compare the outperformance of our method, we also implemented Coppersmith et. al. proposed method and achieved an 86% overall accuracy of detecting PTSD users BIBREF11 following the same training-test dataset distribution. Fig FIGREF28 illustrates the comparisons between LAXARY and Coppersmith et. al. proposed method. Here we can see, the outperformance of our proposed method as well as the importance of $s-score$ estimation. We also illustrates the importance of $\\alpha -score$ and $S-score$ in Fig FIGREF30. Fig FIGREF30 illustrates that if we change the number of training samples (%), LAXARY models outperforms Coppersmith et. al. proposed model under any condition.",
        "type": "Document"
      },
      {
        "id": "15df01f7-487d-4979-bb29-596280598bab",
        "metadata": {
          "vector_store_key": "1909.08824-4",
          "chunk_id": 39,
          "document_id": "1909.08824",
          "start_idx": 22975,
          "end_idx": 23437
        },
        "page_content": "Experimental results demonstrate that CWVAE outperforms baseline methods in both the accuracy and diversity of generations. We thank the anonymous reviewers for their constructive comments, and gratefully acknowledge the support of the National Key Research and Development Program of China (SQ2018AAA010010), the National Key Research and Development Program of China (2018YFB1005103), the National Natural Science Foundation of China (NSFC) via Grant 61702137.",
        "type": "Document"
      },
      {
        "id": "d9d10da4-cfc8-450e-a37a-62d0e3f013fe",
        "metadata": {
          "vector_store_key": "2002.11268-1",
          "chunk_id": 3,
          "document_id": "2002.11268",
          "start_idx": 1909,
          "end_idx": 2557
        },
        "page_content": "These methods have not replaced the simple Shallow Fusion method as the go-to method in most of the ASR community. Part of the appeal of Shallow Fusion is that it does not require model retraining \u2013 it can be applied purely at decoding time. The Density Ratio approach proposed here can be seen as an extension of Shallow Fusion, sharing some of its simplicity and practicality, but offering a theoretical grounding in Bayes' rule. After describing the historical context, theory and practical implementation of the proposed Density Ratio method, this article describes experiments comparing the method to Shallow Fusion in a cross-domain scenario.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the baseline model used?",
    "llm_answer": "Baseline1, which represents a span of sentences by concatenating the vectors of the starting and ending sentences.\n",
    "context": [
      {
        "id": "fa88df39-033d-4e0f-891b-162e0f9b0424",
        "metadata": {
          "vector_store_key": "1709.08299-4",
          "chunk_id": 19,
          "document_id": "1709.08299",
          "start_idx": 11042,
          "end_idx": 11836
        },
        "page_content": "The neural baselines are built by the following steps. We first use the shared training data to build a general systems, and choose the best performing model (in terms of cloze validation set) as baseline. Use User Query validation data for further tuning the systems with 10-fold cross-validations. Increase dropout rate BIBREF9 to 0.5 for preventing over-fitting issue. All baseline systems are chosen according to the performance of the validation set. The participant system results are given in Table 2 and 3 . As we can see that two neural baselines are competitive among participant systems and AoA Reader successfully outperform AS Reader and all participant systems in single model condition, which proves that it is a strong baseline system even without further fine-tuning procedure.",
        "type": "Document"
      },
      {
        "id": "39cb1ec4-6c9f-4dbd-b781-29fe9d4ad19f",
        "metadata": {
          "vector_store_key": "1912.01046-3",
          "chunk_id": 23,
          "document_id": "1912.01046",
          "start_idx": 13260,
          "end_idx": 13980
        },
        "page_content": "Following this idea, Baseline1 represents a span of sentences by concatenating the vectors of the starting and ending sentences. The left diagram in Fig. FIGREF15 illustrates the Baseline1 model. Model. The model takes two inputs, a transcript, $\\lbrace s_1, s_2, ... s_n\\rbrace $ where $s_i$ are individual sentences and a question, $q$. The output is the span scores, $y$, the scores over all possible spans. GLoVe BIBREF14 is used for the word representations in the transcript and the questions. We use two bi-LSTMs BIBREF15 to encode the transcript. where n is the number of sentences . The output of Passage-level Encoding, $p$, is a sequence of vector, $p_i$, which represents the latent meaning of each sentence.",
        "type": "Document"
      },
      {
        "id": "763bf540-a941-47b1-bdeb-25182eab991e",
        "metadata": {
          "vector_store_key": "2001.11381-4",
          "chunk_id": 55,
          "document_id": "2001.11381",
          "start_idx": 31798,
          "end_idx": 32460
        },
        "page_content": "Tampoco consideramos la utilizaci\u00f3n de un baseline de tipo aleatorio, porque los resultados carecer\u00edan de la homosintaxis y ser\u00eda sumamente f\u00e1cil obtener mejores resultados. Dicho lo anterior, el Modelo 1 podr\u00eda ser considerado como nuestro propio baseline. A continuaci\u00f3n presentamos un protocolo de evaluaci\u00f3n manual de los resultados obtenidos. El experimento consisti\u00f3 en la generaci\u00f3n de 15 frases por cada uno de los tres modelos propuestos. Para cada modelo, se consideraron tres queries: $Q=$ {AMOR, GUERRA, SOL}, generando 5 frases con cada uno. Las 15 frases fueron mezcladas entre s\u00ed y reagrupadas por queries, antes de presentarlas a los evaluadores.",
        "type": "Document"
      },
      {
        "id": "fcbb237d-302f-45b0-886a-31ec58fabb9f",
        "metadata": {
          "vector_store_key": "1909.09484-4",
          "chunk_id": 35,
          "document_id": "1909.09484",
          "start_idx": 19125,
          "end_idx": 19742
        },
        "page_content": "An Adam optimizer BIBREF22 is used for training our models and baselines, with a learning rate of 0.001 for supervised training and 0.0001 for reinforcement learning. In reinforcement learning, the decay parameter $\\lambda $ is set to 0.8. The weight decay is set to 0.001. And early stopping is performed on developing set. The experimental results of the proposed model and baselines will be analyzed from the following aspects. BPRA Results: As shown in Table TABREF35, most of the models have similar performance on BPRA on these two datasets, which can guarantee a consistent impact on the dialogue policy maker.",
        "type": "Document"
      },
      {
        "id": "5e76f24e-4c75-4a34-b69c-7fe1f8d12771",
        "metadata": {
          "vector_store_key": "1909.02635-2",
          "chunk_id": 15,
          "document_id": "1909.02635",
          "start_idx": 9069,
          "end_idx": 9719
        },
        "page_content": "This includes (i) Majority Class, (ii) Exact Match of an ingredient $e$ in recipe step $s_t$, and (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo$_{token}$ and ELMo$_{sent}$) BIBREF22 compared to GPT.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What domains are present in the data?",
    "llm_answer": "Lifestyle, hotel, attraction, restaurant, metro, taxi.\n",
    "context": [
      {
        "id": "786eaa4f-6843-428f-b802-626e6014bcbf",
        "metadata": {
          "vector_store_key": "2002.01359-4",
          "chunk_id": 22,
          "document_id": "2002.01359",
          "start_idx": 12592,
          "end_idx": 13359
        },
        "page_content": "The 20 domains present across the train, dev and test datasets are listed in Table TABREF10, as are the details regarding which domains are present in each of the datasets. We create synthetic implementations of a total of 45 services or APIs over these domains. Our simulator framework interacts with these services to generate dialogue outlines, which are structured representations of dialogue semantics. We then use a crowd-sourcing procedure to paraphrase these outlines to natural language utterances. Our novel crowd-sourcing procedure preserves all annotations obtained from the simulator and does not require any extra annotations after dialogue collection. In this section, we describe these steps briefly and then present analyses of the collected dataset.",
        "type": "Document"
      },
      {
        "id": "2e76af14-b064-4501-b27a-103fd6bbbeaf",
        "metadata": {
          "vector_store_key": "1907.11499-3",
          "chunk_id": 12,
          "document_id": "1907.11499",
          "start_idx": 6293,
          "end_idx": 7203
        },
        "page_content": "For example, the definition of the \u201cLifestyle\u201d domain is \u201cthe interests, opinions, behaviors, and behavioral orientations of an individual, group, or culture\u201d. Figure FIGREF5 provides an overview of our Domain Detection Network, which we call DetNet. The model comprises two modules; an encoder learns representations for words and sentences whilst incorporating prior domain information; a detector generates domain scores for words, sentences, and documents by selectively attending to previously encoded information. We describe the two modules in more detail below. We learn representations for words and sentences using identical encoders with separate learning parameters. Given a document, the two encoders implement the following steps: INLINEFORM0   For each sentence INLINEFORM0 , the word-level encoder yields contextualized word representations INLINEFORM1 and their attention weights INLINEFORM2 .",
        "type": "Document"
      },
      {
        "id": "c07c3f70-286e-4184-b00d-d192e9886970",
        "metadata": {
          "vector_store_key": "2002.11893-4",
          "chunk_id": 12,
          "document_id": "2002.11893",
          "start_idx": 7300,
          "end_idx": 8009
        },
        "page_content": "Our corpus is to simulate scenarios where a traveler seeks tourism information and plans her or his travel in Beijing. Domains include hotel, attraction, restaurant, metro, and taxi. The data collection process is summarized as below: Database Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database. For the taxi domain, there is no need to store the information. Instead, we can call the API directly if necessary. Goal Generation: a multi-domain goal generator was designed based on the database.",
        "type": "Document"
      },
      {
        "id": "ed2b65a3-293a-4a8c-874e-949c480c257f",
        "metadata": {
          "vector_store_key": "2002.11893-4",
          "chunk_id": 13,
          "document_id": "2002.11893",
          "start_idx": 8009,
          "end_idx": 8625
        },
        "page_content": "Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context. To make workers understand the task more easily, we crafted templates to generate natural language descriptions for each structured goal. Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality.",
        "type": "Document"
      },
      {
        "id": "63424b33-ce8f-4d5e-94ed-403d702844ba",
        "metadata": {
          "vector_store_key": "1907.11499-2",
          "chunk_id": 18,
          "document_id": "1907.11499",
          "start_idx": 10010,
          "end_idx": 10650
        },
        "page_content": "For this, we created a third test set from the New York Times, in addition to our Wikipedia-based English and Chinese datasets. For all three corpora, we randomly sampled two documents for each domain, and then from each document, we sampled one long paragraph or a few consecutive short paragraphs containing 8\u201312 sentences. Amazon Mechanical Turkers were asked to read these sentences and assign a domain based on the seven labels used in this paper (multiple labels were allowed). Participants were provided with domain definitions. We obtained five annotations per sentence and adopted the majority label as the sentence's domain label.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many texts/datapoints are in the SemEval, TASS and SENTIPOLC datasets?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "c5f16988-f9bb-43d0-a24e-a2d52ae2d42a",
        "metadata": {
          "vector_store_key": "1802.05574-4",
          "chunk_id": 11,
          "document_id": "1802.05574",
          "start_idx": 6530,
          "end_idx": 7227
        },
        "page_content": "This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available and the corpus provides both an XML and a simple text version of each article. We randomly selected 2 sentences with more than two words from each paper using the simple text version of the paper.",
        "type": "Document"
      },
      {
        "id": "8eda3794-6a7e-4b10-9199-08b35489b3d9",
        "metadata": {
          "vector_store_key": "1911.12569-2",
          "chunk_id": 24,
          "document_id": "1911.12569",
          "start_idx": 14526,
          "end_idx": 15247
        },
        "page_content": "The SemEval 2016 corpus contains tweets which are classified into positive, negative or other. It contains 2,914 training and 1,956 test instances. The SSEC corpus is annotated with anger, anticipation, disgust, fear, joy, sadness, surprise and trust labels. Each tweet could belong to one or more emotion classes and one sentiment class. Table TABREF15 shows the data statistics of SemEval 2016 task 6 and SSEC which are used for sentiment and emotion analysis, respectively. The SemEval 2016 task 6 corpus contains tweets from Twitter. Since the tweets are derived from an environment with the constraint on the number of characters, there is an inherent problem of word concatenation, contractions and use of hashtags.",
        "type": "Document"
      },
      {
        "id": "731d76b9-6365-4a8c-872e-c14badc3b626",
        "metadata": {
          "vector_store_key": "1911.07228-0",
          "chunk_id": 12,
          "document_id": "1911.07228",
          "start_idx": 6115,
          "end_idx": 6702
        },
        "page_content": "The dataset contains four different types of label: Location (LOC), Person (PER), Organization (ORG) and Miscellaneous - Name of an entity that do not belong to 3 types above (Table TABREF15). Although the corpus has more information about the POS and chunks, but we do not use them as features in our model. There are two folders with 267 text files of training data and 45 text files of test data. They all have their own format. We take 21 first text files and 22 last text files and 22 sentences of the 22th text file and 55 sentences of the 245th text file to be a development data.",
        "type": "Document"
      },
      {
        "id": "04a7c8d6-3fb1-4ab3-95ad-2b94df9c2bd8",
        "metadata": {
          "vector_store_key": "1612.05270-4",
          "chunk_id": 33,
          "document_id": "1612.05270",
          "start_idx": 19301,
          "end_idx": 19987
        },
        "page_content": "This can be understood because the rules of SemEval promote the use of external knowledge. Table TABREF24 compares our performance on five different languages; we do not apply language-dependent techniques. For each comparison, we took a labeled corpus from BIBREF3 (Arabic) and BIBREF21 (the remaining languages). According to author's reports, all tweets were manually labeled by native speakers as pos, neg, or neu. The Arabic dataset contains INLINEFORM0 items; the other datasets contain from 58 thousand tweets to more than 157 thousand tweets. We were able to fetch a fraction of the original datasets; so, we drop the necessary items to hold the original class-population ratio.",
        "type": "Document"
      },
      {
        "id": "a5dbbbf1-2283-487d-89f4-f5cbdfec92c4",
        "metadata": {
          "vector_store_key": "1905.06566-1",
          "chunk_id": 31,
          "document_id": "1905.06566",
          "start_idx": 17089,
          "end_idx": 17889
        },
        "page_content": "For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from New York Times dataset. We used the same training/validation/test splits as in xu:2019:arxiv, which contain 137,778 documents for training, 17,222 for validation and 17,223 for test. To create sentence level labels for extractive summarization, we used a strategy similar to nallapati:2017:aaai. We label the subset of sentences in a document that maximizes Rouge BIBREF39 (against the human summary) as True and all other sentences as False.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "In which languages did the approach outperform the reported results?",
    "llm_answer": "English, French, Spanish, Italian, Arabic, German, Portuguese, Russian, and Swedish.\n",
    "context": [
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      },
      {
        "id": "d2e2382c-8e5a-4f0c-92c5-2f119e6e1178",
        "metadata": {
          "vector_store_key": "1910.07481-0",
          "chunk_id": 20,
          "document_id": "1910.07481",
          "start_idx": 11787,
          "end_idx": 12518
        },
        "page_content": "It also surpassed the Baselinee by 0.18 points on the newstest2017 with strong statistical significance, and by 0.15 BLEU points on the newstest2018 but this time with no statistical evidence. These encouraging results prompted us to extend experiments to another language pair: English-French. En$\\rightarrow $Fr: The Document system obtained the best results considering all metrics on all test sets with strong statistical evidence. It surpassed the Baseline by 1.09 BLEU points and 0.85 TER points on tst2015, 0.75 BLEU points and 0.76 TER points on tst2014, and 0.48 BLEU points and 0.68 TER points on tst2013. Fr$\\rightarrow $En: Of all experiments, this language pair shows the most important improvements over the Baseline.",
        "type": "Document"
      },
      {
        "id": "1e9a203f-4f10-4083-b714-495e3ee467a6",
        "metadata": {
          "vector_store_key": "1612.05270-2",
          "chunk_id": 3,
          "document_id": "1612.05270",
          "start_idx": 2007,
          "end_idx": 2743
        },
        "page_content": "We compare our approach ranking in three international contests: TASS'15, SemEval'15-16 and SENTIPOLC'14, for Spanish, English and Italian respectively; the remaining languages are compared directly with the results reported in the literature. The experimental results locate our approach in good positions for all considered competitions; and excellent results in the other five languages tested. Finally, even when our method is almost cross-language, it can be extended to take advantage of language dependencies; we also provide experimental evidence of the advantages of using these language-dependent techniques. The rest of the manuscript is organized as follows. Section SECREF2 describes our proposed Sentiment Analysis method.",
        "type": "Document"
      },
      {
        "id": "fe08e4eb-b77c-4d8d-91b5-4813b6cc1d06",
        "metadata": {
          "vector_store_key": "1612.05270-2",
          "chunk_id": 2,
          "document_id": "1612.05270",
          "start_idx": 1203,
          "end_idx": 2007
        },
        "page_content": "Furthermore, our second contribution is to create a well-performing baseline to compare new sentiment classifiers in a broad range of languages or to bootstrap new sentiment analysis systems. Our approach is based on selecting the best text-transforming techniques that optimize some performance measures where the chosen techniques are robust to typical writing errors. In this context, we propose a robust multilingual sentiment analysis method, tested in eight different languages: Spanish, English, Italian, Arabic, German, Portuguese, Russian and Swedish. We compare our approach ranking in three international contests: TASS'15, SemEval'15-16 and SENTIPOLC'14, for Spanish, English and Italian respectively; the remaining languages are compared directly with the results reported in the literature.",
        "type": "Document"
      },
      {
        "id": "a440212d-750c-4498-9795-15df3b9daf6d",
        "metadata": {
          "vector_store_key": "1809.04686-1",
          "chunk_id": 25,
          "document_id": "1809.04686",
          "start_idx": 13977,
          "end_idx": 14793
        },
        "page_content": "Table 1 summarizes the accuracy of our proposed system for these three different approaches and the state-of-the-art results on all the tasks. The first row in the table shows the baseline accuracy of our system for all four datasets. The second row shows the result from initializing with a pre-trained multilingual NMT encoder. It can be seen that this provides a significant improvement in accuracy, an average of 4.63%, across all the tasks. This illustrates that the multilingual NMT encoder has successfully learned transferable contextualized representations that are leveraged by the classifier component of our proposed system. These results are in line with the results in BIBREF5 where the authors used the representations from the top NMT encoder layer as an additional input to the task-specific system.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which is the baseline model?",
    "llm_answer": "The context describes multiple baselines, including a general system trained on shared data, a Baseline1 model using bi-LSTMs, and a LSTM baseline using ELMo embeddings.  It doesn't identify a single, overarching \"the\" baseline.\n",
    "context": [
      {
        "id": "fa88df39-033d-4e0f-891b-162e0f9b0424",
        "metadata": {
          "vector_store_key": "1709.08299-4",
          "chunk_id": 19,
          "document_id": "1709.08299",
          "start_idx": 11042,
          "end_idx": 11836
        },
        "page_content": "The neural baselines are built by the following steps. We first use the shared training data to build a general systems, and choose the best performing model (in terms of cloze validation set) as baseline. Use User Query validation data for further tuning the systems with 10-fold cross-validations. Increase dropout rate BIBREF9 to 0.5 for preventing over-fitting issue. All baseline systems are chosen according to the performance of the validation set. The participant system results are given in Table 2 and 3 . As we can see that two neural baselines are competitive among participant systems and AoA Reader successfully outperform AS Reader and all participant systems in single model condition, which proves that it is a strong baseline system even without further fine-tuning procedure.",
        "type": "Document"
      },
      {
        "id": "39cb1ec4-6c9f-4dbd-b781-29fe9d4ad19f",
        "metadata": {
          "vector_store_key": "1912.01046-3",
          "chunk_id": 23,
          "document_id": "1912.01046",
          "start_idx": 13260,
          "end_idx": 13980
        },
        "page_content": "Following this idea, Baseline1 represents a span of sentences by concatenating the vectors of the starting and ending sentences. The left diagram in Fig. FIGREF15 illustrates the Baseline1 model. Model. The model takes two inputs, a transcript, $\\lbrace s_1, s_2, ... s_n\\rbrace $ where $s_i$ are individual sentences and a question, $q$. The output is the span scores, $y$, the scores over all possible spans. GLoVe BIBREF14 is used for the word representations in the transcript and the questions. We use two bi-LSTMs BIBREF15 to encode the transcript. where n is the number of sentences . The output of Passage-level Encoding, $p$, is a sequence of vector, $p_i$, which represents the latent meaning of each sentence.",
        "type": "Document"
      },
      {
        "id": "763bf540-a941-47b1-bdeb-25182eab991e",
        "metadata": {
          "vector_store_key": "2001.11381-4",
          "chunk_id": 55,
          "document_id": "2001.11381",
          "start_idx": 31798,
          "end_idx": 32460
        },
        "page_content": "Tampoco consideramos la utilizaci\u00f3n de un baseline de tipo aleatorio, porque los resultados carecer\u00edan de la homosintaxis y ser\u00eda sumamente f\u00e1cil obtener mejores resultados. Dicho lo anterior, el Modelo 1 podr\u00eda ser considerado como nuestro propio baseline. A continuaci\u00f3n presentamos un protocolo de evaluaci\u00f3n manual de los resultados obtenidos. El experimento consisti\u00f3 en la generaci\u00f3n de 15 frases por cada uno de los tres modelos propuestos. Para cada modelo, se consideraron tres queries: $Q=$ {AMOR, GUERRA, SOL}, generando 5 frases con cada uno. Las 15 frases fueron mezcladas entre s\u00ed y reagrupadas por queries, antes de presentarlas a los evaluadores.",
        "type": "Document"
      },
      {
        "id": "fcbb237d-302f-45b0-886a-31ec58fabb9f",
        "metadata": {
          "vector_store_key": "1909.09484-4",
          "chunk_id": 35,
          "document_id": "1909.09484",
          "start_idx": 19125,
          "end_idx": 19742
        },
        "page_content": "An Adam optimizer BIBREF22 is used for training our models and baselines, with a learning rate of 0.001 for supervised training and 0.0001 for reinforcement learning. In reinforcement learning, the decay parameter $\\lambda $ is set to 0.8. The weight decay is set to 0.001. And early stopping is performed on developing set. The experimental results of the proposed model and baselines will be analyzed from the following aspects. BPRA Results: As shown in Table TABREF35, most of the models have similar performance on BPRA on these two datasets, which can guarantee a consistent impact on the dialogue policy maker.",
        "type": "Document"
      },
      {
        "id": "5e76f24e-4c75-4a34-b69c-7fe1f8d12771",
        "metadata": {
          "vector_store_key": "1909.02635-2",
          "chunk_id": 15,
          "document_id": "1909.02635",
          "start_idx": 9069,
          "end_idx": 9719
        },
        "page_content": "This includes (i) Majority Class, (ii) Exact Match of an ingredient $e$ in recipe step $s_t$, and (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo$_{token}$ and ELMo$_{sent}$) BIBREF22 compared to GPT.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the main contribution of the paper? ",
    "llm_answer": "The paper demonstrates how text analytic approaches can help understand different aspects of global governance, highlighting international development issues prioritized by governments in the UN and key country-specific factors associated with discussing development issues.\n",
    "context": [
      {
        "id": "e900003c-e4a4-4760-af29-1415e5bd0e01",
        "metadata": {
          "vector_store_key": "1708.05873-0",
          "chunk_id": 35,
          "document_id": "1708.05873",
          "start_idx": 20206,
          "end_idx": 20343
        },
        "page_content": "The paper more broadly demonstrates how text analytic approaches can help us to better understand different aspects of global governance.",
        "type": "Document"
      },
      {
        "id": "316d151a-06ef-4dc6-bf85-da0eb956ffd7",
        "metadata": {
          "vector_store_key": "1904.08386-4",
          "chunk_id": 17,
          "document_id": "1904.08386",
          "start_idx": 10140,
          "end_idx": 10411
        },
        "page_content": "Additionally, we thank Nader Akoury, Garrett Bernstein, Chenghao Lv, Ari Kobren, Kalpesh Krishna, Saumya Lal, Tu Vu, Zhichao Yang, Mengxue Zhang and the UMass NLP group for suggestions that improved the paper's clarity, coverage of related work, and analysis experiments.",
        "type": "Document"
      },
      {
        "id": "482acd0f-398b-47d3-9b79-e01b3396adec",
        "metadata": {
          "vector_store_key": "1908.08419-1",
          "chunk_id": 6,
          "document_id": "1908.08419",
          "start_idx": 3477,
          "end_idx": 4155
        },
        "page_content": "To sum up, the main contributions of our work are summarized as follows: The rest of this paper is organized as follows. Section SECREF2 briefly reviews the related work on CWS and active learning. Section SECREF3 presents an active learning method for CWS. We experimentally evaluate our proposed method in Section SECREF4 . Finally, Section SECREF5 concludes the paper and envisions on future work. In past decades, researches on CWS have a long history and various methods have been proposed BIBREF13 , BIBREF14 , BIBREF15 , which is an important task for Chinese NLP BIBREF7 . These methods are mainly focus on two categories: supervised learning and deep learning BIBREF2 .",
        "type": "Document"
      },
      {
        "id": "86603ed1-ec07-4ecd-8822-e85593c7e4de",
        "metadata": {
          "vector_store_key": "2002.11268-2",
          "chunk_id": 39,
          "document_id": "2002.11268",
          "start_idx": 21048,
          "end_idx": 21138
        },
        "page_content": "The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "4601b764-90a4-4982-aa25-e18e8f659496",
        "metadata": {
          "vector_store_key": "1708.05873-0",
          "chunk_id": 6,
          "document_id": "1708.05873",
          "start_idx": 3307,
          "end_idx": 3967
        },
        "page_content": "The paper makes two contributions using this approach: (1) It sheds light on the main international development issues that governments prioritise in the UN; and (2) It identifies the key country-specific factors associated with governments discussing development issues in their GD statements. In the analysis we consider the nature of international development issues raised in the UN General Debates, and the effect of structural covariates on the level of developmental rhetoric in the GD statements. To do this, we first implement a structural topic model BIBREF4 . This enables us to identify the key international development topics discussed in the GD.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they get the formal languages?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "c7d343d1-f033-46c4-b03f-17dac9615d98",
        "metadata": {
          "vector_store_key": "1910.07481-0",
          "chunk_id": 14,
          "document_id": "1910.07481",
          "start_idx": 8370,
          "end_idx": 9086
        },
        "page_content": "We evaluate the English-German systems on newstest2017, newstest2018 and newstest2019 where documents consist of newspaper articles to keep consistency with the training data. English to French and French to English systems are evaluated over IWSLT TED tst2013, tst2014 and tst2015 where documents are transcriptions of TED conferences (see Table TABREF5). Prior to experiments, corpora are tokenized using Moses tokenizer BIBREF19. To limit vocabulary size, we adopt the BPE subword unit approach BIBREF20, through the SentencePiece toolkit BIBREF21, with 32K rules. We use the OpenNMT framework BIBREF22 in its TensorFlow version to create and train our models. All experiments are run on a single NVIDIA V100 GPU.",
        "type": "Document"
      },
      {
        "id": "da8aca90-472e-4c4c-a3a9-8a49a97a8227",
        "metadata": {
          "vector_store_key": "1811.01001-4",
          "chunk_id": 13,
          "document_id": "1811.01001",
          "start_idx": 7246,
          "end_idx": 7935
        },
        "page_content": "Given only a small fraction of samples in a formal language, with values of $n$ (and $m$ ) ranging from 1 to a certain training threshold $N$ , they trained an LSTM model until its full convergence on the training set and then tested it on a more generalized set. They showed that their LSTM model outperformed the previous approaches in capturing and generalizing the aforementioned formal languages. By analyzing the cell states and the activations of the gates in their LSTM model, they further demonstrated that the network learns how to count up and down at certain places in the sample sequences to encode information about the underlying structure of each of these formal languages.",
        "type": "Document"
      },
      {
        "id": "3a2c2a69-2b71-452a-9e00-5fff1abfc71a",
        "metadata": {
          "vector_store_key": "1911.03894-1",
          "chunk_id": 2,
          "document_id": "1911.03894",
          "start_idx": 1257,
          "end_idx": 2052
        },
        "page_content": "Even though multilingual models give remarkable results, they are often larger and their results still lag behind their monolingual counterparts BIBREF12. This is particularly inconvenient as it hinders their practical use in NLP systems as well as the investigation of their language modeling capacity, something that remains to be investigated in the case of, for instance, morphologically rich languages. We take advantage of the newly available multilingual corpus OSCAR BIBREF13 and train a monolingual language model for French using the RoBERTa architecture. We pretrain the model - which we dub CamemBERT- and evaluate it in four different downstream tasks for French: part-of-speech (POS) tagging, dependency parsing, named entity recognition (NER) and natural language inference (NLI).",
        "type": "Document"
      },
      {
        "id": "e3185e10-5c71-4140-a588-bff2d801fe36",
        "metadata": {
          "vector_store_key": "1808.09409-4",
          "chunk_id": 0,
          "document_id": "1808.09409",
          "start_idx": 0,
          "end_idx": 670
        },
        "page_content": "A learner language (interlanguage) is an idiolect developed by a learner of a second or foreign language which may preserve some features of his/her first language. Previously, encouraging results of automatically building the syntactic analysis of learner languages were reported BIBREF0 , but it is still unknown how semantic processing performs, while parsing a learner language (L2) into semantic representations is the foundation of a variety of deeper analysis of learner languages, e.g., automatic essay scoring. In this paper, we study semantic parsing for interlanguage, taking semantic role labeling (SRL) as a case task and learner Chinese as a case language.",
        "type": "Document"
      },
      {
        "id": "1a1b6db9-cd37-4e7f-a353-4c2a516d1dad",
        "metadata": {
          "vector_store_key": "2002.08899-3",
          "chunk_id": 40,
          "document_id": "2002.08899",
          "start_idx": 20994,
          "end_idx": 21684
        },
        "page_content": "These include synchronous grammars BIBREF17, data augmentation BIBREF18, Meta learning BIBREF19, and hard-coded maps or copying capabilities from input to output BIBREF20 BIBREF21. All require hard-coded rules that are often broken by the real world. Neural and cognitive theories provide an imperative for computational models to understand human language by separating representations of word meanings from those of syntax. Using this constraint, we introduced new neural units that can provide this separation for the purpose of translating human languages. When added to an LSTM encoder and decoder, our units showed improvements in all of our experiment domains over the typical model.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is a confusion network or lattice?",
    "llm_answer": "A confusion network (CN) and lattice are graph-like structures where arcs connected by nodes represent hypothesized words.  Each node in a lattice marks a timestamp in an utterance, and each arc represents a hypothesized word with its corresponding acoustic and language model scores.\n",
    "context": [
      {
        "id": "00e60b00-2185-4de3-8e32-ba53bc70fa0a",
        "metadata": {
          "vector_store_key": "1810.13024-1",
          "chunk_id": 14,
          "document_id": "1810.13024",
          "start_idx": 7913,
          "end_idx": 8557
        },
        "page_content": "When evaluating performance on lattices and confusion networks, these metrics are computed across all arcs in the network. A number of important downstream and upstream applications rely on accurate confidence scores in graph-like structures, such as confusion networks (CN) in Fig. 2 and lattices in Fig. 2 , where arcs connected by nodes represent hypothesised words. This section describes an extension of BiRNNs to CNs and lattices. Fig. 2 shows that compared to 1-best sequences in Fig. 2 , each node in a CN may have multiple incoming arcs. Thus, a decision needs to be made on how to optimally propagate information to the outgoing arcs.",
        "type": "Document"
      },
      {
        "id": "491c1e0d-1d59-4eaf-9a2a-7a82ab545bf6",
        "metadata": {
          "vector_store_key": "1810.13024-1",
          "chunk_id": 3,
          "document_id": "1810.13024",
          "start_idx": 2015,
          "end_idx": 2805
        },
        "page_content": "Extending confidence estimation to confusion network and lattice structures can be straightforward for some approaches, such as decision trees, and challenging for others, such as recurrent forms of neural networks. The previous work on encoding graph structures into neural networks BIBREF13 has mostly focused on embedding lattices into a fixed dimensional vector representation BIBREF14 , BIBREF15 . This paper examines a particular example of extending a bi-directional recurrent neural network (BiRNN) BIBREF16 to confusion network and lattice structures. This requires specifying how BiRNN states are propagated in the forward and backward directions, how to merge a variable number of BiRNN states, and how target confidence values are assigned to confusion network and lattice arcs.",
        "type": "Document"
      },
      {
        "id": "a5b8aa04-0791-43d1-b976-aed7738d38c1",
        "metadata": {
          "vector_store_key": "1810.13024-2",
          "chunk_id": 20,
          "document_id": "1810.13024",
          "start_idx": 11974,
          "end_idx": 12720
        },
        "page_content": "As illustrated in Fig. 2 , each node in a lattice marks a timestamp in an utterance and each arc represents a hypothesised word with its corresponding acoustic and language model scores. Although lattices do not normally obey a linear graph structure, if they are traversed in the topological order, no changes are required to compute confidences over lattice structures. The way the information is propagated in these graph structures is similar to the forward-backward algorithm BIBREF17 . There, the forward probability at time $t$ is  $$\\overrightarrow{h}_{t+1}^{(i)} = \\overrightarrow{h}_{t} x_{t+1}^{(i)}, \\;\\text{where } \\overrightarrow{h}_{t} = \\sum _{j} \\alpha _{i,j} \\overrightarrow{h}_{t}^{(j)}$$   (Eq. 20)  Compared to equations Eqn.",
        "type": "Document"
      },
      {
        "id": "f36b6e2d-1b95-4743-b359-8f7824f50c9d",
        "metadata": {
          "vector_store_key": "1902.09087-6",
          "chunk_id": 49,
          "document_id": "1902.09087",
          "start_idx": 27352,
          "end_idx": 28157
        },
        "page_content": "But without the help of words, it is hard to distinguish the \u201cRen\u201d(people) in \u201cDuoShaoRen\u201d(how many people) and \u201cChuangShiRen\u201d(founder), so it loses the most important information. While in lattice, although overlaps are limited, \u201cWangZhan\u201d(website, \u201cWang\u201d web, \u201cZhan\u201d station) can match \u201cWangZhi\u201d(Internet addresses, \u201cWang\u201d web, \u201cZhi\u201d addresses) and also relate to \u201cDaoHang\u201d(navigate), from which it may infer that \u201cWangZhan\u201d(website) refers to \u201ctao606 seller website navigation\u201d(a website name). Moreover, \u201cYongHu\u201d(user) can match \u201cRen\u201d(people). With cooperations between characters and words, it catches the key points of the question and eliminates the other two candidates, as a result, it finds the correct answer. Deep learning models have been widely adopted in natural language sentence matching.",
        "type": "Document"
      },
      {
        "id": "9549fd45-4abf-4b9b-9589-61c74edbf57f",
        "metadata": {
          "vector_store_key": "1902.09087-6",
          "chunk_id": 54,
          "document_id": "1902.09087",
          "start_idx": 30394,
          "end_idx": 31378
        },
        "page_content": "Rather than relying on a word sequence only, our model takes word lattice as input. By performing CNNs over multiple n-gram context to exploit multi-granularity information, LCNs can relieve the word mismatch challenges. Thorough experiments show that our model can better explore the word lattice via convolutional operations and rich context-aware pooling, thus outperforms the state-of-the-art models and competitive baselines by a large margin. Further analyses exhibit that lattice input takes advantages of word and character level information, and the vocabulary based lattice constructor outperforms the strategies that combine characters and different word segmentations together. This work is supported by Natural Science Foundation of China (Grant No. 61672057, 61672058, 61872294); the UK Engineering and Physical Sciences Research Council under grants EP/M01567X/1 (SANDeRs) and EP/M015793/1 (DIVIDEND); and the Royal Society International Collaboration Grant (IE161012).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How close do clusters match to ground truth tone categories?",
    "llm_answer": "In Mandarin, the clusters closely match the ground truth tone categories.  In Cantonese, the model found 5 clusters, which is one fewer than the 6 phonemic tones.\n",
    "context": [
      {
        "id": "e009daf9-0234-445b-9c61-0f825437c0c5",
        "metadata": {
          "vector_store_key": "1910.08987-0",
          "chunk_id": 17,
          "document_id": "1910.08987",
          "start_idx": 9796,
          "end_idx": 10431
        },
        "page_content": "In Mandarin, the 4 clusters correspond very well with the the 4 phonemic tone categories, and the generated contours closely match the ground truth in Figure FIGREF2. There is some overlap between tones 3 and 4; this is because tone 3 is sometimes realized a low-falling tone without the final rise, a process known as half T3 sandhi BIBREF19, thus, it may overlap with tone 4 (falling tone). In Cantonese, the 5 clusters A-E correspond to low-falling, mid-level, high-level, mid-rising, and low-rising tones. Tone clustering in Cantonese is expected to be more difficult than in Mandarin because of 6 contrastive tones, rather than 4.",
        "type": "Document"
      },
      {
        "id": "c2b6b69b-6968-4167-8fbf-a67fc8d7a25e",
        "metadata": {
          "vector_store_key": "1910.08987-0",
          "chunk_id": 20,
          "document_id": "1910.08987",
          "start_idx": 11182,
          "end_idx": 11851
        },
        "page_content": "This is especially a problem in Cantonese, which has tones that differ only on pitch level and not contour: for example, a mid-level tone near the end of a phrase may have the same absolute pitch as a low-level tone at the start of a phrase. To test this hypothesis, we evaluate the model on only the first syllable of every word, which eliminates carry-over and declination effects (Table TABREF14). In both Mandarin and Cantonese, the clustering is more accurate when using only the first syllables, compared to using all of the syllables. We propose a model for unsupervised clustering and discovery of phonemic tones in tonal languages, using spoken words as input.",
        "type": "Document"
      },
      {
        "id": "84854a3a-dd69-43a5-9157-973072de15db",
        "metadata": {
          "vector_store_key": "1910.08987-1",
          "chunk_id": 16,
          "document_id": "1910.08987",
          "start_idx": 9034,
          "end_idx": 9796
        },
        "page_content": "Thus, we take only clusters larger than a threshold, determined empirically from the distribution of cluster sizes; the rest are considered spurious clusters and we treat them as unclustered. Finally, we feed the remaining cluster centers into the decoder to generate a prototype pitch contour for each cluster. Figure FIGREF9 shows the latent space learned by the autoencoders and the clustering output. Our model found 4 tone clusters in Mandarin, matching the number of phonemic tones (Table TABREF12) and 5 in Cantonese, which is one fewer than the number of phonemic tones (Table TABREF13). In Mandarin, the 4 clusters correspond very well with the the 4 phonemic tone categories, and the generated contours closely match the ground truth in Figure FIGREF2.",
        "type": "Document"
      },
      {
        "id": "58f82a9b-05bf-4f6f-b09d-faf0bd7c1536",
        "metadata": {
          "vector_store_key": "1710.10609-3",
          "chunk_id": 33,
          "document_id": "1710.10609",
          "start_idx": 18362,
          "end_idx": 19040
        },
        "page_content": "The initialization of centroids was performed using INLINEFORM2 sampling approach BIBREF14 . For the clusters so obtained we have computed F1 and ARI measures as before and compared with the K-means approach. We used the partitioning formed by the 8 categories (from which the utterance pairs were selected) as the ground truth clustering. Table TABREF20 summarizes the results. We observe that for K-means algorithm, the vectors generated from sequence to sequence model perform better than the vectors generated using paragraph to vector for both the domains. This is expected as the vectors generated from sequence to sequence model encode some adjacency information as well.",
        "type": "Document"
      },
      {
        "id": "9e70f221-27e6-4465-8f48-36518f689593",
        "metadata": {
          "vector_store_key": "1909.00183-1",
          "chunk_id": 48,
          "document_id": "1909.00183",
          "start_idx": 26767,
          "end_idx": 27419
        },
        "page_content": "Although our dataset has a classification hand-coded by a human operator, we do not use it in our analysis. Indeed, one of our aims is to explore the relevance of the fixed external classes as compared to content-driven groupings obtained in an unsupervised manner. Therefore we provide a double route to quantify the quality of the clusters by computing two complementary measures: (i) an intrinsic measure of topic coherence, and (ii) a measure of similarity to the external hand-coded categories. Topic coherence of text: As an intrinsic measure of consistency of word association, we use the pointwise mutual information ($PMI$) BIBREF19, BIBREF47.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what are the evaluation metrics?",
    "llm_answer": "Manual evaluation, automatic evaluation metrics (ROUGE, BLEU, METEOR), task-based evaluation;  Task completion ratio, User satisfaction degree, Response fluency, Number of dialogue turns, BPRA (Belief Per-Response Accuracy);  number of turns in a dialogue, ratio of task completion.\n",
    "context": [
      {
        "id": "61e54692-2da3-41fb-b1b0-c3469ffa76bf",
        "metadata": {
          "vector_store_key": "1607.06025-0",
          "chunk_id": 17,
          "document_id": "1607.06025",
          "start_idx": 10067,
          "end_idx": 10777
        },
        "page_content": "These attributes are conditional information that is fed to the models, like the discrete label is in our models. As recognized by BIBREF37 , the evaluation metrics of text-generating models fall into three categories: manual evaluation, automatic evaluation metrics, task-based evaluation. In evaluation based on human judgment each generated textual example is inspected manually. The automatic evaluation metrics, like ROUGE, BLEU and METEOR, compare human texts and generated texts. BIBREF38 shows METEOR has the strongest correlation with human judgments in image description evaluation. The last category is task-based evaluation, where the impact of the generated texts on a particular task is measured.",
        "type": "Document"
      },
      {
        "id": "8c81c383-8e40-4094-8e18-ab0a2d32126a",
        "metadata": {
          "vector_store_key": "1905.08949-6",
          "chunk_id": 18,
          "document_id": "1905.08949",
          "start_idx": 10340,
          "end_idx": 11074
        },
        "page_content": "Although the datasets are commonly shared between QG and QA, it is not the case for evaluation: it is challenging to define a gold standard of proper questions to ask. Meaningful, syntactically correct, semantically sound and natural are all useful criteria, yet they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks. As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE BIBREF43 , are also widely used.",
        "type": "Document"
      },
      {
        "id": "ab676abe-1fd3-450a-a82d-153595442021",
        "metadata": {
          "vector_store_key": "1709.10217-1",
          "chunk_id": 11,
          "document_id": "1709.10217",
          "start_idx": 6267,
          "end_idx": 6922
        },
        "page_content": "For building the dialogue systems of participants, we release an example set of complete user intent and three data files of flight, train and hotel in JSON format. There are five evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the number of total tasks. User satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied, neutral, satisfied and very satisfied, respectively. Response fluency: There are three scores -1, 0, 1, which indicate nonfluency, neutral, fluency. Number of dialogue turns: The number of utterances in a task-completed dialogue.",
        "type": "Document"
      },
      {
        "id": "fa230c6a-9ae2-4313-8be2-bde2493abce6",
        "metadata": {
          "vector_store_key": "1909.09484-4",
          "chunk_id": 29,
          "document_id": "1909.09484",
          "start_idx": 15790,
          "end_idx": 16412
        },
        "page_content": "We evaluate the performance of the proposed model in three aspects: (1) the accuracy of the dialogue state tracker, it aims to show the impact of the dialogue state tracker on the dialogue policy maker; (2) the accuracy of dialogue policy maker, it aims to explain the performance of different methods of constructing dialogue policy; (3) the quality of the final response, it aims to explain the impact of the dialogue policy on the final dialogue response. The evaluation metrics are listed as follows: BPRA: Belief Per-Response Accuracy (BPRA) tests the ability to generate the correct user intents during the dialogue.",
        "type": "Document"
      },
      {
        "id": "74a8f350-1cdc-4b68-87aa-e719da16ac2c",
        "metadata": {
          "vector_store_key": "1709.10217-3",
          "chunk_id": 4,
          "document_id": "1709.10217",
          "start_idx": 2049,
          "end_idx": 2735
        },
        "page_content": "For the task-oriented system, although there are some objective evaluation metrics, such as the number of turns in a dialogue, the ratio of task completion, etc., there is no gold standard for automatically evaluating two (or more) dialogue systems when considering the satisfaction of the human and the fluency of the generated dialogue. To promote the development of the evaluation technology for dialogue systems, especially considering the language characteristics of Chinese, we organize the first evaluation of Chinese human-computer dialogue technology. In this paper, we will present the evaluation scheme and the released corpus in detail. The rest of this paper is as follows.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "which datasets were used in evaluation?",
    "llm_answer": "Laptop, Restaurant, CMRC-2017 (Cloze and User Query Tracks), avsd.\n",
    "context": [
      {
        "id": "1b887290-2c77-48b6-b1a2-fc513d707afc",
        "metadata": {
          "vector_store_key": "1909.00124-1",
          "chunk_id": 20,
          "document_id": "1909.00124",
          "start_idx": 11467,
          "end_idx": 12190
        },
        "page_content": "The other two datasets are laptop and restaurant datasets collected from SemEval-2016 . The former consists of laptop review sentences and the latter consists of restaurant review sentences. The original datasets (i.e., Laptop and Restaurant) were annotated with aspect polarity in each sentence. We used all sentences with only one polarity (positive or negative) for their aspects. That is, we only used sentences with aspects having the same sentiment label in each sentence. Thus, the sentiment of each aspect gives the ground-truth as the sentiments of all aspects are the same. For each clean-labeled dataset, the sentences are randomly partitioned into training set and test set with $80\\%$ and $20\\%$, respectively.",
        "type": "Document"
      },
      {
        "id": "c266479e-4dd5-481c-80c7-22f460a342f6",
        "metadata": {
          "vector_store_key": "1709.08299-1",
          "chunk_id": 9,
          "document_id": "1709.08299",
          "start_idx": 5611,
          "end_idx": 6386
        },
        "page_content": "The proposed dataset is typically used for the 1st Evaluation on Chinese Machine Reading Comprehension (CMRC-2017), which aims to provide a communication platform to the Chinese communities in the related fields. In this evaluation, we provide two tracks. We provide a shared training data for both tracks and separated evaluation data. Cloze Track: In this track, the participants are required to use the large-scale training data to train their cloze system and evaluate on the cloze evaluation track, where training and test set are exactly the same type. User Query Track: This track is designed for using transfer learning or domain adaptation to minimize the gap between cloze training data and user query evaluation data, i.e. training and testing is fairly different.",
        "type": "Document"
      },
      {
        "id": "b8ca75dd-84d9-4514-9a0b-ed8771d04dfc",
        "metadata": {
          "vector_store_key": "1605.08675-1",
          "chunk_id": 90,
          "document_id": "1605.08675",
          "start_idx": 49692,
          "end_idx": 50451
        },
        "page_content": "However, because of the small difference between the techniques including title, for the sake of simplicity, the single sentence is used in the final evaluation. To impose a realistic challenge to the system, the evaluation set, used at this stage, substantially differs from the one used during the development (see section SECREF80 ). A configuration for the final evaluation has been prepared based on results of the experiments. All of the tested versions share the following features: no question analysis corrections, question classification and query generation solutions which proved best in the previous experiments (see section SECREF18 ), a retrieved set of documents including 20 articles, no minimal confidence, singe sentence context with title.",
        "type": "Document"
      },
      {
        "id": "b7e2d393-7a9c-415c-8ed4-e25103a1870e",
        "metadata": {
          "vector_store_key": "1709.08299-2",
          "chunk_id": 23,
          "document_id": "1709.08299",
          "start_idx": 13308,
          "end_idx": 13983
        },
        "page_content": "We hope the release of the full dataset (including hidden test set) could help the participants have a better knowledge of their systems and encourage more researchers to do experiments on. We would like to thank the anonymous reviewers for their thorough reviewing and providing thoughtful comments to improve our paper. We thank the Sixteenth China National Conference on Computational Linguistics (CCL 2017) and Nanjing Normal University for providing space for evaluation workshop. Also we want to thank our resource team for annotating and verifying evaluation data. This work was supported by the National 863 Leading Technology Research Project via grant 2015AA015409.",
        "type": "Document"
      },
      {
        "id": "73a83f60-b2c9-406d-b4cd-5720639e5f06",
        "metadata": {
          "vector_store_key": "1812.07023-4",
          "chunk_id": 18,
          "document_id": "1812.07023",
          "start_idx": 10358,
          "end_idx": 10986
        },
        "page_content": "After 10 such qa turns, the questioner wraps up by writing a summary of the video based on the conversation. Dataset statistics such as the number of dialogues, turns, and words for the avsd dataset are presented in Table TABREF5 . For the initially released prototype dataset, the training set of the avsd dataset corresponds to videos taken from the training set of the Charades dataset while the validation and test sets of the avsd dataset correspond to videos taken from the validation set of the Charades dataset. For the official dataset, training, validation and test sets are drawn from the corresponding Charades sets.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what are the baselines?",
    "llm_answer": "The baselines include:  neural baselines, ANRES_Sing_R, ANRES_Sing_Z, ANRES_Seq_R, ANRES_Seq_Z, i-vector system, x-vector system, Sliding Window (SW), Sliding Window + Distance (SW+D), 3, 4, and 5-gram language models, and a bidirectional Recurrent Neural Network (RNN) language model.\n",
    "context": [
      {
        "id": "fa88df39-033d-4e0f-891b-162e0f9b0424",
        "metadata": {
          "vector_store_key": "1709.08299-4",
          "chunk_id": 19,
          "document_id": "1709.08299",
          "start_idx": 11042,
          "end_idx": 11836
        },
        "page_content": "The neural baselines are built by the following steps. We first use the shared training data to build a general systems, and choose the best performing model (in terms of cloze validation set) as baseline. Use User Query validation data for further tuning the systems with 10-fold cross-validations. Increase dropout rate BIBREF9 to 0.5 for preventing over-fitting issue. All baseline systems are chosen according to the performance of the validation set. The participant system results are given in Table 2 and 3 . As we can see that two neural baselines are competitive among participant systems and AoA Reader successfully outperform AS Reader and all participant systems in single model condition, which proves that it is a strong baseline system even without further fine-tuning procedure.",
        "type": "Document"
      },
      {
        "id": "348b5dbb-b442-4f57-9f6e-c2038d069875",
        "metadata": {
          "vector_store_key": "1709.08299-4",
          "chunk_id": 16,
          "document_id": "1709.08299",
          "start_idx": 9232,
          "end_idx": 9901
        },
        "page_content": "The paragraph should be read carefully and judged whether appropriate for asking questions No more than 5 questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models.",
        "type": "Document"
      },
      {
        "id": "64beee2a-a1a9-454b-917e-917a34317fa7",
        "metadata": {
          "vector_store_key": "2004.01878-3",
          "chunk_id": 27,
          "document_id": "2004.01878",
          "start_idx": 14569,
          "end_idx": 15289
        },
        "page_content": "In summary, the following four baselines are designed: ANRES_Sing_R: randomly initializing the states for each single trading day. ANRES_Sing_Z: initializing the states as zeros for each single trading day. ANRES_Seq_R: randomly initializing the first states for each trading sequence only. ANRES_Seq_Z: initializing the first states as zeros for each trading sequence only. Development set results on predicting S&P 500 index are shown in Table TABREF13. We can see that modeling recurrent value sequences performs better than treating each trading day separately, which shows that modeling trading sequences can capture the correlations between trading days and the non-linear compositional effects of multiple events.",
        "type": "Document"
      },
      {
        "id": "02bef31f-a2b5-4b89-8063-62dc0b0f91eb",
        "metadata": {
          "vector_store_key": "1911.01799-4",
          "chunk_id": 19,
          "document_id": "1911.01799",
          "start_idx": 11188,
          "end_idx": 11946
        },
        "page_content": "Two state-of-the-art baseline systems were built following the Kaldi SITW recipe BIBREF22: an i-vector system BIBREF3 and an x-vector system BIBREF10. For the i-vector system, the acoustic feature involved 24-dimensional MFCCs plus the log energy, augmented by the first- and second-order derivatives. We also applied the cepstral mean normalization (CMN) and the energy-based voice active detection (VAD). The universal background model (UBM) consisted of $2,048$ Gaussian components, and the dimensionality of the i-vector space was 400. LDA was applied to reduce the dimensionality of the i-vectors to 150. The PLDA model was used for scoring BIBREF4. For the x-vector system, the feature-learning component was a 5-layer time-delay neural network (TDNN).",
        "type": "Document"
      },
      {
        "id": "ddbf653d-2b06-40f5-b102-1128cf41b68a",
        "metadata": {
          "vector_store_key": "1707.03904-0",
          "chunk_id": 31,
          "document_id": "1707.03904",
          "start_idx": 17195,
          "end_idx": 17895
        },
        "page_content": "For Quasar-T we also test the Sliding Window (SW) and Sliding Window + Distance (SW+D) baselines proposed in BIBREF13 . The scores were computed for the list of candidate solutions described in Section \"Context Retrieval\" . For Quasar-S, since the answers come from a fixed vocabulary of entities, we test language model baselines which predict the most likely entity to appear in a given context. We train three n-gram baselines using the SRILM toolkit BIBREF21 for $n=3,4,5$ on the entire corpus of all Stack Overflow posts. The output predictions are restricted to the output vocabulary of entities. We also train a bidirectional Recurrent Neural Network (RNN) language model (based on GRU units).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What monolingual word representations are used?",
    "llm_answer": "Google's Word2Vec and Stanford's GloVe.\n",
    "context": [
      {
        "id": "f797f5b0-ddf5-4c3e-a8e0-790d66162b81",
        "metadata": {
          "vector_store_key": "1809.08510-1",
          "chunk_id": 3,
          "document_id": "1809.08510",
          "start_idx": 2082,
          "end_idx": 2843
        },
        "page_content": "Doing so would allow us to learn a task in one language and reap the benefits of all other languages without needing multilingual datasets. Our attempt to learn these representations begins by taking inspiration from linguistics and formalizing UG as an optimization problem. We train downstream models using language agnostic universal representations on a set of tasks and show the ability for the downstream models to generalize to languages that we did not train on. Our work attempts to unite universal (task agnostic) representations with multilingual (language agnostic) representations BIBREF9 , BIBREF10 . The recent trend in universal representations has been moving away from context-less unsupervised word embeddings to context-rich representations.",
        "type": "Document"
      },
      {
        "id": "cb4078b8-00d7-4e01-a91b-7292c78d6351",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 12,
          "document_id": "1910.04269",
          "start_idx": 7197,
          "end_idx": 7983
        },
        "page_content": "The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel). Several state-of-the-art results on various audio classification tasks have been obtained by using log-Mel spectrograms of raw audio, as features BIBREF19. Convolutional Neural Networks have demonstrated an excellent performance gain in classification of these features BIBREF20, BIBREF21 against other machine learning techniques. It has been shown that using attention layers with ConvNets further enhanced their performance BIBREF22. This motivated us to develop a CNN-based architecture with attention since this approach hasn\u2019t been applied to the task of language identification before.",
        "type": "Document"
      },
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      },
      {
        "id": "e033fcf1-2977-4b36-a537-de74c22a51c1",
        "metadata": {
          "vector_store_key": "1910.12618-6",
          "chunk_id": 24,
          "document_id": "1910.12618",
          "start_idx": 13337,
          "end_idx": 13999
        },
        "page_content": "The second representation is a neural word embedding. It consists in representing every word in the corpus by a real-valued vector of dimension $q$. Such models are usually obtained by learning a vector representation from word co-occurrences in a very large corpus (typically hundred thousands of documents, such as Wikipedia articles for example). The two most popular embeddings are probably Google's Word2Vec BIBREF19 and Standford's GloVe BIBREF20. In the former, a neural network is trained to predict a word given its context (continuous bag of word model), whereas in the latter a matrix factorization scheme on the log co-occurences of words is applied.",
        "type": "Document"
      },
      {
        "id": "9210eab3-d084-4395-859a-51482ae8da57",
        "metadata": {
          "vector_store_key": "1910.04269-1",
          "chunk_id": 34,
          "document_id": "1910.04269",
          "start_idx": 19227,
          "end_idx": 19993
        },
        "page_content": "2D attention models focused on the important features extracted by convolutional layers and bi-directional GRU captured the temporal features. Several of the spoken languages in Europe belong to the Indo-European family. Within this family, the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Katze in German and Ciao in Italian becomes Chao in Spanish.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Do they build one model per topic or on all topics?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "9dc69e28-1cf2-4d3c-9f8f-18a6f0169737",
        "metadata": {
          "vector_store_key": "1708.05873-2",
          "chunk_id": 9,
          "document_id": "1708.05873",
          "start_idx": 5213,
          "end_idx": 5988
        },
        "page_content": "Following BIBREF8 we generate a set of candidate models ranging between 3 and 50 topics. We then plot the exclusivity and semantic coherence (numbers closer to 0 indicate higher coherence), with a linear regression overlaid (Figure FIGREF3 ). Models above the regression line have a \u201cbetter\u201d exclusivity-semantic coherence trade off. We select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence. The topic quality is usually evaluated by highest probability words, which is presented in Figure FIGREF4 . Figure FIGREF4 provides a list of the main topics (and the highest probability words associated these topics) that emerge from the STM of UN General Debate statements.",
        "type": "Document"
      },
      {
        "id": "0ab61107-67bf-493c-aa5f-199d370caaa3",
        "metadata": {
          "vector_store_key": "1703.04617-2",
          "chunk_id": 26,
          "document_id": "1703.04617",
          "start_idx": 14460,
          "end_idx": 15206
        },
        "page_content": "The previous models are often trained for all questions without explicitly discriminating different question types; however, for a target question, both the common features shared by all questions and the specific features for a specific type of question are further considered in this paper, as they could potentially obey different distributions. In this paper we further explicitly model different types of questions in the end-to-end training. We start from a simple way to first analyze the word frequency of all questions, and obtain top-10 most frequent question types: what, how, who, when, which, where, why, be, whose, and whom, in which be stands for the questions beginning with different forms of the word be such as is, am, and are.",
        "type": "Document"
      },
      {
        "id": "8554c6d9-844e-48d1-8ece-a014b8922168",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 47,
          "document_id": "1809.04960",
          "start_idx": 27365,
          "end_idx": 27722
        },
        "page_content": "We introduce a variation topic model to represent the topics, and match the articles and comments by the similarity of their topics. Experiments show that our topic-based approach significantly outperforms previous lexicon-based models. The model can also profit from paired corpora and achieves state-of-the-art performance under semi-supervised scenarios.",
        "type": "Document"
      },
      {
        "id": "266139a8-c68b-428f-9148-108176241a21",
        "metadata": {
          "vector_store_key": "1701.06538-8",
          "chunk_id": 61,
          "document_id": "1701.06538",
          "start_idx": 33014,
          "end_idx": 33624
        },
        "page_content": "We vary the number of experts between models, using an ordinary MoE layer with 32 experts and hierarchical MoE layers with 256, 1024, 4096, 16384, 65536 and 131072 experts. For the hierarchical MoE layers, the first level branching factors are 32, 32, 64, 128, 256 and 256, respectively. Models are trained on a cluster of 32 Tesla K40 GPUs, except for the last two models, which are trained on clusters of 64 and 128 GPUs so as to have enough memory for all the parameters. For all models, training batch sizes are approximately 2.5 million words. Models are trained once-through over about 100 billion words.",
        "type": "Document"
      },
      {
        "id": "f8601d54-e685-49e0-ba0f-182456c1bca5",
        "metadata": {
          "vector_store_key": "1604.05781-2",
          "chunk_id": 23,
          "document_id": "1604.05781",
          "start_idx": 12856,
          "end_idx": 13558
        },
        "page_content": "Topics are considered as mixtures of unigrams by estimating conditional distributions over unigrams: $P(w|T)$ , the probability of unigram $w$ given topic $T$ and documents are considered as mixtures of topics via $P(T|d)$ , the probability of topic $T$ given document $d$ . These distributions are then found via statistical inference given the observed distributions of unigrams across documents. The total number of topics is a parameter chosen by the practitioner. For this study we used the MALLET v2.0.8RC3 topic modeling toolkit BIBREF40 for model inference. By inspecting the most probable unigrams per topic (according to $P(w|T)$ ), we found 10 topics provided meaningful and distinct topics.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Do they quantitavely or qualitatively evalute the output of their low-rank approximation to verify the grouping of lexical items?",
    "llm_answer": "Quantitatively, using Recall@k, Mean Rank (MR), and Mean Reciprocal Rank (MRR).  The context also describes qualitative evaluation via human annotation (highlighting semantically-equivalent phrases).\n",
    "context": [
      {
        "id": "a1fef963-ff2f-4345-8e27-3598f044ffb0",
        "metadata": {
          "vector_store_key": "1807.09671-4",
          "chunk_id": 19,
          "document_id": "1807.09671",
          "start_idx": 11086,
          "end_idx": 12011
        },
        "page_content": "Our extensions of this work are along three dimensions: (i) we crack the \u201cblack-box\u201d of the low-rank approximation algorithm to understand if it indeed allows lexically-diverse but semantically-similar items to share co-occurrence statistics; (ii) we compare the ILP-based summarization framework with state-of-the-art baselines, including a popular neural encoder-decoder model for summarization; (iii) we expand the student feedback datasets to include responses collected from materials science and engineering, statistics for industrial engineers, and data structures. We additionally experiment with reviews and news articles. Analyzing the unique characteristics of each dataset allows us to identify crucial factors influencing the summarization performance. With the fast development of Massive Open Online Courses (MOOC) platforms, more attention is being dedicated to analyzing educationally-oriented language data.",
        "type": "Document"
      },
      {
        "id": "f3dfd3be-6a8d-43cc-9837-48f3197c6dc7",
        "metadata": {
          "vector_store_key": "1807.09671-4",
          "chunk_id": 35,
          "document_id": "1807.09671",
          "start_idx": 19881,
          "end_idx": 20572
        },
        "page_content": "Another reason we choose the student responses is that we have advanced annotation allowing us to perform an intrinsic evaluation to test whether the low-rank approximation does capture similar concepts or not. An example of the annotation is shown in Table TABREF15 , where phrases in the student responses that are semantically the same as the summary phrases are highlighted with the same color by human annotators. For example, \u201cerror bounding\" (S2), \u201cerror boundary\" (S4), \u201cfinding that error\" (S3), and \u201cdetermining the critical value for error\" (S7) are semantically equivalent to \u201cError bounding\" in the human summary. Details of the intrinsic evaluation are introduced in SECREF20 .",
        "type": "Document"
      },
      {
        "id": "eda9281d-be5b-4da8-9fae-5f3146858b76",
        "metadata": {
          "vector_store_key": "1807.09671-0",
          "chunk_id": 7,
          "document_id": "1807.09671",
          "start_idx": 3991,
          "end_idx": 4868
        },
        "page_content": "At the heart of the algorithm is a sentence-concept co-occurrence matrix, used to determine if a sentence contains important concepts and whether two sentences share the same concepts. We introduce a low-rank approximation to the co-occurrence matrix and optimize it using the proximal gradient method. The resulting system thus allows different sentences to share co-occurrence statistics. For example, \u201cThe activity with the bicycle parts\" will be allowed to partially contain \u201cbike elements\" although the latter phrase does not appear in the sentence. The low-rank matrix approximation provides an effective way to implicitly group lexically-diverse but semantically-similar expressions. It can handle out-of-vocabulary expressions and domain-specific terminologies well, hence being a more principled approach than heuristically calculating similarities of word embeddings.",
        "type": "Document"
      },
      {
        "id": "ae02c780-2742-4a78-aacf-5b0ebd2578ae",
        "metadata": {
          "vector_store_key": "1809.04960-1",
          "chunk_id": 31,
          "document_id": "1809.04960",
          "start_idx": 18100,
          "end_idx": 18863
        },
        "page_content": "Random: After selecting the correct, plausible, and popular comments, we fill the candidate set with randomly selected comments from the training set so that there are 200 unique comments in the candidate set. Following previous work, we measure the rank in terms of the following metrics: Recall@k: The proportion of human comments found in the top-k recommendations. Mean Rank (MR): The mean rank of the human comments. Mean Reciprocal Rank (MRR): The mean reciprocal rank of the human comments. The evaluation protocol is compatible with both retrieval models and generative models. The retrieval model can directly rank the comments by assigning a score for each comment, while the generative model can rank the candidates by the model's log-likelihood score.",
        "type": "Document"
      },
      {
        "id": "71d29f99-326d-4f7f-935a-37fe7c759abd",
        "metadata": {
          "vector_store_key": "1910.02339-3",
          "chunk_id": 53,
          "document_id": "1910.02339",
          "start_idx": 28254,
          "end_idx": 29102
        },
        "page_content": "During our experiments we observed that there are about 30% noisy examples (on which the execution script fails to get the correct answer on the ground truth program). Therefore, we report both execution accuracy (the final multi-choice answer after running the execution engine) and operation sequence accuracy (where the generated operation sequence must match the ground truth sequence exactly). The AlgoLisp dataset BIBREF17 is a program synthesis dataset, which has 79k/9k/10k training/dev/testing samples. Each sample contains a problem description, a corresponding Lisp program tree, and 10 input-output testing pairs. We parse the program tree into a straight-line sequence of commands from leaves to root and (as in MathQA) use the symbol $\\#_i$ to indicate the result of the $i^{\\mathrm {th}}$ command (generated previously by the model).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How well does their system perform on the development set of SRE?",
    "llm_answer": "The system showed an 11% relative improvement compared to the primary system on the SRE 2016 development set, but the results may be misleading due to consistent recording conditions for all speakers.\n",
    "context": [
      {
        "id": "23976a7a-1b28-442c-baa1-8b2f84b234bb",
        "metadata": {
          "vector_store_key": "1611.00514-0",
          "chunk_id": 30,
          "document_id": "1611.00514",
          "start_idx": 17171,
          "end_idx": 17817
        },
        "page_content": "We evaluated the system on almost 6k out of 24k trials from the other portion to avoid any over-fitting, particularly important for the domain adaptation technique. This resulted in a relative improvement of 11% compared to the primary system in terms of the primary metric. However, the results can be misleading, since the recording condition may be the same for all speakers in the development set. This section reports on the CPU execution time (single threaded), and the amount of memory used to process a single trial, which includes the time for creating models from the enrolment data and the time needed for processing the test segments.",
        "type": "Document"
      },
      {
        "id": "f8df8d47-d7f9-4c66-833b-0447fa89afe3",
        "metadata": {
          "vector_store_key": "1611.00514-4",
          "chunk_id": 3,
          "document_id": "1611.00514",
          "start_idx": 1901,
          "end_idx": 2817
        },
        "page_content": "We outline in this paper the Intelligent Voice system, techniques and results obtained on the SRE 2016 development set that will mirror the evaluation condition as well as the timing report. Section SECREF2 describes the data used for the system training. The front-end and back-end processing of the system are presented in Sections SECREF3 and SECREF4 respectively. In Section SECREF5 , we describe experimental evaluation of the system on the SRE 2016 development set. Finally, we present a timing analysis of the system in Section SECREF6 . The fixed training condition is used to build our speaker recognition system. Only conversational telephone speech data from datasets released through the linguistic data consortium (LDC) have been used, including NIST SRE 2004-2010 and the Switchboard corpora (Switchboard Cellular Parts I and II, Switchboard2 Phase I,II and III) for different steps of system training.",
        "type": "Document"
      },
      {
        "id": "3699d425-11f4-45cc-aae5-6a93d3e32b86",
        "metadata": {
          "vector_store_key": "1707.03904-0",
          "chunk_id": 8,
          "document_id": "1707.03904",
          "start_idx": 4627,
          "end_idx": 5317
        },
        "page_content": "We evaluate Quasar against human testers, as well as several baselines ranging from na\u00efve heuristics to state-of-the-art machine readers. The best performing baselines achieve $33.6\\%$ and $28.5\\%$ on Quasar-S and Quasar-T, while human performance is $50\\%$ and $60.6\\%$ respectively. For the automatic systems, we see an interesting tension between searching and reading accuracies \u2013 retrieving more documents in the search phase leads to a higher coverage of answers, but makes the comprehension task more difficult. We also collect annotations on a subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short.",
        "type": "Document"
      },
      {
        "id": "5f712cdd-f7ca-421d-b6c6-81178c228e50",
        "metadata": {
          "vector_store_key": "1706.08032-2",
          "chunk_id": 28,
          "document_id": "1706.08032",
          "start_idx": 16200,
          "end_idx": 16915
        },
        "page_content": "We evaluate the efficiency of SR for the model in Table V of our full paper . We also conduct two experiments on two separate models: DeepCNN and Bi-LSTM in order to show the effectiveness of combination of DeepCNN and Bi-LSTM. In addition, the model using TwitterGlove outperform the model using GoogleW2V because TwitterGlove captures more information in Twitter than GoogleW2V. These results show that the character-level information and SR have a great impact on Twitter Data. The pre-train word vectors are good, universal feature extractors. The difference between our model and other approaches is the ability of our model to capture important features by using SR and combine these features at high benefit.",
        "type": "Document"
      },
      {
        "id": "452e7fa2-0a32-47c1-83c6-7258ee550cfc",
        "metadata": {
          "vector_store_key": "1808.09409-1",
          "chunk_id": 23,
          "document_id": "1808.09409",
          "start_idx": 13128,
          "end_idx": 13849
        },
        "page_content": "In particular, the sentences selected for the CoNLL 2009 shared task are used here for parameter estimation. Note that, since the Berkeley parser is based on PCFGLA grammar, it may fail to get the syntactic outputs for some sentences, while the other parser does not have that problem. In this case, we have made sure that both parsers can parse all 1,200 sentences successfully. The overall performances of the three SRL systems on both L1 and L2 data (150 parallel sentences for each mother tongue) are shown in Table TABREF11 . For all systems, significant decreases on different mother languages can be consistently observed, highlighting the weakness of applying L1-sentence-trained systems to process learner texts.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which of the classifiers showed the best performance?",
    "llm_answer": "AttEmbedDecoder\n",
    "context": [
      {
        "id": "050432d2-b325-4282-ae97-1513abbfedc7",
        "metadata": {
          "vector_store_key": "2003.06279-7",
          "chunk_id": 36,
          "document_id": "2003.06279",
          "start_idx": 21531,
          "end_idx": 22144
        },
        "page_content": "While Figures FIGREF14 \u2013 FIGREF16 show the relative behavior in the accuracy, it still interesting to observe the absolute accuracy rate obtained with the classifiers. In Table TABREF17, we show the best accuracy rate (i.e. $\\max \\Gamma _+ = \\max _p \\Gamma _+(p)$) for GloVe. We also show the average difference in performance ($\\langle \\Gamma _+ - \\Gamma _0 \\rangle $) and the total number of cases in which an improvement in performance was observed ($N_+$). $N_+$ ranges in the interval $0 \\le N_+ \\le 20$. Table TABREF17 summarizes the results obtained for $w = \\lbrace 1.0, 5.0, 10.0\\rbrace $ thousand words.",
        "type": "Document"
      },
      {
        "id": "971142fa-a58a-47ae-8d5a-4ad5205893d3",
        "metadata": {
          "vector_store_key": "1607.06025-9",
          "chunk_id": 56,
          "document_id": "1607.06025",
          "start_idx": 32743,
          "end_idx": 33538
        },
        "page_content": "The AttEmbedDecoder model was the best model according to our main metric \u2013 the accuracy of the classifier trained on the generated dataset. The hidden dimension INLINEFORM0 of the BaseEmbedDecoder was selected so that the model was comparable to AttEmbedDecoder in terms of the number of parameters INLINEFORM1 . The accuracies of classifiers generated by BaseEmbedDecoder are still lower than the accuracies of classifiers generated by AttEmbedDecoder, which shows that the attention mechanism helps the models. Table TABREF44 shows the performance of generated datasets compared to the original one. The best generated dataset was generated by AttEmbedDecoder. The accuracy of its classifier is only 2.7 % lower than the accuracy of classifier generated on the original human crafted dataset.",
        "type": "Document"
      },
      {
        "id": "cae387d5-b5b1-44de-a443-42d90d1eb1c2",
        "metadata": {
          "vector_store_key": "2003.06279-7",
          "chunk_id": 32,
          "document_id": "2003.06279",
          "start_idx": 19100,
          "end_idx": 19861
        },
        "page_content": "The relative improvement in performance is given by $\\Gamma _+{(p)}/\\Gamma _0$, where $\\Gamma _+{(p)}$ is the accuracy rate obtained when $p\\%$ additional edges are included and $\\Gamma _0 = \\Gamma _+{(p=0)}$, i.e. $\\Gamma _0$ is the accuracy rate measured from the traditional co-occurrence model. We only show the highest relative improvements in performance for each classifier. In our analysis, we considered also samples of text with distinct length, since the performance of network-based methods is sensitive to text length BIBREF34. In this figure, we considered samples comprising $w=\\lbrace 1.0, 2.5, 5.0, 10.0\\rbrace $ thousand words. The results obtained for GloVe show that the highest relative improvements in performance occur for decision trees.",
        "type": "Document"
      },
      {
        "id": "4f72307a-0a25-4c37-9a12-004e692be9a5",
        "metadata": {
          "vector_store_key": "2003.06279-2",
          "chunk_id": 33,
          "document_id": "2003.06279",
          "start_idx": 19861,
          "end_idx": 20566
        },
        "page_content": "The results obtained for GloVe show that the highest relative improvements in performance occur for decision trees. This is apparent specially for the shortest samples. For $w=1,000$ words, the decision tree accuracy is enhanced by a factor of almost 50% when $p=20\\%$. An excellent gain in performance is also observed for both Naive Bayes and SVM classifiers, when $p=18\\%$ and $p=12\\%$, respectively. When $w=2,500$ words, the highest improvements was observed for the decision tree algorithm. A minor improvement was observed for the kNN method. A similar behavior occurred for $w=5,000$ words. Interestingly, SVM seems to benefit from the use of additional edges when larger documents are considered.",
        "type": "Document"
      },
      {
        "id": "c93f2257-9006-466a-af1f-9340e6b4ab7d",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 28,
          "document_id": "2001.00137",
          "start_idx": 16720,
          "end_idx": 17443
        },
        "page_content": "Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector Classifier with L1 and L2 penalty and tolerance of $10^{-3}$; Regularized linear classifier with Stochastic Gradient Descent (SGD) learning with regularization term $alpha=10^{-4}$ and L1, L2 and Elastic-Net penalty; Nearest Centroid with Euclidian metric, where classification is done by representing each class with a centroid;",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are the keywords associated with events such as protests selected?",
    "llm_answer": "Keywords are selected by searching for word-pairs that achieve the highest number of spikes matching the days of events.  The method uses the Jaccard similarity metric to value spikes matching events and penalize other cases.\n",
    "context": [
      {
        "id": "6b5fa9e4-3a07-4b7a-88a9-ca54bf63a807",
        "metadata": {
          "vector_store_key": "1901.00570-6",
          "chunk_id": 2,
          "document_id": "1901.00570",
          "start_idx": 1084,
          "end_idx": 1832
        },
        "page_content": "The main challenge in keyword-based models is to determine which words to use in the first place, especially as people use words in a non-standard way, particularly on Twitter. In this research, we aim for detecting large events as soon as they happen with near-live sensitivity. For example, When spontaneous protests occur just after recent news such as increasing taxes or decreasing budget, we need to have indicators to raise the flag of a happening protest. Identifying these indicators requires to select a set of words that are mostly associated with the events of interest such as protests. We then track the volume of these words and evaluate the probability of an event occurring given the current volume of each of the tracked features.",
        "type": "Document"
      },
      {
        "id": "d0c436d8-447a-434d-9e8b-2a5f88602fa1",
        "metadata": {
          "vector_store_key": "1901.00570-6",
          "chunk_id": 8,
          "document_id": "1901.00570",
          "start_idx": 4393,
          "end_idx": 4987
        },
        "page_content": "This feature selection method is built upon the assumption that people discuss an event on the day of that event more than on any day before or after the event. This implies that word-pairs related to the event will form a spike on this specific day. Some of the spiking word-pairs are related to the nature of the event itself, such as \u201ctaxi protest\u201d or \u201cfair education\u201d. These word-pairs will appear once or twice along the time frame. Meanwhile, more generic word-pairs such as \u201chuman rights\u201d or \u201clabour strike\u201d will spike more frequently in the days of events regardless the protest nature.",
        "type": "Document"
      },
      {
        "id": "9523d82a-d256-43bd-bd8b-307a3b2dde98",
        "metadata": {
          "vector_store_key": "1901.00570-0",
          "chunk_id": 9,
          "document_id": "1901.00570",
          "start_idx": 4987,
          "end_idx": 5595
        },
        "page_content": "Meanwhile, more generic word-pairs such as \u201chuman rights\u201d or \u201clabour strike\u201d will spike more frequently in the days of events regardless the protest nature. To test our method, we developed two experiments using all the tweets in Melbourne and Sydney over a period of 640 days. The total number of tweets exceeded 4 million tweets per day, with a total word-pair count of 12 million different word-pairs per day, forming 6 billion word-pairs over the entire timeframe. The selected word-pairs from in each city are used as features to classify if there will be an event or not on a specific day in that city.",
        "type": "Document"
      },
      {
        "id": "d0f92e69-92d9-47a2-a1ec-544358c3a994",
        "metadata": {
          "vector_store_key": "1901.00570-6",
          "chunk_id": 5,
          "document_id": "1901.00570",
          "start_idx": 2904,
          "end_idx": 3589
        },
        "page_content": "According to the distributional semantic hypothesis, event-related words are likely to be used on the day of an event more frequently than any normal day before or after the event. This will form a spike in the keyword count magnitude along the timeline as illustrated in Figure FIGREF6 . To find the words most associated with events, we search for the words that achieve the highest number of spikes matching the days of events. We use the Jaccard similarity metric as it values the spikes matching events and penalizes spikes with no event and penalizes events without spikes. Separate words can be noisy due to the misuse of the term by people, especially in big data environments.",
        "type": "Document"
      },
      {
        "id": "42fae3bf-7c3f-4d7f-be4a-8f2007fef9e5",
        "metadata": {
          "vector_store_key": "1901.00570-6",
          "chunk_id": 6,
          "document_id": "1901.00570",
          "start_idx": 3304,
          "end_idx": 4009
        },
        "page_content": "Separate words can be noisy due to the misuse of the term by people, especially in big data environments. So, we rather used the word-pairs as textual features in order to capture the context of the word. For example, this can differentiate between the multiple usages of the word \u201cstrike\u201d within the contexts of \u201clightning strike\u201d, \u201cfootball strike\u201d and \u201clabour strike\u201d In this paper, we propose a method to find the best word-pairs to represent the events of interest. These word-pairs can be used for time series analysis to predict future events as indicated in Figure FIGREF1 . They can also be used as seeds for topic modelling, or to find related posts and word-pairs using dynamic query expansion.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many speeches are in the dataset?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "896d2a09-7143-45f3-a2e2-490ad497e48f",
        "metadata": {
          "vector_store_key": "1808.06834-0",
          "chunk_id": 33,
          "document_id": "1808.06834",
          "start_idx": 18169,
          "end_idx": 18764
        },
        "page_content": "We also tried to identify them automatically using fastText algorithm and provided the results. The analysis is done for understanding the purpose of the speeches in the parliament. We also presented our results on binary stance classification of the speeches whether the member is in favour of the debate topic or not. In future, we would like to increase the size of the dataset by including sessions of previous years which are not yet digitized. Sessions before 2009 are yet to be digitalised by the Lok Sabha editorial of India. Also we plan to include Rajya Sabha debates into the dataset.",
        "type": "Document"
      },
      {
        "id": "a22cce10-cf7d-4a75-a5a2-99d8a3f066ee",
        "metadata": {
          "vector_store_key": "1705.03151-5",
          "chunk_id": 46,
          "document_id": "1705.03151",
          "start_idx": 25347,
          "end_idx": 26030
        },
        "page_content": "These are: Mandarin, Cantonese, Indonesian, Japanese, Russian, Korean and Vietnamese. The data volume for each language is approximately 10 hours of speech signals recorded by 24 speakers (12 males and 12 females), with each speaker recording approximately 300 utterances in reading style by mobile phones, with a sampling rate of 16kHz and a sample size of 16 bits. Each dataset was split into a training set consisting of 18 speakers, and a test set consisting of 6 speakers. For Mandarin, Cantonese, Vietnamese and Indonesian, the recording was conducted in a quiet environment. For Russian, Korean and Japanese, there are 2 recording conditions for each speaker, quiet and noisy.",
        "type": "Document"
      },
      {
        "id": "10b24597-0988-4038-bfdf-7b2b866c6465",
        "metadata": {
          "vector_store_key": "1705.03151-5",
          "chunk_id": 44,
          "document_id": "1705.03151",
          "start_idx": 24163,
          "end_idx": 24956
        },
        "page_content": "The sampling rate is 8 kHz and the sample size is 16 bits. In this paper, we chose speech data from seven languages in the Babel database: Assamese, Bengali, Cantonese, Georgian, Pashto Tagalog and Turkish. For each language, an official training and development dataset were provided. The training datasets contain both conversational and scripted speech, and the development datasets only contain conversational speech. We used the entire training set of each language for model training, but randomly selected $2,000$ utterances from the development set of each language to perform testing. The training data sets from the seven languages are as follows: Assamese 75 hours, Bengali 87 hours, Cantonese 175 hours, Georgian 64 hours, Pashto 111 hours, Tagalog 116 hours and Turkish 107 hours.",
        "type": "Document"
      },
      {
        "id": "803ff186-3aed-4cba-953b-b516d9d3c3c5",
        "metadata": {
          "vector_store_key": "1808.06834-0",
          "chunk_id": 7,
          "document_id": "1808.06834",
          "start_idx": 3816,
          "end_idx": 4543
        },
        "page_content": "Parliamentary data is a major source of socially relevant content. A new series of workshops are being conducted for the sole purpose of encouraging research in parliamentary debates ParlClarin. As a preliminary step, we created four major categories of the speeches spoken by the parliament members. The definitions and examples of the four categories are explained in the below tables respectively. The examples are taken from a debate on NABARD bill in Lok Sabha. A speech can be labelled with multiple categories as members can appreciate and raise issues in the same speech. The following points are the contributions of this paper : Many linguists around the globe are concentrating on creation of parliamentary datasets.",
        "type": "Document"
      },
      {
        "id": "54e29f92-b537-4e33-8952-d8f2331f0a87",
        "metadata": {
          "vector_store_key": "1808.06834-0",
          "chunk_id": 1,
          "document_id": "1808.06834",
          "start_idx": 660,
          "end_idx": 1350
        },
        "page_content": "Since the data is unstructured , it cannot be computationally analyzed. There is a need to shape the data into a structured format for analysis. This data is important as it can be used to visualize person, party and agenda level semantics in the house. The data that we get from parliamentary proceedings has presence of sarcasm, interjections and allegations which makes it difficult to apply standard NLP techniques BIBREF0 . Members of the parliament discuss various important aspects and there is a strong purpose behind every speech. We wanted to analyze this particular aspect. Traditional polar stances (for or against) do not justify for the diplomatic intricacies in the speeches.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is different in BERT-gen from standard BERT?",
    "llm_answer": "BERT-gen allows for direct text generation from pre-trained encoders, either in mono- or multi-modal setups, whereas standard BERT is fine-tuned.\n",
    "context": [
      {
        "id": "811bcb74-e89a-4f0f-b42f-60b757b8c7e6",
        "metadata": {
          "vector_store_key": "1909.02635-2",
          "chunk_id": 24,
          "document_id": "1909.02635",
          "start_idx": 14075,
          "end_idx": 14804
        },
        "page_content": "BERT mainly differs in that it is bidirectional, though we also use the pre-trained [CLS] and [SEP] tokens instead of introducing new tokens in the input vocabulary and training them from scratch during fine-tuning. Owing to the lengths of the processes, all our experiments are performed on BERT$_{BASE}$. The most significant prior work on this dataset is the work of BIBREF15. However, their data condition differs significantly from ours: they train on a large noisy training set and do not use any of the high-quality labeled data, instead treating it as dev and test data. Consequently, their model achieves low performance, roughly 56 $F_1 $ while ours achieves $82.5$ $F_1$ (though these are not the exact same test set).",
        "type": "Document"
      },
      {
        "id": "88576c86-5b45-4b1a-af4c-e98a370d4b62",
        "metadata": {
          "vector_store_key": "1902.00821-3",
          "chunk_id": 16,
          "document_id": "1902.00821",
          "start_idx": 8621,
          "end_idx": 9372
        },
        "page_content": "Unlike ELMo BIBREF31 and ULMFiT BIBREF32 that are designed to provide additional features for an end task, BERT adopts a fine-tuning approach that requires almost no specific architecture design for end tasks, but parameter intensive models on BERT itself. As such, BERT requires pre-training on large-scale data (Wikipedia articles) to fill intensive parameters in exchange for human structured architecture designs for specific end-tasks that carry human's understanding of data of those tasks. One training example of BERT is formulated as $(\\texttt {[CLS]}, x_{1:j}, \\texttt {[SEP]}, x_{j+1:n}, \\texttt {[SEP]})$ , where [CLS] and [SEP] are special tokens and $x_{1:n}$ is a document splited into two sides of sentences $x_{1:j}$ and $x_{j+1:n}$ .",
        "type": "Document"
      },
      {
        "id": "15d4ae8c-088b-47b2-a34e-c95754a2f5e3",
        "metadata": {
          "vector_store_key": "2002.10832-7",
          "chunk_id": 52,
          "document_id": "2002.10832",
          "start_idx": 29320,
          "end_idx": 30187
        },
        "page_content": "Finally, Image + Caption $>$ Image only can be attributed to BERT fine-tuning, contributing to an increase in the observed gap, and its emergence in earlier layers. We investigated whether the abstractions encoded in a pre-trained BERT model can generalize beyond text. We proposed BERT-gen, a novel methodology that allows to directly generate text from out-of-the-box pre-trained encoders, either in mono- or multi- modal setups. Moreover, we applied BERT-gen to Visual Question Generation, obtaining state-of-the-art results on two established datasets. We showed how a simple linear projection is sufficient to effectively align visual and textual representations. In future works, we plan to extend BERT-gen to other modalities, such as audio or video, exploring the potential interactions that can emerge in scenarios where more than two modalities are present.",
        "type": "Document"
      },
      {
        "id": "815dabed-beaf-4042-bae7-9eccd2eeffdf",
        "metadata": {
          "vector_store_key": "2002.10832-7",
          "chunk_id": 8,
          "document_id": "2002.10832",
          "start_idx": 4686,
          "end_idx": 5451
        },
        "page_content": "We show how a simple linear mapping, projecting visual embeddings into the first layer, is enough to ground BERT in the visual realm: text and image object representations are found to be effectively aligned, and the attention over words transfers to attention over the relevant objects in the image. Our contributions can be summarized as follows: we introduce BERT-gen, a novel method for generating text using BERT, that can be applied in both mono and multi-modal settings; we show that the semantic abstractions encoded in pre-trained BERT can generalize to another modality; we report state-of-the art results on the VQG task; we provide extensive ablation analyses to interpret the behavior of BERT-gen under different configurations (mono- or multi- modal).",
        "type": "Document"
      },
      {
        "id": "13c5643a-1a78-420a-9366-40b685ac8c5f",
        "metadata": {
          "vector_store_key": "1902.00821-0",
          "chunk_id": 42,
          "document_id": "1902.00821",
          "start_idx": 22102,
          "end_idx": 22750
        },
        "page_content": "BERT is the vanilla BERT model directly fine-tuned on $(\\text{RC})_2$ . We use this baseline for ablation study on the effectiveness of pre-tuning. All these BERT's variants are used to answer RQ2. BERT+review first tunes BERT on domain reviews using the same objectives as BERT pre-training and then fine-tunes on $(\\text{RC})_2$ . We use this baseline to show that a simple domain-adaptation of BERT is not good. BERT+CoQA first fine-tunes BERT on the supervised CoQA data and then fine-tunes on $(\\text{RC})_2$ . We use this baseline to show that pre-tuning is very competitive even compared with models trained from large-scale supervised data.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are multimodal representations combined?",
    "llm_answer": "The feature vectors from different embedding sets are concatenated.\n",
    "context": [
      {
        "id": "16d4b3d8-3496-4c40-a981-f3889a190337",
        "metadata": {
          "vector_store_key": "1603.00968-2",
          "chunk_id": 12,
          "document_id": "1603.00968",
          "start_idx": 7096,
          "end_idx": 7879
        },
        "page_content": "At the classification layer we then obtain a feature vector INLINEFORM10 for each embedding set, and we can simply concatenate these together to form the final feature vector INLINEFORM11 to feed into the softmax function, where INLINEFORM12 . This representation contains feature vectors generated from all sets of embeddings under consideration. We call this method multiple group CNN (MG-CNN). Here groups refer to the features generated from different embeddings. Note that this differs from `multi-channel' models because at the convolution layer we use different filters on each word embedding matrix independently, whereas in a standard multi-channel approach each filter would consider all channels simultaneously and generate a scalar from all channels at each local region.",
        "type": "Document"
      },
      {
        "id": "56f56118-ed4f-4879-94fa-1f8ee08cd4d8",
        "metadata": {
          "vector_store_key": "1910.03814-3",
          "chunk_id": 40,
          "document_id": "1910.03814",
          "start_idx": 21721,
          "end_idx": 22352
        },
        "page_content": "Besides the different architectures, we have tried different training strategies, such as initializing the CNN weights with a model already trained solely with MMHS150K images or using dropout to force the multimodal models to use the visual information. Eventually, though, these models end up using almost only the text input for the prediction and producing very similar results to those of the textual models. The proposed multimodal models, such as TKM, have shown good performance in other tasks, such as VQA. Next, we analyze why they do not perform well in this task and with this data: [noitemsep,leftmargin=*] Noisy data.",
        "type": "Document"
      },
      {
        "id": "7f6c8c7b-6ba7-4526-bcef-d6a7f4452244",
        "metadata": {
          "vector_store_key": "1806.00722-1",
          "chunk_id": 6,
          "document_id": "1806.00722",
          "start_idx": 3482,
          "end_idx": 4276
        },
        "page_content": "Typically, NMT models use the encoder-attention-decoder framework BIBREF1 , and potentially use multi-layer structure for both encoder and decoder. Given a source sentence INLINEFORM2 with length INLINEFORM3 , the encoder calculates hidden representations by layer. We denote the representation in the INLINEFORM4 -th layer as INLINEFORM5 , with dimension INLINEFORM6 , where INLINEFORM7 is the dimension of features in layer INLINEFORM8 . The hidden representation at each position INLINEFORM9 is either calculated by: DISPLAYFORM0  for recurrent transformation INLINEFORM0 such as LSTM and GRU, or by: DISPLAYFORM0  for parallel transformation INLINEFORM0 . On the other hand, the decoder layers INLINEFORM1 follow similar structure, while getting extra representations from the encoder side.",
        "type": "Document"
      },
      {
        "id": "af6f3203-24d5-47c4-bfe4-d761a3bf9323",
        "metadata": {
          "vector_store_key": "1809.04686-1",
          "chunk_id": 19,
          "document_id": "1809.04686",
          "start_idx": 10360,
          "end_idx": 11268
        },
        "page_content": "Our multilingual NMT model consists of a shared multilingual encoder and two decoders, one for English and the other for French. The multilingual encoder uses one bi-directional LSTM, followed by three stacked layers of uni-directional LSTMs in the encoder. Each decoder consists of four stacked LSTM layers, with the first LSTM layers intertwined with additive attention networks BIBREF33 to learn a source-target alignment function. All the uni-directional LSTMs are equipped with residual connections BIBREF45 to ease the optimization, both in the encoder and the decoders. LSTM hidden units and the shared source-target embedding dimensions are set to 512. Similar to BIBREF30 , multilingual NMT model is trained in a multi-task learning setup, where each decoder is augmented with a task-specific loss, minimizing the negative conditional log-likelihood of the target sequence given the source sequence.",
        "type": "Document"
      },
      {
        "id": "85f2f7e7-7aff-485c-828c-9f420ab5b7ef",
        "metadata": {
          "vector_store_key": "2002.06053-1",
          "chunk_id": 40,
          "document_id": "2002.06053",
          "start_idx": 23613,
          "end_idx": 24367
        },
        "page_content": "In this representation model, a text is represented as a vector of bag-of-words, where the multiplicity of the words is taken into account, but the order of the words in the text is lost BIBREF70. For instance, the SMILES of ampicillin \u201cCC1(C(N2C(S1)C(C2=O)NC(=O)C( C3=CC=CC=C3)N)C(=O)O)C\" can be represented as a bag-of 8-mers as follows: {\u201cCC1(C(N2\", \u201cC1(C(N2C\", \u201c1(C(N2C(\", \u201c(C(N2C(S\",...,\u201cN)C(=O)O\" ,\u201c)C(=O)O)\" ,\u201cC(=O)O)C\" }. We can vectorize it as $S = [1, 1, 1, 1, ...,1, 1, 1]$ in which each number refers to the frequency of the corresponding 8-mer. Bag-of-words representation was used in molecular similarity computation, in which the SMILES string and the LINGOs extracted from it were treated as the sentence and words, respectively BIBREF42.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the problem with existing metrics that they are trying to address?",
    "llm_answer": "Existing metrics fail to properly address biases in datasets, leading algorithms to exploit them, resulting in poor real-world performance;  they also don't correlate well with human judgment in some areas (e.g., machine translation).  Some metrics are not suited for unbalanced categories in datasets.\n",
    "context": [
      {
        "id": "157f74aa-d453-4906-acf8-239bdfd9f7ef",
        "metadata": {
          "vector_store_key": "2001.02380-5",
          "chunk_id": 139,
          "document_id": "2001.02380",
          "start_idx": 75278,
          "end_idx": 76132
        },
        "page_content": "Firstly, the quantitative nature of the metric allows us to rank signaling strength in a way that humans have not to date been able to apply: using ${\\Delta }_s$, we can say which instances of which signals are evaluated as stronger, by how much, and which words within a multi-word signal instance are the most important (e.g. weekdays in dates are important, the commas are not). Secondly, the potential for negative values of the metric opens the door to the study of negative signals, or `distractors', which we have only touched upon briefly in this paper. And finally, we consider the availability of multiple measurements for a single DM or other discourse signal to be a potentially very interesting window into the relative ambiguity of different signaling devices (cf. BIBREF16) and for research on the contexts in which such ambiguity results.",
        "type": "Document"
      },
      {
        "id": "e34ec430-93a0-4067-875a-f939ddf574fd",
        "metadata": {
          "vector_store_key": "1703.09684-9",
          "chunk_id": 9,
          "document_id": "1703.09684",
          "start_idx": 5278,
          "end_idx": 5981
        },
        "page_content": "For example, on COCO-VQA, improving accuracy on `Is/Are' questions by 15% will increase overall accuracy by over 5%, but answering all `Why/Where' questions correctly will increase accuracy by only 4.1% BIBREF10 . Due to the inability of the existing evaluation metrics to properly address these biases, algorithms trained on these datasets learn to exploit these biases, resulting in systems that work poorly when deployed in the real-world. For related reasons, major benchmarks released in the last decade do not use simple accuracy for evaluating image recognition and related computer vision tasks, but instead use metrics such as mean-per-class accuracy that compensates for unbalanced categories.",
        "type": "Document"
      },
      {
        "id": "b8da75b2-a962-4b59-9a53-800e0c324ca8",
        "metadata": {
          "vector_store_key": "2002.06053-9",
          "chunk_id": 75,
          "document_id": "2002.06053",
          "start_idx": 45023,
          "end_idx": 45831
        },
        "page_content": "As this interdisciplinary field grows, novel opportunities come hand in hand with novel challenges. The major challenges that can be observed from investigating these studies can be summarized as follows: (i) the need for universalized benchmarks and metrics, (ii) reproducibility of the published methodologies, (iii) bias in available data, and (iv) biological and chemical interpretability/explainability of the solutions. There are several steps in the drug discovery pipeline, from affinity prediction to the prediction of other chemical properties such as toxicity, and solubility. The use of different datasets and different evaluation metrics makes the assessment of model performance challenging. Comprehensive benchmarking platforms that can assess the success of different tools are still lacking.",
        "type": "Document"
      },
      {
        "id": "ab73a02a-f9c2-4d3c-81d9-d9142caefb09",
        "metadata": {
          "vector_store_key": "1809.08731-4",
          "chunk_id": 41,
          "document_id": "1809.08731",
          "start_idx": 23042,
          "end_idx": 23844
        },
        "page_content": "This was supported by isabelle-cherry-foster:2017:EMNLP2017, who proposed a so-called challenge set approach as an alternative. graham-EtAl:2016:COLING performed a large-scale evaluation of human-targeted metrics for machine translation, which can be seen as a compromise between human evaluation and fully automatic metrics. They also found fully automatic metrics to correlate only weakly or moderately with human judgments. bojar2016ten further confirmed that automatic MT evaluation methods do not perform well with a single reference. The need of better metrics for MT has been addressed since 2008 in the WMT metrics shared task BIBREF31 , BIBREF32 . For unsupervised dialogue generation, liu-EtAl:2016:EMNLP20163 obtained close to no correlation with human judgements for BLEU, ROUGE and METEOR.",
        "type": "Document"
      },
      {
        "id": "dd65ae52-1193-4705-933e-02e417dc2dc1",
        "metadata": {
          "vector_store_key": "1607.06025-0",
          "chunk_id": 49,
          "document_id": "1607.06025",
          "start_idx": 28660,
          "end_idx": 29373
        },
        "page_content": "The main purpose of the figure is not show absolute values for each of the metrics, but to compare the metrics' curves to the curve of our main metric, the accuracy of the classifier. The first metric \u2013 Premise-Hypothesis Distance \u2013 represents the average Jaccard distance between the premise and the generated hypothesis. Datasets generated with low latent dimensions have hypotheses more similar to premises, which indicates that the generated hypotheses are more trivial and less diverse than hypothesis generated with higher latent dimensions. We also evaluated the models with standard language generation metrics ROUGE-L and METEOR. The metrics are negatively correlated with the accuracy of the classifier.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are discourse embeddings analyzed?",
    "llm_answer": "t-SNE clustering is performed on the discourse embeddings, examining the closest neighbors of each embedding to observe whether similar discourse relations group together (e.g., explanation and interpretation).\n",
    "context": [
      {
        "id": "68c77081-d5b8-46ff-9891-4dd59b0b0356",
        "metadata": {
          "vector_store_key": "1709.02271-4",
          "chunk_id": 37,
          "document_id": "1709.02271",
          "start_idx": 21458,
          "end_idx": 22170
        },
        "page_content": "We examine the closest neighbors of each embedding, and observe that similar discourse relations tend to go together (e.g., explanation and interpretation; consequence and result). Some examples are given in Table TABREF29 . However, it is unclear how this pattern helps improve classification performance. We intend to investigate this question in future work. Global vs. Local featurization. As described in Section SECREF17 , the global reading processes all the discourse features for one entity at a time, while the local approach reads one sentence (or one sentence pair) at a time. In all the relevant experiments, global featurization showed a clear performance advantage (on average 1 point gain in F1).",
        "type": "Document"
      },
      {
        "id": "339fc34a-df58-4e7a-a678-e6bab73490f6",
        "metadata": {
          "vector_store_key": "1709.02271-4",
          "chunk_id": 35,
          "document_id": "1709.02271",
          "start_idx": 20085,
          "end_idx": 20915
        },
        "page_content": "Further, we found an input-length threshold for the discourse features to help (Section SECREF26 ). Not surprisingly, discourse does not contribute on shorter texts. Many of the feature grids are empty for these shorter texts\u2013 either there are no coreference chains or they are not correctly resolved. Currently we only have empirical results on short novel chunks and movie reviews, but believe the finding would generalize to Twitter or blog posts. Discourse embeddings. It does not come as a surprise that discourse embedding-based models perform better than their relation probability-based peers. The former (i) leverages the weight learning of the entire computational graph of the CNN rather than only the softmax layer, as the PV models do, and (ii) provides a more fine-grained featurization of the discourse information.",
        "type": "Document"
      },
      {
        "id": "4ec28905-ae2e-4b47-9203-18e381e85532",
        "metadata": {
          "vector_store_key": "2003.12738-2",
          "chunk_id": 34,
          "document_id": "2003.12738",
          "start_idx": 19171,
          "end_idx": 19881
        },
        "page_content": "Specifically, we use a pre-trained BERT to encode a generated sentence and a ground-truth response, and average the output representation of both to obtain the sentence embeddings. We denote such contextualized sentence embedding as $\\textbf {EMB}_\\textbf {BERT}$. In the human evaluation, we prepare multiple-choice questions for human evaluators and the answers are the generation results from the five models (Seq2Seq, CVAE, Transformer, GVT, and SVT). we first randomly sample 100 dialogues and their corresponding responses from our models and the baselines. For each response, we assign three human annotators to select the most coherent (on topic) response to the context (multiple answers are allowed).",
        "type": "Document"
      },
      {
        "id": "4ddc1ee3-b3a9-4f12-8a72-923df68a9d37",
        "metadata": {
          "vector_store_key": "1912.05238-7",
          "chunk_id": 13,
          "document_id": "1912.05238",
          "start_idx": 7386,
          "end_idx": 8185
        },
        "page_content": "To measure the valuation, we make use of implicit association tests (IATs) and their connections to word embeddings. Word and Sentence Embeddings. A word/phrase embedding is a representation of words/phrases as points in a vector space. All approaches have in common that more related or even similar text entities lie close to each other in the vector space, whereas distinct words/phrases can be found in distant regions BIBREF10. This enables determining semantic similarities in a language. Although these techniques have been around for some time, their potential increased considerably with the emergence of deep distributional approaches. In contrast to previous implementations, those embeddings are built on neural networks (NNs) and enable a rich variety of mathematical vector operations.",
        "type": "Document"
      },
      {
        "id": "400b1788-5a30-45d8-93c4-ba08a91525cc",
        "metadata": {
          "vector_store_key": "1709.02271-4",
          "chunk_id": 36,
          "document_id": "1709.02271",
          "start_idx": 20609,
          "end_idx": 21458
        },
        "page_content": "The former (i) leverages the weight learning of the entire computational graph of the CNN rather than only the softmax layer, as the PV models do, and (ii) provides a more fine-grained featurization of the discourse information. Rather than merely taking a probability over grammatical relation transitions (in GR) or discourse relation types (in RST), in DE-based models we learn the dependency between grammatical relation transitions/discourse relations through the INLINEFORM0 -sized filter sweeps. To further study the information encoded in the discourse embeddings, we perform t-SNE clustering BIBREF20 on them, using the best performing model CNN2-DE (global). We examine the closest neighbors of each embedding, and observe that similar discourse relations tend to go together (e.g., explanation and interpretation; consequence and result).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are discourse features incorporated into the model?",
    "llm_answer": "Discourse features are embedded in a parallel CNN at the input end, or used as a feature vector concatenated to the output layer.  Global featurization of discourse embeddings is also used.\n",
    "context": [
      {
        "id": "339fc34a-df58-4e7a-a678-e6bab73490f6",
        "metadata": {
          "vector_store_key": "1709.02271-4",
          "chunk_id": 35,
          "document_id": "1709.02271",
          "start_idx": 20085,
          "end_idx": 20915
        },
        "page_content": "Further, we found an input-length threshold for the discourse features to help (Section SECREF26 ). Not surprisingly, discourse does not contribute on shorter texts. Many of the feature grids are empty for these shorter texts\u2013 either there are no coreference chains or they are not correctly resolved. Currently we only have empirical results on short novel chunks and movie reviews, but believe the finding would generalize to Twitter or blog posts. Discourse embeddings. It does not come as a surprise that discourse embedding-based models perform better than their relation probability-based peers. The former (i) leverages the weight learning of the entire computational graph of the CNN rather than only the softmax layer, as the PV models do, and (ii) provides a more fine-grained featurization of the discourse information.",
        "type": "Document"
      },
      {
        "id": "68c77081-d5b8-46ff-9891-4dd59b0b0356",
        "metadata": {
          "vector_store_key": "1709.02271-4",
          "chunk_id": 37,
          "document_id": "1709.02271",
          "start_idx": 21458,
          "end_idx": 22170
        },
        "page_content": "We examine the closest neighbors of each embedding, and observe that similar discourse relations tend to go together (e.g., explanation and interpretation; consequence and result). Some examples are given in Table TABREF29 . However, it is unclear how this pattern helps improve classification performance. We intend to investigate this question in future work. Global vs. Local featurization. As described in Section SECREF17 , the global reading processes all the discourse features for one entity at a time, while the local approach reads one sentence (or one sentence pair) at a time. In all the relevant experiments, global featurization showed a clear performance advantage (on average 1 point gain in F1).",
        "type": "Document"
      },
      {
        "id": "a66cf568-96af-4294-a5cb-f0a729076778",
        "metadata": {
          "vector_store_key": "1709.02271-4",
          "chunk_id": 34,
          "document_id": "1709.02271",
          "start_idx": 19425,
          "end_idx": 20085
        },
        "page_content": "In addition, the type of discourse information and the way in which it is featurized are tantamount to this performance improvement: RST features provide overall stronger improvement, and the global reading scheme for discourse embedding works better than the local one. The discourse embedding proves to be a superior featurization technique, as evidenced by the generally higher performance of CNN2-DE models over CNN2-PV models. With an SVM, where the option is not available, we are only able to use relation probability vectors to obtain a very modest performance improvement. Further, we found an input-length threshold for the discourse features to help",
        "type": "Document"
      },
      {
        "id": "9ab27152-b44c-4df8-a340-9ac56d72fc72",
        "metadata": {
          "vector_store_key": "2002.01359-4",
          "chunk_id": 23,
          "document_id": "2002.01359",
          "start_idx": 13359,
          "end_idx": 14132
        },
        "page_content": "In this section, we describe these steps briefly and then present analyses of the collected dataset. All the services are implemented using a SQL engine. Since entity attributes are often correlated, we decided not to sample synthetic entities and instead relied on sampling entities from Freebase. The dialogue simulator interacts with the services to generate valid dialogue outlines. The simulator consists of two agents playing the roles of the user and the system. Both agents interact with each other using a finite set of actions specified through dialogue acts over a probabilistic automaton designed to capture varied dialogue trajectories. At the start of the conversation, the user agent is seeded with a scenario, which is a sequence of intents to be fulfilled.",
        "type": "Document"
      },
      {
        "id": "c958f35c-5ec0-46ab-94bb-a449fe073b4b",
        "metadata": {
          "vector_store_key": "1709.02271-4",
          "chunk_id": 5,
          "document_id": "1709.02271",
          "start_idx": 3254,
          "end_idx": 3926
        },
        "page_content": "We apply these models to datasets of varying sizes and genres, and find that adding any discourse information improves AA consistently on longer documents, but has mixed results on shorter documents. Further, embedding the discourse features in a parallel CNN at the input end yields better performance than concatenating them to the output layer as a feature vector (Section SECREF3 ). The global featurization is more effective than the local one. We also show that SVMs, which can only use discourse probability vectors, neither produce a competitive performance (even with fine-tuning), nor generalize in using the discourse information effectively. Entity-grid model.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What discourse features are used?",
    "llm_answer": "The provided context describes discourse embeddings, but does not explicitly list the discourse features used.\n",
    "context": [
      {
        "id": "339fc34a-df58-4e7a-a678-e6bab73490f6",
        "metadata": {
          "vector_store_key": "1709.02271-4",
          "chunk_id": 35,
          "document_id": "1709.02271",
          "start_idx": 20085,
          "end_idx": 20915
        },
        "page_content": "Further, we found an input-length threshold for the discourse features to help (Section SECREF26 ). Not surprisingly, discourse does not contribute on shorter texts. Many of the feature grids are empty for these shorter texts\u2013 either there are no coreference chains or they are not correctly resolved. Currently we only have empirical results on short novel chunks and movie reviews, but believe the finding would generalize to Twitter or blog posts. Discourse embeddings. It does not come as a surprise that discourse embedding-based models perform better than their relation probability-based peers. The former (i) leverages the weight learning of the entire computational graph of the CNN rather than only the softmax layer, as the PV models do, and (ii) provides a more fine-grained featurization of the discourse information.",
        "type": "Document"
      },
      {
        "id": "54b7cd3f-057d-46d3-b5a8-589b56834b8a",
        "metadata": {
          "vector_store_key": "1709.05413-9",
          "chunk_id": 43,
          "document_id": "1709.05413",
          "start_idx": 23519,
          "end_idx": 24930
        },
        "page_content": "In this section, we describe the setup and results of our conversational modeling experiments on the data we collected using our fine-grained taxonomy of customer service dialogue acts. We begin with an overview of the features and classes used, followed by our experimental setup and results for each experiment performed. The following list describes the set of features used for our dialogue act classification tasks: Word/Punctuation: binary bag-of-word unigrams, binary existence of a question mark, binary existence of an exclamation mark in a turn Temporal: response time of a turn (time in seconds elapsed between the posting time of the previous turn and that of the current turn) Second-Person Reference: existence of an explicit second-person reference in the turn (you, your, you're) Emotion: count of words in each of the 8 emotion classes from the NRC emotion lexicon BIBREF28 (anger, anticipation, disgust, fear, joy, negative, positive, sadness, surprise, and trust) Dialogue: lexical indicators in the turn: opening greetings (hi, hello, greetings, etc), closing greetings (bye, goodbye), yes-no questions (turns with questions starting with do, did, can, could, etc), wh- questions (turns with questions starting with who, what, where, etc), thanking (thank*), apology (sorry, apolog*), yes-answer, and no-answer Table TABREF30 shows the division of classes we use for each of our experiments.",
        "type": "Document"
      },
      {
        "id": "75d16ed6-a7ad-40a1-90ee-ae1b3faeca70",
        "metadata": {
          "vector_store_key": "1709.05413-9",
          "chunk_id": 44,
          "document_id": "1709.05413",
          "start_idx": 23658,
          "end_idx": 24845
        },
        "page_content": "The following list describes the set of features used for our dialogue act classification tasks: Word/Punctuation: binary bag-of-word unigrams, binary existence of a question mark, binary existence of an exclamation mark in a turn Temporal: response time of a turn (time in seconds elapsed between the posting time of the previous turn and that of the current turn) Second-Person Reference: existence of an explicit second-person reference in the turn (you, your, you're) Emotion: count of words in each of the 8 emotion classes from the NRC emotion lexicon BIBREF28 (anger, anticipation, disgust, fear, joy, negative, positive, sadness, surprise, and trust) Dialogue: lexical indicators in the turn: opening greetings (hi, hello, greetings, etc), closing greetings (bye, goodbye), yes-no questions (turns with questions starting with do, did, can, could, etc), wh- questions (turns with questions starting with who, what, where, etc), thanking (thank*), apology (sorry, apolog*), yes-answer, and no-answer Table TABREF30 shows the division of classes we use for each of our experiments. We select our classes using the distribution of annotations we observe in our data collection phase",
        "type": "Document"
      },
      {
        "id": "80ff0c96-279e-464e-bf19-44d96bf94373",
        "metadata": {
          "vector_store_key": "2001.02380-0",
          "chunk_id": 9,
          "document_id": "2001.02380",
          "start_idx": 4741,
          "end_idx": 5549
        },
        "page_content": "For example, BIBREF20 incorporated discourse information into the task of automated text comprehension and benefited from such information without relying on explicit annotations of discourse structure during training, which outperformed state-of-the-art text comprehension systems at the time. Towards this goal, we begin by reviewing some previous work in the traditions sketched out above in the next section, and point out some open questions which we would like to address. In Section SECREF3 we present the discourse annotated data that we will be using, which covers a number of English text types from the Web annotated for 20 discourse relations in the framework of Rhetorical Structure Theory, and is enriched with human annotations of discourse relation signaling devices for a subset of the data.",
        "type": "Document"
      },
      {
        "id": "24f8dfec-c722-47c3-89b3-12384c534e78",
        "metadata": {
          "vector_store_key": "2002.11893-9",
          "chunk_id": 14,
          "document_id": "2002.11893",
          "start_idx": 8370,
          "end_idx": 9067
        },
        "page_content": "Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states. Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three experts were employed to manually annotate dialogue acts and states for 50 dialogues. The results show that our annotations are of high quality.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are proof paths?",
    "llm_answer": "Proof paths are paths from the thesis to leaf nodes in an argument tree, representing a particular line of reasoning.\n",
    "context": [
      {
        "id": "9543c15b-f9b0-4920-a896-b050761b6164",
        "metadata": {
          "vector_store_key": "2004.03034-4",
          "chunk_id": 22,
          "document_id": "2004.03034",
          "start_idx": 11252,
          "end_idx": 11929
        },
        "page_content": "In an argument tree, the claims from the thesis node (root) to each leaf node, form an argument path. This argument path represents a particular line of reasoning for the given thesis. Similarly, for each claim, all the claims along the path from the thesis to the claim, represent the context for the claim. For example, in Figure FIGREF1, the context for O1 consists of only the thesis, whereas the context for S3 consists of both the thesis and O1 since S3 is provided to support the claim O1 which is an opposing claim for the thesis. The claims are not constructed independently from their context since they are written in consideration with the line of reasoning so far.",
        "type": "Document"
      },
      {
        "id": "e4d7df02-b55d-43f9-83a8-fa7d9c86d0be",
        "metadata": {
          "vector_store_key": "2004.03034-4",
          "chunk_id": 15,
          "document_id": "2004.03034",
          "start_idx": 7805,
          "end_idx": 8563
        },
        "page_content": "Figure FIGREF1 shows a partial argument tree for the argument thesis \u201cPhysical torture of prisoners is an acceptable interrogation tool.\u201d. Each node in the argument tree corresponds to a claim, and these argument trees are constructed and edited collaboratively by the users of the platform. Except the thesis, every claim in the argument tree either opposes or supports its parent claim. Each path from the root to leaf nodes corresponds to an argument path which represents a particular line of reasoning on the given controversial topic. Moreover, each claim has impact votes assigned by the users of the platform. The impact votes evaluate how impactful a claim is within its context, which consists of its predecessor claims from the thesis of the tree.",
        "type": "Document"
      },
      {
        "id": "4754895f-840b-44a3-82ad-147d56484525",
        "metadata": {
          "vector_store_key": "2004.02393-1",
          "chunk_id": 0,
          "document_id": "2004.02393",
          "start_idx": 0,
          "end_idx": 736
        },
        "page_content": "NLP tasks that require multi-hop reasoning have recently enjoyed rapid progress, especially on multi-hop question answering BIBREF0, BIBREF1, BIBREF2. Advances have benefited from rich annotations of supporting evidence, as in the popular multi-hop QA and relation extraction benchmarks, e.g., HotpotQA BIBREF3 and DocRED BIBREF4, where the evidence sentences for the reasoning process were labeled by human annotators. Such evidence annotations are crucial for modern model training, since they provide finer-grained supervision for better guiding the model learning. Furthermore, they allow a pipeline fashion of model training, with each step, such as passage ranking and answer extraction, trained as a supervised learning sub-task.",
        "type": "Document"
      },
      {
        "id": "2e9bc875-8388-4e49-b220-41436528bd3d",
        "metadata": {
          "vector_store_key": "1906.03538-2",
          "chunk_id": 42,
          "document_id": "1906.03538",
          "start_idx": 24299,
          "end_idx": 25016
        },
        "page_content": "We combine online resources, web data and crowdsourcing and create a high-quality dataset, in order to drive research on this problem. Finally, we build and evaluate strong baseline supervised systems for this problem. Our hope is that this dataset would bring more attention to this important problem and would speed up the progress in this direction. There are two aspects that we defer to future work. First, the systems designed here assumed that the input are valid claim sentences. To make use of such systems, one needs to develop mechanisms to recognize valid argumentative structures. In addition, we ignore trustworthiness and credibility issues, important research issues that are addressed in other works.",
        "type": "Document"
      },
      {
        "id": "cb411974-f88f-44cc-8582-2035026bf8d0",
        "metadata": {
          "vector_store_key": "1807.08204-4",
          "chunk_id": 14,
          "document_id": "1807.08204",
          "start_idx": 7240,
          "end_idx": 7884
        },
        "page_content": "Note that the maximum proof score is given by the fact $f \\in \\mathcal {K}$ that maximises the similarity between its components and the goal $\\mathcal {K}$0 : solving the maximisation problem in eq:inference can be equivalently stated as a nearest neighbour search problem. In this work, we use ANNS during the forward pass for considering only the most promising proof paths during the construction of the neural network. From ex:inference, we can see that the inference problem can be reduced to a nearest neighbour search problem. Given a query $g$ , the problem is finding the fact(s) in $\\mathcal {K}$ that maximise the unification score.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What external sources are used?",
    "llm_answer": "Twitter, NASA (via Vermont Advanced Computing Core), NSF (via BIGDATA grant), Google, and DARPA.\n",
    "context": [
      {
        "id": "ada0c52d-5c4c-4087-8df6-931ef27e13d1",
        "metadata": {
          "vector_store_key": "1805.09959-1",
          "chunk_id": 43,
          "document_id": "1805.09959",
          "start_idx": 24782,
          "end_idx": 25486
        },
        "page_content": "The authors wish to acknowledge the Vermont Advanced Computing Core, which is supported by NASA (NNX-08AO96G) at the University of Vermont which provided High Performance Computing resources that contributed to the research results reported within this poster. EMC was supported by the Vermont Complex Systems Center. CMD and PSD were supported by an NSF BIGDATA grant IIS-1447634. There are three types of endpoints to access data from Twitter. The `spritzer' (1%) and `gardenhose' (10%) endpoints were both implemented to collect publicly posted relevant data for our analysis. The third type of endpoint is the `Firehose' feed, a full 100% sample, which can be purchased via subscription from Twitter.",
        "type": "Document"
      },
      {
        "id": "598753ae-f85a-4b8a-91f2-1a842414776d",
        "metadata": {
          "vector_store_key": "1906.03538-2",
          "chunk_id": 43,
          "document_id": "1906.03538",
          "start_idx": 24759,
          "end_idx": 25446
        },
        "page_content": "In addition, we ignore trustworthiness and credibility issues, important research issues that are addressed in other works. The authors would like to thank Jennifer Sheffield, Stephen Mayhew, Shyam Upadhyay, Nitish Gupta and the anonymous reviewers for insightful comments and suggestions. This work was supported in part by a gift from Google and by Contract HR0011-15-2-0025 with the US Defense Advanced Research Projects Agency (DARPA). The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. We provide brief statistics on the sources of different content in our dataset in Table TABREF46 .",
        "type": "Document"
      },
      {
        "id": "02e8c704-8405-466b-88ff-ff42a3075d4e",
        "metadata": {
          "vector_store_key": "1910.00458-9",
          "chunk_id": 47,
          "document_id": "1910.00458",
          "start_idx": 25974,
          "end_idx": 26708
        },
        "page_content": "The extractive QA tasks primarily focus on locating text spans from the given document/corpus to answer questions BIBREF2. Answers in abstractive datasets such as MS MARCO BIBREF24, SearchQA BIBREF25, and NarrativeQA BIBREF26 are human-generated and based on source documents or summaries in free text format. However, since annotators tend to copy spans as answers BIBREF27, the majority of answers are still extractive in these datasets. The multi-choice QA datasets are collected either via crowd sourcing, or collected from examinations designed by educational experts BIBREF7. In this type of QA datasets, besides token matching, a significant portion of questions require multi-sentence reasoning and external knowledge BIBREF5.",
        "type": "Document"
      },
      {
        "id": "d5542446-6a00-4824-bd44-fe7f4011be04",
        "metadata": {
          "vector_store_key": "1710.01507-4",
          "chunk_id": 0,
          "document_id": "1710.01507",
          "start_idx": 0,
          "end_idx": 689
        },
        "page_content": "The Internet provides instant access to a wide variety of online content, news included. Formerly, users had static preferences, gravitating towards their trusted sources, incurring an unwavering sense of loyalty. The same cannot be said for current trends since users are likely to go with any source readily available to them. In order to stay in business, news agencies have switched, in part, to a digital front. Usually, they generate revenue by (1) advertisements on their websites, or (2) a subscription based model for articles that might interest users. However, since the same information is available via multiple sources, no comment can be made on the preference of the reader.",
        "type": "Document"
      },
      {
        "id": "66991414-1b00-4293-8337-71d2acde5db6",
        "metadata": {
          "vector_store_key": "1910.12618-0",
          "chunk_id": 9,
          "document_id": "1910.12618",
          "start_idx": 5241,
          "end_idx": 5907
        },
        "page_content": "Textual sources such as weather reports on the other hand are easy to find, usually available on a daily basis and free. The main contribution of our paper is to suggest the use of a certain type of textual documents, namely daily weather report, to build forecasters of the daily national electricity load, average temperature and wind speed for both France and the United-Kingdom (UK). Consequently this work represents a significant break with traditional methods, and we do not intend to best state-of-the-art approaches. Textual information is naturally more fuzzy than numerical one, and as such the same accuracy is not expected from the presented approaches.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much better peformance is achieved in human evaluation when model is trained considering proposed metric?",
    "llm_answer": "The provided text states the proposed comparative evaluator correlates better with human judgment than BLEU and perplexity, and outperforms other parameterized metrics like the adversarial evaluator and ADEM.  It does *not* quantify the *amount* of improvement.\n",
    "context": [
      {
        "id": "239cc6aa-01e0-4622-91f9-05e7083798ee",
        "metadata": {
          "vector_store_key": "2002.05058-2",
          "chunk_id": 48,
          "document_id": "2002.05058",
          "start_idx": 26480,
          "end_idx": 27267
        },
        "page_content": "We can see that the proposed comparative evaluator with skill rating significantly outperforms all compared baselines, including comparative evaluator with averaged sample-level scores. This demonstrates the effectiveness of the skill rating system for performing model-level comparison with pairwise sample-level evaluation. In addition, the poor correlation between conventional evaluation metrics including BLEU and perplexity demonstrates the necessity of better automated evaluation metrics in open domain NLG evaluation. We further investigate the impact of imperfect metrics on training NLG models. As described in the human evaluation procedure, we perform 10 runs to test the reliability of each metric when used to perform hyperparameter tuning and early-stopping respectively.",
        "type": "Document"
      },
      {
        "id": "8e08d1ba-2d60-49ef-bb6e-78f7af40117a",
        "metadata": {
          "vector_store_key": "2002.05058-2",
          "chunk_id": 45,
          "document_id": "2002.05058",
          "start_idx": 25160,
          "end_idx": 25851
        },
        "page_content": "We can see that the proposed comparative evaluator correlates far better with human judgment than BLEU and perplexity. When compared with recently proposed parameterized metrics including adversarial evaluator and ADEM, our model consistently outperforms them by a large margin, which demonstrates that our comparison-based evaluation metric is able to evaluate sample quality more accurately. In addition, we find that evaluating generated samples by comparing it with a set of randomly selected samples or using sample-level skill rating performs almost equally well. This is not surprising as the employed skill rating is able to handle the inherent variance of players (i.e. NLG models).",
        "type": "Document"
      },
      {
        "id": "3fba92b6-8039-4697-9041-dbacce6feba2",
        "metadata": {
          "vector_store_key": "2002.05058-2",
          "chunk_id": 56,
          "document_id": "2002.05058",
          "start_idx": 31744,
          "end_idx": 32299
        },
        "page_content": "By transferring pretrained natural language understanding knowledge from BERT and fine-tuning with strong and weak supervision examples and human preference annotations, our model correlates better with human judgment than other compared metrics. In addition, we find that when used as evaluation metrics, conventional metrics such as BLEU and perplexity may affect the training stage of NLG models as they may lead to sub-optimal hyperparameter choice and checkpoint selection. Our model, in contrast, is much more reliable when performing these choices.",
        "type": "Document"
      },
      {
        "id": "0c070008-5c07-4c13-ab5e-76daec86eb90",
        "metadata": {
          "vector_store_key": "1909.09484-4",
          "chunk_id": 31,
          "document_id": "1909.09484",
          "start_idx": 16716,
          "end_idx": 17421
        },
        "page_content": "But our model actually generates each individual token of actions, and we consider a prediction to be correct only if every token of the model output matches the corresponding token in the ground truth. BLEU BIBREF19: The metric evaluates the quality of the final response generated by natural language generator. The metric is usually used to measure the performance of the task-oriented dialogue system. We also choose the following metrics to evaluate the efficiency of training the model: $\\mathbf {Time_{full}}$: The time for training the whole model, which is important for industry settings. $\\mathbf {Time_{DP}}$: The time for training the dialogue policy maker in a task-oriented dialogue system.",
        "type": "Document"
      },
      {
        "id": "efc35877-edef-4cf0-bbaf-7957784a0178",
        "metadata": {
          "vector_store_key": "2002.05058-5",
          "chunk_id": 51,
          "document_id": "2002.05058",
          "start_idx": 28525,
          "end_idx": 30211
        },
        "page_content": "We present several comparison examples in the Dailydialog dataset for qualitative analysis of the proposed comparative evaluator. From the first example, we can see that the comparative evaluator is capable of identifying that generic and dull responses (i.e. \u201cI don't know\u201d) should be considered as of worse quality. The second example suggests that our approach handles the diversity in possible responses well, as it regards both positive response and negative response as valid responses. Hopefully, these examples may provide us with some insights about why the proposed metric correlates better with human preference. To better understand the proposed comparative evaluator and analyze the relative importance of its different components, we conduct an ablation study with several variants of the proposed model: w/o comparison: Evaluating generated samples without comparison, which degrades to the adversarial evaluation method. w/o strong supervision: Training the comparative evaluator without \u201cstrong supervision\u201d, which models the inductive bias that human written reference samples are generally of better quality compared with that generated by NLG models. w/o weak supervision: Training without \u201cweak supervision\u201d, which models the inductive bias that the quality of NLG models generally improves during training. w/o human preference annotation Training without human annotated preference data (i.e. only with strong and weak supervision). w/o tie option The variant of comparative evaluator where the model must select the better sample rather than able to admit its uncertainty. w/o BERT The variant where the model is trained from scratch instead of fine-tuning BERT.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much transcribed data is available for for Ainu language?",
    "llm_answer": "Most of the recordings are not transcribed.\n",
    "context": [
      {
        "id": "343f2cee-7852-4aa6-8e39-b7512c6916e4",
        "metadata": {
          "vector_store_key": "2002.06675-2",
          "chunk_id": 5,
          "document_id": "2002.06675",
          "start_idx": 2733,
          "end_idx": 3367
        },
        "page_content": "At present, there are only less than 10 native speakers, and UNESCO listed their language as critically endangered in 2009 BIBREF8. In response to this situation, Ainu folklore and songs have been actively recorded since the late 20th century in efforts initiated by the Government of Japan. For example, the Ainu Museum started audio recording of Ainu folklore in 1976 with the cooperation of a few Ainu elders which resulted in the collection of speech data with the total duration of roughly 700 hours. This kind of data should be a key to the understanding of Ainu culture, but most of it is not transcribed and fully studied yet.",
        "type": "Document"
      },
      {
        "id": "ba823263-fed8-4a30-b347-6d2908e0948b",
        "metadata": {
          "vector_store_key": "2002.06675-2",
          "chunk_id": 9,
          "document_id": "2002.06675",
          "start_idx": 4973,
          "end_idx": 5710
        },
        "page_content": "There have so far been a few studies dealing with the Ainu language. ainulrec built a dependency tree bank in the scheme of Universal Dependencies. postag developed tools for part-of-speech (POS) tagging and word segmentation. Ainu speech recognition was tried by ainutrans with 2.5 hours of Ainu folklore data even though the Ainu language was not their main target. Their phone error rare was about 40% which is not an accuracy level for practical use yet. It appears that there has not been a substantial Ainu speech recognition study yet that utilizes corpora of a reasonable size. Therefore, our first step was to build a speech corpus for ASR based on the data sets provided by the Ainu Museum and the Nibutani Ainu Culture Museum.",
        "type": "Document"
      },
      {
        "id": "b26e7e1a-b5cc-4c67-a535-d96dcc4e617e",
        "metadata": {
          "vector_store_key": "2002.06675-2",
          "chunk_id": 6,
          "document_id": "2002.06675",
          "start_idx": 3108,
          "end_idx": 3861
        },
        "page_content": "This kind of data should be a key to the understanding of Ainu culture, but most of it is not transcribed and fully studied yet. The Ainu language is an agglutinative language and has some similarities to Japanese. However, its genealogical relationship with other languages has not been clearly understood yet. Among its features such as closed syllables and personal verbal affixes, one important feature is that there are many compound words. For example, a word atuykorkamuy (means \u201ca sea turtle\u201d) can be disassembled into atuy (\u201cthe sea\u201d), kor (\u201cto have\u201d), and kamuy (\u201cgod\u201d). Although the Ainu people did not traditionally have a writing system, the Ainu language is currently written following the examples in a reference book \u201cAkor itak\u201d BIBREF9.",
        "type": "Document"
      },
      {
        "id": "cef462e5-1aaf-42c3-86c5-482680798784",
        "metadata": {
          "vector_store_key": "2002.06675-2",
          "chunk_id": 2,
          "document_id": "2002.06675",
          "start_idx": 1082,
          "end_idx": 1668
        },
        "page_content": "Hence, there is a strong demand for an ASR system for the Ainu language. We started a project of Ainu ASR and this article is the first report of this project. We have built an Ainu speech corpus based on data provided by the Ainu Museum and the Nibutani Ainu Culture Museum. The oral recordings in this data consist of folklore and folk songs, and we chose the former to construct the ASR model. The end-to-end method of speech recognition has been proposed recently and has achieved performance comparable to that of the conventional DNN-HMM hybrid modeling BIBREF0, BIBREF1, BIBREF2.",
        "type": "Document"
      },
      {
        "id": "f77f5531-5ee9-4763-b5e5-e07851abc4e1",
        "metadata": {
          "vector_store_key": "2002.06675-2",
          "chunk_id": 1,
          "document_id": "2002.06675",
          "start_idx": 707,
          "end_idx": 1321
        },
        "page_content": "The Ainu are an indigenous people of northern Japan and Sakhakin in Russia, but their language has been fading away ever since the Meiji Restoration and Modernization. On the other hand, active efforts to preserve their culture have been initiated by the Government of Japan, and exceptionally large oral recordings have been made. Nevertheless, a majority of the recordings have not been transcribed and utilized effectively. Since transcribing them requires expertise in the Ainu language, not so many people are able to work on this task. Hence, there is a strong demand for an ASR system for the Ainu language.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What baseline approaches do they compare against?",
    "llm_answer": "ILP and ILP+MC.  (and various neural baselines)\n",
    "context": [
      {
        "id": "34560b01-4da0-4895-ae23-1cc40f23fb59",
        "metadata": {
          "vector_store_key": "1712.03547-2",
          "chunk_id": 23,
          "document_id": "1712.03547",
          "start_idx": 12678,
          "end_idx": 13085
        },
        "page_content": "We evaluated the proposed and the baseline method on the interpretability of the learned embeddings. We also evaluated the methods on different KG tasks and compared their performance. We found that the proposed method achieves better interpretability while maintaining comparable performance on KG tasks. As next steps, we plan to evaluate the generalizability of the method with more recent KG embeddings.",
        "type": "Document"
      },
      {
        "id": "fcbb237d-302f-45b0-886a-31ec58fabb9f",
        "metadata": {
          "vector_store_key": "1909.09484-4",
          "chunk_id": 35,
          "document_id": "1909.09484",
          "start_idx": 19125,
          "end_idx": 19742
        },
        "page_content": "An Adam optimizer BIBREF22 is used for training our models and baselines, with a learning rate of 0.001 for supervised training and 0.0001 for reinforcement learning. In reinforcement learning, the decay parameter $\\lambda $ is set to 0.8. The weight decay is set to 0.001. And early stopping is performed on developing set. The experimental results of the proposed model and baselines will be analyzed from the following aspects. BPRA Results: As shown in Table TABREF35, most of the models have similar performance on BPRA on these two datasets, which can guarantee a consistent impact on the dialogue policy maker.",
        "type": "Document"
      },
      {
        "id": "fa88df39-033d-4e0f-891b-162e0f9b0424",
        "metadata": {
          "vector_store_key": "1709.08299-4",
          "chunk_id": 19,
          "document_id": "1709.08299",
          "start_idx": 11042,
          "end_idx": 11836
        },
        "page_content": "The neural baselines are built by the following steps. We first use the shared training data to build a general systems, and choose the best performing model (in terms of cloze validation set) as baseline. Use User Query validation data for further tuning the systems with 10-fold cross-validations. Increase dropout rate BIBREF9 to 0.5 for preventing over-fitting issue. All baseline systems are chosen according to the performance of the validation set. The participant system results are given in Table 2 and 3 . As we can see that two neural baselines are competitive among participant systems and AoA Reader successfully outperform AS Reader and all participant systems in single model condition, which proves that it is a strong baseline system even without further fine-tuning procedure.",
        "type": "Document"
      },
      {
        "id": "dcbb3e93-d5ad-43bf-80ac-ec1ae660eda5",
        "metadata": {
          "vector_store_key": "1812.07023-3",
          "chunk_id": 31,
          "document_id": "1812.07023",
          "start_idx": 17294,
          "end_idx": 18075
        },
        "page_content": "Their approach, however, is not public as of yet. We observe the following for our models: Since the official test set has not been released publicly, results reported on the official test set have been provided by the challenge organizers. For the prototype test set and for the ablation study presented in Table TABREF24 , we use the same code for evaluation metrics as used by BIBREF11 for fairness and comparability. We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below: Our primary architectural differences over the baseline model are: not concatenating the question, answer pairs before encoding them, the auxiliary decoder module, and using the Time-Extended FiLM module for feature extraction.",
        "type": "Document"
      },
      {
        "id": "9838eb05-14ff-4887-a72d-41e0c96027f0",
        "metadata": {
          "vector_store_key": "1807.09671-4",
          "chunk_id": 65,
          "document_id": "1807.09671",
          "start_idx": 37179,
          "end_idx": 37826
        },
        "page_content": "Therefore, in the following experiments, we focus on the direct comparison with the ILP and ILP+MC and leave the comparison to other baselines as future work. The proposed method achieved better summarization performance on Eng, CS2016, movie, and peer than the ILP baseline. Unfortunately, it does not work as expected on two courses for student responses (Stat2015 and Stat2016), review camera and news documents. This leaves the research question when and why the proposed method works better. In order to investigate what are key factors that impact the performance, we would like to perform additional experiments using synthesized data sets.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "how many domains did they experiment with?",
    "llm_answer": "20\n",
    "context": [
      {
        "id": "63424b33-ce8f-4d5e-94ed-403d702844ba",
        "metadata": {
          "vector_store_key": "1907.11499-2",
          "chunk_id": 18,
          "document_id": "1907.11499",
          "start_idx": 10010,
          "end_idx": 10650
        },
        "page_content": "For this, we created a third test set from the New York Times, in addition to our Wikipedia-based English and Chinese datasets. For all three corpora, we randomly sampled two documents for each domain, and then from each document, we sampled one long paragraph or a few consecutive short paragraphs containing 8\u201312 sentences. Amazon Mechanical Turkers were asked to read these sentences and assign a domain based on the seven labels used in this paper (multiple labels were allowed). Participants were provided with domain definitions. We obtained five annotations per sentence and adopted the majority label as the sentence's domain label.",
        "type": "Document"
      },
      {
        "id": "786eaa4f-6843-428f-b802-626e6014bcbf",
        "metadata": {
          "vector_store_key": "2002.01359-4",
          "chunk_id": 22,
          "document_id": "2002.01359",
          "start_idx": 12592,
          "end_idx": 13359
        },
        "page_content": "The 20 domains present across the train, dev and test datasets are listed in Table TABREF10, as are the details regarding which domains are present in each of the datasets. We create synthetic implementations of a total of 45 services or APIs over these domains. Our simulator framework interacts with these services to generate dialogue outlines, which are structured representations of dialogue semantics. We then use a crowd-sourcing procedure to paraphrase these outlines to natural language utterances. Our novel crowd-sourcing procedure preserves all annotations obtained from the simulator and does not require any extra annotations after dialogue collection. In this section, we describe these steps briefly and then present analyses of the collected dataset.",
        "type": "Document"
      },
      {
        "id": "a0e7fdb9-ae87-4700-a7f4-76bf89caa336",
        "metadata": {
          "vector_store_key": "1907.11499-3",
          "chunk_id": 2,
          "document_id": "1907.11499",
          "start_idx": 957,
          "end_idx": 1759
        },
        "page_content": "The ability to handle a wide variety of domains has become more pertinent with the rise of data-hungry machine learning techniques like neural networks and their application to a plethora of textual media ranging from news articles to twitter, blog posts, medical journals, Reddit comments, and parliamentary debates BIBREF0 , BIBREF3 , BIBREF4 , BIBREF5 . The question of how to best deal with multiple domains when training data is available for one or few of them has met with much interest in the literature. The field of domain adaptation BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 aims at improving the learning of a predictive function in a target domain where there is little or no labeled data, using knowledge transferred from a source domain where sufficient labeled data is available.",
        "type": "Document"
      },
      {
        "id": "76aadc7f-33c4-44a9-af6c-872d377a9822",
        "metadata": {
          "vector_store_key": "1701.03214-2",
          "chunk_id": 7,
          "document_id": "1701.03214",
          "start_idx": 4151,
          "end_idx": 4868
        },
        "page_content": "Note that in the \u201cfine tuning\u201d method, the vocabulary obtained from the out-of-domain data is used for the in-domain data; while for the \u201cmulti domain\u201d and \u201cmixed fine tuning\u201d methods, we use a vocabulary obtained from the mixed in-domain and out-of-domain data for all the training stages. We conducted NMT domain adaptation experiments in two different settings as follows: Chinese-to-English translation was the focus of the high quality in-domain corpus setting. We utilized the resource rich patent out-of-domain data to augment the resource poor spoken language in-domain data. The patent domain MT was conducted on the Chinese-English subtask (NTCIR-CE) of the patent MT task at the NTCIR-10 workshop BIBREF9 .",
        "type": "Document"
      },
      {
        "id": "8a633645-d65e-4532-b1a1-8660a18e412d",
        "metadata": {
          "vector_store_key": "1907.11499-3",
          "chunk_id": 3,
          "document_id": "1907.11499",
          "start_idx": 1759,
          "end_idx": 2389
        },
        "page_content": "The field of domain adaptation BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 aims at improving the learning of a predictive function in a target domain where there is little or no labeled data, using knowledge transferred from a source domain where sufficient labeled data is available. Another line of work BIBREF11 , BIBREF12 , BIBREF13 assumes that labeled data may exist for multiple domains, but in insufficient amounts to train classifiers for one or more of them. The aim of multi-domain text classification is to leverage all the available resources in order to improve system performance across domains simultaneously.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How long is the dataset?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "9ca5340d-587b-4e94-bcbe-160570788673",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 35,
          "document_id": "1910.12618",
          "start_idx": 19294,
          "end_idx": 19871
        },
        "page_content": "For instance for random forests it can correspond to the maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014 and 2015 serve as validation set (730 days).",
        "type": "Document"
      },
      {
        "id": "73a83f60-b2c9-406d-b4cd-5720639e5f06",
        "metadata": {
          "vector_store_key": "1812.07023-4",
          "chunk_id": 18,
          "document_id": "1812.07023",
          "start_idx": 10358,
          "end_idx": 10986
        },
        "page_content": "After 10 such qa turns, the questioner wraps up by writing a summary of the video based on the conversation. Dataset statistics such as the number of dialogues, turns, and words for the avsd dataset are presented in Table TABREF5 . For the initially released prototype dataset, the training set of the avsd dataset corresponds to videos taken from the training set of the Charades dataset while the validation and test sets of the avsd dataset correspond to videos taken from the validation set of the Charades dataset. For the official dataset, training, validation and test sets are drawn from the corresponding Charades sets.",
        "type": "Document"
      },
      {
        "id": "731d76b9-6365-4a8c-872e-c14badc3b626",
        "metadata": {
          "vector_store_key": "1911.07228-0",
          "chunk_id": 12,
          "document_id": "1911.07228",
          "start_idx": 6115,
          "end_idx": 6702
        },
        "page_content": "The dataset contains four different types of label: Location (LOC), Person (PER), Organization (ORG) and Miscellaneous - Name of an entity that do not belong to 3 types above (Table TABREF15). Although the corpus has more information about the POS and chunks, but we do not use them as features in our model. There are two folders with 267 text files of training data and 45 text files of test data. They all have their own format. We take 21 first text files and 22 last text files and 22 sentences of the 22th text file and 55 sentences of the 245th text file to be a development data.",
        "type": "Document"
      },
      {
        "id": "c92bd6ba-08c0-4fbc-b92b-c509a6863d91",
        "metadata": {
          "vector_store_key": "1705.03151-5",
          "chunk_id": 45,
          "document_id": "1705.03151",
          "start_idx": 24956,
          "end_idx": 25631
        },
        "page_content": "The training data sets from the seven languages are as follows: Assamese 75 hours, Bengali 87 hours, Cantonese 175 hours, Georgian 64 hours, Pashto 111 hours, Tagalog 116 hours and Turkish 107 hours. The average duration of the test utterances is $4.15$ seconds, ranging from $0.19$ seconds to $30.85$ seconds. The AP16-OL7 database was originally created by Speechocean Inc., targeted towards various speech processing tasks (mainly speech recognition), and was used as the official data for the AP16-OLR LID challenge. The database contains seven datasets, each in a particular language. These are: Mandarin, Cantonese, Indonesian, Japanese, Russian, Korean and Vietnamese.",
        "type": "Document"
      },
      {
        "id": "d87d327c-2858-4b5d-ae8a-860d401f63bd",
        "metadata": {
          "vector_store_key": "1908.05434-1",
          "chunk_id": 27,
          "document_id": "1908.05434",
          "start_idx": 14432,
          "end_idx": 15064
        },
        "page_content": "We have ensured that the raw dataset has no overlap with the labeled dataset to avoid bias in test accuracy. While it is possible to scrape more raw data, we did not observe significant improvements in model performances when the size of raw data increased from INLINEFORM0 70,000 to INLINEFORM1 170,000, hence we assume that the current raw dataset is sufficiently large. The labeled dataset is called Trafficking-10k. It consists of 12,350 ads from Backpage labeled by experts in human trafficking detection BIBREF9 . Each label is one of seven ordered levels of likelihood that the corresponding ad comes from a human trafficker.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is a string kernel?",
    "llm_answer": "A string kernel is a kernel function used to measure the pairwise similarity between text samples, based on character n-grams.\n",
    "context": [
      {
        "id": "004f0882-25f0-4533-aa1d-40d479363460",
        "metadata": {
          "vector_store_key": "1811.01734-0",
          "chunk_id": 17,
          "document_id": "1811.01734",
          "start_idx": 10415,
          "end_idx": 11163
        },
        "page_content": "For example, in text mining, string kernels can be used to measure the pairwise similarity between text samples, simply based on character n-grams. Various string kernel functions have been proposed to date BIBREF35 , BIBREF38 , BIBREF19 . Perhaps one of the most recently introduced string kernels is the histogram intersection string kernel BIBREF19 . For two strings over an alphabet INLINEFORM0 , INLINEFORM1 , the intersection string kernel is formally defined as follows: DISPLAYFORM0  where INLINEFORM0 is the number of occurrences of n-gram INLINEFORM1 as a substring in INLINEFORM2 , and INLINEFORM3 is the length of INLINEFORM4 . The spectrum string kernel or the presence bits string kernel can be defined in a similar fashion BIBREF19 .",
        "type": "Document"
      },
      {
        "id": "4868357c-48e5-40a3-a4f8-c6c457be82dd",
        "metadata": {
          "vector_store_key": "1811.01734-0",
          "chunk_id": 2,
          "document_id": "1811.01734",
          "start_idx": 1210,
          "end_idx": 2029
        },
        "page_content": "In fact, methods based on string kernels have demonstrated impressive results in various text classification tasks ranging from native language identification BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 and authorship identification BIBREF22 to dialect identification BIBREF23 , BIBREF17 , BIBREF24 , sentiment analysis BIBREF10 , BIBREF25 and automatic essay scoring BIBREF26 . As long as a labeled training set is available, string kernels can reach state-of-the-art results in various languages including English BIBREF19 , BIBREF10 , BIBREF26 , Arabic BIBREF27 , BIBREF20 , BIBREF17 , BIBREF24 , Chinese BIBREF25 and Norwegian BIBREF20 . Different from all these recent approaches, we use unlabeled data from the test set in a transductive setting in order to significantly increase the performance of string kernels.",
        "type": "Document"
      },
      {
        "id": "8714fb2e-e7ce-4965-b757-b8365612624f",
        "metadata": {
          "vector_store_key": "1811.01734-0",
          "chunk_id": 14,
          "document_id": "1811.01734",
          "start_idx": 8613,
          "end_idx": 9570
        },
        "page_content": "String kernels were also successfully used in authorship identification BIBREF22 . More recently, various combinations of string kernels reached state-of-the-art accuracy rates in native language identification BIBREF19 and Arabic dialect identification BIBREF17 . Interestingly, string kernels have been used in cross-domain settings without any domain adaptation, obtaining impressive results. For instance, Ionescu et al. BIBREF19 have employed string kernels in a cross-corpus (and implicitly cross-domain) native language identification experiment, improving the state-of-the-art accuracy by a remarkable INLINEFORM0 . Gim\u00e9nez-P\u00e9rez et al. BIBREF10 have used string kernels for single-source and multi-source polarity classification. Remarkably, they obtain state-of-the-art performance without using knowledge from the target domain, which indicates that string kernels provide robust results in the cross-domain setting without any domain adaptation.",
        "type": "Document"
      },
      {
        "id": "8f085286-8cb3-4cea-b472-a90194c5afcf",
        "metadata": {
          "vector_store_key": "1811.01734-0",
          "chunk_id": 18,
          "document_id": "1811.01734",
          "start_idx": 11163,
          "end_idx": 11981
        },
        "page_content": "The spectrum string kernel or the presence bits string kernel can be defined in a similar fashion BIBREF19 . Transductive string kernels. We present a simple and straightforward approach to produce a transductive similarity measure suitable for strings. We take the following steps to derive transductive string kernels. For a given kernel (similarity) function INLINEFORM0 , we first build the full kernel matrix INLINEFORM1 , by including the pairwise similarities of samples from both the train and the test sets. For a training set INLINEFORM2 of INLINEFORM3 samples and a test set INLINEFORM4 of INLINEFORM5 samples, such that INLINEFORM6 , each component in the full kernel matrix is defined as follows: DISPLAYFORM0  where INLINEFORM0 and INLINEFORM1 are samples from the set INLINEFORM2 , for all INLINEFORM3 .",
        "type": "Document"
      },
      {
        "id": "4480c788-1799-4224-a131-8e0203ba8a5d",
        "metadata": {
          "vector_store_key": "1811.01734-0",
          "chunk_id": 16,
          "document_id": "1811.01734",
          "start_idx": 9570,
          "end_idx": 10341
        },
        "page_content": "It is important to note that the training and the test speech samples prepared for the shared task were recorded in different setups BIBREF37 , or in other words, the training and the test sets are drawn from different distributions. Different from all these recent approaches BIBREF19 , BIBREF10 , BIBREF17 , we use unlabeled data from the target domain to significantly increase the performance of string kernels in cross-domain text classification, particularly in English polarity classification. String kernels. Kernel functions BIBREF38 capture the intuitive notion of similarity between objects in a specific domain. For example, in text mining, string kernels can be used to measure the pairwise similarity between text samples, simply based on character n-grams.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they correlate NED with emotional bond levels?",
    "llm_answer": "They compute Pearson's correlation coefficients between the proposed NED measure (both patient-to-therapist and therapist-to-patient) and patient-perceived emotional bond ratings.  They also correlate the emotional bond with baseline measures.\n",
    "context": [
      {
        "id": "c8218bb0-9dc8-4b67-be6d-a4e08820f59e",
        "metadata": {
          "vector_store_key": "1804.08782-1",
          "chunk_id": 26,
          "document_id": "1804.08782",
          "start_idx": 14985,
          "end_idx": 15706
        },
        "page_content": "According to prior work, both from domain theory BIBREF16 and from experimental validation BIBREF6 , a high emotional bond in patient-therapist interactions in the suicide therapy domain is associated with more entrainment. In this experiment, we compute the correlation of the proposed NED measure with the patient-perceived emotional bond ratings. Since the proposed measure is asymmetric in nature, we compute the measures for both patient-to-therapist and therapist-to-patient entrainment. We also compute the correlation of emotional bond with the baselines used in Experiment 1. We report Pearson's correlation coefficients ( INLINEFORM0 ) for this experiment in Table TABREF26 along with their INLINEFORM1 -values.",
        "type": "Document"
      },
      {
        "id": "7bd66d3a-d44c-4efb-9268-3959cb400619",
        "metadata": {
          "vector_store_key": "1804.08782-1",
          "chunk_id": 27,
          "document_id": "1804.08782",
          "start_idx": 15706,
          "end_idx": 16526
        },
        "page_content": "We report Pearson's correlation coefficients ( INLINEFORM0 ) for this experiment in Table TABREF26 along with their INLINEFORM1 -values. We test against the null hypothesis INLINEFORM2 that there is no linear association between emotional bond and the candidate measure. Results in Table TABREF26 show that the patient-to-therapist NED is negatively correlated with emotional bond with high statistical significance ( INLINEFORM0 ). This negative sign is consistent with previous studies as higher distance in acoustic features indicates lower entrainment. However, the therapist-to-patient NED does not have a significant correlation with emotional bond. A possible explanation for this finding is that the emotional bond is reported by the patient and influenced by the degree of their perceived therapist-entrainment.",
        "type": "Document"
      },
      {
        "id": "020e09a4-46b6-4e40-bd9d-d7f3d7083c1d",
        "metadata": {
          "vector_store_key": "1804.08782-1",
          "chunk_id": 29,
          "document_id": "1804.08782",
          "start_idx": 17064,
          "end_idx": 17826
        },
        "page_content": "Figure FIGREF27 shows the results of a session with high emotional bond and another one with low emotional bond (with values of 7 and 1 respectively) as a 2-dimensional scatter plot. Visibly there is some separation between the sessions with low and high emotional bond. In this work, a novel deep neural network-based Neural Entrainment Distance (NED) measure is proposed for capturing entrainment in conversational speech. The neural network architecture consisting of an encoder and a decoder is trained on the Fisher corpus in an unsupervised training framework and then the measure is defined on the bottleneck embedding. We show that the proposed measure can distinguish between real and fake sessions by capturing presence of entrainment in real sessions.",
        "type": "Document"
      },
      {
        "id": "a711fffe-5026-48a0-b378-e49c86bb0101",
        "metadata": {
          "vector_store_key": "1804.08782-1",
          "chunk_id": 28,
          "document_id": "1804.08782",
          "start_idx": 16226,
          "end_idx": 17064
        },
        "page_content": "A possible explanation for this finding is that the emotional bond is reported by the patient and influenced by the degree of their perceived therapist-entrainment. Thus, equipped with an asymmetric measure, we are also able to identify the latent directionality of the emotional bond metric. The complexity measure (Baseline 2) also shows statistically significant correlation, but the value of INLINEFORM1 is lower than that of the proposed measure. To analyze the embeddings encoded by our model, we also compute a t-SNE BIBREF20 transformation of the difference of all patient-to-therapist turn embedding pairs, denoted as INLINEFORM0 in Equation (3). Figure FIGREF27 shows the results of a session with high emotional bond and another one with low emotional bond (with values of 7 and 1 respectively) as a 2-dimensional scatter plot.",
        "type": "Document"
      },
      {
        "id": "7888c37a-9698-4e64-9bba-9941be318eeb",
        "metadata": {
          "vector_store_key": "1804.08782-1",
          "chunk_id": 25,
          "document_id": "1804.08782",
          "start_idx": 13324,
          "end_idx": 14126
        },
        "page_content": "Also, for baseline 2 we choose the session with higher value of the measure as real, since it measures similarity. As we can see in Table TABREF24 , our proposed NED measure achieves higher accuracy than all baselines on the Fisher corpus. The accuracy of our measure declines in the Suicide corpus as compared to the Fisher corpus, which is probably due to data mismatch as the model was trained on Fisher (mismatch of acoustics, recording conditions, sampling frequency, interaction style etc.). However, our measure still performs better than all baselines on Suicide corpus. According to prior work, both from domain theory BIBREF16 and from experimental validation BIBREF6 , a high emotional bond in patient-therapist interactions in the suicide therapy domain is associated with more entrainment.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was their F1 score on the Bengali NER corpus?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "a91def49-1da0-451d-b442-ba0eadd3a583",
        "metadata": {
          "vector_store_key": "1701.09123-7",
          "chunk_id": 75,
          "document_id": "1701.09123",
          "start_idx": 40392,
          "end_idx": 41142
        },
        "page_content": "(see Table TABREF58 ), results in Table TABREF71 show that our nl-cluster model outperforms the best result published on CoNLL 2002 BIBREF45 by 3.83 points in F1 score. Adding the English Illinois NER gazetteers BIBREF31 and trigram and character n-gram features increases the score to 85.04 F1, 5.41 points better than previous published work on this dataset. We also compared our system with the more recently developed SONAR-1 corpus and the companion NERD system distributed inside its release BIBREF33 . They report 84.91 F1 for the six main named entity types via 10-fold cross validation. For this comparison we chose the local, nl-cluster and nl-cluster-dict configurations from Table TABREF71 and run them on SONAR-1 using the same settings.",
        "type": "Document"
      },
      {
        "id": "881327b3-fadf-4da1-b31b-7c5f8db473b2",
        "metadata": {
          "vector_store_key": "1707.03764-3",
          "chunk_id": 28,
          "document_id": "1707.03764",
          "start_idx": 14236,
          "end_idx": 14932
        },
        "page_content": "Overall, N-GrAM came first in the shared task, with a score of 0.8253 for gender 0.9184 for variety, a joint score of 0.8361 and an average score of 0.8599 (final rankings were taken from this average score BIBREF0 ). For the global scores, all languages are combined. We present finer-grained scores showing the breakdown per language in Table TABREF24 . We compare our gender and variety accuracies against the LDR-baseline BIBREF10 , a low dimensionality representation especially tailored to language variety identification, provided by the organisers. The final column, + 2nd shows the difference between N-GrAM and that achieved by the second-highest ranked system (excluding the baseline).",
        "type": "Document"
      },
      {
        "id": "377f848a-d1fe-418c-8704-a07215820d2f",
        "metadata": {
          "vector_store_key": "1701.02877-4",
          "chunk_id": 60,
          "document_id": "1701.02877",
          "start_idx": 31066,
          "end_idx": 31678
        },
        "page_content": "These reflect the raised OOV and drift rates found in previous work BIBREF12 , BIBREF53 . Another explanation is that there is higher noise in variation, and that the drift is not longitudinal, but rather general. This is partially addressed by RQ3, which we will address next, in Section \"RQ3: Impact of NE Diversity\" . To summarise, our findings are: [noitemsep] Overall, F1 scores vary widely across corpora. Trends can be marked in some genres. On average, newswire corpora and OntoNotes MZ are the easiest corpora and ACE UN, WEB and TWI are the hardest corpora for NER methods to reach good performance on.",
        "type": "Document"
      },
      {
        "id": "2931baeb-cf5d-4cf6-b187-7353329a1b45",
        "metadata": {
          "vector_store_key": "1701.02877-1",
          "chunk_id": 7,
          "document_id": "1701.02877",
          "start_idx": 3480,
          "end_idx": 4340
        },
        "page_content": "In line with prior analyses of NER performance BIBREF2 , BIBREF11 , we carry out corpus analysis and introduce briefly the NER methods used for experimentation. Unlike prior efforts, however, our main objectives are to uncover the impact of NE diversity and context diversity on performance (measured primarily by F1 score), and also to study the relationship between OOV NEs and features and F1. See Section \"Experiments\" for details. To ensure representativeness and comprehensiveness, our experimental findings are based on key benchmark NER corpora spanning multiple genres, time periods, and corpus annotation methodologies and guidelines. As detailed in Section \"Datasets\" , the corpora studied are OntoNotes BIBREF29 , ACE BIBREF30 , MUC 7 BIBREF31 , the Ritter NER corpus BIBREF21 , the MSM 2013 corpus BIBREF32 , and the UMBC Twitter corpus BIBREF33 .",
        "type": "Document"
      },
      {
        "id": "c58505f0-2392-4562-87d7-2f3130b95957",
        "metadata": {
          "vector_store_key": "1701.02877-4",
          "chunk_id": 52,
          "document_id": "1701.02877",
          "start_idx": 26848,
          "end_idx": 27485
        },
        "page_content": "What is very interesting to see is that they are much higher than the results on the biggest training corpus, OntoNotes NW. For instance, SENNA has an F1 of 78.04 on OntoNotes, compared to an F1 of 92.39 and 86.44 for CoNLL Test A and Test B respectively. So even though OntoNotes NW is more than twice the size of CoNLL in terms of NEs (see Table 4 ), NERC performance is much higher on CoNLL. NERC performance with respect to training corpus size is represented in Figure 1 . The latter figure confirms that although there is some correlation between corpus size and F1, the variance between results on comparably sized corpora is big.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the size of the dataset?",
    "llm_answer": "The answer is not found in the context.  While various dataset sizes are mentioned (e.g., 2,557 days, 12,350 ads), there is no single figure for the overall size of *one* dataset.\n",
    "context": [
      {
        "id": "9ca5340d-587b-4e94-bcbe-160570788673",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 35,
          "document_id": "1910.12618",
          "start_idx": 19294,
          "end_idx": 19871
        },
        "page_content": "For instance for random forests it can correspond to the maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014 and 2015 serve as validation set (730 days).",
        "type": "Document"
      },
      {
        "id": "8be62763-27f4-46ff-b2bb-edb6a6b2fa04",
        "metadata": {
          "vector_store_key": "1607.06025-3",
          "chunk_id": 42,
          "document_id": "1607.06025",
          "start_idx": 24463,
          "end_idx": 25198
        },
        "page_content": "The new dataset consists of training set, which is generated using examples from the original training set, and a development set, which is generated from the original development set. The beam size for beam search was set to 1. The details of the decision are presented in Section SECREF35 . Some datasets were constructed by filtering the generated datasets according to various thresholds. Thus, the generated datasets were constructed to contain enough examples, so that the filtered datasets had at least the number of examples as the original dataset. In the end, all the datasets were trimmed down to the size of the original dataset by selecting the samples sequentially from the beginning until the dataset had the right size.",
        "type": "Document"
      },
      {
        "id": "73a83f60-b2c9-406d-b4cd-5720639e5f06",
        "metadata": {
          "vector_store_key": "1812.07023-4",
          "chunk_id": 18,
          "document_id": "1812.07023",
          "start_idx": 10358,
          "end_idx": 10986
        },
        "page_content": "After 10 such qa turns, the questioner wraps up by writing a summary of the video based on the conversation. Dataset statistics such as the number of dialogues, turns, and words for the avsd dataset are presented in Table TABREF5 . For the initially released prototype dataset, the training set of the avsd dataset corresponds to videos taken from the training set of the Charades dataset while the validation and test sets of the avsd dataset correspond to videos taken from the validation set of the Charades dataset. For the official dataset, training, validation and test sets are drawn from the corresponding Charades sets.",
        "type": "Document"
      },
      {
        "id": "d87d327c-2858-4b5d-ae8a-860d401f63bd",
        "metadata": {
          "vector_store_key": "1908.05434-1",
          "chunk_id": 27,
          "document_id": "1908.05434",
          "start_idx": 14432,
          "end_idx": 15064
        },
        "page_content": "We have ensured that the raw dataset has no overlap with the labeled dataset to avoid bias in test accuracy. While it is possible to scrape more raw data, we did not observe significant improvements in model performances when the size of raw data increased from INLINEFORM0 70,000 to INLINEFORM1 170,000, hence we assume that the current raw dataset is sufficiently large. The labeled dataset is called Trafficking-10k. It consists of 12,350 ads from Backpage labeled by experts in human trafficking detection BIBREF9 . Each label is one of seven ordered levels of likelihood that the corresponding ad comes from a human trafficker.",
        "type": "Document"
      },
      {
        "id": "731d76b9-6365-4a8c-872e-c14badc3b626",
        "metadata": {
          "vector_store_key": "1911.07228-0",
          "chunk_id": 12,
          "document_id": "1911.07228",
          "start_idx": 6115,
          "end_idx": 6702
        },
        "page_content": "The dataset contains four different types of label: Location (LOC), Person (PER), Organization (ORG) and Miscellaneous - Name of an entity that do not belong to 3 types above (Table TABREF15). Although the corpus has more information about the POS and chunks, but we do not use them as features in our model. There are two folders with 267 text files of training data and 45 text files of test data. They all have their own format. We take 21 first text files and 22 last text files and 22 sentences of the 22th text file and 55 sentences of the 245th text file to be a development data.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many examples do they have in the target domain?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "e0019a17-84f2-4e13-9ce7-eea52cc4a2f1",
        "metadata": {
          "vector_store_key": "2002.01359-4",
          "chunk_id": 26,
          "document_id": "2002.01359",
          "start_idx": 15153,
          "end_idx": 15785
        },
        "page_content": "Figure FIGREF8 shows the frequency of the different dialogue acts contained in the dataset. The dataset also contains a significant number of unseen domains/APIs in the dev and test sets. 77% of the dialogue turns in the test set and 45% of the turns in dev set contain at least one service not present in the training set. This facilitates the development of models which can generalize to new domains with very few labelled examples. The submissions from 25 teams included a variety of approaches and innovative solutions to specific problems posed by this dataset. For the workshop, we received submissions from 9 of these teams.",
        "type": "Document"
      },
      {
        "id": "09b3bc64-e8c7-49da-b29c-9339375bb494",
        "metadata": {
          "vector_store_key": "2002.11893-4",
          "chunk_id": 34,
          "document_id": "2002.11893",
          "start_idx": 19370,
          "end_idx": 20128
        },
        "page_content": "According to the type of user goal, we group the dialogues in the training set into five categories: 417 dialogues have only one sub-goal in HAR domains. 1573 dialogues have multiple sub-goals (2$\\sim $3) in HAR domains. However, these sub-goals do not have cross-domain informable slots. 691 dialogues have multiple sub-goals in HAR domains and at least one sub-goal in the metro or taxi domain (3$\\sim $5 sub-goals). The sub-goals in HAR domains do not have cross-domain informable slots. 1,759 dialogues have multiple sub-goals (2$\\sim $5) in HAR domains with cross-domain informable slots. 572 dialogues have multiple sub-goals in HAR domains with cross-domain informable slots and at least one sub-goal in the metro or taxi domain (3$\\sim $5 sub-goals).",
        "type": "Document"
      },
      {
        "id": "786eaa4f-6843-428f-b802-626e6014bcbf",
        "metadata": {
          "vector_store_key": "2002.01359-4",
          "chunk_id": 22,
          "document_id": "2002.01359",
          "start_idx": 12592,
          "end_idx": 13359
        },
        "page_content": "The 20 domains present across the train, dev and test datasets are listed in Table TABREF10, as are the details regarding which domains are present in each of the datasets. We create synthetic implementations of a total of 45 services or APIs over these domains. Our simulator framework interacts with these services to generate dialogue outlines, which are structured representations of dialogue semantics. We then use a crowd-sourcing procedure to paraphrase these outlines to natural language utterances. Our novel crowd-sourcing procedure preserves all annotations obtained from the simulator and does not require any extra annotations after dialogue collection. In this section, we describe these steps briefly and then present analyses of the collected dataset.",
        "type": "Document"
      },
      {
        "id": "63424b33-ce8f-4d5e-94ed-403d702844ba",
        "metadata": {
          "vector_store_key": "1907.11499-2",
          "chunk_id": 18,
          "document_id": "1907.11499",
          "start_idx": 10010,
          "end_idx": 10650
        },
        "page_content": "For this, we created a third test set from the New York Times, in addition to our Wikipedia-based English and Chinese datasets. For all three corpora, we randomly sampled two documents for each domain, and then from each document, we sampled one long paragraph or a few consecutive short paragraphs containing 8\u201312 sentences. Amazon Mechanical Turkers were asked to read these sentences and assign a domain based on the seven labels used in this paper (multiple labels were allowed). Participants were provided with domain definitions. We obtained five annotations per sentence and adopted the majority label as the sentence's domain label.",
        "type": "Document"
      },
      {
        "id": "5aba0c8a-1e24-40b4-8d8d-8579cf563fcd",
        "metadata": {
          "vector_store_key": "2002.11893-3",
          "chunk_id": 35,
          "document_id": "2002.11893",
          "start_idx": 20128,
          "end_idx": 20845
        },
        "page_content": "1,759 dialogues have multiple sub-goals (2$\\sim $5) in HAR domains with cross-domain informable slots. 572 dialogues have multiple sub-goals in HAR domains with cross-domain informable slots and at least one sub-goal in the metro or taxi domain (3$\\sim $5 sub-goals). The data statistics are shown in Table TABREF26. As mentioned in Section SECREF14, we generate independent multi-domain, cross multi-domain, and traffic domain sub-goals one by one. Thus in terms of the task complexity, we have S<M<CM and M<M+T<CM+T, which is supported by the average number of sub-goals, semantic tuples, and turns per dialogue in Table TABREF26. The average number of tokens also becomes larger when the goal becomes more complex.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the baseline model?",
    "llm_answer": "Baseline1, a model that takes two inputs (transcript and question) and outputs span scores.  A general system built using shared training data, chosen based on cloze validation set performance, and further tuned with 10-fold cross-validation on user query validation data.  Model 1 is also presented as a baseline.\n",
    "context": [
      {
        "id": "fa88df39-033d-4e0f-891b-162e0f9b0424",
        "metadata": {
          "vector_store_key": "1709.08299-4",
          "chunk_id": 19,
          "document_id": "1709.08299",
          "start_idx": 11042,
          "end_idx": 11836
        },
        "page_content": "The neural baselines are built by the following steps. We first use the shared training data to build a general systems, and choose the best performing model (in terms of cloze validation set) as baseline. Use User Query validation data for further tuning the systems with 10-fold cross-validations. Increase dropout rate BIBREF9 to 0.5 for preventing over-fitting issue. All baseline systems are chosen according to the performance of the validation set. The participant system results are given in Table 2 and 3 . As we can see that two neural baselines are competitive among participant systems and AoA Reader successfully outperform AS Reader and all participant systems in single model condition, which proves that it is a strong baseline system even without further fine-tuning procedure.",
        "type": "Document"
      },
      {
        "id": "39cb1ec4-6c9f-4dbd-b781-29fe9d4ad19f",
        "metadata": {
          "vector_store_key": "1912.01046-3",
          "chunk_id": 23,
          "document_id": "1912.01046",
          "start_idx": 13260,
          "end_idx": 13980
        },
        "page_content": "Following this idea, Baseline1 represents a span of sentences by concatenating the vectors of the starting and ending sentences. The left diagram in Fig. FIGREF15 illustrates the Baseline1 model. Model. The model takes two inputs, a transcript, $\\lbrace s_1, s_2, ... s_n\\rbrace $ where $s_i$ are individual sentences and a question, $q$. The output is the span scores, $y$, the scores over all possible spans. GLoVe BIBREF14 is used for the word representations in the transcript and the questions. We use two bi-LSTMs BIBREF15 to encode the transcript. where n is the number of sentences . The output of Passage-level Encoding, $p$, is a sequence of vector, $p_i$, which represents the latent meaning of each sentence.",
        "type": "Document"
      },
      {
        "id": "763bf540-a941-47b1-bdeb-25182eab991e",
        "metadata": {
          "vector_store_key": "2001.11381-4",
          "chunk_id": 55,
          "document_id": "2001.11381",
          "start_idx": 31798,
          "end_idx": 32460
        },
        "page_content": "Tampoco consideramos la utilizaci\u00f3n de un baseline de tipo aleatorio, porque los resultados carecer\u00edan de la homosintaxis y ser\u00eda sumamente f\u00e1cil obtener mejores resultados. Dicho lo anterior, el Modelo 1 podr\u00eda ser considerado como nuestro propio baseline. A continuaci\u00f3n presentamos un protocolo de evaluaci\u00f3n manual de los resultados obtenidos. El experimento consisti\u00f3 en la generaci\u00f3n de 15 frases por cada uno de los tres modelos propuestos. Para cada modelo, se consideraron tres queries: $Q=$ {AMOR, GUERRA, SOL}, generando 5 frases con cada uno. Las 15 frases fueron mezcladas entre s\u00ed y reagrupadas por queries, antes de presentarlas a los evaluadores.",
        "type": "Document"
      },
      {
        "id": "fcbb237d-302f-45b0-886a-31ec58fabb9f",
        "metadata": {
          "vector_store_key": "1909.09484-4",
          "chunk_id": 35,
          "document_id": "1909.09484",
          "start_idx": 19125,
          "end_idx": 19742
        },
        "page_content": "An Adam optimizer BIBREF22 is used for training our models and baselines, with a learning rate of 0.001 for supervised training and 0.0001 for reinforcement learning. In reinforcement learning, the decay parameter $\\lambda $ is set to 0.8. The weight decay is set to 0.001. And early stopping is performed on developing set. The experimental results of the proposed model and baselines will be analyzed from the following aspects. BPRA Results: As shown in Table TABREF35, most of the models have similar performance on BPRA on these two datasets, which can guarantee a consistent impact on the dialogue policy maker.",
        "type": "Document"
      },
      {
        "id": "5e76f24e-4c75-4a34-b69c-7fe1f8d12771",
        "metadata": {
          "vector_store_key": "1909.02635-2",
          "chunk_id": 15,
          "document_id": "1909.02635",
          "start_idx": 9069,
          "end_idx": 9719
        },
        "page_content": "This includes (i) Majority Class, (ii) Exact Match of an ingredient $e$ in recipe step $s_t$, and (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo$_{token}$ and ELMo$_{sent}$) BIBREF22 compared to GPT.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Where did they get training data?",
    "llm_answer": "Data released by the APDA shared task organizers.\n",
    "context": [
      {
        "id": "8c714d0f-b982-4837-aefc-3b59003e3a67",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 15,
          "document_id": "1912.13109",
          "start_idx": 7410,
          "end_idx": 8070
        },
        "page_content": "Train-test split: The labelled dataset that was available for this task was very limited in number of examples and thus as noted above few data augmentation techniques were applied to boost the learning of the network. Before applying augmentation, a train-test split of 78%-22% was done from the original, cleansed data set. Thus, 700 tweets/messages were held out for testing. All model evaluation were done in on the test set that got generated by this process. The results presented in this report are based on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data.",
        "type": "Document"
      },
      {
        "id": "36adf7ba-6ece-4ad5-84b3-0a38b137a4fc",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 16,
          "document_id": "1912.13109",
          "start_idx": 8070,
          "end_idx": 8831
        },
        "page_content": "The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we employed various operational techniques for optimizing on the learning process. These processes and its implementation details will be discussed later but they were learning rate decay, early stopping, model checkpointing and reducing learning rate on plateau.",
        "type": "Document"
      },
      {
        "id": "1d262be4-4dbc-447d-b93f-275422d87e7e",
        "metadata": {
          "vector_store_key": "1909.04181-0",
          "chunk_id": 2,
          "document_id": "1909.04181",
          "start_idx": 1249,
          "end_idx": 1941
        },
        "page_content": "For the purpose of our experiments, we use data released by the APDA shared task organizers. The dataset is divided into train and test by organizers. The training set is distributed with labels for the three tasks of age, dialect, and gender. Following the standard shared tasks set up, the test set is distributed without labels and participants were expected to submit their predictions on test. The shared task predictions are expected by organizers at the level of users. The distribution has 100 tweets for each user, and so each tweet is distributed with a corresponding user id. As such, in total, the distributed training data has 2,250 users, contributing a total of 225,000 tweets.",
        "type": "Document"
      },
      {
        "id": "32fba02b-0c12-4a85-9c43-bb936afdc125",
        "metadata": {
          "vector_store_key": "1909.07734-4",
          "chunk_id": 4,
          "document_id": "1909.07734",
          "start_idx": 2305,
          "end_idx": 2987
        },
        "page_content": "Approaches used by the teams included deep neural networks and SVM classifiers. In the following sections we expand on the challenge and the data. We then briefly describe the various approaches used by the teams, and conclude with a summary and some notes. Detailed descriptions of the various submissions are available in the teams' technical reports. The two datasets used for the challenge are Friends and EmotionPush, part of the EmotionLines corpus BIBREF4. The datasets contain English-language dialogues of varying lengths. For the competition, we provided 1,000 labeled dialogues from each dataset for training, and 240 unlabeled dialogues from each dataset for evaluation.",
        "type": "Document"
      },
      {
        "id": "219e1d29-ce32-45bf-8ebf-2888f80e650e",
        "metadata": {
          "vector_store_key": "1909.04181-0",
          "chunk_id": 3,
          "document_id": "1909.04181",
          "start_idx": 1941,
          "end_idx": 2640
        },
        "page_content": "As such, in total, the distributed training data has 2,250 users, contributing a total of 225,000 tweets. The official task test set contains 720,00 tweets posted by 720 users. For our experiments, we split the training data released by organizers into 90% TRAIN set (202,500 tweets from 2,025 users) and 10% DEV set (22,500 tweets from 225 users). The age task labels come from the tagset {under-25, between-25 and 34, above-35}. For dialects, the data are labeled with 15 classes, from the set {Algeria, Egypt, Iraq, Kuwait, Lebanon-Syria, Lybia, Morocco, Oman, Palestine-Jordan, Qatar, Saudi Arabia, Sudan, Tunisia, UAE, Yemen}. The gender task involves binary labels from the set {male, female}.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What extraction model did they use?",
    "llm_answer": "A multi-task learning framework with a sequence-to-sequence model for answer synthesis.\n",
    "context": [
      {
        "id": "eaa45ca2-7c16-447a-9500-b45093cc3efd",
        "metadata": {
          "vector_store_key": "1904.07342-3",
          "chunk_id": 9,
          "document_id": "1904.07342",
          "start_idx": 4887,
          "end_idx": 5594
        },
        "page_content": "We experimented with different feature extraction methods and classification models. Feature extractions examined include Tokenizer, Unigram, Bigram, 5-char-gram, and td-idf methods. Models include both neural nets (e.g. RNNs, CNNs) and standard machine learning tools (e.g. Naive Bayes with Laplace Smoothing, k-clustering, SVM with linear kernel). Model accuracies are reported in Table FIGREF3 . The RNN pre-trained using GloVe word embeddings BIBREF6 achieved the higest test accuracy. We pass tokenized features into the embedding layer, followed by an LSTM BIBREF7 with dropout and ReLU activation, and a dense layer with sigmoid activation. We apply an Adam optimizer on the binary crossentropy loss.",
        "type": "Document"
      },
      {
        "id": "abde311f-db3e-44fd-96aa-882a0a2aeeb0",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 17,
          "document_id": "1912.13109",
          "start_idx": 8552,
          "end_idx": 9307
        },
        "page_content": "These processes and its implementation details will be discussed later but they were learning rate decay, early stopping, model checkpointing and reducing learning rate on plateau. For the loss function we chose categorical cross entropy loss in finding the most optimal weights/parameters of the model. Formally this loss function for the model is defined as below: The double sum is over the number of observations and the categories respectively. While the model probability is the probability that the observation i belongs to category c. Among the model architectures we experimented with and without data augmentation were: Fully Connected dense networks: Model hyperparameters were inspired from the previous work done by Vo et al and Mathur et al.",
        "type": "Document"
      },
      {
        "id": "79853218-a120-4757-ad23-8cfca74bd91b",
        "metadata": {
          "vector_store_key": "1908.07218-2",
          "chunk_id": 6,
          "document_id": "1908.07218",
          "start_idx": 3602,
          "end_idx": 4390
        },
        "page_content": "Illustrated in Figure 2 , the extraction algorithm before refinement consists of five steps. Definition concept expansion. As many words are synonymous with some concepts, many word senses are defined trivially by one concept. For example, the definition of UTF8bkai\u99ff\u99ac (excellent steed) is simply {UTF8bkai\u99ff\u99ac $\\vert $ ExcellentSteed}. The triviality is resolved by expanding such definitions by one layer, e.g., replacing {UTF8bkai\u99ff\u99ac $\\vert $ ExcellentSteed} with {UTF8bkai\u99ac $\\vert $ horse:qualification={HighQuality $\\vert $ UTF8bkai\u512a\u8cea}}, i.e., the definition of UTF8bkai\u99ff\u99ac $\\vert $ ExcellentSteed. Definition string parsing. We parse each definition into a directed graph. Each node in the graph is either a word, a concept, or a function relation, e.g., or() at the bottom of Table 1 .",
        "type": "Document"
      },
      {
        "id": "8d107288-be1d-43f0-9370-33a944cdd332",
        "metadata": {
          "vector_store_key": "1706.04815-0",
          "chunk_id": 46,
          "document_id": "1706.04815",
          "start_idx": 25771,
          "end_idx": 26602
        },
        "page_content": "The extraction model aims to match the question and passage and predict most important sub-spans in the passage related to the question as evidence. Then, the synthesis model synthesizes the question information and the evidence snippet to generate the final answer. We propose a multi-task learning framework to improve the evidence extraction model by passage ranking to extract the evidence snippet, and use the sequence-to-sequence model for answer synthesis. We conduct experiments on the MS-MARCO dataset. Results demonstrate that our approach outperforms pure answer extraction model and other existing methods. We only annotate one evidence snippet in the sequence-to-sequence model for synthesizing answer, which cannot solve the question whose answer comes from multiple evidences, such as the second example in Table 1 .",
        "type": "Document"
      },
      {
        "id": "a8a499e4-7752-44c8-be98-646a509eaa4d",
        "metadata": {
          "vector_store_key": "1705.03261-3",
          "chunk_id": 4,
          "document_id": "1705.03261",
          "start_idx": 2353,
          "end_idx": 3012
        },
        "page_content": "The details about how we train our model and extract the DDI type from text are described in the remaining sections. In DDI extraction task, NLP methods or machine learning approaches are proposed by most of the work. Chowdhury BIBREF14 and Thomas et al. BIBREF11 proposed methods that use linguistic phenomenons and two-stage SVM to classify DDIs. FBK-irst BIBREF10 is a follow-on work which applies kernel method to the existing model and outperforms it. Neural network based approaches have been proposed by several works. Liu et al. BIBREF9 employ CNN for DDI extraction for the first time which outperforms the traditional machine learning based methods.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which datasets did they experiment on?",
    "llm_answer": "novel-9, novel-50, IMDB62, SCI, Stat2015, Stat2016, CS2016\n",
    "context": [
      {
        "id": "6d614157-50dc-4aaa-af03-adc7f0282a12",
        "metadata": {
          "vector_store_key": "1709.02271-3",
          "chunk_id": 20,
          "document_id": "1709.02271",
          "start_idx": 11344,
          "end_idx": 11969
        },
        "page_content": "(Section SECREF26 ). The statistics for the three datasets used in the experiments are summarized in Table TABREF16 . novel-9. This dataset was compiled by F&H14: a collection of 19 novels by 9 nineteenth century British and American authors in the Project Gutenberg. To compare to F&H14, we apply the same resampling method (F&H14, Section 4.2) to correct the imbalance in authors by oversampling the texts of less-represented authors. novel-50. This dataset extends novel-9, compiling the works of 50 randomly selected authors of the same period. For each author, we randomly select 5 novels for a total 250 novels. IMDB62.",
        "type": "Document"
      },
      {
        "id": "93a841ff-3b54-4752-90d9-794ea465a2e0",
        "metadata": {
          "vector_store_key": "1709.01256-4",
          "chunk_id": 38,
          "document_id": "1709.01256",
          "start_idx": 20976,
          "end_idx": 21630
        },
        "page_content": "We generated six data sets using different random seeds, and each data set contained six corpora (Corpus 0 - 5). Table TABREF48 summarizes the first data set. In each data set, we name the initial corpus Corpus 0, and define INLINEFORM0 as the timestamp when we started this simulation process. We set INLINEFORM1 , INLINEFORM2 . Corpus INLINEFORM3 corresponds to documents generated before timestamp INLINEFORM4 . We extracted document revisions from Corpus INLINEFORM5 and compared the revisions generated in (Corpus INLINEFORM6 - Corpus INLINEFORM7 ) with the ground truths in Table TABREF48 . Hence, we ran four experiments on this data set in total.",
        "type": "Document"
      },
      {
        "id": "c5f16988-f9bb-43d0-a24e-a2d52ae2d42a",
        "metadata": {
          "vector_store_key": "1802.05574-4",
          "chunk_id": 11,
          "document_id": "1802.05574",
          "start_idx": 6530,
          "end_idx": 7227
        },
        "page_content": "This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available and the corpus provides both an XML and a simple text version of each article. We randomly selected 2 sentences with more than two words from each paper using the simple text version of the paper.",
        "type": "Document"
      },
      {
        "id": "711988d5-b6d5-455f-a849-15926d3a08fb",
        "metadata": {
          "vector_store_key": "1910.12618-2",
          "chunk_id": 12,
          "document_id": "1910.12618",
          "start_idx": 6819,
          "end_idx": 7418
        },
        "page_content": "The rest of this paper is organized as follows. The following section introduces the two data sets used to conduct our study. Section 3 presents the different machine learning approaches used and how they were tuned. Section 4 highlights the main results of our study, while section 5 concludes this paper and gives insight on future possible work. In order to prove the consistency of our work, experiments have been conducted on two data sets, one for France and the other for the UK. In this section details about the text and time series data are given, as well as the major preprocessing steps.",
        "type": "Document"
      },
      {
        "id": "8e741c56-0352-4660-89e2-d995b2fada35",
        "metadata": {
          "vector_store_key": "1807.09671-7",
          "chunk_id": 34,
          "document_id": "1807.09671",
          "start_idx": 19653,
          "end_idx": 20306
        },
        "page_content": "While the 2nd and 3rd data sets are from the same course, Statistics for Industrial Engineers, they were taught in 2015 and 2016 respectively (henceforth Stat2015 and Stat2016), at the Bo\u01e7azi\u00e7i University in Turkey. The course was taught in English while the official language is Turkish. The last one is from a fundamental undergraduate Computer Science course (data structures) at a local U.S. university taught in 2016 (henceforth CS2016). Another reason we choose the student responses is that we have advanced annotation allowing us to perform an intrinsic evaluation to test whether the low-rank approximation does capture similar concepts or not.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do slot binary classifiers improve performance?",
    "llm_answer": "Slot binary classifiers predict the probability of a slot being requested by the user, improving performance by providing information for the candidate tracker and the candidate classifier.\n",
    "context": [
      {
        "id": "3b725205-feb6-4329-a20a-ebb951a34271",
        "metadata": {
          "vector_store_key": "1908.02402-7",
          "chunk_id": 28,
          "document_id": "1908.02402",
          "start_idx": 14333,
          "end_idx": 15057
        },
        "page_content": "\\end{split}$$   (Eq. 12)  The loss function for all requestable slot binary classifiers is:  $$\\small \\begin{split}\n\\mathcal {L}^R =& - \\frac{1}{|\\lbrace k^R\\rbrace |} \\sum _{k^R} \\\\\n&z^{k^R} \\log (y^{k^R}) + (1-z^{k^R}) \\log (1-y^{k^R}). \\end{split}$$   (Eq. 13)  The generated informable slot values $I_t = \\lbrace Y^{k^I}\\rbrace $ are used as constraints of the KB query. The KB is composed of one or more relational tables and each entity is a record in one table. The query is performed to select a subset of the entities that satisfy those constraints. For instance, if the informable slots are {price=cheap, area=north}, all the restaurants that have attributes of those fields equal to those values will be returned.",
        "type": "Document"
      },
      {
        "id": "8dff25f5-34e7-4433-9854-60504b7aa62a",
        "metadata": {
          "vector_store_key": "2002.01359-1",
          "chunk_id": 34,
          "document_id": "2002.01359",
          "start_idx": 19704,
          "end_idx": 20376
        },
        "page_content": "A slot tagger identifies slot values, which are used to update the candidate tracker. The candidate classifier uses the utterances and slot/intent descriptions to predict the final dialogue state. They also use an additional loss to penalize incorrect prediction on which slots appear in the current turn. We consider the following metrics for automatic evaluation of different submissions. Joint goal accuracy has been used as the primary metric to rank the submissions. Active Intent Accuracy: The fraction of user turns for which the active intent has been correctly predicted. Requested Slot F1: The macro-averaged F1 score for requested slots over all eligible turns.",
        "type": "Document"
      },
      {
        "id": "4e9ddeb2-1430-447e-bfd0-64a94078232d",
        "metadata": {
          "vector_store_key": "1908.02402-2",
          "chunk_id": 27,
          "document_id": "1908.02402",
          "start_idx": 14333,
          "end_idx": 15157
        },
        "page_content": "The concatenation of $c^{k^R}$ and $e^{k^R}$ , the embedding vector of one requestable slot $k^R$ , is passed as input and $h^{E}_{l}$ as the initial state to the GRU. Finally, a sigmoid non-linearity is applied to the product of a weight vector $W_{y}^{R}$ and the output of the GRU $h^{k^R}$ to obtain $y^{k^R}$ , which is the probability of the slot being requested by the user. $$\\small \\begin{split}\n&c^{k^R} = \\text{Attn}(h^{E}_{l}, \\lbrace h_{i}^E\\rbrace ),\\\\\n&h^{k^R} = \\text{GRU}_R\\Big ( (c^{k^R}\\circ e^{k^R}), h^{E}_{l} \\Big ),\\\\\n&y^{k^R} = \\sigma (W_{y}^{R} \\cdot h^{k^R}). \\end{split}$$   (Eq. 12)  The loss function for all requestable slot binary classifiers is:  $$\\small \\begin{split}\n\\mathcal {L}^R =& - \\frac{1}{|\\lbrace k^R\\rbrace |} \\sum _{k^R} \\\\\n&z^{k^R} \\log (y^{k^R}) + (1-z^{k^R}) \\log (1-y^{k^R}).",
        "type": "Document"
      },
      {
        "id": "1498f778-b332-469c-aa0c-cf572bf19ada",
        "metadata": {
          "vector_store_key": "1909.00754-7",
          "chunk_id": 47,
          "document_id": "1909.00754",
          "start_idx": 24267,
          "end_idx": 24991
        },
        "page_content": "StateNet BIBREF3 achieves state-of-the-art performance with the property that its parameters are independent of the number of slot values in the candidate set, and it also supports online training or inference with dynamically changing slots and values. Given a slot that needs tracking, it only needs to perform inference once to make the prediction for a turn, but this also means that its inference time complexity is proportional to the number of slots. TRADE BIBREF4 achieves state-of-the-art performance on the MultiWoZ dataset by applying the copy mechanism for the value sequence generation. Since TRADE takes $n$ combinations of the domains and slots as the input, the inference time complexity of TRADE is $O(n)$ .",
        "type": "Document"
      },
      {
        "id": "02768a0c-4926-4db6-8835-bf7458e07498",
        "metadata": {
          "vector_store_key": "2002.01359-1",
          "chunk_id": 28,
          "document_id": "2002.01359",
          "start_idx": 16301,
          "end_idx": 16965
        },
        "page_content": "Slot values are inferred using a TRADE-like encoder-decoder setup with a 3-way slot status gate, using the utterance encoding and schema element embeddings as context. Team 5 BIBREF22: They predict values for categorical slots using a softmax over all candidate values. Non-categorical slot values are predicted by first predicting the status of each slot and then using a BiLSTM-CRF layer for BIO tagging BIBREF23. They also utilize a slot adoption tracker to predict if the values proposed by the system are accepted by the user. Team 9 BIBREF24: This team submitted the winning entry, beating the second-placed team by around 9% in terms of joint goal accuracy.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What baselines have been used in this work?",
    "llm_answer": "Neural baselines (with different models), ANRES_Sing_R, ANRES_Sing_Z, ANRES_Seq_R, ANRES_Seq_Z, i-vector system, and x-vector system.\n",
    "context": [
      {
        "id": "fa88df39-033d-4e0f-891b-162e0f9b0424",
        "metadata": {
          "vector_store_key": "1709.08299-4",
          "chunk_id": 19,
          "document_id": "1709.08299",
          "start_idx": 11042,
          "end_idx": 11836
        },
        "page_content": "The neural baselines are built by the following steps. We first use the shared training data to build a general systems, and choose the best performing model (in terms of cloze validation set) as baseline. Use User Query validation data for further tuning the systems with 10-fold cross-validations. Increase dropout rate BIBREF9 to 0.5 for preventing over-fitting issue. All baseline systems are chosen according to the performance of the validation set. The participant system results are given in Table 2 and 3 . As we can see that two neural baselines are competitive among participant systems and AoA Reader successfully outperform AS Reader and all participant systems in single model condition, which proves that it is a strong baseline system even without further fine-tuning procedure.",
        "type": "Document"
      },
      {
        "id": "39cb1ec4-6c9f-4dbd-b781-29fe9d4ad19f",
        "metadata": {
          "vector_store_key": "1912.01046-3",
          "chunk_id": 23,
          "document_id": "1912.01046",
          "start_idx": 13260,
          "end_idx": 13980
        },
        "page_content": "Following this idea, Baseline1 represents a span of sentences by concatenating the vectors of the starting and ending sentences. The left diagram in Fig. FIGREF15 illustrates the Baseline1 model. Model. The model takes two inputs, a transcript, $\\lbrace s_1, s_2, ... s_n\\rbrace $ where $s_i$ are individual sentences and a question, $q$. The output is the span scores, $y$, the scores over all possible spans. GLoVe BIBREF14 is used for the word representations in the transcript and the questions. We use two bi-LSTMs BIBREF15 to encode the transcript. where n is the number of sentences . The output of Passage-level Encoding, $p$, is a sequence of vector, $p_i$, which represents the latent meaning of each sentence.",
        "type": "Document"
      },
      {
        "id": "64beee2a-a1a9-454b-917e-917a34317fa7",
        "metadata": {
          "vector_store_key": "2004.01878-3",
          "chunk_id": 27,
          "document_id": "2004.01878",
          "start_idx": 14569,
          "end_idx": 15289
        },
        "page_content": "In summary, the following four baselines are designed: ANRES_Sing_R: randomly initializing the states for each single trading day. ANRES_Sing_Z: initializing the states as zeros for each single trading day. ANRES_Seq_R: randomly initializing the first states for each trading sequence only. ANRES_Seq_Z: initializing the first states as zeros for each trading sequence only. Development set results on predicting S&P 500 index are shown in Table TABREF13. We can see that modeling recurrent value sequences performs better than treating each trading day separately, which shows that modeling trading sequences can capture the correlations between trading days and the non-linear compositional effects of multiple events.",
        "type": "Document"
      },
      {
        "id": "02bef31f-a2b5-4b89-8063-62dc0b0f91eb",
        "metadata": {
          "vector_store_key": "1911.01799-4",
          "chunk_id": 19,
          "document_id": "1911.01799",
          "start_idx": 11188,
          "end_idx": 11946
        },
        "page_content": "Two state-of-the-art baseline systems were built following the Kaldi SITW recipe BIBREF22: an i-vector system BIBREF3 and an x-vector system BIBREF10. For the i-vector system, the acoustic feature involved 24-dimensional MFCCs plus the log energy, augmented by the first- and second-order derivatives. We also applied the cepstral mean normalization (CMN) and the energy-based voice active detection (VAD). The universal background model (UBM) consisted of $2,048$ Gaussian components, and the dimensionality of the i-vector space was 400. LDA was applied to reduce the dimensionality of the i-vectors to 150. The PLDA model was used for scoring BIBREF4. For the x-vector system, the feature-learning component was a 5-layer time-delay neural network (TDNN).",
        "type": "Document"
      },
      {
        "id": "763bf540-a941-47b1-bdeb-25182eab991e",
        "metadata": {
          "vector_store_key": "2001.11381-4",
          "chunk_id": 55,
          "document_id": "2001.11381",
          "start_idx": 31798,
          "end_idx": 32460
        },
        "page_content": "Tampoco consideramos la utilizaci\u00f3n de un baseline de tipo aleatorio, porque los resultados carecer\u00edan de la homosintaxis y ser\u00eda sumamente f\u00e1cil obtener mejores resultados. Dicho lo anterior, el Modelo 1 podr\u00eda ser considerado como nuestro propio baseline. A continuaci\u00f3n presentamos un protocolo de evaluaci\u00f3n manual de los resultados obtenidos. El experimento consisti\u00f3 en la generaci\u00f3n de 15 frases por cada uno de los tres modelos propuestos. Para cada modelo, se consideraron tres queries: $Q=$ {AMOR, GUERRA, SOL}, generando 5 frases con cada uno. Las 15 frases fueron mezcladas entre s\u00ed y reagrupadas por queries, antes de presentarlas a los evaluadores.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what bottlenecks were identified?",
    "llm_answer": "The need for universalized benchmarks and metrics, reproducibility of published methodologies, bias in available data, and biological/chemical interpretability/explainability of solutions.  Also, the use of different datasets and evaluation metrics makes model performance assessment challenging, and comprehensive benchmarking platforms are lacking.\n",
    "context": [
      {
        "id": "80911f24-68f0-45cb-a65a-c54370ab3292",
        "metadata": {
          "vector_store_key": "1707.03764-3",
          "chunk_id": 11,
          "document_id": "1707.03764",
          "start_idx": 5958,
          "end_idx": 6709
        },
        "page_content": "This also shows that distinguishing words include both time-specific ones, like \u201cgilmore\u201d and \u201cimacelebrityau\u201d, and general words from everyday life, which are less likely to be subject to time-specific trends, like \u201cplayer\u201d, and \u201cchocolate\u201d. This section is meant to highlight all of the potential contributions to the systems which turned out to be detrimental to performance, when compared to the simpler system that we have described in Section SECREF2 . We divide our attempts according to the different ways we attempted to enhance performance: manipulating the data itself (adding more, and changing preprocessing), using a large variety of features, and changing strategies in modelling the problem by using different algorithms and paradigms.",
        "type": "Document"
      },
      {
        "id": "abde311f-db3e-44fd-96aa-882a0a2aeeb0",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 17,
          "document_id": "1912.13109",
          "start_idx": 8552,
          "end_idx": 9307
        },
        "page_content": "These processes and its implementation details will be discussed later but they were learning rate decay, early stopping, model checkpointing and reducing learning rate on plateau. For the loss function we chose categorical cross entropy loss in finding the most optimal weights/parameters of the model. Formally this loss function for the model is defined as below: The double sum is over the number of observations and the categories respectively. While the model probability is the probability that the observation i belongs to category c. Among the model architectures we experimented with and without data augmentation were: Fully Connected dense networks: Model hyperparameters were inspired from the previous work done by Vo et al and Mathur et al.",
        "type": "Document"
      },
      {
        "id": "36adf7ba-6ece-4ad5-84b3-0a38b137a4fc",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 16,
          "document_id": "1912.13109",
          "start_idx": 8070,
          "end_idx": 8831
        },
        "page_content": "The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we employed various operational techniques for optimizing on the learning process. These processes and its implementation details will be discussed later but they were learning rate decay, early stopping, model checkpointing and reducing learning rate on plateau.",
        "type": "Document"
      },
      {
        "id": "b8da75b2-a962-4b59-9a53-800e0c324ca8",
        "metadata": {
          "vector_store_key": "2002.06053-9",
          "chunk_id": 75,
          "document_id": "2002.06053",
          "start_idx": 45023,
          "end_idx": 45831
        },
        "page_content": "As this interdisciplinary field grows, novel opportunities come hand in hand with novel challenges. The major challenges that can be observed from investigating these studies can be summarized as follows: (i) the need for universalized benchmarks and metrics, (ii) reproducibility of the published methodologies, (iii) bias in available data, and (iv) biological and chemical interpretability/explainability of the solutions. There are several steps in the drug discovery pipeline, from affinity prediction to the prediction of other chemical properties such as toxicity, and solubility. The use of different datasets and different evaluation metrics makes the assessment of model performance challenging. Comprehensive benchmarking platforms that can assess the success of different tools are still lacking.",
        "type": "Document"
      },
      {
        "id": "b2afb44d-91c2-48bb-8d83-4adc8ef4dcd4",
        "metadata": {
          "vector_store_key": "1807.09671-2",
          "chunk_id": 66,
          "document_id": "1807.09671",
          "start_idx": 37518,
          "end_idx": 38304
        },
        "page_content": "In order to investigate what are key factors that impact the performance, we would like to perform additional experiments using synthesized data sets. A variety of attributes that might impact the performance are summarized in Table TABREF41 , categorized into two types. The input attributes are extracted from the input original documents and the summaries attributes are extracted from human summaries and the input documents as well. Here are some important attributes we expect to have a big impact on the performance. The attributes extracted from the corpora are shown in Table TABREF42 . Note, a bigram that appears more often in original documents has a better chance to be included in human summaries as indicated by INLINEFORM0 , INLINEFORM1 , INLINEFORM2 , and INLINEFORM3 .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much do they outperform BiLSTMs in Sentiment Analysis?",
    "llm_answer": "The provided text states that the proposed model significantly outperforms BiLSTMs in sentiment analysis, achieving new state-of-the-art performance on multiple datasets.  It also outperforms other complex models, including those from BIBREF4, BIBREF6, and even BIBREF8 on specific datasets.  However, a precise numerical difference isn't given.\n",
    "context": [
      {
        "id": "64dffb18-ce4c-4177-b952-5ccc367c7599",
        "metadata": {
          "vector_store_key": "1911.02711-1",
          "chunk_id": 5,
          "document_id": "1911.02711",
          "start_idx": 3057,
          "end_idx": 3919
        },
        "page_content": "We evaluate our proposed model on the SNAP (Stanford Network Analysis Project) Amazon review datasets BIBREF8, which contain not only reviews and ratings, but also golden summaries. In scenarios where there is no user-written summary for a review, we use pointer-generator network BIBREF9 to generate abstractive summaries. Empirical results show that our model significantly outperforms all strong baselines, including joint modeling, separate encoder and joint encoder methods. In addition, our model achieves new state-of-the-art performance, attaining 2.1% (with generated summary) and 4.8% (with golden summary) absolutely improvements compared to the previous best method on SNAP Amazon review benchmark. The majority of recent sentiment analysis models are based on either convolutional or recurrent neural networks to encode sequences BIBREF10, BIBREF11.",
        "type": "Document"
      },
      {
        "id": "391e4e01-9c84-4d5c-94f4-3c80ade9a144",
        "metadata": {
          "vector_store_key": "1811.09786-4",
          "chunk_id": 21,
          "document_id": "1811.09786",
          "start_idx": 11504,
          "end_idx": 12322
        },
        "page_content": "As such, for our experiments, when considering only the encoder and keeping all other components constant, 3L-BiLSTM has equal parameters to RCRN while RCRN and 3L-BiLSTM are approximately three times larger than BiLSTM. This section discusses the overall empirical evaluation of our proposed RCRN model. In order to verify the effectiveness of our proposed RCRN architecture, we conduct extensive experiments across several tasks in the NLP domain. Sentiment analysis is a text classification problem in which the goal is to determine the polarity of a given sentence/document. We conduct experiments on both sentence and document level. More concretely, we use 16 Amazon review datasets from BIBREF32 , the well-established Stanford Sentiment TreeBank (SST-5/SST-2) BIBREF33 and the IMDb Sentiment dataset BIBREF34 .",
        "type": "Document"
      },
      {
        "id": "42b8279f-22ec-4ed7-be95-7748ae23c008",
        "metadata": {
          "vector_store_key": "1710.01492-2",
          "chunk_id": 39,
          "document_id": "1710.01492",
          "start_idx": 21338,
          "end_idx": 22268
        },
        "page_content": "Fully unsupervised learning is not a popular method for addressing sentiment analysis tasks. Yet, some features used in sentiment analysis have been learned in an unsupervised way, e.g., Brown clusters to generalize over words BIBREF58 . Similarly, word embeddings are typically trained from raw tweets that have no annotation for sentiment (even though there is also work on sentiment-specific word embeddings BIBREF57 , which uses distant supervision). Despite the wide variety of knowledge sources explored so far in the literature, sentiment polarity lexicons remain the most commonly used resource for the task of sentiment analysis. Until recently, such sentiment polarity lexicons were manually crafted and were thus of small to moderate size, e.g., LIWC BIBREF59 has 2,300 words, the General Inquirer BIBREF60 contains 4,206 words, Bing Liu's lexicon BIBREF22 includes 6,786 words, and MPQA BIBREF14 has about 8,000 words.",
        "type": "Document"
      },
      {
        "id": "900e031e-8a0b-42b2-a35d-c4819af01f3b",
        "metadata": {
          "vector_store_key": "1909.02764-2",
          "chunk_id": 35,
          "document_id": "1909.02764",
          "start_idx": 19412,
          "end_idx": 20121
        },
        "page_content": "Regarding the audio signal, we observe a macro $\\text{F}_1$ score of 29 % (P=42 %, R=22 %). There is a bias towards negative emotions, which results in a small number of detected joy predictions (R=4 %). Insecurity and annoyance are frequently confused. The experimental setting for the evaluation of emotion recognition from text is as follows: We evaluate the BiLSTM model in three different experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora.",
        "type": "Document"
      },
      {
        "id": "5c5cbbb8-c925-489c-9424-0e43be40f938",
        "metadata": {
          "vector_store_key": "1809.10644-1",
          "chunk_id": 15,
          "document_id": "1809.10644",
          "start_idx": 8483,
          "end_idx": 9158
        },
        "page_content": "Our results indicate better performance than several more complex approaches, including BIBREF4 's best model (which used word and part-of-speech ngrams, sentiment, readability, text, and Twitter specific features), BIBREF6 (which used two fold classification and a hybrid of word and character CNNs, using approximately twice the parameters we use excluding the word embeddings) and even recent work by BIBREF8 , (whose best model relies on GRUs, metadata including popularity, network reciprocity, and subscribed lists). On the SR dataset, we outperform BIBREF8 's text based model by 3 F1 points, while just falling short of the Text + Metadata Interleaved Training model.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what state of the accuracy did they obtain?",
    "llm_answer": "Word recognition accuracy reached about 80% in the speaker-closed setting and 60% on average in the speaker-open setting.\n",
    "context": [
      {
        "id": "799aee48-83fb-428f-ba63-cf2bf1a9ec07",
        "metadata": {
          "vector_store_key": "1804.03396-5",
          "chunk_id": 58,
          "document_id": "1804.03396",
          "start_idx": 30874,
          "end_idx": 31510
        },
        "page_content": "We randomly sample 25 articles respectively from the 6 datasets (in total of 1002 ground truth triples with 2691 labeled answer locations) and let two human annotators label the Triple Accuracy for each ground truth triple and the Contextual Consistency for each answer location. The two human annotators reached an agreement on 131 of 132 randomly selected Triple Accuracy cases and on 229 of 234 randomly selected Contextual Consistency cases. The human evaluation results are shown in Table 8 . We can find that the Triple Accuracy and the Triple Consistency is acceptable while the Contextual Consistency still needs to be improved.",
        "type": "Document"
      },
      {
        "id": "050432d2-b325-4282-ae97-1513abbfedc7",
        "metadata": {
          "vector_store_key": "2003.06279-7",
          "chunk_id": 36,
          "document_id": "2003.06279",
          "start_idx": 21531,
          "end_idx": 22144
        },
        "page_content": "While Figures FIGREF14 \u2013 FIGREF16 show the relative behavior in the accuracy, it still interesting to observe the absolute accuracy rate obtained with the classifiers. In Table TABREF17, we show the best accuracy rate (i.e. $\\max \\Gamma _+ = \\max _p \\Gamma _+(p)$) for GloVe. We also show the average difference in performance ($\\langle \\Gamma _+ - \\Gamma _0 \\rangle $) and the total number of cases in which an improvement in performance was observed ($N_+$). $N_+$ ranges in the interval $0 \\le N_+ \\le 20$. Table TABREF17 summarizes the results obtained for $w = \\lbrace 1.0, 5.0, 10.0\\rbrace $ thousand words.",
        "type": "Document"
      },
      {
        "id": "d6ad8869-204c-4044-8af7-4c7f461884e2",
        "metadata": {
          "vector_store_key": "2002.06675-4",
          "chunk_id": 32,
          "document_id": "2002.06675",
          "start_idx": 17063,
          "end_idx": 17700
        },
        "page_content": "The model was trained by these scripts until 30th epoch. From 31$^{\\rm {st}}$ and 40th epoch, the model was fine-turned by the Ainu script. Phone units are used for JNAS and WSJ throughout the experiments. Table 4 shows the phone error rates (PERs) and word error rates (WERs) for the speaker-closed and speaker-open settings. The `average' is weighted by the numbers of tokens in the ground truth transcriptions for speaker-wise evaluation sets. The word recognition accuracy reached about 80% in the speaker-closed setting. In the speaker-open setting it was 60% on average and varied greatly from speaker to speaker (from 50% to 70%).",
        "type": "Document"
      },
      {
        "id": "8dce665d-9665-4bef-8d03-212f5e982d92",
        "metadata": {
          "vector_store_key": "1605.08675-3",
          "chunk_id": 82,
          "document_id": "1605.08675",
          "start_idx": 45319,
          "end_idx": 46017
        },
        "page_content": "Of course, the final evaluation needs to be checked manually. As mentioned in previous section, the results consist of two groups: experiments, showing an influence of some aspects of algorithm on performance, and a final assessment. Both use the Polish Wikipedia as a knowledge base, whereas the questions asked belong to development and evaluation sets, respectively. In this section, recall measures percentage of questions, to which RAFAEL gave any answer, whereas precision denotes percentage of question answered correctly. When analysing results of different entity recognition techniques, we need to remember that they strongly rely on output of the question analysis, which is not perfect.",
        "type": "Document"
      },
      {
        "id": "1aa58b63-7d45-4c30-8497-358be6b0b602",
        "metadata": {
          "vector_store_key": "1804.03396-5",
          "chunk_id": 56,
          "document_id": "1804.03396",
          "start_idx": 29719,
          "end_idx": 30662
        },
        "page_content": "Note that our framework takes the whole document as the input while the baseline systems take the individual sentence as the input, which means the experiment setting is much more difficult for our framework. Finally, we perform a human evaluation on our QA4IE benchmark to verify the reliability of former experiments. The evaluation metrics are as follows: Triple Accuracy is to check whether each ground truth triple is accurate (one cannot find conflicts between the ground truth triple and the corresponding article) because the ground truth triples from WikiData and DBpedia may be incorrect or incomplete. Contextual Consistency is to check whether the context of each answer location is consistent with the corresponding ground truth triple (one can infer from the context to obtain the ground truth triple) because we keep all matched answer locations as ground truths but some of them may be irrelevant with the corresponding triple.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what models did they compare to?",
    "llm_answer": "MatchZoo models, Arc1, CDSSM, CNN-jieba/PKU/CTB, BIBREF15, ESIM, Decomposable Attention Model, KIM, BERT, and models pretrained on Wikipedia and BookCorpus.\n",
    "context": [
      {
        "id": "52a5c7b9-197c-4c52-9a22-93a7f3f1bb7a",
        "metadata": {
          "vector_store_key": "1910.12618-2",
          "chunk_id": 45,
          "document_id": "1910.12618",
          "start_idx": 24114,
          "end_idx": 24750
        },
        "page_content": "In order to tone it down and to increase the consistency of our results, the different models are run $B=10$ times. The results presented hereafter correspond to the average and standard-deviation on those runs. The RF model denoted as \"sel\" is the one with the reduced number of features, whereas the other RF uses the full vocabulary. We also considered an aggregated forecaster (abridged Agg), consisting of the average of the two best individual ones in terms of RMSE. All the neural network methods have a reduced vocabulary size $V^*$. The results for the French and UK data are respectively given by tables TABREF26 and TABREF27.",
        "type": "Document"
      },
      {
        "id": "755e0282-5de9-4232-b21f-6859ca5db76a",
        "metadata": {
          "vector_store_key": "1802.05322-4",
          "chunk_id": 19,
          "document_id": "1802.05322",
          "start_idx": 10325,
          "end_idx": 11021
        },
        "page_content": "The second model was a KNN which was chosen because of it is simple and does not require the pre-training that the MLP needs. The implementation of this model comes from scikit-learn's neighbors module and is called KNeighborsClassifier. The only parameter that was changed after some trial and error was the k-parameter which was set to 3. Both models were fitted using the train set and then predictions were done for the test set. Table TABREF38 shows the INLINEFORM0 , INLINEFORM1 and INLINEFORM2 for the models. The KNN model had a higher accuracy of INLINEFORM3 compared to MPL's accuracy of INLINEFORM4 and the KNN model had a higher recall but slightly lower precision than the MLP model.",
        "type": "Document"
      },
      {
        "id": "d32ae63d-b598-4253-add8-2b40556bbc8d",
        "metadata": {
          "vector_store_key": "1902.09087-2",
          "chunk_id": 33,
          "document_id": "1902.09087",
          "start_idx": 18209,
          "end_idx": 18947
        },
        "page_content": "We also compare with the state-of-the-art models in DBQA BIBREF15 , BIBREF16 . Here, we mainly describe the main results on the DBQA dataset, while we find very similar trends on the KBRE dataset. Table TABREF26 summarizes the main results on the two datasets. We can see that the simple MatchZoo models perform the worst. Although Arc1 and CDSSM are also constructed in the siamese architecture with CNN layers, they do not employ multiple kernel sizes and residual connections, and fail to capture the relatedness in a multi-granularity fashion. BIBREF15 is similar to our word level models (CNN-jieba/PKU/CTB), but outperforms our models by around 3%, since it benefits from an extra interaction layer with fine tuned hyper-parameters.",
        "type": "Document"
      },
      {
        "id": "07b84dd0-f246-4c86-b216-a68029541a1a",
        "metadata": {
          "vector_store_key": "2002.05058-5",
          "chunk_id": 53,
          "document_id": "2002.05058",
          "start_idx": 30143,
          "end_idx": 30900
        },
        "page_content": "We evaluate these model variants on the Dailydialog dataset. Results are presented in Table 5. We can see that comparison-based evaluation is very effective as our model correlates much better than adversarial evaluator. The tie option is also very important as it can prevent the comparative evaluator from making uncertain decision and model the inductive bias that samples generated by the same model are generally of similar quality, which may help our model generalize better. As for different sources of training examples, we find that human preference annotation is the most important, which is not surprising. In addition, we find that the proposed weak supervision also helps, but is of smaller relative importance compared with strong supervision.",
        "type": "Document"
      },
      {
        "id": "dfcb060a-a7cd-4162-8c13-2229a797791e",
        "metadata": {
          "vector_store_key": "1906.06448-1",
          "chunk_id": 30,
          "document_id": "1906.06448",
          "start_idx": 17231,
          "end_idx": 18020
        },
        "page_content": "(Bilateral Multi-Perspective Matching Model; BIBREF31 , BIBREF31 ), ESIM (Enhanced Sequential Inference Model; BIBREF32 , BIBREF32 ), Decomposable Attention Model BIBREF33 , KIM (Knowledge-based Inference Model; BIBREF34 , BIBREF34 ), and BERT (Bidirectional Encoder Representations from Transformers model; BIBREF35 , BIBREF35 ). Regarding BERT, we checked the performance of a model pretrained on Wikipedia and BookCorpus for language modeling and trained with SNLI and MultiNLI. For other models, we checked the performance trained with SNLI. In agreement with our dataset, we regarded the prediction label contradiction as non-entailment. Table 6 shows that the accuracies of all models were better on upward inferences, in accordance with the reported results of the GLUE leaderboard.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "which benchmark tasks did they experiment on?",
    "llm_answer": "Molecule generation, prediction of binding affinity and toxicity, question answering (QA), named entity recognition (NER).\n",
    "context": [
      {
        "id": "080dd9ac-808a-4979-9787-23390d67fde8",
        "metadata": {
          "vector_store_key": "2002.06053-9",
          "chunk_id": 76,
          "document_id": "2002.06053",
          "start_idx": 45630,
          "end_idx": 46366
        },
        "page_content": "Comprehensive benchmarking platforms that can assess the success of different tools are still lacking. A benchmarking environment rigorously brings together the suitable data sets and evaluation methodologies in order to provide a fair comparison between the available tools. Such environments are available for molecule generation task from MOSES BIBREF123 and GuacaMol BIBREF124. MoleculeNet is also a similar attempt to build a benchmarking platform for tasks such as prediction of binding affinity and toxicity BIBREF82. Despite the focus on sharing datasets and source codes on popular software development platforms such as GitHub (github.com) or Zenodo (zenodo.org), it is still a challenge to use data or code from other groups.",
        "type": "Document"
      },
      {
        "id": "0fea8c56-43db-40e9-8857-3a13216e128c",
        "metadata": {
          "vector_store_key": "1611.00514-0",
          "chunk_id": 31,
          "document_id": "1611.00514",
          "start_idx": 17817,
          "end_idx": 18459
        },
        "page_content": "This section reports on the CPU execution time (single threaded), and the amount of memory used to process a single trial, which includes the time for creating models from the enrolment data and the time needed for processing the test segments. The analysis was performed on an Intel(R) Xeon(R) CPU E5-2670 2.60GHz. The results are shown in Table TABREF27 . We used the time command in Unix to report these results. The user time is the actual CPU time used in executing the process (single thread). The real time is the wall clock time (the elapsed time including time slices used by other processes and the time the process spends blocked).",
        "type": "Document"
      },
      {
        "id": "36adf7ba-6ece-4ad5-84b3-0a38b137a4fc",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 16,
          "document_id": "1912.13109",
          "start_idx": 8070,
          "end_idx": 8831
        },
        "page_content": "The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we employed various operational techniques for optimizing on the learning process. These processes and its implementation details will be discussed later but they were learning rate decay, early stopping, model checkpointing and reducing learning rate on plateau.",
        "type": "Document"
      },
      {
        "id": "31d590cc-f828-4a88-a077-fb680eb013eb",
        "metadata": {
          "vector_store_key": "1804.03396-8",
          "chunk_id": 38,
          "document_id": "1804.03396",
          "start_idx": 20650,
          "end_idx": 21263
        },
        "page_content": "Our model is trained on a GTX 1080 Ti GPU and it takes about 14 hours on small sized QA4IE datasets. We implement our model with TensorFlow BIBREF47 and optimize the computational expensive LSTM layers with LSTMBlockFusedCell. We first perform experiments in QA settings to evaluate our QA model on both SQuAD dataset and QA4IE benchmark. Since our goal is to solve IE, not QA, the motivation of this part of experiments is to evaluate the performance of our model and make a comparison between QA4IE benchmark and existing datasets. Two metrics are introduced in the SQuAD dataset: Exact Match (EM) and F1-score.",
        "type": "Document"
      },
      {
        "id": "222384a3-c2da-4009-a468-d7fd1883099e",
        "metadata": {
          "vector_store_key": "1701.02877-8",
          "chunk_id": 9,
          "document_id": "1701.02877",
          "start_idx": 4697,
          "end_idx": 5380
        },
        "page_content": "Since the goal of this study is to compare NER performance on corpora from diverse domains and genres, seven benchmark NER corpora are included, spanning newswire, broadcast conversation, Web content, and social media (see Table 1 for details). These datasets were chosen such that they have been annotated with the same or very similar entity classes, in particular, names of people, locations, and organisations. Thus corpora including only domain-specific entities (e.g. biomedical corpora) were excluded. The choice of corpora was also motivated by their chronological age; we wanted to ensure a good temporal spread, in order to study possible effects of entity drift over time.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the proficiency score calculated?",
    "llm_answer": "The proficiency score is computed by summing the scores related to 6 indicators (from 3 to 6 in some campaigns), where each indicator can be 0, 1, or 2 (bad, medium, good).\n",
    "context": [
      {
        "id": "248b8a3b-9437-4919-b356-126f91001db5",
        "metadata": {
          "vector_store_key": "2001.08051-3",
          "chunk_id": 1,
          "document_id": "2001.08051",
          "start_idx": 703,
          "end_idx": 1399
        },
        "page_content": "This level is expressed according to the well-known Common European Framework of Reference for Languages (Council of Europe, 2001) scale. The CEFR defines 6 levels of proficiency: A1 (beginner), A2, B1, B2, C1 and C2. The levels considered in the evaluation campaigns where the data have been collected are: A1, A2 and B1. The indicators measure the linguistic competence of test takers both in relation to the content (e.g. grammatical correctness, lexical richness, semantic coherence, etc.) and to the speaking capabilities (e.g. pronunciation, fluency, etc.). Refer to Section SECREF2 for a description of the adopted indicators. The learners are Italian students, between 9 and 16 years old.",
        "type": "Document"
      },
      {
        "id": "fcf8be2f-41ef-4628-a599-a4f691aa1c11",
        "metadata": {
          "vector_store_key": "2001.08051-3",
          "chunk_id": 15,
          "document_id": "2001.08051",
          "start_idx": 8175,
          "end_idx": 8857
        },
        "page_content": "However, a detailed linguistic evaluation cannot be performed without allowing the students to express themselves in both written sentences and spoken utterances, which typically require the intervention of human experts to be scored. Tables and report some statistics extracted from both the written and spoken data collected so far in all the campaigns. Each written or spoken item received a total score by human experts, computed by summing up the scores related to 6 indicators in 2017/2018 (from 3 to 6 in the 2016 campaign, according to the proficiency levels and the type of test). Each indicator can assume a value 0, 1, 2, corresponding to bad, medium, good, respectively.",
        "type": "Document"
      },
      {
        "id": "ad485702-a004-4bba-b419-c962e99f8c37",
        "metadata": {
          "vector_store_key": "2001.05467-0",
          "chunk_id": 19,
          "document_id": "2001.05467",
          "start_idx": 10861,
          "end_idx": 11504
        },
        "page_content": "During training, this score is obtained by evaluating the diversity of the ground-truth target sequence (see Figure FIGREF8); during test time, we instead feed the model a diversity label scaled by a score of our choice (i.e., when we want the model to generate a more diverse response, we scale the label's embedding by a higher score, while to generate a duller response, we scale the embedding by a lower one). We also explore a model (see Figure FIGREF11) which regularizes on the discrete token level, because merely monitoring output probability distribution may ignore certain bad styles such as repetition (e.g. \"I don't don't know.\").",
        "type": "Document"
      },
      {
        "id": "6450df79-d9a0-4b5a-ac79-4ab8d2bd72f9",
        "metadata": {
          "vector_store_key": "2001.08051-3",
          "chunk_id": 16,
          "document_id": "2001.08051",
          "start_idx": 8531,
          "end_idx": 9232
        },
        "page_content": "Each indicator can assume a value 0, 1, 2, corresponding to bad, medium, good, respectively. The list of the indicators used by the experts to score written sentences and spoken utterances in the evaluations, grouped by similarity, is reported in Table . Since every utterance was scored by only one expert, it was not possible to evaluate any kind of agreement among experts. For future evaluations, more experts are expected to provide independent scoring on same data sets, so this kind of evaluation will be possible. The speaking part of the proficiency tests in 2017/2018 consists of 47 question prompts provided in written form: 24 in English and 23 in German, divided according to CEFR levels.",
        "type": "Document"
      },
      {
        "id": "792c1284-33b0-44fe-a5f3-3f73995f473e",
        "metadata": {
          "vector_store_key": "1903.07398-0",
          "chunk_id": 21,
          "document_id": "1903.07398",
          "start_idx": 11749,
          "end_idx": 12406
        },
        "page_content": "To compute this score, many samples of a TTS system is given to human evaluators and rated on a score from 1 (Bad) to 5 (Excellent). the MOS is then computed as the arithmetic mean of these score: DISPLAYFORM0  Where INLINEFORM0 are individual ratings for a given sample by N subjects. For TTS models from google and Baidu, they utilized Amazon mechanical Turk to collect and generate MOS score from larger number of workers. However due to our limited resources, we chose to collect MOS score from friends and families (total 6 people). For training time comparison, we choose the training time as when attention alignment start to become linear and clear.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What proficiency indicators are used to the score the utterances?",
    "llm_answer": "6 indicators in 2017/2018 (from 3 to 6 in the 2016 campaign).\n",
    "context": [
      {
        "id": "fcf8be2f-41ef-4628-a599-a4f691aa1c11",
        "metadata": {
          "vector_store_key": "2001.08051-3",
          "chunk_id": 15,
          "document_id": "2001.08051",
          "start_idx": 8175,
          "end_idx": 8857
        },
        "page_content": "However, a detailed linguistic evaluation cannot be performed without allowing the students to express themselves in both written sentences and spoken utterances, which typically require the intervention of human experts to be scored. Tables and report some statistics extracted from both the written and spoken data collected so far in all the campaigns. Each written or spoken item received a total score by human experts, computed by summing up the scores related to 6 indicators in 2017/2018 (from 3 to 6 in the 2016 campaign, according to the proficiency levels and the type of test). Each indicator can assume a value 0, 1, 2, corresponding to bad, medium, good, respectively.",
        "type": "Document"
      },
      {
        "id": "6450df79-d9a0-4b5a-ac79-4ab8d2bd72f9",
        "metadata": {
          "vector_store_key": "2001.08051-3",
          "chunk_id": 16,
          "document_id": "2001.08051",
          "start_idx": 8531,
          "end_idx": 9232
        },
        "page_content": "Each indicator can assume a value 0, 1, 2, corresponding to bad, medium, good, respectively. The list of the indicators used by the experts to score written sentences and spoken utterances in the evaluations, grouped by similarity, is reported in Table . Since every utterance was scored by only one expert, it was not possible to evaluate any kind of agreement among experts. For future evaluations, more experts are expected to provide independent scoring on same data sets, so this kind of evaluation will be possible. The speaking part of the proficiency tests in 2017/2018 consists of 47 question prompts provided in written form: 24 in English and 23 in German, divided according to CEFR levels.",
        "type": "Document"
      },
      {
        "id": "8d3d6108-f60f-414c-ac43-27d7006321d9",
        "metadata": {
          "vector_store_key": "2001.08051-3",
          "chunk_id": 43,
          "document_id": "2001.08051",
          "start_idx": 22887,
          "end_idx": 23799
        },
        "page_content": "As for proficiency indicators, one first step that could be taken in order to increase accuracy in the evaluation phase both for human and automatic scoring would be to divide the second indicator (pronunciation and fluency) into two different indicators, since fluent students might not necessarily have good pronunciation skills and vice versa, drawing for example on the IELTS Speaking band descriptors. Also, next campaigns might consider an additional indicator specifically addressed to score prosody (in particular intonation and rhythm), especially for A2 and B1 level test-takers. Considering the scope of the evaluation campaign, it is important to be aware of the limitations of the associated data sets: proficiency levels limited to A1, B1 and B2 (CEFR); custom indicators conceived for expert evaluation (not particularly suitable for automated evaluation); limited amount of responses per speaker.",
        "type": "Document"
      },
      {
        "id": "7dc5e5cc-4e4c-43d1-bd37-471bea0a2a2f",
        "metadata": {
          "vector_store_key": "2001.08051-0",
          "chunk_id": 34,
          "document_id": "2001.08051",
          "start_idx": 17832,
          "end_idx": 18591
        },
        "page_content": "Additional annotations could be included to address topics related to other spurious segments, as for example: understandable words pronounced in other languages or by other students, detection of phonological interference, detection of spontaneous speech phenomena, detection of overlapped speech, etc. In order to measure specific proficiency indicators, e.g. related to pronunciation and fluency, suprasegmental annotations can be also inserted in the corpus. The corpus is a valuable resource for training and evaluating a scoring classifier based on different approaches. Preliminary results BIBREF0 show that the usage of suitable linguistic features mainly based on statistical language models allow to predict the scores assigned by the human experts.",
        "type": "Document"
      },
      {
        "id": "38f8d874-3025-4de4-b0df-dadb6cbd7003",
        "metadata": {
          "vector_store_key": "2001.08051-0",
          "chunk_id": 42,
          "document_id": "2001.08051",
          "start_idx": 21888,
          "end_idx": 22887
        },
        "page_content": "Some of the question prompts show some lacks that can be filled in without major difficulty: on the one hand, in the spoken part, questions do not require test-takers to shift tense and some are too suggestive and close-ended; on the other hand, in the written part, some question prompts are presented both in source and target language, thus causing or encouraging code-mixing and negative transfer phenomena. The elicitation techniques in a broader sense will be object of revision (see BIBREF34 and specifically on children speech BIBREF35) in order to maximise the quality of the corpus. As for proficiency indicators, one first step that could be taken in order to increase accuracy in the evaluation phase both for human and automatic scoring would be to divide the second indicator (pronunciation and fluency) into two different indicators, since fluent students might not necessarily have good pronunciation skills and vice versa, drawing for example on the IELTS Speaking band descriptors.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What accuracy is achieved by the speech recognition system?",
    "llm_answer": "91.2% accuracy on utterances containing words, sentences, and conversations.  Word recognition accuracy reached about 80% in the speaker-closed setting and 60% on average in the speaker-open setting.\n",
    "context": [
      {
        "id": "343fa539-680b-4b43-9631-124e054c3bf9",
        "metadata": {
          "vector_store_key": "1909.13362-0",
          "chunk_id": 39,
          "document_id": "1909.13362",
          "start_idx": 23022,
          "end_idx": 23711
        },
        "page_content": "Our reported accuracy of $94.9 \\pm 0.3\\%$ on the Manipuri dataset is furthest from state of the art. We suspect this to be due to having limited amounts of training data; the $97.5\\%$ accurate system from BIBREF35 supplemented their data-driven approach with rules of syllabification. Examples from the outputs of the Base model can give us insight into what the model does well and what types of words it struggles with. The total number of sounds across languages is vast, but not infinite, as Ladefoged and Maddieson's The Sounds of the the World's Languages demonstrates BIBREF42. Different languages choose different inventories from the total producible by the human vocal apparatus.",
        "type": "Document"
      },
      {
        "id": "d511bb7a-2f5d-4402-b99f-9a52ecba3fd8",
        "metadata": {
          "vector_store_key": "1601.02543-3",
          "chunk_id": 1,
          "document_id": "1601.02543",
          "start_idx": 628,
          "end_idx": 1339
        },
        "page_content": "Before commercial deployment of a speech solution it is imperative to have a quantitative measure of the performance of the speech solution which is primarily based on the speech recognition accuracy of the speech engine used. Generally, the recognition performance of any speech recognition based solution is quantitatively evaluated by putting it to actual use by the people who are the intended users and then analyzing the logs to identify successful and unsuccessful transactions. This evaluation is then used to identifying any further improvement in the speech recognition based solution to better the overall transaction completion rates. This process of evaluation is both time consuming and expensive.",
        "type": "Document"
      },
      {
        "id": "d6ad8869-204c-4044-8af7-4c7f461884e2",
        "metadata": {
          "vector_store_key": "2002.06675-4",
          "chunk_id": 32,
          "document_id": "2002.06675",
          "start_idx": 17063,
          "end_idx": 17700
        },
        "page_content": "The model was trained by these scripts until 30th epoch. From 31$^{\\rm {st}}$ and 40th epoch, the model was fine-turned by the Ainu script. Phone units are used for JNAS and WSJ throughout the experiments. Table 4 shows the phone error rates (PERs) and word error rates (WERs) for the speaker-closed and speaker-open settings. The `average' is weighted by the numbers of tokens in the ground truth transcriptions for speaker-wise evaluation sets. The word recognition accuracy reached about 80% in the speaker-closed setting. In the speaker-open setting it was 60% on average and varied greatly from speaker to speaker (from 50% to 70%).",
        "type": "Document"
      },
      {
        "id": "dbdf269c-fe97-4416-b6be-91113bd07f3a",
        "metadata": {
          "vector_store_key": "1907.00758-4",
          "chunk_id": 31,
          "document_id": "1907.00758",
          "start_idx": 17373,
          "end_idx": 18401
        },
        "page_content": "(Section SECREF4 ). Analysis: We analyse the performance of our model across different conditions. Table shows the model accuracy broken down by utterance type. The model achieves 91.2% accuracy on utterances containing words, sentences, and conversations, all of which exhibit natural variation in speech. The model is less successful with Articulatory utterances, which contain isolated phones occurring once or repeated (e.g., \u201csh sh sh\"). Such utterances contain subtle tongue movement, making it more challenging to correlate the visual signal with the audio. And indeed, the model finds the correct offset for only 55.9% of Articulatory utterances. A further analysis shows that 84.4% (N INLINEFORM0 90) of stop consonants (e.g., \u201ct\u201d), which are relied upon by therapists as the most salient audiovisual synchronisation cues BIBREF3 , are correctly synchronised by our model, compared to 48.6% (N INLINEFORM1 140) of vowels, which contain less distinct movement and are also more challenging for therapists to synchronise.",
        "type": "Document"
      },
      {
        "id": "c618b1a4-deaa-463f-a29c-695ddd668998",
        "metadata": {
          "vector_store_key": "1611.00514-4",
          "chunk_id": 33,
          "document_id": "1611.00514",
          "start_idx": 18686,
          "end_idx": 19586
        },
        "page_content": "For enrolment, we have reported the time required to extract a model from a segment with a duration of 140 seconds and speech duration of 60 seconds. The time and memory required for front-end processing are negligible compared to the i-vector extraction stage, since they only include matrix operations. The time required for our SAD is also reported which increases linearly with the duration of segment. We have presented the Intelligent Voice speaker recognition system used for the NIST 2016 speaker recognition evaluation. Our system is based on a score fusion of MFCC- and PLP-based i-vector/PLDA systems. We have described the main components of the system including, acoustic feature extraction, speech activity detection, i-vector extraction as front-end processing, and language normalization, short-duration compensation, channel compensation and domain adaptation as back-end processing.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the speech recognition system evaluated?",
    "llm_answer": "Generally, by putting the solution to actual use by intended users, analyzing logs to identify successful/unsuccessful transactions, and using this data to identify areas for improvement in recognition.  A Wizard of Oz (WoZ) approach is often employed to maintain user experience during testing.\n",
    "context": [
      {
        "id": "d511bb7a-2f5d-4402-b99f-9a52ecba3fd8",
        "metadata": {
          "vector_store_key": "1601.02543-3",
          "chunk_id": 1,
          "document_id": "1601.02543",
          "start_idx": 628,
          "end_idx": 1339
        },
        "page_content": "Before commercial deployment of a speech solution it is imperative to have a quantitative measure of the performance of the speech solution which is primarily based on the speech recognition accuracy of the speech engine used. Generally, the recognition performance of any speech recognition based solution is quantitatively evaluated by putting it to actual use by the people who are the intended users and then analyzing the logs to identify successful and unsuccessful transactions. This evaluation is then used to identifying any further improvement in the speech recognition based solution to better the overall transaction completion rates. This process of evaluation is both time consuming and expensive.",
        "type": "Document"
      },
      {
        "id": "8c8e6e04-afe6-4bf0-a219-af0351553058",
        "metadata": {
          "vector_store_key": "1601.02543-3",
          "chunk_id": 2,
          "document_id": "1601.02543",
          "start_idx": 1049,
          "end_idx": 1838
        },
        "page_content": "This process of evaluation is both time consuming and expensive. For evaluation one needs to identify a set of users and also identify the set of actual usage situations and perform the test. It is also important that the set of users are able to use the system with ease meaning that even in the test conditions the performance of the system, should be good, while this can not usually be guaranteed this aspect of keeping the user experience good makes it necessary to employ a wizard of Oz (WoZ) approach. Typically this requires a human agent in the loop during actual speech transaction where the human agent corrects any mis-recognition by actually listening to the conversation between the human user and the machine without the user knowing that there is a human agent in the loop.",
        "type": "Document"
      },
      {
        "id": "e299fa06-a91e-4da9-94d2-11e2d1d6a7f8",
        "metadata": {
          "vector_store_key": "1601.02543-3",
          "chunk_id": 3,
          "document_id": "1601.02543",
          "start_idx": 1838,
          "end_idx": 2520
        },
        "page_content": "Typically this requires a human agent in the loop during actual speech transaction where the human agent corrects any mis-recognition by actually listening to the conversation between the human user and the machine without the user knowing that there is a human agent in the loop. The use of WoZ is another expense in the testing a speech solution. All this makes testing a speech solution an expensive and time consuming procedure. In this paper, we describe a method to evaluate the performance of a speech solution without actual people using the system as is usually done. We then show how this method was adopted to evaluate a speech recognition based solution as a case study.",
        "type": "Document"
      },
      {
        "id": "7c585b62-b44b-418a-8e3b-f980dbe05e36",
        "metadata": {
          "vector_store_key": "1601.02543-0",
          "chunk_id": 4,
          "document_id": "1601.02543",
          "start_idx": 2135,
          "end_idx": 2687
        },
        "page_content": "We then show how this method was adopted to evaluate a speech recognition based solution as a case study. This is the main contribution of the paper. The rest of the paper is organized as follows. The method for evaluation without testing is described in Section SECREF2 . In Section SECREF3 we present a case study and conclude in Section SECREF4 . Fig. FIGREF1 shows the schematic of a typical menu based speech solution having 3 nodes. At each node there are a set of words that the user is expected to speak and the system is supposed to recognize.",
        "type": "Document"
      },
      {
        "id": "c618b1a4-deaa-463f-a29c-695ddd668998",
        "metadata": {
          "vector_store_key": "1611.00514-4",
          "chunk_id": 33,
          "document_id": "1611.00514",
          "start_idx": 18686,
          "end_idx": 19586
        },
        "page_content": "For enrolment, we have reported the time required to extract a model from a segment with a duration of 140 seconds and speech duration of 60 seconds. The time and memory required for front-end processing are negligible compared to the i-vector extraction stage, since they only include matrix operations. The time required for our SAD is also reported which increases linearly with the duration of segment. We have presented the Intelligent Voice speaker recognition system used for the NIST 2016 speaker recognition evaluation. Our system is based on a score fusion of MFCC- and PLP-based i-vector/PLDA systems. We have described the main components of the system including, acoustic feature extraction, speech activity detection, i-vector extraction as front-end processing, and language normalization, short-duration compensation, channel compensation and domain adaptation as back-end processing.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many of the utterances are transcribed?",
    "llm_answer": "2000\n",
    "context": [
      {
        "id": "7253fa13-81ac-447b-b753-ad94eb9f24d1",
        "metadata": {
          "vector_store_key": "2001.08051-4",
          "chunk_id": 26,
          "document_id": "2001.08051",
          "start_idx": 13349,
          "end_idx": 13985
        },
        "page_content": "As a consequence, we decided to apply the following transcription rules: only the main speaker has to be transcribed; presence of other voices (schoolmates, teacher) should be reported only with the label \u201c@voices\u201d, presence of whispered speech was found to be significant, so it should be explicitly marked with the label \u201c()\u201d, badly pronounced words have to be marked by a \u201c#\u201d sign, without trying to phonetically transcribe the pronounced sounds; \u201c#*\u201d marks incomprehensible speech; speech in a different language from the target language has to be reported by means of an explicit marker \u201cI am 10 years old @it(io ho gi\u00e0 risposto)\u201d.",
        "type": "Document"
      },
      {
        "id": "9091882e-511c-41be-976e-0ef3f1dc964b",
        "metadata": {
          "vector_store_key": "2001.08051-4",
          "chunk_id": 27,
          "document_id": "2001.08051",
          "start_idx": 13985,
          "end_idx": 14647
        },
        "page_content": "\u201c#*\u201d marks incomprehensible speech; speech in a different language from the target language has to be reported by means of an explicit marker \u201cI am 10 years old @it(io ho gi\u00e0 risposto)\u201d. Next, we concatenated utterances to be transcribed into blocks of about 5 minutes each. We noticed that knowing the question and hearing several answers could be of great help for transcribing some poorly pronounced words or phrases. Therefore, each block contains only answers to the same question, explicitly reported at the beginning of the block. We engaged about 30 students from two Italian linguistic high schools (namely \u201cC\u201d and \u201cS\u201d) to perform manual transcriptions.",
        "type": "Document"
      },
      {
        "id": "ba362bd8-eafb-4b4c-a357-90a62dde72ef",
        "metadata": {
          "vector_store_key": "2001.08051-4",
          "chunk_id": 25,
          "document_id": "2001.08051",
          "start_idx": 13029,
          "end_idx": 13918
        },
        "page_content": "In order to create both an adaptation and an evaluation set for ASR, we manually transcribed part of the 2017 data sets. We defined an initial set of guidelines for the annotation, which were used by 5 researchers to manually transcribe about 20 minutes of audio data. This experience led to a discussion, from which a second set of guidelines originated, aiming at reaching a reasonable trade-off between transcription accuracy and speed. As a consequence, we decided to apply the following transcription rules: only the main speaker has to be transcribed; presence of other voices (schoolmates, teacher) should be reported only with the label \u201c@voices\u201d, presence of whispered speech was found to be significant, so it should be explicitly marked with the label \u201c()\u201d, badly pronounced words have to be marked by a \u201c#\u201d sign, without trying to phonetically transcribe the pronounced sounds;",
        "type": "Document"
      },
      {
        "id": "5469a00c-ae11-43a7-91ed-0fcc7da2f811",
        "metadata": {
          "vector_store_key": "1911.13087-3",
          "chunk_id": 6,
          "document_id": "1911.13087",
          "start_idx": 3651,
          "end_idx": 4295
        },
        "page_content": "The reason that we have taken only 200 sentences is to have a smaller dictionary and also to increase the repetition of each word in the narrated speech. We transformed the corpus sentences, which are in Persian-Arabic script, into the format which complies with the suggested phones for the related Sorani letters (see Section SECREF6). Two thousand narration files were created. We used Audacity to record the narrations. We used a normal laptop in a quiet room and minimized the background noise. However, we could not manage to avoid the noise of the fan of the laptop. A single speaker narrated the 2000 sentences, which took several days.",
        "type": "Document"
      },
      {
        "id": "a22cce10-cf7d-4a75-a5a2-99d8a3f066ee",
        "metadata": {
          "vector_store_key": "1705.03151-5",
          "chunk_id": 46,
          "document_id": "1705.03151",
          "start_idx": 25347,
          "end_idx": 26030
        },
        "page_content": "These are: Mandarin, Cantonese, Indonesian, Japanese, Russian, Korean and Vietnamese. The data volume for each language is approximately 10 hours of speech signals recorded by 24 speakers (12 males and 12 females), with each speaker recording approximately 300 utterances in reading style by mobile phones, with a sampling rate of 16kHz and a sample size of 16 bits. Each dataset was split into a training set consisting of 18 speakers, and a test set consisting of 6 speakers. For Mandarin, Cantonese, Vietnamese and Indonesian, the recording was conducted in a quiet environment. For Russian, Korean and Japanese, there are 2 recording conditions for each speaker, quiet and noisy.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many utterances are in the corpus?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "29d9c930-4b33-4a4e-9839-78217bd5766f",
        "metadata": {
          "vector_store_key": "2002.06675-1",
          "chunk_id": 10,
          "document_id": "2002.06675",
          "start_idx": 5333,
          "end_idx": 5896
        },
        "page_content": "Therefore, our first step was to build a speech corpus for ASR based on the data sets provided by the Ainu Museum and the Nibutani Ainu Culture Museum. In this section we explain the content of the data sets and how we modified it for our ASR corpus. The corpus we have prepared for ASR in this study is composed of text and speech. Table 1 shows the number of episodes and the total speech duration for each speaker. Among the total of eight speakers, the data of the speakers KM and UT is from the Ainu Museum, and the rest is from Nibutani Ainu Culture Museum.",
        "type": "Document"
      },
      {
        "id": "5469a00c-ae11-43a7-91ed-0fcc7da2f811",
        "metadata": {
          "vector_store_key": "1911.13087-3",
          "chunk_id": 6,
          "document_id": "1911.13087",
          "start_idx": 3651,
          "end_idx": 4295
        },
        "page_content": "The reason that we have taken only 200 sentences is to have a smaller dictionary and also to increase the repetition of each word in the narrated speech. We transformed the corpus sentences, which are in Persian-Arabic script, into the format which complies with the suggested phones for the related Sorani letters (see Section SECREF6). Two thousand narration files were created. We used Audacity to record the narrations. We used a normal laptop in a quiet room and minimized the background noise. However, we could not manage to avoid the noise of the fan of the laptop. A single speaker narrated the 2000 sentences, which took several days.",
        "type": "Document"
      },
      {
        "id": "b05ce8a1-ee5d-4c77-8a9a-91429ee17e6a",
        "metadata": {
          "vector_store_key": "2001.11381-3",
          "chunk_id": 26,
          "document_id": "2001.11381",
          "start_idx": 14593,
          "end_idx": 15417
        },
        "page_content": "Este corpus fue utilizado principalmente en los dos modelos generativos: modelo basado en cadenas de Markov (Secci\u00f3n SECREF13) y modelo basado en la generaci\u00f3n de Texto enlatado (Canned Text, Secci\u00f3n SECREF15). En este trabajo proponemos tres modelos h\u00edbridos (combinaciones de modelos generativos cl\u00e1sicos y aproximaciones sem\u00e1nticas) para la producci\u00f3n de frases literarias. Hemos adaptado dos modelos generativos, usando an\u00e1lisis sint\u00e1ctico superficial (shallow parsing) y un modelo de aprendizaje profundo (Deep Learning) BIBREF25, combinados con tres modelos desarrollados de aproximaci\u00f3n sem\u00e1ntica. En una primera fase, los modelos generativos recuperan la informaci\u00f3n gramatical de cada palabra del corpus 8KF (ver Secci\u00f3n SECREF3), en forma de etiquetas POS (Part of Speech), a trav\u00e9s de un an\u00e1lisis morfosint\u00e1ctico.",
        "type": "Document"
      },
      {
        "id": "44deae74-772b-4dc8-b79a-490728a54c03",
        "metadata": {
          "vector_store_key": "1907.00758-4",
          "chunk_id": 17,
          "document_id": "1907.00758",
          "start_idx": 9960,
          "end_idx": 10622
        },
        "page_content": "The utterances have been categorised by the type of task the child was given, and are labelled as: Words (A), Non-words (B), Sentence (C), Articulatory (D), Non-speech (E), or Conversations (F). See BIBREF15 for details. Each utterance consists of 3 files: audio, ultrasound, and parameter. The audio file is a RIFF wave file, sampled at 22.05 KHz, containing the speech of the child and therapist. The ultrasound file consists of a sequence of ultrasound frames capturing the midsagittal view of the child's tongue. A single ultrasound frame is recorded as a 2D matrix where each column represents the ultrasound reflection intensities along a single scan line.",
        "type": "Document"
      },
      {
        "id": "6af1b78b-163b-4db4-aeba-d56f2e7841ef",
        "metadata": {
          "vector_store_key": "2001.08051-1",
          "chunk_id": 3,
          "document_id": "2001.08051",
          "start_idx": 1923,
          "end_idx": 2785
        },
        "page_content": "The tasks that can be addressed by using the corpus are very challenging and pose many problems, which have only partially been solved by the interested scientific community. From the ASR perspective, major difficulties are represented by: a) recognition of both child and non-native speech, i.e. Italian pupils speaking both English and German, b) presence of a large number of spontaneous speech phenomena (hesitations, false starts, fragments of words, etc.), c) presence of multiple languages (English, Italian and German words are frequently uttered in response to a single question), d) presence of a significant level of background noise due to the fact that the microphone remains open for a fixed time interval (e.g. 20 seconds - depending on the question), and e) presence of non-collaborative speakers (students often joke, laugh, speak softly, etc.).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does their model outperform both the state-of-the-art systems?",
    "llm_answer": "The model significantly outperforms state-of-the-art systems in overall performance, with improvements in specific categories (Person and Miscellaneous).  Specific percentage improvements are given for various comparisons but not a single overall figure.\n",
    "context": [
      {
        "id": "acfbe9ea-af02-42f7-84b2-1c4b30db2f68",
        "metadata": {
          "vector_store_key": "1909.11833-0",
          "chunk_id": 21,
          "document_id": "1909.11833",
          "start_idx": 12031,
          "end_idx": 12806
        },
        "page_content": "We compare our model SIM with a number of baseline systems: delexicalization model BIBREF8, BIBREF1, the neural belief tracker model (NBT) BIBREF0, global-locally self-attentive model GLAD BIBREF5, large-scale belief tracking model LSBT BIBREF7 and scalable multi-domain dialogue state tracking model SMDST BIBREF6. Table TABREF17 shows that, on WoZ dataset, SIM achieves a new state-of-the-art joint goal accuracy of 89.5%, a significant improvement of 1.4% over GLAD, and turn request accuracy of 97.3%, 0.2% above GLAD. On DSTC2 dataset, where noisy ASR results are used as user utterance during test, SIM obtains comparable results with GLAD. Furthermore, the better representation in SIM makes it significantly outperform previous slot-independent models LSBT and SMDST.",
        "type": "Document"
      },
      {
        "id": "59630f23-3c9c-47bb-b331-4027472a3a6b",
        "metadata": {
          "vector_store_key": "1603.04553-1",
          "chunk_id": 20,
          "document_id": "1603.04553",
          "start_idx": 10753,
          "end_idx": 11450
        },
        "page_content": "Specifically, our model achieves improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford system, the winner of the CoNLL 2011 shared task, on the CoNLL 2012 development and test sets, respectively. The improvements on CoNLL F1 score over the Multigraph model are 1.41% and 1.77% on the development and test sets, respectively. Comparing with the MIR model, we obtain significant improvements of 2.62% and 3.02% on CoNLL F1 score. To make a thorough empirical comparison with previous studies, Table 3 (below the dashed line) also shows the results of some state-of-the-art supervised coreference resolution systems \u2014 IMS: the second best system in the CoNLL 2012 shared task BIBREF28 ;",
        "type": "Document"
      },
      {
        "id": "2e7cfe41-b41c-475c-83f6-e7d39903180d",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 40,
          "document_id": "1909.13375",
          "start_idx": 21052,
          "end_idx": 21792
        },
        "page_content": "While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1. All that, while keeping in mind we compare a base model to a large model (for reference, note the 8 point improvement between MTMSNbase and MTMSNlarge in both EM and F1). Our best model, large-squad, exhibits a huge improvement of 29.7 in EM and 15.1 in F1 compared to MTMSNlarge. When comparing single-span performance, our best model exhibits slightly better results, but it should be noted that it retains the single-span heads from NABERT+, while in MTMSN they have one head to predict both single-span and multi-span answers.",
        "type": "Document"
      },
      {
        "id": "11bbd522-28ef-4580-9084-84720bfb9758",
        "metadata": {
          "vector_store_key": "2001.06888-0",
          "chunk_id": 42,
          "document_id": "2001.06888",
          "start_idx": 22556,
          "end_idx": 22770
        },
        "page_content": "Evaluation results of our proposed model compared to other state of the art methods show its superiority to these methods in overall while in two categories (Person and Miscellaneous) our model outperformed others.",
        "type": "Document"
      },
      {
        "id": "0c4bd793-a8e9-4861-8601-03f439261b66",
        "metadata": {
          "vector_store_key": "1910.00458-6",
          "chunk_id": 21,
          "document_id": "1910.00458",
          "start_idx": 12016,
          "end_idx": 12817
        },
        "page_content": "In the table, we first report the accuracy of the SOTA models in the leaderboard. We then report the performance of our re-implementation of fine-tuned models as another set of strong baselines, among which the RoBERTa-Large model has already surpassed the previous SOTA. For these baselines, the top-level classifier is a two-layer FCNN for BERT-based models and a one-layer FCNN for the RoBERTa-Large model. Lastly, we report model performances that use all our proposed method, MMM (MAN classifier + speaker normalization + two stage learning strategies). As direct comparisons, we also list the accuracy increment between MMM and the baseline with the same sentence encoder marked by the parentheses, from which we can see that the performance augmentation is over 9% for BERT-Base and BERT-Large.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the state-of-the art?",
    "llm_answer": "In recent years, deep learning methods, particularly deep convolutional networks, dominate sentiment analysis.  For coreference resolution, systems like IMS, Latent-Tree, and Berkeley are state-of-the-art supervised models.  In dialogue systems, RNNs pre-trained using GloVe word embeddings are achieving high accuracy.  Various other models like MLP, SVM, and Random Forest are used for classification.\n",
    "context": [
      {
        "id": "a6dff15d-6323-4ff2-9d1f-f503bd1a9ef5",
        "metadata": {
          "vector_store_key": "1710.01492-5",
          "chunk_id": 35,
          "document_id": "1710.01492",
          "start_idx": 19077,
          "end_idx": 19782
        },
        "page_content": "Traditionally, the above features were fed into classifiers such as Maximum Entropy (MaxEnt) and Support Vector Machines (SVM) with various kernels. However, observation over the SemEval Twitter sentiment task in recent years shows growing interest in, and by now clear dominance of methods based on deep learning. In particular, the best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks BIBREF53 , BIBREF54 . Conversely, kernel machines seem to be less frequently used than in the past, and the use of learning methods other than the ones mentioned above is at this point scarce. All these models are examples of supervised learning as they need labeled training data.",
        "type": "Document"
      },
      {
        "id": "904a361a-2d3c-41fa-8807-2303b37ca27b",
        "metadata": {
          "vector_store_key": "1603.04553-3",
          "chunk_id": 21,
          "document_id": "1603.04553",
          "start_idx": 11450,
          "end_idx": 12153
        },
        "page_content": "To make a thorough empirical comparison with previous studies, Table 3 (below the dashed line) also shows the results of some state-of-the-art supervised coreference resolution systems \u2014 IMS: the second best system in the CoNLL 2012 shared task BIBREF28 ; Latent-Tree: the latent tree model BIBREF29 obtaining the best results in the shared task; Berkeley: the Berkeley system with the final feature set BIBREF12 ; LaSO: the structured perceptron system with non-local features BIBREF30 ; Latent-Strc: the latent structure system BIBREF31 ; Model-Stack: the entity-centric system with model stacking BIBREF32 ; and Non-Linear: the non-linear mention-ranking model with feature representations BIBREF33 .",
        "type": "Document"
      },
      {
        "id": "96043e06-1efa-49ff-8520-149bdab45f31",
        "metadata": {
          "vector_store_key": "1709.10217-2",
          "chunk_id": 1,
          "document_id": "1709.10217",
          "start_idx": 728,
          "end_idx": 1424
        },
        "page_content": "The development of DM technology is from rule-based approach and supervised learning based approach to reinforcement learning based approach BIBREF15 . The NLG technology is through pattern-based approach, sentence planning approach and end-to-end deep learning approach BIBREF16 , BIBREF17 , BIBREF18 . In application, there are massive products that are based on the technology of human-computer dialogue, such as Apple Siri, Amazon Echo, Microsoft Cortana, Facebook Messenger and Google Allo etc. Although the blooming of human-computer dialogue technology in both academia and industry, how to evaluate a dialogue system, especially an open domain chit-chat system, is still an open question.",
        "type": "Document"
      },
      {
        "id": "eaa45ca2-7c16-447a-9500-b45093cc3efd",
        "metadata": {
          "vector_store_key": "1904.07342-3",
          "chunk_id": 9,
          "document_id": "1904.07342",
          "start_idx": 4887,
          "end_idx": 5594
        },
        "page_content": "We experimented with different feature extraction methods and classification models. Feature extractions examined include Tokenizer, Unigram, Bigram, 5-char-gram, and td-idf methods. Models include both neural nets (e.g. RNNs, CNNs) and standard machine learning tools (e.g. Naive Bayes with Laplace Smoothing, k-clustering, SVM with linear kernel). Model accuracies are reported in Table FIGREF3 . The RNN pre-trained using GloVe word embeddings BIBREF6 achieved the higest test accuracy. We pass tokenized features into the embedding layer, followed by an LSTM BIBREF7 with dropout and ReLU activation, and a dense layer with sigmoid activation. We apply an Adam optimizer on the binary crossentropy loss.",
        "type": "Document"
      },
      {
        "id": "3e561a91-d33e-4edf-bb45-28350d16064c",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 26,
          "document_id": "2001.00137",
          "start_idx": 15470,
          "end_idx": 16181
        },
        "page_content": "We focus on the three following services, where the first two are commercial services and last one is open source with two separate backends: Google Dialogflow (formerly Api.ai) , SAP Conversational AI (formerly Recast.ai) and Rasa (spacy and tensorflow backend) . Shridhar et al. BIBREF12 proposed a word embedding method that doesn't suffer from out-of-vocabulary issues. The authors achieve this by using hash tokens in the alphabet instead of a single word, making it vocabulary independent. For classification, classifiers such as Multilayer Perceptron (MLP), Support Vector Machine (SVM) and Random Forest are used. A complete list of classifiers and training specifications are given in Section SECREF31.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was the performance on the self-collected corpus?",
    "llm_answer": "The best result achieved was a cosine similarity of 0.598.\n",
    "context": [
      {
        "id": "64d6e9ea-b5f7-434f-b304-09ef710ab6d9",
        "metadata": {
          "vector_store_key": "1911.12579-0",
          "chunk_id": 7,
          "document_id": "1911.12579",
          "start_idx": 4252,
          "end_idx": 4927
        },
        "page_content": "More recently NN based models yield state-of-the-art performance in multiple NLP tasks BIBREF24 BIBREF25 with the word embeddings. One of the advantages of such techniques is they use unsupervised approaches for learning representations and do not require annotated corpus which is rare for low-resourced Sindhi language. Such representions can be trained on large unannotated corpora, and then generated representations can be used in the NLP tasks which uses a small amount of labelled data. In this paper, we address the problems of corpus construction by collecting a large corpus of more than 61 million words from multiple web resources using the web-scrappy framework.",
        "type": "Document"
      },
      {
        "id": "3f796a82-78d4-4d2f-8165-b133aeaab300",
        "metadata": {
          "vector_store_key": "1911.12579-0",
          "chunk_id": 22,
          "document_id": "1911.12579",
          "start_idx": 12899,
          "end_idx": 13803
        },
        "page_content": "The corpus is a collection of human language text BIBREF31 built with a specific purpose. However, the statistical analysis of the corpus provides quantitative, reusable data, and an opportunity to examine intuitions and ideas about language. Therefore, the corpus has great importance for the study of written language to examine the text. In fact, realizing the necessity of large text corpus for Sindhi, we started this research by collecting raw corpus from multiple web resource using web-scrappy framwork for extraction of news columns of daily Kawish and Awami Awaz Sindhi newspapers, Wikipedia dumps, short stories and sports news from Wichaar social blog, news from Focus Word press blog, historical writings, novels, stories, books from Sindh Salamat literary websites, novels, history and religious books from Sindhi Adabi Board and tweets regarding news and sports are collected from twitter.",
        "type": "Document"
      },
      {
        "id": "ed5a02d2-9c9c-426a-8259-43f1563ee03d",
        "metadata": {
          "vector_store_key": "1911.12579-5",
          "chunk_id": 8,
          "document_id": "1911.12579",
          "start_idx": 4616,
          "end_idx": 5443
        },
        "page_content": "In this paper, we address the problems of corpus construction by collecting a large corpus of more than 61 million words from multiple web resources using the web-scrappy framework. After the collection of the corpus, we carefully preprocessed for the filtration of noisy text, e.g., the HTML tags and vocabulary of the English language. The statistical analysis is also presented for the letter, word frequencies and identification of stop-words. Finally, the corpus is utilized to generate Sindhi word embeddings using state-of-the-art GloVe BIBREF26 SG and CBoW BIBREF27 BIBREF20 BIBREF24 algorithms. The popular intrinsic evaluation method BIBREF20 BIBREF28 BIBREF29 of calculating cosine similarity between word vectors and WordSim353 BIBREF30 are employed to measure the performance of the learned Sindhi word embeddings.",
        "type": "Document"
      },
      {
        "id": "32b13f96-26ca-4593-b990-ee24b1316210",
        "metadata": {
          "vector_store_key": "1602.08741-3",
          "chunk_id": 26,
          "document_id": "1602.08741",
          "start_idx": 14573,
          "end_idx": 15159
        },
        "page_content": "It is also worth noting that number of \"missing\" pairs is negligible in 7-days corpus: the only missing word (and pair) is \"russian\u0439\u0435\u043b\u044c\", Yale, the name of university in the USA. There are no \"missing\" words in 15-days corpus. Training the model on 15-days corpus took 8 hours on our machine with 2 cores and 4Gb of RAM. We have an intuition that further improvements are possible with larger corpus. Comparing our results to ones reported by RUSSE participants, we conclude that our best result of 0.598 is comparable to other results, as it (virtually) encloses the top-10 of results.",
        "type": "Document"
      },
      {
        "id": "192ef8cf-b156-4ba5-a170-1e436e925cf4",
        "metadata": {
          "vector_store_key": "1905.11037-4",
          "chunk_id": 8,
          "document_id": "1905.11037",
          "start_idx": 4736,
          "end_idx": 5389
        },
        "page_content": "Table 2 details the statistics of the corpus (see also Appendix \"Corpus distribution\" ). Note that similar to Twitter corpora, fan fiction stories can be deleted over time by users or admins, causing losses in the dataset. We tokenized the samples with BIBREF14 and merged the occurrences of multi-word spells into a single token. This work addresses the task as a classification problem, and in particular as a sequence to label classification problem. For this reason, we rely on standard models used for this type of task: multinomial logistic regression, a multi-layered perceptron, convolutional neural networks and long short-term memory networks.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the size of their dataset?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "9ca5340d-587b-4e94-bcbe-160570788673",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 35,
          "document_id": "1910.12618",
          "start_idx": 19294,
          "end_idx": 19871
        },
        "page_content": "For instance for random forests it can correspond to the maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014 and 2015 serve as validation set (730 days).",
        "type": "Document"
      },
      {
        "id": "73a83f60-b2c9-406d-b4cd-5720639e5f06",
        "metadata": {
          "vector_store_key": "1812.07023-4",
          "chunk_id": 18,
          "document_id": "1812.07023",
          "start_idx": 10358,
          "end_idx": 10986
        },
        "page_content": "After 10 such qa turns, the questioner wraps up by writing a summary of the video based on the conversation. Dataset statistics such as the number of dialogues, turns, and words for the avsd dataset are presented in Table TABREF5 . For the initially released prototype dataset, the training set of the avsd dataset corresponds to videos taken from the training set of the Charades dataset while the validation and test sets of the avsd dataset correspond to videos taken from the validation set of the Charades dataset. For the official dataset, training, validation and test sets are drawn from the corresponding Charades sets.",
        "type": "Document"
      },
      {
        "id": "d87d327c-2858-4b5d-ae8a-860d401f63bd",
        "metadata": {
          "vector_store_key": "1908.05434-1",
          "chunk_id": 27,
          "document_id": "1908.05434",
          "start_idx": 14432,
          "end_idx": 15064
        },
        "page_content": "We have ensured that the raw dataset has no overlap with the labeled dataset to avoid bias in test accuracy. While it is possible to scrape more raw data, we did not observe significant improvements in model performances when the size of raw data increased from INLINEFORM0 70,000 to INLINEFORM1 170,000, hence we assume that the current raw dataset is sufficiently large. The labeled dataset is called Trafficking-10k. It consists of 12,350 ads from Backpage labeled by experts in human trafficking detection BIBREF9 . Each label is one of seven ordered levels of likelihood that the corresponding ad comes from a human trafficker.",
        "type": "Document"
      },
      {
        "id": "0fcc020b-3016-4745-902c-d73226505bf1",
        "metadata": {
          "vector_store_key": "1910.11949-3",
          "chunk_id": 17,
          "document_id": "1910.11949",
          "start_idx": 9318,
          "end_idx": 10010
        },
        "page_content": "Each source contains 5,000 images with 5 questions per image, adding a total of 15,000 images with 75,000 questions. COCO dataset includes images of complex everyday scenes containing common objects in their natural context, but it is limited in terms of the concepts it covers. Bing dataset contains more event related questions and has a wider range of questions longitudes (between 3 and 20 words), while Flickr questions are shorter (less than 6 words) and the images appear to be more casual. We use two datasets to train our chatbot model. The first one is the Persona-chat BIBREF15 which contains dialogues between two people with different profiles that are trying to know each other.",
        "type": "Document"
      },
      {
        "id": "8be62763-27f4-46ff-b2bb-edb6a6b2fa04",
        "metadata": {
          "vector_store_key": "1607.06025-3",
          "chunk_id": 42,
          "document_id": "1607.06025",
          "start_idx": 24463,
          "end_idx": 25198
        },
        "page_content": "The new dataset consists of training set, which is generated using examples from the original training set, and a development set, which is generated from the original development set. The beam size for beam search was set to 1. The details of the decision are presented in Section SECREF35 . Some datasets were constructed by filtering the generated datasets according to various thresholds. Thus, the generated datasets were constructed to contain enough examples, so that the filtered datasets had at least the number of examples as the original dataset. In the end, all the datasets were trimmed down to the size of the original dataset by selecting the samples sequentially from the beginning until the dataset had the right size.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "how was the dataset built?",
    "llm_answer": "The dataset was built in three steps: scraping PDF files from the Lok Sabha website, converting the PDFs to text files, and converting the unstructured information into a structured format.  Other datasets (novel-9, novel-50, IMDB62) were constructed using different methods, described in the text.\n",
    "context": [
      {
        "id": "8be62763-27f4-46ff-b2bb-edb6a6b2fa04",
        "metadata": {
          "vector_store_key": "1607.06025-3",
          "chunk_id": 42,
          "document_id": "1607.06025",
          "start_idx": 24463,
          "end_idx": 25198
        },
        "page_content": "The new dataset consists of training set, which is generated using examples from the original training set, and a development set, which is generated from the original development set. The beam size for beam search was set to 1. The details of the decision are presented in Section SECREF35 . Some datasets were constructed by filtering the generated datasets according to various thresholds. Thus, the generated datasets were constructed to contain enough examples, so that the filtered datasets had at least the number of examples as the original dataset. In the end, all the datasets were trimmed down to the size of the original dataset by selecting the samples sequentially from the beginning until the dataset had the right size.",
        "type": "Document"
      },
      {
        "id": "cba5e199-51a0-480b-ac30-c7035a74c736",
        "metadata": {
          "vector_store_key": "1607.06025-2",
          "chunk_id": 43,
          "document_id": "1607.06025",
          "start_idx": 25198,
          "end_idx": 26010
        },
        "page_content": "In the end, all the datasets were trimmed down to the size of the original dataset by selecting the samples sequentially from the beginning until the dataset had the right size. Also, the datasets were filtered so that each of the labels was represented equally. All the models, including classification and discriminative models, were trained with hidden dimension INLINEFORM0 set to 150, unless otherwise noted. Our implementation is accessible at http://github.com/jstarc/nli_generation. It is based on libraries Keras and Theano BIBREF45 . First, the classification model OrigClass was trained on the original dataset. This model was then used throughout the experiments for filtering the datasets, comparison, etc. Notice that we have assumed OrigClass to be ground truth for the purpose of our experiments.",
        "type": "Document"
      },
      {
        "id": "9b758cdd-bd9a-401f-8afd-ea03ad4e7faa",
        "metadata": {
          "vector_store_key": "1905.10044-9",
          "chunk_id": 21,
          "document_id": "1905.10044",
          "start_idx": 11683,
          "end_idx": 12403
        },
        "page_content": "Part of the value of this dataset is that it contains questions that people genuinely want to answer. To explore this further, we manually define a set of topics that questions can be about. An author categorized 200 questions into these topics. The results can be found in the upper half of Table . Questions were often about entertainment media (including T.V., movies, and music), along with other popular topics like sports. However, there are still a good portion of questions asking for more general factual knowledge, including ones about historical events or the natural world. We also broke the questions into categories based on what kind of information they were requesting, shown in the lower half of Table .",
        "type": "Document"
      },
      {
        "id": "52c382f9-744d-4ff7-a207-ad3ce700fa78",
        "metadata": {
          "vector_store_key": "1808.06834-0",
          "chunk_id": 13,
          "document_id": "1808.06834",
          "start_idx": 6943,
          "end_idx": 7533
        },
        "page_content": "For example, in the debate type \"Government Bills\", a minister places a bill on the table and discussion is carried out on the bill where as in the debate type \"Matter under 377\", each speaker raises an issue of which he is concerned of but no discussion is done on the issues. The creation of the dataset involved 3 steps. The first step was to scrap the pdf files from the Lok Sabha website. Each pdf file is a session. The second step was to convert the pdf files into text files for easy parsing. The challenge here was to convert this unstructured information into a structured format.",
        "type": "Document"
      },
      {
        "id": "6d614157-50dc-4aaa-af03-adc7f0282a12",
        "metadata": {
          "vector_store_key": "1709.02271-3",
          "chunk_id": 20,
          "document_id": "1709.02271",
          "start_idx": 11344,
          "end_idx": 11969
        },
        "page_content": "(Section SECREF26 ). The statistics for the three datasets used in the experiments are summarized in Table TABREF16 . novel-9. This dataset was compiled by F&H14: a collection of 19 novels by 9 nineteenth century British and American authors in the Project Gutenberg. To compare to F&H14, we apply the same resampling method (F&H14, Section 4.2) to correct the imbalance in authors by oversampling the texts of less-represented authors. novel-50. This dataset extends novel-9, compiling the works of 50 randomly selected authors of the same period. For each author, we randomly select 5 novels for a total 250 novels. IMDB62.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what processing was done on the speeches before being parsed?",
    "llm_answer": "Manual transcription, segmentation based on clear instructions to evaluators, and structuring into a relational database.\n",
    "context": [
      {
        "id": "734bcf43-c520-48e1-a71f-4951f5dbe1b6",
        "metadata": {
          "vector_store_key": "1708.04557-0",
          "chunk_id": 45,
          "document_id": "1708.04557",
          "start_idx": 25161,
          "end_idx": 25904
        },
        "page_content": "While all speeches made in D\u00e1il \u00c9ireann are dutifully recorded, the architecture of the data set, where digitized versions of speeches are stored, makes it impossible to apply any of the existing text analysis software. Speeches are currently stored by D\u00e1il \u00c9ireann in more than half a million separate HTML files with entries that are not related to one another. In this paper we present a new database of speeches that was created with the purpose of allowing the estimation of policy preferences of individual politicians. For that reason we created a relational database where speeches are related to the members database and structured in terms of dates, topics of debates, and names of speakers, their constituency and party affiliation.",
        "type": "Document"
      },
      {
        "id": "896d2a09-7143-45f3-a2e2-490ad497e48f",
        "metadata": {
          "vector_store_key": "1808.06834-0",
          "chunk_id": 33,
          "document_id": "1808.06834",
          "start_idx": 18169,
          "end_idx": 18764
        },
        "page_content": "We also tried to identify them automatically using fastText algorithm and provided the results. The analysis is done for understanding the purpose of the speeches in the parliament. We also presented our results on binary stance classification of the speeches whether the member is in favour of the debate topic or not. In future, we would like to increase the size of the dataset by including sessions of previous years which are not yet digitized. Sessions before 2009 are yet to be digitalised by the Lok Sabha editorial of India. Also we plan to include Rajya Sabha debates into the dataset.",
        "type": "Document"
      },
      {
        "id": "54e29f92-b537-4e33-8952-d8f2331f0a87",
        "metadata": {
          "vector_store_key": "1808.06834-0",
          "chunk_id": 1,
          "document_id": "1808.06834",
          "start_idx": 660,
          "end_idx": 1350
        },
        "page_content": "Since the data is unstructured , it cannot be computationally analyzed. There is a need to shape the data into a structured format for analysis. This data is important as it can be used to visualize person, party and agenda level semantics in the house. The data that we get from parliamentary proceedings has presence of sarcasm, interjections and allegations which makes it difficult to apply standard NLP techniques BIBREF0 . Members of the parliament discuss various important aspects and there is a strong purpose behind every speech. We wanted to analyze this particular aspect. Traditional polar stances (for or against) do not justify for the diplomatic intricacies in the speeches.",
        "type": "Document"
      },
      {
        "id": "463438ef-caf2-4802-8966-9430a0960731",
        "metadata": {
          "vector_store_key": "1808.08850-3",
          "chunk_id": 23,
          "document_id": "1808.08850",
          "start_idx": 13023,
          "end_idx": 13771
        },
        "page_content": "The selected videos cover different topics like technology, human rights, terrorism and politics with a length variation between 2 and 10 minutes. To encourage the diversity of content format we included newscasts, interviews, reports and round tables. During the transcription phase we opted for a manual transcription process because we observed that using transcripts from an ASR system will difficult in a large degree the manual segmentation process. The number of words per transcript oscilate between 271 and 1,602 with a total number of 8,080. We gave clear instructions to three evaluators ( INLINEFORM0 ) of how segmentation was needed to be perform, including the SU concept and how punctuation marks were going to be taken into account.",
        "type": "Document"
      },
      {
        "id": "d9e49fee-6098-495f-b261-c464af0725e1",
        "metadata": {
          "vector_store_key": "1708.04557-0",
          "chunk_id": 44,
          "document_id": "1708.04557",
          "start_idx": 24356,
          "end_idx": 25161
        },
        "page_content": "This and related questions are outside the scope of this paper and can be pursued by researchers with the help of our database of parliamentary speeches. Policy preferences of individual politicians (ministers or TDs in general), are inherently unobservable. However, we have abundant data on speeches made by political actors. The latest developments in automated text analysis techniques allow us to estimate the policy positions of individual actors from these speeches. In relation to Irish political actors such estimation has been hindered by the structure of the available data. While all speeches made in D\u00e1il \u00c9ireann are dutifully recorded, the architecture of the data set, where digitized versions of speeches are stored, makes it impossible to apply any of the existing text analysis software.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance change of the textual semantic similarity task when no error and maximum errors (noise) are present?",
    "llm_answer": "Very good numbers when no error (0.0% error); significant drop in performance as error percentage increases.\n",
    "context": [
      {
        "id": "5bc180f1-e616-436f-a0ae-3039c38d429d",
        "metadata": {
          "vector_store_key": "2003.12932-1",
          "chunk_id": 14,
          "document_id": "2003.12932",
          "start_idx": 7618,
          "end_idx": 8311
        },
        "page_content": "Figure FIGREF12stsa and FIGREF12stsb shows Pearson-Spearman correlation vs % of error for textual semantic similarity on STS-B dataset after fine tuning for 10 and 50 epochs respectively. It is clear from the above plots that as we increase the percentage of error, for each of the three tasks, we see a significant drop in BERT\u2019s performance. Also, from the plots it is evident that the reason for this drop in performance is introduction of noise (spelling mistakes). After all we get very good numbers, for each of the three tasks, when there is no error (0.0 % error). To understand the reason behind the drop in performance, first we need to understand how BERT processes input text data.",
        "type": "Document"
      },
      {
        "id": "37bf835c-58ac-40c1-b3c9-d56259b7e264",
        "metadata": {
          "vector_store_key": "2003.12932-0",
          "chunk_id": 13,
          "document_id": "2003.12932",
          "start_idx": 7097,
          "end_idx": 7869
        },
        "page_content": "Let us discuss the results from the above mentioned experiments. We show the plots of accuracy vs noise for each of the tasks. For IMDB, we fine tune the model for the sentiment analysis task. We plot F1 score vs % of error, as shown in Figure FIGREF6. Figure FIGREF6imdba shows the performance after fine tuning for 10 epochs, while Figure FIGREF6imdbb shows the performance after fine tuning for 50 epochs. Similarly, Figure FIGREF9ssta and Figure FIGREF9sstb) shows F1 score vs % of error for Sentiment analysis on SST-2 dataset after fine tuning for 10 and 50 epochs respectively. Figure FIGREF12stsa and FIGREF12stsb shows Pearson-Spearman correlation vs % of error for textual semantic similarity on STS-B dataset after fine tuning for 10 and 50 epochs respectively.",
        "type": "Document"
      },
      {
        "id": "08eed729-18e0-4c2b-8dde-cd4c4a591d7a",
        "metadata": {
          "vector_store_key": "1705.03151-3",
          "chunk_id": 78,
          "document_id": "1705.03151",
          "start_idx": 42286,
          "end_idx": 43072
        },
        "page_content": "It can be seen that the PTN system is more noise-robust: with more noise corruption, the gap between the i-vector system and the PTN system becomes less significant, and the PTN system is better than the i-vector system in terms of $C_{avg}$ when the noise level is high (SNR=10). This can be observed more clearly in Fig. 6 , where the performance degradation rates compared to the noise-free condition are shown. The figure shows that when the noise increases, the performance degradation with the PTN system is less significant compared to the degradation with the i-vector system. As the Babel speech data is much more noisy than the AP16-OLR speech, this noise robustness with the PTN approach partly explains why the relative performance is inconsistent between the two databases.",
        "type": "Document"
      },
      {
        "id": "b8af840b-cbb4-4571-b8d4-17d5d5bd771a",
        "metadata": {
          "vector_store_key": "1909.08041-2",
          "chunk_id": 40,
          "document_id": "1909.08041",
          "start_idx": 21880,
          "end_idx": 22616
        },
        "page_content": "To further study the effects of upstream semantic retrieval towards downstream tasks, we change training or inference data between intermediate layers and then examine how this modification will affect the downstream performance. We fixed $h_p=0$ (the value achieving the best performance) and re-trained all the downstream parameters and track their performance as $k_p$ (the number of selected paragraph) being changed from 1 to 12. The increasing of $k_p$ means a potential higher coverage of the answer but more noise in the retrieved facts. Fig. FIGREF17 shows the results. As can be seen that the EM scores for supporting fact retrieval, answer prediction, and joint performance increase sharply when $k_p$ is changed from 1 to 2.",
        "type": "Document"
      },
      {
        "id": "385278e5-9dc8-479b-aa51-1d7e735b84ad",
        "metadata": {
          "vector_store_key": "1808.09409-1",
          "chunk_id": 31,
          "document_id": "1808.09409",
          "start_idx": 17778,
          "end_idx": 18479
        },
        "page_content": "After fixing the boundaries of spans, the neural syntax-agnostic system catches up with the other, illustrating that though both systems handle boundary detection poorly on the L2 sentences, the neural syntax-agnostic one suffers more from this type of errors. Excluding boundary errors (after moving, merging, splitting spans and fixing boundaries), we also compare two systems on L2 in terms of detailed label identification, so as to observe which semantic role is more likely to be incorrectly labeled. Figure FIGREF24 shows the confusion matrices. Comparing (a) with (c) and (b) with (d), we can see that the syntax-based and the neural system often overly label A1 when processing learner texts.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which sentiment analysis data set has a larger performance drop when a 10% error is introduced?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "37bf835c-58ac-40c1-b3c9-d56259b7e264",
        "metadata": {
          "vector_store_key": "2003.12932-0",
          "chunk_id": 13,
          "document_id": "2003.12932",
          "start_idx": 7097,
          "end_idx": 7869
        },
        "page_content": "Let us discuss the results from the above mentioned experiments. We show the plots of accuracy vs noise for each of the tasks. For IMDB, we fine tune the model for the sentiment analysis task. We plot F1 score vs % of error, as shown in Figure FIGREF6. Figure FIGREF6imdba shows the performance after fine tuning for 10 epochs, while Figure FIGREF6imdbb shows the performance after fine tuning for 50 epochs. Similarly, Figure FIGREF9ssta and Figure FIGREF9sstb) shows F1 score vs % of error for Sentiment analysis on SST-2 dataset after fine tuning for 10 and 50 epochs respectively. Figure FIGREF12stsa and FIGREF12stsb shows Pearson-Spearman correlation vs % of error for textual semantic similarity on STS-B dataset after fine tuning for 10 and 50 epochs respectively.",
        "type": "Document"
      },
      {
        "id": "6e964ac3-a82e-490e-b0b4-e00967227066",
        "metadata": {
          "vector_store_key": "2001.00137-1",
          "chunk_id": 31,
          "document_id": "2001.00137",
          "start_idx": 18607,
          "end_idx": 19358
        },
        "page_content": "The stack of multilayer perceptrons are trained for 100 and 1,000 epochs with Adam Optimizer, learning rate of $10^{-3}$, weight decay of $10^{-5}$, MSE loss criterion and batch size the same as BERT (4 for the Twitter Sentiment Corpus and 8 for the Chatbot Intent Classification Corpus). Experimental results for the Twitter Sentiment Classification task on Kaggle's Sentiment140 Corpus dataset, displayed in Table TABREF37, show that our model has better F1-micros scores, outperforming the baseline models by 6$\\%$ to 8$\\%$. We evaluate our model and baseline models on three versions of the dataset. The first one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80$\\%$ against BERT's 72$\\%$.",
        "type": "Document"
      },
      {
        "id": "96873d13-d159-48ce-a4d8-81d08959d1d9",
        "metadata": {
          "vector_store_key": "1904.07342-1",
          "chunk_id": 16,
          "document_id": "1904.07342",
          "start_idx": 8754,
          "end_idx": 9435
        },
        "page_content": "(see Section SECREF4 ). Note that we perform these mean comparisons on all event-related data, since the low number of geo-tagged samples would produce an underpowered study. In Figure FIGREF8 , we see that overall sentiment averages rarely show movement post-event: that is, only Hurricane Florence shows a significant difference in average tweet sentiment pre- and post-event at the 1% level, corresponding to a 0.12 point decrease in positive climate change sentiment. However, controlling for the same group of users tells a different story: both Hurricane Florence and Hurricane Michael have significant tweet sentiment average differences pre- and post-event at the 1% level.",
        "type": "Document"
      },
      {
        "id": "12a82030-4aca-4d9d-82e6-30f9bc204694",
        "metadata": {
          "vector_store_key": "1904.07342-1",
          "chunk_id": 18,
          "document_id": "1904.07342",
          "start_idx": 9722,
          "end_idx": 10475
        },
        "page_content": "Hurricane Michael sees an increase in average tweet sentiment of 0.11 points, which reverses the direction of tweets from mostly negative pre-event to mostly positive post-event. Likely due to similar bias reasons, the Mendocino wildfires in California see a 0.06 point decrease in overall sentiment post-event, but a 0.09 point increase in within-cohort sentiment. Methodologically, we assert that overall averages are not robust results to use in sentiment analyses. We now comment on the two events yielding similar results between overall and within-cohort comparisons. Most tweets regarding the Bomb Cyclone have negative sentiment, though sentiment increases by 0.02 and 0.04 points post-event for overall and within-cohort averages, respectively.",
        "type": "Document"
      },
      {
        "id": "8b73dbb1-4c32-466d-a3ef-9dd49740ed9a",
        "metadata": {
          "vector_store_key": "1803.07771-3",
          "chunk_id": 1,
          "document_id": "1803.07771",
          "start_idx": 885,
          "end_idx": 1641
        },
        "page_content": "Although many achievements have been made and sentiment analysis has been successfully used in various commercial applications, its accuracy can be further improved. The construction of a high-accuracy sentiment classification model usually entails the challenging compilation of training sets with numerous samples and sufficiently accurate labels. The reason behind this difficulty is two-fold. First, sentiment is somewhat subjective, and a sample may receive different labels from different users. Second, some texts contain complex sentiment representations, and a single label is difficult to provide. We conduct a statistical analysis of public Chinese sentiment text sets in GitHub. The results show that the average label error is larger than 10%.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much is pre-training loss increased in Low/Medium/Hard level of pruning?",
    "llm_answer": "The context states that pre-training loss increases as weights necessary for fitting the pre-training data are pruned (medium levels).  No specific values of loss increase are given.\n",
    "context": [
      {
        "id": "14bf3271-bd0b-49be-b3fa-6b2f0e07ad39",
        "metadata": {
          "vector_store_key": "2002.08307-4",
          "chunk_id": 21,
          "document_id": "2002.08307",
          "start_idx": 11864,
          "end_idx": 12551
        },
        "page_content": "We then pruned at various sparsity levels and continued training for 5 more epochs (7 for 80/90% sparsity), at which point the training losses became comparable to those of models pruned during pre-training. We repeat this for learning rates in $[2, 3, 4, 5] \\times 10^{-5}$ and show the results with the best development accuracy in Figure FIGREF15 / Table TABREF27. We also measure the difference in which weights are selected for pruning during pre-training vs. downstream fine-tuning and plot the results in Figure FIGREF25. Figure FIGREF15 shows that the first 30-40% of weights pruned by magnitude weight pruning do not impact pre-training loss or inference on any downstream task.",
        "type": "Document"
      },
      {
        "id": "2e2fa126-a962-4c01-8f3c-b0dbfd2235c9",
        "metadata": {
          "vector_store_key": "2002.08307-4",
          "chunk_id": 23,
          "document_id": "2002.08307",
          "start_idx": 12878,
          "end_idx": 13571
        },
        "page_content": "Pre-training loss increases as we prune weights necessary for fitting the pre-training data (Table TABREF27). Feature activations of the hidden layers start to diverge from models with low levels of pruning (Figure FIGREF18). Downstream accuracy also begins to degrade at this point. We believe this observation may point towards a more principled stopping criterion for pruning. Currently, the only way to know how much to prune is by trial and (dev-set) error. Predictors of performance degradation while pruning might help us decide which level of sparsity is appropriate for a given trained network without trying many at once. Why does pruning at these levels hurt downstream performance?",
        "type": "Document"
      },
      {
        "id": "7e8f6ed1-b54b-4208-8f14-c400e11ac3b0",
        "metadata": {
          "vector_store_key": "2002.08307-4",
          "chunk_id": 22,
          "document_id": "2002.08307",
          "start_idx": 12186,
          "end_idx": 12878
        },
        "page_content": "Figure FIGREF15 shows that the first 30-40% of weights pruned by magnitude weight pruning do not impact pre-training loss or inference on any downstream task. These weights can be pruned either before or after fine-tuning. This makes sense from the perspective of pruning as sparse architecture search: when we initialize BERT-Base, we initialize many possible subnetworks. SGD selects the best one for pre-training and pushes the rest of the weights to 0. We can then prune those weights without affecting the output of the network. Past 40% pruning, performance starts to degrade. Pre-training loss increases as we prune weights necessary for fitting the pre-training data (Table TABREF27).",
        "type": "Document"
      },
      {
        "id": "fe44c0a0-10c1-48ea-8432-abe5ae6960ae",
        "metadata": {
          "vector_store_key": "2002.08307-4",
          "chunk_id": 24,
          "document_id": "2002.08307",
          "start_idx": 13401,
          "end_idx": 14265
        },
        "page_content": "Why does pruning at these levels hurt downstream performance? On one hand, pruning deletes pre-training information by setting weights to 0, preventing the transfer of the useful inductive biases learned during pre-training. On the other hand, pruning regularizes the model by keeping certain weights at zero, which might prevent fitting downstream datasets. Figure FIGREF15 and Table TABREF27 show information deletion is the main cause of performance degradation between 40 - 60% sparsity, since pruning and information deletion degrade models by the same amount. Information deletion would not be a problem if pre-training and downstream datasets contained similar information. However, pre-training is effective precisely because the pre-training dataset is much larger than the labeled downstream dataset, which allows learning of more robust representations.",
        "type": "Document"
      },
      {
        "id": "f69467a9-6641-46ed-912b-4abd050e7974",
        "metadata": {
          "vector_store_key": "2002.08307-4",
          "chunk_id": 4,
          "document_id": "2002.08307",
          "start_idx": 2239,
          "end_idx": 2911
        },
        "page_content": "We chose magnitude weight pruning, which compresses models by removing weights close to 0, because it is one of the most fine-grained and effective compression methods and because there are many interesting ways to view pruning, which we explore in the next section. Our findings are as follows: Low levels of pruning (30-40%) do not increase pre-training loss or affect transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. This information is not equally useful to each task; tasks degrade linearly with pre-train loss, but at different rates.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the average length of the recordings?",
    "llm_answer": "4.74 seconds\n",
    "context": [
      {
        "id": "55c30e76-1156-4598-b752-3cb69409cbcc",
        "metadata": {
          "vector_store_key": "1910.04269-4",
          "chunk_id": 15,
          "document_id": "1910.04269",
          "start_idx": 9121,
          "end_idx": 9829
        },
        "page_content": "As an average human's voice is around 300 Hz and according to Nyquist-Shannon sampling theorem all the useful frequencies (0-300 Hz) are preserved with sampling at 8 kHz, therefore, we sampled raw audio files from all six languages at 8 kHz The average length of audio files in this dataset was about 10.4 seconds and standard deviation was 2.3 seconds. For our experiments, the audio length was set to 10 seconds. If the audio files were shorter than 10 second, then the data was repeated and concatenated. If audio files were longer, then the data was truncated. We applied the following design principles to all our models: Every convolutional layer is always followed by an appropriate max pooling layer.",
        "type": "Document"
      },
      {
        "id": "00e532d4-d3a8-4126-84ce-7f17e240d0c5",
        "metadata": {
          "vector_store_key": "1909.06522-1",
          "chunk_id": 16,
          "document_id": "1909.06522",
          "start_idx": 8840,
          "end_idx": 9509
        },
        "page_content": "To obtain diverse noisy audios, we use AudioSet, which consists of 632 audio event classes and a collection of over 2 million manually-annotated 10-second sound clips from YouTube videos BIBREF28. Note that in our video datasets, video lengths vary between 10 seconds and 5 minutes, with an average duration of about 2 minutes. Rather than constantly repeating the 10-second sound clip to match the original minute-long audio, we superpose each sound clip on the short utterances via audio segmentation. Specifically, we first use an initial bootstrap model to align each original long audio, and segment each audio into around 10-second utterances via word boundaries.",
        "type": "Document"
      },
      {
        "id": "e936b081-0322-4d1b-b3d9-c5c86f80fe9c",
        "metadata": {
          "vector_store_key": "2001.08051-0",
          "chunk_id": 23,
          "document_id": "2001.08051",
          "start_idx": 12196,
          "end_idx": 12775
        },
        "page_content": "On average, the audio signal quality is nearly good, while the main problem is caused by a high percentage of extraneous speech. This is due to the fact that organisers decided to use a fixed duration - which depends on the question - for recording spoken utterances, so that all the recordings for a given question have the same length. However, while it is rare that a speaker has not enough time to answer, it is quite common that, especially after the end of the utterance, some other speech (e.g. comments, jokes with mates, indications from the teachers, etc.) is captured.",
        "type": "Document"
      },
      {
        "id": "8d6dc6bb-87ea-4460-a4fd-dede0d49128a",
        "metadata": {
          "vector_store_key": "2002.11268-3",
          "chunk_id": 21,
          "document_id": "2002.11268",
          "start_idx": 11315,
          "end_idx": 12080
        },
        "page_content": "The test set is taken from 296 videos from 13 categories, with each video averaging 5 minutes in length, corresponding to 25 hours of audio and 250,000 word tokens in total. Target-domain Dev & Eval sets (Voice Search). The Voice Search dev and eval sets each consist of approximately 7,500 anonymized utterances (about 33,000 words and corresponding to about 8 hours of audio), distinct from the fine-tuning data described earlier, but representative of the same Voice Search service. The first set of experiments uses an RNN-T model trained on {audio, transcript} pairs taken from segmented YouTube videos, and evaluates the cross-domain generalization of this model to test utterances taken from a Voice Search dataset, with and without fusion to an external LM.",
        "type": "Document"
      },
      {
        "id": "518e9057-d249-4c95-a1b8-33e3807408fd",
        "metadata": {
          "vector_store_key": "1705.03151-5",
          "chunk_id": 47,
          "document_id": "1705.03151",
          "start_idx": 26030,
          "end_idx": 26733
        },
        "page_content": "For Russian, Korean and Japanese, there are 2 recording conditions for each speaker, quiet and noisy. The average duration (including silence) of all the $12,939$ test utterances of the seven languages is $4.74$ seconds, ranging from $1.08$ seconds to $18.06$ seconds. The phonetic DNN is a TDNN structure, and the LID model is based on the LSTM-RNN. The raw feature used for those models consists of 23-dimensional Fbanks, with a symmetric 2-frame window for RNN and a symmetric 4-frame window for TDNN to splice neighboring frames. All the experiments were conducted with Kaldi BIBREF43 . The default configurations of the Kaldi WSJ s5 nnet3 recipe were used to train the phonetic DNN and the LID RNN.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What were their results?",
    "llm_answer": "The system surpassed baselines in machine translation (English-French), achieving better BLEU and TER scores on various test sets.  Answer prediction scores were also higher, with improvements in EM and F1.  FEVER scores also improved substantially.  Human evaluation of incident descriptions showed surprisingly low agreement with recorded harm levels.  No major improvements were found in this human evaluation aspect and results varied between runs.\n",
    "context": [
      {
        "id": "d2e2382c-8e5a-4f0c-92c5-2f119e6e1178",
        "metadata": {
          "vector_store_key": "1910.07481-0",
          "chunk_id": 20,
          "document_id": "1910.07481",
          "start_idx": 11787,
          "end_idx": 12518
        },
        "page_content": "It also surpassed the Baselinee by 0.18 points on the newstest2017 with strong statistical significance, and by 0.15 BLEU points on the newstest2018 but this time with no statistical evidence. These encouraging results prompted us to extend experiments to another language pair: English-French. En$\\rightarrow $Fr: The Document system obtained the best results considering all metrics on all test sets with strong statistical evidence. It surpassed the Baseline by 1.09 BLEU points and 0.85 TER points on tst2015, 0.75 BLEU points and 0.76 TER points on tst2014, and 0.48 BLEU points and 0.68 TER points on tst2013. Fr$\\rightarrow $En: Of all experiments, this language pair shows the most important improvements over the Baseline.",
        "type": "Document"
      },
      {
        "id": "e6cc80ff-8234-41d0-a536-79a0fdc8eced",
        "metadata": {
          "vector_store_key": "1909.08041-4",
          "chunk_id": 31,
          "document_id": "1909.08041",
          "start_idx": 16805,
          "end_idx": 17517
        },
        "page_content": "The scores for answer predictions are also higher than all previous best results with $\\sim $8 absolute points increase on EM and $\\sim $9 absolute points on F1. All the improvements are consistent between test and dev set evaluation. Similarly for FEVER, we showed F1 for evidence, the Label Accuracy, and the FEVER Score (same as benchmark evaluation) for models in Table TABREF9. Our system obtained substantially higher scores than all previously published results with a $\\sim $4 and $\\sim $3 points absolute improvement on Label Accuracy and FEVER Score. In particular, the system gains 74.62 on the evidence F1, 22 points greater that of the second system, demonstrating its ability on semantic retrieval.",
        "type": "Document"
      },
      {
        "id": "beee7b9e-00bc-4ef2-8232-722a694c8ed8",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 28,
          "document_id": "1909.00542",
          "start_idx": 15261,
          "end_idx": 15995
        },
        "page_content": "Therefore we do not comment on the results, other than observing that the \u201cfirst $n$\u201d baseline produces the same results as the neural regressor. As mentioned in Section SECREF3, the labels used for the classification experiments are the 5 sentences with highest ROUGE-SU4 F1 score. Macquarie University's participation in BioASQ 7 focused on the task of generating the ideal answers. The runs use query-based extractive techniques and we experiment with classification, regression, and reinforcement learning approaches. At the time of writing there were no human evaluation results, and based on ROUGE-F1 scores under cross-validation on the training data we observed that classification approaches outperform regression approaches.",
        "type": "Document"
      },
      {
        "id": "3974aa33-b722-4ac7-a813-657d9c549bee",
        "metadata": {
          "vector_store_key": "1909.00183-9",
          "chunk_id": 91,
          "document_id": "1909.00183",
          "start_idx": 50786,
          "end_idx": 51481
        },
        "page_content": "In order to have a further evaluation of our results, we asked three clinicians to analyse ex novo a randomly chosen sample of 135 descriptions of incidents, and to determine their degree of harm based on the information in the incident report. The sample was selected from the O-ranked dataset and no extra information apart from the text was provided. We then compared the DoH assigned by the clinicians with both the results of our classifier and the recorded DoH in the dataset. Remarkably, the agreement rate of the clinicians' assessment with the recorded DoH was surprisingly low. For example, the agreement in the `No Harm' incidents was only 38%, and in the `Severe' incidents only 49%.",
        "type": "Document"
      },
      {
        "id": "8cc8bd92-31e2-4453-8f44-51b502dbf306",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 22,
          "document_id": "1909.00542",
          "start_idx": 11832,
          "end_idx": 12601
        },
        "page_content": "The results, not shown in the paper, indicated no major improvement, and re-runs of the experiments showed different results on different runs. Consequently, our submission to BioASQ included the original system using $tf.idf$ as input features in all batches but batch 2, as described in Section SECREF7. As mentioned in Section SECREF5, there appears to be a large discrepancy between ROUGE Recall and the human evaluations. This section describes a correlation analysis between human and ROUGE evaluations using the runs of all participants to all previous BioASQ challenges that included human evaluations (Phase B, ideal answers). The human evaluation results were scraped from the BioASQ Results page, and the ROUGE results were kindly provided by the organisers.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the prediction accuracy of the model?",
    "llm_answer": "82.0%\n",
    "context": [
      {
        "id": "06462dee-4a06-4c7c-87ee-59d9dd1e931d",
        "metadata": {
          "vector_store_key": "1607.06025-5",
          "chunk_id": 58,
          "document_id": "1607.06025",
          "start_idx": 33998,
          "end_idx": 34682
        },
        "page_content": "The accuracy of this classifier was 82.0%, which is 0.8 % better than the classifier trained solely on the original training set. However, the lowest average loss is achieved by the classifier trained on the original dataset. We also did a qualitative evaluation of the generated hypothesis. Hypotheses are mostly grammatically sound. Sometimes the models incorrectly use indefinite articles, for instance \u201dan phone\u201d, or possessive pronouns \u201da man uses her umbrella\u201d. These may be due to the fact the system must learn the right indefinite article for every word separately. On the other hand, the models sometimes generate hypotheses that showcase more advanced grammatical patterns.",
        "type": "Document"
      },
      {
        "id": "bf3ac110-751b-414e-8fc6-6054068e6c96",
        "metadata": {
          "vector_store_key": "1802.05322-3",
          "chunk_id": 9,
          "document_id": "1802.05322",
          "start_idx": 4909,
          "end_idx": 5555
        },
        "page_content": "( INLINEFORM0 ), true negative ( INLINEFORM1 ), false positive ( INLINEFORM2 ) and false negative ( INLINEFORM3 ) which can be seen in table TABREF16 . Accuracy is a measurement of how correct a model's predictions are and is defined as DISPLAYFORM0  . Precision is a ratio of how often positive predictions actually are positve and is defined as DISPLAYFORM0  . Recall is a measurement of how good the model is to find all true positives and is defined as DISPLAYFORM0  . BIBREF5  It has been shown that when calculating precision and recall on multi-label classifiers, it can be advantageous to use micro averaged precision and recall BIBREF6 .",
        "type": "Document"
      },
      {
        "id": "8402f139-fae0-4991-82bd-f2f64bf36044",
        "metadata": {
          "vector_store_key": "1907.00758-0",
          "chunk_id": 27,
          "document_id": "1907.00758",
          "start_idx": 15449,
          "end_idx": 16135
        },
        "page_content": "By placing a threshold of 0.5 on predicted distances, the model achieves 69.9% binary classification accuracy on training samples, 64.7% on validation samples, and 65.3% on test samples. Synchronisation offset prediction: Section SECREF3 described briefly how to use our model to predict the synchronisation offset for test utterances. To obtain a discretised set of offset candidates, we retrieve the true offsets of the training utterances, and find that they fall in the range [0, 179] ms. We discretise this range taking 45ms steps and rendering 40 candidate values (45ms is the smaller of the absolute values of the detectability boundaries, INLINEFORM0 125 and INLINEFORM1 45 ms).",
        "type": "Document"
      },
      {
        "id": "e6cc80ff-8234-41d0-a536-79a0fdc8eced",
        "metadata": {
          "vector_store_key": "1909.08041-4",
          "chunk_id": 31,
          "document_id": "1909.08041",
          "start_idx": 16805,
          "end_idx": 17517
        },
        "page_content": "The scores for answer predictions are also higher than all previous best results with $\\sim $8 absolute points increase on EM and $\\sim $9 absolute points on F1. All the improvements are consistent between test and dev set evaluation. Similarly for FEVER, we showed F1 for evidence, the Label Accuracy, and the FEVER Score (same as benchmark evaluation) for models in Table TABREF9. Our system obtained substantially higher scores than all previously published results with a $\\sim $4 and $\\sim $3 points absolute improvement on Label Accuracy and FEVER Score. In particular, the system gains 74.62 on the evidence F1, 22 points greater that of the second system, demonstrating its ability on semantic retrieval.",
        "type": "Document"
      },
      {
        "id": "8eadeabb-5370-4958-bea8-48a63ba10138",
        "metadata": {
          "vector_store_key": "2001.11268-5",
          "chunk_id": 40,
          "document_id": "2001.11268",
          "start_idx": 22945,
          "end_idx": 23764
        },
        "page_content": "In practice, the detachment between model predictions and assignment of labels means that a reviewer who wishes to switch between high recall and high precision results can do so very quickly, without obtaining new predictions from the model itself. More visualizations can be found in this project's GitHub repository , including true class labels and a detailed breakdown of true and false predictions for each class. The highest proportion of false classification appears between the results and conclusion classes. The fine-tuned multilingual model showed marginally inferior classification scores on the exclusively English testing data. However, this model's contribution is not limited to the English language because its interior weights embed a shared vocabulary of 100 languages, including German and Chinese.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the dataset used in the paper?",
    "llm_answer": "SCI, novel-9, novel-50, IMDB62.\n",
    "context": [
      {
        "id": "c5f16988-f9bb-43d0-a24e-a2d52ae2d42a",
        "metadata": {
          "vector_store_key": "1802.05574-4",
          "chunk_id": 11,
          "document_id": "1802.05574",
          "start_idx": 6530,
          "end_idx": 7227
        },
        "page_content": "This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available and the corpus provides both an XML and a simple text version of each article. We randomly selected 2 sentences with more than two words from each paper using the simple text version of the paper.",
        "type": "Document"
      },
      {
        "id": "711988d5-b6d5-455f-a849-15926d3a08fb",
        "metadata": {
          "vector_store_key": "1910.12618-2",
          "chunk_id": 12,
          "document_id": "1910.12618",
          "start_idx": 6819,
          "end_idx": 7418
        },
        "page_content": "The rest of this paper is organized as follows. The following section introduces the two data sets used to conduct our study. Section 3 presents the different machine learning approaches used and how they were tuned. Section 4 highlights the main results of our study, while section 5 concludes this paper and gives insight on future possible work. In order to prove the consistency of our work, experiments have been conducted on two data sets, one for France and the other for the UK. In this section details about the text and time series data are given, as well as the major preprocessing steps.",
        "type": "Document"
      },
      {
        "id": "6d614157-50dc-4aaa-af03-adc7f0282a12",
        "metadata": {
          "vector_store_key": "1709.02271-3",
          "chunk_id": 20,
          "document_id": "1709.02271",
          "start_idx": 11344,
          "end_idx": 11969
        },
        "page_content": "(Section SECREF26 ). The statistics for the three datasets used in the experiments are summarized in Table TABREF16 . novel-9. This dataset was compiled by F&H14: a collection of 19 novels by 9 nineteenth century British and American authors in the Project Gutenberg. To compare to F&H14, we apply the same resampling method (F&H14, Section 4.2) to correct the imbalance in authors by oversampling the texts of less-represented authors. novel-50. This dataset extends novel-9, compiling the works of 50 randomly selected authors of the same period. For each author, we randomly select 5 novels for a total 250 novels. IMDB62.",
        "type": "Document"
      },
      {
        "id": "06e6bbde-6248-49dd-b3d1-ed9e42585402",
        "metadata": {
          "vector_store_key": "1912.01046-2",
          "chunk_id": 4,
          "document_id": "1912.01046",
          "start_idx": 2523,
          "end_idx": 3469
        },
        "page_content": "The dataset includes about 6,000 triples, comprised of videos, questions, and answer spans manually collected from screencast tutorial videos with spoken narratives for a photo-editing software. Section SECREF4 presents the baseline models and their experiment details on the sentence-level prediction and video segment retrieval tasks on our dataset. Then, we discuss the experimental results in Section SECREF5 and conclude the paper in Section SECREF6. Most relevant to our proposed work is the reading comprehension task, which is a question answering task involving a piece of text such as a paragraph or article. Such datasets for the reading comprehension task, such as SQuAD BIBREF6 based on Wikipedia, TriviaQA BIBREF7 constructed from trivia questions with answer evidence from Wikipedia, or those from Hermann et al. based on CNN and Daily Mail articles BIBREF8 are factoid-based, meaning the answers typically involve a single entity.",
        "type": "Document"
      },
      {
        "id": "52c382f9-744d-4ff7-a207-ad3ce700fa78",
        "metadata": {
          "vector_store_key": "1808.06834-0",
          "chunk_id": 13,
          "document_id": "1808.06834",
          "start_idx": 6943,
          "end_idx": 7533
        },
        "page_content": "For example, in the debate type \"Government Bills\", a minister places a bill on the table and discussion is carried out on the bill where as in the debate type \"Matter under 377\", each speaker raises an issue of which he is concerned of but no discussion is done on the issues. The creation of the dataset involved 3 steps. The first step was to scrap the pdf files from the Lok Sabha website. Each pdf file is a session. The second step was to convert the pdf files into text files for easy parsing. The challenge here was to convert this unstructured information into a structured format.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does the SCAN dataset evaluate compositional generalization?",
    "llm_answer": "The SCAN dataset evaluates compositional generalization by testing the ability to generalize known primitive verbs to valid unseen constructions and, later, by including compositional generalization based on known templates (e.g., \"Jump around right\").\n",
    "context": [
      {
        "id": "e11b96d4-517a-4ccb-b170-540815c43211",
        "metadata": {
          "vector_store_key": "1904.09708-0",
          "chunk_id": 63,
          "document_id": "1904.09708",
          "start_idx": 33092,
          "end_idx": 33751
        },
        "page_content": "However, it is also possible that the simplicity of the semantic stream in the model is also important for improving compositional generalization. To test this, we replaced the linear layer in the semantic stream with a nonlinear neural network. From the model description in the main text:  $$p(y_i|y_1, y_2, ..., y_{i-1}, \\mathbf {x}) = f(d_i),$$   (Eq. 37)  In the original model, $f$ was parameterized with a simple linear layer, but here we use a two-layer feedforward network with a ReLU nonlinearity, before a softmax is applied to generate a distribution over the possible actions. We tested this model on the add-primitive splits of the SCAN dataset.",
        "type": "Document"
      },
      {
        "id": "79104f57-bf22-4fa0-b9e5-936e3e9cf756",
        "metadata": {
          "vector_store_key": "1904.09708-2",
          "chunk_id": 67,
          "document_id": "1904.09708",
          "start_idx": 35353,
          "end_idx": 36210
        },
        "page_content": "The Syntactic Attention model shows a substantial improvement over previously reported results at the lowest numbers of \"jump\" examples used for training (see Figure 8 and Table 4 ). Compositional generalization performance is already quite high at 1 example, and at 2 examples is almost perfect (99.997% correct). The compositional generalization splits of the SCAN dataset were originally designed to test for the ability to generalize known primitive verbs to valid unseen constructions BIBREF2 . Further work with SCAN augmented this set of tests to include compositional generalization based not on known verbs but on known templates BIBREF3 . These template splits included the following (see Figure 9 for examples): Jump around right: All command sequences with the phrase \"jump around right\" are held out of the training set and subsequently tested.",
        "type": "Document"
      },
      {
        "id": "36f42f06-f513-4e92-af5c-5a436e3ee3f7",
        "metadata": {
          "vector_store_key": "1904.09708-0",
          "chunk_id": 32,
          "document_id": "1904.09708",
          "start_idx": 16527,
          "end_idx": 17492
        },
        "page_content": "To test our hypothesis that compositional generalization requires a separation between syntax (i.e. sequential information used for alignment), and semantics (i.e. the mapping from individual source words to individual targets), we conducted two more experiments: Sequential semantics. An additional biLSTM was used to process the semantics of the sentence: $m_j = [\\overrightarrow{m_j};\\overleftarrow{m_j}]$ , where $\\overrightarrow{m_j}$ and $\\overleftarrow{m_j}$ are the vectors produced for the source word $x_j$ by a biLSTM on the forward and backward passes, respectively. These $m_j$ replace those generated by the simple linear layer in the Syntactic Attention model (in equation ( 8 )). Syntax-action. Syntactic information was allowed to directly influence the output at each time step in the decoder: $p(y_i|y_1, y_2, ..., y_{i-1}, \\mathbf {x}) = f([d_i; c_i])$ , where again $f$ is parameterized with a linear function and a softmax output nonlinearity.",
        "type": "Document"
      },
      {
        "id": "2d340e42-6e66-425f-ab9d-c8a5224fc3b9",
        "metadata": {
          "vector_store_key": "1906.00180-3",
          "chunk_id": 9,
          "document_id": "1906.00180",
          "start_idx": 5341,
          "end_idx": 6148
        },
        "page_content": "Their research suggests that recurrent networks only perform well provided that there are no systematic discrepancies between train and test data, whereas human learning is robust with respect to such differences thanks to compositionality. In this section, we report more positive results on compositional reasoning of our Siamese networks. We focus on zero-shot generalization: correct classification of examples of a type that has not been observed before. Provided that atomic constituents and production rules are understood, compositionality does not require that abundantly many instances embodying a semantic category are observed. We will consider in turn what set-up is required to demonstrate zero-shot generalization to unseen lengths, and to generalization to sentences composed of novel words.",
        "type": "Document"
      },
      {
        "id": "c5f575e5-e3c4-436a-be51-f78487f1d8c8",
        "metadata": {
          "vector_store_key": "1909.02635-0",
          "chunk_id": 37,
          "document_id": "1909.02635",
          "start_idx": 21765,
          "end_idx": 22389
        },
        "page_content": "For Recipes, we mainly want to investigate cases of ingredients getting re-engaged in the recipe not in a raw form but in a combined nature with other ingredients and henceforth no explicit mention. For example, eggs in step 4 of Figure FIGREF2a exemplifies this case. The performance in such cases is indicative of how strongly the model can track compositional entities. We also examine the performance for cases where the ingredient is referred by some other name. Formally, we pick the set of examples where the ground truth is a transition from $0 \\rightarrow 1$ (not present to present) and the 1 is a \u201ccombined\u201d case.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much does this system outperform prior work?",
    "llm_answer": "The system achieves new state-of-the-art results on HotpotQA, with large-margin improvements on all metrics, doubling the joint EM and increasing EM by ~8 points and F1 by ~9 points.  It also outperforms a strong feature-rich system for Task A when sufficient training data is available.\n",
    "context": [
      {
        "id": "23976a7a-1b28-442c-baa1-8b2f84b234bb",
        "metadata": {
          "vector_store_key": "1611.00514-0",
          "chunk_id": 30,
          "document_id": "1611.00514",
          "start_idx": 17171,
          "end_idx": 17817
        },
        "page_content": "We evaluated the system on almost 6k out of 24k trials from the other portion to avoid any over-fitting, particularly important for the domain adaptation technique. This resulted in a relative improvement of 11% compared to the primary system in terms of the primary metric. However, the results can be misleading, since the recording condition may be the same for all speakers in the development set. This section reports on the CPU execution time (single threaded), and the amount of memory used to process a single trial, which includes the time for creating models from the enrolment data and the time needed for processing the test segments.",
        "type": "Document"
      },
      {
        "id": "7e3794a8-4c49-4ef6-b670-6e8d7e4dae71",
        "metadata": {
          "vector_store_key": "1905.06566-4",
          "chunk_id": 45,
          "document_id": "1905.06566",
          "start_idx": 25128,
          "end_idx": 25731
        },
        "page_content": "We asked the subjects to rank the outputs of these systems from best to worst. As shown in Table 4 , the output of $\\text{\\sc Hibert}_M$ is selected as the best in 30% of cases and we obtained lower mean rank than all systems except for Human. We also converted the rank numbers into ratings (rank $i$ to $7-i$ ) and applied student $t$ -test on the ratings. $\\text{\\sc Hibert}_M$ is significantly different from all systems in comparison ( $p < 0.05$ ), which indicates our model still lags behind Human, but is better than all other systems. As mentioned earlier, our pre-training includes two stages.",
        "type": "Document"
      },
      {
        "id": "f90822ba-4fa5-42e6-a024-a51338ec33eb",
        "metadata": {
          "vector_store_key": "1909.08041-4",
          "chunk_id": 30,
          "document_id": "1909.08041",
          "start_idx": 16157,
          "end_idx": 16805
        },
        "page_content": "We chose the best system based on the dev set, and used that for submitting private test predictions on both FEVER and HotpotQA . As can be seen in Table TABREF8, with the proposed hierarchical system design, the whole pipeline system achieves new start-of-the-art on HotpotQA with large-margin improvements on all the metrics. More specifically, the biggest improvement comes from the EM for the supporting fact which in turn leads to doubling of the joint EM on previous best results. The scores for answer predictions are also higher than all previous best results with $\\sim $8 absolute points increase on EM and $\\sim $9 absolute points on F1.",
        "type": "Document"
      },
      {
        "id": "c938d47c-e132-4294-bd51-25295fb776d2",
        "metadata": {
          "vector_store_key": "1603.07044-3",
          "chunk_id": 28,
          "document_id": "1603.07044",
          "start_idx": 15079,
          "end_idx": 15744
        },
        "page_content": "Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited training data is given, both feature-rich based system and our system are worse than the IR system. For task C, our system also got comparable results with the feature-rich based system. If we do a simple system combination (average the rank score) between our system and the IR system, the combined system will give large gains on tasks B and C.",
        "type": "Document"
      },
      {
        "id": "874d1312-9fdb-46fe-865b-ce75dcb445d6",
        "metadata": {
          "vector_store_key": "1812.07023-3",
          "chunk_id": 32,
          "document_id": "1812.07023",
          "start_idx": 17666,
          "end_idx": 18428
        },
        "page_content": "We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below: Our primary architectural differences over the baseline model are: not concatenating the question, answer pairs before encoding them, the auxiliary decoder module, and using the Time-Extended FiLM module for feature extraction. These, combined with using scheduled sampling and running hyperparameter optimization over the validation set to select hyperparameters, give us the observed performance boost. We observe that our models generate fairly relevant responses to questions in the dialogues, and models with audio-visual inputs respond to audio-visual questions (e.g. \u201cis there any voices or music ?\u201d) correctly more often.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the baseline systems that are compared against?",
    "llm_answer": "Participant systems, AoA Reader, AS Reader\n",
    "context": [
      {
        "id": "fa88df39-033d-4e0f-891b-162e0f9b0424",
        "metadata": {
          "vector_store_key": "1709.08299-4",
          "chunk_id": 19,
          "document_id": "1709.08299",
          "start_idx": 11042,
          "end_idx": 11836
        },
        "page_content": "The neural baselines are built by the following steps. We first use the shared training data to build a general systems, and choose the best performing model (in terms of cloze validation set) as baseline. Use User Query validation data for further tuning the systems with 10-fold cross-validations. Increase dropout rate BIBREF9 to 0.5 for preventing over-fitting issue. All baseline systems are chosen according to the performance of the validation set. The participant system results are given in Table 2 and 3 . As we can see that two neural baselines are competitive among participant systems and AoA Reader successfully outperform AS Reader and all participant systems in single model condition, which proves that it is a strong baseline system even without further fine-tuning procedure.",
        "type": "Document"
      },
      {
        "id": "348b5dbb-b442-4f57-9f6e-c2038d069875",
        "metadata": {
          "vector_store_key": "1709.08299-4",
          "chunk_id": 16,
          "document_id": "1709.08299",
          "start_idx": 9232,
          "end_idx": 9901
        },
        "page_content": "The paragraph should be read carefully and judged whether appropriate for asking questions No more than 5 questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models.",
        "type": "Document"
      },
      {
        "id": "1aa58b63-7d45-4c30-8497-358be6b0b602",
        "metadata": {
          "vector_store_key": "1804.03396-5",
          "chunk_id": 56,
          "document_id": "1804.03396",
          "start_idx": 29719,
          "end_idx": 30662
        },
        "page_content": "Note that our framework takes the whole document as the input while the baseline systems take the individual sentence as the input, which means the experiment setting is much more difficult for our framework. Finally, we perform a human evaluation on our QA4IE benchmark to verify the reliability of former experiments. The evaluation metrics are as follows: Triple Accuracy is to check whether each ground truth triple is accurate (one cannot find conflicts between the ground truth triple and the corresponding article) because the ground truth triples from WikiData and DBpedia may be incorrect or incomplete. Contextual Consistency is to check whether the context of each answer location is consistent with the corresponding ground truth triple (one can infer from the context to obtain the ground truth triple) because we keep all matched answer locations as ground truths but some of them may be irrelevant with the corresponding triple.",
        "type": "Document"
      },
      {
        "id": "3699d425-11f4-45cc-aae5-6a93d3e32b86",
        "metadata": {
          "vector_store_key": "1707.03904-0",
          "chunk_id": 8,
          "document_id": "1707.03904",
          "start_idx": 4627,
          "end_idx": 5317
        },
        "page_content": "We evaluate Quasar against human testers, as well as several baselines ranging from na\u00efve heuristics to state-of-the-art machine readers. The best performing baselines achieve $33.6\\%$ and $28.5\\%$ on Quasar-S and Quasar-T, while human performance is $50\\%$ and $60.6\\%$ respectively. For the automatic systems, we see an interesting tension between searching and reading accuracies \u2013 retrieving more documents in the search phase leads to a higher coverage of answers, but makes the comprehension task more difficult. We also collect annotations on a subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short.",
        "type": "Document"
      },
      {
        "id": "1c67409f-2f57-4674-ae18-af1d704d9ae8",
        "metadata": {
          "vector_store_key": "1804.08782-1",
          "chunk_id": 24,
          "document_id": "1804.08782",
          "start_idx": 13324,
          "end_idx": 14085
        },
        "page_content": "The experiment is conducted on two datasets: a subset (10%) of Fisher corpus set aside as test data and Suicide corpus. We use a number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 . For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose the best performing one for reporting, thus providing an upper-bound performance. Also, for baseline 2 we choose the session with higher value of the measure as real, since it measures similarity.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What systems are tested?",
    "llm_answer": "Systems developed by 74 participants (final 28), and 43 submitted systems.  Baseline systems, and the framework's system are also tested.\n",
    "context": [
      {
        "id": "949d7969-e0e3-47c4-b0e2-3b20b6a0682c",
        "metadata": {
          "vector_store_key": "1605.08675-3",
          "chunk_id": 8,
          "document_id": "1605.08675",
          "start_idx": 4550,
          "end_idx": 5247
        },
        "page_content": "However, with DeepER, a candidate answer can undergo the same recognition process and be compared to the actual expected entity, not string. Thanks to automatic evaluation vast experiments requiring numerous evaluations may be performed swiftly; saving massive amount of time and human resources. As a test set, authentic questions from a popular Polish quiz TV show are used. Results of experiments, testing (among others) the optimal context length, a number of retrieved documents, a type of entity recognition solution, appear in section SECREF88 . To avoid overfitting, the final system evaluation is executed on a separate test set, previously unused in development, and is checked manually.",
        "type": "Document"
      },
      {
        "id": "8d9f5c38-6bc0-4659-9333-e06a908e8875",
        "metadata": {
          "vector_store_key": "1709.10217-4",
          "chunk_id": 15,
          "document_id": "1709.10217",
          "start_idx": 8466,
          "end_idx": 9077
        },
        "page_content": "If the tester says \u201ctoday\u201d, the systems developed by the participants should understand that he/she indicates the date of April 18, 2017. There are 74 participants who are signing up the evaluation. The final number of participants is 28 and the number of submitted systems is 43. Table TABREF14 and TABREF15 show the evaluation results of the closed test and open test of the task 1 respectively. Due to the space limitation, we only present the top 5 results of task 1. We will add the complete lists of the evaluation results in the version of full paper. Note that for task 2, there are 7 submitted systems.",
        "type": "Document"
      },
      {
        "id": "1c2721da-ef50-4f82-92a2-9f11a92306e0",
        "metadata": {
          "vector_store_key": "2002.05058-2",
          "chunk_id": 49,
          "document_id": "2002.05058",
          "start_idx": 27267,
          "end_idx": 27919
        },
        "page_content": "As described in the human evaluation procedure, we perform 10 runs to test the reliability of each metric when used to perform hyperparameter tuning and early-stopping respectively. In each run, we select the best hyperparameter combination or early-stopping checkpoint based on each of the five compared metrics. Human evaluation is then employed to identify the best choice. We evaluate the performance of each metric by how many times (out of 10) they succeeded in selecting the best hyperparameter combination or early-stopping checkpoint (out of 4) and the average human-annotated score for their selected models. The results are shown in Table 3.",
        "type": "Document"
      },
      {
        "id": "1aa58b63-7d45-4c30-8497-358be6b0b602",
        "metadata": {
          "vector_store_key": "1804.03396-5",
          "chunk_id": 56,
          "document_id": "1804.03396",
          "start_idx": 29719,
          "end_idx": 30662
        },
        "page_content": "Note that our framework takes the whole document as the input while the baseline systems take the individual sentence as the input, which means the experiment setting is much more difficult for our framework. Finally, we perform a human evaluation on our QA4IE benchmark to verify the reliability of former experiments. The evaluation metrics are as follows: Triple Accuracy is to check whether each ground truth triple is accurate (one cannot find conflicts between the ground truth triple and the corresponding article) because the ground truth triples from WikiData and DBpedia may be incorrect or incomplete. Contextual Consistency is to check whether the context of each answer location is consistent with the corresponding ground truth triple (one can infer from the context to obtain the ground truth triple) because we keep all matched answer locations as ground truths but some of them may be irrelevant with the corresponding triple.",
        "type": "Document"
      },
      {
        "id": "cfef221f-ca1d-4bd6-b83f-e80a6a9aa618",
        "metadata": {
          "vector_store_key": "1906.03338-0",
          "chunk_id": 40,
          "document_id": "1906.03338",
          "start_idx": 21733,
          "end_idx": 22448
        },
        "page_content": "These complementary settings will (i) challenge the semantic capacities of a system, and (ii) unveil the extent to which a system is focusing on the discourse context when making decisions. We will offer our testing environments to the research community through a platform that provides datasets and scripts and a table to trace the results of content-based systems. This work has been supported by the German Research Foundation as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) under grant no. GRK 1994/1 and by the Leibniz ScienceCampus \u201cEmpirical Linguistics and Computational Language Modeling\u201d, supported by the Leibniz Association under grant no.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What benchmark datasets they use?",
    "llm_answer": "Airline Travel Information Systems (ATIS), SNIPS Natural Language Understanding, SQuAD, OntoNotes, ACE, MUC 7, Ritter NER, MSM 2013, UMBC Twitter.\n",
    "context": [
      {
        "id": "080dd9ac-808a-4979-9787-23390d67fde8",
        "metadata": {
          "vector_store_key": "2002.06053-9",
          "chunk_id": 76,
          "document_id": "2002.06053",
          "start_idx": 45630,
          "end_idx": 46366
        },
        "page_content": "Comprehensive benchmarking platforms that can assess the success of different tools are still lacking. A benchmarking environment rigorously brings together the suitable data sets and evaluation methodologies in order to provide a fair comparison between the available tools. Such environments are available for molecule generation task from MOSES BIBREF123 and GuacaMol BIBREF124. MoleculeNet is also a similar attempt to build a benchmarking platform for tasks such as prediction of binding affinity and toxicity BIBREF82. Despite the focus on sharing datasets and source codes on popular software development platforms such as GitHub (github.com) or Zenodo (zenodo.org), it is still a challenge to use data or code from other groups.",
        "type": "Document"
      },
      {
        "id": "97c2297b-ef76-4c8e-b026-f576a354ad47",
        "metadata": {
          "vector_store_key": "1909.06937-2",
          "chunk_id": 22,
          "document_id": "1909.06937",
          "start_idx": 13127,
          "end_idx": 13822
        },
        "page_content": "We evaluate our proposed CM-Net on three real-word datasets, and statistics are listed in Table TABREF32. The Airline Travel Information Systems (ATIS) corpus BIBREF12 is the most widely used benchmark for the SLU research. Please note that, there are extra named entity features in the ATIS, which almost determine slot tags. These hand-crafted features are not generally available in open domains BIBREF25, BIBREF29, therefore we train our model purely on the training set without additional hand-crafted features. SNIPS Natural Language Understanding benchmark BIBREF11 is collected in a crowsourced fashion by Snips. The intents of this dataset are more balanced when compared with the ATIS.",
        "type": "Document"
      },
      {
        "id": "59c0f923-f0e2-4368-b5fa-048069bbb974",
        "metadata": {
          "vector_store_key": "1910.12618-1",
          "chunk_id": 39,
          "document_id": "1910.12618",
          "start_idx": 20978,
          "end_idx": 21881
        },
        "page_content": "However the inputs of the numerical benchmark should be hence comparable to the information contained in the weather reports. Considering they mainly contain calendar (day of the week and month) as well as temperature and wind information, the benchmark of comparison is a random forest trained on four features only: the time of the year (whose value is 0 on January the 1st and 1 on December the 31st with a linear growth in between), the day of the week, the national average temperature and wind speed. The metrics of evaluation are the Mean Absolute Percentage Error (MAPE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and the $R^2$ coefficient given by: where $T$ is the number of test samples, $y_t$ and $\\hat{y}_t$ are respectively the ground truth and the prediction for the document of day $t$, and $\\overline{y}$ is the empirical average of the time series over the test sample.",
        "type": "Document"
      },
      {
        "id": "895b9934-11e7-438a-aedf-2435c41cdf73",
        "metadata": {
          "vector_store_key": "1905.08949-8",
          "chunk_id": 17,
          "document_id": "1905.08949",
          "start_idx": 9885,
          "end_idx": 10631
        },
        "page_content": "Table 1 presents a listing of the NQG corpora grouped by their cognitive level and answer type, along with their statistics. Among them, SQuAD was used by most groups as the benchmark to evaluate their NQG models. This provides a fair comparison between different techniques. However, it raises the issue that most NQG models work on factoid questions with answer as text span, leaving other types of QG problems less investigated, such as generating deep multi-choice questions. To overcome this, a wider variety of corpora should be benchmarked against in future NQG research. Although the datasets are commonly shared between QG and QA, it is not the case for evaluation: it is challenging to define a gold standard of proper questions to ask.",
        "type": "Document"
      },
      {
        "id": "fafef8a0-39be-4007-b0e6-a36081a57786",
        "metadata": {
          "vector_store_key": "1701.02877-1",
          "chunk_id": 8,
          "document_id": "1701.02877",
          "start_idx": 3965,
          "end_idx": 4697
        },
        "page_content": "As detailed in Section \"Datasets\" , the corpora studied are OntoNotes BIBREF29 , ACE BIBREF30 , MUC 7 BIBREF31 , the Ritter NER corpus BIBREF21 , the MSM 2013 corpus BIBREF32 , and the UMBC Twitter corpus BIBREF33 . To eliminate potential bias from the choice of statistical NER approach, experiments are carried out with three differently-principled NER approaches, namely Stanford NER BIBREF34 , SENNA BIBREF35 and CRFSuite BIBREF36 (see Section \"NER Models and Features\" for details). Since the goal of this study is to compare NER performance on corpora from diverse domains and genres, seven benchmark NER corpora are included, spanning newswire, broadcast conversation, Web content, and social media (see Table 1 for details).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they combine MonaLog with BERT?",
    "llm_answer": "The hybrid model uses MonaLog's high precision on Entailment and Contradiction.  If MonaLog predicts E or C, it's trusted; otherwise, BERT is used.\n",
    "context": [
      {
        "id": "702ad706-69fa-4e55-9608-0559bddf31ab",
        "metadata": {
          "vector_store_key": "1910.08772-1",
          "chunk_id": 43,
          "document_id": "1910.08772",
          "start_idx": 24468,
          "end_idx": 25125
        },
        "page_content": "Intuitively, we would expect smaller gains since MonaLog already handles a fair amount of the entailments and contradictions, i.e., those cases where BERT profits from more examples. However the experiments show that the hybrid system reaches an even higher accuracy of 87.16%, more than 2 percent points above the baseline, equivalent to roughly 100 more problems correctly solved. Setting the high threshold for BERT to return E or C further improves accuracy to 87.49%. This brings us into the range of the state-of-the-art results, even though a direct comparison is not possible because of the differences between the corrected and uncorrected dataset.",
        "type": "Document"
      },
      {
        "id": "75173ef4-f094-4732-bf63-4010f934b58f",
        "metadata": {
          "vector_store_key": "1910.08772-1",
          "chunk_id": 30,
          "document_id": "1910.08772",
          "start_idx": 17349,
          "end_idx": 17936
        },
        "page_content": "Based on these results, we tested a hybrid model of MonaLog and BERT (see Table TABREF27) where we exploit MonaLog's strength: Since MonaLog has a very high precision on Entailment and Contradiction, we can always trust MonaLog if it predicts E or C; when it returns N, we then fall back to BERT. This hybrid model improves the accuracy of BERT by 1% absolute to 85.95% on the corrected SICK. On the uncorrected SICK dataset, the hybrid system performs worse than BERT. Since MonaLog is optimized for the corrected SICK, it may mislabel many E and C judgments in the uncorrected dataset.",
        "type": "Document"
      },
      {
        "id": "7f7fa242-94b6-43d7-9feb-52979ec05b61",
        "metadata": {
          "vector_store_key": "1910.08772-1",
          "chunk_id": 36,
          "document_id": "1910.08772",
          "start_idx": 20556,
          "end_idx": 21280
        },
        "page_content": "If BERT trained on the manually annotated SICK training data is improved by adding data generated by MonaLog, then we can conclude that the generated data is of high quality, even comparable to human annotated data, which is what we found. More specifically, we compare the performance of BERT models trained on a) SICK training data alone, and b) SICK training data plus the entailing and contradictory pairs generated by MonaLog. All experiments are carried out using our corrected version of the SICK data set. However, note that MonaLog is designed to only generate entailments and contradictions. Thus, we only have access to newly generated examples for those two cases, we do not acquire any additional neutral cases.",
        "type": "Document"
      },
      {
        "id": "bf02fc66-615f-4a1c-abec-f079189bf6bd",
        "metadata": {
          "vector_store_key": "1910.08772-1",
          "chunk_id": 31,
          "document_id": "1910.08772",
          "start_idx": 17936,
          "end_idx": 18712
        },
        "page_content": "Since MonaLog is optimized for the corrected SICK, it may mislabel many E and C judgments in the uncorrected dataset. The stand-alone BERT system performs better on the uncorrected data (86.74%) than the corrected set (85.00%). The corrected set may be too inconsistent since only a part has been checked. Overall, these hybird results show that it is possible to combine our high-precision system with deep learning architectures. However, more work is necessary to optimize this combined system. Upon closer inspection, some of MonaLog's errors consist of difficult cases, as shown in Table TABREF29. For example, in ex. 359, if our knowledge base ${K}$ contains the background fact $\\mbox{\\em chasing} \\le \\mbox{\\em running}$, then MonaLog's judgment of C would be correct.",
        "type": "Document"
      },
      {
        "id": "5d350687-9401-46e8-91cb-57ec6c1a2886",
        "metadata": {
          "vector_store_key": "1910.08772-1",
          "chunk_id": 42,
          "document_id": "1910.08772",
          "start_idx": 23769,
          "end_idx": 24468
        },
        "page_content": "This demonstrates both the robustness of BERT in the current experiment and the usefulness of the generated data. The more data we add, the better the system performs. We also see that raising the threshold to relabel uncertain cases as neutral gives a small boost, from 86.51% to 86.71%. This translates into 10 cases where the relabeling corrected the answer. Finally, we also investigated whether the hybrid system, i.e., MonaLog followed by the re-trained BERT, can also profit from the additional training data. Intuitively, we would expect smaller gains since MonaLog already handles a fair amount of the entailments and contradictions, i.e., those cases where BERT profits from more examples.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they select monotonicity facts?",
    "llm_answer": "They collected examples from the GLUE diagnostic dataset, FraCaS, and 11 linguistics publications.  They reclassified problems from GLUE using their own criteria, and crowdsourced and collected data from linguistics publications.\n",
    "context": [
      {
        "id": "38aa62a2-82b1-4f1a-bcc8-b9adde59dc24",
        "metadata": {
          "vector_store_key": "1906.06448-1",
          "chunk_id": 25,
          "document_id": "1906.06448",
          "start_idx": 14187,
          "end_idx": 14919
        },
        "page_content": "Regarding previous manually-curated datasets, we collected 93 examples for monotonicity reasoning from the GLUE diagnostic dataset, and 37 single-premise problems from FraCaS. Both the GLUE diagnostic dataset and FraCaS categorize problems by their types of monotonicity reasoning, but we found that each dataset has different classification criteria. Thus, following GLUE, we reclassified problems into three types of monotone reasoning (upward, downward, and non-monotone) by checking if they include (i) the target monotonicity operator in both the premise and the hypothesis and (ii) the phrase replacement in its argument position. In the GLUE diagnostic dataset, there are several problems whose gold labels are contradiction.",
        "type": "Document"
      },
      {
        "id": "46492af0-0d29-489e-add5-6fca39d3ebd9",
        "metadata": {
          "vector_store_key": "1906.06448-1",
          "chunk_id": 24,
          "document_id": "1906.06448",
          "start_idx": 13803,
          "end_idx": 14655
        },
        "page_content": "We also collect monotonicity inference problems from previous manually curated datasets and linguistics publications. The motivation is that previous linguistics publications related to monotonicity reasoning are expected to contain well-designed inference problems, which might be challenging problems for NLI models. We collected 1,184 examples from 11 linguistics publications BIBREF19 , BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26 , BIBREF27 , BIBREF28 , BIBREF29 . Regarding previous manually-curated datasets, we collected 93 examples for monotonicity reasoning from the GLUE diagnostic dataset, and 37 single-premise problems from FraCaS. Both the GLUE diagnostic dataset and FraCaS categorize problems by their types of monotonicity reasoning, but we found that each dataset has different classification criteria.",
        "type": "Document"
      },
      {
        "id": "c73aed8b-d9d7-4405-ac4a-c2f49e4f7fb6",
        "metadata": {
          "vector_store_key": "1906.06448-1",
          "chunk_id": 3,
          "document_id": "1906.06448",
          "start_idx": 1838,
          "end_idx": 2560
        },
        "page_content": "( \"Introduction\" )), as witness the fact that ( \"Introduction\" ) entails ( \"Introduction\" ). To properly handle both directions of monotonicity, NLI models must detect monotonicity operators (e.g., all, not) and their arguments from the syntactic structure. For previous datasets containing monotonicity inference problems, FraCaS BIBREF8 and the GLUE diagnostic dataset BIBREF9 are manually-curated datasets for testing a wide range of linguistic phenomena. However, monotonicity problems are limited to very small sizes (FraCaS: 37/346 examples and GLUE: 93/1650 examples). The limited syntactic patterns and vocabularies in previous test sets are obstacles in accurately evaluating NLI models on monotonicity reasoning.",
        "type": "Document"
      },
      {
        "id": "419942d8-1555-4b16-b06e-02ed1e86040c",
        "metadata": {
          "vector_store_key": "1906.06448-1",
          "chunk_id": 4,
          "document_id": "1906.06448",
          "start_idx": 2342,
          "end_idx": 3195
        },
        "page_content": "The limited syntactic patterns and vocabularies in previous test sets are obstacles in accurately evaluating NLI models on monotonicity reasoning. To tackle this issue, we present a new evaluation dataset that covers a wide range of monotonicity reasoning that was created by crowdsourcing and collected from linguistics publications (Section \"Dataset\" ). Compared with manual or automatic construction, we can collect naturally-occurring examples by crowdsourcing and well-designed ones from linguistics publications. To enable the evaluation of skills required for monotonicity reasoning, we annotate each example in our dataset with linguistic tags associated with monotonicity reasoning. We measure the performance of state-of-the-art NLI models on monotonicity reasoning and investigate their generalization ability in upward and downward reasoning",
        "type": "Document"
      },
      {
        "id": "60776a84-ac83-4607-8069-5ccba408d1f9",
        "metadata": {
          "vector_store_key": "1910.08772-2",
          "chunk_id": 6,
          "document_id": "1910.08772",
          "start_idx": 3338,
          "end_idx": 4151
        },
        "page_content": "To our knowledge, our approach is the first attempt to use monotonicity for data augmentation, and we show that such augmentation can generate high-quality training data with which models like BERT can improve performance. The goal of NLI is to determine, given a premise set $P$ and a hypothesis sentence $H$, whether $H$ follows from the meaning of $P$ BIBREF21. In this paper, we look at single-premise problems that involve making a standard 3-way classification decision (i.e., Entailment (H), Contradict (C) and Neutral (N)). Our general monotonicity reasoning system works according to the pipeline in Figure FIGREF1. Given a premise text, we first do Arrow Tagging by assigning polarity annotations (i.e., the arrows $\\uparrow ,\\downarrow $, which are the basic primitives of our logic) to tokens in text.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the 12 categories devised?",
    "llm_answer": "Activity/sports recognition, sentiment understanding, positional reasoning, subordinate object recognition, and Appreciation, Call for Action, Issue, Blaming.\n",
    "context": [
      {
        "id": "3d614b27-7069-49fc-9a72-55fd62fcc017",
        "metadata": {
          "vector_store_key": "1703.09684-8",
          "chunk_id": 19,
          "document_id": "1703.09684",
          "start_idx": 10950,
          "end_idx": 11703
        },
        "page_content": "Questions were classified as activity or sports recognition questions if the answer was one of nine common sports or one of fifteen common activities and the question contained common verbs describing actions or sports, e.g., playing, throwing, etc. For counting, the question had to begin with `How many' and the answer had to be a small countable integer (1-16). The other categories were determined using regular expressions. For example, a question of the form `Are feeling ?' was classified as sentiment understanding and `What is to the right of/left of/ behind the ?' was classified as positional reasoning. Similarly, `What <OBJECT CATEGORY> is in the image?' and similar templates were used to populate subordinate object recognition questions.",
        "type": "Document"
      },
      {
        "id": "0ed71a6b-a1aa-4a60-a95d-09b3534a2593",
        "metadata": {
          "vector_store_key": "1707.02377-4",
          "chunk_id": 35,
          "document_id": "1707.02377",
          "start_idx": 20784,
          "end_idx": 21482
        },
        "page_content": "The 100 categories includes categories under sports, entertainment, literature, and politics etc. Examples of categories include American drama films, Directorial debut films, Major League Baseball pitchers and Sydney Swans players. Body texts (the second paragraph) were extracted for each page as a document. For each category, we select 1,000 documents with unique category label, and 100 documents were used for training and 900 documents for testing. The remaining documents are used as unlabeled data. The 100 classes are balanced in the training and testing sets. For this data set, we learn the word embedding and document representation for all the algorithms using all the available data.",
        "type": "Document"
      },
      {
        "id": "c9f7abd2-294f-4edf-877c-d51901723181",
        "metadata": {
          "vector_store_key": "1701.02877-5",
          "chunk_id": 90,
          "document_id": "1701.02877",
          "start_idx": 45887,
          "end_idx": 46595
        },
        "page_content": "This has some overlap with ORG, but also includes terms such as \u201cmuslims\" and \u201cDanes\", which are too broad for the ACE-related definition of ORGANIZATION. Full details can be found in the OntoNotes 5.0 release notes and the (brief) CoNLL 2003 annotation categories. Notice how the CoNLL guidelines are much more terse, being generally non-prose, but also manage to cram in fairly comprehensive lists of sub-kinds of entities in each case. This is likely to make the CoNLL classes include a diverse range of entities, with the many suggestions acting as generative material for the annotator, and therefore providing a broader range of annotations from which to generalise from \u2013 i.e., slightly easier to tag.",
        "type": "Document"
      },
      {
        "id": "2ccd98df-7d21-47a7-8155-8f56101e9dc1",
        "metadata": {
          "vector_store_key": "2002.08899-4",
          "chunk_id": 30,
          "document_id": "2002.08899",
          "start_idx": 15529,
          "end_idx": 16346
        },
        "page_content": "They all highlight the relevant terminal, and syntactic categories that are usually associated with that word. The associated categories typically are either those of that word, the phrases headed by the category of that word, or those that select or are selected by that word. The relevant nonterminal terminology is as follows BIBREF6: \u201c(in\u201d is a preposition or subordinating conjunction, \u201c(np\u201d is a noun phrase, \u201c(pp\u201d is a prepositional phrase, \u201c(np-subj\u201d is a noun phrase with a surface subject marking, \u201c(vp\u201d is a verb phrase, \u201c(vbn\u201d is a verb in the past participle, \u201c(adjp\u201d is an adjective phrase, \u201c(vbp\u201d is a non-3rd person singular present verb, \u201c(prp\u201d is a personal pronoun, \u201c(rb\u201d is an adverb, \u201c(sq\u201d is the main clause of a wh-question, or it indicates an inverted yes or no question, and \u201c(s\u201d is the root.",
        "type": "Document"
      },
      {
        "id": "09928a4a-176a-4ee4-af13-7734d83824e7",
        "metadata": {
          "vector_store_key": "1808.06834-2",
          "chunk_id": 4,
          "document_id": "1808.06834",
          "start_idx": 2250,
          "end_idx": 2981
        },
        "page_content": "After investigating and highlighting statements with different intentions we came up with a minimal set of 4 mutually exclusive categories with different degrees of correlation with the traditional two polar categories (for and against). It is observed that any statement by a participating member will fall into one of these categories namely - Appreciation, Call for Action, Issue, Blaming. For example, if the debate consists of more of issues, one can infer that the bill is not serving the its purpose in a well manner. Also, this preliminary step will lead to new areas of research such as detection of appreciation, blame in similar lines of argument mining which is evolving in the recent years in the field of linguistics.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what are the off-the-shelf systems discussed in the paper?",
    "llm_answer": "Random Guess, Top Frequency, AS Reader.  i-vector system and x-vector system.\n",
    "context": [
      {
        "id": "9c8055d5-e46a-4a0d-9b61-c86e88e414ff",
        "metadata": {
          "vector_store_key": "1701.09123-1",
          "chunk_id": 38,
          "document_id": "1701.09123",
          "start_idx": 20744,
          "end_idx": 21535
        },
        "page_content": "Their baseline system combined with their phrase embeddings trained with infused lexicons allow them to report the best CoNLL 2003 result so far. The best system of the GermEval 2014 task built an ensemble of classifiers and pattern extractors to find the most likely tag sequence BIBREF48 . They paid special attention to out of vocabulary words which are addressed by semi-supervised word representation features and an ensemble of POS taggers. Furthermore, remaining unknown candidate mentions are tackled by look-up via the Wikipedia API. Apart from the feature types, the last two columns of Table TABREF13 refer to whether the systems are publicly available and whether any external resources used for training are made available (e.g., induced word embeddings, gazetteers or corpora).",
        "type": "Document"
      },
      {
        "id": "89dab844-9bb9-4fec-a502-667adab9a945",
        "metadata": {
          "vector_store_key": "1709.08299-0",
          "chunk_id": 17,
          "document_id": "1709.08299",
          "start_idx": 9901,
          "end_idx": 10685
        },
        "page_content": "In this paper, we provide four baseline systems, including two simple ones and two neural network models. The details of the baseline systems are illustrated as follows. Random Guess: In this baseline, we randomly choose one word in the document as the answer. Top Frequency: We choose the most frequent word in the document as the answer. AS Reader: We implemented Attention Sum Reader (AS Reader) BIBREF3 for modeling document and query and predicting the answer with the Pointer Network BIBREF7 , which is a popular framework for cloze-style reading comprehension. Apart from setting embedding and hidden layer size as 256, we did not change other hyper-parameters and experimental setups as used in Kadlec et al. kadlec-etal-2016, nor we tuned the system for further improvements.",
        "type": "Document"
      },
      {
        "id": "c93f2257-9006-466a-af1f-9340e6b4ab7d",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 28,
          "document_id": "2001.00137",
          "start_idx": 16720,
          "end_idx": 17443
        },
        "page_content": "Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector Classifier with L1 and L2 penalty and tolerance of $10^{-3}$; Regularized linear classifier with Stochastic Gradient Descent (SGD) learning with regularization term $alpha=10^{-4}$ and L1, L2 and Elastic-Net penalty; Nearest Centroid with Euclidian metric, where classification is done by representing each class with a centroid;",
        "type": "Document"
      },
      {
        "id": "904a361a-2d3c-41fa-8807-2303b37ca27b",
        "metadata": {
          "vector_store_key": "1603.04553-3",
          "chunk_id": 21,
          "document_id": "1603.04553",
          "start_idx": 11450,
          "end_idx": 12153
        },
        "page_content": "To make a thorough empirical comparison with previous studies, Table 3 (below the dashed line) also shows the results of some state-of-the-art supervised coreference resolution systems \u2014 IMS: the second best system in the CoNLL 2012 shared task BIBREF28 ; Latent-Tree: the latent tree model BIBREF29 obtaining the best results in the shared task; Berkeley: the Berkeley system with the final feature set BIBREF12 ; LaSO: the structured perceptron system with non-local features BIBREF30 ; Latent-Strc: the latent structure system BIBREF31 ; Model-Stack: the entity-centric system with model stacking BIBREF32 ; and Non-Linear: the non-linear mention-ranking model with feature representations BIBREF33 .",
        "type": "Document"
      },
      {
        "id": "02bef31f-a2b5-4b89-8063-62dc0b0f91eb",
        "metadata": {
          "vector_store_key": "1911.01799-4",
          "chunk_id": 19,
          "document_id": "1911.01799",
          "start_idx": 11188,
          "end_idx": 11946
        },
        "page_content": "Two state-of-the-art baseline systems were built following the Kaldi SITW recipe BIBREF22: an i-vector system BIBREF3 and an x-vector system BIBREF10. For the i-vector system, the acoustic feature involved 24-dimensional MFCCs plus the log energy, augmented by the first- and second-order derivatives. We also applied the cepstral mean normalization (CMN) and the energy-based voice active detection (VAD). The universal background model (UBM) consisted of $2,048$ Gaussian components, and the dimensionality of the i-vector space was 400. LDA was applied to reduce the dimensionality of the i-vectors to 150. The PLDA model was used for scoring BIBREF4. For the x-vector system, the feature-learning component was a 5-layer time-delay neural network (TDNN).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many rules had to be defined?",
    "llm_answer": "5\n",
    "context": [
      {
        "id": "15ece6da-9949-4373-9ade-cf972243007b",
        "metadata": {
          "vector_store_key": "1706.08032-4",
          "chunk_id": 14,
          "document_id": "1706.08032",
          "start_idx": 8279,
          "end_idx": 8929
        },
        "page_content": "We use five rules in the semantic rules set because other five rules are only used to compute polarity of words after POS tagging or Parsing steps. We follow the same naming convention for rules utilized by BIBREF11 to represent the rules utilized in our proposed method. The rules utilized in the proposed method are displayed in Table TABREF15 in which is included examples from STS Corpus and output after using the rules. Table TABREF16 illustrates the number of processed sentences on each dataset. To construct embedding inputs for our model, we use a fixed-sized word vocabulary INLINEFORM0 and a fixed-sized character vocabulary INLINEFORM1 .",
        "type": "Document"
      },
      {
        "id": "63eed496-733f-4d2b-ae24-bd2cd43a0168",
        "metadata": {
          "vector_store_key": "1905.07471-2",
          "chunk_id": 12,
          "document_id": "1905.07471",
          "start_idx": 6600,
          "end_idx": 7241
        },
        "page_content": "Each rule is on average forty lines of code that traverses a dependency parse tree according to its pre-specified pattern, extracting the matching spans at each step. A master function parse() determines which rule to apply based on the question type which is categorized by nsubj presence, and the type of question (who/what/etc.). Most questions contain an nsubj which makes the parse task easier, as this will also be the subject of the tuple. We allow the master parse() method try multiple rules. It first tries very specific rules (e.g. a parser for how questions where no subject is identified), then falls down to more generic rules.",
        "type": "Document"
      },
      {
        "id": "90bdeca7-bcc9-4521-81f7-af4c4bbdac6f",
        "metadata": {
          "vector_store_key": "1909.05438-3",
          "chunk_id": 2,
          "document_id": "1909.05438",
          "start_idx": 1012,
          "end_idx": 1663
        },
        "page_content": "Our key idea is to use these rules to collect modest question-programs pairs as the starting point, and then leverage automatically generated examples to improve the accuracy and generality of the model. This presents three challenges including how to generate examples in an efficient way, how to measure the quality of generated examples which might contain errors and noise, and how to train a semantic parser that makes robust predictions for examples covered by rules and generalizes well to uncovered examples. We address the aforementioned challenges with a framework consisting of three key components. The first component is a data generator.",
        "type": "Document"
      },
      {
        "id": "3e7fb4ea-9a6c-47ff-9640-0a4001d28f4d",
        "metadata": {
          "vector_store_key": "1701.09123-5",
          "chunk_id": 20,
          "document_id": "1701.09123",
          "start_idx": 10534,
          "end_idx": 11256
        },
        "page_content": "Using such framework, research rapidly evolved from rule-based approaches (consisting of manually handcrafted rules) to language independent systems focused on learning supervised statistical models. Thus, while in the MUC 6 competition 5 out of 8 systems were rule-based, in CoNLL 2003 16 teams participated in the English task all using statistical-based NERC BIBREF39 . Table TABREF10 describes the 12 datasets used in this paper. The first half lists the corpora used for in-domain evaluation whereas the lower half contains the out-of-domain datasets. The CoNLL NER shared tasks focused on language independent machine learning approaches for 4 entity types: person, location, organization and miscellaneous entities.",
        "type": "Document"
      },
      {
        "id": "c630b6be-cc52-4297-bad8-134c4951c3a8",
        "metadata": {
          "vector_store_key": "1909.12140-3",
          "chunk_id": 3,
          "document_id": "1909.12140",
          "start_idx": 2151,
          "end_idx": 2767
        },
        "page_content": "It takes a sentence as input and performs a recursive transformation process that is based upon a small set of 35 hand-crafted grammar rules for the English version and 29 rules for the German approach. These patterns were heuristically determined in a comprehensive linguistic analysis and encode syntactic and lexical features that can be derived from a sentence's parse tree. Each rule specifies (1) how to split up and rephrase the input into structurally simplified sentences and (2) how to set up a semantic hierarchy between them. They are recursively applied on a given source sentence in a top-down fashion.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was performance of classifiers before/after using distant supervision?",
    "llm_answer": "Distant supervision *improved* classifier performance in some cases, but also *deteriorated* performance in other cases, particularly when the model was already trained on a good amount of clean data.  The improvement was more pronounced in the low-resource languages and with smaller models.\n",
    "context": [
      {
        "id": "cfccef6f-0f68-4488-aaa5-2fd3b073ad51",
        "metadata": {
          "vector_store_key": "2003.08370-3",
          "chunk_id": 28,
          "document_id": "2003.08370",
          "start_idx": 15607,
          "end_idx": 16244
        },
        "page_content": "Still, the F1-score also drops nearly half for the BERT model in the 1k setting compared to the full dataset. For 1k and 2k labeled data, the distant supervision helps to improve the model's performance. However, once the model trained only on clean data reaches a higher F1-score than the distant supervision technique, the model trained on clean and distantly-supervised data deteriorates. This suggests that the BERT model overfits too much on the noise in the distant supervision. In this study, we analysed distant supervision techniques and label-noise handling for NER in Hausa and Yor\u00f9b\u00e1, two languages from developing countries.",
        "type": "Document"
      },
      {
        "id": "035ebec7-671b-408b-ac26-c57b9d390421",
        "metadata": {
          "vector_store_key": "2003.08370-3",
          "chunk_id": 24,
          "document_id": "2003.08370",
          "start_idx": 13135,
          "end_idx": 13788
        },
        "page_content": "The results for Hausa are given in Table TABREF14. Training with a mix of 50% clean and 50% distantly-supervised data performs 15 F1-score points below using the whole 100% clean data which is to be expected due to the lower quality of the distantly-supervised labels. Using the Noise Channel closes half of this gap. Due to the limited availability of the dataset, we could unfortunately not investigate this further, but it shows already the benefits that are possible through noise-handling. An evaluation of the distant supervision for Yor\u00f9b\u00e1 is given in Table TABREF14. The quality of the automatically annotated labels differs between the classes.",
        "type": "Document"
      },
      {
        "id": "6420545b-a6e8-4d82-89f5-cdfabece37ac",
        "metadata": {
          "vector_store_key": "2003.08370-3",
          "chunk_id": 29,
          "document_id": "2003.08370",
          "start_idx": 16244,
          "end_idx": 16988
        },
        "page_content": "In this study, we analysed distant supervision techniques and label-noise handling for NER in Hausa and Yor\u00f9b\u00e1, two languages from developing countries. We showed that they can be successfully leveraged in a realistic low-resource scenario to double a classifier's performance. If model size is not a constraint, the more complex BERT model clearly outperforms the smaller Bi-LSTM architecture. Nevertheless, there is still a large gap between the best performing model on Yor\u00f9b\u00e1 with 66 F1-score and the state-of-the-art in English around 90. We see several interesting follow-ups to these evaluations. In the future, we want to evaluate if noise handling methods can also allow the more complex BERT model to benefit from distant supervision.",
        "type": "Document"
      },
      {
        "id": "805ce3a7-161e-4eab-9177-3411eba6fb51",
        "metadata": {
          "vector_store_key": "2003.08370-2",
          "chunk_id": 3,
          "document_id": "2003.08370",
          "start_idx": 1861,
          "end_idx": 2551
        },
        "page_content": "In this work, we study two methods of distant supervision for NER: Automatic annotation rules and matching of lists of entities from an external knowledge source. While distant supervision has been successfully used for high resource languages, it is not straight forward that these also work in low-resource settings where the amount of available external information might be much lower. The knowledge graph of Wikidata e.g. contains 4 million person names in English while only 32 thousand such names are available in Yor\u00f9b\u00e1, many of which are Western names. Orthogonally to distant supervision, the pre-training of word embeddings is a key component for training many neural NLP models.",
        "type": "Document"
      },
      {
        "id": "1f1764b7-12fa-4398-a677-341b312b04fd",
        "metadata": {
          "vector_store_key": "2004.02393-3",
          "chunk_id": 23,
          "document_id": "2004.02393",
          "start_idx": 12877,
          "end_idx": 13573
        },
        "page_content": "First, training a ranker with distant supervision performs significantly better than the random baseline, showing that the training process itself has a certain degree of denoising ability to distinguish the more informative signals from distant supervision labels. By introducing additional inductive bias of orders, the conditional selection model further improves with a large margin. Finally, our cooperative game gives the best performance, showing that a trained Reasoner has the ability of ignoring entity links that are irrelevant to the reasoning chain. Table TABREF22 demonstrates the effect of selecting directions, together with the methods' recall on head passages and tail passages.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big are the datasets used?",
    "llm_answer": "The provided text mentions various datasets with sizes like 2,557 days, 2,922 days, 730 days, 19 novels, 50 authors, 250 novels, 5,000 images, 15,000 images, 75,000 questions.  More sizes are referenced, but not provided in full detail.\n",
    "context": [
      {
        "id": "9ca5340d-587b-4e94-bcbe-160570788673",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 35,
          "document_id": "1910.12618",
          "start_idx": 19294,
          "end_idx": 19871
        },
        "page_content": "For instance for random forests it can correspond to the maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014 and 2015 serve as validation set (730 days).",
        "type": "Document"
      },
      {
        "id": "6d614157-50dc-4aaa-af03-adc7f0282a12",
        "metadata": {
          "vector_store_key": "1709.02271-3",
          "chunk_id": 20,
          "document_id": "1709.02271",
          "start_idx": 11344,
          "end_idx": 11969
        },
        "page_content": "(Section SECREF26 ). The statistics for the three datasets used in the experiments are summarized in Table TABREF16 . novel-9. This dataset was compiled by F&H14: a collection of 19 novels by 9 nineteenth century British and American authors in the Project Gutenberg. To compare to F&H14, we apply the same resampling method (F&H14, Section 4.2) to correct the imbalance in authors by oversampling the texts of less-represented authors. novel-50. This dataset extends novel-9, compiling the works of 50 randomly selected authors of the same period. For each author, we randomly select 5 novels for a total 250 novels. IMDB62.",
        "type": "Document"
      },
      {
        "id": "1e2bf238-b0a0-40af-9d8d-e6fea4ad79d0",
        "metadata": {
          "vector_store_key": "1910.09399-3",
          "chunk_id": 105,
          "document_id": "1910.09399",
          "start_idx": 56273,
          "end_idx": 57009
        },
        "page_content": "For example, image data captured from a variety of photo-ready devices, such as smart-phones, and online social media services opened the door to the analysis of large amounts of media datasets BIBREF70. The availability of large media datasets allow new frameworks and algorithms to be proposed and tested on real-world data. A summary of some reviewed methods and benchmark datasets used for validation is reported in Table TABREF43. In addition, the performance of different GANs with respect to the benchmark datasets and performance metrics is reported in Table TABREF48. In order to synthesize images from text descriptions, many frameworks have taken a minimalistic approach by creating small and background-less images BIBREF73.",
        "type": "Document"
      },
      {
        "id": "0fcc020b-3016-4745-902c-d73226505bf1",
        "metadata": {
          "vector_store_key": "1910.11949-3",
          "chunk_id": 17,
          "document_id": "1910.11949",
          "start_idx": 9318,
          "end_idx": 10010
        },
        "page_content": "Each source contains 5,000 images with 5 questions per image, adding a total of 15,000 images with 75,000 questions. COCO dataset includes images of complex everyday scenes containing common objects in their natural context, but it is limited in terms of the concepts it covers. Bing dataset contains more event related questions and has a wider range of questions longitudes (between 3 and 20 words), while Flickr questions are shorter (less than 6 words) and the images appear to be more casual. We use two datasets to train our chatbot model. The first one is the Persona-chat BIBREF15 which contains dialogues between two people with different profiles that are trying to know each other.",
        "type": "Document"
      },
      {
        "id": "fdf582fb-3fd0-4e0a-9bba-39ff6f6c125d",
        "metadata": {
          "vector_store_key": "1909.06522-0",
          "chunk_id": 22,
          "document_id": "1909.06522",
          "start_idx": 11687,
          "end_idx": 12449
        },
        "page_content": "For each language, the train and test set size are described in Table TABREF10, and most training data were Pages. On each language we also had a small validation set for model parameter tuning. Each monolingual ASR baseline was trained on language-specific data only. The character sets of these 7 languages have little overlap except that (i) they all include common basic Latin alphabet, and (ii) both Hindi and Marathi use Devanagari script. We took the union of 7 character sets therein as the multilingual grapheme set (Section SECREF2), which contained 432 characters. In addition, we deliberately split 7 languages into two groups, such that the languages within each group were more closely related in terms of language family, orthography or phonology.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What datasets are used for training/testing models? ",
    "llm_answer": "MS COCO, Bing, Flickr (question generation); AIDA-Train, Wikipedia (entity linking); CUB, MSCOCO, Oxford-102 (text-to-image synthesis);  BIBREF4 small diagnostic, Geoquery, Wall Street Journal, Tatoeba (other tasks).  AIDA-B, other datasets (evaluation).\n",
    "context": [
      {
        "id": "947a5cf6-1656-4ca3-827b-105c8344c1d6",
        "metadata": {
          "vector_store_key": "1910.11949-3",
          "chunk_id": 16,
          "document_id": "1910.11949",
          "start_idx": 8700,
          "end_idx": 9318
        },
        "page_content": "In particular, we use two types of datasets to train our models: A dataset that maps pictures with questions, and an open-domain conversation dataset. The details of the two datasets are as follows. We use MS COCO, Bing and Flickr datasets from BIBREF26 to train the model that generates questions. These datasets contain natural questions about images with the purpose of knowing more about the picture. As can be seen in the Figure FIGREF8, questions cannot be answered by only looking at the image. Each source contains 5,000 images with 5 questions per image, adding a total of 15,000 images with 75,000 questions.",
        "type": "Document"
      },
      {
        "id": "458b51fc-c0d4-482d-90b3-d4cefed66ac3",
        "metadata": {
          "vector_store_key": "1902.00330-3",
          "chunk_id": 37,
          "document_id": "1902.00330",
          "start_idx": 20853,
          "end_idx": 21565
        },
        "page_content": "We implement our models in Tensorflow and run experiments on 4 Tesla V100 GPU. We conduct experiments on several different types of public datasets including news and encyclopedia corpus. The training set is AIDA-Train and Wikipedia datasets, where AIDA-Train contains 18448 mentions and Wikipedia contains 25995 mentions. In order to compare with the previous methods, we evaluate our model on AIDA-B and other datasets. These datasets are well-known and have been used for the evaluation of most entity linking systems. The statistics of the datasets are shown in Table 1. AIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.",
        "type": "Document"
      },
      {
        "id": "eac31fee-fbdc-4a80-be44-f2145c81562a",
        "metadata": {
          "vector_store_key": "1910.09399-3",
          "chunk_id": 106,
          "document_id": "1910.09399",
          "start_idx": 56647,
          "end_idx": 57443
        },
        "page_content": "In order to synthesize images from text descriptions, many frameworks have taken a minimalistic approach by creating small and background-less images BIBREF73. In most cases, the experiments were conducted on simple datasets, initially containing images of birds and flowers. BIBREF8 contributed to these data sets by adding corresponding natural language text descriptions to subsets of the CUB, MSCOCO, and Oxford-102 datasets, which facilitated the work on text-to-image synthesis for several papers released more recently. While most deep learning algorithms use MNIST BIBREF74 dataset as the benchmark, there are three main datasets that are commonly used for evaluation of proposed GAN models for text-to-image synthesis: CUB BIBREF75, Oxford BIBREF76, COCO BIBREF77, and CIFAR-10 BIBREF78.",
        "type": "Document"
      },
      {
        "id": "c282a797-7698-46e0-b5da-692be2bae246",
        "metadata": {
          "vector_store_key": "1804.08186-2",
          "chunk_id": 294,
          "document_id": "1804.08186",
          "start_idx": 167030,
          "end_idx": 167629
        },
        "page_content": "The types of models as well as the sources of training data used in the literature are diverse, and work to date has not compared and evaluated these in a systematic manner, making it difficult to draw broader conclusions about what the \u201cbest\u201d method for actually is. We have attempted to synthesize results to date to identify a set of \u201cbest practices\u201d, but these should be treated as guidelines and should always be considered in the broader context of a target application. Existing work on serves to illustrate that the scope and depth of the problem are much greater than they may first appear.",
        "type": "Document"
      },
      {
        "id": "e90ad604-3846-4081-883b-cd98ccc50234",
        "metadata": {
          "vector_store_key": "2002.08899-1",
          "chunk_id": 14,
          "document_id": "2002.08899",
          "start_idx": 7425,
          "end_idx": 8096
        },
        "page_content": "We used BIBREF4's BIBREF4 small diagnostic, the Geoquery semantic parsing dataset BIBREF5, the Wall Street Journal syntactic parsing dataset of sentences up to length 10 BIBREF6, and the Tatoeba BIBREF7 English to Chinese translation dataset processed by BIBREF8 BIBREF8. To avoid the biases that can be introduced with hyperparameter tuning, we used the same hyperparameters with every model on every domain. These were chosen arbitrarily and kept after they enabled all models to reach a similar train accuracy (typically, close to 100 percent) and after they enabled all models to achieve a peak validation performance and then gradually yield worse validation scores.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How better is gCAS approach compared to other approaches?",
    "llm_answer": "gCAS outperforms other models in act prediction, frame prediction, entity F$_1$, and Success F$_1$ in most domains, especially in movie and restaurant domains.  It handles multi-label slots easily, making it better at generating shorter, more accurate sequences.\n",
    "context": [
      {
        "id": "922bd76c-6d67-4357-a3e8-2ee216e92600",
        "metadata": {
          "vector_store_key": "1908.11546-1",
          "chunk_id": 20,
          "document_id": "1908.11546",
          "start_idx": 10297,
          "end_idx": 10936
        },
        "page_content": "The advantage of gCAS in the restaurant domain is much more evident: the agent's inform act usually has multiple slots (see example 2 in Table TABREF15) and this makes classification and sequence generation harder, but gCAS multi-label slots decoder handles it easily. Table TABREF14 shows the turn-level acts and frame prediction performance. CAS and gCAS outperform all other models in acts prediction in terms of F$_1$ score. The main reason is that CAS and gCAS output a tuple at each recurrent step, which makes for shorter sequences that are easier to generate compared to the long sequences of Seq2Seq (example 2 in Table TABREF15).",
        "type": "Document"
      },
      {
        "id": "08dd5bc7-63c5-4e3b-9a85-e74cdbc09541",
        "metadata": {
          "vector_store_key": "1908.11546-1",
          "chunk_id": 21,
          "document_id": "1908.11546",
          "start_idx": 10936,
          "end_idx": 11548
        },
        "page_content": "The main reason is that CAS and gCAS output a tuple at each recurrent step, which makes for shorter sequences that are easier to generate compared to the long sequences of Seq2Seq (example 2 in Table TABREF15). The classification method has a good precision score, but a lower recall score, suggesting it has problems making granular decisions (example 2 in Table TABREF15). At the frame level, gCAS still outperforms all other methods. The performance difference between CAS and gCAS on frames becomes much more evident, suggesting that gCAS is more capable of predicting slots that are consistent with the act.",
        "type": "Document"
      },
      {
        "id": "8c221898-e7bf-4d6b-bd3e-c72df4b292be",
        "metadata": {
          "vector_store_key": "1908.11546-1",
          "chunk_id": 19,
          "document_id": "1908.11546",
          "start_idx": 10057,
          "end_idx": 10881
        },
        "page_content": "The reason is that in the movie domain the proportion of turns with multiple acts is higher (52%), while in the other two domains it is lower (30%). gCAS also outperforms all other models in terms of Success F$_1$ in the movie and restaurant domain but is outperformed by the classification model in the taxi domain. The reason is that in the taxi domain, the agent usually informs the user at the last turn, while in all previous turns the agent usually requests information from the user. It is easy for the classification model to overfit this pattern. The advantage of gCAS in the restaurant domain is much more evident: the agent's inform act usually has multiple slots (see example 2 in Table TABREF15) and this makes classification and sequence generation harder, but gCAS multi-label slots decoder handles it easily.",
        "type": "Document"
      },
      {
        "id": "fce672d9-e5d2-4242-b698-784dfd250604",
        "metadata": {
          "vector_store_key": "1908.11546-1",
          "chunk_id": 22,
          "document_id": "1908.11546",
          "start_idx": 11163,
          "end_idx": 11842
        },
        "page_content": "The performance difference between CAS and gCAS on frames becomes much more evident, suggesting that gCAS is more capable of predicting slots that are consistent with the act. This finding is also consistent with their Entity F$_1$ and Success F$_1$ performance. However, gCAS's act-slot pair performance is far from perfect. The most common failure case is on non-critical slots (like `genre' in the example in Table TABREF4): gCAS does not predict them, while it predicts the critical ones (like `moviename' in the example in Table TABREF4). Table TABREF15 shows predictions of all methods from two emblematic examples. Example 1 is a frequent single-act multi-slots agent act.",
        "type": "Document"
      },
      {
        "id": "a3b364bf-6adb-4c9b-8db4-11be290cc074",
        "metadata": {
          "vector_store_key": "1908.11546-1",
          "chunk_id": 18,
          "document_id": "1908.11546",
          "start_idx": 9320,
          "end_idx": 10057
        },
        "page_content": "CAS and gCAS do not adopt a beam search since their inference steps are much less than Seq2Seq methods. All models use Adam optimizer BIBREF24 with a learning rate of 0.001. As shown in Table TABREF13, gCAS outperforms all other methods on Entity F$_1$ in all three domains. Compared to Seq2Seq, the performance advantage of gCAS in the taxi and restaurant domains is small, while it is more evident in the movie domain. The reason is that in the movie domain the proportion of turns with multiple acts is higher (52%), while in the other two domains it is lower (30%). gCAS also outperforms all other models in terms of Success F$_1$ in the movie and restaurant domain but is outperformed by the classification model in the taxi domain.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is specific to gCAS cell?",
    "llm_answer": "gCAS uses a proposed recurrent cell with separate continue, act, and slots units sequentially connected.\n",
    "context": [
      {
        "id": "1331b916-be72-477f-855f-14f192ec86f6",
        "metadata": {
          "vector_store_key": "1908.11546-3",
          "chunk_id": 17,
          "document_id": "1908.11546",
          "start_idx": 9079,
          "end_idx": 9685
        },
        "page_content": "The hidden state of the GRU and the predictions at the previous step are passed to the cell at the next step connecting them sequentially. gCAS uses our proposed recurrent cell which contains separate continue, act and slots unit that are sequentially connected. The classification architecture has two fully connected layers of size 128, and the remaining models have a hidden size of 64 and a teacher-forcing rate of 0.5. Seq2Seq and Copy Seq2Seq use a beam search with beam size 10 during inference. CAS and gCAS do not adopt a beam search since their inference steps are much less than Seq2Seq methods.",
        "type": "Document"
      },
      {
        "id": "30447ca8-5a83-4b60-ab8e-509b0229431a",
        "metadata": {
          "vector_store_key": "1908.11546-3",
          "chunk_id": 16,
          "document_id": "1908.11546",
          "start_idx": 8483,
          "end_idx": 9079
        },
        "page_content": "CAS adopts a single GRU BIBREF20 for decoding and uses three different fully connected layers for mapping the output of the GRU to continue, act and slots. For each step in the sequence of CAS tuples, given the output of the GRU, continue, act and slot predictions are obtained by separate heads, each with one fully connected layer. The hidden state of the GRU and the predictions at the previous step are passed to the cell at the next step connecting them sequentially. gCAS uses our proposed recurrent cell which contains separate continue, act and slots unit that are sequentially connected.",
        "type": "Document"
      },
      {
        "id": "388f48ce-440a-4955-b353-b79e06a7a71f",
        "metadata": {
          "vector_store_key": "1908.11546-3",
          "chunk_id": 27,
          "document_id": "1908.11546",
          "start_idx": 14057,
          "end_idx": 14709
        },
        "page_content": "The CAS decoder and the gCAS cell can also be used in a user simulator and gCAS can be applied in the encoder. A few directions for improvement have also been identified: 1) improving the performance on non-critical slots, 2) tuning the decoder with RL, 3) text generation from gCAS. We leave them as future work. We would like to express our special thanks to Alexandros Papangelis and Gokhan Tur for their support and contribution. We also would like to thank Xiujun Li for his help on dataset preparation and Jane Hung for her valuable comments. Bing Liu is partially supported by the NSF grant IIS-1910424 and a research gift from Northrop Grumman.",
        "type": "Document"
      },
      {
        "id": "85c2e2f2-1554-4c94-8ceb-242d4f098c80",
        "metadata": {
          "vector_store_key": "1912.10806-4",
          "chunk_id": 15,
          "document_id": "1912.10806",
          "start_idx": 8494,
          "end_idx": 9071
        },
        "page_content": "Each memory cell recurrently has a core self-connected linear unit called \u201c Constant Error Carousel (CEC)\u201d [13], which provides short-term memory storage and has three gates: Input gate, which controls the information from a new input to the memory cell, is given by where $h_{t-1}$ is the hidden state at the time step $t-1$; $i_t$ is the output of the input gate layer at the time step $t$; $\\hat{c}_t$ is the candidate value to be added to the output at the time step $t$; $b_i$ and $b_c$ are biases of the input gate layer and the candidate value computation, respectively;",
        "type": "Document"
      },
      {
        "id": "5b5b202e-fd1f-4622-a39c-da589815d00e",
        "metadata": {
          "vector_store_key": "1910.09399-9",
          "chunk_id": 34,
          "document_id": "1910.09399",
          "start_idx": 17997,
          "end_idx": 18675
        },
        "page_content": "Whereas the original GAN was trained only with samples from the data distribution, resulting in the generated sample reflecting the general data distribution, cGAN enables directing the model to generate more tailored outputs. In Figure FIGREF14, the condition vector is the class label (text string) \"Red bird\", which is fed to both the generator and discriminator. It is important, however, that the condition vector is related to the real data. If the model in Figure FIGREF14 was trained with the same set of real data (red birds) but the condition text was \"Yellow fish\", the generator would learn to create images of red birds when conditioned with the text \"Yellow fish\".",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the source of external knowledge?",
    "llm_answer": "Manually defined rules and world knowledge, such as number/gender requirements of pronouns, selectional preference, and eventuality knowledge.\n",
    "context": [
      {
        "id": "e31e2de4-8411-427b-8f1d-99c60b379407",
        "metadata": {
          "vector_store_key": "1905.10238-1",
          "chunk_id": 44,
          "document_id": "1905.10238",
          "start_idx": 24008,
          "end_idx": 24884
        },
        "page_content": "In previous work, external knowledge including manually defined rules BIBREF1 , BIBREF9 , such as number/gender requirement of different pronouns, and world knowledge BIBREF14 , such as selectional preference BIBREF29 , BIBREF30 and eventuality knowledge BIBREF31 , have been proved to be helpful for pronoun coreference resolution. Recently, with the development of deep learning, BIBREF12 proposed an end-to-end model that learns contextual information with an LSTM module and proved that such knowledge is helpful for coreference resolution when the context is properly encoded. The aforementioned two types of knowledge have their own advantages: the contextual information covers diverse text expressions that are difficult to be predefined while the external knowledge is usually more precisely constructed and able to provide extra information beyond the training data.",
        "type": "Document"
      },
      {
        "id": "56eab52a-0448-4016-89a3-351c9f80cf16",
        "metadata": {
          "vector_store_key": "1905.10238-1",
          "chunk_id": 45,
          "document_id": "1905.10238",
          "start_idx": 24884,
          "end_idx": 25710
        },
        "page_content": "The aforementioned two types of knowledge have their own advantages: the contextual information covers diverse text expressions that are difficult to be predefined while the external knowledge is usually more precisely constructed and able to provide extra information beyond the training data. Different from previous work, we explore the possibility of joining the two types of knowledge for pronoun coreference resolution rather than use only one of them. To the best of our knowledge, this is the first attempt that uses deep learning model to incorporate contextual information and external knowledge for pronoun coreference resolution. In this paper, we proposed a two-layer model for pronoun coreference resolution, where the first layer encodes contextual information and the second layer leverages external knowledge.",
        "type": "Document"
      },
      {
        "id": "cd7f1d02-ca93-470b-ba99-16df83309c4c",
        "metadata": {
          "vector_store_key": "1905.10238-1",
          "chunk_id": 5,
          "document_id": "1905.10238",
          "start_idx": 2960,
          "end_idx": 3729
        },
        "page_content": "In this paper, we propose a two-layer model to address the question while solving two challenges of incorporating external knowledge into deep models for pronoun coreference resolution, where the challenges include: first, different cases have their knowledge preference, i.e., some knowledge is exclusively important for certain cases, which requires the model to be flexible in selecting appropriate knowledge per case; second, the availability of knowledge resources is limited and such resources normally contain noise, which requires the model to be robust in learning from them. Consequently, in our model, the first layer predicts the relations between candidate noun phrases and the target pronoun based on the contextual information learned by neural networks.",
        "type": "Document"
      },
      {
        "id": "beb051dd-d2d3-421f-ab80-8ccde9f72d72",
        "metadata": {
          "vector_store_key": "1905.10238-0",
          "chunk_id": 40,
          "document_id": "1905.10238",
          "start_idx": 21742,
          "end_idx": 22445
        },
        "page_content": "As a comparison, in our model, integrating external knowledge can help to solve such challenges, e.g., for Example A, SP knowledge helps when Plurality and AG cannot distinguish all candidates. To clearly illustrate how our model leverages the external knowledge, we visualize the knowledge attention of the correct reference against other candidates via heatmaps in Figure 7 . Two interesting observations are drawn from the visualization. First, given two candidates, if they are significantly different in one feature, our model tends to pay more attention to that feature. Take AG as an example, in Example A, the AG features of all candidates consistently match the pronoun `he' (all male/neutral).",
        "type": "Document"
      },
      {
        "id": "7a10baaf-f5d2-4598-9ed3-e3f222e5db28",
        "metadata": {
          "vector_store_key": "1905.10238-1",
          "chunk_id": 4,
          "document_id": "1905.10238",
          "start_idx": 2013,
          "end_idx": 2960
        },
        "page_content": "Example C requires a more general type of knowledge that `cats can climb trees but a dog normally does not'. All of these knowledge are difficult to be learned from training data. Considering the importance of both contextual information and external human knowledge, how to jointly leverage them becomes an important question for pronoun coreference resolution. In this paper, we propose a two-layer model to address the question while solving two challenges of incorporating external knowledge into deep models for pronoun coreference resolution, where the challenges include: first, different cases have their knowledge preference, i.e., some knowledge is exclusively important for certain cases, which requires the model to be flexible in selecting appropriate knowledge per case; second, the availability of knowledge resources is limited and such resources normally contain noise, which requires the model to be robust in learning from them.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What were the sizes of the test sets?",
    "llm_answer": "1841 IPUs\n",
    "context": [
      {
        "id": "1639966f-28c0-46f5-9b17-6919c4c47061",
        "metadata": {
          "vector_store_key": "2002.06675-1",
          "chunk_id": 27,
          "document_id": "2002.06675",
          "start_idx": 14342,
          "end_idx": 15134
        },
        "page_content": "In this section the setting and results of ASR experiments are described and the results are discussed. The ASR experiments were performed in speaker-open condition as well as speaker-closed condition. In the speaker-closed condition, two episodes were set aside from each speaker as development and test sets. Thereafter, the total sizes of the development and test sets turns out to be 1585 IPUs spanning 2 hours 23 minutes and 1841 IPUs spanning 2 hours and 48 minutes respectively. The ASR model is trained with the rest data. In the speaker-open condition, all the data except for the test speaker's were used for training As it would be difficult to train the model if all of the data of speaker KM or UT were removed, experiments using their speaker-open conditions were not conducted.",
        "type": "Document"
      },
      {
        "id": "8be62763-27f4-46ff-b2bb-edb6a6b2fa04",
        "metadata": {
          "vector_store_key": "1607.06025-3",
          "chunk_id": 42,
          "document_id": "1607.06025",
          "start_idx": 24463,
          "end_idx": 25198
        },
        "page_content": "The new dataset consists of training set, which is generated using examples from the original training set, and a development set, which is generated from the original development set. The beam size for beam search was set to 1. The details of the decision are presented in Section SECREF35 . Some datasets were constructed by filtering the generated datasets according to various thresholds. Thus, the generated datasets were constructed to contain enough examples, so that the filtered datasets had at least the number of examples as the original dataset. In the end, all the datasets were trimmed down to the size of the original dataset by selecting the samples sequentially from the beginning until the dataset had the right size.",
        "type": "Document"
      },
      {
        "id": "fdf582fb-3fd0-4e0a-9bba-39ff6f6c125d",
        "metadata": {
          "vector_store_key": "1909.06522-0",
          "chunk_id": 22,
          "document_id": "1909.06522",
          "start_idx": 11687,
          "end_idx": 12449
        },
        "page_content": "For each language, the train and test set size are described in Table TABREF10, and most training data were Pages. On each language we also had a small validation set for model parameter tuning. Each monolingual ASR baseline was trained on language-specific data only. The character sets of these 7 languages have little overlap except that (i) they all include common basic Latin alphabet, and (ii) both Hindi and Marathi use Devanagari script. We took the union of 7 character sets therein as the multilingual grapheme set (Section SECREF2), which contained 432 characters. In addition, we deliberately split 7 languages into two groups, such that the languages within each group were more closely related in terms of language family, orthography or phonology.",
        "type": "Document"
      },
      {
        "id": "d04490fb-54c7-4b77-9831-9832486d6c06",
        "metadata": {
          "vector_store_key": "1909.13362-0",
          "chunk_id": 31,
          "document_id": "1909.13362",
          "start_idx": 18503,
          "end_idx": 19255
        },
        "page_content": "Each training epoch iterated over the training set to optimize the model parameters. The development set was used to tune the hyperparameters of the model, such as the batch size and the phone embedding dimension. The test set was exclusively used for reporting the model accuracy. The datasets were split randomly by percentages 80 (training), 10 (development), and 10 (test). For the English CELEX dataset of $89,402$ words, this resulted in $71,522$ words for training and $8,940$ words for each development and training. For each experiment, models were initialized with a random set of parameter weights. BIBREF37 showed that differences in random number generation produce statistically significant variances in the accuracy of LSTM-based models.",
        "type": "Document"
      },
      {
        "id": "cba5e199-51a0-480b-ac30-c7035a74c736",
        "metadata": {
          "vector_store_key": "1607.06025-2",
          "chunk_id": 43,
          "document_id": "1607.06025",
          "start_idx": 25198,
          "end_idx": 26010
        },
        "page_content": "In the end, all the datasets were trimmed down to the size of the original dataset by selecting the samples sequentially from the beginning until the dataset had the right size. Also, the datasets were filtered so that each of the labels was represented equally. All the models, including classification and discriminative models, were trained with hidden dimension INLINEFORM0 set to 150, unless otherwise noted. Our implementation is accessible at http://github.com/jstarc/nli_generation. It is based on libraries Keras and Theano BIBREF45 . First, the classification model OrigClass was trained on the original dataset. This model was then used throughout the experiments for filtering the datasets, comparison, etc. Notice that we have assumed OrigClass to be ground truth for the purpose of our experiments.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which datasets are used?",
    "llm_answer": "SST-2, Yelp, MRPC, STS-B, QQP, SQuAD 1.1, SQuAD 2.0, MRQA, MS COCO, Bing, Flickr, novel-9, novel-50, IMDB62, avsd, Charades.\n",
    "context": [
      {
        "id": "2ded8a9e-d4bc-48fa-9555-3885b88eba42",
        "metadata": {
          "vector_store_key": "1910.00458-5",
          "chunk_id": 32,
          "document_id": "1910.00458",
          "start_idx": 17414,
          "end_idx": 18132
        },
        "page_content": "For the sentiment analysis category, we used the Stanford Sentiment Treebank (SST-2) dataset from the GLUE benchmark BIBREF22 (around 60k train examples) and the Yelp dataset (around 430k train examples). For the paraphrase category, three paraphrasing datasets are used from the GLUE benchmark: Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), and Quora Question Pairs (QQP), which are denoted as \u201cGLUE-Para.\u201d. For the span-based QA, we use the SQuAD 1.1, SQuAD 2.0 , and MRQA which is a joint dataset including six popular span-based QA datasets. Table TABREF23 summarizes the results. We see that sentiment analysis datasets do not help much with our target MCQA datasets.",
        "type": "Document"
      },
      {
        "id": "8e741c56-0352-4660-89e2-d995b2fada35",
        "metadata": {
          "vector_store_key": "1807.09671-7",
          "chunk_id": 34,
          "document_id": "1807.09671",
          "start_idx": 19653,
          "end_idx": 20306
        },
        "page_content": "While the 2nd and 3rd data sets are from the same course, Statistics for Industrial Engineers, they were taught in 2015 and 2016 respectively (henceforth Stat2015 and Stat2016), at the Bo\u01e7azi\u00e7i University in Turkey. The course was taught in English while the official language is Turkish. The last one is from a fundamental undergraduate Computer Science course (data structures) at a local U.S. university taught in 2016 (henceforth CS2016). Another reason we choose the student responses is that we have advanced annotation allowing us to perform an intrinsic evaluation to test whether the low-rank approximation does capture similar concepts or not.",
        "type": "Document"
      },
      {
        "id": "947a5cf6-1656-4ca3-827b-105c8344c1d6",
        "metadata": {
          "vector_store_key": "1910.11949-3",
          "chunk_id": 16,
          "document_id": "1910.11949",
          "start_idx": 8700,
          "end_idx": 9318
        },
        "page_content": "In particular, we use two types of datasets to train our models: A dataset that maps pictures with questions, and an open-domain conversation dataset. The details of the two datasets are as follows. We use MS COCO, Bing and Flickr datasets from BIBREF26 to train the model that generates questions. These datasets contain natural questions about images with the purpose of knowing more about the picture. As can be seen in the Figure FIGREF8, questions cannot be answered by only looking at the image. Each source contains 5,000 images with 5 questions per image, adding a total of 15,000 images with 75,000 questions.",
        "type": "Document"
      },
      {
        "id": "6d614157-50dc-4aaa-af03-adc7f0282a12",
        "metadata": {
          "vector_store_key": "1709.02271-3",
          "chunk_id": 20,
          "document_id": "1709.02271",
          "start_idx": 11344,
          "end_idx": 11969
        },
        "page_content": "(Section SECREF26 ). The statistics for the three datasets used in the experiments are summarized in Table TABREF16 . novel-9. This dataset was compiled by F&H14: a collection of 19 novels by 9 nineteenth century British and American authors in the Project Gutenberg. To compare to F&H14, we apply the same resampling method (F&H14, Section 4.2) to correct the imbalance in authors by oversampling the texts of less-represented authors. novel-50. This dataset extends novel-9, compiling the works of 50 randomly selected authors of the same period. For each author, we randomly select 5 novels for a total 250 novels. IMDB62.",
        "type": "Document"
      },
      {
        "id": "73a83f60-b2c9-406d-b4cd-5720639e5f06",
        "metadata": {
          "vector_store_key": "1812.07023-4",
          "chunk_id": 18,
          "document_id": "1812.07023",
          "start_idx": 10358,
          "end_idx": 10986
        },
        "page_content": "After 10 such qa turns, the questioner wraps up by writing a summary of the video based on the conversation. Dataset statistics such as the number of dialogues, turns, and words for the avsd dataset are presented in Table TABREF5 . For the initially released prototype dataset, the training set of the avsd dataset corresponds to videos taken from the training set of the Charades dataset while the validation and test sets of the avsd dataset correspond to videos taken from the validation set of the Charades dataset. For the official dataset, training, validation and test sets are drawn from the corresponding Charades sets.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much does it minimally cost to fine-tune some model according to benchmarking framework?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "1cba40f2-d559-4eb3-94a8-bac01abf2315",
        "metadata": {
          "vector_store_key": "2002.05829-4",
          "chunk_id": 8,
          "document_id": "2002.05829",
          "start_idx": 4796,
          "end_idx": 5438
        },
        "page_content": "For example, we keep track of the time and cost of a BERT model pretrained from scratch. After every thousand of pretraining steps, we clone the model for fine-tuning and see if the final performance can reach our cut-off level. When the level is reached, time and cost for pretraining is used for comparison. Models faster or cheaper to pretrain are recommended. For fine-tuning phase, we consider the time and cost each model requires to reach certain multi-task performance fine-tuned from given pretrained models because for each single task with different difficulty and instance number, the fine-tuning characteristics may differ a lot.",
        "type": "Document"
      },
      {
        "id": "7206a28d-531a-43e4-bfe4-eff398259dc4",
        "metadata": {
          "vector_store_key": "2002.05829-4",
          "chunk_id": 20,
          "document_id": "2002.05829",
          "start_idx": 11733,
          "end_idx": 12431
        },
        "page_content": "For fine-tuning and inference phase, we conduct extensive experiments on given hardware (GTX 2080Ti GPU) with different model settings as shown in Table TABREF8 and Table TABREF9. We also collect the devlopment set performance with time in fine-tuning to investigate in how the model are fine-tuned for different tasks. In our fine-tuning setting, we are given a specific hardware and software configuration, we adjust the hyper-parameter to minimize the time required for fine-tuning towards cut-off performance. For example, we choose proper batchsize and learning rate for BERTBASE to make sure the model converges and can reach expected performance as soon as possible with parameter searching.",
        "type": "Document"
      },
      {
        "id": "9197972f-f755-45b8-90d5-be959f400890",
        "metadata": {
          "vector_store_key": "2002.05829-4",
          "chunk_id": 16,
          "document_id": "2002.05829",
          "start_idx": 9408,
          "end_idx": 10109
        },
        "page_content": "When it does, we count the time and cost in the pretraining process for benchmarking and analysis. For fine-tuning phase, we want to compare the general efficiency of pretrained model reaching cut-off performance on selected dataset. During fine-tuning, we evaluate the half-fine-tuned model on development set after a certain number of steps. When the performance reach our cut-off performance, we count the time and cost in this fine-tuning process for benchmarking and analysis. To be specific, for a single pretrained model, the efficiency score on different tasks is defined as the sum of normalized time and cost. Here we normalize the time and cost because they vary dramatically between tasks.",
        "type": "Document"
      },
      {
        "id": "7b67d968-dc02-441e-a704-15d58c91e24b",
        "metadata": {
          "vector_store_key": "2002.05829-1",
          "chunk_id": 26,
          "document_id": "2002.05829",
          "start_idx": 14660,
          "end_idx": 15107
        },
        "page_content": "We have compared the fine-tuning efficiency of given models during baseline testing and demonstrated more parameters lead to slower fine-tuning when using same model but does not hold when model changes. We expect more submissions in the future to flourish and enrich our benchmark. This work is supported by the Institute of Energy Efficiency (IEE) at UCSB's seed grant in Summer 2019 to improve the energy efficiency of AI and machine learning..",
        "type": "Document"
      },
      {
        "id": "999d2a51-e9e0-4d91-ba58-c0c2ef43fa3b",
        "metadata": {
          "vector_store_key": "2002.05829-1",
          "chunk_id": 21,
          "document_id": "2002.05829",
          "start_idx": 12431,
          "end_idx": 13078
        },
        "page_content": "For example, we choose proper batchsize and learning rate for BERTBASE to make sure the model converges and can reach expected performance as soon as possible with parameter searching. As shown in Figure FIGREF15, the fine-tuning performance curve differs a lot among pretrained models. The x-axis denoting time consumed is shown in log-scale for better comparison of different models. None of the models acutally take the lead in all tasks. However, if two pretrained models are in the same family, such as BERTBASE and BERTLARGE, the model with smaller number of parameters tend to converge a bit faster than the other in the NER and SST-2 task.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What models are included in baseline benchmarking results?",
    "llm_answer": "Neural baselines, AoA Reader, AS Reader, and participant systems.\n",
    "context": [
      {
        "id": "5f809c1c-7760-4694-bed2-9dfe5d2b8656",
        "metadata": {
          "vector_store_key": "2002.05829-3",
          "chunk_id": 6,
          "document_id": "2002.05829",
          "start_idx": 3580,
          "end_idx": 4345
        },
        "page_content": "Our Hulk benchmark, as shown in Figure FIGREF5, utilizes several classic datasets that have been widely adopted in the community as benchmarking tasks to benchmark energy efficiency and compares pretrained models in a multi-task fashion. The tasks include natural language inference task MNLI BIBREF14, sentiment analysis task SST-2 BIBREF15 and Named Entity Recognition Task CoNLL-2003 BIBREF16. Such tasks are selected to provide a thourough comparison of end-to-end energy efficiency in pretraining, fine-tuning and inference. With the Hulk benchmark, we quantify the energy efficiency of model pretraining, fine-tuning and inference phase by comparing the time and cost they require to reach certain overall task-specific performance level on selected datasets.",
        "type": "Document"
      },
      {
        "id": "4d3a2966-d599-49bc-bc57-9c03fe6d8dbf",
        "metadata": {
          "vector_store_key": "2002.05829-3",
          "chunk_id": 7,
          "document_id": "2002.05829",
          "start_idx": 4345,
          "end_idx": 5119
        },
        "page_content": "With the Hulk benchmark, we quantify the energy efficiency of model pretraining, fine-tuning and inference phase by comparing the time and cost they require to reach certain overall task-specific performance level on selected datasets. The design principle and benchmarking process are detailed in section SECREF2. We also explore the relation between model parameter and fine-tuning efficiency and demonstrate consistency of energy efficiency between tasks for different pretrained models. For pretraining phase, the benchmark is designed to favor energy efficient models in terms of time and cost that each model takes to reach certain multi-task performance pretrained from scratch. For example, we keep track of the time and cost of a BERT model pretrained from scratch.",
        "type": "Document"
      },
      {
        "id": "f3776613-5f75-4322-8d4e-69a0ede7f7df",
        "metadata": {
          "vector_store_key": "1811.00942-0",
          "chunk_id": 16,
          "document_id": "1811.00942",
          "start_idx": 9219,
          "end_idx": 9822
        },
        "page_content": "For the QRNNs, we used the first 350 words of the test set, and averaged the ms/query and mJ/query. For KN-5, we used the entire test set for evaluation, since the latency was much lower. To adjust for the base power load, we subtracted idle power draw from energy usage. For a different perspective, we further evaluated all the models under a desktop environment, using an i7-4790k CPU and Titan V GPU. Because the base power load for powering a desktop is much higher than running neural language models, we collected only latency statistics. We used the entire test set, since the QRNN runs quickly.",
        "type": "Document"
      },
      {
        "id": "fa88df39-033d-4e0f-891b-162e0f9b0424",
        "metadata": {
          "vector_store_key": "1709.08299-4",
          "chunk_id": 19,
          "document_id": "1709.08299",
          "start_idx": 11042,
          "end_idx": 11836
        },
        "page_content": "The neural baselines are built by the following steps. We first use the shared training data to build a general systems, and choose the best performing model (in terms of cloze validation set) as baseline. Use User Query validation data for further tuning the systems with 10-fold cross-validations. Increase dropout rate BIBREF9 to 0.5 for preventing over-fitting issue. All baseline systems are chosen according to the performance of the validation set. The participant system results are given in Table 2 and 3 . As we can see that two neural baselines are competitive among participant systems and AoA Reader successfully outperform AS Reader and all participant systems in single model condition, which proves that it is a strong baseline system even without further fine-tuning procedure.",
        "type": "Document"
      },
      {
        "id": "eaad0382-a031-4123-ba6f-9bd1afa901b7",
        "metadata": {
          "vector_store_key": "1904.09708-6",
          "chunk_id": 59,
          "document_id": "1904.09708",
          "start_idx": 31097,
          "end_idx": 31784
        },
        "page_content": "All evaluation runs are reported in the main text: for each evaluation except for the add-jump split, models were trained 5 times with different random seeds, and performance was measured with means and standard deviations of accuracy. For the add-jump split, we included 25 runs to get a more accurate assessment of performance. This revealed a strong skew in the distribution of results, so we included the median as the main measure of performance. Occasionally, the model did not train at all due to an unknown error (possibly very poor random initialization, high learning rate or numerical error). For this reason, we excluded runs in which training accuracy did not get above 10%.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "It looks like learning to paraphrase questions, a neural scoring model and a answer selection model cannot be trained end-to-end. How are they trained?",
    "llm_answer": "The entire system, including paraphrase generation, scoring, and answer selection, is trained end-to-end using question-answer pairs.\n",
    "context": [
      {
        "id": "4f97e005-87d3-4045-a097-287761e8672a",
        "metadata": {
          "vector_store_key": "1708.06022-6",
          "chunk_id": 6,
          "document_id": "1708.06022",
          "start_idx": 3508,
          "end_idx": 4279
        },
        "page_content": "We first generate paraphrases for the question, which can be obtained by one or several paraphrasing systems. A neural scoring model predicts the quality of the generated paraphrases, while learning to assign higher weights to those which are more likely to yield correct answers. The paraphrases and the original question are fed into a QA model that predicts a distribution over answers given the question. The entire system is trained end-to-end using question-answer pairs as a supervision signal. The framework is flexible, it does not rely on specific paraphrase or QA models. In fact, this plug-and-play functionality allows to learn specific paraphrases for different QA tasks and to explore the merits of different paraphrasing models for different applications.",
        "type": "Document"
      },
      {
        "id": "839d707b-6844-4972-9695-67edf2e30c72",
        "metadata": {
          "vector_store_key": "1703.04617-2",
          "chunk_id": 1,
          "document_id": "1703.04617",
          "start_idx": 722,
          "end_idx": 1486
        },
        "page_content": "The recent availability of relatively large training datasets (see Section \"Related Work\" for more details) has made it more feasible to train and estimate rather complex models in an end-to-end fashion for these problems, in which a whole model is fit directly with given question-answer tuples and the resulting model has shown to be rather effective. In this paper, we take a closer look at modeling questions in such an end-to-end neural network framework, since we regard question understanding is of importance for such problems. We first introduced syntactic information to help encode questions. We then viewed and modelled different types of questions and the information shared among them as an adaptation problem and proposed adaptation models for them.",
        "type": "Document"
      },
      {
        "id": "482207c9-55ab-42cd-87f7-264e2b6083fa",
        "metadata": {
          "vector_store_key": "1601.01705-7",
          "chunk_id": 10,
          "document_id": "1601.01705",
          "start_idx": 5538,
          "end_idx": 6265
        },
        "page_content": "The model we describe in this paper has a unified framework for handling both the perceptual and schema cases, and differs from existing work primarily in learning a differentiable execution model with continuous evaluation results. Neural models for question answering are also a subject of current interest. These include approaches that model the task directly as a multiclass classification problem BIBREF10 , models that attempt to embed questions and answers in a shared vector space BIBREF11 and attentional models that select words from documents sources BIBREF2 . Such approaches generally require that answers can be retrieved directly based on surface linguistic features, without requiring intermediate computation.",
        "type": "Document"
      },
      {
        "id": "52bdec24-f169-488d-a45b-ea1e4579190f",
        "metadata": {
          "vector_store_key": "1601.06068-3",
          "chunk_id": 41,
          "document_id": "1601.06068",
          "start_idx": 22049,
          "end_idx": 22812
        },
        "page_content": "Other three systems use paraphrases generated from an L-PCFG grammar. naive uses a word lattice with a single start-to-end path representing the input question itself, ppdb uses a word lattice constructed using the PPDB rules, and bilayered uses bi-layered L-PCFG to build word lattices. Note that naive does not require any parallel resource to train, ppdb requires an external paraphrase database, and bilayered, like mt, needs a parallel corpus with paraphrase pairs. We tune our classifier features and GraphParser features on the development data. We use the best setting from tuning for evaluation on the test data. We described a grammar method to generate paraphrases for questions, and applied it to a question answering system based on semantic parsing.",
        "type": "Document"
      },
      {
        "id": "5d93151f-5a43-4c71-95d0-190676a08995",
        "metadata": {
          "vector_store_key": "1706.04815-0",
          "chunk_id": 4,
          "document_id": "1706.04815",
          "start_idx": 2148,
          "end_idx": 3038
        },
        "page_content": "We build an evidence extraction model to predict the most important sub-spans from the passages as evidence, and then develop an answer synthesis model which takes the evidence as additional features along with the question and passage to further elaborate the final answers. Specifically, we develop the answer extraction model with state-of-the-art attention based neural networks which predict the start and end positions of evidence snippets. As multiple passages are provided for each question in the MS-MARCO dataset, we propose incorporating passage ranking as an additional task to improve the results of evidence extraction under a multi-task learning framework. We use the bidirectional recurrent neural networks (RNN) for the word-level representation, and then apply the attention mechanism BIBREF2 to incorporate matching information from question to passage at the word level.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much more accurate is the model than the baseline?",
    "llm_answer": "Improvements range from 0.5% to 3.0% in accuracy, and over 9% for BERT-Base and BERT-Large.\n",
    "context": [
      {
        "id": "14c77096-531d-4d71-9236-61c2ccdb8cfa",
        "metadata": {
          "vector_store_key": "1910.00458-6",
          "chunk_id": 44,
          "document_id": "1910.00458",
          "start_idx": 24199,
          "end_idx": 24865
        },
        "page_content": "Compared with the baselines, MMM leads to improvements ranging from 0.5% to 3.0% in accuracy. Our best result is obtained by the RoBERTa-Large encoder. In order to investigate how well our model performs for different types of questions, we did an error analysis by first randomly selecting 150 samples that had wrong predictions by the BERT-Base baseline model from the development set of DREAM dataset. We then manually classified them into several question types, as shown in Table TABREF34. The annotation criterion is described in the Section 3 of the Supplementary Material. We see that the BERT-Base baseline model still does not do well on matching problems.",
        "type": "Document"
      },
      {
        "id": "ed78ec94-fc34-4c26-8c67-ef278ed3bb5e",
        "metadata": {
          "vector_store_key": "2001.02380-2",
          "chunk_id": 102,
          "document_id": "2001.02380",
          "start_idx": 53602,
          "end_idx": 54275
        },
        "page_content": "Although the random baseline is actually very slightly higher (perhaps because eliminated EDUs were often longer ones, sharing small amounts of material with larger parts of the text, and therefore prone to penalizing the baseline; many words mean more chances for a random guess to be wrong), model accuracy is substantially better in this scenario, reaching a 40% chance of hitting a signal with only one guess, exceeding 53% with two guesses, and capping at 64% for recall@3, over 20 points above baseline. Finally, the right panel in the figure shows recall when only DMs are considered. In this scenario, a random guess fares very poorly, since most words are not DMs.",
        "type": "Document"
      },
      {
        "id": "36dcf150-24e8-459c-a8c0-aa8fe1c0018f",
        "metadata": {
          "vector_store_key": "1909.02764-4",
          "chunk_id": 36,
          "document_id": "1909.02764",
          "start_idx": 19976,
          "end_idx": 20568
        },
        "page_content": "We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with 40 % of the data from the same domain (results shown in the column \u201cIn-Domain\u201d in Table TABREF19). Excluding AMMER, we achieve an average micro $\\text{F}_1$ of 68 %, with best results of F$_1$=73 % on TEC. The model trained on our AMMER corpus achieves an F1 score of 57%. This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set.",
        "type": "Document"
      },
      {
        "id": "0c4bd793-a8e9-4861-8601-03f439261b66",
        "metadata": {
          "vector_store_key": "1910.00458-6",
          "chunk_id": 21,
          "document_id": "1910.00458",
          "start_idx": 12016,
          "end_idx": 12817
        },
        "page_content": "In the table, we first report the accuracy of the SOTA models in the leaderboard. We then report the performance of our re-implementation of fine-tuned models as another set of strong baselines, among which the RoBERTa-Large model has already surpassed the previous SOTA. For these baselines, the top-level classifier is a two-layer FCNN for BERT-based models and a one-layer FCNN for the RoBERTa-Large model. Lastly, we report model performances that use all our proposed method, MMM (MAN classifier + speaker normalization + two stage learning strategies). As direct comparisons, we also list the accuracy increment between MMM and the baseline with the same sentence encoder marked by the parentheses, from which we can see that the performance augmentation is over 9% for BERT-Base and BERT-Large.",
        "type": "Document"
      },
      {
        "id": "874d1312-9fdb-46fe-865b-ce75dcb445d6",
        "metadata": {
          "vector_store_key": "1812.07023-3",
          "chunk_id": 32,
          "document_id": "1812.07023",
          "start_idx": 17666,
          "end_idx": 18428
        },
        "page_content": "We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below: Our primary architectural differences over the baseline model are: not concatenating the question, answer pairs before encoding them, the auxiliary decoder module, and using the Time-Extended FiLM module for feature extraction. These, combined with using scheduled sampling and running hyperparameter optimization over the validation set to select hyperparameters, give us the observed performance boost. We observe that our models generate fairly relevant responses to questions in the dialogues, and models with audio-visual inputs respond to audio-visual questions (e.g. \u201cis there any voices or music ?\u201d) correctly more often.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is new state-of-the-art performance on CoNLL-2009 dataset?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "59630f23-3c9c-47bb-b331-4027472a3a6b",
        "metadata": {
          "vector_store_key": "1603.04553-1",
          "chunk_id": 20,
          "document_id": "1603.04553",
          "start_idx": 10753,
          "end_idx": 11450
        },
        "page_content": "Specifically, our model achieves improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford system, the winner of the CoNLL 2011 shared task, on the CoNLL 2012 development and test sets, respectively. The improvements on CoNLL F1 score over the Multigraph model are 1.41% and 1.77% on the development and test sets, respectively. Comparing with the MIR model, we obtain significant improvements of 2.62% and 3.02% on CoNLL F1 score. To make a thorough empirical comparison with previous studies, Table 3 (below the dashed line) also shows the results of some state-of-the-art supervised coreference resolution systems \u2014 IMS: the second best system in the CoNLL 2012 shared task BIBREF28 ;",
        "type": "Document"
      },
      {
        "id": "456b24a0-2d89-4a89-88bc-f3adfe9eed46",
        "metadata": {
          "vector_store_key": "1603.04553-1",
          "chunk_id": 23,
          "document_id": "1603.04553",
          "start_idx": 12556,
          "end_idx": 13350
        },
        "page_content": "Experimental results on the data from CoNLL-2012 shared task show that our system significantly improves the accuracy on different evaluation metrics over the baseline systems. One possible direction for future work is to differentiate more resolution modes. Another one is to add more precise or even event-based features to improve the model's performance. This research was supported in part by DARPA grant FA8750-12-2-0342 funded under the DEFT program. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA. Appendix A. Derivation of Model Learning Formally, we iteratively estimate the model parameters $\\theta $ , employing the following EM algorithm: For simplicity, we denote: $",
        "type": "Document"
      },
      {
        "id": "aa97b630-1f34-4c1d-8ea0-86388a52b430",
        "metadata": {
          "vector_store_key": "1603.04553-1",
          "chunk_id": 19,
          "document_id": "1603.04553",
          "start_idx": 10672,
          "end_idx": 11346
        },
        "page_content": "All the results are given by the latest version of CoNLL-2012 scorer  Table 3 illustrates the results of our model together as baseline with two deterministic systems, namely Stanford: the Stanford system BIBREF10 and Multigraph: the unsupervised multigraph system BIBREF26 , and one unsupervised system, namely MIR: the unsupervised system using most informative relations BIBREF27 . Our model outperforms the three baseline systems on all the evaluation metrics. Specifically, our model achieves improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford system, the winner of the CoNLL 2011 shared task, on the CoNLL 2012 development and test sets, respectively.",
        "type": "Document"
      },
      {
        "id": "c51ff493-9c93-4d5b-b54c-2d08eb62fd6b",
        "metadata": {
          "vector_store_key": "1902.10525-5",
          "chunk_id": 44,
          "document_id": "1902.10525",
          "start_idx": 23110,
          "end_idx": 23846
        },
        "page_content": "Finally, we show a comparison of our old and new systems with the literature on the IAM-OnDB dataset in Table TABREF49 . Our method establishes a new state of the art result when relying on closed data using IAM-OnDB, as well as when relying on our in-house data that we use for our production system, which was not tuned for the IAM-OnDB data and for which none of the IAM-OnDB data was used for training. To better understand where the improvements come from, we discuss the differences between the previous state-of-the-art system (Graves et al. BLSTM BIBREF24 ) and this work across four dimensions: input pre-processing and feature extraction, neural network architecture, CTC training and decoding, and model training methodology.",
        "type": "Document"
      },
      {
        "id": "c58505f0-2392-4562-87d7-2f3130b95957",
        "metadata": {
          "vector_store_key": "1701.02877-4",
          "chunk_id": 52,
          "document_id": "1701.02877",
          "start_idx": 26848,
          "end_idx": 27485
        },
        "page_content": "What is very interesting to see is that they are much higher than the results on the biggest training corpus, OntoNotes NW. For instance, SENNA has an F1 of 78.04 on OntoNotes, compared to an F1 of 92.39 and 86.44 for CoNLL Test A and Test B respectively. So even though OntoNotes NW is more than twice the size of CoNLL in terms of NEs (see Table 4 ), NERC performance is much higher on CoNLL. NERC performance with respect to training corpus size is represented in Figure 1 . The latter figure confirms that although there is some correlation between corpus size and F1, the variance between results on comparably sized corpora is big.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are two strong baseline methods authors refer to?",
    "llm_answer": "AoA Reader and AS Reader\n",
    "context": [
      {
        "id": "5e76f24e-4c75-4a34-b69c-7fe1f8d12771",
        "metadata": {
          "vector_store_key": "1909.02635-2",
          "chunk_id": 15,
          "document_id": "1909.02635",
          "start_idx": 9069,
          "end_idx": 9719
        },
        "page_content": "This includes (i) Majority Class, (ii) Exact Match of an ingredient $e$ in recipe step $s_t$, and (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo$_{token}$ and ELMo$_{sent}$) BIBREF22 compared to GPT.",
        "type": "Document"
      },
      {
        "id": "46e563f1-f3ad-4871-9d28-8409ceeb53b8",
        "metadata": {
          "vector_store_key": "1701.00185-0",
          "chunk_id": 38,
          "document_id": "1701.00185",
          "start_idx": 20509,
          "end_idx": 21535
        },
        "page_content": "Besides K-means, Skip-thought Vectors, Recursive Neural Network and Paragraph Vector based clustering methods, four baseline clustering methods are directly based on the popular unsupervised dimensionality reduction methods as described in Section SECREF11 . We further compare our approach with some other non-biased neural networks, such as bidirectional RNN. More details are listed as follows: K-means K-means BIBREF42 on original keyword features which are respectively weighted with term frequency (TF) and term frequency-inverse document frequency (TF-IDF). Skip-thought Vectors (SkipVec) This baseline BIBREF35 gives an off-the-shelf encoder to produce highly generic sentence representations. The encoder is trained using a large collection of novels and provides three encoder modes, that are unidirectional encoder (SkipVec (Uni)) with 2,400 dimensions, bidirectional encoder (SkipVec (Bi)) with 2,400 dimensions and combined encoder (SkipVec (Combine)) with SkipVec (Uni) and SkipVec (Bi) of 2,400 dimensions each.",
        "type": "Document"
      },
      {
        "id": "3d67e1d2-d559-497f-a1af-c77f4ff25101",
        "metadata": {
          "vector_store_key": "1709.01256-2",
          "chunk_id": 32,
          "document_id": "1709.01256",
          "start_idx": 17510,
          "end_idx": 18277
        },
        "page_content": "This section reports the results of the experiments conducted on two data sets for evaluating the performances of wDTW and wTED against other baseline methods. We denote the following distance/similarity measures. wDTW: Our semantic distance measure explained in Section SECREF21 . wTED: Our semantic distance measure explained in Section SECREF23 . WMD: The Word Mover's Distance introduced in Section SECREF1 . WMD adapts the earth mover's distance to the space of documents. VSM: The similarity measure introduced in Section UID12 . PV-DTW: PV-DTW is the same as Algorithm SECREF21 except that the distance between two paragraphs is not based on Algorithm SECREF20 but rather computed as INLINEFORM0 where INLINEFORM1 is the PV embedding of paragraph INLINEFORM2 .",
        "type": "Document"
      },
      {
        "id": "1c67409f-2f57-4674-ae18-af1d704d9ae8",
        "metadata": {
          "vector_store_key": "1804.08782-1",
          "chunk_id": 24,
          "document_id": "1804.08782",
          "start_idx": 13324,
          "end_idx": 14085
        },
        "page_content": "The experiment is conducted on two datasets: a subset (10%) of Fisher corpus set aside as test data and Suicide corpus. We use a number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 . For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose the best performing one for reporting, thus providing an upper-bound performance. Also, for baseline 2 we choose the session with higher value of the measure as real, since it measures similarity.",
        "type": "Document"
      },
      {
        "id": "fa88df39-033d-4e0f-891b-162e0f9b0424",
        "metadata": {
          "vector_store_key": "1709.08299-4",
          "chunk_id": 19,
          "document_id": "1709.08299",
          "start_idx": 11042,
          "end_idx": 11836
        },
        "page_content": "The neural baselines are built by the following steps. We first use the shared training data to build a general systems, and choose the best performing model (in terms of cloze validation set) as baseline. Use User Query validation data for further tuning the systems with 10-fold cross-validations. Increase dropout rate BIBREF9 to 0.5 for preventing over-fitting issue. All baseline systems are chosen according to the performance of the validation set. The participant system results are given in Table 2 and 3 . As we can see that two neural baselines are competitive among participant systems and AoA Reader successfully outperform AS Reader and all participant systems in single model condition, which proves that it is a strong baseline system even without further fine-tuning procedure.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many category tags are considered?",
    "llm_answer": "100\n",
    "context": [
      {
        "id": "2a7e32ea-8023-40f9-b0bf-8ec10a41149f",
        "metadata": {
          "vector_store_key": "1709.05413-2",
          "chunk_id": 14,
          "document_id": "1709.05413",
          "start_idx": 7795,
          "end_idx": 8672
        },
        "page_content": "The taxonomy contains a total of 220 tags, divided into four main categories: communicative status, information level, forward-looking function, and backward-looking function. Jurafsky, Shriberg, and Biasca develop a less fine-grained taxonomy of 42 tags based on DAMSL BIBREF1 . Stolcke et al. employ a similar set for general conversation BIBREF2 , citing that \"content- and task-related distinctions will always play an important role in effective DA [Dialogue Act] labeling.\" Many researchers have tackled the task of developing different speech and dialogue act taxonomies and coding schemes BIBREF12 , BIBREF13 , BIBREF14 , BIBREF15 . For the purposes of our own research, we require a set of dialogue acts that is more closely representative of customer service domain interactions - thus we expand upon previously defined taxonomies and develop a more fine-grained set.",
        "type": "Document"
      },
      {
        "id": "0ed71a6b-a1aa-4a60-a95d-09b3534a2593",
        "metadata": {
          "vector_store_key": "1707.02377-4",
          "chunk_id": 35,
          "document_id": "1707.02377",
          "start_idx": 20784,
          "end_idx": 21482
        },
        "page_content": "The 100 categories includes categories under sports, entertainment, literature, and politics etc. Examples of categories include American drama films, Directorial debut films, Major League Baseball pitchers and Sydney Swans players. Body texts (the second paragraph) were extracted for each page as a document. For each category, we select 1,000 documents with unique category label, and 100 documents were used for training and 900 documents for testing. The remaining documents are used as unlabeled data. The 100 classes are balanced in the training and testing sets. For this data set, we learn the word embedding and document representation for all the algorithms using all the available data.",
        "type": "Document"
      },
      {
        "id": "073e1331-a9c5-4656-b39c-bba446af4ef1",
        "metadata": {
          "vector_store_key": "1909.02776-2",
          "chunk_id": 45,
          "document_id": "1909.02776",
          "start_idx": 24982,
          "end_idx": 25840
        },
        "page_content": "Documents are categorized into six categories such as political, economic and so on. The length of documents ranges from 4 to 156 sentences. Overall, it has about 2,500 sentences. All features introduced in section SECREF4 are calculated. Pre-processing, sentence and word tokenization, stop words removal, and part of speech tagging is performed using the Hazm library BIBREF43. The majority of features have a range between zero and one. Other features are passed to a min-max scaler to transform into the same range. For the category feature which is nominal, the one-hot-encoding method applied and six flag features used instead. In assigning the target to a sentence, as mentioned in section (SECREF16), the goal is to assign a number between 0 and 1, with higher values as an indicator that the sentence is present in the majority of golden summaries.",
        "type": "Document"
      },
      {
        "id": "5523d951-2e9f-46b0-bc8e-ecd6513d703d",
        "metadata": {
          "vector_store_key": "1703.09684-4",
          "chunk_id": 21,
          "document_id": "1703.09684",
          "start_idx": 12100,
          "end_idx": 12876
        },
        "page_content": "For example, for counting we use 8 templates, e.g., `How many <objects> are there?', `How many <objects> are in the photo?', etc. Since the COCO and Visual Genome use different annotation formats, we discuss them separately. Sport recognition, counting, subordinate object recognition, object presence, scene understanding, positional reasoning, and absurd questions were created from COCO, similar to the scheme used in BIBREF18 . For counting, we count the number of object instances in an image annotation. To minimize ambiguity, this was only done if objects covered an area of at least 2,000 pixels. For subordinate object recognition, we create questions that require identifying an object's subordinate-level object classification based on its larger semantic category.",
        "type": "Document"
      },
      {
        "id": "3d614b27-7069-49fc-9a72-55fd62fcc017",
        "metadata": {
          "vector_store_key": "1703.09684-8",
          "chunk_id": 19,
          "document_id": "1703.09684",
          "start_idx": 10950,
          "end_idx": 11703
        },
        "page_content": "Questions were classified as activity or sports recognition questions if the answer was one of nine common sports or one of fifteen common activities and the question contained common verbs describing actions or sports, e.g., playing, throwing, etc. For counting, the question had to begin with `How many' and the answer had to be a small countable integer (1-16). The other categories were determined using regular expressions. For example, a question of the form `Are feeling ?' was classified as sentiment understanding and `What is to the right of/left of/ behind the ?' was classified as positional reasoning. Similarly, `What <OBJECT CATEGORY> is in the image?' and similar templates were used to populate subordinate object recognition questions.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What domain does the dataset fall into?",
    "llm_answer": "The dataset contains questions about entertainment media, sports, and general factual knowledge.\n",
    "context": [
      {
        "id": "2e76af14-b064-4501-b27a-103fd6bbbeaf",
        "metadata": {
          "vector_store_key": "1907.11499-3",
          "chunk_id": 12,
          "document_id": "1907.11499",
          "start_idx": 6293,
          "end_idx": 7203
        },
        "page_content": "For example, the definition of the \u201cLifestyle\u201d domain is \u201cthe interests, opinions, behaviors, and behavioral orientations of an individual, group, or culture\u201d. Figure FIGREF5 provides an overview of our Domain Detection Network, which we call DetNet. The model comprises two modules; an encoder learns representations for words and sentences whilst incorporating prior domain information; a detector generates domain scores for words, sentences, and documents by selectively attending to previously encoded information. We describe the two modules in more detail below. We learn representations for words and sentences using identical encoders with separate learning parameters. Given a document, the two encoders implement the following steps: INLINEFORM0   For each sentence INLINEFORM0 , the word-level encoder yields contextualized word representations INLINEFORM1 and their attention weights INLINEFORM2 .",
        "type": "Document"
      },
      {
        "id": "ec11c575-9cdc-40f2-b742-781c89922785",
        "metadata": {
          "vector_store_key": "1701.02877-9",
          "chunk_id": 11,
          "document_id": "1701.02877",
          "start_idx": 5586,
          "end_idx": 6230
        },
        "page_content": "Domain describes the dominant subject matter of text, which might give specialised vocabulary or specific, unusal word senses. For example, \u201cbroadcast news\" is a genre, describing the manner of use of language, whereas \u201cfinancial text\" or \u201cpopular culture\" are domains, describing the topic. One notable exception to this terminology is social media, which tends to be a blend of myriad domains and genres, with huge variation in both these dimensions BIBREF38 , BIBREF39 ; for simplicity, we also refer to this as a genre here. In chronological order, the first corpus included here is MUC 7, which is the last of the MUC challenges BIBREF31 .",
        "type": "Document"
      },
      {
        "id": "5f751c7d-dc7b-474b-ac83-60046b5f27c3",
        "metadata": {
          "vector_store_key": "1811.01734-4",
          "chunk_id": 0,
          "document_id": "1811.01734",
          "start_idx": 1,
          "end_idx": 718
        },
        "page_content": "Domain shift is a fundamental problem in machine learning, that has attracted a lot of attention in the natural language processing and vision communities BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 . To understand and address this problem, generated by the lack of labeled data in a target domain, researchers have studied the behavior of machine learning methods in cross-domain settings BIBREF2 , BIBREF11 , BIBREF10 and came up with various domain adaptation techniques BIBREF12 , BIBREF5 , BIBREF6 , BIBREF9 . In cross-domain classification, a classifier is trained on data from a source domain and tested on data from a (different) target domain.",
        "type": "Document"
      },
      {
        "id": "9b758cdd-bd9a-401f-8afd-ea03ad4e7faa",
        "metadata": {
          "vector_store_key": "1905.10044-9",
          "chunk_id": 21,
          "document_id": "1905.10044",
          "start_idx": 11683,
          "end_idx": 12403
        },
        "page_content": "Part of the value of this dataset is that it contains questions that people genuinely want to answer. To explore this further, we manually define a set of topics that questions can be about. An author categorized 200 questions into these topics. The results can be found in the upper half of Table . Questions were often about entertainment media (including T.V., movies, and music), along with other popular topics like sports. However, there are still a good portion of questions asking for more general factual knowledge, including ones about historical events or the natural world. We also broke the questions into categories based on what kind of information they were requesting, shown in the lower half of Table .",
        "type": "Document"
      },
      {
        "id": "786eaa4f-6843-428f-b802-626e6014bcbf",
        "metadata": {
          "vector_store_key": "2002.01359-4",
          "chunk_id": 22,
          "document_id": "2002.01359",
          "start_idx": 12592,
          "end_idx": 13359
        },
        "page_content": "The 20 domains present across the train, dev and test datasets are listed in Table TABREF10, as are the details regarding which domains are present in each of the datasets. We create synthetic implementations of a total of 45 services or APIs over these domains. Our simulator framework interacts with these services to generate dialogue outlines, which are structured representations of dialogue semantics. We then use a crowd-sourcing procedure to paraphrase these outlines to natural language utterances. Our novel crowd-sourcing procedure preserves all annotations obtained from the simulator and does not require any extra annotations after dialogue collection. In this section, we describe these steps briefly and then present analyses of the collected dataset.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are EAC evaluated?",
    "llm_answer": "EAC is evaluated qualitatively (using ISO 9241) and quantitatively (using automatic evaluation like perplexity and manual evaluation),  with specific metrics depending on the task.  For question answering, F$_1$ score is used.  For RL agents, training and testing curves are reported, and the agent's best validation performance is used for test evaluation.  Other metrics include precision, recall, F$_1$ score, BLEU, EMR, and Success F$_1$ score.\n",
    "context": [
      {
        "id": "74f6d11f-dad7-42be-9e7a-785347faecaf",
        "metadata": {
          "vector_store_key": "1906.09774-1",
          "chunk_id": 34,
          "document_id": "1906.09774",
          "start_idx": 20144,
          "end_idx": 20930
        },
        "page_content": "In addition, we also investigate the available affective resources which usually use in the emotion classification task. In this part, we only focus on English resources and found several resources from the old one such as LIWC and Emolex to the new one, including DepecheMood and EmoWordNet. In the final part, we gather information about how to evaluate the performance of EAC, and we can classify the approach into two techniques, including qualitative and quantitative assessment. For qualitative assessment, most studies used ISO 9241, which covers several aspects such as efficiency, effectiveness, and satisfaction. While in quantitative analysis, two techniques can be used, including automatic evaluation (by using perplexity) and manual evaluation (involving human judgement).",
        "type": "Document"
      },
      {
        "id": "8c1d82aa-97ca-4ec2-aab4-3e568640daa3",
        "metadata": {
          "vector_store_key": "1906.09774-1",
          "chunk_id": 11,
          "document_id": "1906.09774",
          "start_idx": 6547,
          "end_idx": 7200
        },
        "page_content": "Table TABREF10 summarizes this information includes the objective and exploited approach of each work. In early development, EAC is designed by using a rule-based approach. However, in recent years mostly EAC exploit neural-based approach. Studies in EAC development become a hot topic start from 2017, noted by the first shared task in Emotion Generation Challenge on NLPCC 2017 BIBREF31 . Based on Table TABREF10 this research line continues to gain massive attention from scholars in the latest years. Based on Table TABREF10 , we can see that most of all recent EAC was built by using encoder-decoder architecture with sequence-to-sequence learning.",
        "type": "Document"
      },
      {
        "id": "bf8e6727-73ec-4cbe-88eb-2e4916ba4074",
        "metadata": {
          "vector_store_key": "1906.09774-4",
          "chunk_id": 22,
          "document_id": "1906.09774",
          "start_idx": 12959,
          "end_idx": 13869
        },
        "page_content": "Other functionalities, such as chatbots' ability to execute the task as requested, the output linguistic accuracy, and ease of use suggested being assessed BIBREF58 . Meanwhile, from the human aspect, most of the studies suggest each conversational machine should pass Turing test BIBREF21 . Other prominent abilities that chatbot needs to be mastered can respond to specific questions and able to maintain themed discussion. Satisfaction aspect has three categories, including affect, ethics and behaviour, and accessibility. Affect is the most suitable assessment categories for EAC. This category asses several quality aspects such as, chatbots' ability to convey personality, give conversational cues, provide emotional information through tone, inflexion, and expressivity, entertain and/or enable the participant to enjoy the interaction and also read and respond to moods of human participant BIBREF59 .",
        "type": "Document"
      },
      {
        "id": "6a443055-4b4a-46e1-a2dc-b3211ac62d38",
        "metadata": {
          "vector_store_key": "1908.10449-0",
          "chunk_id": 20,
          "document_id": "1908.10449",
          "start_idx": 10741,
          "end_idx": 11449
        },
        "page_content": "Since iMRC involves both MRC and RL, we adopt evaluation metrics from both settings. First, as a question answering task, we use $\\text{F}_1$ score to compare predicted answers against ground-truth, as in previous works. When there exist multiple ground-truth answers, we report the max $\\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we evaluate an agent's performance during both its training and testing phases. During training, we report training curves averaged over 3 random seeds. During test, we follow common practice in supervised learning tasks where we report the agent's test performance corresponding to its best validation performance .",
        "type": "Document"
      },
      {
        "id": "4db19387-f4f2-46e7-a362-3521bdda8413",
        "metadata": {
          "vector_store_key": "1908.02402-9",
          "chunk_id": 45,
          "document_id": "1908.02402",
          "start_idx": 23649,
          "end_idx": 24447
        },
        "page_content": "We evaluate the performance concerning belief state tracking, response language quality, and task completion. For belief state tracking, we report precision, recall, and F $_1$ score of informable slot values and requestable slots. BLEU BIBREF34 is applied to the generated agent responses for evaluating language quality. Although it is a poor choice for evaluating dialogue systems BIBREF35 , we still report it in order to compare with previous work that has adopted it. For task completion evaluation, the Entity Match Rate (EMR) BIBREF7 and Success F $_1$ score (SuccF $_1$ ) BIBREF10 are reported. EMR evaluates whether a system can correctly retrieve the user's indicated entity (record) from the KB based on the generated constraints so it can have only a score of 0 or 1 for each dialogue.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is triangulation?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "a46b1cd8-1a5d-4159-a79c-0ac7a608a859",
        "metadata": {
          "vector_store_key": "1705.03151-4",
          "chunk_id": 29,
          "document_id": "1705.03151",
          "start_idx": 16603,
          "end_idx": 17346
        },
        "page_content": "With the PTN model, feature extraction is trained on speech data labelled with phones or words which are highly informative and fine-grained (compared to language labels), leading to a strong DNN model for phonetic feature extraction. Importantly, phone discrimination and language identification are naturally correlated (from our phonetic perspective), which means that the phonetic features learned with the strong phone/word supervision involves rich information suitable for LID. This is an example of transfer learning, where a related task (i.e., phone discrimination) is used to learn features for another task (LID). The PTN approach also involves another two transfer learning schemes: cross language and cross condition (databases).",
        "type": "Document"
      },
      {
        "id": "20337331-11f0-4bc7-a287-d3a40e727305",
        "metadata": {
          "vector_store_key": "1901.00570-4",
          "chunk_id": 31,
          "document_id": "1901.00570",
          "start_idx": 16924,
          "end_idx": 17693
        },
        "page_content": "Distance correlation measures the statistical distance between probability distributions by dividing the Brownian covariance (distance covariance) between X and Y by the product of the distance standard deviations BIBREF28 , BIBREF29 . TF-IDF is the short of term frequency-inverse document frequency technique that is used for word selection for classification problems. The concept of this technique is to give the words that occur frequently within a specific class high weight as a feature and to penalize the words that occur frequently among multiple classes. for example; the term \u201cShakespeare\u201d is considered a useful feature to classify English literature documents as it occurs frequently in English literature and rarely occurs in any other kind of documents.",
        "type": "Document"
      },
      {
        "id": "510ce943-695e-4696-8af8-67d9e0592095",
        "metadata": {
          "vector_store_key": "2002.02224-1",
          "chunk_id": 14,
          "document_id": "2002.02224",
          "start_idx": 8143,
          "end_idx": 8887
        },
        "page_content": "The computing similarity between the parts of text presumes that a decrease of similarity means a topical border of two text segments. This approach was introduced by Hearst BIBREF22 and was used by Choi BIBREF25 and Heinonen BIBREF26 as well. Another approach takes word frequencies and presumes a border according to different key words extracted. Reynar BIBREF27 authored graphical method based on statistics called dotplotting. Similar techniques were used by Ye BIBREF28 or Saravanan BIBREF29. Bommarito et al. BIBREF30 introduced a Python library combining different features including pre-trained models to the use for automatic legal text segmentation. Li BIBREF31 included neural network into his method to segment Chinese legal texts.",
        "type": "Document"
      },
      {
        "id": "ce4807f3-a224-4f57-80e4-521b162a313e",
        "metadata": {
          "vector_store_key": "1912.07025-1",
          "chunk_id": 23,
          "document_id": "1912.07025",
          "start_idx": 14690,
          "end_idx": 15408
        },
        "page_content": "After applying non-maximal suppression to remove overlapping boxes with relatively smaller objectness scores, the final set of boxes which remain are termed `object proposals' or Regions-of-Interest (RoI). Multi-Task Branch Networks: The RoIs obtained from RPN are warped into fixed dimensions and overlaid on feature maps extracted from the backbone to obtain RoI-specific features. These features are fed to three parallel task sub-networks. The first sub-network maps these features to region labels (e.g. Hole,Character-Line-Segment) while the second sub-network maps the RoI features to bounding boxes. The third sub-network is fully convolutional and maps the features to the pixel mask of the underlying region.",
        "type": "Document"
      },
      {
        "id": "bb7506d2-3899-48e2-8c8e-51c11908de9c",
        "metadata": {
          "vector_store_key": "1808.09920-2",
          "chunk_id": 18,
          "document_id": "1808.09920",
          "start_idx": 9575,
          "end_idx": 10205
        },
        "page_content": "The algorithm is parameterized by a graph convolutional network (GCN) BIBREF13 , in particular, we employ relational-GCNs BIBREF17 , an extended version that accommodates edges of different types. In Section \"Entity relational graph convolutional network\" we describe the propagation rule. Each step of the algorithm (also referred to as a hop) updates all node representations in parallel. In particular, a node is updated as a function of messages from its direct neighbours, and a message is possibly specific to a certain relation. At the end of the first step, every node is aware of every other node it connects directly to.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What languages do they use?",
    "llm_answer": "Amharic, Arabic, Hindi, Somali, Chinese, Korean, Japanese, French, German, Swedish, Spanish, Portuguese, Italian, Russian, Polish, Norwegian, Romanian, Dutch, Czech, Turkish, Thai, Urdu, Mandarin, Cantonese, Indonesian, Japanese, Russian, Korean, Vietnamese, English, Spanish, French, German, Russian, Italian, Bengali, Hindi, and Telegu.\n",
    "context": [
      {
        "id": "9cd0820a-fcb8-4bf5-b481-c19a5a179e77",
        "metadata": {
          "vector_store_key": "1909.09270-5",
          "chunk_id": 26,
          "document_id": "1909.09270",
          "start_idx": 14042,
          "end_idx": 14768
        },
        "page_content": "These languages are: Amharic (amh: LDC2016E87), Arabic (ara: LDC2016E89), Hindi (hin: LDC2017E62), and Somali (som: LDC2016E91). These come from a variety of sources including discussion forums, newswire, and social media. The labelset is Person, Organization, Location, Geo-political entity. We define train/development/test splits, taking care to keep a similar distribution of genres in each split. Data statistics for all languages are shown in Table TABREF25. We create partial annotations by perturbing gold annotated data in two ways: lowering recall (to simulate missing entities), and lowering precision (to simulate noisy annotations). To lower recall, we replace gold named entity tags with $O$ tags (for non-name).",
        "type": "Document"
      },
      {
        "id": "71ab2f60-065f-4932-8b5f-6103425e923c",
        "metadata": {
          "vector_store_key": "1911.03562-4",
          "chunk_id": 32,
          "document_id": "1911.03562",
          "start_idx": 17312,
          "end_idx": 18029
        },
        "page_content": "These include: Chinese, Arabic, Korean, Japanese, and Hindi (Asian) as well as French, German, Swedish, Spanish, Portuguese, and Italian (European). This is followed by the relatively less widely spoken European languages (such as Russian, Polish, Norwegian, Romanian, Dutch, and Czech) and Asian languages (such as Turkish, Thai, and Urdu). Most of the well-represented languages are from the Indo-European language family. Yet, even in the limited landscape of the most common 122 languages, vast swathes are barren with inattention. Notable among these is the extremely low representation of languages from Africa, languages from non-Indo-European language families, and Indigenous languages from around the world.",
        "type": "Document"
      },
      {
        "id": "c28fe057-2cfc-4978-9c0a-84fd286d4b98",
        "metadata": {
          "vector_store_key": "1612.06897-3",
          "chunk_id": 39,
          "document_id": "1612.06897",
          "start_idx": 20315,
          "end_idx": 20488
        },
        "page_content": "We presented results on two diverse language pairs German $\\rightarrow $ English and Chinese $\\rightarrow $ English (usually very challenging pairs for machine translation).",
        "type": "Document"
      },
      {
        "id": "a22cce10-cf7d-4a75-a5a2-99d8a3f066ee",
        "metadata": {
          "vector_store_key": "1705.03151-5",
          "chunk_id": 46,
          "document_id": "1705.03151",
          "start_idx": 25347,
          "end_idx": 26030
        },
        "page_content": "These are: Mandarin, Cantonese, Indonesian, Japanese, Russian, Korean and Vietnamese. The data volume for each language is approximately 10 hours of speech signals recorded by 24 speakers (12 males and 12 females), with each speaker recording approximately 300 utterances in reading style by mobile phones, with a sampling rate of 16kHz and a sample size of 16 bits. Each dataset was split into a training set consisting of 18 speakers, and a test set consisting of 6 speakers. For Mandarin, Cantonese, Vietnamese and Indonesian, the recording was conducted in a quiet environment. For Russian, Korean and Japanese, there are 2 recording conditions for each speaker, quiet and noisy.",
        "type": "Document"
      },
      {
        "id": "cb4078b8-00d7-4e01-a91b-7292c78d6351",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 12,
          "document_id": "1910.04269",
          "start_idx": 7197,
          "end_idx": 7983
        },
        "page_content": "The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel). Several state-of-the-art results on various audio classification tasks have been obtained by using log-Mel spectrograms of raw audio, as features BIBREF19. Convolutional Neural Networks have demonstrated an excellent performance gain in classification of these features BIBREF20, BIBREF21 against other machine learning techniques. It has been shown that using attention layers with ConvNets further enhanced their performance BIBREF22. This motivated us to develop a CNN-based architecture with attention since this approach hasn\u2019t been applied to the task of language identification before.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How they evaluate their approach?",
    "llm_answer": "They evaluate their approach by: 1) using human evaluators to assess the balance of positive/negative sentiment, topic range, and relatability of dialogs; 2) having human experts evaluate the quality of prescriptions, considering potential curative effect, not just literal similarity; 3) comparing model performance based on how often a metric selected the best hyperparameters/early-stopping checkpoint, and the average human score for those selected models; 4) using human experts familiar with the Oshiete-goo QA community to evaluate AI system outputs (content match, grammar, suitability) on unseen questions.\n",
    "context": [
      {
        "id": "4be44773-b407-4314-abd0-d9c32e99174b",
        "metadata": {
          "vector_store_key": "1908.07816-1",
          "chunk_id": 7,
          "document_id": "1908.07816",
          "start_idx": 4025,
          "end_idx": 4656
        },
        "page_content": "We consider factors such as the balance of positive and negative sentiments in test dialogs, a well-chosen range of topics, and dialogs that our human evaluators can relate. It is the first time such an approach is designed with consideration for the human judges. Our main goal is to increase the objectivity of the results and reduce judges' mistakes due to out-of-context dialogs they have to evaluate. The rest of the paper unfolds as follows. Section SECREF2 discusses some related work. In Section SECREF3, we give detailed description of the methodology. We present experimental results and some analysis in Section SECREF4.",
        "type": "Document"
      },
      {
        "id": "1c2721da-ef50-4f82-92a2-9f11a92306e0",
        "metadata": {
          "vector_store_key": "2002.05058-2",
          "chunk_id": 49,
          "document_id": "2002.05058",
          "start_idx": 27267,
          "end_idx": 27919
        },
        "page_content": "As described in the human evaluation procedure, we perform 10 runs to test the reliability of each metric when used to perform hyperparameter tuning and early-stopping respectively. In each run, we select the best hyperparameter combination or early-stopping checkpoint based on each of the five compared metrics. Human evaluation is then employed to identify the best choice. We evaluate the performance of each metric by how many times (out of 10) they succeeded in selecting the best hyperparameter combination or early-stopping checkpoint (out of 4) and the average human-annotated score for their selected models. The results are shown in Table 3.",
        "type": "Document"
      },
      {
        "id": "4c32b91e-41de-4214-8907-1d3f59805d18",
        "metadata": {
          "vector_store_key": "1801.09030-4",
          "chunk_id": 38,
          "document_id": "1801.09030",
          "start_idx": 19834,
          "end_idx": 20571
        },
        "page_content": "Both of the professors enjoy over five years of practicing traditional Chinese medical treatment. The evaluators are asked to evaluate the prescriptions with scores between 0 and 10. Both the textual symptoms and the standard reference are given, which is similar to the form of evaluation in a normal TCM examination. Different from the automatic evaluation method, the human evaluators focus on the potential curative effect of the candidate answers, rather than merely the literal similarity. We believe this way of evaluation is much more reasonable and close to reality. Because the evaluation procedure is very time consuming (each item requires more than 1 minute), we only ask the evaluators to judge the results from test set 2.",
        "type": "Document"
      },
      {
        "id": "4e761f78-ffd6-4ccb-9a33-daac9a9849b1",
        "metadata": {
          "vector_store_key": "1912.00864-7",
          "chunk_id": 46,
          "document_id": "1912.00864",
          "start_idx": 26721,
          "end_idx": 27480
        },
        "page_content": "Following evaluations made by crowdsourced evaluators BIBREF29, we conducted human evaluations to judge the outputs of CLSTM and those of NAGM. Different from BIBREF29, we hired human experts who had experience in Oshiete-goo QA community service. Thus, they were familiar with the sorts of answers provided by and to the QA community. The experts asked questions, which were not included in our training datasets, to the AI system and rated the answers; one answer per question. The experts rated the answers as follows: (1) the content of the answer matched the question, and the grammar was okay; (2) the content was suitable, but the grammar was poor; (3) the content was not suitable, but the grammar was okay; (4) both the content and grammar were poor.",
        "type": "Document"
      },
      {
        "id": "cb140e2d-7de1-4242-97b5-d5a79a157ca5",
        "metadata": {
          "vector_store_key": "1912.00864-7",
          "chunk_id": 48,
          "document_id": "1912.00864",
          "start_idx": 27772,
          "end_idx": 28540
        },
        "page_content": "The evaluators were given more than a week to carefully evaluate the generated answers, so we consider that their judgments are reliable. Each expert evaluated 50 questions. We combined the scores of the experts by summing them. They did not know the identity of the system in the evaluation and reached their decisions independently. Table TABREF22 and Table TABREF22 present the results. The numbers are percentages. Table 7 presents examples of questions and answers. For Oshiete-goo results, the original Japanese and translated English are presented. The questions are very long and include long background descriptions before the questions themselves. These results indicate that the experts were much more satisfied with the outputs of NAGM than those of CLSTM.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How large is the corpus?",
    "llm_answer": "9.7 million words\n",
    "context": [
      {
        "id": "248d0947-e589-4dce-a602-15e3c4af536e",
        "metadata": {
          "vector_store_key": "1710.06700-4",
          "chunk_id": 12,
          "document_id": "1710.06700",
          "start_idx": 6389,
          "end_idx": 7092
        },
        "page_content": "This diacritized corpus was created by a commercial vendor and contains 9.7 million words with almost 200K unique surface words. About 73% of the corpus is in MSA and covers variety of genres like politics, economy, sports, society, etc. and the remaining part is mostly religious texts written in classical Arabic (CA). The effectiveness of using this corpus in building state-of-the-art diacritizer was proven in BIBREF10 . For example, the word \u00d9\u0088\u00d8\u00a8\u00d9\u0086\u00d9\u0088\u00d8\u00af> (wbnwd) \u201cand items\u201d is found 4 times in this corpus with two full diacritization forms \u00d9\u0088\u00d9\u008e\u00d8\u00a8\u00d9\u008f\u00d9\u0086\u00d9\u008f\u00d9\u0088\u00d8\u00af\u00d9\u0090\u00d8\u008c \u00d9\u0088\u00d9\u008e\u00d8\u00a8\u00d9\u008f\u00d9\u0086\u00d9\u008f\u00d9\u0088\u00d8\u00af\u00d9\u008d> (wabunudi, wabunudK) \u201citems, with different grammatical case endings\u201d which appeared 3 times and once respectively.",
        "type": "Document"
      },
      {
        "id": "3f796a82-78d4-4d2f-8165-b133aeaab300",
        "metadata": {
          "vector_store_key": "1911.12579-0",
          "chunk_id": 22,
          "document_id": "1911.12579",
          "start_idx": 12899,
          "end_idx": 13803
        },
        "page_content": "The corpus is a collection of human language text BIBREF31 built with a specific purpose. However, the statistical analysis of the corpus provides quantitative, reusable data, and an opportunity to examine intuitions and ideas about language. Therefore, the corpus has great importance for the study of written language to examine the text. In fact, realizing the necessity of large text corpus for Sindhi, we started this research by collecting raw corpus from multiple web resource using web-scrappy framwork for extraction of news columns of daily Kawish and Awami Awaz Sindhi newspapers, Wikipedia dumps, short stories and sports news from Wichaar social blog, news from Focus Word press blog, historical writings, novels, stories, books from Sindh Salamat literary websites, novels, history and religious books from Sindhi Adabi Board and tweets regarding news and sports are collected from twitter.",
        "type": "Document"
      },
      {
        "id": "51221f55-649a-490a-869b-65b63c24ed2a",
        "metadata": {
          "vector_store_key": "2001.11381-3",
          "chunk_id": 32,
          "document_id": "2001.11381",
          "start_idx": 18090,
          "end_idx": 18812
        },
        "page_content": "El corpus es analizado frase a frase, reemplazando cada palabra por su respectiva etiqueta POS. Al final del an\u00e1lisis, se obtiene un nuevo corpus 8KPOS con $s = 7~679$ secuencias de etiquetas POS, correspondientes al mismo n\u00famero de frases del corpus 8KF. Las secuencias del corpus 8KPOS sirven como conjunto de entrenamiento para el algoritmo de Viterbi, que calcula las probabilidades de transici\u00f3n, que ser\u00e1n usadas para generar cadenas de Markov. Las $s$ estructuras del corpus 8KPOS procesadas con el algoritmo de Viterbi son representadas en una matriz de transici\u00f3n $P_{[s \\times s]}$. $P$ ser\u00e1 utilizada para crear nuevas secuencias de etiquetas POS no existentes en el corpus 8KPOS, simulando un proceso creativo.",
        "type": "Document"
      },
      {
        "id": "d4ec7692-542b-43bb-923f-1ab312460b92",
        "metadata": {
          "vector_store_key": "1709.05404-4",
          "chunk_id": 29,
          "document_id": "1709.05404",
          "start_idx": 14491,
          "end_idx": 15168
        },
        "page_content": "The final hyperbole dataset consists of 582 posts per class (1,164 posts in total). To recap, Table TABREF10 summarizes the total number of posts for each subset of our final corpus. Our primary goal is not to optimize classification results, but to explore how results vary across different subcorpora and corpus properties. We also aim to demonstrate that the quality of our corpus makes it more straightforward to achieve high classification performance. We apply both supervised learning using SVM (from Scikit-Learn BIBREF25 ) and weakly-supervised linguistic pattern learning using AutoSlog-TS BIBREF13 . These reveal different aspects of the corpus. Supervised Learning.",
        "type": "Document"
      },
      {
        "id": "ba0c799b-df62-4c0e-b054-e5aed4073da4",
        "metadata": {
          "vector_store_key": "1909.00183-5",
          "chunk_id": 25,
          "document_id": "1909.00183",
          "start_idx": 14402,
          "end_idx": 15249
        },
        "page_content": "On the other hand, reducing the size of the corpus from 13 million to 1 million records did not affect the benchmarking dramatically. This robustness of the results to the size of the training corpus was confirmed further with the use of more detailed metrics, as discussed below in Section SECREF27 (see e.g., Figure FIGREF29). Based on our benchmarking, henceforth we use the Doc2Vec model trained on the 13+ million NRLS records with the following hyper-parameters: {training method = dbow, number of dimensions for feature vectors size = 300, number of epochs = 10, window size = 15, minimum count = 5, number of negative samples = 5, random down-sampling threshold for frequent words = 0.001 }. As an indication of computational cost, the training of this model takes approximately 11 hours (run in parallel with 7 threads) on shared servers.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How large is the dataset?",
    "llm_answer": "2.6 GB of raw text.\n",
    "context": [
      {
        "id": "d87d327c-2858-4b5d-ae8a-860d401f63bd",
        "metadata": {
          "vector_store_key": "1908.05434-1",
          "chunk_id": 27,
          "document_id": "1908.05434",
          "start_idx": 14432,
          "end_idx": 15064
        },
        "page_content": "We have ensured that the raw dataset has no overlap with the labeled dataset to avoid bias in test accuracy. While it is possible to scrape more raw data, we did not observe significant improvements in model performances when the size of raw data increased from INLINEFORM0 70,000 to INLINEFORM1 170,000, hence we assume that the current raw dataset is sufficiently large. The labeled dataset is called Trafficking-10k. It consists of 12,350 ads from Backpage labeled by experts in human trafficking detection BIBREF9 . Each label is one of seven ordered levels of likelihood that the corresponding ad comes from a human trafficker.",
        "type": "Document"
      },
      {
        "id": "78e083f1-0666-4414-8066-d724fc284323",
        "metadata": {
          "vector_store_key": "1901.09755-0",
          "chunk_id": 25,
          "document_id": "1901.09755",
          "start_idx": 14080,
          "end_idx": 14670
        },
        "page_content": "For English, it should be noted that the size of the 2015 set is less than half with respect to the 2014 dataset in terms of tokens, and only one third in number of targets. The French, Spanish and Dutch datasets are quite similar in terms of tokens although the number of targets in the Dutch dataset is comparatively smaller, possibly due to the tendency to construct compound terms in that language. The Russian dataset is the largest whereas the Turkish set is by far the smallest one. Additionally, we think it is also interesting to note the low number of targets that are multiwords.",
        "type": "Document"
      },
      {
        "id": "d737a5bf-ce39-4820-99fc-0f50564ca1a7",
        "metadata": {
          "vector_store_key": "2001.09332-2",
          "chunk_id": 15,
          "document_id": "2001.09332",
          "start_idx": 7575,
          "end_idx": 8365
        },
        "page_content": "The dataset (composed of 2.6 GB of raw text) includes $421\\,829\\,960$ words divided into $17\\,305\\,401$ sentences. The text was previously preprocessed by removing the words whose absolute frequency was less than 5 and eliminating all special characters. Since it is impossible to represent every imaginable numerical value, but not wanting to eliminate the concept of \u201cnumerical representation\" linked to certain words, it was also decided to replace every number present in the text with the particular $\\langle NUM \\rangle $ token; which probably also assumes a better representation in the embedding space (not separating into the various possible values). All the words were then transformed to lowercase (to avoid a double presence) finally producing a vocabulary of $618\\,224$ words.",
        "type": "Document"
      },
      {
        "id": "44909104-821e-478b-84f1-a36df82c7ece",
        "metadata": {
          "vector_store_key": "1909.08041-5",
          "chunk_id": 23,
          "document_id": "1909.08041",
          "start_idx": 12832,
          "end_idx": 13487
        },
        "page_content": "HotpotQA: This dataset is a recent large-scale QA dataset that brings in new features: (1) the questions require finding and reasoning over multiple documents; (2) the questions are diverse and not limited to pre-existing KBs; (3) it offers a new comparison question type BIBREF4. We experimented our system on HotpotQA in the fullwiki setting, where a system must find the answer to a question in the scope of the entire Wikipedia, an ideal MRS setup. The sizes of the train, dev and test split are 90,564, 7,405, and 7,405. More importantly, HotpotQA also provides human-annotated sentence-level supporting facts that are needed to answer each question.",
        "type": "Document"
      },
      {
        "id": "73a83f60-b2c9-406d-b4cd-5720639e5f06",
        "metadata": {
          "vector_store_key": "1812.07023-4",
          "chunk_id": 18,
          "document_id": "1812.07023",
          "start_idx": 10358,
          "end_idx": 10986
        },
        "page_content": "After 10 such qa turns, the questioner wraps up by writing a summary of the video based on the conversation. Dataset statistics such as the number of dialogues, turns, and words for the avsd dataset are presented in Table TABREF5 . For the initially released prototype dataset, the training set of the avsd dataset corresponds to videos taken from the training set of the Charades dataset while the validation and test sets of the avsd dataset correspond to videos taken from the validation set of the Charades dataset. For the official dataset, training, validation and test sets are drawn from the corresponding Charades sets.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was their perplexity score?",
    "llm_answer": "The answer is not found in the context.  While the text discusses perplexity and its use in evaluating models, it does not provide a specific perplexity score for any model.\n",
    "context": [
      {
        "id": "7d48dfa5-90f7-42e3-96dc-62e890eff036",
        "metadata": {
          "vector_store_key": "1908.07816-0",
          "chunk_id": 35,
          "document_id": "1908.07816",
          "start_idx": 20127,
          "end_idx": 20829
        },
        "page_content": "According to the context given, the raters were instructed to evaluate the quality of the responses based on three criteria: (1) grammatical correctness\u2014whether or not the response is fluent and free of grammatical mistakes; (2) contextual coherence\u2014whether or not the response is context sensitive to the previous dialog history; (3) emotional appropriateness\u2014whether or not the response conveys the right emotion and feels as if it had been produced by a human. For each criterion, the raters gave scores of either 0, 1 or 2, where 0 means bad, 2 means good, and 1 indicates neutral. Table TABREF34 gives the perplexity scores obtained by the three models on the two validation sets and the test set.",
        "type": "Document"
      },
      {
        "id": "5cc44f3d-1c70-4272-8659-7e6139f2cfe3",
        "metadata": {
          "vector_store_key": "1908.07816-0",
          "chunk_id": 36,
          "document_id": "1908.07816",
          "start_idx": 20250,
          "end_idx": 20944
        },
        "page_content": "Table TABREF34 gives the perplexity scores obtained by the three models on the two validation sets and the test set. As shown in the table, MEED achieves the lowest perplexity score on all three sets. We also conducted t-test on the perplexity obtained, and results show significant improvements (with $p$-value $<0.05$). Table TABREF34, TABREF35 and TABREF35 summarize the human evaluation results on the responses' grammatical correctness, contextual coherence, and emotional appropriateness, respectively. In the tables, we give the percentage of votes each model received for the three scores, the average score obtained with improvements over S2S, and the agreement score among the raters.",
        "type": "Document"
      },
      {
        "id": "b6f4fd7e-b628-4869-9189-daeefe92964b",
        "metadata": {
          "vector_store_key": "1909.08824-4",
          "chunk_id": 29,
          "document_id": "1909.08824",
          "start_idx": 16791,
          "end_idx": 17613
        },
        "page_content": "Perplexity measures the probability of model to regenerate the exact targets, which is particular suitable for evaluating the model performance on one-to-many problem BIBREF20. Further, we employ BLEU score to evaluate the accuracy of generations BIBREF21, and the number of distinct n-gram to evaluate the diversity of generations BIBREF6. The distinct is normalized to $[0, 1]$ by dividing the total number of generated tokens. Since automatic evaluation of generations is still a challenging task BIBREF22, we also conduct human evaluations on the model performance. Five human experts are employed to evaluate the coherence, diversity and fluency of generated targets. Experts are asked to vote for if a generation is fluent or coherent for each generated target, and give a 1-5 score for the diversity of generations.",
        "type": "Document"
      },
      {
        "id": "e3d5c602-1b71-4778-b4e1-39dc857ecd54",
        "metadata": {
          "vector_store_key": "1809.08731-4",
          "chunk_id": 19,
          "document_id": "1809.08731",
          "start_idx": 10841,
          "end_idx": 11540
        },
        "page_content": "The score of a sentence $S$ is calculated as  $$\\text{NCE}(S) = \\tfrac{1}{|S|} \\ln (p_M(S))$$   (Eq. 22)  with $p_M(S)$ being the probability assigned to the sentence by a LM. We employ the same LMs as for SLOR, i.e., LMs trained on words (WordNCE) and WordPieces (WPNCE). Our next baseline is perplexity, which corresponds to the exponentiated cross-entropy:  $$\\text{PPL}(S) = \\exp (-\\text{NCE}(S))$$   (Eq. 24)  Due to its popularity, we also performed initial experiments with BLEU BIBREF17 . Its correlation with human scores was so low that we do not consider it in our final experiments. Following earlier work BIBREF2 , we evaluate our metrics using Pearson correlation with human judgments.",
        "type": "Document"
      },
      {
        "id": "15117d1f-cd68-482e-8139-c1c532853aee",
        "metadata": {
          "vector_store_key": "2004.03744-0",
          "chunk_id": 40,
          "document_id": "2004.03744",
          "start_idx": 21943,
          "end_idx": 22676
        },
        "page_content": "For EtP-BUTD-VE, perplexity is used to select the best model parameters at validation time. It is computed between the explanations produced by the LSTM and ground truth explanations from the validation set. When we train ExplToLabel-VE on e-SNLI-VE-2.0, we obtain a balanced accuracy of 90.55% on the test set. As reported in Table TABREF30, the overall PaE-BUTD-VE system achieves 69.40% balanced accuracy on the test set of e-SNLI-VE-2.0, which is a 3% decrease from the non-explanatory BUTD counterpart (72.52%). However, by setting $\\alpha $ to zero and selecting the model that gives the best perplexity per word at validation, the quality of explanation significantly increased, with 35% relevance, based on manual evaluation.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What parallel corpus did they use?",
    "llm_answer": "Europarl Fr-En and En-De parallel corpora.\n",
    "context": [
      {
        "id": "b4ddc5a8-ae87-453f-a3e5-8abee1ffd2b8",
        "metadata": {
          "vector_store_key": "1704.00253-3",
          "chunk_id": 14,
          "document_id": "1704.00253",
          "start_idx": 8347,
          "end_idx": 9041
        },
        "page_content": "The corpus can be formed by back-translating a target-side monolingual corpus into the source language BIBREF0 . Similar to the source-originated case, it can be built from a pivot-target bilingual corpus using a pivot-to-source translation model BIBREF3 . The process of building each synthetic parallel corpus is illustrated in Figure 1 . As shown in Figure 1 , the previous studies on pseudo parallel data share a common property: synthetic and ground truth sentences are biased on a single side of sentence pairs. In such a case where the synthetic parallel data are the only or major resource used to train NMT, this may severely limit the availability of the given pseudo parallel corpus.",
        "type": "Document"
      },
      {
        "id": "de2465b8-bc43-4557-b11e-b5ad1b2f0d72",
        "metadata": {
          "vector_store_key": "1909.00279-3",
          "chunk_id": 23,
          "document_id": "1909.00279",
          "start_idx": 13381,
          "end_idx": 14083
        },
        "page_content": "(Section SECREF27) To this end, we built a dataset as described in Section SECREF18. Evaluation metrics and baselines are described in Section SECREF21 and SECREF22. For the implementation details of building the dataset and models, please refer to supplementary materials. Training and Validation Sets We collected a corpus of poems and a corpus of vernacular literature from online resources. The poem corpus contains 163K quatrain poems from Tang Poems and Song Poems, the vernacular literature corpus contains 337K short paragraphs from 281 famous books, the corpus covers various literary forms including prose, fiction and essay. Note that our poem corpus and a vernacular corpus are not aligned.",
        "type": "Document"
      },
      {
        "id": "d93d5861-5809-44f4-8606-4758bdf6945e",
        "metadata": {
          "vector_store_key": "1704.00253-3",
          "chunk_id": 19,
          "document_id": "1704.00253",
          "start_idx": 11360,
          "end_idx": 12079
        },
        "page_content": "Mixing pseudo parallel corpora derived from different sources, however, inevitably brings diversity, which affects the capacity of the resulting corpus. We isolate this factor by building both source- and target-originated synthetic corpora from the identical source-to-target real parallel corpus. Our experiments are performed on French (Fr) $\\leftrightarrow $ German (De) translation tasks. Throughout the remaining paper, we use the notation * to denote the synthetic part of the pseudo sentence pairs. By choosing English (En) as the pivot language, we perform pivot alignments for identical English segments on Europarl Fr-En and En-De parallel corpora BIBREF18 , constructing a multi-parallel corpus of Fr-En-De.",
        "type": "Document"
      },
      {
        "id": "2d11bd5b-76dc-4a06-b568-28f996b411ab",
        "metadata": {
          "vector_store_key": "2001.11381-3",
          "chunk_id": 25,
          "document_id": "2001.11381",
          "start_idx": 14044,
          "end_idx": 14881
        },
        "page_content": "Estas son, sin embargo, las condiciones que presenta un corpus literario real. Un corpus heterog\u00e9neo de casi 8 000 frases literarias fue constituido manualmente a partir de poemas, discursos, citas, cuentos y otras obras. Se evitaron cuidadosamente las frases de lengua general, y tambi\u00e9n aquellas demasiado cortas ($N \\le 3$ palabras) o demasiado largas ($N \\ge 30$ palabras). El vocabulario empleado es complejo y est\u00e9tico, adem\u00e1s que el uso de ciertas figuras literarias como la rima, la an\u00e1fora, la met\u00e1fora y otras pueden ser observadas en estas frases. Las caracter\u00edsticas del corpus 8KF se muestran en la Tabla TABREF6. Este corpus fue utilizado principalmente en los dos modelos generativos: modelo basado en cadenas de Markov (Secci\u00f3n SECREF13) y modelo basado en la generaci\u00f3n de Texto enlatado (Canned Text, Secci\u00f3n SECREF15).",
        "type": "Document"
      },
      {
        "id": "3f796a82-78d4-4d2f-8165-b133aeaab300",
        "metadata": {
          "vector_store_key": "1911.12579-0",
          "chunk_id": 22,
          "document_id": "1911.12579",
          "start_idx": 12899,
          "end_idx": 13803
        },
        "page_content": "The corpus is a collection of human language text BIBREF31 built with a specific purpose. However, the statistical analysis of the corpus provides quantitative, reusable data, and an opportunity to examine intuitions and ideas about language. Therefore, the corpus has great importance for the study of written language to examine the text. In fact, realizing the necessity of large text corpus for Sindhi, we started this research by collecting raw corpus from multiple web resource using web-scrappy framwork for extraction of news columns of daily Kawish and Awami Awaz Sindhi newspapers, Wikipedia dumps, short stories and sports news from Wichaar social blog, news from Focus Word press blog, historical writings, novels, stories, books from Sindh Salamat literary websites, novels, history and religious books from Sindhi Adabi Board and tweets regarding news and sports are collected from twitter.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "In which setting they achieve the state of the art?",
    "llm_answer": "SemEval-2015 and SemEval-2016.\n",
    "context": [
      {
        "id": "9806de68-7d6d-44f9-adb5-401dfa84457f",
        "metadata": {
          "vector_store_key": "1908.02402-4",
          "chunk_id": 55,
          "document_id": "1908.02402",
          "start_idx": 29604,
          "end_idx": 29912
        },
        "page_content": "The experiment suggests that this architecture is competitive with state-of-the-art models, while at the same time providing a more practical solution for real-world applications. We would like to thank Alexandros Papangelis, Janice Lam, Stefan Douglas Webb and SIGDIAL reviewers for their valuable comments.",
        "type": "Document"
      },
      {
        "id": "a6dff15d-6323-4ff2-9d1f-f503bd1a9ef5",
        "metadata": {
          "vector_store_key": "1710.01492-5",
          "chunk_id": 35,
          "document_id": "1710.01492",
          "start_idx": 19077,
          "end_idx": 19782
        },
        "page_content": "Traditionally, the above features were fed into classifiers such as Maximum Entropy (MaxEnt) and Support Vector Machines (SVM) with various kernels. However, observation over the SemEval Twitter sentiment task in recent years shows growing interest in, and by now clear dominance of methods based on deep learning. In particular, the best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks BIBREF53 , BIBREF54 . Conversely, kernel machines seem to be less frequently used than in the past, and the use of learning methods other than the ones mentioned above is at this point scarce. All these models are examples of supervised learning as they need labeled training data.",
        "type": "Document"
      },
      {
        "id": "18eced3a-fbf6-40a7-b546-fe11d4346eb5",
        "metadata": {
          "vector_store_key": "1701.09123-9",
          "chunk_id": 27,
          "document_id": "1701.09123",
          "start_idx": 14490,
          "end_idx": 15257
        },
        "page_content": "Therefore, we will summarize those approaches that are most relevant to our work, especially those we will directly compared with in Section SECREF4 . Since CoNLL shared tasks, the most competitive approaches have been supervised systems learning CRF, SVM, Maximum Entropy or Averaged Perceptron models. In any case, while the machine learning method is important, it has also been demonstrated that good performance might largely be due to the feature set used BIBREF45 . Table TABREF13 provides an overview of the features used by previous best scoring approaches for each of the five languages we address in this paper. Traditionally, local features have included contextual and orthographic information, affixes, character-based features, prediction history, etc.",
        "type": "Document"
      },
      {
        "id": "904a361a-2d3c-41fa-8807-2303b37ca27b",
        "metadata": {
          "vector_store_key": "1603.04553-3",
          "chunk_id": 21,
          "document_id": "1603.04553",
          "start_idx": 11450,
          "end_idx": 12153
        },
        "page_content": "To make a thorough empirical comparison with previous studies, Table 3 (below the dashed line) also shows the results of some state-of-the-art supervised coreference resolution systems \u2014 IMS: the second best system in the CoNLL 2012 shared task BIBREF28 ; Latent-Tree: the latent tree model BIBREF29 obtaining the best results in the shared task; Berkeley: the Berkeley system with the final feature set BIBREF12 ; LaSO: the structured perceptron system with non-local features BIBREF30 ; Latent-Strc: the latent structure system BIBREF31 ; Model-Stack: the entity-centric system with model stacking BIBREF32 ; and Non-Linear: the non-linear mention-ranking model with feature representations BIBREF33 .",
        "type": "Document"
      },
      {
        "id": "84311e92-1a2e-4e47-b34d-d3e1f63e5141",
        "metadata": {
          "vector_store_key": "1910.08210-5",
          "chunk_id": 31,
          "document_id": "1910.08210",
          "start_idx": 17119,
          "end_idx": 17791
        },
        "page_content": "Moreover, no ablated variant is able to solve the tasks\u2014it is the combination of ablated features that enables to win consistently. Qualitatively, the ablated variants converge to locally optimum policies in which the agent often picks up a random item and then attacks the correct monster, resulting in a $\\sim 50$% win rate. Table FIGREF29 shows that all models, with the exception of the CNN baseline, generalise to new evaluation environments with dynamics and world configurations not seen during training, with outperforming FiLM and the CNN model. We find similar results for , its ablated variants, and baselines on other tasks (see appendix SECREF11 for details).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What they formulate the question generation as?",
    "llm_answer": "The task of automatically generating questions from various inputs such as raw text, database, or semantic representation.\n",
    "context": [
      {
        "id": "3fcc3bd0-34a7-465a-9253-d631267d3868",
        "metadata": {
          "vector_store_key": "1905.08949-0",
          "chunk_id": 0,
          "document_id": "1905.08949",
          "start_idx": 0,
          "end_idx": 664
        },
        "page_content": "Question Generation (QG) concerns the task of \u201cautomatically generating questions from various inputs such as raw text, database, or semantic representation\" BIBREF0 . People have the ability to ask rich, creative, and revealing questions BIBREF1 ; e.g., asking Why did Gollum betray his master Frodo Baggins? after reading the fantasy novel The Lord of the Rings. How can machines be endowed with the ability to ask relevant and to-the-point questions, given various inputs? This is a challenging, complementary task to Question Answering (QA). Both QA and QG require an in-depth understanding of the input source and the ability to reason over relevant contexts.",
        "type": "Document"
      },
      {
        "id": "52cb3d3c-cc4d-4722-a47e-d4547e2150a5",
        "metadata": {
          "vector_store_key": "1902.11049-0",
          "chunk_id": 0,
          "document_id": "1902.11049",
          "start_idx": 0,
          "end_idx": 731
        },
        "page_content": "Posing questions about a document in natural language is a crucial aspect of the effort to automatically process natural language data, enabling machines to ask clarification questions BIBREF0 , become more robust to queries BIBREF1 , and to act as automatic tutors BIBREF2 . Recent approaches to question generation have used Seq2Seq BIBREF3 models with attention BIBREF4 and a form of copy mechanism BIBREF5 , BIBREF6 . Such models are trained to generate a plausible question, conditioned on an input document and answer span within that document BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 . There are currently no dedicated question generation datasets, and authors have used the context-question-answer triples available in SQuAD.",
        "type": "Document"
      },
      {
        "id": "4b089285-b3b9-42b5-8910-f089dc14d3d3",
        "metadata": {
          "vector_store_key": "1605.08675-9",
          "chunk_id": 31,
          "document_id": "1605.08675",
          "start_idx": 17446,
          "end_idx": 18317
        },
        "page_content": "The goal of question analysis is to examine a question and extract all the information that suffices for answer finding. A resulting data structure, called question model, contains the following elements: Question type \u2013 a description of expected answer type, instructing the system, what type of data could be returned as an answer. It has three levels of specificity: General question type \u2013 one of the types of factoid questions, enumerated at the beginning of this chapter, Named entity type \u2013 applicable only in case general type equals named entity. Possible values are the following: place, continent, river, lake, mountain, mountain range, island, archipelago, sea, celestial body, country, state, city, nationality, person, first name, last name, band, dynasty, organisation, company, event, date, century, year, period, number, quantity, vehicle, animal, title.",
        "type": "Document"
      },
      {
        "id": "af0f5b4e-ba5a-422d-9235-4155e849896d",
        "metadata": {
          "vector_store_key": "1905.08949-7",
          "chunk_id": 6,
          "document_id": "1905.08949",
          "start_idx": 3366,
          "end_idx": 4159
        },
        "page_content": "Given a sentence or a paragraph as input, content selection selects a particular salient topic worthwhile to ask about and determines the question type (What, When, Who, etc.). Approaches either take a syntactic BIBREF11 , BIBREF12 , BIBREF13 or semantic BIBREF14 , BIBREF3 , BIBREF15 , BIBREF16 tack, both starting by applying syntactic or semantic parsing, respectively, to obtain intermediate symbolic representations. Question construction then converts intermediate representations to a natural language question, taking either a tranformation- or template-based approach. The former BIBREF17 , BIBREF18 , BIBREF13 rearranges the surface form of the input sentence to produce the question; the latter BIBREF19 , BIBREF20 , BIBREF21 generates questions from pre-defined question templates.",
        "type": "Document"
      },
      {
        "id": "dc06247e-764c-408c-838d-7c962c1d5917",
        "metadata": {
          "vector_store_key": "1806.04524-2",
          "chunk_id": 5,
          "document_id": "1806.04524",
          "start_idx": 2865,
          "end_idx": 3564
        },
        "page_content": "Our goal is to effectively provide an adaptive learning experience in terms of style and difficulty, and thus better serve users' needs BIBREF9 . In this context, we study the ability of our proposed architectures in learning to generate quizzes based on data derived of the interaction of users with the platform. The problem of fill-in-the-blank question generation has been studied in the past by several authors. Perhaps the earlies approach is by BIBREF1 , who proposed a cloze question generation system which focuses on distractor generation using search engines to automatically measure English proficiency. In the same research line, we also find the work of BIBREF2 , BIBREF3 and BIBREF4 .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Was the degree of offensiveness taken as how generally offensive the text was, or how personally offensive it was to the annotator?",
    "llm_answer": "The context states that offensiveness was rated on a 6-point Likert scale, but does not specify whether it was judged as general or personal offensiveness.\n",
    "context": [
      {
        "id": "041e3822-9297-4b03-9677-1fb0d490ad71",
        "metadata": {
          "vector_store_key": "1910.03814-4",
          "chunk_id": 20,
          "document_id": "1910.03814",
          "start_idx": 11210,
          "end_idx": 11855
        },
        "page_content": "This is indeed a subjective task, highly dependent on the annotator convictions and sensitivity. However, we expect to get cleaner annotations the more strong the attack is, which are the publications we are more interested on detecting. We also detected that several users annotate tweets for hate speech just by spotting slur. As already said previously, just the use of particular words can be offensive to many people, but this is not the task we aim to solve. We have not included in our experiments those hits that were made in less than 3 seconds, understanding that it takes more time to grasp the multimodal context and make a decision.",
        "type": "Document"
      },
      {
        "id": "43c47d12-e2ca-4181-a435-74b3a5c29e68",
        "metadata": {
          "vector_store_key": "1911.03842-0",
          "chunk_id": 7,
          "document_id": "1911.03842",
          "start_idx": 4144,
          "end_idx": 4826
        },
        "page_content": "If annotators selected that the content was offensive or maybe offensive, they were asked to place it in one of four categories \u2013 racist, sexist, classist, other \u2013 and to provide a reason for their response. Just over 2% of personas were flagged by at least one annotator, and these personas are removed from the dataset. We further examined gender bias in personas. Annotators were asked to label the gender of each character based on their persona description (choosing \u201cneutral\" if it was not explicit in the persona). This annotation is possible because some personas include lines such as I am a young woman, although the majority of personas do not mention an explicit gender.",
        "type": "Document"
      },
      {
        "id": "1339527f-0d22-49ff-a867-6945112829bd",
        "metadata": {
          "vector_store_key": "1701.08118-1",
          "chunk_id": 11,
          "document_id": "1701.08118",
          "start_idx": 5835,
          "end_idx": 6474
        },
        "page_content": "After these filtering steps, our corpus consists of 541 tweets, none of which are duplicates, contain links or pictures, or are retweets or replies. As a first measurement of the frequency of hate speech in our corpus, we personally annotated them based on our previous expertise. The 541 tweets were split into six parts and each part was annotated by two out of six annotators in order to determine if hate speech was present or not. The annotators were rotated so that each pair of annotators only evaluated one part. Additionally the offensiveness of a tweet was rated on a 6-point Likert scale, the same scale used later in the study.",
        "type": "Document"
      },
      {
        "id": "1fae7f9d-4880-45f6-b480-184901d92b22",
        "metadata": {
          "vector_store_key": "1701.08118-0",
          "chunk_id": 3,
          "document_id": "1701.08118",
          "start_idx": 1735,
          "end_idx": 2454
        },
        "page_content": "A given statement may be considered hate speech or not depending on someone's cultural background and personal sensibilities. The wording of the question may also play a role. We decided to investigate the issue of reliability further by conducting a more comprehensive study across a large number of annotators, which we present in this paper. Our contribution in this paper is threefold: For the purpose of building a classifier, warner2012 define hate speech as \u201cabusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation\u201d. More recent approaches rely on lists of guidelines such as a tweet being hate speech if it \u201cuses a sexist or racial slur\u201d BIBREF2 .",
        "type": "Document"
      },
      {
        "id": "d74062ce-8ee9-4a70-b018-8c7f29ccf785",
        "metadata": {
          "vector_store_key": "1701.08118-0",
          "chunk_id": 2,
          "document_id": "1701.08118",
          "start_idx": 1017,
          "end_idx": 1735
        },
        "page_content": "This data is typically obtained by manually annotating a set of texts based on whether a certain element contains hate speech. The reliability of the human annotations is essential, both to ensure that the algorithm can accurately learn the characteristics of hate speech, and as an upper bound on the expected performance BIBREF1 , BIBREF2 . As a preliminary step, six annotators rated 469 tweets. We found that agreement was very low (see Section 3). We then carried out group discussions to find possible reasons. They revealed that there is considerable ambiguity in existing definitions. A given statement may be considered hate speech or not depending on someone's cultural background and personal sensibilities.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which embeddings do they detect biases in?",
    "llm_answer": "Word embeddings.\n",
    "context": [
      {
        "id": "756e4124-8036-48fd-939b-1f692dc4d9d2",
        "metadata": {
          "vector_store_key": "1905.09866-2",
          "chunk_id": 14,
          "document_id": "1905.09866",
          "start_idx": 7056,
          "end_idx": 7785
        },
        "page_content": "But we know that the system isn't allowed to return this candidate, since the original analogy code rules out the possibility of returning as D any of the query terms INLINEFORM0 , making it impossible to obtain man is to doctor as woman is to doctor (where INLINEFORM1 ). This means that the bias isn't necessarily (or at least not only) in the representations themselves, rather in the way we query them. So, what do the embedding spaces actually tell if you let them return any word in the vocabulary? We took a selection of mainstream, striking examples from the literature on embedding bias, and tested them fairly, without posing any constraint on the returned term, exactly as we did for all analogies in Section SECREF3 .",
        "type": "Document"
      },
      {
        "id": "697e6717-4881-4748-bf0a-d02bb156ba19",
        "metadata": {
          "vector_store_key": "1905.09866-2",
          "chunk_id": 15,
          "document_id": "1905.09866",
          "start_idx": 7785,
          "end_idx": 8457
        },
        "page_content": "We took a selection of mainstream, striking examples from the literature on embedding bias, and tested them fairly, without posing any constraint on the returned term, exactly as we did for all analogies in Section SECREF3 . In Table TABREF9 we report these examples, organised by the papers which discussed them, together with the returned term as reported in the paper itself, and the top two terms returned when using our modified code (1st and 2nd, respectively). Each example is tested over the same embedding space used in the corresponding paper. What immediately stands out is that, bar a few exceptions, we do not obtain the term reported in the respective paper.",
        "type": "Document"
      },
      {
        "id": "1903e4a1-a3c7-4902-bbff-006975f9dbe8",
        "metadata": {
          "vector_store_key": "1910.14497-0",
          "chunk_id": 0,
          "document_id": "1910.14497",
          "start_idx": 0,
          "end_idx": 882
        },
        "page_content": "Word embeddings, or vector representations of words, are an important component of Natural Language Processing (NLP) models and necessary for many downstream tasks. However, word embeddings, including embeddings commonly deployed for public use, have been shown to exhibit unwanted societal stereotypes and biases, raising concerns about disparate impact on axes of gender, race, ethnicity, and religion BIBREF0, BIBREF1. The impact of this bias has manifested in a range of downstream tasks, ranging from autocomplete suggestions BIBREF2 to advertisement delivery BIBREF3, increasing the likelihood of amplifying harmful biases through the use of these models. The most well-established method thus far for mitigating bias relies on projecting target words onto a bias subspace (such as a gender subspace) and subtracting out the difference between the resulting distances BIBREF0.",
        "type": "Document"
      },
      {
        "id": "39941890-4b47-4c9f-b845-ff22eb626ab0",
        "metadata": {
          "vector_store_key": "1905.09866-0",
          "chunk_id": 29,
          "document_id": "1905.09866",
          "start_idx": 14777,
          "end_idx": 15571
        },
        "page_content": "As a side note, we would like to mention that in an earlier version of their paper, BIBREF18 accidentally searched for the inverse of the intended query, and still managed to find biased examples. This seems to be a further, strong, indication that strategies like this are not fully suitable to demonstrate the presence of bias in embeddings. If analogies might not be the most appropriate tool to capture certain relations, surely matters have been made worse by selecting results in order to prove (and emphasise) the presence of human bias. Using such sensational \u201cparty tricks\" BIBREF4 is harmful, as they get easily propagated both in science itself BIBREF19 , BIBREF20 , BIBREF21 , even outside NLP and AI BIBREF22 , as well as in popularised articles of the calibre of Nature BIBREF23 .",
        "type": "Document"
      },
      {
        "id": "75a9f85b-d5cb-4559-9dc6-b4a5f2b694ec",
        "metadata": {
          "vector_store_key": "1905.09866-2",
          "chunk_id": 5,
          "document_id": "1905.09866",
          "start_idx": 2717,
          "end_idx": 3404
        },
        "page_content": "As far as we know, this is the first work that evaluates and reports analogies in an unrestricted fashion, since the analogy code is always used as is. Our experiments and results suggest that the mainstream examples as well as the use of the analogy task itself as a tool to detect bias should be revised and reconsidered. Warning This work does not mean at all to downplay the presence and danger of human biases in word embeddings. On the contrary: embeddings do encode human biases, and we believe that this issue deserves the full attention of the field. However, we also believe that overemphasising and specifically seeking biases to achieve sensational results is not beneficial.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the two PharmaCoNER subtasks?",
    "llm_answer": "Named Entity Recognition (NER) and Relation Extraction (RE).\n",
    "context": [
      {
        "id": "0c22cc86-4d30-4e9d-8a0a-136533b4cda5",
        "metadata": {
          "vector_store_key": "1905.07464-4",
          "chunk_id": 3,
          "document_id": "1905.07464",
          "start_idx": 1864,
          "end_idx": 2670
        },
        "page_content": "Precipitants are defined as substances, drugs, or a drug class involved in an interaction. Task 2 is focused on identifying sentence-level interactions; concretely, the goal is to identify the interacting precipitant, the type of the interaction, and outcome of the interaction. The interaction outcome depends on the interaction type as follows. Pharmacodynamic (PD) interactions are associated with a specified effect corresponding to a span within the text that describes the outcome of the interaction. Naturally, it is possible for a precipitant to be involved in multiple PD interactions. Pharmacokinetic (PK) interactions are associated with a label from a fixed vocabulary of National Cancer Institute (NCI) Thesaurus codes indicating various levels of increase/decrease in functional measurements.",
        "type": "Document"
      },
      {
        "id": "714c2932-8212-4069-872a-bcf848f02a2b",
        "metadata": {
          "vector_store_key": "1905.07464-4",
          "chunk_id": 4,
          "document_id": "1905.07464",
          "start_idx": 2369,
          "end_idx": 3210
        },
        "page_content": "Pharmacokinetic (PK) interactions are associated with a label from a fixed vocabulary of National Cancer Institute (NCI) Thesaurus codes indicating various levels of increase/decrease in functional measurements. For example, consider the sentence: \u201cThere is evidence that treatment with phenytoin leads to to decrease intestinal absorption of furosemide, and consequently to lower peak serum furosemide concentrations.\u201d Here, phenytoin is involved in a PK interaction with the label drug, furosemide, and the type of PK interaction is indicated by the NCI Thesaurus code C54615 which describes a decrease in the maximum serum concentration (C INLINEFORM0 ) of the label drug. Lastly, unspecified (UN) interactions are interactions with an outcome that is not explicitly stated in the text and usually indicated through cautionary statements.",
        "type": "Document"
      },
      {
        "id": "83468c71-7468-4e25-8efd-0a94a3031586",
        "metadata": {
          "vector_store_key": "1905.07464-2",
          "chunk_id": 6,
          "document_id": "1905.07464",
          "start_idx": 3631,
          "end_idx": 4317
        },
        "page_content": "Each drug label is a collection of sections (e.g., DOSAGE & ADMINISTRATION, CONTRAINDICATIONS, and WARNINGS) where each section contains one or more sentences. Each sentence is annotated with a list of zero or more mentions and interactions. The training data released for this task contains 22 drug labels, referred to as Training-22, with gold standard annotations. Two test sets of 57 and 66 drug labels, referred to as Test Set 1 and 2 respectively, with gold standard annotations are used to evaluate participating systems. As Training-22 is a relatively small dataset, we additionally utilize an external dataset with 180 annotated drug labels dubbed NLM-180 BIBREF5 (more later).",
        "type": "Document"
      },
      {
        "id": "4df8c39c-a159-4e61-ad0e-64a6e03ae054",
        "metadata": {
          "vector_store_key": "1912.09152-2",
          "chunk_id": 24,
          "document_id": "1912.09152",
          "start_idx": 12464,
          "end_idx": 13194
        },
        "page_content": "The data structures built during pre-processing are efficiently stored on disk for subsequent runs, so the pre-processing is redone only when resources are edited. According to the organizers, and taking into account the ha of the tiny subset from the background dataset released to the participants, the system obtained the scores presented in table TABREF16, ranking as second best system for subtask1 and best system for subtask2 BIBREF8.. Our results are consistent with our poor understanding of the classes for subtask 1. Having a null knowledge of Pharmacology, Biomedicine or even Chemistry, assigning classes (as requested for subtask 1) to entities is very hard, while providing a sctid (subtask 2) seems an easier goal.",
        "type": "Document"
      },
      {
        "id": "db8dbd71-2891-4819-be9b-543157283c99",
        "metadata": {
          "vector_store_key": "1905.07464-4",
          "chunk_id": 2,
          "document_id": "1905.07464",
          "start_idx": 1201,
          "end_idx": 1864
        },
        "page_content": "The Drug-Drug Interaction Extraction from Drug Labels track of TAC 2018 BIBREF4 , organized by the U.S. Food and Drug Administration (FDA) and U.S. National Library of Medicine (NLM), is established with the goal of transforming the contents of SPLs into a machine-readable format with linkage to standard terminologies. We focus on the first two tasks of the DDI track involving named entity recognition (NER) and relation extraction (RE). Task 1 is focused on identifying mentions in the text corresponding to precipitants, interaction triggers, and interaction effects. Precipitants are defined as substances, drugs, or a drug class involved in an interaction.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they perform data augmentation?",
    "llm_answer": "For text, they use a replacement-based method, replacing words with synonyms, and contextual augmentation using a label-conditional bidirectional language model.  They also simulate real-world revisions by adding, deleting, or replacing words, sentences, paragraphs, section names, and document titles.\n",
    "context": [
      {
        "id": "6dea1fa6-c495-46b4-bdbf-5856cc00f0d6",
        "metadata": {
          "vector_store_key": "1812.06705-2",
          "chunk_id": 0,
          "document_id": "1812.06705",
          "start_idx": 0,
          "end_idx": 733
        },
        "page_content": "Deep neural network-based models are easy to overfit and result in losing their generalization due to limited size of training data. In order to address the issue, data augmentation methods are often applied to generate more training samples. Recent years have witnessed great success in applying data augmentation in the field of speech area BIBREF0 , BIBREF1 and computer vision BIBREF2 , BIBREF3 , BIBREF4 . Data augmentation in these areas can be easily performed by transformations like resizing, mirroring, random cropping, and color shifting. However, applying these universal transformations to texts is largely randomized and uncontrollable, which makes it impossible to ensure the semantic invariance and label correctness.",
        "type": "Document"
      },
      {
        "id": "74417d32-2f59-4419-a417-d81456681a51",
        "metadata": {
          "vector_store_key": "1910.08772-0",
          "chunk_id": 24,
          "document_id": "1910.08772",
          "start_idx": 13739,
          "end_idx": 14545
        },
        "page_content": "For the data augmentation experiment in section SECREF5, we only performed fine-tuning on the corrected SICK. As shown in a recent SICK annotation experiment by kalouli2019explaining, annotation is a complicated issue influenced by linguistic and non-linguistic factors. We leave checking the full SICK dataset to future work. The goal of experiment 1 is to test how accurately MonaLog solves problems in a large-scale dataset. We first used the system to solve the 495 problems in the trial set and then manually identified the cases in which the system failed. Then we determined which syntactic transformations are needed for MonaLog. After improving the results on the trial data by introducing a preprocessing step to handle limited syntactic variation (see below), we applied MonaLog on the test set.",
        "type": "Document"
      },
      {
        "id": "bdb2f0de-d3fe-4b1d-bc62-e80bb51d497e",
        "metadata": {
          "vector_store_key": "1812.06705-2",
          "chunk_id": 1,
          "document_id": "1812.06705",
          "start_idx": 733,
          "end_idx": 1421
        },
        "page_content": "However, applying these universal transformations to texts is largely randomized and uncontrollable, which makes it impossible to ensure the semantic invariance and label correctness. For example, given a movie review \u201cThe actors is good\", by mirroring we get \u201cdoog si srotca ehT\", or by random cropping we get \u201cactors is\", both of which are meaningless. Existing data augmentation methods for text are often loss of generality, which are developed with handcrafted rules or pipelines for specific domains. A general approach for text data augmentation is replacement-based method, which generates new sentences by replacing the words in the sentences with relevant words (e.g. synonyms).",
        "type": "Document"
      },
      {
        "id": "a74c83e2-31a1-4b85-9e14-a34fc39b503c",
        "metadata": {
          "vector_store_key": "1812.06705-2",
          "chunk_id": 2,
          "document_id": "1812.06705",
          "start_idx": 1057,
          "end_idx": 1973
        },
        "page_content": "A general approach for text data augmentation is replacement-based method, which generates new sentences by replacing the words in the sentences with relevant words (e.g. synonyms). However, words with synonyms from a handcrafted lexical database likes WordNet BIBREF5 are very limited , and the replacement-based augmentation with synonyms can only produce limited diverse patterns from the original texts. To address the limitation of replacement-based methods, Kobayashi BIBREF6 proposed contextual augmentation for labeled sentences by offering a wide range of substitute words, which are predicted by a label-conditional bidirectional language model according to the context. But contextual augmentation suffers from two shortages: the bidirectional language model is simply shallow concatenation of a forward and backward model, and the usage of LSTM models restricts their prediction ability to a short range.",
        "type": "Document"
      },
      {
        "id": "6d7e1fda-02df-491f-93a1-de0c2f84c4b0",
        "metadata": {
          "vector_store_key": "1709.01256-4",
          "chunk_id": 36,
          "document_id": "1709.01256",
          "start_idx": 19769,
          "end_idx": 20455
        },
        "page_content": "The generation process of the simulated data sets is designed to mimic the real world. Users open some existing documents in a file system, make some changes (e.g. addition, deletion or replacement), and save them as separate documents. These documents become revisions of the original documents. We started from an initial corpus that did not have revisions, and kept adding new documents and revising existing documents. Similar to a file system, at any moment new documents could be added and/or some of the current documents could be revised. The revision operations we used were deletion, addition and replacement of words, sentences, paragraphs, section names and document titles.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the characteristics of the city dialect?",
    "llm_answer": "City dialects exhibit similar linguistic distances, unlike rural dialects, which are more homogeneous.  City dialects are characterized by a more uniform vocabulary, including most keywords, in contrast to rural dialects that use more specific or unique utterances.\n",
    "context": [
      {
        "id": "5e783686-94b8-4d22-ae45-0e0b6f60868f",
        "metadata": {
          "vector_store_key": "1702.06777-1",
          "chunk_id": 6,
          "document_id": "1702.06777",
          "start_idx": 2732,
          "end_idx": 3467
        },
        "page_content": "We find that, due to language diversity, cities and main towns have similar linguistic distances unlike rural areas, which differ in their homogeneous forms. but obtained with a completely different method Our corpus consists of approximately 11 million geotagged tweets produced in Europe in Spanish language between October 2014 and June 2016. (Although we will focus on Spain, we will not consider in this work the speech of the Canary Islands due to difficulties with the data extraction) . The classification of tweets is accomplished by applying the Compact Language Detector (CLD) BIBREF16 to our dataset. CLD exhibits accurate benchmarks and is thus good for our purposes, although a different detector might be used BIBREF17 .",
        "type": "Document"
      },
      {
        "id": "6bf3c170-2241-4c14-a533-36e7506074c3",
        "metadata": {
          "vector_store_key": "1702.06777-0",
          "chunk_id": 0,
          "document_id": "1702.06777",
          "start_idx": 0,
          "end_idx": 729
        },
        "page_content": "Dialects are language varieties defined across space. These varieties can differ in distinct linguistic levels (phonetic, morphosyntactic, lexical), which determine a particular regional speech BIBREF0 . The extension and boundaries (always diffuse) of a dialect area are obtained from the variation of one or many features such as, e.g., the different word alternations for a given concept. Typically, the dialect forms plotted on a map appear as a geographical continuum that gradually connects places with slightly different diatopic characteristics. A dialectometric analysis aims at a computational approach to dialect distribution, providing quantitative linguistic distances between locations BIBREF1 , BIBREF2 , BIBREF3 .",
        "type": "Document"
      },
      {
        "id": "4817ab85-d519-48ed-a024-9e0987bee148",
        "metadata": {
          "vector_store_key": "1904.08386-0",
          "chunk_id": 6,
          "document_id": "1904.08386",
          "start_idx": 3467,
          "end_idx": 4167
        },
        "page_content": "Capturing the essential components of each city in a single vector is thus not as simple as it is with more standard forms of text. Nevertheless, we hope that representations from language models trained over billions of words of text can extract some meaningful semantics from these descriptions. We experiment with three different pretrained representations: ELMo BIBREF5 , BERT BIBREF6 , and GloVe BIBREF18 . To produce a single city embedding, we compute the TF-IDF weighted element-wise mean of the token-level representations. For all pretrained methods, we additionally reduce the dimensionality of the city embeddings to 40 using PCA for increased compatibility with our clustering algorithm.",
        "type": "Document"
      },
      {
        "id": "4b1db6f6-18eb-403e-a524-9e20fe3a9eca",
        "metadata": {
          "vector_store_key": "1705.09665-8",
          "chunk_id": 14,
          "document_id": "1705.09665",
          "start_idx": 8063,
          "end_idx": 9008
        },
        "page_content": "In the remainder of this section we formally introduce a methodology for mapping communities in this space defined by their distinctiveness and dynamicity (examples in Figure FIGREF3 .B). Our approach follows the intuition that a distinctive community will use language that is particularly specific, or unique, to that community. Similarly, a dynamic community will use volatile language that rapidly changes across successive windows of time. To capture this intuition automatically, we start by defining word-level measures of specificity and volatility. We then extend these word-level primitives to characterize entire comments, and the community itself. Our characterizations of words in a community are motivated by methodology from prior literature that compares the frequency of a word in a particular setting to its frequency in some background distribution, in order to identify instances of linguistic variation BIBREF21 , BIBREF19 .",
        "type": "Document"
      },
      {
        "id": "4f567fc3-4e43-4e5c-921a-4523c6d48ac4",
        "metadata": {
          "vector_store_key": "1702.06777-1",
          "chunk_id": 24,
          "document_id": "1702.06777",
          "start_idx": 13345,
          "end_idx": 14086
        },
        "page_content": "In these cells, the use of standard Spanish language is widespread due probably to school education, media, travelers, etc. The character of its vocabulary is more uniform as compared with the purple group. While the purple cluster prefer particular utterances, the lexicon of the urban group includes most of the keywords. Importantly, we emphasize that both distance measures (cosine similarity and Jensen-Shanon) give rise to the same result, with little discrepancies on the numerical values that are not significant. The presence of two Twitter superdialects (urban and rural) has been recently suggested BIBREF10 based on a machine learning approach. Here, we arrive at the same conclusion but with a totally distinct model and corpus.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the characteristics of the rural dialect?",
    "llm_answer": "Rural regions have small, scattered populations and possess more specific words in their lexicon.\n",
    "context": [
      {
        "id": "6bf3c170-2241-4c14-a533-36e7506074c3",
        "metadata": {
          "vector_store_key": "1702.06777-0",
          "chunk_id": 0,
          "document_id": "1702.06777",
          "start_idx": 0,
          "end_idx": 729
        },
        "page_content": "Dialects are language varieties defined across space. These varieties can differ in distinct linguistic levels (phonetic, morphosyntactic, lexical), which determine a particular regional speech BIBREF0 . The extension and boundaries (always diffuse) of a dialect area are obtained from the variation of one or many features such as, e.g., the different word alternations for a given concept. Typically, the dialect forms plotted on a map appear as a geographical continuum that gradually connects places with slightly different diatopic characteristics. A dialectometric analysis aims at a computational approach to dialect distribution, providing quantitative linguistic distances between locations BIBREF1 , BIBREF2 , BIBREF3 .",
        "type": "Document"
      },
      {
        "id": "b59f7fd4-451b-4d0a-af14-bb62f1c95499",
        "metadata": {
          "vector_store_key": "1702.06777-0",
          "chunk_id": 23,
          "document_id": "1702.06777",
          "start_idx": 12632,
          "end_idx": 13345
        },
        "page_content": "Those cells that have similar colors in either figure FIGREF16 or figure FIGREF17 are expected to be ascribed to the same dialect zone. Thus, we can distinguish two main regions or clusters in the maps. The purple background covers most of the map and represents rural regions with small, scattered population. Our analysis shows that this group of cells possesses more specific words in their lexicon. In contrast, the green and yellow cells form a second cluster that is largely concentrated on the center and along the coastline, which correspond to big cities and industrialized areas. In these cells, the use of standard Spanish language is widespread due probably to school education, media, travelers, etc.",
        "type": "Document"
      },
      {
        "id": "5e783686-94b8-4d22-ae45-0e0b6f60868f",
        "metadata": {
          "vector_store_key": "1702.06777-1",
          "chunk_id": 6,
          "document_id": "1702.06777",
          "start_idx": 2732,
          "end_idx": 3467
        },
        "page_content": "We find that, due to language diversity, cities and main towns have similar linguistic distances unlike rural areas, which differ in their homogeneous forms. but obtained with a completely different method Our corpus consists of approximately 11 million geotagged tweets produced in Europe in Spanish language between October 2014 and June 2016. (Although we will focus on Spain, we will not consider in this work the speech of the Canary Islands due to difficulties with the data extraction) . The classification of tweets is accomplished by applying the Compact Language Detector (CLD) BIBREF16 to our dataset. CLD exhibits accurate benchmarks and is thus good for our purposes, although a different detector might be used BIBREF17 .",
        "type": "Document"
      },
      {
        "id": "4b1db6f6-18eb-403e-a524-9e20fe3a9eca",
        "metadata": {
          "vector_store_key": "1705.09665-8",
          "chunk_id": 14,
          "document_id": "1705.09665",
          "start_idx": 8063,
          "end_idx": 9008
        },
        "page_content": "In the remainder of this section we formally introduce a methodology for mapping communities in this space defined by their distinctiveness and dynamicity (examples in Figure FIGREF3 .B). Our approach follows the intuition that a distinctive community will use language that is particularly specific, or unique, to that community. Similarly, a dynamic community will use volatile language that rapidly changes across successive windows of time. To capture this intuition automatically, we start by defining word-level measures of specificity and volatility. We then extend these word-level primitives to characterize entire comments, and the community itself. Our characterizations of words in a community are motivated by methodology from prior literature that compares the frequency of a word in a particular setting to its frequency in some background distribution, in order to identify instances of linguistic variation BIBREF21 , BIBREF19 .",
        "type": "Document"
      },
      {
        "id": "687e8842-f313-4086-a5f7-f716700cd5e4",
        "metadata": {
          "vector_store_key": "1702.06777-0",
          "chunk_id": 17,
          "document_id": "1702.06777",
          "start_idx": 9408,
          "end_idx": 10079
        },
        "page_content": "As a consequence, these consistencies support the validity of our data. The novelty of our approach is to further analyze this dialect distribution with a quantitative measure as discussed below. Let us quantify the lexical difference between regions using the concept cold as an illustration. First, we generate a symmetric matrix of linguistic distances INLINEFORM0 between pairs of cells INLINEFORM1 and INLINEFORM2 with INLINEFORM3 calculated using equation ( EQREF5 ) or equation ( EQREF8 ). Then, we find the maximum possible INLINEFORM4 value in the matrix ( INLINEFORM5 ) and select either its corresponding INLINEFORM6 or INLINEFORM7 index as the reference cell.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance of the models on the tasks?",
    "llm_answer": "The models achieve 42.48% accuracy for age, 37.50% for dialect, and 57.81% for gender.  In evidence extraction, single and ensemble models achieve 42.23 and 44.11 ROUGE-L, respectively.  Answer synthesis models improve by 3.72% and 3.65% in ROUGE-L.  Passage selection accuracy is 38.9%.  A German sub-task achieved a macro F1 score of 0.62.  Other results are reported in tables TABREF20, TABREF21, TABREF27, and FIGREF15.\n",
    "context": [
      {
        "id": "9e077bf6-e910-4db3-a016-91f8083ea703",
        "metadata": {
          "vector_store_key": "1804.08139-1",
          "chunk_id": 21,
          "document_id": "1804.08139",
          "start_idx": 12407,
          "end_idx": 13125
        },
        "page_content": "It is noteworthy that our models are also space efficient since the task-specific information is extracted by using only a query vector, instead of a BiLSTM layer in the shared-private models. We also present the convergence properties of our models on the development datasets compared to other multi-task models in Figure FIGREF36 . We can see that PSP-MTL converges much more slowly than the rest four models because each task-specific classifier should consider the output of shared layer which is quite unstable during the beginning of training phrase. Moreover, benefit from the attention mechanism which is useful in feature extraction, SA-TML and DA-MTL are converged much more quickly than the rest of models.",
        "type": "Document"
      },
      {
        "id": "6d0ed0d4-8aef-4137-9d7e-80309ea15dc1",
        "metadata": {
          "vector_store_key": "1909.04181-4",
          "chunk_id": 7,
          "document_id": "1909.04181",
          "start_idx": 4041,
          "end_idx": 4697
        },
        "page_content": "We train the network for 15 epochs and save the model at the end of each epoch, choosing the model that performs highest accuracy on DEV as our best model. We present our best result on DEV in Table TABREF7. We report all our results using accuracy. Our best model obtains 42.48% for age, 37.50% for dialect, and 57.81% for gender. All models obtains best results with 2 epochs. For each task, we fine-tune on the BERT-Base Muultilingual Cased model relesed by the authors BIBREF1 . The model was pre-trained on Wikipedia of 104 languages (including Arabic) with 12 layer, 768 hidden units each, 12 attention heads, and has 110M parameters in entire model.",
        "type": "Document"
      },
      {
        "id": "7bd99408-11ce-4476-9ae6-abfc66eb50a4",
        "metadata": {
          "vector_store_key": "1706.04815-2",
          "chunk_id": 39,
          "document_id": "1706.04815",
          "start_idx": 21864,
          "end_idx": 22575
        },
        "page_content": "For the evidence extraction part, our proposed multi-task learning framework achieves 42.23 and 44.11 for the single and ensemble model in terms of ROUGE-L. For the answer synthesis, the single and ensemble models improve 3.72% and 3.65% respectively in terms of ROUGE-L. We observe the consistent improvement when applying our answer synthesis model to other answer span prediction models, such as BiDAF and Prediction. We analyze the result of incorporating passage ranking as an additional task. We compare our multi-task framework with two baselines as shown in Table 4 . For passage selection, our multi-task model achieves the accuracy of 38.9, which outperforms the pure answer prediction model with 4.3.",
        "type": "Document"
      },
      {
        "id": "3c1aa4b1-b844-4fd7-b07b-8f49ced02854",
        "metadata": {
          "vector_store_key": "1909.12642-2",
          "chunk_id": 15,
          "document_id": "1909.12642",
          "start_idx": 8419,
          "end_idx": 8999
        },
        "page_content": "Our model got the first position in the German sub-task with a macro F1 score of 0.62. The results of sub-task B and sub-task C is shown in table TABREF20 and TABREF21 respectively. In the results of subtask A, models are mainly affected by imbalance of the dataset. The training dataset of Hindi dataset was more balanced than English or German dataset. Hence, the results were around 0.78. As the dataset in German language was highly imbalanced, the results drops to 0.62. In subtask B, the highest F1 score reached was by the profane class for each language in table TABREF20.",
        "type": "Document"
      },
      {
        "id": "96914622-addb-4d58-90e5-11d336dba5ae",
        "metadata": {
          "vector_store_key": "2002.08307-2",
          "chunk_id": 16,
          "document_id": "2002.08307",
          "start_idx": 9023,
          "end_idx": 9662
        },
        "page_content": "We then fine-tune these pruned models on tasks from the General Language Understanding Evaluation (GLUE) benchmark, which is a standard set of 9 tasks that include sentiment analysis, natural language inference, etc. We avoid WNLI, which is known to be problematic. We also avoid tasks with less than 5k training examples because the results tend to be noisy (RTE, MRPC, STS-B). We fine-tune a separate model on each of the remaining 5 GLUE tasks for 3 epochs and try 4 learning rates: $[2, 3, 4, 5] \\times 10^{-5}$. The best evaluation accuracies are averaged and plotted in Figure FIGREF15. Individual task results are in Table TABREF27.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What other non-neural baselines do the authors compare to? ",
    "llm_answer": "TF-IDF (Lexical) and LDA (Topic).\n",
    "context": [
      {
        "id": "fa88df39-033d-4e0f-891b-162e0f9b0424",
        "metadata": {
          "vector_store_key": "1709.08299-4",
          "chunk_id": 19,
          "document_id": "1709.08299",
          "start_idx": 11042,
          "end_idx": 11836
        },
        "page_content": "The neural baselines are built by the following steps. We first use the shared training data to build a general systems, and choose the best performing model (in terms of cloze validation set) as baseline. Use User Query validation data for further tuning the systems with 10-fold cross-validations. Increase dropout rate BIBREF9 to 0.5 for preventing over-fitting issue. All baseline systems are chosen according to the performance of the validation set. The participant system results are given in Table 2 and 3 . As we can see that two neural baselines are competitive among participant systems and AoA Reader successfully outperform AS Reader and all participant systems in single model condition, which proves that it is a strong baseline system even without further fine-tuning procedure.",
        "type": "Document"
      },
      {
        "id": "22a4966b-fb5b-4fbf-bc0d-5fa53cc012ce",
        "metadata": {
          "vector_store_key": "1909.00124-4",
          "chunk_id": 25,
          "document_id": "1909.00124",
          "start_idx": 14255,
          "end_idx": 14886
        },
        "page_content": "The first DNN baseline is CNN BIBREF25, which does not handle noisy labels. The other five were designed to handle noisy labels. The comparison results are shown in Table TABREF12. From the results, we can make the following observations. (1) Our NetAb model achieves the best ACC and F1 on all datasets except for F1 of negative class on Laptop. The results demonstrate the superiority of NetAb. (2) NetAb outperforms the baselines designed for learning with noisy labels. These baselines are inferior to ours as they were tailored for image classification. Note that we found no existing method to deal with noisy labels for SSC.",
        "type": "Document"
      },
      {
        "id": "1c67409f-2f57-4674-ae18-af1d704d9ae8",
        "metadata": {
          "vector_store_key": "1804.08782-1",
          "chunk_id": 24,
          "document_id": "1804.08782",
          "start_idx": 13324,
          "end_idx": 14085
        },
        "page_content": "The experiment is conducted on two datasets: a subset (10%) of Fisher corpus set aside as test data and Suicide corpus. We use a number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 . For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose the best performing one for reporting, thus providing an upper-bound performance. Also, for baseline 2 we choose the session with higher value of the measure as real, since it measures similarity.",
        "type": "Document"
      },
      {
        "id": "7888c37a-9698-4e64-9bba-9941be318eeb",
        "metadata": {
          "vector_store_key": "1804.08782-1",
          "chunk_id": 25,
          "document_id": "1804.08782",
          "start_idx": 13324,
          "end_idx": 14126
        },
        "page_content": "Also, for baseline 2 we choose the session with higher value of the measure as real, since it measures similarity. As we can see in Table TABREF24 , our proposed NED measure achieves higher accuracy than all baselines on the Fisher corpus. The accuracy of our measure declines in the Suicide corpus as compared to the Fisher corpus, which is probably due to data mismatch as the model was trained on Fisher (mismatch of acoustics, recording conditions, sampling frequency, interaction style etc.). However, our measure still performs better than all baselines on Suicide corpus. According to prior work, both from domain theory BIBREF16 and from experimental validation BIBREF6 , a high emotional bond in patient-therapist interactions in the suicide therapy domain is associated with more entrainment.",
        "type": "Document"
      },
      {
        "id": "96576214-2c10-4049-ba11-db649d59677b",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 26,
          "document_id": "1809.04960",
          "start_idx": 15085,
          "end_idx": 15736
        },
        "page_content": "Unsupervised baseline models are as follows: TF-IDF (Lexical, Non-Neural) is an important unsupervised baseline. We use the concatenation of the title and the body as the query to retrieve the candidate comment set by means of the similarity of the tf-idf value. The model is trained on unpaired articles and comments, which is the same as our proposed model. LDA (Topic, Non-Neural) is a popular unsupervised topic model, which discovers the abstract \"topics\" that occur in a collection of documents. We train the LDA with the articles and comments in the training set. The model retrieves the comments by the similarity of the topic representations.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "On what dataset is Aristo system trained?",
    "llm_answer": "Training partition of the five datasets.\n",
    "context": [
      {
        "id": "2c1f8954-6ce6-4ec3-942b-38212bb1e754",
        "metadata": {
          "vector_store_key": "1909.01958-0",
          "chunk_id": 41,
          "document_id": "1909.01958",
          "start_idx": 23138,
          "end_idx": 23927
        },
        "page_content": "Each dataset is divided into train, development, and test partitions, the test partitions being \u201cblind\u201d, i.e., hidden to both the researchers and the Aristo system during training. All questions are taken verbatim from the original sources, with no rewording or modification. As mentioned earlier, we use only the non-diagram, multiple choice (NDMC) questions. We exclude questions with an associated diagram that is required to interpret the question. In the occasional case where two questions share the same preamble, the preamble is repeated for each question so they are independent. The Aristo solvers are trained using questions in the training partition (each solver is trained independently, as described earlier), and then the combination is fine-tuned using the development set.",
        "type": "Document"
      },
      {
        "id": "17142b67-e5af-4cf5-a604-9876391cf272",
        "metadata": {
          "vector_store_key": "1909.01958-0",
          "chunk_id": 45,
          "document_id": "1909.01958",
          "start_idx": 25217,
          "end_idx": 25982
        },
        "page_content": "Note that Aristo is a single system run on the five datasets (not retuned for each dataset in turn). Most notably, Aristo's scores on the Regents Exams far exceed earlier performances (e.g., BID0;BID25), and represents a new high-point on science questions. In addition, the results show the dramatic impact of new language modeling technology, embodied in AristoBERT and AristoRoBERTa, the scores for these two solvers dominating the performance of the overall system. Even on the ARC-Challenge questions, containing a wide variety of difficult questions, the language modeling based solvers dominate. The general increasing trend of solver scores from left to right in the table loosely reflects the progression of the NLP field over the six years of the project.",
        "type": "Document"
      },
      {
        "id": "e67b794a-b38c-4558-8e8b-94e6c4179701",
        "metadata": {
          "vector_store_key": "1909.01958-0",
          "chunk_id": 42,
          "document_id": "1909.01958",
          "start_idx": 23547,
          "end_idx": 24292
        },
        "page_content": "The Aristo solvers are trained using questions in the training partition (each solver is trained independently, as described earlier), and then the combination is fine-tuned using the development set. The Regents exam questions are taken verbatim from the New York Regents Examination board, using the 4th Grade Science, 8th Grade Science, and 12th Grade Living Environment examinations. The questions are partitioned into train/dev/test by exam, i.e., each exam is either in train, dev, or test but not split up between them. The ARC dataset is a larger corpus of science questions drawn from public resources across the country, spanning grades 3 to 9, and also includes the Regents 4th and 8th questions (using the same train/dev/test split).",
        "type": "Document"
      },
      {
        "id": "f90259d2-837d-49ca-9d82-1fb6cac38023",
        "metadata": {
          "vector_store_key": "1909.01958-0",
          "chunk_id": 58,
          "document_id": "1909.01958",
          "start_idx": 32151,
          "end_idx": 32865
        },
        "page_content": "This paper reports on Aristo\u2014the first system to achieve a score of over 90% on the non-diagram, multiple choice part of the New York Regents 8th Grade Science Exam, demonstrating that modern NLP methods can result in mastery of this task. Although Aristo only answers multiple choice questions without diagrams, and operates only in the domain of science, it nevertheless represents an important milestone towards systems that can read and understand. The momentum on this task has been remarkable, with accuracy moving from roughly 60% to over 90% in just three years. Finally, the use of independently authored questions from a standardized test allows us to benchmark AI performance relative to human students.",
        "type": "Document"
      },
      {
        "id": "38dc5ace-8cbb-4472-be3c-bcbb1d192abe",
        "metadata": {
          "vector_store_key": "1909.01958-0",
          "chunk_id": 12,
          "document_id": "1909.01958",
          "start_idx": 7178,
          "end_idx": 7877
        },
        "page_content": "Later work has harnessed state-of-the-art tools for large-scale language modeling and deep learning (BID29;BID30), which have come to dominate the performance of the overall system and reflects the stunning progress of the field of NLP as a whole. We now describe the architecture of Aristo, and provide a brief summary of the solvers it uses. The current configuration of Aristo comprises of eight solvers, described shortly, each of which attempts to answer a multiple choice question. To study particular phenomena and develop solvers, the project has created larger datasets to amplify and study different problems, resulting in 10 new datasets and 5 large knowledge resources for the community.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many roles are proposed?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "9b9300dc-2f37-4975-8541-a12d68970c17",
        "metadata": {
          "vector_store_key": "1911.03243-2",
          "chunk_id": 2,
          "document_id": "1911.03243",
          "start_idx": 1345,
          "end_idx": 2111
        },
        "page_content": "To address coverage, we employ two independent workers plus an additional one for consolidation \u2014 similar to conventional expert-annotation practices. In addition to yielding 25% more roles, our coverage gain is demonstrated by evaluating against expertly annotated data and comparison with PropBank (Section SECREF4). To foster future research, we release an assessed high-quality gold dataset along with our reproducible protocol and evaluation scheme, and report the performance of the existing parser BIBREF5 as a baseline. In QA-SRL, a role question adheres to a 7-slot template, with slots corresponding to a WH-word, the verb, auxiliaries, argument placeholders (SUBJ, OBJ), and prepositions, where some slots are optional BIBREF4 (see appendix for examples).",
        "type": "Document"
      },
      {
        "id": "90230658-1cf3-4406-9bb8-5337b29e2731",
        "metadata": {
          "vector_store_key": "1911.03243-2",
          "chunk_id": 3,
          "document_id": "1911.03243",
          "start_idx": 2111,
          "end_idx": 2920
        },
        "page_content": "In QA-SRL, a role question adheres to a 7-slot template, with slots corresponding to a WH-word, the verb, auxiliaries, argument placeholders (SUBJ, OBJ), and prepositions, where some slots are optional BIBREF4 (see appendix for examples). Such question captures the corresponding semantic role with a natural easily understood expression. The set of all non-overlapping answers for the question is then considered as the set of arguments associated with that role. This broad question-based definition of roles captures traditional cases of syntactically-linked arguments, but also additional semantic arguments clearly implied by the sentence meaning (see example (2) in Table TABREF4). The original 2015 QA-SRL dataset BIBREF4 was annotated by non-expert workers after completing a brief training procedure.",
        "type": "Document"
      },
      {
        "id": "ab42b5be-2df6-4256-8b23-298807602712",
        "metadata": {
          "vector_store_key": "1806.07711-1",
          "chunk_id": 46,
          "document_id": "1806.07711",
          "start_idx": 24081,
          "end_idx": 24913
        },
        "page_content": "Based on an analysis of a random sample composed by 100 WordNet noun and verb glosses, we identified and named the main semantic roles and their compositions present on dictionary definitions. Moreover, we compared the identified semantic patterns with the definitions' syntactic structure, pointing out the features that can serve as input for automatic role labeling. The proposed semantic roles list is by no means definitive or exhaustive, but a first step at highlighting and formalizing the most relevant aspects of widely used intensional level definitions. As future work, we intend to implement a rule-based classifier, using the identified syntactic patterns to generate an initial annotated dataset, which can be manually curated and subsequently feed a machine learning model able to annotate definitions in large scale.",
        "type": "Document"
      },
      {
        "id": "930a7d38-34e6-46ae-a94c-500d178475de",
        "metadata": {
          "vector_store_key": "1911.03243-1",
          "chunk_id": 8,
          "document_id": "1911.03243",
          "start_idx": 4841,
          "end_idx": 5585
        },
        "page_content": "In this consolidation task, the worker validates questions, merges, splits or modifies answers for the same role according to guidelines, and removes redundant roles by picking the more naturally phrased questions. For example, in Table TABREF4 ex. 1, one worker could have chosen \u201c47 people\u201d, while another chose \u201cthe councillor\u201d; in this case the consolidator would include both of those answers. In Section SECREF4, we show that this process yields better coverage. For example annotations, please refer to the appendix. We refine the previous guidelines by emphasizing several semantic features: correctly using modal verbs and negations in the question, and choosing answers that coincide with a single entity (example 1 in Table TABREF4).",
        "type": "Document"
      },
      {
        "id": "5895c309-8093-410b-b266-b0844ebbcb32",
        "metadata": {
          "vector_store_key": "1911.03243-2",
          "chunk_id": 21,
          "document_id": "1911.03243",
          "start_idx": 12513,
          "end_idx": 13442
        },
        "page_content": "Based on this inspection, the parser completely misses 23% of the 154 roles present in the gold-data, out of which, 17% are implied. While the parser correctly predicts 82% of non-implied roles, it skips half of the implied ones. We introduced a refined crowdsourcing pipeline and a corresponding evaluation methodology for QA-SRL. It enabled us to release a new gold standard for evaluations, notably of much higher coverage of core and implied roles than the previous Dense evaluation dataset. We believe that our annotation methodology and dataset would facilitate future research on natural semantic annotations and QA-SRL parsing. For completeness, we include several examples with some questions restructured into its 7 template slots in Table TABREF26 As described in section 3 The consolidator receives two sets of QA annotations and merges them according to the guidelines to produce an exhaustive and consistent QA set.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What language technologies have been introduced in the past?",
    "llm_answer": "whatthelang, an off-the-shelf classifier; a Twitter-specific tool for language identification.\n",
    "context": [
      {
        "id": "1080ea19-e0d6-4652-964b-f81076ccb084",
        "metadata": {
          "vector_store_key": "1804.08186-2",
          "chunk_id": 223,
          "document_id": "1804.08186",
          "start_idx": 126960,
          "end_idx": 127803
        },
        "page_content": "Another distinguishing feature of is that it comes pre-trained with data for 1400 languages, which is the highest number by a large margin of any off-the-shelf system. whatthelang is a recent language identifier written in Python, which utilizes the FastText NN-based text classification algorithm. It supports 176 languages. implements an off-the-shelf classifier trained using Wikipedia data, covering 122 languages. Although not described as such, the actual classification algorithm used is a linear model, and is thus closely related to both NB and a cosine-based vector space model. In addition to the above-mentioned general-purpose language identifiers, there have also been efforts to produce pre-trained language identifiers targeted specifically at Twitter messages. is a Twitter-specific tool with built-in models for 19 languages.",
        "type": "Document"
      },
      {
        "id": "c28fe057-2cfc-4978-9c0a-84fd286d4b98",
        "metadata": {
          "vector_store_key": "1612.06897-3",
          "chunk_id": 39,
          "document_id": "1612.06897",
          "start_idx": 20315,
          "end_idx": 20488
        },
        "page_content": "We presented results on two diverse language pairs German $\\rightarrow $ English and Chinese $\\rightarrow $ English (usually very challenging pairs for machine translation).",
        "type": "Document"
      },
      {
        "id": "0a7d831d-b88f-470e-931c-6ce545b154ab",
        "metadata": {
          "vector_store_key": "1910.05456-2",
          "chunk_id": 45,
          "document_id": "1910.05456",
          "start_idx": 25600,
          "end_idx": 26346
        },
        "page_content": "To name some specific examples, marques2011study investigated the effect of a stay abroad on Spanish L2 acquisition, including learning of its verbal morphology in English speakers. jia2003acquisition studied how Mandarin Chinese-speaking children learned the English plural morpheme. nicoladis2012young studied the English past tense acquisition in Chinese\u2013English and French\u2013English bilingual children. They found that, while both groups showed similar production accuracy, they differed slightly in the type of errors they made. Also considering the effect of the native language explicitly, yang2004impact investigated the acquisition of the tense-aspect system in an L2 for speakers of a native language which does not mark tense explicitly.",
        "type": "Document"
      },
      {
        "id": "22504978-69cc-4504-9abc-41d5651e4391",
        "metadata": {
          "vector_store_key": "1804.08186-3",
          "chunk_id": 248,
          "document_id": "1804.08186",
          "start_idx": 140712,
          "end_idx": 141342
        },
        "page_content": "An interesting area to explore is the trade-off between the number of languages supported and the accuracy per-language. From existing results it is not clear if it is possible to continue increasing the number of languages supported without adversely affecting the average accuracy, but it would be useful to quantify if this is actually the case across a broad range of text sources. mostlanguages lists the articles where the with more than 30 languages has been investigated. \u201cUnseen\u201d languages are languages that we do not have training data for but may nonetheless be encountered by a system when applied to real-world data.",
        "type": "Document"
      },
      {
        "id": "9ece3dbc-6d9e-49d3-838c-c59b224fb280",
        "metadata": {
          "vector_store_key": "2002.02427-3",
          "chunk_id": 26,
          "document_id": "2002.02427",
          "start_idx": 14704,
          "end_idx": 14917
        },
        "page_content": "Our next step is to experiment with other languages such as Hindi and Italian. The work of Paolo Rosso was partially funded by the Spanish MICINN under the research project MISMIS-FAKEnHATE (PGC2018-096212-B-C31).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they define local variance?",
    "llm_answer": "Local variance loss is the reciprocal of the variance of the attention distribution, calculated using the median value, expecting the model to focus on salient parts.\n",
    "context": [
      {
        "id": "371af5f5-3799-4ad2-a746-53de67e4a821",
        "metadata": {
          "vector_store_key": "1910.11491-0",
          "chunk_id": 10,
          "document_id": "1910.11491",
          "start_idx": 5295,
          "end_idx": 5923
        },
        "page_content": "Mathematically, when only a small number of values are large, the shape of the distribution is sharp and the variance of the attention distribution is large. Drawing on the concept of variance in mathematics, local variance loss is defined as the reciprocal of its variance expecting the attention model to be able to focus on more salient parts. The standard variance calculation is based on the mean of the distribution. However, as previous work BIBREF15, BIBREF16 mentioned that the median value is more robust to outliers than the mean value, we use the median value to calculate the variance of the attention distribution.",
        "type": "Document"
      },
      {
        "id": "70c61b26-a63d-46ad-bd2e-25e6c1e4b3ae",
        "metadata": {
          "vector_store_key": "1910.11491-0",
          "chunk_id": 13,
          "document_id": "1910.11491",
          "start_idx": 7214,
          "end_idx": 7912
        },
        "page_content": "Similar to the definition of local variance loss, the global variance loss is formulated as: where $g_i$ represents the difference between the accumulated attention weight and maximum attention weight at $i$-th position. The model is firstly pre-trained to minimize the maximum-likelihood loss, which is widely used in sequence generation tasks. We define $y^* = \\lbrace y^*_1, \\cdots , y_T^*\\rbrace $ as the ground-truth output sequence for a given input sequence $x$, then the loss function is formulated as: After converging, the model is further optimized with local variance loss and global variance loss. The mix of loss functions is: where $\\lambda _1$ and $\\lambda _2$ are hyper-parameters.",
        "type": "Document"
      },
      {
        "id": "5e324f22-148a-43b8-904d-a9b2a70296e8",
        "metadata": {
          "vector_store_key": "1910.11491-0",
          "chunk_id": 12,
          "document_id": "1910.11491",
          "start_idx": 6331,
          "end_idx": 7214
        },
        "page_content": "Different from the coverage mechanism BIBREF11, BIBREF10 tracking attention distributions of previous timesteps, we maintain the sum of attention distributions over all decoder timesteps, denoted as $A$. The $i$-th value of $A$ represents the accumulated attention that the input state at $i$-th position has received throughout the whole decoding process. Without repeated high attention being paid to the same location, the difference between the sum of attention weight and maximum attention weight of $i$-th input state among all timesteps should be small. Moreover, the whole distribution of the difference over all input positions should have a flat shape. Similar to the definition of local variance loss, the global variance loss is formulated as: where $g_i$ represents the difference between the accumulated attention weight and maximum attention weight at $i$-th position.",
        "type": "Document"
      },
      {
        "id": "f561722f-e311-4a07-b7e8-48209c899de1",
        "metadata": {
          "vector_store_key": "1705.09665-2",
          "chunk_id": 19,
          "document_id": "1705.09665",
          "start_idx": 10493,
          "end_idx": 11184
        },
        "page_content": "Using our word-level primitives, we define the specificity of an utterance INLINEFORM0 in INLINEFORM1 , INLINEFORM2 as the average specificity of each word in the utterance. The volatility of utterances is defined analogously. Having described these word-level measures, we now proceed to establish the primary axes of our typology: Distinctiveness. A community with a very distinctive identity will tend to have distinctive interests, expressed through specialized language. Formally, we define the distinctiveness of a community INLINEFORM0 as the average specificity of all utterances in INLINEFORM1 . We refer to a community with a less distinctive identity as being generic. Dynamicity.",
        "type": "Document"
      },
      {
        "id": "fd976b81-9b89-493a-9a87-bd9b0f13bec2",
        "metadata": {
          "vector_store_key": "1709.08858-1",
          "chunk_id": 12,
          "document_id": "1709.08858",
          "start_idx": 6298,
          "end_idx": 6937
        },
        "page_content": "Note that we cannot compute the variance if some $a_i$ does not have the value of SU. Further, it may be also possible that all $a_i$ may have the same SU, sharing identical neighbors. In this case, the variance becomes an extreme value, that is, 0. In these cases, we consider that we cannot perform the statistical test. We used FIL9, which is freely available as the test corpus for Word2Vec and is derived from Wikipedia. We compute 200-dimensional distributed vector representations with default parameter. In this situation, all-uppercase are converted into lower case. This is why all proper nouns are in lower case in this example.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "how do they measure discussion quality?",
    "llm_answer": "Comparing annotations of student talk to English Education experts' evaluations (quadratic-weighted kappa).  The best model (R-squared) included argumentation, specificity, and knowledge domain.\n",
    "context": [
      {
        "id": "59e61546-2e27-4149-9700-d3703bd3cb80",
        "metadata": {
          "vector_store_key": "1909.03023-1",
          "chunk_id": 22,
          "document_id": "1909.03023",
          "start_idx": 12213,
          "end_idx": 13005
        },
        "page_content": "We also BIBREF32 explored the validity of our coding scheme by comparing our annotations of student talk to English Education experts' evaluations (quadratic-weighted kappa of 0.544) of the discussion's quality. Using stepwise regressions, we found that the best model of discussion quality (R-squared of INLINEFORM0 ) included all three of our coding dimensions: argumentation, specificity, and knowledge domain. Our annotation scheme introduces opportunities for the educational community to conduct further research on the relationship between features of student talk, student learning, and discussion quality. Although Chisholm and Godley Chisholm:11 and we found relations between our coding constructs and discussion quality, these were small-scale studies based on manual annotations.",
        "type": "Document"
      },
      {
        "id": "1b28681d-3338-42fb-b71b-ddaf423e0968",
        "metadata": {
          "vector_store_key": "1909.03023-4",
          "chunk_id": 2,
          "document_id": "1909.03023",
          "start_idx": 1133,
          "end_idx": 1882
        },
        "page_content": "This limitation is partly due to the time-intensive work required to analyze discourse data through qualitative methods such as ethnography and discourse analysis. Thus, qualitative case studies have generated compelling theories about the specific features of student talk that lead to high-quality discussions, but few findings can be generalized and leveraged to influence instructional improvements across ELA classrooms. As a first step towards developing an automated system for detecting the features of student talk that lead to high quality discussions, we propose a new annotation scheme for student talk during ELA \u201ctext-based\" discussions - that is, discussions that center on a text or piece of literature (e.g., book, play, or speech).",
        "type": "Document"
      },
      {
        "id": "7961b42a-2442-49ad-9720-6f0f1394e267",
        "metadata": {
          "vector_store_key": "2002.11893-9",
          "chunk_id": 32,
          "document_id": "2002.11893",
          "start_idx": 18341,
          "end_idx": 19079
        },
        "page_content": "Since dialogue act annotation is not a classification problem, we didn't use Fleiss' kappa to measure the agreement among experts. We used dialogue act F1 and state accuracy to measure the agreement between each two experts' annotations. The average dialogue act F1 is 94.59% and the average state accuracy is 93.55%. We then compared our annotations with each expert's annotations which are regarded as gold standard. The average dialogue act F1 is 95.36% and the average state accuracy is 94.95%, which indicates the high quality of our annotations. After removing uncompleted dialogues, we collected 6,012 dialogues in total. The dataset is split randomly for training/validation/test, where the statistics are shown in Table TABREF25.",
        "type": "Document"
      },
      {
        "id": "4be44773-b407-4314-abd0-d9c32e99174b",
        "metadata": {
          "vector_store_key": "1908.07816-1",
          "chunk_id": 7,
          "document_id": "1908.07816",
          "start_idx": 4025,
          "end_idx": 4656
        },
        "page_content": "We consider factors such as the balance of positive and negative sentiments in test dialogs, a well-chosen range of topics, and dialogs that our human evaluators can relate. It is the first time such an approach is designed with consideration for the human judges. Our main goal is to increase the objectivity of the results and reduce judges' mistakes due to out-of-context dialogs they have to evaluate. The rest of the paper unfolds as follows. Section SECREF2 discusses some related work. In Section SECREF3, we give detailed description of the methodology. We present experimental results and some analysis in Section SECREF4.",
        "type": "Document"
      },
      {
        "id": "6907dc05-fe51-434b-a713-3e3255725f72",
        "metadata": {
          "vector_store_key": "1709.10217-3",
          "chunk_id": 3,
          "document_id": "1709.10217",
          "start_idx": 1892,
          "end_idx": 2612
        },
        "page_content": "For example, for the input message \u201cHow is it going today?\u201d, the responses can be \u201cI'm fine!\u201d, \u201cNot bad.\u201d, \u201cI feel so depressed!\u201d, \u201cWhat a bad day!\u201d, etc. There may be infinite number of responses for an open domain messages. Hence, it is difficult to construct a gold standard (usually a reference set) to evaluate a response which is generated by an open domain chit-chat system. For the task-oriented system, although there are some objective evaluation metrics, such as the number of turns in a dialogue, the ratio of task completion, etc., there is no gold standard for automatically evaluating two (or more) dialogue systems when considering the satisfaction of the human and the fluency of the generated dialogue.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what were the baselines?",
    "llm_answer": "Neural baselines, i-vector system, x-vector system, ANRES_Sing_R, ANRES_Sing_Z, ANRES_Seq_R, ANRES_Seq_Z, Baseline1.\n",
    "context": [
      {
        "id": "fa88df39-033d-4e0f-891b-162e0f9b0424",
        "metadata": {
          "vector_store_key": "1709.08299-4",
          "chunk_id": 19,
          "document_id": "1709.08299",
          "start_idx": 11042,
          "end_idx": 11836
        },
        "page_content": "The neural baselines are built by the following steps. We first use the shared training data to build a general systems, and choose the best performing model (in terms of cloze validation set) as baseline. Use User Query validation data for further tuning the systems with 10-fold cross-validations. Increase dropout rate BIBREF9 to 0.5 for preventing over-fitting issue. All baseline systems are chosen according to the performance of the validation set. The participant system results are given in Table 2 and 3 . As we can see that two neural baselines are competitive among participant systems and AoA Reader successfully outperform AS Reader and all participant systems in single model condition, which proves that it is a strong baseline system even without further fine-tuning procedure.",
        "type": "Document"
      },
      {
        "id": "02bef31f-a2b5-4b89-8063-62dc0b0f91eb",
        "metadata": {
          "vector_store_key": "1911.01799-4",
          "chunk_id": 19,
          "document_id": "1911.01799",
          "start_idx": 11188,
          "end_idx": 11946
        },
        "page_content": "Two state-of-the-art baseline systems were built following the Kaldi SITW recipe BIBREF22: an i-vector system BIBREF3 and an x-vector system BIBREF10. For the i-vector system, the acoustic feature involved 24-dimensional MFCCs plus the log energy, augmented by the first- and second-order derivatives. We also applied the cepstral mean normalization (CMN) and the energy-based voice active detection (VAD). The universal background model (UBM) consisted of $2,048$ Gaussian components, and the dimensionality of the i-vector space was 400. LDA was applied to reduce the dimensionality of the i-vectors to 150. The PLDA model was used for scoring BIBREF4. For the x-vector system, the feature-learning component was a 5-layer time-delay neural network (TDNN).",
        "type": "Document"
      },
      {
        "id": "348b5dbb-b442-4f57-9f6e-c2038d069875",
        "metadata": {
          "vector_store_key": "1709.08299-4",
          "chunk_id": 16,
          "document_id": "1709.08299",
          "start_idx": 9232,
          "end_idx": 9901
        },
        "page_content": "The paragraph should be read carefully and judged whether appropriate for asking questions No more than 5 questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models.",
        "type": "Document"
      },
      {
        "id": "64beee2a-a1a9-454b-917e-917a34317fa7",
        "metadata": {
          "vector_store_key": "2004.01878-3",
          "chunk_id": 27,
          "document_id": "2004.01878",
          "start_idx": 14569,
          "end_idx": 15289
        },
        "page_content": "In summary, the following four baselines are designed: ANRES_Sing_R: randomly initializing the states for each single trading day. ANRES_Sing_Z: initializing the states as zeros for each single trading day. ANRES_Seq_R: randomly initializing the first states for each trading sequence only. ANRES_Seq_Z: initializing the first states as zeros for each trading sequence only. Development set results on predicting S&P 500 index are shown in Table TABREF13. We can see that modeling recurrent value sequences performs better than treating each trading day separately, which shows that modeling trading sequences can capture the correlations between trading days and the non-linear compositional effects of multiple events.",
        "type": "Document"
      },
      {
        "id": "39cb1ec4-6c9f-4dbd-b781-29fe9d4ad19f",
        "metadata": {
          "vector_store_key": "1912.01046-3",
          "chunk_id": 23,
          "document_id": "1912.01046",
          "start_idx": 13260,
          "end_idx": 13980
        },
        "page_content": "Following this idea, Baseline1 represents a span of sentences by concatenating the vectors of the starting and ending sentences. The left diagram in Fig. FIGREF15 illustrates the Baseline1 model. Model. The model takes two inputs, a transcript, $\\lbrace s_1, s_2, ... s_n\\rbrace $ where $s_i$ are individual sentences and a question, $q$. The output is the span scores, $y$, the scores over all possible spans. GLoVe BIBREF14 is used for the word representations in the transcript and the questions. We use two bi-LSTMs BIBREF15 to encode the transcript. where n is the number of sentences . The output of Passage-level Encoding, $p$, is a sequence of vector, $p_i$, which represents the latent meaning of each sentence.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which soft-selection approaches are evaluated?",
    "llm_answer": "BERT-Soft.  ATAE-LSTM.\n",
    "context": [
      {
        "id": "bd27d8cc-bb4b-48d1-b8c7-bb37e097be0e",
        "metadata": {
          "vector_store_key": "1909.11297-1",
          "chunk_id": 41,
          "document_id": "1909.11297",
          "start_idx": 24363,
          "end_idx": 25225
        },
        "page_content": "In this paper, we propose a hard-selection approach for aspect-based sentiment analysis, which determines the start and end positions of the opinion snippet for a given input aspect. The deep associations between the sentence and aspect, and the long-term dependencies within the sentence are taken into consideration by leveraging the pre-trained BERT model. With the hard selection of the opinion snippet, our approach can alleviate the attention distraction problem of traditional attention-based soft-selection methods. Experimental results demonstrate the effectiveness of our method. Especially, our hard-selection approach outperforms soft-selection approaches significantly when handling multi-aspect sentences with different sentiment polarities. This work is supported by National Science and Technology Major Project, China (Grant No. 2018YFB0204304).",
        "type": "Document"
      },
      {
        "id": "dd08357b-cd9d-4377-95ad-d016689efa55",
        "metadata": {
          "vector_store_key": "1905.10238-2",
          "chunk_id": 36,
          "document_id": "1905.10238",
          "start_idx": 19497,
          "end_idx": 20200
        },
        "page_content": "The results are shown in Table 3 , which clearly show the necessity of the knowledge. Interestingly, AG contributes the most among all knowledge types, which indicates that potentially more cases in the evaluation dataset demand on the AG knowledge than others. More importantly, the results also prove the effectiveness of the knowledge attention module, which contributes to the performance gap between our model and the Feature Concatenation one. Effect of Different Pruning Thresholds We try different thresholds $t$ for the softmax pruning in selecting reliable candidates. The effects of different thresholds on reducing candidates and overall performance are shown in Figure 5 and 6 respectively.",
        "type": "Document"
      },
      {
        "id": "5061a335-7c15-40c1-a5b9-4fb56d0438de",
        "metadata": {
          "vector_store_key": "1909.11297-2",
          "chunk_id": 37,
          "document_id": "1909.11297",
          "start_idx": 21995,
          "end_idx": 22714
        },
        "page_content": "For soft-selection methods, the attention distraction is inevitable due to their way in calculating the attention weights for every single word. The noisy or irrelevant words could seize more attention weights than the ground truth opinion words. Our method considers the opinion snippet as a consecutive whole, which is more resistant to attention distraction. In this section, we visualize the attention weights for BERT-Soft and opinion snippets for BERT-Hard. As demonstrated in Figure FIGREF39, the multi-aspect sentence \u201cthe appetizers are OK, but the service is slow\u201d belongs to the category Diff. Firstly, the attention weights of BERT-Soft scatter among the whole sentence and could attend to irrelevant words.",
        "type": "Document"
      },
      {
        "id": "88819c84-f625-4d15-bcab-7ce14178e843",
        "metadata": {
          "vector_store_key": "1909.11297-1",
          "chunk_id": 1,
          "document_id": "1909.11297",
          "start_idx": 805,
          "end_idx": 1538
        },
        "page_content": "These attention-based methods have brought the ABSA task remarkable performance improvement. Previous attention-based methods can be categorized as soft-selection approaches since the attention weights scatter across the whole sentence and every word is taken into consideration with different weights. This usually results in attention distraction BIBREF7, i.e., attending on noisy or misleading words, or opinion words from other aspects. Take Figure FIGREF1 as an example, for the aspect place in the sentence \u201cthe food is usually good but it certainly is not a relaxing place to go\u201d, we visualize the attention weights from the model ATAE-LSTM BIBREF2. As we can see, the words \u201cgood\u201d and \u201cbut\u201d are dominant in attention weights.",
        "type": "Document"
      },
      {
        "id": "fe508347-6c6a-47d4-9808-5efb4c8fd325",
        "metadata": {
          "vector_store_key": "2002.05058-2",
          "chunk_id": 33,
          "document_id": "2002.05058",
          "start_idx": 18180,
          "end_idx": 18966
        },
        "page_content": "This procedure is performed iteratively until convergence, which is defined as the order of skill ratings of compared models keeps the same after each model is selected at least 50 times. While the sampling procedure can be optimized by bayesian optimization BIBREF24 or multi-armed bandit algorithms BIBREF25, we choose to keep the method as simple as possible and use random sampling. We set up experiments in order to answer the following research questions: RQ1: Can the comparative evaluator correlate better with human preference in sample-level than previous automated metrics when evaluating open domain NLG models? RQ2: Can the comparative evaluator correlate better with human preference in model-level, so that our approach can measure the progress on open domain NLG better?",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is slot filing dataset?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "abe85bde-9b7b-4bfb-9688-0c11e182137a",
        "metadata": {
          "vector_store_key": "1909.00754-7",
          "chunk_id": 30,
          "document_id": "1909.00754",
          "start_idx": 15367,
          "end_idx": 16012
        },
        "page_content": "Since the name slot rarely occurs in the dataset, it is not included in our experiments, following previous literature BIBREF3 , BIBREF20 . Our model is also tested on the multi-domain dataset, MultiWoZ BIBREF9 . It has a more complex ontology with 7 domains and 25 predefined slots. Since the combined slot-value pairs representation of the belief states has to be applied for the model with $O(n)$ ITC, the total number of slots is 35. The statistics of these two datsets are shown in Table 2 . Based on the statistics from these two datasets, we can calculate the theoretical Inference Time Multiplier (ITM), $K$ , as a metric of scalability.",
        "type": "Document"
      },
      {
        "id": "8dff25f5-34e7-4433-9854-60504b7aa62a",
        "metadata": {
          "vector_store_key": "2002.01359-1",
          "chunk_id": 34,
          "document_id": "2002.01359",
          "start_idx": 19704,
          "end_idx": 20376
        },
        "page_content": "A slot tagger identifies slot values, which are used to update the candidate tracker. The candidate classifier uses the utterances and slot/intent descriptions to predict the final dialogue state. They also use an additional loss to penalize incorrect prediction on which slots appear in the current turn. We consider the following metrics for automatic evaluation of different submissions. Joint goal accuracy has been used as the primary metric to rank the submissions. Active Intent Accuracy: The fraction of user turns for which the active intent has been correctly predicted. Requested Slot F1: The macro-averaged F1 score for requested slots over all eligible turns.",
        "type": "Document"
      },
      {
        "id": "b65cc1cc-2ff7-41b6-8adb-86a06d99e3f5",
        "metadata": {
          "vector_store_key": "2002.01359-0",
          "chunk_id": 17,
          "document_id": "2002.01359",
          "start_idx": 9830,
          "end_idx": 10483
        },
        "page_content": "Having such a list is impractical for slots like date or time because they have infinitely many possible values or for slots like movie or song names, for which new values are periodically added. Such slots are specifically identified as non-categorical slots. In our evaluation sets, we ensured the presence of a significant number of values which were not previously seen in the training set to evaluate the performance of models on unseen values. Some slots like gender, number of people, etc. are classified as categorical and we provide a list of all possible values for them. However, these values are assumed to be not consistent across services.",
        "type": "Document"
      },
      {
        "id": "4f33e83c-d199-47c4-a8c1-b9481642f476",
        "metadata": {
          "vector_store_key": "1909.00754-7",
          "chunk_id": 2,
          "document_id": "1909.00754",
          "start_idx": 1036,
          "end_idx": 1831
        },
        "page_content": "To generate a distribution over the candidate set, previous works often take each of the slot-value pairs as input for scoring. However, in real-world scenarios, it is often not practical to enumerate all possible slot value pairs and perform scoring from a large dynamically changing knowledge base BIBREF11 . To tackle this problem, a popular direction is to build a fixed-length candidate set that is dynamically updated throughout the dialogue development. cpt briefly summaries the inference time complexity of multiple state-of-the-art DST models following this direction. Since the inference complexity of all of previous model is at least proportional to the number of the slots, these models will struggle to scale to multi-domain datasets with much larger numbers of pre-defined slots.",
        "type": "Document"
      },
      {
        "id": "f0a10a30-e309-4f1d-b350-14d8b04b9adb",
        "metadata": {
          "vector_store_key": "1909.06937-2",
          "chunk_id": 28,
          "document_id": "1909.06937",
          "start_idx": 16468,
          "end_idx": 17146
        },
        "page_content": "We conjecture that the named entity feature in the ATIS has a great impact on the slot filling result as illustrated in Section SECREF34. Since the SNIPS is collected from multiple domains with more balanced labels when compared with the ATIS, the slot filling $F_1$ score on the SNIPS is able to demonstrate the superiority of our CM-Net. It is noteworthy that the CM-Net achieves comparable results when compared with models that exploit additional language models BIBREF27, BIBREF28. We conduct auxiliary experiments by leveraging the well-known BERT BIBREF35 as an external resource for a relatively fair comparison with those models, and report details in Section SECREF48.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How large is the dataset they generate?",
    "llm_answer": "The generated datasets were trimmed down to the size of the original dataset.\n",
    "context": [
      {
        "id": "8be62763-27f4-46ff-b2bb-edb6a6b2fa04",
        "metadata": {
          "vector_store_key": "1607.06025-3",
          "chunk_id": 42,
          "document_id": "1607.06025",
          "start_idx": 24463,
          "end_idx": 25198
        },
        "page_content": "The new dataset consists of training set, which is generated using examples from the original training set, and a development set, which is generated from the original development set. The beam size for beam search was set to 1. The details of the decision are presented in Section SECREF35 . Some datasets were constructed by filtering the generated datasets according to various thresholds. Thus, the generated datasets were constructed to contain enough examples, so that the filtered datasets had at least the number of examples as the original dataset. In the end, all the datasets were trimmed down to the size of the original dataset by selecting the samples sequentially from the beginning until the dataset had the right size.",
        "type": "Document"
      },
      {
        "id": "9ca5340d-587b-4e94-bcbe-160570788673",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 35,
          "document_id": "1910.12618",
          "start_idx": 19294,
          "end_idx": 19871
        },
        "page_content": "For instance for random forests it can correspond to the maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014 and 2015 serve as validation set (730 days).",
        "type": "Document"
      },
      {
        "id": "d87d327c-2858-4b5d-ae8a-860d401f63bd",
        "metadata": {
          "vector_store_key": "1908.05434-1",
          "chunk_id": 27,
          "document_id": "1908.05434",
          "start_idx": 14432,
          "end_idx": 15064
        },
        "page_content": "We have ensured that the raw dataset has no overlap with the labeled dataset to avoid bias in test accuracy. While it is possible to scrape more raw data, we did not observe significant improvements in model performances when the size of raw data increased from INLINEFORM0 70,000 to INLINEFORM1 170,000, hence we assume that the current raw dataset is sufficiently large. The labeled dataset is called Trafficking-10k. It consists of 12,350 ads from Backpage labeled by experts in human trafficking detection BIBREF9 . Each label is one of seven ordered levels of likelihood that the corresponding ad comes from a human trafficker.",
        "type": "Document"
      },
      {
        "id": "899be7c2-e8c3-4ff8-b79f-ab8799631fc9",
        "metadata": {
          "vector_store_key": "1912.00864-5",
          "chunk_id": 34,
          "document_id": "1912.00864",
          "start_idx": 19660,
          "end_idx": 20343
        },
        "page_content": "On average, the answers to these questions consisted of about 3.5 conclusions and supplements selected by human experts. The questions, conclusions, and supplements had average lengths of 482, 41, and 46 characters, respectively. There were 9,779 word tokens in the questions and 6,317 tokens in answers; the overlap was 4,096. We also used the Yahoo nfL6 dataset, the largest publicly available English non-factoid CQA dataset. It has 499,078 answers to 87,361 questions. We fine-tuned the model by using questions in the \u201csocial science\u201d, \u201csociety & culture\u201d, and \u201carts & humanities\u201d categories, since they require diverse answers. This yielded 114,955 answers to 13,579 questions.",
        "type": "Document"
      },
      {
        "id": "1e2bf238-b0a0-40af-9d8d-e6fea4ad79d0",
        "metadata": {
          "vector_store_key": "1910.09399-3",
          "chunk_id": 105,
          "document_id": "1910.09399",
          "start_idx": 56273,
          "end_idx": 57009
        },
        "page_content": "For example, image data captured from a variety of photo-ready devices, such as smart-phones, and online social media services opened the door to the analysis of large amounts of media datasets BIBREF70. The availability of large media datasets allow new frameworks and algorithms to be proposed and tested on real-world data. A summary of some reviewed methods and benchmark datasets used for validation is reported in Table TABREF43. In addition, the performance of different GANs with respect to the benchmark datasets and performance metrics is reported in Table TABREF48. In order to synthesize images from text descriptions, many frameworks have taken a minimalistic approach by creating small and background-less images BIBREF73.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the weaknesses of their proposed interpretability quantification method?",
    "llm_answer": "The word intrusion test is expensive, requiring human effort for each evaluation, and only yields a binary decision (interpretable/not interpretable), not a measure of the degree of interpretability.\n",
    "context": [
      {
        "id": "df75b724-4bf2-4732-95f8-3094860ceceb",
        "metadata": {
          "vector_store_key": "1804.03396-5",
          "chunk_id": 55,
          "document_id": "1804.03396",
          "start_idx": 29371,
          "end_idx": 30069
        },
        "page_content": "However, the analysis on semantic roles and parsing trees cannot work very well on complicated input sentences like the 2nd and the 3rd cases. Besides, the baseline systems can hardly solve the last two cases which require inference on input sentences. Our framework works very well on this dataset with the QA measurements EM $= 91.87$ and F1 $= 93.53$ and the IE measurements can be found in Figure 3 . Most of the error cases are the fourth case which is acceptable by human annotators. Note that our framework takes the whole document as the input while the baseline systems take the individual sentence as the input, which means the experiment setting is much more difficult for our framework.",
        "type": "Document"
      },
      {
        "id": "ca2f4675-7641-443a-ac36-c601a6f8e62b",
        "metadata": {
          "vector_store_key": "1711.00331-4",
          "chunk_id": 83,
          "document_id": "1711.00331",
          "start_idx": 46397,
          "end_idx": 47163
        },
        "page_content": "Therefore, the proposed framework can be a valuable tool in guiding future research on obtaining interpretable yet effective embedding spaces for many NLP tasks that critically rely on semantic information. For instance, performance evaluation of more interpretable word embeddings on higher level NLP tasks (i.e. sentiment analysis, named entity recognition, question answering) and the relation between interpretability and NLP performance can be worthwhile. We thank the anonymous reviewers for their constructive and helpful comments that have significantly improved our paper. This work was supported in part by a European Molecular Biology Organization Installation Grant (IG 3028), by a TUBA GEBIP fellowship, and by a BAGEP 2017 award of the Science Academy.",
        "type": "Document"
      },
      {
        "id": "a02c03c3-8485-439e-a28e-4deeb304a420",
        "metadata": {
          "vector_store_key": "1810.09774-1",
          "chunk_id": 8,
          "document_id": "1810.09774",
          "start_idx": 3680,
          "end_idx": 4312
        },
        "page_content": "The second, and equally important, contribution is that our results highlight that the current NLI datasets do not capture the nuances of NLI extensively enough. The ability of NLI systems to generalize and related skepticism has been raised in a number of recent papers. BIBREF1 show that the generalization capabilities of state-of-the-art NLI systems, in cases where some kind of external lexical knowledge is needed, drops dramatically when the SNLI test set is replaced by a test set where the premise and the hypothesis are otherwise identical except for at most one word. The results show a very significant drop in accuracy.",
        "type": "Document"
      },
      {
        "id": "319799ca-bfad-464b-8569-37bcbf301e5c",
        "metadata": {
          "vector_store_key": "1711.00331-4",
          "chunk_id": 4,
          "document_id": "1711.00331",
          "start_idx": 2177,
          "end_idx": 3065
        },
        "page_content": "Model interpretability is also becoming increasingly relevant from a regulatory standpoint, as evidenced by the recent EU regulation that grants people with a \u201cright to explanation\" regarding automatic decision making algorithms BIBREF16 . Although word embeddings are a dominant part of NLP research, most studies aim to maximize the task performance on standard benchmark tests such as MEN BIBREF17 or Simlex-999 BIBREF18 . While improved test performance is undoubtedly beneficial, an embedding with enhanced performance does not necessarily reveal any insight about the semantic structure that it captures. A systematic assessment of the semantic structure intrinsic to word embeddings would enable an improved understanding of this popular approach, would allow for comparisons among different embeddings in terms of interpretability and potentially motivate new research directions.",
        "type": "Document"
      },
      {
        "id": "5fea6d89-bf55-4c2c-bd6e-a671765f81a2",
        "metadata": {
          "vector_store_key": "1711.00331-0",
          "chunk_id": 59,
          "document_id": "1711.00331",
          "start_idx": 32733,
          "end_idx": 33456
        },
        "page_content": "Although evaluating interpretability based on human judgements is an effective approach, word intrusion is an expensive method since it requires human effort for each evaluation. Furthermore, the word intrusion test does not quantify the interpretability levels of the embedding dimensions, instead it yields a binary decision as to whether a dimension is interpretable or not. However, using continuous values is more adequate than making binary evaluations since interpretability levels may vary gradually across dimensions. We propose a framework that addresses both of these issues by providing automated, continuous valued evaluations of interpretability while keeping the basis of the evaluations as human judgements.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What advantages does their proposed method of quantifying interpretability have over the human-in-the-loop evaluation they compare to?",
    "llm_answer": "Their proposed method is automated and provides continuous values for interpretability, whereas the word intrusion test is expensive and yields only binary decisions.\n",
    "context": [
      {
        "id": "738e7d38-dead-4f6e-9f40-4ce35a00c503",
        "metadata": {
          "vector_store_key": "2004.01694-5",
          "chunk_id": 1,
          "document_id": "2004.01694",
          "start_idx": 882,
          "end_idx": 1568
        },
        "page_content": "Both evaluations were based on current best practices in the field: they used a source-based direct assessment with non-expert annotators, using data sets and the evaluation protocol of the Conference on Machine Translation (WMT). While the results are intriguing, especially because they are based on best practices in MT evaluation, BIBREF5 warn against taking their results as evidence for human\u2013machine parity, and caution that for well-resourced language pairs, an update of WMT evaluation style will be needed to keep up with the progress in machine translation. We concur that these findings have demonstrated the need to critically re-evaluate the design of human MT evaluation.",
        "type": "Document"
      },
      {
        "id": "5fea6d89-bf55-4c2c-bd6e-a671765f81a2",
        "metadata": {
          "vector_store_key": "1711.00331-0",
          "chunk_id": 59,
          "document_id": "1711.00331",
          "start_idx": 32733,
          "end_idx": 33456
        },
        "page_content": "Although evaluating interpretability based on human judgements is an effective approach, word intrusion is an expensive method since it requires human effort for each evaluation. Furthermore, the word intrusion test does not quantify the interpretability levels of the embedding dimensions, instead it yields a binary decision as to whether a dimension is interpretable or not. However, using continuous values is more adequate than making binary evaluations since interpretability levels may vary gradually across dimensions. We propose a framework that addresses both of these issues by providing automated, continuous valued evaluations of interpretability while keeping the basis of the evaluations as human judgements.",
        "type": "Document"
      },
      {
        "id": "b959a395-4c48-4b12-9e38-272c30c2a1b0",
        "metadata": {
          "vector_store_key": "1711.00331-0",
          "chunk_id": 60,
          "document_id": "1711.00331",
          "start_idx": 33082,
          "end_idx": 33773
        },
        "page_content": "We propose a framework that addresses both of these issues by providing automated, continuous valued evaluations of interpretability while keeping the basis of the evaluations as human judgements. The basic idea behind our framework is that humans interpret dimensions by trying to group the most distinctive words in the dimensions (i.e. top or bottom rank words), an idea also leveraged by the word intrusion test. Based on this key idea, it can be noted that if a dataset represents all the possible groups humans can form, instead of relying on human evaluations, one can simply check whether the distinctive words of the embedding dimensions are present together in any of these groups.",
        "type": "Document"
      },
      {
        "id": "27b06e21-47ad-4f88-a432-147d29bcb2a4",
        "metadata": {
          "vector_store_key": "2004.01694-5",
          "chunk_id": 2,
          "document_id": "2004.01694",
          "start_idx": 1221,
          "end_idx": 1893
        },
        "page_content": "We concur that these findings have demonstrated the need to critically re-evaluate the design of human MT evaluation. Our paper investigates three aspects of human MT evaluation, with a special focus on assessing human\u2013machine parity: the choice of raters, the use of linguistic context, and the creation of reference translations. We focus on the data shared by BIBREF3, and empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation. We find that for all three aspects, human translations are judged more favourably, and significantly better than MT, when we make changes that we believe strengthen the evaluation design.",
        "type": "Document"
      },
      {
        "id": "8e08d1ba-2d60-49ef-bb6e-78f7af40117a",
        "metadata": {
          "vector_store_key": "2002.05058-2",
          "chunk_id": 45,
          "document_id": "2002.05058",
          "start_idx": 25160,
          "end_idx": 25851
        },
        "page_content": "We can see that the proposed comparative evaluator correlates far better with human judgment than BLEU and perplexity. When compared with recently proposed parameterized metrics including adversarial evaluator and ADEM, our model consistently outperforms them by a large margin, which demonstrates that our comparison-based evaluation metric is able to evaluate sample quality more accurately. In addition, we find that evaluating generated samples by comparing it with a set of randomly selected samples or using sample-level skill rating performs almost equally well. This is not surprising as the employed skill rating is able to handle the inherent variance of players (i.e. NLG models).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How was lexical diversity measured?",
    "llm_answer": "MTLD, Distinct-1, Distinct-2, Diversity-32, Diversity-AUC, Yule's I, and vector comparison measures.\n",
    "context": [
      {
        "id": "a521ec55-8888-471b-b041-19e128173cd5",
        "metadata": {
          "vector_store_key": "1703.05260-9",
          "chunk_id": 50,
          "document_id": "1703.05260",
          "start_idx": 27555,
          "end_idx": 28223
        },
        "page_content": "However, this measure is known to be sensitive to text length (see e.g. Tweedie1998), which would result in very small values for InScript and relatively large ones for DeScript, given the large average difference of text lengths between the corpora. Instead, we decided to use the Measure of Textual Lexical Diversity (MTLD) (McCarthy2010, McCarthy2005), which is familiar in corpus linguistics. This metric measures the average number of tokens in a text that are needed to retain a type-token ratio above a certain threshold. If the MTLD for a text is high, many tokens are needed to lower the type-token ratio under the threshold, so the text is lexically diverse.",
        "type": "Document"
      },
      {
        "id": "293b77af-2b12-4a88-b70a-5228ad0bd530",
        "metadata": {
          "vector_store_key": "2001.05467-1",
          "chunk_id": 26,
          "document_id": "2001.05467",
          "start_idx": 14255,
          "end_idx": 15066
        },
        "page_content": "BIBREF18 (BIBREF18) also made similar observations on BLEU. To evaluate diversity, we employ two evaluation metrics from previous work, namely Distinct-1 and Distinct-2 BIBREF2. These are the ratios between the number of unique tokens and all tokens for unigrams and bigrams, respectively. In addition, we propose a novel diversity graph and its corresponding metric, which we name Diversity-32 and Diversity-AUC, respectively. We gather statistics of sentence, unigram, bigram and trigram, and sort their normalized frequencies from highest to lowest. Observing that all four graphs follow long-tail distributions, we only keep the highest 32 frequencies and plot them. We then calculate one minus the Area under Curve (Diversity-AUC) for each graph, which draws a high-level picture of how diverse a model is.",
        "type": "Document"
      },
      {
        "id": "363eccaa-3da0-44b0-9e25-d2091f41a385",
        "metadata": {
          "vector_store_key": "1911.00133-1",
          "chunk_id": 22,
          "document_id": "1911.00133",
          "start_idx": 11709,
          "end_idx": 12388
        },
        "page_content": "We also examine the overall lexical diversity of each domain by calculating Yule's I measure BIBREF13. fig:domain-yule shows the lexical diversity of our data, both for all words in the vocabulary and for only words in LIWC's \u201cnegemo\u201d word list. Yule's I measure reflects the repetitiveness of the data (as opposed to the broader coverage measured by our LIWC analysis). We notice exceptionally low lexical diversity for the mental illness domains, which we believe is due to the structured, clinical language surrounding mental illnesses. For example, posters in these domains discuss topics such as symptoms, medical care, and diagnoses (fig:stress-example, tab:data-examples).",
        "type": "Document"
      },
      {
        "id": "a8da3e5b-c736-4668-9bf7-e3775e05839c",
        "metadata": {
          "vector_store_key": "1702.06777-0",
          "chunk_id": 10,
          "document_id": "1702.06777",
          "start_idx": 5489,
          "end_idx": 6217
        },
        "page_content": "We follow a majority criterion, i.e., we depict the cell with the keyword color whose absolute frequency is maximum. This procedure nicely yields a useful geographical representation of how the different variants for a concept are distributed over the space. The dialectometric differences are quantified between regions defined with the aid of our cells. For this purpose we take into account two metrics, which we now briefly discuss. This metric is a vector comparison measure. It is widely used in text classification, information retrieval and data mining BIBREF20 . Let INLINEFORM0 and INLINEFORM1 be two vectors whose components are given by the relative frequencies of the lexical variations for a concept within a cell.",
        "type": "Document"
      },
      {
        "id": "f2c3cda6-3bba-4962-8b98-3dbd95e8db64",
        "metadata": {
          "vector_store_key": "1804.08186-6",
          "chunk_id": 113,
          "document_id": "1804.08186",
          "start_idx": 64862,
          "end_idx": 65537
        },
        "page_content": "BIBREF92 , BIBREF314 summed up the feature weights and divided them by the number of words in the test document in order to set a threshold to detect unknown languages. Their language identifier obtained 89% precision and 94% recall when classifying documents between five languages. BIBREF192 used a weighting method combining alphabets, prefixes, suffixes and words. BIBREF233 summed up values from a word trigram ranking, basic dictionary and morphological analyzer lookup. BIBREF282 summed up language labels of the surrounding words to identify the language of the current word. BIBREF200 summed up points awarded by the presence of morphological and syntactic features.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which existing models does this approach outperform?",
    "llm_answer": "State-of-the-art methods, VHRED (attn), and Reranking-RL.  Also, all other models by a large margin (with a hybrid model).\n",
    "context": [
      {
        "id": "11bbd522-28ef-4580-9084-84720bfb9758",
        "metadata": {
          "vector_store_key": "2001.06888-0",
          "chunk_id": 42,
          "document_id": "2001.06888",
          "start_idx": 22556,
          "end_idx": 22770
        },
        "page_content": "Evaluation results of our proposed model compared to other state of the art methods show its superiority to these methods in overall while in two categories (Person and Miscellaneous) our model outperformed others.",
        "type": "Document"
      },
      {
        "id": "c282a797-7698-46e0-b5da-692be2bae246",
        "metadata": {
          "vector_store_key": "1804.08186-2",
          "chunk_id": 294,
          "document_id": "1804.08186",
          "start_idx": 167030,
          "end_idx": 167629
        },
        "page_content": "The types of models as well as the sources of training data used in the literature are diverse, and work to date has not compared and evaluated these in a systematic manner, making it difficult to draw broader conclusions about what the \u201cbest\u201d method for actually is. We have attempted to synthesize results to date to identify a set of \u201cbest practices\u201d, but these should be treated as guidelines and should always be considered in the broader context of a target application. Existing work on serves to illustrate that the scope and depth of the problem are much greater than they may first appear.",
        "type": "Document"
      },
      {
        "id": "ad1d7753-3848-45ad-a7ec-2f9bff0e735c",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 18,
          "document_id": "1912.13109",
          "start_idx": 9307,
          "end_idx": 10125
        },
        "page_content": "While the model probability is the probability that the observation i belongs to category c. Among the model architectures we experimented with and without data augmentation were: Fully Connected dense networks: Model hyperparameters were inspired from the previous work done by Vo et al and Mathur et al. This was also used as a baseline model but we did not get appreciable performance on such architecture due to FC networks not being able to capture local and long term dependencies. Convolution based architectures: Architecture and hyperparameter choices were chosen from the past study Deon on the subject. We were able to boost the performance as compared to only FC based network but we noticed better performance from architectures that are suitable to sequences such as text messages or any timeseries data.",
        "type": "Document"
      },
      {
        "id": "bbc12882-454b-4106-942a-a7df2d750084",
        "metadata": {
          "vector_store_key": "2001.05467-3",
          "chunk_id": 31,
          "document_id": "2001.05467",
          "start_idx": 17277,
          "end_idx": 17972
        },
        "page_content": "The F1 results are shown in Table TABREF24. Among all single models, LFT performs the best, followed by MinAvgOut. RL is also comparable with previous state-of-the-art models VHRED (attn) and Reranking-RL. We think that this is because LFT exerts no force in pulling the model predictions away from the ground-truth tokens, but rather just makes itself aware of how dull each response is. Consequently, its responses appear more relevant than the other two approaches. Moreover, the hybrid model (last row) outperforms all other models by a large margin. One might expect that minimizing AVGOUT causes the models to move further away from the ground-truth tokens, so that it will hurt relevance.",
        "type": "Document"
      },
      {
        "id": "07b84dd0-f246-4c86-b216-a68029541a1a",
        "metadata": {
          "vector_store_key": "2002.05058-5",
          "chunk_id": 53,
          "document_id": "2002.05058",
          "start_idx": 30143,
          "end_idx": 30900
        },
        "page_content": "We evaluate these model variants on the Dailydialog dataset. Results are presented in Table 5. We can see that comparison-based evaluation is very effective as our model correlates much better than adversarial evaluator. The tie option is also very important as it can prevent the comparative evaluator from making uncertain decision and model the inductive bias that samples generated by the same model are generally of similar quality, which may help our model generalize better. As for different sources of training examples, we find that human preference annotation is the most important, which is not surprising. In addition, we find that the proposed weak supervision also helps, but is of smaller relative importance compared with strong supervision.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What human evaluation method is proposed?",
    "llm_answer": "Score-based human evaluation.\n",
    "context": [
      {
        "id": "738ef3d4-03db-438a-b62e-82917bd577f7",
        "metadata": {
          "vector_store_key": "2002.05058-8",
          "chunk_id": 40,
          "document_id": "2002.05058",
          "start_idx": 22381,
          "end_idx": 23078
        },
        "page_content": "We choose to perform score-based human evaluation for four reasons: 1) the ADEM baseline requires human-annotated score as training examples, 2) we can construct up to $\\binom{2n}{2}$ training examples for our comparative evaluator with $n$ human-annotated scores, 3) score-based human evaluation facilitates the evaluation of correlation scores, and 4) as all other metrics do not perform pairwise comparison, using pairwise human evaluation will likely be biased toward our approach. We sample 20 generated samples from each model (out of 5) of the 20 evaluation groups. We invite 20 human annotators which are all graduate students with good English language proficiency to score these samples.",
        "type": "Document"
      },
      {
        "id": "ddb901c5-37a7-4368-95c0-b79d400a58b7",
        "metadata": {
          "vector_store_key": "1805.04033-8",
          "chunk_id": 52,
          "document_id": "1805.04033",
          "start_idx": 28744,
          "end_idx": 29376
        },
        "page_content": "The proposed human evaluation method is effective and efficient in judging the semantic consistency, which is absent in previous work but is crucial in the accurate evaluation of the text summarization systems. The proposed metric is simple to conduct and easy to interpret. It also provides an insight on how practicable the existing systems are in the real-world scenario. For human evaluation, the annotators are asked to evaluate the summary against the source content based on the goodness of the summary. If the summary is not understandable, relevant or correct according to the source content, the summary is considered bad.",
        "type": "Document"
      },
      {
        "id": "174627da-e53d-491f-84f7-65d4806ef65d",
        "metadata": {
          "vector_store_key": "2004.01694-4",
          "chunk_id": 62,
          "document_id": "2004.01694",
          "start_idx": 35148,
          "end_idx": 35678
        },
        "page_content": "To that end, we have offered a set of recommendations, supported by empirical data, which we believe are needed for assessing human\u2013machine parity, and will strengthen the human evaluation of MT in general. Our recommendations have the aim of increasing the validity of MT evaluation, but we are aware of the high cost of having MT evaluation done by professional translators, and on the level of full documents. We welcome future research into alternative evaluation protocols that can demonstrate their validity at a lower cost.",
        "type": "Document"
      },
      {
        "id": "11bbd522-28ef-4580-9084-84720bfb9758",
        "metadata": {
          "vector_store_key": "2001.06888-0",
          "chunk_id": 42,
          "document_id": "2001.06888",
          "start_idx": 22556,
          "end_idx": 22770
        },
        "page_content": "Evaluation results of our proposed model compared to other state of the art methods show its superiority to these methods in overall while in two categories (Person and Miscellaneous) our model outperformed others.",
        "type": "Document"
      },
      {
        "id": "73fa9d24-b35d-4e01-85d3-6a04bf538c21",
        "metadata": {
          "vector_store_key": "2002.05058-8",
          "chunk_id": 39,
          "document_id": "2002.05058",
          "start_idx": 22071,
          "end_idx": 23097
        },
        "page_content": "As human evaluation is expensive, sample-level evaluation is performed jointly with model-level evaluation, which is also used for evaluating the ability of different metrics for performing hyperparameter search and early-stopping. Concretely, we perform 10 groups of evaluations for performing hyperparameter selecting and early-stopping with five compared automated metrics. In each evaluation, each of the five compared metrics is used to select the best hyperparameter combination or early-stopping checkpoint with other variants fixed. We choose to perform score-based human evaluation for four reasons: 1) the ADEM baseline requires human-annotated score as training examples, 2) we can construct up to $\\binom{2n}{2}$ training examples for our comparative evaluator with $n$ human-annotated scores, 3) score-based human evaluation facilitates the evaluation of correlation scores, and 4) as all other metrics do not perform pairwise comparison, using pairwise human evaluation will likely be biased toward our approach.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What languages are represented in the dataset?",
    "llm_answer": "Amharic, Arabic, Hindi, Somali, English, French, German, Russian, Italian, Bengali, Telegu, Mandarin, Cantonese, Indonesian, Japanese, Korean, and Vietnamese.\n",
    "context": [
      {
        "id": "9cd0820a-fcb8-4bf5-b481-c19a5a179e77",
        "metadata": {
          "vector_store_key": "1909.09270-5",
          "chunk_id": 26,
          "document_id": "1909.09270",
          "start_idx": 14042,
          "end_idx": 14768
        },
        "page_content": "These languages are: Amharic (amh: LDC2016E87), Arabic (ara: LDC2016E89), Hindi (hin: LDC2017E62), and Somali (som: LDC2016E91). These come from a variety of sources including discussion forums, newswire, and social media. The labelset is Person, Organization, Location, Geo-political entity. We define train/development/test splits, taking care to keep a similar distribution of genres in each split. Data statistics for all languages are shown in Table TABREF25. We create partial annotations by perturbing gold annotated data in two ways: lowering recall (to simulate missing entities), and lowering precision (to simulate noisy annotations). To lower recall, we replace gold named entity tags with $O$ tags (for non-name).",
        "type": "Document"
      },
      {
        "id": "fdf582fb-3fd0-4e0a-9bba-39ff6f6c125d",
        "metadata": {
          "vector_store_key": "1909.06522-0",
          "chunk_id": 22,
          "document_id": "1909.06522",
          "start_idx": 11687,
          "end_idx": 12449
        },
        "page_content": "For each language, the train and test set size are described in Table TABREF10, and most training data were Pages. On each language we also had a small validation set for model parameter tuning. Each monolingual ASR baseline was trained on language-specific data only. The character sets of these 7 languages have little overlap except that (i) they all include common basic Latin alphabet, and (ii) both Hindi and Marathi use Devanagari script. We took the union of 7 character sets therein as the multilingual grapheme set (Section SECREF2), which contained 432 characters. In addition, we deliberately split 7 languages into two groups, such that the languages within each group were more closely related in terms of language family, orthography or phonology.",
        "type": "Document"
      },
      {
        "id": "2f8e9d5b-32b1-4b15-bfdb-c136c904ee9c",
        "metadata": {
          "vector_store_key": "1910.11493-2",
          "chunk_id": 9,
          "document_id": "1910.11493",
          "start_idx": 5320,
          "end_idx": 6006
        },
        "page_content": "Data for all but four languages (Basque, Kurmanji, Murrinhpatha, and Sorani) are extracted from English Wiktionary, a large multi-lingual crowd-sourced dictionary with morphological paradigms for many lemmata. 20 of the 100 language pairs are either distantly related or unrelated; this allows speculation into the relative importance of data quantity and linguistic relatedness. For each language, the basic data consists of triples of the form (lemma, feature bundle, inflected form), as in tab:sub1data. The first feature in the bundle always specifies the core part of speech (e.g., verb). For each language pair, separate files contain the high- and low-resource training examples.",
        "type": "Document"
      },
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      },
      {
        "id": "a22cce10-cf7d-4a75-a5a2-99d8a3f066ee",
        "metadata": {
          "vector_store_key": "1705.03151-5",
          "chunk_id": 46,
          "document_id": "1705.03151",
          "start_idx": 25347,
          "end_idx": 26030
        },
        "page_content": "These are: Mandarin, Cantonese, Indonesian, Japanese, Russian, Korean and Vietnamese. The data volume for each language is approximately 10 hours of speech signals recorded by 24 speakers (12 males and 12 females), with each speaker recording approximately 300 utterances in reading style by mobile phones, with a sampling rate of 16kHz and a sample size of 16 bits. Each dataset was split into a training set consisting of 18 speakers, and a test set consisting of 6 speakers. For Mandarin, Cantonese, Vietnamese and Indonesian, the recording was conducted in a quiet environment. For Russian, Korean and Japanese, there are 2 recording conditions for each speaker, quiet and noisy.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How faster is training and decoding compared to former models?",
    "llm_answer": "Training is similar in speed to BIAF, but decoding is fastest, followed by BIAF. STACKPTR is significantly slower for both.\n",
    "context": [
      {
        "id": "fde1c807-f9a0-42fa-846b-e0ce91a8d657",
        "metadata": {
          "vector_store_key": "1607.06025-9",
          "chunk_id": 67,
          "document_id": "1607.06025",
          "start_idx": 39177,
          "end_idx": 39962
        },
        "page_content": "The best model achieved INLINEFORM0 accuracy, which is only INLINEFORM1 less than the accuracy of the classifier trained on the original human written dataset, while the best dataset combined with the original dataset has achieved the highest accuracy. This model learns a decoder and a mapping embedding for each training example. It outperforms the more standard encoder-decoder networks. Although more parameters are needed to be trained, less are updated on each batch. We have also shown that the attention mechanism improves the model. The analysis has confirmed our hypothesis that a good dataset contains accurate, non-trivial and comprehensible examples. To further examine the quality of generated hypothesis, they were compared against the original human written hypotheses.",
        "type": "Document"
      },
      {
        "id": "c9cbfb5c-ad62-46cb-8e57-6760a5e6db14",
        "metadata": {
          "vector_store_key": "1707.00110-3",
          "chunk_id": 27,
          "document_id": "1707.00110",
          "start_idx": 14497,
          "end_idx": 15228
        },
        "page_content": "We measured decoding time over the full validation set, not including time used for model setup and data loading, averaged across 10 runs. The average sequence length for examples in this data was 35, and we expect more significant speedups for tasks with longer sequences, as suggested by our experiments on toy data. Note that in our NMT examples/experiments, $K\\approx T$ , but we obtain computational savings from the fact that $K \\ll D$ . We may be able to set $K \\ll T$ , as in toy copying, and still get very good performance in other tasks. For instance, in summarization the source is complex but the representation of the source required to perform the task is \"simple\" (i.e. all that is needed to generate the abstract).",
        "type": "Document"
      },
      {
        "id": "874d1312-9fdb-46fe-865b-ce75dcb445d6",
        "metadata": {
          "vector_store_key": "1812.07023-3",
          "chunk_id": 32,
          "document_id": "1812.07023",
          "start_idx": 17666,
          "end_idx": 18428
        },
        "page_content": "We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below: Our primary architectural differences over the baseline model are: not concatenating the question, answer pairs before encoding them, the auxiliary decoder module, and using the Time-Extended FiLM module for feature extraction. These, combined with using scheduled sampling and running hyperparameter optimization over the validation set to select hyperparameters, give us the observed performance boost. We observe that our models generate fairly relevant responses to questions in the dialogues, and models with audio-visual inputs respond to audio-visual questions (e.g. \u201cis there any voices or music ?\u201d) correctly more often.",
        "type": "Document"
      },
      {
        "id": "6824b4f2-136e-4c26-a2a7-a10cf7d4c97e",
        "metadata": {
          "vector_store_key": "1810.00663-8",
          "chunk_id": 46,
          "document_id": "1810.00663",
          "start_idx": 25788,
          "end_idx": 26550
        },
        "page_content": "The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as can be seen in performance drops in Table TABREF28 for the new environments. Nonetheless, the insights from the previous section still hold: masking in the output layer and reordering the graph triplets tend to increase performance. Even though the results in Table TABREF28 suggest that there is room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM.",
        "type": "Document"
      },
      {
        "id": "1f0f6a33-fa92-4b5f-8679-bad3fe4f9f34",
        "metadata": {
          "vector_store_key": "1911.08673-4",
          "chunk_id": 42,
          "document_id": "1911.08673",
          "start_idx": 22595,
          "end_idx": 23233
        },
        "page_content": "In order to verify the time complexity analysis of our model, we measured the running time and speed of BIAF, STACKPTR and our model on PTB training and development set using the projective algorithm. The comparison in Table TABREF24 shows that in terms of convergence time, our model is basically the same speed as BIAF, while STACKPTR is much slower. For decoding, our model is the fastest, followed by BIAF. STACKPTR is unexpectedly the slowest. This is because the time cost of attention scoring in decoding is not negligible when compared with the processing speed and actually even accounts for a significant portion of the runtime.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What datasets are used to evaluate the model?",
    "llm_answer": "novel-50, IMDB62, AIDA-B, and other datasets (news and encyclopedia corpus).\n",
    "context": [
      {
        "id": "9a908f8f-2077-4b70-85b8-2941b3d7ba32",
        "metadata": {
          "vector_store_key": "1708.06022-2",
          "chunk_id": 33,
          "document_id": "1708.06022",
          "start_idx": 18725,
          "end_idx": 19404
        },
        "page_content": "In the following we introduce these datasets, provide implementation details for our model, describe the systems used for comparison, and present our results. Our model was trained on three datasets, representative of different types of QA tasks. The first two datasets focus on question answering over a structured knowledge base, whereas the third one is specific to answer sentence selection. This dataset BIBREF31 contains $3,778$ training instances and $2,032$ test instances. Questions were collected by querying the Google Suggest API. A breadth-first search beginning with wh- was conducted and the answers were crowd-sourced using Freebase as the backend knowledge base.",
        "type": "Document"
      },
      {
        "id": "3d80e22b-95de-45ee-854b-6dd9c24fc625",
        "metadata": {
          "vector_store_key": "1709.02271-3",
          "chunk_id": 27,
          "document_id": "1709.02271",
          "start_idx": 15339,
          "end_idx": 16061
        },
        "page_content": "To confirm that our models generalize, we pick the best models from the baseline-dataset experiments and evaluate on the novel-50 and IMDB62 datasets. For novel-50, the chunking size applied is 2000-word as per the baseline-dataset experiment results, and for IMDB62, texts are not chunked (i.e., we feed the models with the original reviews directly). For model comparison, we also run the SVMs (i.e., SVM2 and SVM2-PV) used in the baseline-dataset experiment. All the experiments conducted here are multi-class classification with macro-averaged F1 evaluation. Model configurations. Following F15, we perform 5-fold cross-validation. The embedding sizes are tuned on novel-9 (multi-class condition): 50 for char-bigrams;",
        "type": "Document"
      },
      {
        "id": "458b51fc-c0d4-482d-90b3-d4cefed66ac3",
        "metadata": {
          "vector_store_key": "1902.00330-3",
          "chunk_id": 37,
          "document_id": "1902.00330",
          "start_idx": 20853,
          "end_idx": 21565
        },
        "page_content": "We implement our models in Tensorflow and run experiments on 4 Tesla V100 GPU. We conduct experiments on several different types of public datasets including news and encyclopedia corpus. The training set is AIDA-Train and Wikipedia datasets, where AIDA-Train contains 18448 mentions and Wikipedia contains 25995 mentions. In order to compare with the previous methods, we evaluate our model on AIDA-B and other datasets. These datasets are well-known and have been used for the evaluation of most entity linking systems. The statistics of the datasets are shown in Table 1. AIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.",
        "type": "Document"
      },
      {
        "id": "795c2b4f-cbcd-4a74-bace-ccc94b474ac8",
        "metadata": {
          "vector_store_key": "1705.10586-3",
          "chunk_id": 26,
          "document_id": "1705.10586",
          "start_idx": 15083,
          "end_idx": 15665
        },
        "page_content": "The datasets are a good mix of polished (AG) and noisy (Yelp and Amazon reviews), long (Sogou) and short (DBP and AG), large (Amazon reviews) and small (AG) datasets. And thus the results over these datasets serve as good evaluation on the quality of the model. In this paper, we take 128 ASCII characters as character set, by which most of the testing documents are composite. We define word length as 20 and character embedding length as 100. If a word with characters less than 20, we will pad it with zeros. If the length is larger than 20, we just take the first 20 characters.",
        "type": "Document"
      },
      {
        "id": "951de0ef-328c-4f84-83e7-f3d72ab2203d",
        "metadata": {
          "vector_store_key": "1909.00154-1",
          "chunk_id": 38,
          "document_id": "1909.00154",
          "start_idx": 20824,
          "end_idx": 21499
        },
        "page_content": "Encode choice models for train, development and test sets using the learned embeddings Estimate choice model accordingly using its train set Evaluate the new model using the test set Since there is stochasticity in the embeddings training model, we will repeat the above multiple times, for the different experiments in the paper (and report the respective mean and standard deviation statistics). Whenever we want to analyse a particular model (e.g. to check the coefficients of a choice model), we select the one with the highest likelihood at the development set (i.e. in practice, its out-of-sample generalization performance), and report its performance on the test set.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the source of the dataset?",
    "llm_answer": "The SCI dataset is from the OA-STM corpus, derived from the 10 most published disciplines, with sentences randomly selected from articles.  The avsd dataset uses videos from the Charades dataset.\n",
    "context": [
      {
        "id": "8e741c56-0352-4660-89e2-d995b2fada35",
        "metadata": {
          "vector_store_key": "1807.09671-7",
          "chunk_id": 34,
          "document_id": "1807.09671",
          "start_idx": 19653,
          "end_idx": 20306
        },
        "page_content": "While the 2nd and 3rd data sets are from the same course, Statistics for Industrial Engineers, they were taught in 2015 and 2016 respectively (henceforth Stat2015 and Stat2016), at the Bo\u01e7azi\u00e7i University in Turkey. The course was taught in English while the official language is Turkish. The last one is from a fundamental undergraduate Computer Science course (data structures) at a local U.S. university taught in 2016 (henceforth CS2016). Another reason we choose the student responses is that we have advanced annotation allowing us to perform an intrinsic evaluation to test whether the low-rank approximation does capture similar concepts or not.",
        "type": "Document"
      },
      {
        "id": "c5f16988-f9bb-43d0-a24e-a2d52ae2d42a",
        "metadata": {
          "vector_store_key": "1802.05574-4",
          "chunk_id": 11,
          "document_id": "1802.05574",
          "start_idx": 6530,
          "end_idx": 7227
        },
        "page_content": "This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available and the corpus provides both an XML and a simple text version of each article. We randomly selected 2 sentences with more than two words from each paper using the simple text version of the paper.",
        "type": "Document"
      },
      {
        "id": "9b758cdd-bd9a-401f-8afd-ea03ad4e7faa",
        "metadata": {
          "vector_store_key": "1905.10044-9",
          "chunk_id": 21,
          "document_id": "1905.10044",
          "start_idx": 11683,
          "end_idx": 12403
        },
        "page_content": "Part of the value of this dataset is that it contains questions that people genuinely want to answer. To explore this further, we manually define a set of topics that questions can be about. An author categorized 200 questions into these topics. The results can be found in the upper half of Table . Questions were often about entertainment media (including T.V., movies, and music), along with other popular topics like sports. However, there are still a good portion of questions asking for more general factual knowledge, including ones about historical events or the natural world. We also broke the questions into categories based on what kind of information they were requesting, shown in the lower half of Table .",
        "type": "Document"
      },
      {
        "id": "73a83f60-b2c9-406d-b4cd-5720639e5f06",
        "metadata": {
          "vector_store_key": "1812.07023-4",
          "chunk_id": 18,
          "document_id": "1812.07023",
          "start_idx": 10358,
          "end_idx": 10986
        },
        "page_content": "After 10 such qa turns, the questioner wraps up by writing a summary of the video based on the conversation. Dataset statistics such as the number of dialogues, turns, and words for the avsd dataset are presented in Table TABREF5 . For the initially released prototype dataset, the training set of the avsd dataset corresponds to videos taken from the training set of the Charades dataset while the validation and test sets of the avsd dataset correspond to videos taken from the validation set of the Charades dataset. For the official dataset, training, validation and test sets are drawn from the corresponding Charades sets.",
        "type": "Document"
      },
      {
        "id": "8be62763-27f4-46ff-b2bb-edb6a6b2fa04",
        "metadata": {
          "vector_store_key": "1607.06025-3",
          "chunk_id": 42,
          "document_id": "1607.06025",
          "start_idx": 24463,
          "end_idx": 25198
        },
        "page_content": "The new dataset consists of training set, which is generated using examples from the original training set, and a development set, which is generated from the original development set. The beam size for beam search was set to 1. The details of the decision are presented in Section SECREF35 . Some datasets were constructed by filtering the generated datasets according to various thresholds. Thus, the generated datasets were constructed to contain enough examples, so that the filtered datasets had at least the number of examples as the original dataset. In the end, all the datasets were trimmed down to the size of the original dataset by selecting the samples sequentially from the beginning until the dataset had the right size.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What were the baselines?",
    "llm_answer": "ANRES_Sing_R, ANRES_Sing_Z, ANRES_Seq_R, ANRES_Seq_Z,  i-vector system, x-vector system.\n",
    "context": [
      {
        "id": "fa88df39-033d-4e0f-891b-162e0f9b0424",
        "metadata": {
          "vector_store_key": "1709.08299-4",
          "chunk_id": 19,
          "document_id": "1709.08299",
          "start_idx": 11042,
          "end_idx": 11836
        },
        "page_content": "The neural baselines are built by the following steps. We first use the shared training data to build a general systems, and choose the best performing model (in terms of cloze validation set) as baseline. Use User Query validation data for further tuning the systems with 10-fold cross-validations. Increase dropout rate BIBREF9 to 0.5 for preventing over-fitting issue. All baseline systems are chosen according to the performance of the validation set. The participant system results are given in Table 2 and 3 . As we can see that two neural baselines are competitive among participant systems and AoA Reader successfully outperform AS Reader and all participant systems in single model condition, which proves that it is a strong baseline system even without further fine-tuning procedure.",
        "type": "Document"
      },
      {
        "id": "02bef31f-a2b5-4b89-8063-62dc0b0f91eb",
        "metadata": {
          "vector_store_key": "1911.01799-4",
          "chunk_id": 19,
          "document_id": "1911.01799",
          "start_idx": 11188,
          "end_idx": 11946
        },
        "page_content": "Two state-of-the-art baseline systems were built following the Kaldi SITW recipe BIBREF22: an i-vector system BIBREF3 and an x-vector system BIBREF10. For the i-vector system, the acoustic feature involved 24-dimensional MFCCs plus the log energy, augmented by the first- and second-order derivatives. We also applied the cepstral mean normalization (CMN) and the energy-based voice active detection (VAD). The universal background model (UBM) consisted of $2,048$ Gaussian components, and the dimensionality of the i-vector space was 400. LDA was applied to reduce the dimensionality of the i-vectors to 150. The PLDA model was used for scoring BIBREF4. For the x-vector system, the feature-learning component was a 5-layer time-delay neural network (TDNN).",
        "type": "Document"
      },
      {
        "id": "348b5dbb-b442-4f57-9f6e-c2038d069875",
        "metadata": {
          "vector_store_key": "1709.08299-4",
          "chunk_id": 16,
          "document_id": "1709.08299",
          "start_idx": 9232,
          "end_idx": 9901
        },
        "page_content": "The paragraph should be read carefully and judged whether appropriate for asking questions No more than 5 questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models.",
        "type": "Document"
      },
      {
        "id": "6b22fb3d-fcd5-44c9-bc40-359ccb0e34cc",
        "metadata": {
          "vector_store_key": "1805.04033-6",
          "chunk_id": 34,
          "document_id": "1805.04033",
          "start_idx": 18664,
          "end_idx": 19368
        },
        "page_content": "To gain a better understanding of the results, we analyze the summaries generated by the baseline model and our proposed model. Some of the summaries are listed in Table TABREF28 . As shown in the table, the summaries generated by the proposed method are much better than the baseline, and we believe they are more precise and informative than the references. In the first one, the baseline system generates a grammatical but unrelated summary, while the proposed method generates a more informative summary. In the second one, the baseline system generates a related but ungrammatical summary, while the proposed method generates a summary related to the source content but different from the reference.",
        "type": "Document"
      },
      {
        "id": "64beee2a-a1a9-454b-917e-917a34317fa7",
        "metadata": {
          "vector_store_key": "2004.01878-3",
          "chunk_id": 27,
          "document_id": "2004.01878",
          "start_idx": 14569,
          "end_idx": 15289
        },
        "page_content": "In summary, the following four baselines are designed: ANRES_Sing_R: randomly initializing the states for each single trading day. ANRES_Sing_Z: initializing the states as zeros for each single trading day. ANRES_Seq_R: randomly initializing the first states for each trading sequence only. ANRES_Seq_Z: initializing the first states as zeros for each trading sequence only. Development set results on predicting S&P 500 index are shown in Table TABREF13. We can see that modeling recurrent value sequences performs better than treating each trading day separately, which shows that modeling trading sequences can capture the correlations between trading days and the non-linear compositional effects of multiple events.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What metadata is included?",
    "llm_answer": "Service name, supported tasks (intents), entity attributes (slots), natural language descriptions of service, intents, and slots.\n",
    "context": [
      {
        "id": "88f907c8-82e0-462a-879c-01a06299bbd7",
        "metadata": {
          "vector_store_key": "1610.00879-1",
          "chunk_id": 7,
          "document_id": "1610.00879",
          "start_idx": 4244,
          "end_idx": 4995
        },
        "page_content": "Heuristics other than these hashtags could have been used for dataset creation. For example, timestamps were a good option to account for time at which a tweet was posted. However, this could not be used because user's local times was not available, since very few users had geolocation enabled. The complete set of features is shown in Table TABREF7 . There are two sets of features: (a) N-gram features, and (b) Stylistic features. We use unigrams and bigrams as N-gram features- considering both presence and count. Table TABREF7 shows the complete set of stylistic features of our prediction system. POS ratios are a set of features that record the proportion of each POS tag in the dataset (for example, the proportion of nouns/adjectives, etc.).",
        "type": "Document"
      },
      {
        "id": "e491c3dd-3bc4-4ae9-bd7e-6e01ad2969a7",
        "metadata": {
          "vector_store_key": "1911.00133-1",
          "chunk_id": 28,
          "document_id": "1911.00133",
          "start_idx": 14845,
          "end_idx": 15743
        },
        "page_content": "In addition to the words of the posts (both as bag-of-n-grams and distributed word embeddings), we include features in three categories: Lexical features. Average, maximum, and minimum scores for pleasantness, activation, and imagery from the Dictionary of Affect in Language (DAL) BIBREF16; the full suite of 93 LIWC features; and sentiment calculated using the Pattern sentiment library BIBREF17. Syntactic features. Part-of-speech unigrams and bigrams, the Flesch-Kincaid Grade Level, and the Automated Readability Index. Social media features. The UTC timestamp of the post; the ratio of upvotes to downvotes on the post, where an upvote roughly corresponds to a reaction of \u201clike\u201d and a downvote to \u201cdislike\u201d (upvote ratio); the net score of the post (karma) (calculated by Reddit, $n_\\text{upvotes} - n_\\text{downvotes}$); and the total number of comments in the entire thread under the post.",
        "type": "Document"
      },
      {
        "id": "46ef86e6-c75a-4cd0-b930-e855c4d48743",
        "metadata": {
          "vector_store_key": "2002.01359-0",
          "chunk_id": 16,
          "document_id": "2002.01359",
          "start_idx": 9091,
          "end_idx": 9830
        },
        "page_content": "The schema contains details like the name of the service, the list of tasks supported by the service (intents) and the attributes of the entities used by the service (slots). The schema also contains natural language descriptions of the service, intents and slots which can be used for developing models which can condition their predictions on the schema. To reflect the constraints present in real-world services and APIs, we impose a few constraints on the data. Our dataset does not expose the set of all possible values for certain slots. Having such a list is impractical for slots like date or time because they have infinitely many possible values or for slots like movie or song names, for which new values are periodically added.",
        "type": "Document"
      },
      {
        "id": "a0c3055d-14ab-4e16-ab2c-92f60f43109f",
        "metadata": {
          "vector_store_key": "1903.00172-3",
          "chunk_id": 43,
          "document_id": "1903.00172",
          "start_idx": 25534,
          "end_idx": 26416
        },
        "page_content": "Any accompanying software and/or data should include licenses and documentation of research review as appropriate. Supplementary material may report preprocessing decisions, model parameters, and other details necessary for the replication of the experiments reported in the paper. Seemingly small preprocessing decisions can sometimes make a large difference in performance, so it is crucial to record such decisions to precisely characterize state-of-the-art methods. Nonetheless, supplementary material should be supplementary (rather than central) to the paper. Submissions that misuse the supplementary material may be rejected without review. Supplementary material may include explanations or details of proofs or derivations that do not fit into the paper, lists of features or feature templates, sample inputs and outputs for a system, pseudo-code or source code, and data.",
        "type": "Document"
      },
      {
        "id": "9cd0820a-fcb8-4bf5-b481-c19a5a179e77",
        "metadata": {
          "vector_store_key": "1909.09270-5",
          "chunk_id": 26,
          "document_id": "1909.09270",
          "start_idx": 14042,
          "end_idx": 14768
        },
        "page_content": "These languages are: Amharic (amh: LDC2016E87), Arabic (ara: LDC2016E89), Hindi (hin: LDC2017E62), and Somali (som: LDC2016E91). These come from a variety of sources including discussion forums, newswire, and social media. The labelset is Person, Organization, Location, Geo-political entity. We define train/development/test splits, taking care to keep a similar distribution of genres in each split. Data statistics for all languages are shown in Table TABREF25. We create partial annotations by perturbing gold annotated data in two ways: lowering recall (to simulate missing entities), and lowering precision (to simulate noisy annotations). To lower recall, we replace gold named entity tags with $O$ tags (for non-name).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Do the images have multilingual annotations or monolingual ones?",
    "llm_answer": "The context does not state whether the images have multilingual or monolingual annotations.\n",
    "context": [
      {
        "id": "9cd0820a-fcb8-4bf5-b481-c19a5a179e77",
        "metadata": {
          "vector_store_key": "1909.09270-5",
          "chunk_id": 26,
          "document_id": "1909.09270",
          "start_idx": 14042,
          "end_idx": 14768
        },
        "page_content": "These languages are: Amharic (amh: LDC2016E87), Arabic (ara: LDC2016E89), Hindi (hin: LDC2017E62), and Somali (som: LDC2016E91). These come from a variety of sources including discussion forums, newswire, and social media. The labelset is Person, Organization, Location, Geo-political entity. We define train/development/test splits, taking care to keep a similar distribution of genres in each split. Data statistics for all languages are shown in Table TABREF25. We create partial annotations by perturbing gold annotated data in two ways: lowering recall (to simulate missing entities), and lowering precision (to simulate noisy annotations). To lower recall, we replace gold named entity tags with $O$ tags (for non-name).",
        "type": "Document"
      },
      {
        "id": "2cf2b621-b832-4a70-b28b-1067f46e9c60",
        "metadata": {
          "vector_store_key": "1905.12260-1",
          "chunk_id": 15,
          "document_id": "1905.12260",
          "start_idx": 8911,
          "end_idx": 9542
        },
        "page_content": "Each token in each query is given a language tag based on the user-set home language of the user making the search on Google Images. For example, if the query \u201cback pain\u201d is made by a user with English as her home language, then the query is stored as \u201cen:back en:pain\u201d. The dataset includes queries in about 130 languages. Though the specific dataset we use is proprietary, BIBREF26 have obtained a similar dataset, using the Google Images search interface, that comprises queries in 100 languages. We present a series of experiments to investigate the usefulness of multimodal image-text data in learning multilingual embeddings.",
        "type": "Document"
      },
      {
        "id": "7708185a-c7dc-4b87-a0dc-57247f7a09f8",
        "metadata": {
          "vector_store_key": "1907.02636-1",
          "chunk_id": 41,
          "document_id": "1907.02636",
          "start_idx": 22423,
          "end_idx": 23209
        },
        "page_content": "Therefore, using multilingual corpora could be a solution for addressing the lack of annotated data, and the performance of the proposed model is expected to be improved by extending the training set. To examine the hypothesis, we ran a number of additional experiments using both the English dataset and Chinese dataset, both of which are described in Section SECREF21 and are not parallel data or comparable data. As pre-trained word embeddings for the bilingual training dataset, we applied a cross-lingual word embedding obtained by the work of Duong el al BIBREF19 , where the English-Chinese cross-lingual dictionary is obtained by simply translating all the English words from English dataset to Chinese and Chinese words from Chinese dataset to English using Google translation.",
        "type": "Document"
      },
      {
        "id": "63da7553-7507-40cd-96a9-bdf053212d00",
        "metadata": {
          "vector_store_key": "1909.07734-3",
          "chunk_id": 9,
          "document_id": "1909.07734",
          "start_idx": 5225,
          "end_idx": 5944
        },
        "page_content": "Sample excerpts from the two datasets, with their annotations and labels, are given in Table . NLP tasks require plenty of data. Due to the relatively small number of samples in our datasets, we added more labeled data using a technique developed in BIBREF7 that was used by the winning team in Kaggle's Toxic Comment Classification Challenge BIBREF8. The augmented datasets are similar to the original data files, but include additional machine-computed utterances for each original utterance. We created the additional utterances using the Google Translate API. Each original utterance was first translated from English into three target languages (German, French, and Italian), and then translated back into English.",
        "type": "Document"
      },
      {
        "id": "30d6bce9-1a15-4911-83a5-fa47b532f278",
        "metadata": {
          "vector_store_key": "1905.12260-1",
          "chunk_id": 31,
          "document_id": "1905.12260",
          "start_idx": 17667,
          "end_idx": 18450
        },
        "page_content": "Each surface form in each query has a distinct embedding, and words with shared surface forms across languages (e.g., English \u201cpain\u201d and French \u201cpain\u201d) have a shared embedding. In this sense, shared surface forms are used as a bridge between languages. This is illustrated in Figure 3 . This may be helpful in certain cases, as for English \u201cactor\u201d and Spanish \u201cactor\u201d. The image representations leverage generic image features, exactly as in Section \"Leveraging Image Understanding\" . In our language-unaware experiment, we use $m = 200$ and $n = 100$ and produce 100-dimensional embeddings. We evaluate our learned multilingual embeddings using six crosslingual semantic similarity tasks, two multilingual document classification tasks, and 13 monolingual semantic similarity tasks.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much important is the visual grounding in the learning of the multilingual representations?",
    "llm_answer": "The context does not explicitly state the importance of visual grounding in learning multilingual representations.  It mentions visual grounding in the context of localization, but not in the context of multilingual representation learning.\n",
    "context": [
      {
        "id": "bf16d0ac-c1a9-47bf-9650-322ba8ae4fcc",
        "metadata": {
          "vector_store_key": "1807.03367-7",
          "chunk_id": 61,
          "document_id": "1807.03367",
          "start_idx": 35498,
          "end_idx": 36239
        },
        "page_content": "Also the task of localization (without multi-agent communication) has recently been studied BIBREF18 , BIBREF48 . paragraph4 0.1ex plus0.1ex minus.1ex-1em Grounded language learning Grounded language learning is motivated by the observation that humans learn language embodied (grounded) in sensorimotor experience of the physical world BIBREF15 , BIBREF45 . On the one hand, work in multi-modal semantics has shown that grounding can lead to practical improvements on various natural language understanding tasks BIBREF14 , BIBREF31 . In robotics, researchers dissatisfied with purely symbolic accounts of meaning attempted to build robotic systems with the aim of grounding meaning in physical experience of the world BIBREF44 , BIBREF46 .",
        "type": "Document"
      },
      {
        "id": "30d6bce9-1a15-4911-83a5-fa47b532f278",
        "metadata": {
          "vector_store_key": "1905.12260-1",
          "chunk_id": 31,
          "document_id": "1905.12260",
          "start_idx": 17667,
          "end_idx": 18450
        },
        "page_content": "Each surface form in each query has a distinct embedding, and words with shared surface forms across languages (e.g., English \u201cpain\u201d and French \u201cpain\u201d) have a shared embedding. In this sense, shared surface forms are used as a bridge between languages. This is illustrated in Figure 3 . This may be helpful in certain cases, as for English \u201cactor\u201d and Spanish \u201cactor\u201d. The image representations leverage generic image features, exactly as in Section \"Leveraging Image Understanding\" . In our language-unaware experiment, we use $m = 200$ and $n = 100$ and produce 100-dimensional embeddings. We evaluate our learned multilingual embeddings using six crosslingual semantic similarity tasks, two multilingual document classification tasks, and 13 monolingual semantic similarity tasks.",
        "type": "Document"
      },
      {
        "id": "fcba1408-84ec-4ade-9c6a-33e208eb7fbe",
        "metadata": {
          "vector_store_key": "1807.03367-5",
          "chunk_id": 23,
          "document_id": "1807.03367",
          "start_idx": 12664,
          "end_idx": 13357
        },
        "page_content": "This finding implies that localization models from image input are limited by their ability to recognize landmarks, and, as a result, would not generalize to unseen environments. To ensure that perception is not the limiting factor when investigating the landmark-grounding and action-grounding capabilities of localization models, we assume \u201cperfect perception\u201d: in lieu of the 360 image view, the tourist is given the landmarks at its current location. More formally, each state observation INLINEFORM0 now equals the set of landmarks at the INLINEFORM1 -location, i.e. INLINEFORM2 . If the INLINEFORM3 -location does not have any visible landmarks, we return a single \u201cempty corner\u201d symbol.",
        "type": "Document"
      },
      {
        "id": "c28ea6cd-5357-45e6-bdad-b41d9eb5c185",
        "metadata": {
          "vector_store_key": "1809.08510-1",
          "chunk_id": 22,
          "document_id": "1809.08510",
          "start_idx": 12888,
          "end_idx": 13595
        },
        "page_content": "We also compare against encodings learned as a by-product of multi-encoder and decoder neural machine translation as a baseline BIBREF28 . We see that UG representations are useful in situations when there is a lack of data in an specific language. The language agnostics properties of UG embeddings allows us to do successful zero-shot learning without needing any parallel corpus, furthermore the ability to generalize from language modeling to sentiment attests for the universal properties of these representations. Although we aren't able to improve over the state of the art in a single language we are able to learn a model that does surprisingly well on a set of languages without multilingual data.",
        "type": "Document"
      },
      {
        "id": "c4d056ee-28bf-4bef-9918-bace1aeac08e",
        "metadata": {
          "vector_store_key": "1809.04686-3",
          "chunk_id": 3,
          "document_id": "1809.04686",
          "start_idx": 1743,
          "end_idx": 2504
        },
        "page_content": "Cross-lingual or multilingual NLP, the task of transferring knowledge from one language to another, serves as a good test bed for evaluating various transfer learning approaches. For cross-lingual NLP, the most widely studied approach is to use multilingual embeddings as features in neural network models. However, research has shown that representations learned in context are more effective BIBREF5 , BIBREF6 ; therefore, we aim at doing better than just using multilingual embeddings in the cross-lingual tasks. Recent progress in multilingual NMT provides a compelling opportunity for obtaining contextualized multilingual representations, as multilingual NMT systems are capable of generalizing to an unseen language direction, i.e. zero-shot translation.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the generative model evaluated?",
    "llm_answer": "The generative model is evaluated by training a classifier on the generated dataset and comparing the accuracy of that classifier to the accuracy of a classifier trained on the original dataset.  A discriminative error rate is also calculated, comparing the generated hypotheses to the original hypotheses.\n",
    "context": [
      {
        "id": "d69c261a-457d-48c6-9280-d01442f445f9",
        "metadata": {
          "vector_store_key": "1607.06025-2",
          "chunk_id": 7,
          "document_id": "1607.06025",
          "start_idx": 4257,
          "end_idx": 4858
        },
        "page_content": "The generated hypothesis is combined with the premise and the label to form a new unseen example. This is done for every example in the original dataset to construct a new dataset. Next, a classifier is trained on the new dataset. Finally, the classifier is evaluated on the original test set. The accuracy of the classifier is the proposed quality metric for the generative model. It can be compared to the accuracy of the classifier trained on the original training set and tested on the original test set. The generative models learn solely from the original training set to regenerate the dataset.",
        "type": "Document"
      },
      {
        "id": "463c0be2-116e-49b9-bfcc-509e70e316c8",
        "metadata": {
          "vector_store_key": "1607.06025-2",
          "chunk_id": 8,
          "document_id": "1607.06025",
          "start_idx": 4669,
          "end_idx": 5395
        },
        "page_content": "The generative models learn solely from the original training set to regenerate the dataset. Thus, the model learns the distribution of the original dataset. Furthermore, the generated dataset is just a random sample from the estimated distribution. To determine how well did the generative model learn the distribution, we observe how close does the accuracy of the classifier trained on the generated dataset approach the accuracy of classifier trained on the original dataset. Our flagship generative network EmbedDecoder works in a similar fashion as the encoder-decoder networks, where the encoder is used to transform the input into a low-dimensional latent representation, from which the decoder reconstructs the input.",
        "type": "Document"
      },
      {
        "id": "277b6532-3b00-493f-81d4-bf4bd8453d7b",
        "metadata": {
          "vector_store_key": "1607.06025-2",
          "chunk_id": 39,
          "document_id": "1607.06025",
          "start_idx": 22867,
          "end_idx": 23549
        },
        "page_content": "The premises and labels from the examples of the original dataset are taken as an input for the generative model. The new hypotheses replace the training hypotheses in the new dataset. Next, the classifier, presented in Section SECREF6 , is trained on the generated dataset. The accuracy of the new classifier is the main metric for evaluating the quality of the generated dataset. All the experiments are performed on the SNLI dataset. There are 549,367 examples in the dataset, divided into training, development and test set. Both the development and test set contain around 10.000 examples. Some examples are labeled with '-', which means there was not enough consensus on them.",
        "type": "Document"
      },
      {
        "id": "6e126e9a-d2e1-472b-8141-6bd877a992b9",
        "metadata": {
          "vector_store_key": "1607.06025-2",
          "chunk_id": 6,
          "document_id": "1607.06025",
          "start_idx": 3531,
          "end_idx": 4257
        },
        "page_content": "Non-trivial examples are essential in a dataset for training a capable machine learning model. Furthermore, we make the following hypothesis. A good dataset for training a NLI classifier consists of a variety of accurate, non-trivial and comprehensible examples. Based on this hypothesis, we propose the following approach for evaluation of generative models, which is also presented in Figure FIGREF2 . First, the generative model is trained on the original training dataset. Then, the premise and label from an example in the original dataset are taken as the input to the generative model to generate a new random hypothesis. The generated hypothesis is combined with the premise and the label to form a new unseen example.",
        "type": "Document"
      },
      {
        "id": "3a90c720-18de-4269-9c7a-dfc7eb51d6b7",
        "metadata": {
          "vector_store_key": "1607.06025-0",
          "chunk_id": 51,
          "document_id": "1607.06025",
          "start_idx": 29848,
          "end_idx": 30583
        },
        "page_content": "The last metric \u2013 discriminative error rate \u2013 is calculated with the discriminative model. The model is trained on the hypotheses from the unfiltered generated dataset on one side and the original hypotheses on the other side. Error rate is calculated on the (generated and original) development sets. Higher error rate indicates that it is more difficult for discriminative model to distinguish between the generated and the original hypotheses, which suggests that the original generating distribution and the distribution of the generative model are more similar. The discriminative model detects that low dimensional generative models generate more trivial examples as also indicated by the distance between premise and hypotheses.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is an example of a health-related tweet?",
    "llm_answer": "The provided context lists examples of health-related topics discussed on Twitter (flu, mental health, etc.), but does not give an example of an *actual* tweet.  The answer is not found in the context.\n",
    "context": [
      {
        "id": "02af4b8c-52ae-42f3-9382-a1980571f7d9",
        "metadata": {
          "vector_store_key": "1709.07916-1",
          "chunk_id": 5,
          "document_id": "1709.07916",
          "start_idx": 3279,
          "end_idx": 4076
        },
        "page_content": "Some examples of Twitter data analysis for health-related topics include: flu BIBREF25 , BIBREF26 , BIBREF27 , BIBREF28 , BIBREF29 , BIBREF30 , mental health BIBREF31 , Ebola BIBREF32 , BIBREF33 , Zika BIBREF34 , medication use BIBREF35 , BIBREF36 , BIBREF37 , diabetes BIBREF38 , and weight loss and obesity BIBREF39 , BIBREF40 , BIBREF41 , BIBREF42 , BIBREF21 . The previous Twitter studies have dealt with extracting common topics of one health issue discussed by the users to better understand common themes; however, this study utilizes an innovative approach to computationally analyze unstructured health related text data exchanged via Twitter to characterize health opinions regarding four common health issues, including diabetes, diet, exercise and obesity (DDEO) on a population level.",
        "type": "Document"
      },
      {
        "id": "310940fb-d773-452b-80c8-6dcbdca418b7",
        "metadata": {
          "vector_store_key": "1709.07916-4",
          "chunk_id": 4,
          "document_id": "1709.07916",
          "start_idx": 2268,
          "end_idx": 3279
        },
        "page_content": "Furthermore, analyzing Twitter data can help health organizations such as state health departments and large healthcare systems to provide health advice and track health opinions of their populations and provide effective health advice when needed BIBREF13 . Among computational methods to analyze tweets, computational linguistics is a well-known developed approach to gain insight into a population, track health issues, and discover new knowledge BIBREF19 , BIBREF20 , BIBREF21 , BIBREF22 . Twitter data has been used for a wide range of health and non-health related applications, such as stock market BIBREF23 and election analysis BIBREF24 . Some examples of Twitter data analysis for health-related topics include: flu BIBREF25 , BIBREF26 , BIBREF27 , BIBREF28 , BIBREF29 , BIBREF30 , mental health BIBREF31 , Ebola BIBREF32 , BIBREF33 , Zika BIBREF34 , medication use BIBREF35 , BIBREF36 , BIBREF37 , diabetes BIBREF38 , and weight loss and obesity BIBREF39 , BIBREF40 , BIBREF41 , BIBREF42 , BIBREF21 .",
        "type": "Document"
      },
      {
        "id": "6f2189ec-d927-474c-b106-04f6f8d5ad3e",
        "metadata": {
          "vector_store_key": "1805.09959-6",
          "chunk_id": 38,
          "document_id": "1805.09959",
          "start_idx": 22824,
          "end_idx": 23534
        },
        "page_content": "We have demonstrated the potential of using context classifiers for identifying diagnostic tweets related to the experience of breast cancer patients. Our framework provides a proof of concept for integrating machine learning with natural language processing as a tool to help connect healthcare providers with patient experiences. These methods can inform the medical community to provide more personalized treatment regimens by evaluating patient satisfaction using social listening. Twitter has also been shown as a useful medium for political support of healthcare policies as well as spreading awareness. Applying these analyses across other social media platforms could provide comparably rich data-sets.",
        "type": "Document"
      },
      {
        "id": "15172f86-a304-48fd-b8fd-35943c589156",
        "metadata": {
          "vector_store_key": "1901.00439-8",
          "chunk_id": 16,
          "document_id": "1901.00439",
          "start_idx": 9420,
          "end_idx": 10018
        },
        "page_content": "List of health news channels and the number of tweets in the dataset from each channel can be examined from Table 1 . The outlook of a typical tweet from the dataset can be examined from Figure 1 . For every tweet, the raw data consists of the tweet text and in most cases followed by a url to the original news article of the particular news source. This url string, if available, is removed from each tweet as it does not possess any natural language information. As Twitter allows several ways for users to interact such as retweeting or mentioning, these actions appear in the raw text as well.",
        "type": "Document"
      },
      {
        "id": "e00de4a6-6051-4966-9658-3de2dfe832c8",
        "metadata": {
          "vector_store_key": "1805.09959-8",
          "chunk_id": 32,
          "document_id": "1805.09959",
          "start_idx": 18962,
          "end_idx": 19705
        },
        "page_content": "Twitter can further be used to remind, motivate and change individual and population health behavior using messages of encouragement (translated to happiness) or dissatisfaction (translated to diminished happiness), for example, with memes that can have knock on social consequences when they are re-tweeted. Furthermore, Twitter may someday be used to benchmark treatment decisions to align with expressed patient sentiments, and to make or change clinical recommendations based upon the trend histories that evolve with identifiable sources but are entirely in the public domain. Analyzing the fluctuation in average word happiness as well as bursts in the frequency distributions can help identify relevant events for further investigation.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the challenge for other language except English",
    "llm_answer": "Morphological richness and differing prefixing/suffixing structures.\n",
    "context": [
      {
        "id": "50580b33-3f37-4708-bb48-721919a2f597",
        "metadata": {
          "vector_store_key": "1910.05456-7",
          "chunk_id": 18,
          "document_id": "1910.05456",
          "start_idx": 9647,
          "end_idx": 10340
        },
        "page_content": "Spanish (SPA), in contrast, is morphologically rich, and disposes of much larger verbal paradigms than English. Like English, it is a suffixing language, and it additionally makes use of internal stem changes (e.g., o $\\rightarrow $ ue). Since English and Spanish are both Indo-European languages, and, thus, relatively similar, we further add a third, unrelated target language. We choose Zulu (ZUL), a Bantoid language. In contrast to the first two, it is strongly prefixing. For pretraining, we choose languages with different degrees of relatedness and varying morphological similarity to English, Spanish, and Zulu. We limit our experiments to languages which are written in Latin script.",
        "type": "Document"
      },
      {
        "id": "59f461e2-3e5e-4fa2-b4bc-375869e36b10",
        "metadata": {
          "vector_store_key": "1804.08186-3",
          "chunk_id": 3,
          "document_id": "1804.08186",
          "start_idx": 1910,
          "end_idx": 2575
        },
        "page_content": "In monolingual , the task is to assign each document a unique language label. Some work has reported near-perfect accuracy for of large documents in a small number of languages, prompting some researchers to label it a \u201csolved task\u201d BIBREF2 . However, in order to attain such accuracy, simplifying assumptions have to be made, such as the aforementioned monolinguality of each document, as well as assumptions about the type and quantity of data, and the number of languages considered. The ability to accurately detect the language that a document is written in is an enabling technology that increases accessibility of data and has a wide variety of applications.",
        "type": "Document"
      },
      {
        "id": "c28fe057-2cfc-4978-9c0a-84fd286d4b98",
        "metadata": {
          "vector_store_key": "1612.06897-3",
          "chunk_id": 39,
          "document_id": "1612.06897",
          "start_idx": 20315,
          "end_idx": 20488
        },
        "page_content": "We presented results on two diverse language pairs German $\\rightarrow $ English and Chinese $\\rightarrow $ English (usually very challenging pairs for machine translation).",
        "type": "Document"
      },
      {
        "id": "22504978-69cc-4504-9abc-41d5651e4391",
        "metadata": {
          "vector_store_key": "1804.08186-3",
          "chunk_id": 248,
          "document_id": "1804.08186",
          "start_idx": 140712,
          "end_idx": 141342
        },
        "page_content": "An interesting area to explore is the trade-off between the number of languages supported and the accuracy per-language. From existing results it is not clear if it is possible to continue increasing the number of languages supported without adversely affecting the average accuracy, but it would be useful to quantify if this is actually the case across a broad range of text sources. mostlanguages lists the articles where the with more than 30 languages has been investigated. \u201cUnseen\u201d languages are languages that we do not have training data for but may nonetheless be encountered by a system when applied to real-world data.",
        "type": "Document"
      },
      {
        "id": "9ece3dbc-6d9e-49d3-838c-c59b224fb280",
        "metadata": {
          "vector_store_key": "2002.02427-3",
          "chunk_id": 26,
          "document_id": "2002.02427",
          "start_idx": 14704,
          "end_idx": 14917
        },
        "page_content": "Our next step is to experiment with other languages such as Hindi and Italian. The work of Paolo Rosso was partially funded by the Spanish MICINN under the research project MISMIS-FAKEnHATE (PGC2018-096212-B-C31).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many categories of offensive language were there?",
    "llm_answer": "Three\n",
    "context": [
      {
        "id": "97ca89e6-f02a-49b1-aff0-47b324e20a77",
        "metadata": {
          "vector_store_key": "1908.04531-9",
          "chunk_id": 14,
          "document_id": "1908.04531",
          "start_idx": 7767,
          "end_idx": 8445
        },
        "page_content": "In this section we give a comprehensive overview of the structure of the task and describe the dataset provided in BIBREF0 . Our work adopts this framing of the offensive language phenomenon. Offensive content is broken into three sub-tasks to be able to effectively identify both the type and the target of the offensive posts. These three sub-tasks are chosen with the objective of being able to capture different types of offensive language, such as hate speech and cyberbullying (section \"Background\" ). In sub-task A the goal is to classify posts as either offensive or not. Offensive posts include insults and threats as well as any form of untargeted profanity BIBREF17 .",
        "type": "Document"
      },
      {
        "id": "ed93cc77-2e05-4e0b-82ba-78888f513024",
        "metadata": {
          "vector_store_key": "1909.12642-0",
          "chunk_id": 0,
          "document_id": "1909.12642",
          "start_idx": 0,
          "end_idx": 743
        },
        "page_content": "In social media, abusive language denotes a text which contains any form of unacceptable language in a post or a comment. Abusive language can be divided into hate speech, offensive language and profanity. Hate speech is a derogatory comment that hurts an entire group in terms of ethnicity, race or gender. Offensive language is similar to derogatory comment, but it is targeted towards an individual. Profanity refers to any use of unacceptable language without a specific target. While profanity is the least threatening, hate speech has the most detrimental effect on the society. Social media moderators are having a hard time in combating the rampant spread of hate speech as it is closely related to the other forms of abusive language.",
        "type": "Document"
      },
      {
        "id": "f1ae64e2-e8d0-4ff0-a09a-12df4c439b55",
        "metadata": {
          "vector_store_key": "1908.04531-9",
          "chunk_id": 16,
          "document_id": "1908.04531",
          "start_idx": 8807,
          "end_idx": 9426
        },
        "page_content": "In Danish this could be a post such as Kalle er faggot... In sub-task B the goal is to classify the type of offensive language by determining if the offensive language is targeted or not. Targeted offensive language contains insults and threats to an individual, group, or others BIBREF17 . Untargeted posts contain general profanity while not clearly targeting anyone BIBREF17 . Only posts labeled as offensive (OFF) in sub-task A are considered in this task. Each sample is annotated with one of the following labels: Targeted Insult (TIN). In English this could be a post such as @USER Please ban this cheating scum.",
        "type": "Document"
      },
      {
        "id": "78d1707e-84ed-4574-8f1c-e02cb0c2a1d3",
        "metadata": {
          "vector_store_key": "1908.04531-6",
          "chunk_id": 1,
          "document_id": "1908.04531",
          "start_idx": 813,
          "end_idx": 1603
        },
        "page_content": "Given the fact that the research on offensive language detection has to a large extent been focused on the English language, we set out to explore the design of models that can successfully be used for both English and Danish. To accomplish this, an appropriate dataset must be constructed, annotated with the guidelines described in BIBREF0 . We, furthermore, set out to analyze the linguistic features that prove hard to detect by analyzing the patterns that prove hard to detect. Offensive language varies greatly, ranging from simple profanity to much more severe types of language. One of the more troublesome types of language is hate speech and the presence of hate speech on social media platforms has been shown to be in correlation with hate crimes in real life settings BIBREF1 .",
        "type": "Document"
      },
      {
        "id": "02cb710a-67cb-45b5-a816-c507fc6fe58c",
        "metadata": {
          "vector_store_key": "1908.04531-9",
          "chunk_id": 15,
          "document_id": "1908.04531",
          "start_idx": 8223,
          "end_idx": 8807
        },
        "page_content": "Offensive posts include insults and threats as well as any form of untargeted profanity BIBREF17 . Each sample is annotated with one of the following labels: In English this could be a post such as #TheNunMovie was just as scary as I thought it would be. Clearly the critics don't think she is terrifyingly creepy. I like how it ties in with #TheConjuring series. In Danish this could be a post such as Kim Larsen var god, men hans d\u00f8d blev alt for hyped. . In English this could be a post such as USER is a #pervert himself!. In Danish this could be a post such as Kalle er faggot...",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which matching features do they employ?",
    "llm_answer": "diff-feature, sim-feature, joint-feature\n",
    "context": [
      {
        "id": "e05029c5-d574-4751-9c5b-6eb8dd9c54f4",
        "metadata": {
          "vector_store_key": "1901.02222-4",
          "chunk_id": 18,
          "document_id": "1901.02222",
          "start_idx": 10420,
          "end_idx": 11092
        },
        "page_content": "And the INLINEFORM6 can be regarded as a sim-feature of INLINEFORM7 and INLINEFORM8 , which emphasizes on the similar parts and neglects the different parts between INLINEFORM9 and INLINEFORM10 . Each feature helps us focus on particular parts between the context vectors and the aligned vectors. These matching features are vector representations with low dimension, but containing high-order semantic information. To make further use of these matching features, we collect them to generate a matching sequence INLINEFORM11 . DISPLAYFORM0   where INLINEFORM0 . The output of this layer is the matching sequence INLINEFORM0 , which stores three kinds of matching features.",
        "type": "Document"
      },
      {
        "id": "36e2206d-8e1c-4cee-8b07-f06a8c2fd439",
        "metadata": {
          "vector_store_key": "1901.02222-4",
          "chunk_id": 17,
          "document_id": "1901.02222",
          "start_idx": 9641,
          "end_idx": 10420
        },
        "page_content": "After matching the context vectors INLINEFORM0 and the aligned vectors INLINEFORM1 by INLINEFORM2 , INLINEFORM3 and INLINEFORM4 , we can get three matching features INLINEFORM5 , INLINEFORM6 and INLINEFORM7 . where INLINEFORM8 , INLINEFORM9 , and INLINEFORM10 . The INLINEFORM0 can be considered as a joint-feature of combing the context vectors INLINEFORM1 with aligned vectors INLINEFORM2 , which preserves all the information. And the INLINEFORM3 can be seen as a diff-feature of the INLINEFORM4 and INLINEFORM5 , which preserves the different parts and removes the similar parts. And the INLINEFORM6 can be regarded as a sim-feature of INLINEFORM7 and INLINEFORM8 , which emphasizes on the similar parts and neglects the different parts between INLINEFORM9 and INLINEFORM10 .",
        "type": "Document"
      },
      {
        "id": "bdd3efe8-8f40-43ef-ad70-373ff0e46ef9",
        "metadata": {
          "vector_store_key": "1902.09087-1",
          "chunk_id": 32,
          "document_id": "1902.09087",
          "start_idx": 17423,
          "end_idx": 18209
        },
        "page_content": "We also implement several state-of-the-art matching models using the open-source project MatchZoo BIBREF19 , where we tune hyper-parameters using grid search, e.g., whether using word or character inputs. Arc1, Arc2, CDSSM are traditional CNNs based matching models proposed by BIBREF20 , BIBREF21 . Arc1 and CDSSM compute the similarity via sentence representations and Arc2 uses the word pair similarities. MV-LSTM BIBREF22 computes the matching score by examining the interaction between the representations from two sentences obtained by a shared BiLSTM encoder. MatchPyramid(MP) BIBREF23 utilizes 2D convolutions and pooling strategies over word pair similarity matrices to compute the matching scores. We also compare with the state-of-the-art models in DBQA BIBREF15 , BIBREF16 .",
        "type": "Document"
      },
      {
        "id": "a1805cf6-2af6-49c8-a9de-d736105854c3",
        "metadata": {
          "vector_store_key": "1901.02222-0",
          "chunk_id": 2,
          "document_id": "1901.02222",
          "start_idx": 1287,
          "end_idx": 1980
        },
        "page_content": "Matching is implemented by several functions based on element-wise operations BIBREF7 , BIBREF8 . Studies on SNLI show that the second one performs better. Though the second framework has made considerable success on the NLI task, there are still some limitations. First, the inference on the mixed matching feature only adopts one-pass process, which means some detailed information would not be retrieved once missing. While the multi-turn inference can overcome this deficiency and make better use of these matching features. Second, the mixed matching feature only concatenates different matching features as the input for aggregation. It lacks interaction among various matching features.",
        "type": "Document"
      },
      {
        "id": "b8f5b376-8332-482d-8df5-d3910d294453",
        "metadata": {
          "vector_store_key": "1901.02222-4",
          "chunk_id": 14,
          "document_id": "1901.02222",
          "start_idx": 8163,
          "end_idx": 8921
        },
        "page_content": "The goal of the matching layer is to match the context vectors INLINEFORM0 and INLINEFORM1 with the corresponding aligned vectors INLINEFORM2 and INLINEFORM3 from multi-perspective to generate a matching sequence. In this layer, we match each context vector INLINEFORM0 against each aligned vector INLINEFORM1 to capture richer semantic information. We design three effective matching functions: INLINEFORM2 , INLINEFORM3 and INLINEFORM4 to match two vectors BIBREF32 , BIBREF5 , BIBREF9 . Each matching function takes the context vector INLINEFORM5 ( INLINEFORM6 ) and the aligned vector INLINEFORM7 ( INLINEFORM8 ) as inputs, then matches the inputs by an feed-forward network based on a particular matching operation and finally outputs a matching vector.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does their method outperform the multi-head attention model?",
    "llm_answer": "Their method (SAW Reader (mul)) outperforms the Attention Sum Reader (AS Reader) baseline by 7.57%.\n",
    "context": [
      {
        "id": "fc053402-c3d1-442e-9ce0-609348300c32",
        "metadata": {
          "vector_store_key": "1806.09103-3",
          "chunk_id": 25,
          "document_id": "1806.09103",
          "start_idx": 14455,
          "end_idx": 15246
        },
        "page_content": "We use three attention layers for all experiments. The GRU hidden units for both the word and subword representation were 128. The default frequency filter proportion was 0.9 and the default merging times of BPE was 1,000. We also apply dropout between layers with a dropout rate of 0.5 . [7]http://www.hfl-tek.com/cmrc2017/leaderboard.html Table TABREF17 shows our results on CMRC-2017 dataset, which shows that our SAW Reader (mul) outperforms all other single models on the test set, with 7.57% improvements compared with Attention Sum Reader (AS Reader) baseline. Although WHU's model achieves the best besides our model on the valid set with only 0.75% below ours, their result on the test set is lower than ours by 2.27%, indicating our model has a satisfactory generalization ability.",
        "type": "Document"
      },
      {
        "id": "1172153e-0a20-4dfe-b748-92bf2c137055",
        "metadata": {
          "vector_store_key": "1910.09295-3",
          "chunk_id": 34,
          "document_id": "1910.09295",
          "start_idx": 18521,
          "end_idx": 19204
        },
        "page_content": "Using only one attention head, thereby attending to only one context position at once, degrades the performance to less than the performance of 10 heads using the standard finetuning scheme. This shows that more attention heads, thereby attending to multiple different contexts at once, is important to boosting performance to state-of-the-art results. While increasing the number of attention heads improves performance, keeping on adding extra heads will not result to an equivalent boost as the performance plateaus after a number of heads. As shown in Figure FIGREF35, the performance boost of the model plateaus after 10 attention heads, which was the default used in the study.",
        "type": "Document"
      },
      {
        "id": "d100c0fd-db45-40e5-bead-e64f6f5e189e",
        "metadata": {
          "vector_store_key": "1908.06606-3",
          "chunk_id": 34,
          "document_id": "1908.06606",
          "start_idx": 20053,
          "end_idx": 20795
        },
        "page_content": "Unfortunately, applying multi-head attention on both period one and period two can not reach convergence in our experiments. This probably because it makes the model too complex to train. The difference on other two methods are the order of concatenation and multi-head attention. Applying multi-head attention on two named entity information $I_{nt}$ and $I_{nq}$ first achieved a better performance with 89.87% in EM-score and 92.88% in F$_1$-score. Applying Concatenation first can only achieve 80.74% in EM-score and 84.42% in F$_1$-score. This is probably due to the processing depth of hidden vectors and dataset size. BERT's output has been modified after many layers but named entity information representation is very close to input.",
        "type": "Document"
      },
      {
        "id": "848d6ac6-7307-4ef4-81fc-ed007db0fe33",
        "metadata": {
          "vector_store_key": "1804.08050-2",
          "chunk_id": 5,
          "document_id": "1804.08050",
          "start_idx": 3027,
          "end_idx": 3768
        },
        "page_content": "Another study BIBREF11 uses a multi-head attention (MHA) to get more suitable alignments. In MHA, multiple attentions are calculated, and then, they are integrated into a single attention. Using MHA enables the model to jointly focus on information from different representation subspaces at different positions BIBREF12 , leading to the improvement of the recognition performance. Inspired by the idea of MHA, in this study we present a new network architecture called multi-head decoder for end-to-end speech recognition as an extension of a multi-head attention model. Instead of the integration in the attention level, our proposed method uses multiple decoders for each attention and integrates their outputs to generate a final output.",
        "type": "Document"
      },
      {
        "id": "cba4f89b-8b30-42ed-95f1-cd92271faf23",
        "metadata": {
          "vector_store_key": "1909.00361-4",
          "chunk_id": 20,
          "document_id": "1909.00361",
          "start_idx": 12332,
          "end_idx": 13142
        },
        "page_content": "To combine the benefit of both inter-attention and self-attention, instead of using Equation 1, we propose a simple modification on multi-head attention mechanism, which is called Self-Adaptive Attention (SAA). First, we calculate self-attention of $B_T$ and $B_S$ and apply the softmax function, as shown in Equation 2 and 3. This is designed to use self-attention to filter the irrelevant part within each representation firstly, and inform the raw dot attention on paying more attention to the self-attended part, making the attention more precise and accurate. Then we use self-attention $A_T$ and $A_S$, inter-attention $A_{TS}$ to get self-attentive attention $\\tilde{A}_{TS}$. We calculate dot product between ${A}_{ST}$ and $B_S$ to obtain attended representation $R^{\\prime } \\in \\mathbb {R}^{L_T*h}$.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How large is the corpus they use?",
    "llm_answer": "9.7 million words\n",
    "context": [
      {
        "id": "248d0947-e589-4dce-a602-15e3c4af536e",
        "metadata": {
          "vector_store_key": "1710.06700-4",
          "chunk_id": 12,
          "document_id": "1710.06700",
          "start_idx": 6389,
          "end_idx": 7092
        },
        "page_content": "This diacritized corpus was created by a commercial vendor and contains 9.7 million words with almost 200K unique surface words. About 73% of the corpus is in MSA and covers variety of genres like politics, economy, sports, society, etc. and the remaining part is mostly religious texts written in classical Arabic (CA). The effectiveness of using this corpus in building state-of-the-art diacritizer was proven in BIBREF10 . For example, the word \u00d9\u0088\u00d8\u00a8\u00d9\u0086\u00d9\u0088\u00d8\u00af> (wbnwd) \u201cand items\u201d is found 4 times in this corpus with two full diacritization forms \u00d9\u0088\u00d9\u008e\u00d8\u00a8\u00d9\u008f\u00d9\u0086\u00d9\u008f\u00d9\u0088\u00d8\u00af\u00d9\u0090\u00d8\u008c \u00d9\u0088\u00d9\u008e\u00d8\u00a8\u00d9\u008f\u00d9\u0086\u00d9\u008f\u00d9\u0088\u00d8\u00af\u00d9\u008d> (wabunudi, wabunudK) \u201citems, with different grammatical case endings\u201d which appeared 3 times and once respectively.",
        "type": "Document"
      },
      {
        "id": "1c11a194-58b2-4cda-851d-6cf17f4aad89",
        "metadata": {
          "vector_store_key": "1909.01958-0",
          "chunk_id": 13,
          "document_id": "1909.01958",
          "start_idx": 7877,
          "end_idx": 8634
        },
        "page_content": "To study particular phenomena and develop solvers, the project has created larger datasets to amplify and study different problems, resulting in 10 new datasets and 5 large knowledge resources for the community. The solvers can be loosely grouped into: Statistical and information retrieval methods Reasoning methods Large-scale language model methods Over the life of the project, the relative importance of the methods has shifted towards large-scale language methods. Several methods make use of the Aristo Corpus, comprising a large Web-crawled corpus ($5 \\times 10^{10}$ tokens (280GB)) originally from the University of Waterloo, combined with targeted science content from Wikipedia, SimpleWikipedia, and several smaller online science texts (BID25).",
        "type": "Document"
      },
      {
        "id": "3f796a82-78d4-4d2f-8165-b133aeaab300",
        "metadata": {
          "vector_store_key": "1911.12579-0",
          "chunk_id": 22,
          "document_id": "1911.12579",
          "start_idx": 12899,
          "end_idx": 13803
        },
        "page_content": "The corpus is a collection of human language text BIBREF31 built with a specific purpose. However, the statistical analysis of the corpus provides quantitative, reusable data, and an opportunity to examine intuitions and ideas about language. Therefore, the corpus has great importance for the study of written language to examine the text. In fact, realizing the necessity of large text corpus for Sindhi, we started this research by collecting raw corpus from multiple web resource using web-scrappy framwork for extraction of news columns of daily Kawish and Awami Awaz Sindhi newspapers, Wikipedia dumps, short stories and sports news from Wichaar social blog, news from Focus Word press blog, historical writings, novels, stories, books from Sindh Salamat literary websites, novels, history and religious books from Sindhi Adabi Board and tweets regarding news and sports are collected from twitter.",
        "type": "Document"
      },
      {
        "id": "e9a9a15f-d11c-469d-b682-6517dc1a88ce",
        "metadata": {
          "vector_store_key": "1809.08731-3",
          "chunk_id": 15,
          "document_id": "1809.08731",
          "start_idx": 8566,
          "end_idx": 9272
        },
        "page_content": "It contains single sentences and two-sentence paragraphs from the Open American National Corpus (OANC), which belong to 4 genres: newswire, letters, journal, and non-fiction. Gold references are manually created and the outputs of 4 compression systems (ILP (extractive), NAMAS (abstractive), SEQ2SEQ (extractive), and T3 (abstractive); cf. toutanova2016dataset for details) for the test data are provided. Each example has 3 to 5 independent human ratings for content and fluency. We are interested in the latter, which is rated on an ordinal scale from 1 (disfluent) through 3 (fluent). We experiment on the 2955 system outputs for the test split. Average fluency scores per system are shown in Table 2 .",
        "type": "Document"
      },
      {
        "id": "192ef8cf-b156-4ba5-a170-1e436e925cf4",
        "metadata": {
          "vector_store_key": "1905.11037-4",
          "chunk_id": 8,
          "document_id": "1905.11037",
          "start_idx": 4736,
          "end_idx": 5389
        },
        "page_content": "Table 2 details the statistics of the corpus (see also Appendix \"Corpus distribution\" ). Note that similar to Twitter corpora, fan fiction stories can be deleted over time by users or admins, causing losses in the dataset. We tokenized the samples with BIBREF14 and merged the occurrences of multi-word spells into a single token. This work addresses the task as a classification problem, and in particular as a sequence to label classification problem. For this reason, we rely on standard models used for this type of task: multinomial logistic regression, a multi-layered perceptron, convolutional neural networks and long short-term memory networks.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many shared layers are in the system?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "30ed24ac-df5b-4321-a866-0e6b111193bc",
        "metadata": {
          "vector_store_key": "2002.06424-7",
          "chunk_id": 47,
          "document_id": "2002.06424",
          "start_idx": 25181,
          "end_idx": 25821
        },
        "page_content": "In our work, we treated the number of shared and task-specific layers as a hyperparameter to be tuned for each dataset, but future work may explore ways to select this aspect of the model architecture in a more principled way. For example, Vandenhende et al. vandenhende2019branched propose using a measure of affinity between tasks to determine how many layers to share in MTL networks. Task affinity scores of NER and RE could be computed for different textual domains or datasets, which could then guide the decision regarding the number of shared and task-specific layers to employ for joint NER and RE models deployed on these domains.",
        "type": "Document"
      },
      {
        "id": "7bff54f2-f693-4590-86c8-e83ad136e74b",
        "metadata": {
          "vector_store_key": "1808.09920-8",
          "chunk_id": 56,
          "document_id": "1808.09920",
          "start_idx": 29949,
          "end_idx": 30632
        },
        "page_content": "Eventually, a 2-layers MLP with [256, 128] hidden units takes the concatenation between $\\lbrace \\mathbf {h}_i^{(L)}\\rbrace _{i=1}^N$ and $\\mathbf {q}$ to predict the probability that a candidate node $v_i$ may be the answer to the query $q$ (see Equation 16 ). During preliminary trials, we experimented with different numbers of R-GCN-layers (in the range 1-7). We observed that with WikiHop, for $L \\ge 3$ models reach essentially the same performance, but more layers increase the time required to train them. Besides, we observed that the gating mechanism learns to keep more and more information from the past at each layer making unnecessary to have more layers than required.",
        "type": "Document"
      },
      {
        "id": "f63753d4-115e-43a6-9aaf-2fefcedcf665",
        "metadata": {
          "vector_store_key": "1905.10044-5",
          "chunk_id": 54,
          "document_id": "1905.10044",
          "start_idx": 30658,
          "end_idx": 30731
        },
        "page_content": "We use 200 dimensional LSTMs and a 100 dimensional fully connected layer.",
        "type": "Document"
      },
      {
        "id": "266139a8-c68b-428f-9148-108176241a21",
        "metadata": {
          "vector_store_key": "1701.06538-8",
          "chunk_id": 61,
          "document_id": "1701.06538",
          "start_idx": 33014,
          "end_idx": 33624
        },
        "page_content": "We vary the number of experts between models, using an ordinary MoE layer with 32 experts and hierarchical MoE layers with 256, 1024, 4096, 16384, 65536 and 131072 experts. For the hierarchical MoE layers, the first level branching factors are 32, 32, 64, 128, 256 and 256, respectively. Models are trained on a cluster of 32 Tesla K40 GPUs, except for the last two models, which are trained on clusters of 64 and 128 GPUs so as to have enough memory for all the parameters. For all models, training batch sizes are approximately 2.5 million words. Models are trained once-through over about 100 billion words.",
        "type": "Document"
      },
      {
        "id": "dd4a4d52-271b-40bb-bafa-058c23dfa2b4",
        "metadata": {
          "vector_store_key": "1902.10525-7",
          "chunk_id": 43,
          "document_id": "1902.10525",
          "start_idx": 22850,
          "end_idx": 23488
        },
        "page_content": "(see Figure FIGREF48 , Table TABREF47 ). We first run experiments without additional feature functions (Figure FIGREF48 , solid lines), then re-compute the results with tuned weights for language models and character classes (Figure FIGREF48 , dashed lines). We observe that for both input formats, using 3 or 5 layers outperforms more shallow networks, and using more layers gives hardly any improvement. Furthermore, using 64 nodes per layer is sufficient, as wider networks give only small improvements, if at all. Finally, we show a comparison of our old and new systems with the literature on the IAM-OnDB dataset in Table TABREF49 .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many additional task-specific layers are introduced?",
    "llm_answer": "Additional NER-specific layers and RE-specific BiRNN layers are introduced, but the exact number is not specified.\n",
    "context": [
      {
        "id": "30ed24ac-df5b-4321-a866-0e6b111193bc",
        "metadata": {
          "vector_store_key": "2002.06424-7",
          "chunk_id": 47,
          "document_id": "2002.06424",
          "start_idx": 25181,
          "end_idx": 25821
        },
        "page_content": "In our work, we treated the number of shared and task-specific layers as a hyperparameter to be tuned for each dataset, but future work may explore ways to select this aspect of the model architecture in a more principled way. For example, Vandenhende et al. vandenhende2019branched propose using a measure of affinity between tasks to determine how many layers to share in MTL networks. Task affinity scores of NER and RE could be computed for different textual domains or datasets, which could then guide the decision regarding the number of shared and task-specific layers to employ for joint NER and RE models deployed on these domains.",
        "type": "Document"
      },
      {
        "id": "83fcdd67-a334-4d01-8735-0ab4b8152301",
        "metadata": {
          "vector_store_key": "2002.06424-7",
          "chunk_id": 19,
          "document_id": "2002.06424",
          "start_idx": 10277,
          "end_idx": 11080
        },
        "page_content": "Overall, previous approaches to joint NER and RE have experimented little with deep task-specificity, with the exception of those models that include additional layers for the RE task. To our knowledge, no work has considered including additional NER-specific layers beyond scoring and/or output layers. This may reflect a residual influence of the pipeline approach in which the NER task must be solved first before additional layers are used to solve the RE task. However, there is no a priori reason to think that the RE task would benefit more from additional task-specific layers than the NER task. We also note that while previous work has tackled joint NER and RE in variety of textual domains, in all cases the number of shared and task-specific parameters is held constant across these domains.",
        "type": "Document"
      },
      {
        "id": "33ee0675-c4df-43bd-83a3-f63a1d9e254c",
        "metadata": {
          "vector_store_key": "2002.06424-1",
          "chunk_id": 40,
          "document_id": "2002.06424",
          "start_idx": 21715,
          "end_idx": 22406
        },
        "page_content": "The fact that the optimal number of task-specific layers differed between the two datasets demonstrates the value of taking the number of shared and task-specific layers to be a hyperparameter of our model architecture. As shown in Table TABREF17, the final hyperparameters used for the CoNLL04 dataset included an additional RE-specific BiRNN layer than did the final hyperparameters used for the ADE dataset. We suspect that this is due to the limited number of relations and entities in the ADE dataset. For most examples in this dataset, it is sufficient to correctly identify a single Drug entity, a single Adverse-Effect entity, and an Adverse-Effect relation between the two entities.",
        "type": "Document"
      },
      {
        "id": "eaaefabc-352e-4c3d-8306-9aa838c4000c",
        "metadata": {
          "vector_store_key": "2002.06424-1",
          "chunk_id": 43,
          "document_id": "2002.06424",
          "start_idx": 23211,
          "end_idx": 23815
        },
        "page_content": "These results show that the proposed architecture benefits from the inclusion of both NER- and RE-specific layers. However, the RE task benefits much more from the inclusion of these task-specific layers than does the NER task. We take this to reflect the fact that the RE task is more difficult than the NER task for the CoNLL04 dataset, and therefore benefits the most from its own task-specific layers. This is consistent with the fact that the hyperparameter setting that performs best on the RE task is that with no NER-specific BiRNN layers, i.e. the setting that retained RE-specific BiRNN layers.",
        "type": "Document"
      },
      {
        "id": "7bff54f2-f693-4590-86c8-e83ad136e74b",
        "metadata": {
          "vector_store_key": "1808.09920-8",
          "chunk_id": 56,
          "document_id": "1808.09920",
          "start_idx": 29949,
          "end_idx": 30632
        },
        "page_content": "Eventually, a 2-layers MLP with [256, 128] hidden units takes the concatenation between $\\lbrace \\mathbf {h}_i^{(L)}\\rbrace _{i=1}^N$ and $\\mathbf {q}$ to predict the probability that a candidate node $v_i$ may be the answer to the query $q$ (see Equation 16 ). During preliminary trials, we experimented with different numbers of R-GCN-layers (in the range 1-7). We observed that with WikiHop, for $L \\ge 3$ models reach essentially the same performance, but more layers increase the time required to train them. Besides, we observed that the gating mechanism learns to keep more and more information from the past at each layer making unnecessary to have more layers than required.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many layers of self-attention does the model have?",
    "llm_answer": "6\n",
    "context": [
      {
        "id": "cf88ed96-c40e-4504-b811-d5b9d32f9c01",
        "metadata": {
          "vector_store_key": "1911.02711-3",
          "chunk_id": 29,
          "document_id": "1911.02711",
          "start_idx": 16238,
          "end_idx": 16956
        },
        "page_content": "More layers do not increase the accuracy on development set. We thus set 2 as the number of review encoder layers in the experiments. The best performing model size is comparable to that of the BiLSTM self-attention, demonstrating that the number of parameters is not the key factor to models' performance. Table TABREF34 and Table TABREF35 show the final results. Our model outperforms all the baseline models and the top-performing models with both generated summary and golden summary, for all the three datasets. In the scenario where golden summaries are used, BiLSTM+self-attention performs the best among all the baselines, which shows that attention is a useful way to integrate summary and review information.",
        "type": "Document"
      },
      {
        "id": "d36e5e14-9d4e-4853-befa-2590904f0344",
        "metadata": {
          "vector_store_key": "1910.11204-1",
          "chunk_id": 21,
          "document_id": "1910.11204",
          "start_idx": 12534,
          "end_idx": 13259
        },
        "page_content": "By this way, the model considers the pairwise relationships between input elements, which highly agrees with the task of SRL, i.e., aiming to find the semantic relations between the candidate argument and predicate in one sentence. Compared to the standard attention, in this paper, we add the dependency information into $Q$ and $V$ in each attention head, like equation (DISPLAY_FORM15) shows: where $E_D$ and $E_R$ mean the syntactic dependency head and relation information respectively. For our multi-layer multi-head self-attention model, we make this change to each head of the first $N$ self-attention layers. Datasets & Evaluation Metrics Our experiments are conducted on the CoNLL-2009 shared task dataset BIBREF20.",
        "type": "Document"
      },
      {
        "id": "07dd1740-7e16-4296-bdbc-f445a8f19b66",
        "metadata": {
          "vector_store_key": "1907.02636-4",
          "chunk_id": 28,
          "document_id": "1907.02636",
          "start_idx": 15332,
          "end_idx": 16002
        },
        "page_content": "For multi-head self-attention module, we employ a stack of 6 multi-head self attention layer, each of which has 4 head and dimension of each head is set to 64. (b) All of the ANN\u2019s parameters are initialized with a uniform distribution ranging from -1 to 1. (c) We train our model with a fixed learning rate of 0.005. The minimum number of epochs for training is set as 30. After the first 30 epochs had been trained, we compute the average F1-score of the validation set by the use of the currently produced model after every epoch had been trained, and stop the training process when the average F1-score of validation set fails to increase during the last ten epochs.",
        "type": "Document"
      },
      {
        "id": "fc053402-c3d1-442e-9ce0-609348300c32",
        "metadata": {
          "vector_store_key": "1806.09103-3",
          "chunk_id": 25,
          "document_id": "1806.09103",
          "start_idx": 14455,
          "end_idx": 15246
        },
        "page_content": "We use three attention layers for all experiments. The GRU hidden units for both the word and subword representation were 128. The default frequency filter proportion was 0.9 and the default merging times of BPE was 1,000. We also apply dropout between layers with a dropout rate of 0.5 . [7]http://www.hfl-tek.com/cmrc2017/leaderboard.html Table TABREF17 shows our results on CMRC-2017 dataset, which shows that our SAW Reader (mul) outperforms all other single models on the test set, with 7.57% improvements compared with Attention Sum Reader (AS Reader) baseline. Although WHU's model achieves the best besides our model on the valid set with only 0.75% below ours, their result on the test set is lower than ours by 2.27%, indicating our model has a satisfactory generalization ability.",
        "type": "Document"
      },
      {
        "id": "eda7231d-b33a-4480-b3c9-2a99a54cf515",
        "metadata": {
          "vector_store_key": "2002.08307-2",
          "chunk_id": 13,
          "document_id": "2002.08307",
          "start_idx": 7615,
          "end_idx": 8311
        },
        "page_content": "Recall that self-attention first projects layer inputs into key, query, and value embeddings via linear projections. While there is a separate key, query, and value projection matrix for each attention head, implementations typically \u201cstack\u201d matrices from each attention head, resulting in only 3 parameter matrices: one for key projections, one for value projections, and one for query projections. We prune each of these matrices separately, calculating a threshold for each. We also prune the linear output projection, which combines outputs from each attention head into a single embedding. We prune word embeddings in the same way we prune feed-foward networks and self-attention parameters.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what are the state of the art methods?",
    "llm_answer": "RNN pre-trained using GloVe word embeddings, LSTM with dropout and ReLU activation, and a dense layer with sigmoid activation using an Adam optimizer on the binary crossentropy loss.  Also, Cavnar and Trenkle's method, implemented as  and achieving 99.8% accuracy.\n",
    "context": [
      {
        "id": "eaa45ca2-7c16-447a-9500-b45093cc3efd",
        "metadata": {
          "vector_store_key": "1904.07342-3",
          "chunk_id": 9,
          "document_id": "1904.07342",
          "start_idx": 4887,
          "end_idx": 5594
        },
        "page_content": "We experimented with different feature extraction methods and classification models. Feature extractions examined include Tokenizer, Unigram, Bigram, 5-char-gram, and td-idf methods. Models include both neural nets (e.g. RNNs, CNNs) and standard machine learning tools (e.g. Naive Bayes with Laplace Smoothing, k-clustering, SVM with linear kernel). Model accuracies are reported in Table FIGREF3 . The RNN pre-trained using GloVe word embeddings BIBREF6 achieved the higest test accuracy. We pass tokenized features into the embedding layer, followed by an LSTM BIBREF7 with dropout and ReLU activation, and a dense layer with sigmoid activation. We apply an Adam optimizer on the binary crossentropy loss.",
        "type": "Document"
      },
      {
        "id": "c282a797-7698-46e0-b5da-692be2bae246",
        "metadata": {
          "vector_store_key": "1804.08186-2",
          "chunk_id": 294,
          "document_id": "1804.08186",
          "start_idx": 167030,
          "end_idx": 167629
        },
        "page_content": "The types of models as well as the sources of training data used in the literature are diverse, and work to date has not compared and evaluated these in a systematic manner, making it difficult to draw broader conclusions about what the \u201cbest\u201d method for actually is. We have attempted to synthesize results to date to identify a set of \u201cbest practices\u201d, but these should be treated as guidelines and should always be considered in the broader context of a target application. Existing work on serves to illustrate that the scope and depth of the problem are much greater than they may first appear.",
        "type": "Document"
      },
      {
        "id": "a8a499e4-7752-44c8-be98-646a509eaa4d",
        "metadata": {
          "vector_store_key": "1705.03261-3",
          "chunk_id": 4,
          "document_id": "1705.03261",
          "start_idx": 2353,
          "end_idx": 3012
        },
        "page_content": "The details about how we train our model and extract the DDI type from text are described in the remaining sections. In DDI extraction task, NLP methods or machine learning approaches are proposed by most of the work. Chowdhury BIBREF14 and Thomas et al. BIBREF11 proposed methods that use linguistic phenomenons and two-stage SVM to classify DDIs. FBK-irst BIBREF10 is a follow-on work which applies kernel method to the existing model and outperforms it. Neural network based approaches have been proposed by several works. Liu et al. BIBREF9 employ CNN for DDI extraction for the first time which outperforms the traditional machine learning based methods.",
        "type": "Document"
      },
      {
        "id": "ae9351ef-a060-4512-9489-a62adfc9c4c2",
        "metadata": {
          "vector_store_key": "1908.08419-1",
          "chunk_id": 7,
          "document_id": "1908.08419",
          "start_idx": 4155,
          "end_idx": 4849
        },
        "page_content": "These methods are mainly focus on two categories: supervised learning and deep learning BIBREF2 . Supervised Learning Methods. Initially, supervised learning methods were widely-used in CWS. Xue BIBREF13 employed a maximum entropy tagger to automatically assign Chinese characters. Zhao et al. BIBREF16 used a conditional random field for tag decoding and considered both feature template selection and tag set selection. However, these methods greatly rely on manual feature engineering BIBREF17 , while handcrafted features are difficult to design, and the size of these features is usually very large BIBREF6 . Deep Learning Methods. Recently, neural networks have been applied in CWS tasks.",
        "type": "Document"
      },
      {
        "id": "7a264899-d3ab-4384-b07e-d784e83be12e",
        "metadata": {
          "vector_store_key": "1804.08186-2",
          "chunk_id": 22,
          "document_id": "1804.08186",
          "start_idx": 12429,
          "end_idx": 13238
        },
        "page_content": "Cavnar and Trenkle's method (which we describe in detail in outofplace) builds up per-document and per-language profiles, and classifies a document according to which language profile it is most similar to, using a rank-order similarity metric. They evaluate their system on 3478 documents in eight languages obtained from USENET newsgroups, reporting a best overall accuracy of 99.8%. Gertjan van Noord produced an implementation of the method of Cavnar and Trenkle named , which has become eponymous with the method itself. is packaged with pre-trained models for a number of languages, and so it is likely that the strong results reported by Cavnar and Trenkle, combined with the ready availability of an \u201coff-the-shelf\u201d implementation, has resulted in the exceptional popularity of this particular method.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which four languages do they experiment with?",
    "llm_answer": "German, English, Chinese, and English.\n",
    "context": [
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      },
      {
        "id": "9ece3dbc-6d9e-49d3-838c-c59b224fb280",
        "metadata": {
          "vector_store_key": "2002.02427-3",
          "chunk_id": 26,
          "document_id": "2002.02427",
          "start_idx": 14704,
          "end_idx": 14917
        },
        "page_content": "Our next step is to experiment with other languages such as Hindi and Italian. The work of Paolo Rosso was partially funded by the Spanish MICINN under the research project MISMIS-FAKEnHATE (PGC2018-096212-B-C31).",
        "type": "Document"
      },
      {
        "id": "22504978-69cc-4504-9abc-41d5651e4391",
        "metadata": {
          "vector_store_key": "1804.08186-3",
          "chunk_id": 248,
          "document_id": "1804.08186",
          "start_idx": 140712,
          "end_idx": 141342
        },
        "page_content": "An interesting area to explore is the trade-off between the number of languages supported and the accuracy per-language. From existing results it is not clear if it is possible to continue increasing the number of languages supported without adversely affecting the average accuracy, but it would be useful to quantify if this is actually the case across a broad range of text sources. mostlanguages lists the articles where the with more than 30 languages has been investigated. \u201cUnseen\u201d languages are languages that we do not have training data for but may nonetheless be encountered by a system when applied to real-world data.",
        "type": "Document"
      },
      {
        "id": "c28fe057-2cfc-4978-9c0a-84fd286d4b98",
        "metadata": {
          "vector_store_key": "1612.06897-3",
          "chunk_id": 39,
          "document_id": "1612.06897",
          "start_idx": 20315,
          "end_idx": 20488
        },
        "page_content": "We presented results on two diverse language pairs German $\\rightarrow $ English and Chinese $\\rightarrow $ English (usually very challenging pairs for machine translation).",
        "type": "Document"
      },
      {
        "id": "9cd0820a-fcb8-4bf5-b481-c19a5a179e77",
        "metadata": {
          "vector_store_key": "1909.09270-5",
          "chunk_id": 26,
          "document_id": "1909.09270",
          "start_idx": 14042,
          "end_idx": 14768
        },
        "page_content": "These languages are: Amharic (amh: LDC2016E87), Arabic (ara: LDC2016E89), Hindi (hin: LDC2017E62), and Somali (som: LDC2016E91). These come from a variety of sources including discussion forums, newswire, and social media. The labelset is Person, Organization, Location, Geo-political entity. We define train/development/test splits, taking care to keep a similar distribution of genres in each split. Data statistics for all languages are shown in Table TABREF25. We create partial annotations by perturbing gold annotated data in two ways: lowering recall (to simulate missing entities), and lowering precision (to simulate noisy annotations). To lower recall, we replace gold named entity tags with $O$ tags (for non-name).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Does DCA or GMM-based attention perform better in experiments?",
    "llm_answer": "DCA performs better than GMM-based attention in the experiments, as visualized results show better performance and faster loss reduction.\n",
    "context": [
      {
        "id": "6426bcfc-aed9-4070-875a-bf004c727933",
        "metadata": {
          "vector_store_key": "1910.10288-0",
          "chunk_id": 31,
          "document_id": "1910.10288",
          "start_idx": 17466,
          "end_idx": 18120
        },
        "page_content": "Compared to GMMv2b, DCA can more easily bound its receptive field (because its prior filter numerically disallows excessive forward movement), which makes it easier to incorporate hard windowing optimizations in production. Another advantage of DCA over GMM attention is that its attention weights are normalized, which helps to stabilize the alignment, especially for coarse-grained alignment tasks. For monotonic alignment tasks like TTS and speech recognition, location-relative attention mechanisms have many advantages and warrant increased consideration and further study. Supplemental materials, including audio examples, are available on the web.",
        "type": "Document"
      },
      {
        "id": "23cb61b1-1e29-4c81-b4e0-11caae3eb342",
        "metadata": {
          "vector_store_key": "1803.09230-0",
          "chunk_id": 24,
          "document_id": "1803.09230",
          "start_idx": 13242,
          "end_idx": 13986
        },
        "page_content": "Visualizations demonstrate that both hybrid and DCA models perform better than vanilla Co-Attention and BiDAF attention mechanisms and reduce the losses faster and increase the dev F1/EM scores faster as well. We made a brief attempt to do a bit of hyperparameter tuning on our proposed DCA model and we report the results in Table 3 . Ideally, hyperparameter tuning for neural network architectures should be done using bayesian hyperparameter optimization but due to the lack of time we tried to do a random search on a small set of hyperparameters that we guessed could be more suitable. While, we didn't find any significantly good set of parameters, we noticed that reducing the hidden size has a minor effect on improving the performance.",
        "type": "Document"
      },
      {
        "id": "15d34473-4dbd-4d14-b65f-501fa2cfe5bf",
        "metadata": {
          "vector_store_key": "1910.10288-0",
          "chunk_id": 12,
          "document_id": "1910.10288",
          "start_idx": 7237,
          "end_idx": 8051
        },
        "page_content": "Despite the fact that GMM attention V1 and V2 use normalized mixture weights and components, the attention weights still end up unnormalized because they are sampled from a continuous probability density function. This can lead to occasional spikes or dropouts in the alignment, and attempting to directly normalize GMM attention weights results in unstable training. Attention normalization isn't a significant problem in fine-grained output-to-text alignment, but becomes more of an issue for coarser-grained alignment tasks where the attention window needs to gradually move to the next index (for example in variable-length prosody transfer applications BIBREF12). Because DCA is in the energy-based attention family, it is normalized by default and should work well for a variety of monotonic alignment tasks.",
        "type": "Document"
      },
      {
        "id": "cc5a5f26-ba43-4f54-b961-4050919ed403",
        "metadata": {
          "vector_store_key": "1910.10288-0",
          "chunk_id": 30,
          "document_id": "1910.10288",
          "start_idx": 16771,
          "end_idx": 17466
        },
        "page_content": "They also tend to align very quickly during training, which makes the occasional alignment failure easy to detect so training can be restarted. In our alignment trials, despite being slower to align on average, LSA attention seemed to have an edge in terms of alignment consistency; however, we have noticed that slower alignment can sometimes lead to worse quality models, probably because the other model components are being optimized in an unaligned state for longer. Compared to GMMv2b, DCA can more easily bound its receptive field (because its prior filter numerically disallows excessive forward movement), which makes it easier to incorporate hard windowing optimizations in production.",
        "type": "Document"
      },
      {
        "id": "a85d70a1-9658-45c4-95e4-83736eebd032",
        "metadata": {
          "vector_store_key": "1910.10288-0",
          "chunk_id": 13,
          "document_id": "1910.10288",
          "start_idx": 8051,
          "end_idx": 8826
        },
        "page_content": "Because DCA is in the energy-based attention family, it is normalized by default and should work well for a variety of monotonic alignment tasks. Another issue with GMM attention is that because it uses a mixture of distributions with infinite support, it isn't necessarily monotonic. At any time, the mechanism could choose to emphasize a component whose mean is at an earlier point in the sequence, or it could expand the variance of a component to look backward in time, potentially hurting alignment stability. To address monotonicity issues, we make modifications to the hybrid location-sensitive mechanism. First we remove the content-based terms, $W\\mathbf {s}_i$ and $Wi$, which prevents the alignment from moving backward due to a query/key match at a past timestep.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What evaluation metric is used?",
    "llm_answer": "Accuracy (Acc), weighted F1 score (F1-w), macro F1 score (F1-m), area under the ROC curve (AUC), equality differences (ED).\n",
    "context": [
      {
        "id": "1c2721da-ef50-4f82-92a2-9f11a92306e0",
        "metadata": {
          "vector_store_key": "2002.05058-2",
          "chunk_id": 49,
          "document_id": "2002.05058",
          "start_idx": 27267,
          "end_idx": 27919
        },
        "page_content": "As described in the human evaluation procedure, we perform 10 runs to test the reliability of each metric when used to perform hyperparameter tuning and early-stopping respectively. In each run, we select the best hyperparameter combination or early-stopping checkpoint based on each of the five compared metrics. Human evaluation is then employed to identify the best choice. We evaluate the performance of each metric by how many times (out of 10) they succeeded in selecting the best hyperparameter combination or early-stopping checkpoint (out of 4) and the average human-annotated score for their selected models. The results are shown in Table 3.",
        "type": "Document"
      },
      {
        "id": "239cc6aa-01e0-4622-91f9-05e7083798ee",
        "metadata": {
          "vector_store_key": "2002.05058-2",
          "chunk_id": 48,
          "document_id": "2002.05058",
          "start_idx": 26480,
          "end_idx": 27267
        },
        "page_content": "We can see that the proposed comparative evaluator with skill rating significantly outperforms all compared baselines, including comparative evaluator with averaged sample-level scores. This demonstrates the effectiveness of the skill rating system for performing model-level comparison with pairwise sample-level evaluation. In addition, the poor correlation between conventional evaluation metrics including BLEU and perplexity demonstrates the necessity of better automated evaluation metrics in open domain NLG evaluation. We further investigate the impact of imperfect metrics on training NLG models. As described in the human evaluation procedure, we perform 10 runs to test the reliability of each metric when used to perform hyperparameter tuning and early-stopping respectively.",
        "type": "Document"
      },
      {
        "id": "61e54692-2da3-41fb-b1b0-c3469ffa76bf",
        "metadata": {
          "vector_store_key": "1607.06025-0",
          "chunk_id": 17,
          "document_id": "1607.06025",
          "start_idx": 10067,
          "end_idx": 10777
        },
        "page_content": "These attributes are conditional information that is fed to the models, like the discrete label is in our models. As recognized by BIBREF37 , the evaluation metrics of text-generating models fall into three categories: manual evaluation, automatic evaluation metrics, task-based evaluation. In evaluation based on human judgment each generated textual example is inspected manually. The automatic evaluation metrics, like ROUGE, BLEU and METEOR, compare human texts and generated texts. BIBREF38 shows METEOR has the strongest correlation with human judgments in image description evaluation. The last category is task-based evaluation, where the impact of the generated texts on a particular task is measured.",
        "type": "Document"
      },
      {
        "id": "d13a761b-92fd-4d26-99f1-f829fcfed252",
        "metadata": {
          "vector_store_key": "2002.10361-3",
          "chunk_id": 31,
          "document_id": "2002.10361",
          "start_idx": 18881,
          "end_idx": 19583
        },
        "page_content": "To measure overall performance, we evaluate models by four metrics: accuracy (Acc), weighted F1 score (F1-w), macro F1 score (F1-m) and area under the ROC curve (AUC). The F1 score coherently combines both precision and recall by $2*\\frac{precision*recall}{precision+recall}$. We report F1-m considering that the datasets are imbalanced. To evaluate group fairness, we measure the equality differences (ED) of true positive/negative and false positive/negative rates for each demographic factor. ED is a standard metric to evaluate fairness and bias of document classifiers BIBREF0, BIBREF4, BIBREF5. This metric sums the differences between the rates within specific user groups and the overall rates.",
        "type": "Document"
      },
      {
        "id": "670dd0d6-8831-4c6f-ba66-53275f205e4a",
        "metadata": {
          "vector_store_key": "1804.08186-8",
          "chunk_id": 172,
          "document_id": "1804.08186",
          "start_idx": 98612,
          "end_idx": 99525
        },
        "page_content": "The system used by BIBREF172 ranked first in the German dialect identification shared task, and the system by BIBREF185 came second (71.65% accuracy) in the Arabic dialect identification shared task. In the previous two sections, we have alluded to issues of evaluation in research to date. In this section, we examine the literature more closely, providing a broad overview of the evaluation metrics that have been used, as well as the experimental settings in which research has been evaluated. The most common approach is to treat the task as a document-level classification problem. Given a set of evaluation documents, each having a known correct label from a closed set of labels (often referred to as the \u201cgold-standard\u201d), and a predicted label for each document from the same set, the document-level accuracy is the proportion of documents that are correctly labeled over the entire evaluation collection.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is any data-to-text generation model trained on this new corpus, what are the results?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "b05ce8a1-ee5d-4c77-8a9a-91429ee17e6a",
        "metadata": {
          "vector_store_key": "2001.11381-3",
          "chunk_id": 26,
          "document_id": "2001.11381",
          "start_idx": 14593,
          "end_idx": 15417
        },
        "page_content": "Este corpus fue utilizado principalmente en los dos modelos generativos: modelo basado en cadenas de Markov (Secci\u00f3n SECREF13) y modelo basado en la generaci\u00f3n de Texto enlatado (Canned Text, Secci\u00f3n SECREF15). En este trabajo proponemos tres modelos h\u00edbridos (combinaciones de modelos generativos cl\u00e1sicos y aproximaciones sem\u00e1nticas) para la producci\u00f3n de frases literarias. Hemos adaptado dos modelos generativos, usando an\u00e1lisis sint\u00e1ctico superficial (shallow parsing) y un modelo de aprendizaje profundo (Deep Learning) BIBREF25, combinados con tres modelos desarrollados de aproximaci\u00f3n sem\u00e1ntica. En una primera fase, los modelos generativos recuperan la informaci\u00f3n gramatical de cada palabra del corpus 8KF (ver Secci\u00f3n SECREF3), en forma de etiquetas POS (Part of Speech), a trav\u00e9s de un an\u00e1lisis morfosint\u00e1ctico.",
        "type": "Document"
      },
      {
        "id": "d78a72c3-99ab-40df-ba08-6eb5ebd41afb",
        "metadata": {
          "vector_store_key": "1910.07481-1",
          "chunk_id": 32,
          "document_id": "1910.07481",
          "start_idx": 18054,
          "end_idx": 18150
        },
        "page_content": "To consider a training corpus with a majority of document delimited data is also very promising.",
        "type": "Document"
      },
      {
        "id": "2d11bd5b-76dc-4a06-b568-28f996b411ab",
        "metadata": {
          "vector_store_key": "2001.11381-3",
          "chunk_id": 25,
          "document_id": "2001.11381",
          "start_idx": 14044,
          "end_idx": 14881
        },
        "page_content": "Estas son, sin embargo, las condiciones que presenta un corpus literario real. Un corpus heterog\u00e9neo de casi 8 000 frases literarias fue constituido manualmente a partir de poemas, discursos, citas, cuentos y otras obras. Se evitaron cuidadosamente las frases de lengua general, y tambi\u00e9n aquellas demasiado cortas ($N \\le 3$ palabras) o demasiado largas ($N \\ge 30$ palabras). El vocabulario empleado es complejo y est\u00e9tico, adem\u00e1s que el uso de ciertas figuras literarias como la rima, la an\u00e1fora, la met\u00e1fora y otras pueden ser observadas en estas frases. Las caracter\u00edsticas del corpus 8KF se muestran en la Tabla TABREF6. Este corpus fue utilizado principalmente en los dos modelos generativos: modelo basado en cadenas de Markov (Secci\u00f3n SECREF13) y modelo basado en la generaci\u00f3n de Texto enlatado (Canned Text, Secci\u00f3n SECREF15).",
        "type": "Document"
      },
      {
        "id": "de2465b8-bc43-4557-b11e-b5ad1b2f0d72",
        "metadata": {
          "vector_store_key": "1909.00279-3",
          "chunk_id": 23,
          "document_id": "1909.00279",
          "start_idx": 13381,
          "end_idx": 14083
        },
        "page_content": "(Section SECREF27) To this end, we built a dataset as described in Section SECREF18. Evaluation metrics and baselines are described in Section SECREF21 and SECREF22. For the implementation details of building the dataset and models, please refer to supplementary materials. Training and Validation Sets We collected a corpus of poems and a corpus of vernacular literature from online resources. The poem corpus contains 163K quatrain poems from Tang Poems and Song Poems, the vernacular literature corpus contains 337K short paragraphs from 281 famous books, the corpus covers various literary forms including prose, fiction and essay. Note that our poem corpus and a vernacular corpus are not aligned.",
        "type": "Document"
      },
      {
        "id": "4196e073-3314-48e8-ad4c-8488e8a91274",
        "metadata": {
          "vector_store_key": "1905.06566-1",
          "chunk_id": 42,
          "document_id": "1905.06566",
          "start_idx": 23254,
          "end_idx": 24010
        },
        "page_content": "One is the hierarchical transformer summarization model (HeriTransfomer; described in \"Extractive Summarization\" ) without pre-training. Note the setting for HeriTransfomer is ( $L=4$ , $H=300$ and $A=4$ ) . We can see that the pre-training (details in Section \"Pre-training\" ) leads to a +1.25 ROUGE improvement. Another baseline is based on a pre-trained BERT BIBREF0 and finetuned on the CNNDM dataset. We used the $\\text{BERT}_{\\text{base}}$ model because our 16G RAM V100 GPU cannot fit $\\text{BERT}_{\\text{large}}$ for the summarization task even with batch size of 1. The positional embedding of BERT supports input length up to 512 words, we therefore split documents with more than 10 sentences into multiple blocks (each block with 10 sentences).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are the potentially relevant text fragments identified?",
    "llm_answer": "A search query, derived from a question model, is used to find potentially relevant documents.  Entity mentions compatible with a question type are extracted from each document, and context is generated around each mention.  A distance measure is then used to assess the relevance of the document content to the question.\n",
    "context": [
      {
        "id": "ab674a67-5069-4697-b94d-f23217a9a6f1",
        "metadata": {
          "vector_store_key": "1908.08419-2",
          "chunk_id": 5,
          "document_id": "1908.08419",
          "start_idx": 3007,
          "end_idx": 3728
        },
        "page_content": "Specifically, we combine information branch and gated neural network to determine if the segment is a legal word, i.e., word score. Meanwhile, we use the hidden layer output of the long short-term memory (LSTM) BIBREF11 to find out how the word is linked to its surroundings, i.e., link score. The final decision on the selection of labeling samples is made by calculating the average of word and link scores on the whole segmented sentence, i.e., sequence score. Besides, to capture coherence over characters, we additionally add K-means clustering features to the input of CRF-based word segmenter. To sum up, the main contributions of our work are summarized as follows: The rest of this paper is organized as follows.",
        "type": "Document"
      },
      {
        "id": "3baf57ae-057c-493b-b205-de0f35c070d8",
        "metadata": {
          "vector_store_key": "1907.11499-3",
          "chunk_id": 5,
          "document_id": "1907.11499",
          "start_idx": 2639,
          "end_idx": 3336
        },
        "page_content": "We refer to this task as domain detection and assume a fairly common setting where the domains of a corpus collection are known and the aim is to identify textual segments which are domain-heavy, i.e., documents, sentences, or phrases providing evidence for a given domain. Domain detection can be formulated as a multilabel classification problem, where a model is trained to recognize domain evidence at the sentence-, phrase-, or word-level. By definition then, domain detection would require training data with fine-grained domain labels, thereby increasing the annotation burden; we must provide labels for training domain detectors and for modeling the task we care about in the first place.",
        "type": "Document"
      },
      {
        "id": "051b74d3-0886-4287-948c-7f3dc0543537",
        "metadata": {
          "vector_store_key": "1901.00570-0",
          "chunk_id": 18,
          "document_id": "1901.00570",
          "start_idx": 9643,
          "end_idx": 10407
        },
        "page_content": "The most recognized techniques are (1) Latent Semantic Indexing (LSI), where the observation matrix is decomposed using singular value decomposition and the data are clustered using K-Means BIBREF7 ,(2) Latent Dirichlet Allocation (LDA), where the words are clustered using Gaussian mixture models (GMM) according to the likelihood of term co-occurrence within the same context BIBREF14 , (3) Word2Vec, which uses a very large corpus to compute continuous vector representations, where we can apply standard vector operations to map one vector to another BIBREF15 . Cheng et al. suggested using space-time scan statistics to detect events by looking for clusters within data across both time and space, regardless of the content of each individual tweet BIBREF16 .",
        "type": "Document"
      },
      {
        "id": "b61fbff6-9989-4be8-b9e3-0ded8355c234",
        "metadata": {
          "vector_store_key": "1605.08675-1",
          "chunk_id": 26,
          "document_id": "1605.08675",
          "start_idx": 14807,
          "end_idx": 15551
        },
        "page_content": "Secondly, it may be annotated using a set of tools (NLP), but this could also happen at an answering stage for selected documents only. After the system receives a question, it gets analysed (QUESTION ANALYSIS) and transformed into a data structure, called question model. One of its constituents, a search query, is used to find a set of documents, which are probably appropriate for the current problem (SEARCH). For each of the documents, all entity mentions compatible with an obtained question type (e.g. monarchs), are extracted (ENTITY RECOGNITION). For each of them, a context is generated (CONTEXT GENERATION). Finally, a distance between a question content and the entity context is computed to asses its relevance (DISTANCE MEASURE).",
        "type": "Document"
      },
      {
        "id": "c521c1b2-8faf-47d5-a531-96a75dec9ce9",
        "metadata": {
          "vector_store_key": "2002.02224-1",
          "chunk_id": 13,
          "document_id": "2002.02224",
          "start_idx": 7597,
          "end_idx": 8432
        },
        "page_content": "Schweighofer BIBREF20 called for generic tools allowing a document segmentation to ease the processing of unstructured data by giving them some structure. Topic-based segmentation often focuses on the identifying specific sentences that present borderlines of different textual segments. The automatic segmentation is not an individual goal \u2013 it always serves as a prerequisite for further tasks requiring structured data. Segmentation is required for the text summarization BIBREF21, BIBREF22, keyword extraction BIBREF23, textual information retrieval BIBREF24, and other applications requiring input in the form of structured data. Major part of research is focused on semantic similarity methods. The computing similarity between the parts of text presumes that a decrease of similarity means a topical border of two text segments.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What dataset did they use?",
    "llm_answer": "novel-9, novel-50, IMDB62, Stat2015, Stat2016, CS2016, Friends, EmotionPush, SCI, Eng\n",
    "context": [
      {
        "id": "6d614157-50dc-4aaa-af03-adc7f0282a12",
        "metadata": {
          "vector_store_key": "1709.02271-3",
          "chunk_id": 20,
          "document_id": "1709.02271",
          "start_idx": 11344,
          "end_idx": 11969
        },
        "page_content": "(Section SECREF26 ). The statistics for the three datasets used in the experiments are summarized in Table TABREF16 . novel-9. This dataset was compiled by F&H14: a collection of 19 novels by 9 nineteenth century British and American authors in the Project Gutenberg. To compare to F&H14, we apply the same resampling method (F&H14, Section 4.2) to correct the imbalance in authors by oversampling the texts of less-represented authors. novel-50. This dataset extends novel-9, compiling the works of 50 randomly selected authors of the same period. For each author, we randomly select 5 novels for a total 250 novels. IMDB62.",
        "type": "Document"
      },
      {
        "id": "8e741c56-0352-4660-89e2-d995b2fada35",
        "metadata": {
          "vector_store_key": "1807.09671-7",
          "chunk_id": 34,
          "document_id": "1807.09671",
          "start_idx": 19653,
          "end_idx": 20306
        },
        "page_content": "While the 2nd and 3rd data sets are from the same course, Statistics for Industrial Engineers, they were taught in 2015 and 2016 respectively (henceforth Stat2015 and Stat2016), at the Bo\u01e7azi\u00e7i University in Turkey. The course was taught in English while the official language is Turkish. The last one is from a fundamental undergraduate Computer Science course (data structures) at a local U.S. university taught in 2016 (henceforth CS2016). Another reason we choose the student responses is that we have advanced annotation allowing us to perform an intrinsic evaluation to test whether the low-rank approximation does capture similar concepts or not.",
        "type": "Document"
      },
      {
        "id": "32fba02b-0c12-4a85-9c43-bb936afdc125",
        "metadata": {
          "vector_store_key": "1909.07734-4",
          "chunk_id": 4,
          "document_id": "1909.07734",
          "start_idx": 2305,
          "end_idx": 2987
        },
        "page_content": "Approaches used by the teams included deep neural networks and SVM classifiers. In the following sections we expand on the challenge and the data. We then briefly describe the various approaches used by the teams, and conclude with a summary and some notes. Detailed descriptions of the various submissions are available in the teams' technical reports. The two datasets used for the challenge are Friends and EmotionPush, part of the EmotionLines corpus BIBREF4. The datasets contain English-language dialogues of varying lengths. For the competition, we provided 1,000 labeled dialogues from each dataset for training, and 240 unlabeled dialogues from each dataset for evaluation.",
        "type": "Document"
      },
      {
        "id": "24aa1324-c152-45e0-8a12-93a41f493139",
        "metadata": {
          "vector_store_key": "1807.09671-7",
          "chunk_id": 33,
          "document_id": "1807.09671",
          "start_idx": 18808,
          "end_idx": 19653
        },
        "page_content": "Menekse:2011 using paper-based surveys from an introductory materials science and engineering class (henceforth Eng) taught in a major U.S. university, and a subset is made public by us BIBREF11 , available at the link: http://www.coursemirror.com/download/dataset. The remaining three courses are collected by us using a mobile application, CourseMIRROR BIBREF57 , BIBREF58 and then the reference summaries for each course are created by human annotators with the proper background. The human annotators are allowed to create abstract summaries using their own words in addition to selecting phrases directly from the responses. While the 2nd and 3rd data sets are from the same course, Statistics for Industrial Engineers, they were taught in 2015 and 2016 respectively (henceforth Stat2015 and Stat2016), at the Bo\u01e7azi\u00e7i University in Turkey.",
        "type": "Document"
      },
      {
        "id": "c5f16988-f9bb-43d0-a24e-a2d52ae2d42a",
        "metadata": {
          "vector_store_key": "1802.05574-4",
          "chunk_id": 11,
          "document_id": "1802.05574",
          "start_idx": 6530,
          "end_idx": 7227
        },
        "page_content": "This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available and the corpus provides both an XML and a simple text version of each article. We randomly selected 2 sentences with more than two words from each paper using the simple text version of the paper.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the size of the dataset?",
    "llm_answer": "The answer is not found in the context.  While various dataset sizes are referenced (e.g., 2,557 days, 730 days, 12,350 ads), a single, overall dataset size is not provided.\n",
    "context": [
      {
        "id": "9ca5340d-587b-4e94-bcbe-160570788673",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 35,
          "document_id": "1910.12618",
          "start_idx": 19294,
          "end_idx": 19871
        },
        "page_content": "For instance for random forests it can correspond to the maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014 and 2015 serve as validation set (730 days).",
        "type": "Document"
      },
      {
        "id": "8be62763-27f4-46ff-b2bb-edb6a6b2fa04",
        "metadata": {
          "vector_store_key": "1607.06025-3",
          "chunk_id": 42,
          "document_id": "1607.06025",
          "start_idx": 24463,
          "end_idx": 25198
        },
        "page_content": "The new dataset consists of training set, which is generated using examples from the original training set, and a development set, which is generated from the original development set. The beam size for beam search was set to 1. The details of the decision are presented in Section SECREF35 . Some datasets were constructed by filtering the generated datasets according to various thresholds. Thus, the generated datasets were constructed to contain enough examples, so that the filtered datasets had at least the number of examples as the original dataset. In the end, all the datasets were trimmed down to the size of the original dataset by selecting the samples sequentially from the beginning until the dataset had the right size.",
        "type": "Document"
      },
      {
        "id": "73a83f60-b2c9-406d-b4cd-5720639e5f06",
        "metadata": {
          "vector_store_key": "1812.07023-4",
          "chunk_id": 18,
          "document_id": "1812.07023",
          "start_idx": 10358,
          "end_idx": 10986
        },
        "page_content": "After 10 such qa turns, the questioner wraps up by writing a summary of the video based on the conversation. Dataset statistics such as the number of dialogues, turns, and words for the avsd dataset are presented in Table TABREF5 . For the initially released prototype dataset, the training set of the avsd dataset corresponds to videos taken from the training set of the Charades dataset while the validation and test sets of the avsd dataset correspond to videos taken from the validation set of the Charades dataset. For the official dataset, training, validation and test sets are drawn from the corresponding Charades sets.",
        "type": "Document"
      },
      {
        "id": "d87d327c-2858-4b5d-ae8a-860d401f63bd",
        "metadata": {
          "vector_store_key": "1908.05434-1",
          "chunk_id": 27,
          "document_id": "1908.05434",
          "start_idx": 14432,
          "end_idx": 15064
        },
        "page_content": "We have ensured that the raw dataset has no overlap with the labeled dataset to avoid bias in test accuracy. While it is possible to scrape more raw data, we did not observe significant improvements in model performances when the size of raw data increased from INLINEFORM0 70,000 to INLINEFORM1 170,000, hence we assume that the current raw dataset is sufficiently large. The labeled dataset is called Trafficking-10k. It consists of 12,350 ads from Backpage labeled by experts in human trafficking detection BIBREF9 . Each label is one of seven ordered levels of likelihood that the corresponding ad comes from a human trafficker.",
        "type": "Document"
      },
      {
        "id": "78e083f1-0666-4414-8066-d724fc284323",
        "metadata": {
          "vector_store_key": "1901.09755-0",
          "chunk_id": 25,
          "document_id": "1901.09755",
          "start_idx": 14080,
          "end_idx": 14670
        },
        "page_content": "For English, it should be noted that the size of the 2015 set is less than half with respect to the 2014 dataset in terms of tokens, and only one third in number of targets. The French, Spanish and Dutch datasets are quite similar in terms of tokens although the number of targets in the Dutch dataset is comparatively smaller, possibly due to the tendency to construct compound terms in that language. The Russian dataset is the largest whereas the Turkish set is by far the smallest one. Additionally, we think it is also interesting to note the low number of targets that are multiwords.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the 12 AV approaches which are examined?",
    "llm_answer": "The answer is not found in the context.  While the text mentions 12 AV methods were applied, the names of the specific methods are not listed.\n",
    "context": [
      {
        "id": "1ae5bef7-39bb-401b-ae6c-c40e134e5bd9",
        "metadata": {
          "vector_store_key": "1906.10551-3",
          "chunk_id": 79,
          "document_id": "1906.10551",
          "start_idx": 43743,
          "end_idx": 44420
        },
        "page_content": "Additionally, we mentioned that determinism must be fulfilled such that an AV method can be rated as reliable. Moreover, we clarified a number of misunderstandings in previous research works and proposed three clear criteria that allow to classify the model category of an AV method, which in turn influences its design and the way how it should be evaluated. In regard to binary-extrinsic AV approaches, we explained which challenges exist and how they affect their applicability. In an experimental setup, we applied 12 existing AV methods on three self-compiled corpora, where the intention behind each corpus was to focus on a different aspect of the methods applicability.",
        "type": "Document"
      },
      {
        "id": "a8e7fe2b-088f-4875-997d-c9f49a8bf8b3",
        "metadata": {
          "vector_store_key": "1906.10551-3",
          "chunk_id": 4,
          "document_id": "1906.10551",
          "start_idx": 2134,
          "end_idx": 2763
        },
        "page_content": "These, however, must be taken into account before AV methods can be applied in real forensic settings. The objective of this paper is to fill this gap and to propose important properties and criteria that are not only intended to characterize AV methods, but also allow their assessment in a more systematic manner. By this, we hope to contribute to the further development of this young research field. Based on the proposed properties, we investigate the applicability of 12 existing AV approaches on three self-compiled corpora, where each corpus involves a specific challenge. The rest of this paper is structured as follows.",
        "type": "Document"
      },
      {
        "id": "9f7f8e3c-d145-4c4c-8179-a23b843f826f",
        "metadata": {
          "vector_store_key": "1906.10551-8",
          "chunk_id": 11,
          "document_id": "1906.10551",
          "start_idx": 6210,
          "end_idx": 6964
        },
        "page_content": "Before we can assess the applicability of AV methods, it is important to understand their fundamental characteristics. Due to the increasing number of proposed AV approaches in the last two decades, the need arose to develop a systematization including the conception, implementation and evaluation of authorship verification methods. In regard to this, only a few attempts have been made so far. In 2004, for example, Koppel and Schler BIBREF13 described for the first time the connection between AV and unary classification, also known as one-class classification. In 2008, Stein et al. BIBREF14 compiled an overview of important algorithmic building blocks for AV where, among other things, they also formulated three AV problems as decision problems.",
        "type": "Document"
      },
      {
        "id": "6769febc-b5a5-417a-87a0-1a02c58bca14",
        "metadata": {
          "vector_store_key": "1906.10551-3",
          "chunk_id": 59,
          "document_id": "1906.10551",
          "start_idx": 32873,
          "end_idx": 33512
        },
        "page_content": "The results of the 12 examined AV methods are listed in Table TABREF41 , where it can be seen that the majority of the examined AV methods yield useful recognition results with a maximum value of 0.792 in terms of c@1. With the exception of the binary-intrinsic approach COAV, the remaining top performing methods belong to the binary-extrinsic category. This category of AV methods has also been superior in the PAN-AV competitions BIBREF11 , BIBREF5 , BIBREF12 , where they outperformed binary-intrinsic and unary approaches three times in a row (2013\u20132015). The top performing approaches Caravel, COAV and NNCD deserve closer attention.",
        "type": "Document"
      },
      {
        "id": "3f8abf9e-1146-4e91-9585-cb954eddcbe5",
        "metadata": {
          "vector_store_key": "1906.10551-8",
          "chunk_id": 9,
          "document_id": "1906.10551",
          "start_idx": 5173,
          "end_idx": 5825
        },
        "page_content": "These include simple synonym replacements, rule-based substitutions or word order permutations. In 2016, Potthast et al. BIBREF10 presented the first large-scale evaluation of three AO approaches that aim to attack 44 AV methods, which were submitted to the PAN-AV competitions during 2013-2015 BIBREF11 , BIBREF5 , BIBREF12 . One of their findings was that even basic AO approaches have a significant impact on many AV methods. More precisely, the best-performing AO approach was able to flip on average INLINEFORM0 % of an authorship verifier\u2019s decisions towards choosing N (\u201cdifferent author\u201d), while in fact Y (\u201csame author\u201d) was correct BIBREF10 .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "how was annotation done?",
    "llm_answer": "Annotators were presented with image-sentence pairs.  They chose a label, highlighted words in the sentence, and explained their decision using at least half of the highlighted words.  Each assignment contained 10 image-sentence pairs.  Workers had to have a previous approval rate above 90%.\n",
    "context": [
      {
        "id": "d98f0383-7447-4782-a08b-ebcc38db70ee",
        "metadata": {
          "vector_store_key": "1907.08937-9",
          "chunk_id": 54,
          "document_id": "1907.08937",
          "start_idx": 30992,
          "end_idx": 31046
        },
        "page_content": "We show the guidance provided for the annotators here.",
        "type": "Document"
      },
      {
        "id": "188012af-4dd7-4f42-9775-be8dcdd871ca",
        "metadata": {
          "vector_store_key": "2004.03744-2",
          "chunk_id": 48,
          "document_id": "2004.03744",
          "start_idx": 26393,
          "end_idx": 27002
        },
        "page_content": "2,060 workers participated in the annotation effort, with an average of 1.98 assignments per worker and a standard deviation of 5.54. We required the workers to have a previous approval rate above 90%. No restriction was put on the workers' location. Each assignment consisted of a set of 10 image-sentence pairs. For each pair, the participant was asked to (a) choose a label, (b) highlight words in the sentence that led to their decision, and (c) explain their decision in a comprehensive and concise manner, using a subset of the words that they highlighted. The instructions are shown in Figure FIGREF42.",
        "type": "Document"
      },
      {
        "id": "7e41f955-4622-4d3a-b465-fd3d8521a43d",
        "metadata": {
          "vector_store_key": "1912.01673-1",
          "chunk_id": 21,
          "document_id": "1912.01673",
          "start_idx": 12287,
          "end_idx": 13032
        },
        "page_content": "The time needed to carry out one piece of annotation (i.e. to provide one seed sentence with all 15 transformations) was on average almost 20 minutes but some annotators easily needed even half an hour. Out of the 4262 distinct sentences, only 188 was recorded more than once. In other words, the chance of two annotators producing the same output string is quite low. The most repeated transformations are by far past, future and ban. The least repeated is paraphrase with only single one repeated. multiple-annots documents this in another way. The 293 annotations are split into groups depending on how many annotators saw the same input sentence: 30 annotations were annotated by one person only, 30 annotations by two different persons etc.",
        "type": "Document"
      },
      {
        "id": "59b60f31-8cc7-4dfb-8ddd-cf714164d57d",
        "metadata": {
          "vector_store_key": "2004.03744-2",
          "chunk_id": 28,
          "document_id": "2004.03744",
          "start_idx": 14971,
          "end_idx": 15718
        },
        "page_content": "As mentioned before, in order to submit the annotation of an image-sentence pair, three steps must be completed: workers must choose a label, highlight words in the hypothesis, and use at least half of the highlighted words to write an explanation for their decision. The last two steps thus follow the quality control of crowd-sourced explanations introduced by Camburu BIBREF6. We also ensured that workers do not simply use a copy of the given hypothesis as explanation. We ensured all the above via in-browser checks before workers' submission. An example of collected explanations is given in Figure FIGREF21. To check the success of our crowdsourcing, we manually assessed the relevance of explanations among a random subset of 100 examples.",
        "type": "Document"
      },
      {
        "id": "99ad02db-5ec2-4581-9917-d2c099b5f1b3",
        "metadata": {
          "vector_store_key": "1912.01673-1",
          "chunk_id": 22,
          "document_id": "1912.01673",
          "start_idx": 12632,
          "end_idx": 13376
        },
        "page_content": "The 293 annotations are split into groups depending on how many annotators saw the same input sentence: 30 annotations were annotated by one person only, 30 annotations by two different persons etc. The last column shows the number of unique outputs obtained in that group. Across all cases, 96.8% of produced strings were unique. In line with instructions, the annotators were using the IMPOSSIBLE option scarcely (95 times, i.e. only 2%). It was also a case of 7 annotators only; the remaining 5 annotators were capable of producing all requested transformations. The top three transformations considered unfeasible were different meaning (using the same set of words), past (esp. for sentences already in the past tense) and simple sentence.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they measure correlation between the prediction and explanation quality?",
    "llm_answer": "The text states that the quality of the explanations strongly correlates with the quality of the predictions, but does not describe the *method* used to measure this correlation.\n",
    "context": [
      {
        "id": "a72d6b47-6f17-47e1-a4a4-b38130540816",
        "metadata": {
          "vector_store_key": "1708.01776-4",
          "chunk_id": 21,
          "document_id": "1708.01776",
          "start_idx": 10758,
          "end_idx": 11407
        },
        "page_content": "We tried several thresholds, some adaptive (such as the average of the output vector's values), but found that a fixed threshold of .5 works best. To evaluate the model's ability to jointly learn to predict and explain its predictions we performed two experiments. First, we investigate how the prediction accuracy is affected by jointly training the network to produce explanations. Second, we evaluate how well the model learns to generate explanations. To understand the role of the explanation content in the learning process we perform both of these experiments for each of the two types of explanation: relevant variables and possible answers.",
        "type": "Document"
      },
      {
        "id": "6f133be2-9011-46f8-ae0c-1d6c2973fd8c",
        "metadata": {
          "vector_store_key": "1708.01776-1",
          "chunk_id": 26,
          "document_id": "1708.01776",
          "start_idx": 13365,
          "end_idx": 14026
        },
        "page_content": "In addition to providing supervision on the correct response at each turn, the simulator provides two types of explanation to the Agent: A natural language assessment of the Agent's prediction which includes language about whether the prediction was correct or not, and a description of what can be inferred in the current state \u2013 both about the possible answers and the relevant variables. We used the relevant variable and possible answer explanations to jointly train a modified E2E memory network to both predict and explain it's predictions. Our experiments show that the quality of the explanations strongly correlates with the quality of the predictions.",
        "type": "Document"
      },
      {
        "id": "b343c539-2aea-401a-a06e-20f0f3cb95b7",
        "metadata": {
          "vector_store_key": "1708.01776-4",
          "chunk_id": 27,
          "document_id": "1708.01776",
          "start_idx": 14026,
          "end_idx": 14446
        },
        "page_content": "Our experiments show that the quality of the explanations strongly correlates with the quality of the predictions. Moreover, when the network has trouble predicting, as it does with queries, requiring it to generate good explanations slows its learning. For future work, we would like to investigate whether we can train the net to generate natural language explanations and how this might affect prediction performance.",
        "type": "Document"
      },
      {
        "id": "0e569b91-4acf-47d8-a31c-d8b04729acb8",
        "metadata": {
          "vector_store_key": "1708.01776-4",
          "chunk_id": 24,
          "document_id": "1708.01776",
          "start_idx": 12373,
          "end_idx": 13187
        },
        "page_content": "We would expect that explanation performance should correlate with prediction performance. Since Possible-answer knowledge is primarily needed to decide if the net has enough information to answer the challenge question without guessing and relevant-variable knowledge is needed for the net to know what to query, we analyzed the network's performance on querying and answering separately. The memory network has particular difficulty learning to query relevant variables, reaching only about .5 accuracy when querying. At the same time, it learns to answer very well, reaching over .9 accuracy there. Since these two parts of the interaction are what we ask it to explain in the two modes, we find that the quality of the explanations strongly correlates with the quality of the algorithm executed by the network.",
        "type": "Document"
      },
      {
        "id": "19d9d9d9-6de8-4738-a87b-b3e2f620922e",
        "metadata": {
          "vector_store_key": "1708.01776-4",
          "chunk_id": 23,
          "document_id": "1708.01776",
          "start_idx": 11950,
          "end_idx": 12724
        },
        "page_content": "As illustrated in Figure FIGREF52 , simultaneously learning possible-answer explanations does not affect prediction, while learning relevant-variable explanation learning severely impairs prediction performance, slowing the learning by roughly a factor of four. We can observe the same outcome for the quality of the explanations learned, shown in Figure FIGREF53 . Here again the performance on possible-answer explanations is significantly higher than for relevant-variable explanations. Possible-answer explanations reach an F-Score of .9, while relevant-variable explanations one of .09 only, with precision and recall only slightly deviating from the F-Score in all experiments. We would expect that explanation performance should correlate with prediction performance.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What datasets are used to evaluate the introduced method?",
    "llm_answer": "SNLI dataset, and three self-compiled corpora.\n",
    "context": [
      {
        "id": "3be44322-a327-4b65-b36a-be0068af802a",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 36,
          "document_id": "1910.12618",
          "start_idx": 19551,
          "end_idx": 20124
        },
        "page_content": "The years 2014 and 2015 serve as validation set (730 days). It is used to tune the hyperparameters of the different approaches. All the data from January the 1st 2016 (974 days for France and 1,096 for the UK) is used as test set, on which the final results are presented. Grid search is applied to find the best combination of values: for each hyperparameter, a range of values is defined, and all the possible combinations are successively tested. The one yielding the lowest RMSE (see section SECREF4) on the validation set is used for the final results on the test one.",
        "type": "Document"
      },
      {
        "id": "277b6532-3b00-493f-81d4-bf4bd8453d7b",
        "metadata": {
          "vector_store_key": "1607.06025-2",
          "chunk_id": 39,
          "document_id": "1607.06025",
          "start_idx": 22867,
          "end_idx": 23549
        },
        "page_content": "The premises and labels from the examples of the original dataset are taken as an input for the generative model. The new hypotheses replace the training hypotheses in the new dataset. Next, the classifier, presented in Section SECREF6 , is trained on the generated dataset. The accuracy of the new classifier is the main metric for evaluating the quality of the generated dataset. All the experiments are performed on the SNLI dataset. There are 549,367 examples in the dataset, divided into training, development and test set. Both the development and test set contain around 10.000 examples. Some examples are labeled with '-', which means there was not enough consensus on them.",
        "type": "Document"
      },
      {
        "id": "b8ca75dd-84d9-4514-9a0b-ed8771d04dfc",
        "metadata": {
          "vector_store_key": "1605.08675-1",
          "chunk_id": 90,
          "document_id": "1605.08675",
          "start_idx": 49692,
          "end_idx": 50451
        },
        "page_content": "However, because of the small difference between the techniques including title, for the sake of simplicity, the single sentence is used in the final evaluation. To impose a realistic challenge to the system, the evaluation set, used at this stage, substantially differs from the one used during the development (see section SECREF80 ). A configuration for the final evaluation has been prepared based on results of the experiments. All of the tested versions share the following features: no question analysis corrections, question classification and query generation solutions which proved best in the previous experiments (see section SECREF18 ), a retrieved set of documents including 20 articles, no minimal confidence, singe sentence context with title.",
        "type": "Document"
      },
      {
        "id": "5a56507b-7ad0-4fcd-894b-86ac24a7c339",
        "metadata": {
          "vector_store_key": "2004.01980-2",
          "chunk_id": 33,
          "document_id": "2004.01980",
          "start_idx": 18723,
          "end_idx": 19566
        },
        "page_content": "All datasets are lower-cased. $\\lambda $ is set as 0.5 in experiments. For each iteration of training, we randomly draw a batch of data either from the source dataset or from the target style corpus, and the sampling strategy follows the uniform distribution with the probability being equal to $\\lambda $. The human evaluation is to have a comprehensive measurement of the performances. We conduct experiments on four criteria, relevance, attraction, fluency, and style strength. We summarize the human evaluation results on the first three criteria in Table TABREF51, and the last criteria in Table TABREF57. Note that through automatic evaluation, the baselines NST, Fine-tuned, and Gigaword-MASS perform poorer than other methods (in Section SECREF58), thereby we removed them in human evaluation to save unnecessary work for human raters.",
        "type": "Document"
      },
      {
        "id": "1ae5bef7-39bb-401b-ae6c-c40e134e5bd9",
        "metadata": {
          "vector_store_key": "1906.10551-3",
          "chunk_id": 79,
          "document_id": "1906.10551",
          "start_idx": 43743,
          "end_idx": 44420
        },
        "page_content": "Additionally, we mentioned that determinism must be fulfilled such that an AV method can be rated as reliable. Moreover, we clarified a number of misunderstandings in previous research works and proposed three clear criteria that allow to classify the model category of an AV method, which in turn influences its design and the way how it should be evaluated. In regard to binary-extrinsic AV approaches, we explained which challenges exist and how they affect their applicability. In an experimental setup, we applied 12 existing AV methods on three self-compiled corpora, where the intention behind each corpus was to focus on a different aspect of the methods applicability.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the results achieved from the introduced method?",
    "llm_answer": "The introduced method yields more robust predictions with limited training data, a more comprehensive summary consistent with the source content, improved Word Error Rate (WER), and overall superiority to state-of-the-art methods in speech recognition, with outperformance in two categories (Person and Miscellaneous).\n",
    "context": [
      {
        "id": "bda58570-a888-45a3-813b-1f0849fd758f",
        "metadata": {
          "vector_store_key": "1909.01958-8",
          "chunk_id": 39,
          "document_id": "1909.01958",
          "start_idx": 21900,
          "end_idx": 22692
        },
        "page_content": "Empirically, this two-step approach yields more robust predictions given limited training data compared to a one-step approach where all solver features are fed directly into a single classification step. This section describes our precise experimental methodology followed by our results. In the experimental results reported below, we omitted questions that utilized diagrams. While these questions are frequent in the test, they are outside of our focus on language and reasoning. Moreover, the diagrams are highly varied (see Figure FIGREF22) and despite work that tackled narrow diagram types, e.g, food chains (BID42), overall progress has been quite limited (BID43). We also omitted questions that require a direct answer (rather than selecting from multiple choices), for two reasons.",
        "type": "Document"
      },
      {
        "id": "7c585b62-b44b-418a-8e3b-f980dbe05e36",
        "metadata": {
          "vector_store_key": "1601.02543-0",
          "chunk_id": 4,
          "document_id": "1601.02543",
          "start_idx": 2135,
          "end_idx": 2687
        },
        "page_content": "We then show how this method was adopted to evaluate a speech recognition based solution as a case study. This is the main contribution of the paper. The rest of the paper is organized as follows. The method for evaluation without testing is described in Section SECREF2 . In Section SECREF3 we present a case study and conclude in Section SECREF4 . Fig. FIGREF1 shows the schematic of a typical menu based speech solution having 3 nodes. At each node there are a set of words that the user is expected to speak and the system is supposed to recognize.",
        "type": "Document"
      },
      {
        "id": "83a2682f-77fb-4412-adf7-91d068eba63d",
        "metadata": {
          "vector_store_key": "1805.04033-6",
          "chunk_id": 36,
          "document_id": "1805.04033",
          "start_idx": 19642,
          "end_idx": 20305
        },
        "page_content": "The summary generated by the proposed method is more comprehensive than the reference, while the reference only includes the facts in the last sentence of the source content. In short, the generated summary of the proposed method is more consistent with the source content. It also exhibits the necessity of the proposed human evaluation. Because when the generated summary is evaluated against the reference, it may seem redundant or wrong, but it is actually true to the source content. While it is arguable that the generated summary is better than the reference, there is no doubt that the generated summary of the proposed method is better than the baseline.",
        "type": "Document"
      },
      {
        "id": "4c1b12a8-7898-4344-8118-816aa40d4d2e",
        "metadata": {
          "vector_store_key": "2001.07263-4",
          "chunk_id": 23,
          "document_id": "2001.07263",
          "start_idx": 13335,
          "end_idx": 14047
        },
        "page_content": "Sorting the results by decreasing number of absolute errors on Hub5'00, Table TABREF7 indicates that each regularization method contributes to the improved WER. SpecAugment is by far the most important method, while using $\\Delta $ and $\\Delta \\Delta $ features or switching off the curriculum learning in the later stage of training have marginal but positive effects. Other direct input level perturbation steps (speed/tempo perturbation and sequence noise injection) are also key techniques that can be found in the upper half of the table. If we compare the worst and baseline models, we find that the relative performance difference between them is nearly unchanged by including the external LM in decoding.",
        "type": "Document"
      },
      {
        "id": "11bbd522-28ef-4580-9084-84720bfb9758",
        "metadata": {
          "vector_store_key": "2001.06888-0",
          "chunk_id": 42,
          "document_id": "2001.06888",
          "start_idx": 22556,
          "end_idx": 22770
        },
        "page_content": "Evaluation results of our proposed model compared to other state of the art methods show its superiority to these methods in overall while in two categories (Person and Miscellaneous) our model outperformed others.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they incorporate human advice?",
    "llm_answer": "They use human evaluators to assess the balance of positive/negative sentiment, range of topics, and relevance to judges in test dialogs.  They also consult domain experts and use annotator feedback and agreement information to improve the taxonomy and annotation design.\n",
    "context": [
      {
        "id": "4be44773-b407-4314-abd0-d9c32e99174b",
        "metadata": {
          "vector_store_key": "1908.07816-1",
          "chunk_id": 7,
          "document_id": "1908.07816",
          "start_idx": 4025,
          "end_idx": 4656
        },
        "page_content": "We consider factors such as the balance of positive and negative sentiments in test dialogs, a well-chosen range of topics, and dialogs that our human evaluators can relate. It is the first time such an approach is designed with consideration for the human judges. Our main goal is to increase the objectivity of the results and reduce judges' mistakes due to out-of-context dialogs they have to evaluate. The rest of the paper unfolds as follows. Section SECREF2 discusses some related work. In Section SECREF3, we give detailed description of the methodology. We present experimental results and some analysis in Section SECREF4.",
        "type": "Document"
      },
      {
        "id": "3bc5ae4e-12e6-4182-8d30-90d1ec6a4a8f",
        "metadata": {
          "vector_store_key": "1909.00183-9",
          "chunk_id": 9,
          "document_id": "1909.00183",
          "start_idx": 5143,
          "end_idx": 5911
        },
        "page_content": "Here we demonstrate that publicly reported measures derived from NHS Staff Surveys can help select ground truth labels that allow supervised training of machine learning classifiers to predict the degree of harm directly from text embeddings. Further, we show that the unsupervised clusters of content derived with our method improve the classification results significantly. An a posteriori manual labelling by three clinicians agree with our predictions based purely on text almost as much as with the original hand-coded labels. These results indicate that incidents can be automatically classified according to their degree of harm based only on their textual descriptions, and underlines the potential of automatic document analysis to help reduce human workload.",
        "type": "Document"
      },
      {
        "id": "f3134b27-7cb8-47ad-a686-06a80c1c7a08",
        "metadata": {
          "vector_store_key": "1910.02789-0",
          "chunk_id": 36,
          "document_id": "1910.02789",
          "start_idx": 19409,
          "end_idx": 20215
        },
        "page_content": "BIBREF39 use high-level guidance expressed in text to enrich a stochastic agent, playing against the built-in AI of Civilization II. They train an agent with the Monte-Carlo search framework in order to jointly learn to identify text that is relevant to a given game state as well as game strategies based only on environment feedback. BIBREF40 utilize natural language in a model-based approach to describe the dynamics and rewards of an environment, showing these can facilitate transfer between different domains. More recently, the structure and compositionality of natural language has been used for representing policies in hierarchical RL. In a paper by BIBREF41, instructions given in natural language were used in order to break down complex problems into high-level plans and lower-level actions.",
        "type": "Document"
      },
      {
        "id": "e6c3201a-e04c-447f-9350-8d171df86abc",
        "metadata": {
          "vector_store_key": "1910.08210-6",
          "chunk_id": 5,
          "document_id": "1910.08210",
          "start_idx": 2879,
          "end_idx": 3667
        },
        "page_content": "Our qualitative analyses show that attends to parts of the document relevant to the goal and environment observations, and that the resulting agents exhibit complex behaviour such as retrieving correct items, engaging correct enemies after acquiring correct items, and avoiding incorrect enemies. Finally, we highlight the complexity of in scaling to longer documents, richer dynamics, and natural language variations. We show that significant improvement in language-grounded policy learning is needed to solve these problems in the future. A growing body of research is learning policies that follow imperative instructions. The granularity of instructions vary from high-level instructions for application control BIBREF11 and games BIBREF5, BIBREF6 to step-by-step navigation BIBREF7.",
        "type": "Document"
      },
      {
        "id": "91702fda-46bb-47e0-9af8-8b7db8b245f2",
        "metadata": {
          "vector_store_key": "1709.05413-9",
          "chunk_id": 78,
          "document_id": "1709.05413",
          "start_idx": 42175,
          "end_idx": 43124
        },
        "page_content": "We conduct binary classification experiments to analyze how our predicted dialogue acts can be used to classify conversations as ending in customer satisfaction, customer frustration, and problem resolution. We observe interesting correlations between the dialogue acts agents use and the outcomes, offering insights into good/bad practices that are more useful for creating context-aware automated customer service systems than generating canned response templates. Future directions for this work revolve around the integration of the insights derived in the design of automated customer service systems. To this end, we aim to improve the taxonomy and annotation design by consulting domain-experts and using annotator feedback and agreement information, derive more powerful features for dialogue act prediction, and automate ranking and selection of best-practice rules based on domain requirements for automated customer service system design.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What affective-based features are used?",
    "llm_answer": "DAL Activation, ANEW Dominance, Emolex Negative, Emolex Fear, LIWC Assent, LIWC Cause, LIWC Certain, LIWC Sad.\n",
    "context": [
      {
        "id": "087424d8-9810-4817-8db9-41c87776a18f",
        "metadata": {
          "vector_store_key": "1901.01911-2",
          "chunk_id": 22,
          "document_id": "1901.01911",
          "start_idx": 12532,
          "end_idx": 13237
        },
        "page_content": "It is worth to be noted that in our best configuration system, not all of affective and dialogue-act features were used in our feature vector. After several optimization steps, we found that some features were not improving the system's performance. Our final list of affective and dialogue-act based features includes: DAL Activation, ANEW Dominance, Emolex Negative, Emolex Fear, LIWC Assent, LIWC Cause, LIWC Certain and LIWC Sad. Therefore, we have only 17 columns of features in the best performing system covering structural, conversational, affective and dialogue-act features. We conducted a further analysis of the classification result obtained by the best performing system (79.50 on accuracy).",
        "type": "Document"
      },
      {
        "id": "987c92fd-c51a-489e-b238-0c93ed8d0e49",
        "metadata": {
          "vector_store_key": "1901.01911-2",
          "chunk_id": 10,
          "document_id": "1901.01911",
          "start_idx": 5677,
          "end_idx": 6413
        },
        "page_content": "Most of the participants exploited word embedding to construct their feature space, beside the Twitter domain features. We developed a new model by exploiting several stylistic and structural features characterizing Twitter language. In addition, we propose to utilize conversational-based features by exploiting the peculiar tree structure of the dataset. We also explored the use of affective based feature by extracting information from several affective resources including dialogue-act inspired features. They were designed taking into account several Twitter data characteristics, and then selecting the most relevant features to improve the classification performance. The set of structural features that we used is listed below.",
        "type": "Document"
      },
      {
        "id": "fb656294-9d44-430e-a20f-56a11aa41bc7",
        "metadata": {
          "vector_store_key": "1901.01911-2",
          "chunk_id": 12,
          "document_id": "1901.01911",
          "start_idx": 6896,
          "end_idx": 7670
        },
        "page_content": "These features are devoted to exploit the peculiar characteristics of the dataset, which have a tree structure reflecting the conversation thread. Text Similarity to Source Tweet: Jaccard Similarity of each tweet with its source tweet. Text Similarity to Replied Tweet: the degree of similarity between the tweet with the previous tweet in the thread (the tweet is a reply to that tweet). Tweet Depth: the depth value is obtained by counting the node from sources (roots) to each tweet in their hierarchy. The idea to use affective features in the context of our task was inspired by recent works on fake news detection, focusing on emotional responses to true and false rumors BIBREF5 , and by the work in BIBREF6 reflecting on the role of affect in dialogue acts BIBREF6 .",
        "type": "Document"
      },
      {
        "id": "984fa400-1f4d-49ae-bb68-86d21c0aaa14",
        "metadata": {
          "vector_store_key": "1901.01911-2",
          "chunk_id": 13,
          "document_id": "1901.01911",
          "start_idx": 7670,
          "end_idx": 8315
        },
        "page_content": "The idea to use affective features in the context of our task was inspired by recent works on fake news detection, focusing on emotional responses to true and false rumors BIBREF5 , and by the work in BIBREF6 reflecting on the role of affect in dialogue acts BIBREF6 . Multi-faceted affective features have been already proven to be effective in some related tasks BIBREF9 , including the stance detection task proposed at SemEval-2016 (Task 6). We used the following affective resources relying on different emotion models. Emolex: it contains 14,182 words associated with eight primary emotion based on the Plutchik model BIBREF10 , BIBREF11 .",
        "type": "Document"
      },
      {
        "id": "129ecf4d-28c5-49f2-8fa6-df315b558b92",
        "metadata": {
          "vector_store_key": "1901.01911-2",
          "chunk_id": 4,
          "document_id": "1901.01911",
          "start_idx": 2157,
          "end_idx": 3013
        },
        "page_content": "Contribution We describe a novel classification approach, by proposing a new feature matrix, which includes two new groups: (a) features exploiting the conversational structure of the dataset BIBREF2 ; (b) affective features relying on the use of a wide range of affective resources capturing different facets of sentiment and other affect-related phenomena. We were also inspired by the fake news study on Twitter in BIBREF5 , showing that false stories inspire fear, disgust, and surprise in replies, while true stories inspire anticipation, sadness, joy, and trust. Meanwhile, from a dialogue act perspective, the study of BIBREF6 found that a relationship exists between the use of an affective lexicon and the communicative intention of an utterance which includes AGREE-ACCEPT (support), REJECT (deny), INFO-REQUEST (question), and OPINION (comment).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is performance improvement proposed methods are used?",
    "llm_answer": "The context describes improvements ranging from nearly 50% for decision trees with 1,000 words and 18% for Naive Bayes and 12% for SVM, to 11% for a domain adaptation technique.\n",
    "context": [
      {
        "id": "4f72307a-0a25-4c37-9a12-004e692be9a5",
        "metadata": {
          "vector_store_key": "2003.06279-2",
          "chunk_id": 33,
          "document_id": "2003.06279",
          "start_idx": 19861,
          "end_idx": 20566
        },
        "page_content": "The results obtained for GloVe show that the highest relative improvements in performance occur for decision trees. This is apparent specially for the shortest samples. For $w=1,000$ words, the decision tree accuracy is enhanced by a factor of almost 50% when $p=20\\%$. An excellent gain in performance is also observed for both Naive Bayes and SVM classifiers, when $p=18\\%$ and $p=12\\%$, respectively. When $w=2,500$ words, the highest improvements was observed for the decision tree algorithm. A minor improvement was observed for the kNN method. A similar behavior occurred for $w=5,000$ words. Interestingly, SVM seems to benefit from the use of additional edges when larger documents are considered.",
        "type": "Document"
      },
      {
        "id": "4c1b12a8-7898-4344-8118-816aa40d4d2e",
        "metadata": {
          "vector_store_key": "2001.07263-4",
          "chunk_id": 23,
          "document_id": "2001.07263",
          "start_idx": 13335,
          "end_idx": 14047
        },
        "page_content": "Sorting the results by decreasing number of absolute errors on Hub5'00, Table TABREF7 indicates that each regularization method contributes to the improved WER. SpecAugment is by far the most important method, while using $\\Delta $ and $\\Delta \\Delta $ features or switching off the curriculum learning in the later stage of training have marginal but positive effects. Other direct input level perturbation steps (speed/tempo perturbation and sequence noise injection) are also key techniques that can be found in the upper half of the table. If we compare the worst and baseline models, we find that the relative performance difference between them is nearly unchanged by including the external LM in decoding.",
        "type": "Document"
      },
      {
        "id": "cae387d5-b5b1-44de-a443-42d90d1eb1c2",
        "metadata": {
          "vector_store_key": "2003.06279-7",
          "chunk_id": 32,
          "document_id": "2003.06279",
          "start_idx": 19100,
          "end_idx": 19861
        },
        "page_content": "The relative improvement in performance is given by $\\Gamma _+{(p)}/\\Gamma _0$, where $\\Gamma _+{(p)}$ is the accuracy rate obtained when $p\\%$ additional edges are included and $\\Gamma _0 = \\Gamma _+{(p=0)}$, i.e. $\\Gamma _0$ is the accuracy rate measured from the traditional co-occurrence model. We only show the highest relative improvements in performance for each classifier. In our analysis, we considered also samples of text with distinct length, since the performance of network-based methods is sensitive to text length BIBREF34. In this figure, we considered samples comprising $w=\\lbrace 1.0, 2.5, 5.0, 10.0\\rbrace $ thousand words. The results obtained for GloVe show that the highest relative improvements in performance occur for decision trees.",
        "type": "Document"
      },
      {
        "id": "8bc13773-3a27-45eb-8020-fde3cb1f1a5a",
        "metadata": {
          "vector_store_key": "1804.08186-8",
          "chunk_id": 190,
          "document_id": "1804.08186",
          "start_idx": 108913,
          "end_idx": 109661
        },
        "page_content": "Relatively few authors give specific details of the relationship between the number of features selected and accuracy. A potential reason for this is that the improvement in accuracy plateaus with increasing feature count, though the exact number of features required varies substantially with the method and the data used. At the lower end of the scale, BIBREF7 report that 300\u2013400 features per language is sufficient. Conversely BIBREF148 found that, for the same method, the best results for the evaluation set were attained with 20,000 features per language. As discussed in standardevaluation, the objective comparison of different methods for is difficult due to the variation in the data that different authors have used to evaluate methods.",
        "type": "Document"
      },
      {
        "id": "23976a7a-1b28-442c-baa1-8b2f84b234bb",
        "metadata": {
          "vector_store_key": "1611.00514-0",
          "chunk_id": 30,
          "document_id": "1611.00514",
          "start_idx": 17171,
          "end_idx": 17817
        },
        "page_content": "We evaluated the system on almost 6k out of 24k trials from the other portion to avoid any over-fitting, particularly important for the domain adaptation technique. This resulted in a relative improvement of 11% compared to the primary system in terms of the primary metric. However, the results can be misleading, since the recording condition may be the same for all speakers in the development set. This section reports on the CPU execution time (single threaded), and the amount of memory used to process a single trial, which includes the time for creating models from the enrolment data and the time needed for processing the test segments.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does transfer learning improve performance on this task?",
    "llm_answer": "Transfer learning achieves an average F\u2081 of 75%, which is better than the in-domain Experiment 1 results.\n",
    "context": [
      {
        "id": "aedfd81c-8fc1-4370-a491-73bedbb556ab",
        "metadata": {
          "vector_store_key": "1909.02764-4",
          "chunk_id": 39,
          "document_id": "1909.02764",
          "start_idx": 21643,
          "end_idx": 22364
        },
        "page_content": "With this procedure we achieve an average performance of F$_1$=75 %, being better than the results from the in-domain Experiment 1. The best performance of F$_1$=76 % is achieved with the model pre-trained on each data set, except for ISEAR. All transfer learning models clearly outperform their simple out-of-domain counterpart. To ensure that this performance increase is not only due to the larger data set, we compare these results to training the model without transfer on a corpus consisting of each corpus together with AMMER (again, in leave-one-out crossvalidation). These results are depicted in column \u201cJoint C.\u201d. Thus, both settings, \u201ctransfer learning\u201d and \u201cjoint corpus\u201d have access to the same information.",
        "type": "Document"
      },
      {
        "id": "e0a9b03b-75a7-408b-9d74-c9f16e83f9d1",
        "metadata": {
          "vector_store_key": "1811.02906-2",
          "chunk_id": 34,
          "document_id": "1811.02906",
          "start_idx": 19100,
          "end_idx": 19242
        },
        "page_content": "We also plan to evaluate more task-agnostic approaches for transfer learning, for instance employing language modeling as a pre-training task.",
        "type": "Document"
      },
      {
        "id": "0be47b58-8b2c-4c2f-9c8b-93a75d0e1158",
        "metadata": {
          "vector_store_key": "1701.02877-9",
          "chunk_id": 85,
          "document_id": "1701.02877",
          "start_idx": 43332,
          "end_idx": 43991
        },
        "page_content": "However, transfer learning becomes more difficult if the target domain is very noisy or, as mentioned already, too different from the seed domain. For example, BIBREF59 unsuccessfully tried to adapt the CoNLL 2003 corpus to a Twitter corpus spanning several topics. They found that hand-annotating a Twitter corpus consisting of 24,000 tokens performs better on new Twitter data than their transfer learning efforts with the CoNLL 2003 corpus. The seed domain for the experiments here is newswire, where we use the classifier trained on the biggest NW corpus investigated in this study, i.e. OntoNotes NW. That classifier is then applied to all other corpora.",
        "type": "Document"
      },
      {
        "id": "3a6c0a1f-879d-4db9-a9b9-7091a14cec6f",
        "metadata": {
          "vector_store_key": "1911.03894-3",
          "chunk_id": 1,
          "document_id": "1911.03894",
          "start_idx": 766,
          "end_idx": 1601
        },
        "page_content": "These transfer learning methods exhibit clear advantages over more traditional task-specific approaches, probably the most important being that they can be trained in an unsupervised manner. They nevertheless come with implementation challenges, namely the amount of data and computational resources needed for pretraining that can reach hundreds of gigabytes of uncompressed text and require hundreds of GPUs BIBREF11, BIBREF9. The latest transformer architecture has gone uses as much as 750GB of plain text and 1024 TPU v3 for pretraining BIBREF10. This has limited the availability of these state-of-the-art models to the English language, at least in the monolingual setting. Even though multilingual models give remarkable results, they are often larger and their results still lag behind their monolingual counterparts BIBREF12.",
        "type": "Document"
      },
      {
        "id": "c7ef5b26-76ba-4a01-84f0-1d90310ab38c",
        "metadata": {
          "vector_store_key": "1804.00079-7",
          "chunk_id": 25,
          "document_id": "1804.00079",
          "start_idx": 14236,
          "end_idx": 15032
        },
        "page_content": "It is evident from Table 2 that adding more tasks improves the transfer performance of our model. Increasing the capacity our sentence encoder with more hidden units (+L) as well as an additional layer (+2L) also lead to improved transfer performance. We observe gains of 1.1-2.0% on the sentiment classification tasks (MR, CR, SUBJ & MPQA) over Infersent. We demonstrate substantial gains on TREC (6% over Infersent and roughly 2% over the CNN-LSTM), outperforming even a competitive supervised baseline. We see similar gains (2.3%) on paraphrase identification (MPRC), closing the gap on supervised approaches trained from scratch. The addition of constituency parsing improves performance on sentence relatedness (SICK-R) and entailment (SICK-E) consistent with observations made by BIBREF48 .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What baseline is used?",
    "llm_answer": "Neural baselines, i-vector system, x-vector system, Majority Class, Exact Match, First Occurrence, and a LSTM baseline.\n",
    "context": [
      {
        "id": "fa88df39-033d-4e0f-891b-162e0f9b0424",
        "metadata": {
          "vector_store_key": "1709.08299-4",
          "chunk_id": 19,
          "document_id": "1709.08299",
          "start_idx": 11042,
          "end_idx": 11836
        },
        "page_content": "The neural baselines are built by the following steps. We first use the shared training data to build a general systems, and choose the best performing model (in terms of cloze validation set) as baseline. Use User Query validation data for further tuning the systems with 10-fold cross-validations. Increase dropout rate BIBREF9 to 0.5 for preventing over-fitting issue. All baseline systems are chosen according to the performance of the validation set. The participant system results are given in Table 2 and 3 . As we can see that two neural baselines are competitive among participant systems and AoA Reader successfully outperform AS Reader and all participant systems in single model condition, which proves that it is a strong baseline system even without further fine-tuning procedure.",
        "type": "Document"
      },
      {
        "id": "763bf540-a941-47b1-bdeb-25182eab991e",
        "metadata": {
          "vector_store_key": "2001.11381-4",
          "chunk_id": 55,
          "document_id": "2001.11381",
          "start_idx": 31798,
          "end_idx": 32460
        },
        "page_content": "Tampoco consideramos la utilizaci\u00f3n de un baseline de tipo aleatorio, porque los resultados carecer\u00edan de la homosintaxis y ser\u00eda sumamente f\u00e1cil obtener mejores resultados. Dicho lo anterior, el Modelo 1 podr\u00eda ser considerado como nuestro propio baseline. A continuaci\u00f3n presentamos un protocolo de evaluaci\u00f3n manual de los resultados obtenidos. El experimento consisti\u00f3 en la generaci\u00f3n de 15 frases por cada uno de los tres modelos propuestos. Para cada modelo, se consideraron tres queries: $Q=$ {AMOR, GUERRA, SOL}, generando 5 frases con cada uno. Las 15 frases fueron mezcladas entre s\u00ed y reagrupadas por queries, antes de presentarlas a los evaluadores.",
        "type": "Document"
      },
      {
        "id": "02bef31f-a2b5-4b89-8063-62dc0b0f91eb",
        "metadata": {
          "vector_store_key": "1911.01799-4",
          "chunk_id": 19,
          "document_id": "1911.01799",
          "start_idx": 11188,
          "end_idx": 11946
        },
        "page_content": "Two state-of-the-art baseline systems were built following the Kaldi SITW recipe BIBREF22: an i-vector system BIBREF3 and an x-vector system BIBREF10. For the i-vector system, the acoustic feature involved 24-dimensional MFCCs plus the log energy, augmented by the first- and second-order derivatives. We also applied the cepstral mean normalization (CMN) and the energy-based voice active detection (VAD). The universal background model (UBM) consisted of $2,048$ Gaussian components, and the dimensionality of the i-vector space was 400. LDA was applied to reduce the dimensionality of the i-vectors to 150. The PLDA model was used for scoring BIBREF4. For the x-vector system, the feature-learning component was a 5-layer time-delay neural network (TDNN).",
        "type": "Document"
      },
      {
        "id": "39cb1ec4-6c9f-4dbd-b781-29fe9d4ad19f",
        "metadata": {
          "vector_store_key": "1912.01046-3",
          "chunk_id": 23,
          "document_id": "1912.01046",
          "start_idx": 13260,
          "end_idx": 13980
        },
        "page_content": "Following this idea, Baseline1 represents a span of sentences by concatenating the vectors of the starting and ending sentences. The left diagram in Fig. FIGREF15 illustrates the Baseline1 model. Model. The model takes two inputs, a transcript, $\\lbrace s_1, s_2, ... s_n\\rbrace $ where $s_i$ are individual sentences and a question, $q$. The output is the span scores, $y$, the scores over all possible spans. GLoVe BIBREF14 is used for the word representations in the transcript and the questions. We use two bi-LSTMs BIBREF15 to encode the transcript. where n is the number of sentences . The output of Passage-level Encoding, $p$, is a sequence of vector, $p_i$, which represents the latent meaning of each sentence.",
        "type": "Document"
      },
      {
        "id": "5e76f24e-4c75-4a34-b69c-7fe1f8d12771",
        "metadata": {
          "vector_store_key": "1909.02635-2",
          "chunk_id": 15,
          "document_id": "1909.02635",
          "start_idx": 9069,
          "end_idx": 9719
        },
        "page_content": "This includes (i) Majority Class, (ii) Exact Match of an ingredient $e$ in recipe step $s_t$, and (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo$_{token}$ and ELMo$_{sent}$) BIBREF22 compared to GPT.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What topic clusters are identified by LDA?",
    "llm_answer": "Substance use, suicidality, anxiety disorders, harassment types (verbal, physical, and visual abuse) and context (peer-to-peer, school employee or work employer, and third-parties), sexual harassment.\n",
    "context": [
      {
        "id": "ad4b6003-3b9d-4b9b-ba4a-deef5d44a9cd",
        "metadata": {
          "vector_store_key": "1809.05752-2",
          "chunk_id": 12,
          "document_id": "1809.05752",
          "start_idx": 7106,
          "end_idx": 7886
        },
        "page_content": "To perform clinical validation of the topics derived from the LDA model, they manually evaluated and annotated the topics, identifying the most informative vocabulary for the top ten topics. With their training data, they found the strongest coherence occurred in topics involving substance use, suicidality, and anxiety disorders. But given the unsupervised nature of the LDA clustering algorithm, the topic coherence they observed is not guaranteed across data sets. [2]The vast majority of patients in our target cohort are dependents on a parental private health insurance plan. Our target data set consists of a corpus of discharge summaries, admission notes, individual encounter notes, and other clinical notes from 220 patients in the OnTrackTM program at McLean Hospital.",
        "type": "Document"
      },
      {
        "id": "761240ab-0b6a-4cf5-be9d-a86671d63be8",
        "metadata": {
          "vector_store_key": "1709.07916-2",
          "chunk_id": 11,
          "document_id": "1709.07916",
          "start_idx": 6241,
          "end_idx": 6955
        },
        "page_content": "LDA assumes that a corpus contains topics such that each word in each document can be assigned to the topics with different degrees of membership BIBREF53 , BIBREF54 , BIBREF55 . Twitter users can post their opinions or share information about a subject to the public. Identifying the main topics of users' tweets provides an interesting point of reference, but conceptualizing larger subtopics of millions of tweets can reveal valuable insight to users' opinions. The topic discovery component of the study approach uses LDA to find main topics, themes, and opinions in the collected tweets. We used the Mallet implementation of LDA BIBREF49 , BIBREF56 with its default settings to explore opinions in the tweets.",
        "type": "Document"
      },
      {
        "id": "820389c1-05b7-495b-a489-f3b532e89d2c",
        "metadata": {
          "vector_store_key": "1709.07916-2",
          "chunk_id": 10,
          "document_id": "1709.07916",
          "start_idx": 5451,
          "end_idx": 6241
        },
        "page_content": "Topic modeling has a wide range of applications in health and medical domains such as predicting protein-protein relationships based on the literature knowledge BIBREF46 , discovering relevant clinical concepts and structures in patients' health records BIBREF47 , and identifying patterns of clinical events in a cohort of brain cancer patients BIBREF48 . Among topic models, Latent Dirichlet Allocation (LDA) BIBREF49 is the most popular effective model BIBREF50 , BIBREF19 as studies have shown that LDA is an effective computational linguistics model for discovering topics in a corpus BIBREF51 , BIBREF52 . LDA assumes that a corpus contains topics such that each word in each document can be assigned to the topics with different degrees of membership BIBREF53 , BIBREF54 , BIBREF55 .",
        "type": "Document"
      },
      {
        "id": "805b2cc9-0445-48cf-9105-88fc27f4b2a8",
        "metadata": {
          "vector_store_key": "1901.00439-3",
          "chunk_id": 6,
          "document_id": "1901.00439",
          "start_idx": 3722,
          "end_idx": 4496
        },
        "page_content": "Even though the objective in topic modeling is slightly different than that of pure clustering, representing each tweet as a topic vector is essentially a way of dimensionality reduction or feature extraction and can further be followed by a clustering algorithm. Proposed topic modeling methods include conventional approaches or variants of them such as Latent Dirichlet Allocation (LDA) BIBREF22 , BIBREF17 , BIBREF9 , BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26 , BIBREF27 , BIBREF19 , BIBREF28 , BIBREF29 and Non-negative Matrix Factorization (NMF) BIBREF30 , BIBREF18 . Note that topic models such as LDA are based on the notion that words belonging to a topic are more likely to appear in the same document and do not assume a distance metric between discovered topics.",
        "type": "Document"
      },
      {
        "id": "33898516-18d5-412c-a858-11ee45bf88f3",
        "metadata": {
          "vector_store_key": "2001.05970-4",
          "chunk_id": 14,
          "document_id": "2001.05970",
          "start_idx": 7605,
          "end_idx": 8446
        },
        "page_content": "We examine 6,760 tweets from the most relevant topic of our LDA model, and categorize them based on the following metrics: harassment types (verbal, physical, and visual abuse) and context (peer-to-peer, school employee or work employer, and third-parties). These labels are based on definitions by the U.S. Dept. of Education BIBREF14. In order to understand the latent topics of those #MeToo tweets for college followers, we first utilize Latent Dirichlet Allocation (LDA) to label universal topics demonstrated by the users. We determine the optimal topic number by selecting the one with the highest coherence score. Since certain words frequently appear in those #MeToo tweets (e.g., sexual harassment, men, women, story, etc.), we transform our corpus using TF-IDF, a term-weighting scheme that discounts the influence of common terms.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much do they outperform previous state-of-the-art?",
    "llm_answer": "Our model achieves improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford system, 1.41% and 1.77% over the Multigraph model, and 2.62% and 3.02% over the MIR model.\n",
    "context": [
      {
        "id": "874d1312-9fdb-46fe-865b-ce75dcb445d6",
        "metadata": {
          "vector_store_key": "1812.07023-3",
          "chunk_id": 32,
          "document_id": "1812.07023",
          "start_idx": 17666,
          "end_idx": 18428
        },
        "page_content": "We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below: Our primary architectural differences over the baseline model are: not concatenating the question, answer pairs before encoding them, the auxiliary decoder module, and using the Time-Extended FiLM module for feature extraction. These, combined with using scheduled sampling and running hyperparameter optimization over the validation set to select hyperparameters, give us the observed performance boost. We observe that our models generate fairly relevant responses to questions in the dialogues, and models with audio-visual inputs respond to audio-visual questions (e.g. \u201cis there any voices or music ?\u201d) correctly more often.",
        "type": "Document"
      },
      {
        "id": "2e7cfe41-b41c-475c-83f6-e7d39903180d",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 40,
          "document_id": "1909.13375",
          "start_idx": 21052,
          "end_idx": 21792
        },
        "page_content": "While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1. All that, while keeping in mind we compare a base model to a large model (for reference, note the 8 point improvement between MTMSNbase and MTMSNlarge in both EM and F1). Our best model, large-squad, exhibits a huge improvement of 29.7 in EM and 15.1 in F1 compared to MTMSNlarge. When comparing single-span performance, our best model exhibits slightly better results, but it should be noted that it retains the single-span heads from NABERT+, while in MTMSN they have one head to predict both single-span and multi-span answers.",
        "type": "Document"
      },
      {
        "id": "9d082380-8736-4e5a-a4c2-426632784cbe",
        "metadata": {
          "vector_store_key": "1710.00341-0",
          "chunk_id": 24,
          "document_id": "1710.00341",
          "start_idx": 12925,
          "end_idx": 13619
        },
        "page_content": "Their formulation of the task is the same as ours, but our corpora and label distributions are not the same, which makes a direct comparison impossible. Still, we can see that regarding overall classification accuracy they improve a baseline from 73.7% to 84.02% with their best model, i.e., a 39.2% relative error reduction. In our case, we go from 66.7% to 80.0%, i.e., an almost identical 39.9% error reduction. These results are very encouraging, especially given the fact that our model is much simpler than theirs regarding the sources of information used (they model the stance of the text, the reliability of the sources, the language style of the articles, and the temporal footprint).",
        "type": "Document"
      },
      {
        "id": "59630f23-3c9c-47bb-b331-4027472a3a6b",
        "metadata": {
          "vector_store_key": "1603.04553-1",
          "chunk_id": 20,
          "document_id": "1603.04553",
          "start_idx": 10753,
          "end_idx": 11450
        },
        "page_content": "Specifically, our model achieves improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford system, the winner of the CoNLL 2011 shared task, on the CoNLL 2012 development and test sets, respectively. The improvements on CoNLL F1 score over the Multigraph model are 1.41% and 1.77% on the development and test sets, respectively. Comparing with the MIR model, we obtain significant improvements of 2.62% and 3.02% on CoNLL F1 score. To make a thorough empirical comparison with previous studies, Table 3 (below the dashed line) also shows the results of some state-of-the-art supervised coreference resolution systems \u2014 IMS: the second best system in the CoNLL 2012 shared task BIBREF28 ;",
        "type": "Document"
      },
      {
        "id": "1858edb9-546d-4edc-9fc6-41fbc071abbd",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 39,
          "document_id": "1909.13375",
          "start_idx": 20678,
          "end_idx": 21407
        },
        "page_content": "The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could be found in Section (SECREF36). Notice that different BERTlarge models were used, so the comparison is less direct. Overall, our large models exhibits similar results to those of MTMSNlarge. For multi-span questions we achieve a significantly better performance. While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is the provided treebank?",
    "llm_answer": "2.4M distinct questions\n",
    "context": [
      {
        "id": "26e94eb2-4a45-4d6a-8042-3fd3dea56518",
        "metadata": {
          "vector_store_key": "1911.03894-2",
          "chunk_id": 17,
          "document_id": "1911.03894",
          "start_idx": 10098,
          "end_idx": 10932
        },
        "page_content": "We perform our work on the four freely available French UD treebanks in UD v2.2: GSD, Sequoia, Spoken, and ParTUT. GSD BIBREF30 is the second-largest treebank available for French after the FTB (described in subsection SECREF25), it contains data from blogs, news articles, reviews, and Wikipedia. The Sequoia treebank BIBREF31, BIBREF32 comprises more than 3000 sentences, from the French Europarl, the regional newspaper L\u2019Est R\u00e9publicain, the French Wikipedia and documents from the European Medicines Agency. Spoken is a corpus converted automatically from the Rhapsodie treebank BIBREF33, BIBREF34 with manual corrections. It consists of 57 sound samples of spoken French with orthographic transcription and phonetic transcription aligned with sound (word boundaries, syllables, and phonemes), syntactic and prosodic annotations.",
        "type": "Document"
      },
      {
        "id": "6e960165-bd56-4241-b0d1-4300fb0baab3",
        "metadata": {
          "vector_store_key": "1810.06743-3",
          "chunk_id": 17,
          "document_id": "1810.06743",
          "start_idx": 10094,
          "end_idx": 10940
        },
        "page_content": "The cross-lingual, cross-resource, and within-resource problems that we'll note mean that we need a tailor-made solution for each language. Showcasing their schemata, the Universal Dependencies and UniMorph projects each present large, annotated datasets. UD's v2.1 release BIBREF1 has 102 treebanks in 60 languages. The large resource, constructed by independent parties, evinces problems in the goal of a universal inventory of annotations. Annotators may choose to omit certain values (like the coerced gender of refrescante in fig:disagreement), and they may disagree on how a linguistic concept is encoded. (See, e.g., BIBREF11 's ( BIBREF11 ) description of the dative case.) Additionally, many of the treebanks were created by fully- or semi-automatic conversion from treebanks with less comprehensive annotation schemata than UD BIBREF0 .",
        "type": "Document"
      },
      {
        "id": "39e3bab1-6060-4bfe-ac91-44a07cb70a24",
        "metadata": {
          "vector_store_key": "1601.06068-2",
          "chunk_id": 10,
          "document_id": "1601.06068",
          "start_idx": 5712,
          "end_idx": 6480
        },
        "page_content": "We construct a treebank by parsing 2.4M distinct questions from Paralex using the BLLIP parser BIBREF25 . Given the treebank, we use the spectral algorithm of narayan-15 to learn an L-PCFG for constituency parsing to learn $G_{\\mathrm {syn}}$ . We follow narayan-15 and use the same feature functions for the inside and outside trees as they use, capturing contextual syntactic information about nonterminals. We refer the reader to narayan-15 for more detailed description of these features. In our experiments, we set the number of latent states to 24. Once we estimate $G_{\\mathrm {syn}}$ from the Paralex corpus, we restrict it for each question to a grammar $G_{\\mathrm {syn}}^{\\prime }$ by keeping only the rules that could lead to a derivation over the lattice.",
        "type": "Document"
      },
      {
        "id": "004f2127-b1fc-498f-9b43-31bd612f9b6c",
        "metadata": {
          "vector_store_key": "1911.03894-2",
          "chunk_id": 18,
          "document_id": "1911.03894",
          "start_idx": 10612,
          "end_idx": 11388
        },
        "page_content": "It consists of 57 sound samples of spoken French with orthographic transcription and phonetic transcription aligned with sound (word boundaries, syllables, and phonemes), syntactic and prosodic annotations. Finally, ParTUT is a conversion of a multilingual parallel treebank developed at the University of Turin, and consisting of a variety of text genres, including talks, legal texts, and Wikipedia articles, among others; ParTUT data is derived from the already-existing parallel treebank Par(allel)TUT BIBREF35 . Table TABREF23 contains a summary comparing the sizes of the treebanks. We evaluate the performance of our models using the standard UPOS accuracy for POS tagging, and Unlabeled Attachment Score (UAS) and Labeled Attachment Score (LAS) for dependency parsing.",
        "type": "Document"
      },
      {
        "id": "998ee625-a755-4697-9cb8-f011a0820ee3",
        "metadata": {
          "vector_store_key": "1604.05781-2",
          "chunk_id": 17,
          "document_id": "1604.05781",
          "start_idx": 9487,
          "end_idx": 10152
        },
        "page_content": "A cause-tree starts with a root cause word (either `caused', `causing' or `causes'), then the two most probable words following (preceding) the root are identified. Next, the root word plus one of the top probable words is combined into a bigram and the top two most probable words following (preceding) this bigram are found. Repeatedly applying this process builds a binary tree representing the $n$ -grams that begin with (terminate at) the root word. This process can continue until a certain $n$ -gram length is reached or until there are no more documents long enough to search. Sentimental analysis was applied to estimate the emotional content of documents.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what dataset was used?",
    "llm_answer": "SCI, novel-9, novel-50, IMDB62, Stat2015, Stat2016, CS2016, avsd, HEOT.\n",
    "context": [
      {
        "id": "c5f16988-f9bb-43d0-a24e-a2d52ae2d42a",
        "metadata": {
          "vector_store_key": "1802.05574-4",
          "chunk_id": 11,
          "document_id": "1802.05574",
          "start_idx": 6530,
          "end_idx": 7227
        },
        "page_content": "This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available and the corpus provides both an XML and a simple text version of each article. We randomly selected 2 sentences with more than two words from each paper using the simple text version of the paper.",
        "type": "Document"
      },
      {
        "id": "6d614157-50dc-4aaa-af03-adc7f0282a12",
        "metadata": {
          "vector_store_key": "1709.02271-3",
          "chunk_id": 20,
          "document_id": "1709.02271",
          "start_idx": 11344,
          "end_idx": 11969
        },
        "page_content": "(Section SECREF26 ). The statistics for the three datasets used in the experiments are summarized in Table TABREF16 . novel-9. This dataset was compiled by F&H14: a collection of 19 novels by 9 nineteenth century British and American authors in the Project Gutenberg. To compare to F&H14, we apply the same resampling method (F&H14, Section 4.2) to correct the imbalance in authors by oversampling the texts of less-represented authors. novel-50. This dataset extends novel-9, compiling the works of 50 randomly selected authors of the same period. For each author, we randomly select 5 novels for a total 250 novels. IMDB62.",
        "type": "Document"
      },
      {
        "id": "8e741c56-0352-4660-89e2-d995b2fada35",
        "metadata": {
          "vector_store_key": "1807.09671-7",
          "chunk_id": 34,
          "document_id": "1807.09671",
          "start_idx": 19653,
          "end_idx": 20306
        },
        "page_content": "While the 2nd and 3rd data sets are from the same course, Statistics for Industrial Engineers, they were taught in 2015 and 2016 respectively (henceforth Stat2015 and Stat2016), at the Bo\u01e7azi\u00e7i University in Turkey. The course was taught in English while the official language is Turkish. The last one is from a fundamental undergraduate Computer Science course (data structures) at a local U.S. university taught in 2016 (henceforth CS2016). Another reason we choose the student responses is that we have advanced annotation allowing us to perform an intrinsic evaluation to test whether the low-rank approximation does capture similar concepts or not.",
        "type": "Document"
      },
      {
        "id": "73a83f60-b2c9-406d-b4cd-5720639e5f06",
        "metadata": {
          "vector_store_key": "1812.07023-4",
          "chunk_id": 18,
          "document_id": "1812.07023",
          "start_idx": 10358,
          "end_idx": 10986
        },
        "page_content": "After 10 such qa turns, the questioner wraps up by writing a summary of the video based on the conversation. Dataset statistics such as the number of dialogues, turns, and words for the avsd dataset are presented in Table TABREF5 . For the initially released prototype dataset, the training set of the avsd dataset corresponds to videos taken from the training set of the Charades dataset while the validation and test sets of the avsd dataset correspond to videos taken from the validation set of the Charades dataset. For the official dataset, training, validation and test sets are drawn from the corresponding Charades sets.",
        "type": "Document"
      },
      {
        "id": "5c4b1458-497e-4f2e-a4e7-c5e739d9cad0",
        "metadata": {
          "vector_store_key": "1912.13109-1",
          "chunk_id": 9,
          "document_id": "1912.13109",
          "start_idx": 4561,
          "end_idx": 5222
        },
        "page_content": "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al. This dataset was important to employ Transfer Learning to our task since the number of labeled dataset was very small. Basic summary and examples of the data from the dataset are below: The obtained data set had many challenges and thus a data preparation task was employed to clean the data and make it ready for the deep learning pipeline.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the citation intent labels in the datasets?",
    "llm_answer": "Background, Method, ResultComparison\n",
    "context": [
      {
        "id": "932ae712-7e72-4d5d-9b2c-ab5e19d49a24",
        "metadata": {
          "vector_store_key": "1904.01608-2",
          "chunk_id": 21,
          "document_id": "1904.01608",
          "start_idx": 12179,
          "end_idx": 12924
        },
        "page_content": "We consider three intent categories outlined in Table 1 : Background, Method and ResultComparison. Below we describe data collection and annotation details. Citation intent of sentence extractions was labeled through the crowdsourcing platform Figure Eight. We selected a sample of papers from the Semantic Scholar corpus, consisting of papers in general computer science and medicine domains. Citation contexts were extracted using science-parse. The annotators were asked to identify the intent of a citation, and were directed to select among three citation intent options: Method, ResultComparison and Background. The annotation interface also included a dummy option Other which helps improve the quality of annotations of other categories.",
        "type": "Document"
      },
      {
        "id": "69a21743-0daa-4ecf-8cbd-5d987fae34da",
        "metadata": {
          "vector_store_key": "1904.01608-6",
          "chunk_id": 17,
          "document_id": "1904.01608",
          "start_idx": 9845,
          "end_idx": 10496
        },
        "page_content": "While there has been a long history of studying citation intents, there are only a few existing publicly available datasets on the task of citation intent classification. We use the most recent and comprehensive (ACL-ARC citations dataset) by BIBREF7 as a benchmark dataset to compare the performance of our model to previous work. In addition, to address the limited scope and size of this dataset, we introduce SciCite, a new dataset of citation intents that addresses multiple scientific domains and is more than five times larger than ACL-ARC. Below is a description of both datasets. ACL-ARC is a dataset of citation intents released by BIBREF7 .",
        "type": "Document"
      },
      {
        "id": "7536356f-41db-42d8-acaf-391560987a4c",
        "metadata": {
          "vector_store_key": "1904.01608-6",
          "chunk_id": 20,
          "document_id": "1904.01608",
          "start_idx": 11408,
          "end_idx": 12179
        },
        "page_content": "To address these limitations, we introduce SciCite, a new dataset of citation intents that is significantly larger, more coarse-grained and general-domain compared with existing datasets. Through examination of citation intents, we found out many of the categories defined in previous work such as motivation, extension or future work, can be considered as background information providing more context for the current research topic. More interesting intent categories are a direct use of a method or comparison of results. Therefore, our dataset provides a concise annotation scheme that is useful for navigating research topics and machine reading of scientific papers. We consider three intent categories outlined in Table 1 : Background, Method and ResultComparison.",
        "type": "Document"
      },
      {
        "id": "b39d5d9c-eb1d-4698-b54a-90f9ea076d51",
        "metadata": {
          "vector_store_key": "1904.01608-2",
          "chunk_id": 1,
          "document_id": "1904.01608",
          "start_idx": 805,
          "end_idx": 1573
        },
        "page_content": "Other applications of citation intent classification are enhanced research experience BIBREF4 , information retrieval BIBREF5 , summarization BIBREF6 , and studying evolution of scientific fields BIBREF7 . In this work, we approach the problem of citation intent classification by modeling the language expressed in the citation context. A citation context includes text spans in a citing paper describing a referenced work and has been shown to be the primary signal in intent classification BIBREF8 , BIBREF9 , BIBREF7 . Existing models for this problem are feature-based, modeling the citation context with respect to a set of predefined hand-engineered features (such as linguistic patterns or cue phrases) and ignoring other signals that could improve prediction.",
        "type": "Document"
      },
      {
        "id": "b6cdd907-9f2f-43e2-b18b-3640c3f39a33",
        "metadata": {
          "vector_store_key": "1904.01608-6",
          "chunk_id": 19,
          "document_id": "1904.01608",
          "start_idx": 10923,
          "end_idx": 11656
        },
        "page_content": "The data includes six intent categories outlined in Table 2 . Most existing datasets contain citation categories that are too fine-grained. Some of these intent categories are very rare or not useful in meta analysis of scientific publications. Since some of these fine-grained categories only cover a minimal percentage of all citations, it is difficult to use them to gain insights or draw conclusions on impacts of papers. Furthermore, these datasets are usually domain-specific and are relatively small (less than 2,000 annotated citations). To address these limitations, we introduce SciCite, a new dataset of citation intents that is significantly larger, more coarse-grained and general-domain compared with existing datasets.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is quality of annotation measured?",
    "llm_answer": "Independent annotator adjudication, comparison with AMT annotations, confidence scores, expert annotator review, and inter-annotator agreement (\u03ba).\n",
    "context": [
      {
        "id": "9157dbf7-6d4a-47a2-b243-c752ebd1231d",
        "metadata": {
          "vector_store_key": "1906.03538-3",
          "chunk_id": 16,
          "document_id": "1906.03538",
          "start_idx": 9500,
          "end_idx": 10225
        },
        "page_content": "To assess the quality of these annotations, two of the authors independently annotate a random subset of instances in the previous step (328 perspectives for 10 claims). Afterwards, the differences were adjudicated. We measure the accuracy adjudicated results with AMT annotations to estimate the quality of our annotation. This results in an accuracy of 94%, which shows high-agreement with the crowdsourced annotations. To enrich the ways the perspectives are phrased, we crowdsource paraphrases of our perspectives. We ask annotators to generate two paraphrases for each of the 15 perspectives in each HIT, for a reward of $1.50. Subsequently, we perform another round of crowdsourcing to verify the generated paraphrases.",
        "type": "Document"
      },
      {
        "id": "183e958e-d410-4270-84ae-afff74caf133",
        "metadata": {
          "vector_store_key": "1906.03538-3",
          "chunk_id": 20,
          "document_id": "1906.03538",
          "start_idx": 11768,
          "end_idx": 12467
        },
        "page_content": "In particular, we create an index of all the perspectives retained from step 2a. For a given evidence paragraph, we retrieve the top relevant perspectives. We ask the annotators to note whether a given evidence paragraph supports a given perspective or not. Each HIT contains a 20 evidence paragraphs and their top 8 relevant candidate perspectives. Each HIT is paid $1 and annotated by at least 4 independent annotators. In order to assess the quality of our annotations, a random subset of instances (4 evidence-perspective pairs) are annotated by two independent authors and the differences are adjudicated. We measure the accuracy of our adjudicated labels versus AMT labels, resulting in 87.7%.",
        "type": "Document"
      },
      {
        "id": "57077cf1-ae0e-4f3b-a5d9-8ba9078ada04",
        "metadata": {
          "vector_store_key": "1805.04033-8",
          "chunk_id": 28,
          "document_id": "1805.04033",
          "start_idx": 15536,
          "end_idx": 16162
        },
        "page_content": "We find that under the protocol, the inter-annotator agreement is quite high. In the evaluation of the test set, a pair is only annotated once to accelerate evaluation. To further maintain consistency, summaries of the same source content will not be distributed to different annotators. First, we show the results for human evaluation, which focuses on the semantic consistency of the summary with its source content. We evaluate the systems implemented by us as well as the reference. We cannot conduct human evaluations for the existing systems from other work, because the output summaries needed are not available for us.",
        "type": "Document"
      },
      {
        "id": "d0649c87-8163-45b9-96c7-88de0238b455",
        "metadata": {
          "vector_store_key": "1904.01608-4",
          "chunk_id": 23,
          "document_id": "1904.01608",
          "start_idx": 13392,
          "end_idx": 14157
        },
        "page_content": "Furthermore, crowdsource workers were required to remain on the annotation page (five annotations) for at least ten seconds before proceeding to the next page. Annotations were dynamically collected. The annotations were aggregated along with a confidence score describing the level of agreement between multiple crowdsource workers. The confidence score is the agreement on a single instance weighted by a trust score (accuracy of the annotator on the initial 50 test questions). To only collect high quality annotations, instances with confidence score of $\\le $ 0.7 were discarded. In addition, a subset of the dataset with 100 samples was re-annotated by a trained, expert annotator to check for quality, and the agreement rate with crowdsource workers was 86%.",
        "type": "Document"
      },
      {
        "id": "96e7f889-659f-4f66-af62-07610d208553",
        "metadata": {
          "vector_store_key": "2002.05058-8",
          "chunk_id": 42,
          "document_id": "2002.05058",
          "start_idx": 23391,
          "end_idx": 24040
        },
        "page_content": "To test the inner-annotator agreement score, we additionally ask them to evaluate another 40 generated samples, of which 20 samples are scored from 1 to 5 and another 20 are evaluated based on pairwise comparison with 4 other generated samples and scored to 1-5 based on how many times they are considered to be better than a reference sample. We get an inter-annotator agreement score $\\kappa =0.53$ for directly scoring and $\\kappa =0.76$ with pairwise comparison, which validates our intuition that evaluation by comparison may be more accurate. These additional human annotations are used as training data for ADEM and the comparative evaluator.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What accuracy score do they obtain?",
    "llm_answer": "The answer is not found in the context.  While accuracy is discussed and a formula is given, no specific accuracy score is reported.\n",
    "context": [
      {
        "id": "050432d2-b325-4282-ae97-1513abbfedc7",
        "metadata": {
          "vector_store_key": "2003.06279-7",
          "chunk_id": 36,
          "document_id": "2003.06279",
          "start_idx": 21531,
          "end_idx": 22144
        },
        "page_content": "While Figures FIGREF14 \u2013 FIGREF16 show the relative behavior in the accuracy, it still interesting to observe the absolute accuracy rate obtained with the classifiers. In Table TABREF17, we show the best accuracy rate (i.e. $\\max \\Gamma _+ = \\max _p \\Gamma _+(p)$) for GloVe. We also show the average difference in performance ($\\langle \\Gamma _+ - \\Gamma _0 \\rangle $) and the total number of cases in which an improvement in performance was observed ($N_+$). $N_+$ ranges in the interval $0 \\le N_+ \\le 20$. Table TABREF17 summarizes the results obtained for $w = \\lbrace 1.0, 5.0, 10.0\\rbrace $ thousand words.",
        "type": "Document"
      },
      {
        "id": "45b196bc-7d5c-48d4-bf34-df3ec42214aa",
        "metadata": {
          "vector_store_key": "1703.06492-9",
          "chunk_id": 41,
          "document_id": "1703.06492",
          "start_idx": 20539,
          "end_idx": 21214
        },
        "page_content": "The accuracy is given by the following:  $$Accuracy_{_{VQA}}=\\frac{1}{N}\\sum _{i=1}^{N}\\min \\left\\lbrace  \\frac{\\sum _{t\\in T_{i}}\\mathbb {I}[a_{i}=t]}{3},1 \\right\\rbrace $$   (Eq. 36)  , where $N$ is the total number of examples, $\\mathbb {I}[\\cdot ]$ denotes an indicator function, $a_{i}$ is the predicted answer and $T_{i}$ is an answer set of the $i^{th}$ example. That is, a predicted answer is considered as a correct one if at least 3 annotators agree with it, and the score depends on the total number of agreements when the predicted answer is not correct. Here, we describe our final results and analysis by the following parts: Does Basic Question Help Accuracy ?",
        "type": "Document"
      },
      {
        "id": "670dd0d6-8831-4c6f-ba66-53275f205e4a",
        "metadata": {
          "vector_store_key": "1804.08186-8",
          "chunk_id": 172,
          "document_id": "1804.08186",
          "start_idx": 98612,
          "end_idx": 99525
        },
        "page_content": "The system used by BIBREF172 ranked first in the German dialect identification shared task, and the system by BIBREF185 came second (71.65% accuracy) in the Arabic dialect identification shared task. In the previous two sections, we have alluded to issues of evaluation in research to date. In this section, we examine the literature more closely, providing a broad overview of the evaluation metrics that have been used, as well as the experimental settings in which research has been evaluated. The most common approach is to treat the task as a document-level classification problem. Given a set of evaluation documents, each having a known correct label from a closed set of labels (often referred to as the \u201cgold-standard\u201d), and a predicted label for each document from the same set, the document-level accuracy is the proportion of documents that are correctly labeled over the entire evaluation collection.",
        "type": "Document"
      },
      {
        "id": "8dce665d-9665-4bef-8d03-212f5e982d92",
        "metadata": {
          "vector_store_key": "1605.08675-3",
          "chunk_id": 82,
          "document_id": "1605.08675",
          "start_idx": 45319,
          "end_idx": 46017
        },
        "page_content": "Of course, the final evaluation needs to be checked manually. As mentioned in previous section, the results consist of two groups: experiments, showing an influence of some aspects of algorithm on performance, and a final assessment. Both use the Polish Wikipedia as a knowledge base, whereas the questions asked belong to development and evaluation sets, respectively. In this section, recall measures percentage of questions, to which RAFAEL gave any answer, whereas precision denotes percentage of question answered correctly. When analysing results of different entity recognition techniques, we need to remember that they strongly rely on output of the question analysis, which is not perfect.",
        "type": "Document"
      },
      {
        "id": "399b4770-7eb5-41f4-b6dc-87c168dadbcc",
        "metadata": {
          "vector_store_key": "1804.08186-8",
          "chunk_id": 173,
          "document_id": "1804.08186",
          "start_idx": 99525,
          "end_idx": 100377
        },
        "page_content": "Given a set of evaluation documents, each having a known correct label from a closed set of labels (often referred to as the \u201cgold-standard\u201d), and a predicted label for each document from the same set, the document-level accuracy is the proportion of documents that are correctly labeled over the entire evaluation collection. This is the most frequently reported metric and conveys the same information as the error rate, which is simply the proportion of documents that are incorrectly labeled (i.e. INLINEFORM0 ). Authors sometimes provide a per-language breakdown of results. There are two distinct ways in which results are generally summarized per-language: (1) precision, in which documents are grouped according to their predicted language; and (2) recall, in which documents are grouped according to what language they are actually written in.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the 12 class bilingual text?",
    "llm_answer": "The context does not contain a description of a 12-class bilingual text.\n",
    "context": [
      {
        "id": "7b4f6c87-2ae2-4b35-8edf-dc07bcbfea9d",
        "metadata": {
          "vector_store_key": "1911.13066-0",
          "chunk_id": 3,
          "document_id": "1911.13066",
          "start_idx": 1971,
          "end_idx": 2713
        },
        "page_content": "However, a particular record in the dataset belonged to one language only. In our case, a record can have either one or two languages. There is very little published work on this specific setting. One way to classify bilingual text is to normalize the different variations of a word to a standard spelling before training the model BIBREF4. However, such normalization requires external resources such as lexical database, and Roman Urdu is under-resourced in this context. Another approach for an under-resourced language is to adapt the resources from resource-rich language BIBREF5. However, such an approach is not generalizable in the case of Roman Urdu text as it is an informal language with no proper grammatical rules and dictionary.",
        "type": "Document"
      },
      {
        "id": "5b8b63a0-b52b-4e8f-83d0-5f7e23e7cbda",
        "metadata": {
          "vector_store_key": "1804.08186-3",
          "chunk_id": 267,
          "document_id": "1804.08186",
          "start_idx": 151090,
          "end_idx": 151967
        },
        "page_content": "To encourage further research on for multilingual documents, in the aforementioned shared task hosted by the Australiasian Language Technology Workshop 2010, discussed in evaluation:sharedtasks, participants were required to predict the language(s) present in a held-out test set containing monolingual and bilingual documents BIBREF378 . The dataset was prepared using data from Wikipedia, and bilingual documents were produced using a segment from an article in one language and a segment from the equivalent article in another language. Equivalence between articles was determined using the cross-language links embedded within each Wikipedia article. The winning entry BIBREF421 first built monolingual models from multilingual training data, and then applied them to a chunked version of the test data, making the final prediction a function of the prediction over chunks.",
        "type": "Document"
      },
      {
        "id": "c2f74a6e-d2af-4f94-acfc-868d117e4fdc",
        "metadata": {
          "vector_store_key": "1905.12260-0",
          "chunk_id": 9,
          "document_id": "1905.12260",
          "start_idx": 5437,
          "end_idx": 6234
        },
        "page_content": "We also provide a method for training and making predictions on multilingual word embeddings even when the language of the text is unknown. Most work on producing multilingual embeddings has relied on crosslingual human-labeled data, such as bilingual lexicons BIBREF13 , BIBREF4 , BIBREF6 , BIBREF14 or parallel/aligned corpora BIBREF15 , BIBREF4 , BIBREF16 , BIBREF17 . These works are also largely bilingual due to either limitations of methods or the requirement for data that exists only for a few language pairs. Bilingual embeddings are less desirable because they do not leverage the relevant resources of other languages. For example, in learning bilingual embeddings for English and French, it may be useful to leverage resources in Spanish, since French and Spanish are closely related.",
        "type": "Document"
      },
      {
        "id": "919d6b43-8e35-4735-9367-0e90cf26dfef",
        "metadata": {
          "vector_store_key": "1911.13066-0",
          "chunk_id": 6,
          "document_id": "1911.13066",
          "start_idx": 3758,
          "end_idx": 4428
        },
        "page_content": "We report a series of experiments using 3 kinds of embedding initialization approaches as well as the effect of attention mechanism BIBREF6. The English language is well studied under the umbrella of NLP, hence many resources and datasets for the different problems are available. However, research on English-Roman Urdu bilingual text lags behind because of non-availability of gold standard datasets. Our second contribution is that we present a large scale annotated dataset in Roman Urdu and English language with code-switching, for multi-class classification. The dataset consists of more than $0.3$ million records and has been made available for future research.",
        "type": "Document"
      },
      {
        "id": "ba76a1e0-c582-4572-afa1-ab46b0d40124",
        "metadata": {
          "vector_store_key": "2001.08051-2",
          "chunk_id": 0,
          "document_id": "2001.08051",
          "start_idx": 0,
          "end_idx": 703
        },
        "page_content": "We have acquired large sets of both written and spoken data during the implementation of campaigns aimed at assessing the proficiency, at school, of Italian pupils learning both German and English. Part of the acquired data has been included in a corpus, named \"Trentino Language Testing\" in schools (TLT-school), that will be described in the following. All the collected sentences have been annotated by human experts in terms of some predefined \u201cindicators\u201d which, in turn, were used to assign the proficiency level to each student undertaking the assigned test. This level is expressed according to the well-known Common European Framework of Reference for Languages (Council of Europe, 2001) scale.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which are the sequence model architectures this method can be transferred across?",
    "llm_answer": "Pipelined and end-to-end architectures.\n",
    "context": [
      {
        "id": "aa0d339f-d49f-4925-800d-8254d16e08e3",
        "metadata": {
          "vector_store_key": "1909.05246-0",
          "chunk_id": 2,
          "document_id": "1909.05246",
          "start_idx": 1057,
          "end_idx": 1757
        },
        "page_content": "The corpus-based method also falls into two main chatbot design architectures which are pipelined and end-to-end architectures BIBREF1. End-to-end chatbots are usually neural networks based BIBREF2, BIBREF3, BIBREF4, BIBREF5 and thus can be adapted to new domains by training on relevant dialogue datasets for that specific domain. Furthermore, all sequence modelling methods can also be used in training end-to-end task-oriented chatbots. A sequence modelling method receives a sequence as input and predicts another sequence as output. For example in the case of machine translation the input could be a sequence of words in a given language and the output would be a sentence in a second language.",
        "type": "Document"
      },
      {
        "id": "9bb77021-8b41-4836-b93f-3673e530e0c8",
        "metadata": {
          "vector_store_key": "1707.00110-3",
          "chunk_id": 0,
          "document_id": "1707.00110",
          "start_idx": 0,
          "end_idx": 699
        },
        "page_content": "Sequence-to-sequence models BIBREF0 , BIBREF1 have achieved state of the art results across a wide variety of tasks, including Neural Machine Translation (NMT) BIBREF2 , BIBREF3 , text summarization BIBREF4 , BIBREF5 , speech recognition BIBREF6 , BIBREF7 , image captioning BIBREF8 , and conversational modeling BIBREF9 , BIBREF10 . The most popular approaches are based on an encoder-decoder architecture consisting of two recurrent neural networks (RNNs) and an attention mechanism that aligns target to source tokens BIBREF2 , BIBREF11 . The typical attention mechanism used in these architectures computes a new attention context at each decoding step based on the current state of the decoder.",
        "type": "Document"
      },
      {
        "id": "f54e65ad-6fb5-4980-a5db-595a453a9d21",
        "metadata": {
          "vector_store_key": "1801.09030-0",
          "chunk_id": 4,
          "document_id": "1801.09030",
          "start_idx": 2227,
          "end_idx": 2990
        },
        "page_content": "Inspired by the great success of natural language generation tasks like neural machine translation (NMT) BIBREF0 , BIBREF1 , BIBREF2 , abstractive summarization BIBREF3 , generative question answering BIBREF4 , and neural dialogue response generation BIBREF5 , BIBREF6 , we propose to adopt the end-to-end paradigm, mainly the sequence to sequence model, to tackle the task of generating TCM prescriptions based on textual symptom descriptions. The sequence to sequence model (seq2seq) consists of an encoder that encodes the input sequence and a decoder that generates the output sequence. The success in the language generation tasks indicates that the seq2seq model can learn the semantic relation between the output sequence and the input sequence quite well.",
        "type": "Document"
      },
      {
        "id": "4d2f9f39-c062-4347-a932-306e42fb509d",
        "metadata": {
          "vector_store_key": "1907.02636-0",
          "chunk_id": 4,
          "document_id": "1907.02636",
          "start_idx": 2297,
          "end_idx": 2958
        },
        "page_content": "Recently, sequence labelling models have been utilized in many NLP tasks. Huang et al. BIBREF6 proposed using a sequence labelling model based on the bidirectional long short-term memory (LSTM) BIBREF7 for the task of named entity recognition (NER). Chiu et al. BIBREF8 and Lample et al. BIBREF9 proposed integrating LSTM encoders with character embedding and the neural sequence labelling model to achieve a remarkable performance on the task of NER as well as part-of-speech (POS) tagging. Besides, Dernoncourt et al. BIBREF10 and Jiang et al. BIBREF11 proposed applying the neural sequence labelling model to the task of de-identification of medical records.",
        "type": "Document"
      },
      {
        "id": "cd2907b7-7ab7-4b47-b1a5-f27faad4fd5c",
        "metadata": {
          "vector_store_key": "1909.00754-1",
          "chunk_id": 51,
          "document_id": "1909.00754",
          "start_idx": 26354,
          "end_idx": 27269
        },
        "page_content": "Recently, sequence generation models have been successfully applied in the realm of multi-label classification (MLC) BIBREF14 . Different from traditional binary relevance methods, they proposed a sequence generation model for MLC tasks which takes into consideration the correlations between labels. Specifically, the model follows the encoder-decoder structure with an attention mechanism BIBREF26 , where the decoder generates a sequence of labels. Similar to language modeling tasks, the decoder output at each time step will be conditioned on the previous predictions during generation. Therefore the correlation between generated labels is captured by the decoder. In this work, we proposed the Conditional Memory Relation Network (COMER), the first dialogue state tracking model that has a constant inference time complexity with respect to the number of domains, slots and values pre-defined in an ontology.",
        "type": "Document"
      }
    ]
  },
  {
    "query": " What percentage of improvement in inference speed is obtained by the proposed method over the newest state-of-the-art methods?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "cae387d5-b5b1-44de-a443-42d90d1eb1c2",
        "metadata": {
          "vector_store_key": "2003.06279-7",
          "chunk_id": 32,
          "document_id": "2003.06279",
          "start_idx": 19100,
          "end_idx": 19861
        },
        "page_content": "The relative improvement in performance is given by $\\Gamma _+{(p)}/\\Gamma _0$, where $\\Gamma _+{(p)}$ is the accuracy rate obtained when $p\\%$ additional edges are included and $\\Gamma _0 = \\Gamma _+{(p=0)}$, i.e. $\\Gamma _0$ is the accuracy rate measured from the traditional co-occurrence model. We only show the highest relative improvements in performance for each classifier. In our analysis, we considered also samples of text with distinct length, since the performance of network-based methods is sensitive to text length BIBREF34. In this figure, we considered samples comprising $w=\\lbrace 1.0, 2.5, 5.0, 10.0\\rbrace $ thousand words. The results obtained for GloVe show that the highest relative improvements in performance occur for decision trees.",
        "type": "Document"
      },
      {
        "id": "702ad706-69fa-4e55-9608-0559bddf31ab",
        "metadata": {
          "vector_store_key": "1910.08772-1",
          "chunk_id": 43,
          "document_id": "1910.08772",
          "start_idx": 24468,
          "end_idx": 25125
        },
        "page_content": "Intuitively, we would expect smaller gains since MonaLog already handles a fair amount of the entailments and contradictions, i.e., those cases where BERT profits from more examples. However the experiments show that the hybrid system reaches an even higher accuracy of 87.16%, more than 2 percent points above the baseline, equivalent to roughly 100 more problems correctly solved. Setting the high threshold for BERT to return E or C further improves accuracy to 87.49%. This brings us into the range of the state-of-the-art results, even though a direct comparison is not possible because of the differences between the corrected and uncorrected dataset.",
        "type": "Document"
      },
      {
        "id": "c5be9fa3-df46-4cc7-9785-307665e09e85",
        "metadata": {
          "vector_store_key": "1908.05969-4",
          "chunk_id": 50,
          "document_id": "1908.05969",
          "start_idx": 26802,
          "end_idx": 27516
        },
        "page_content": "From the table, we can see that our method has a much faster inference speed than Lattice-LSTM when using the LSTM-based sequence modeling layer, and it was also much faster than LR-CNN, which used an CNN architecture to implement the sequence modeling layer. And as expected, our method with the CNN-based sequence modeling layer showed some advantage in inference speed than those with the LSTM-based and Transformer-based sequence model layer. Table TABREF37$-$TABREF43 show the performance of method with the LSTM-based sequence modeling layer compared with Lattice-LSTM and other comparative baselines. Table TABREF37 shows results on OntoNotes, which has gold segmentation for both training and testing data.",
        "type": "Document"
      },
      {
        "id": "258da135-9b77-431b-bf1e-959703fd1557",
        "metadata": {
          "vector_store_key": "1910.08772-2",
          "chunk_id": 0,
          "document_id": "1910.08772",
          "start_idx": 0,
          "end_idx": 809
        },
        "page_content": "There has been rapid progress on natural language inference (NLI) in the last several years, due in large part to recent advances in neural modeling BIBREF0 and the introduction of several new large-scale inference datasets BIBREF1, BIBREF2, BIBREF3, BIBREF4. Given the high performance of current state-of-the-art models, there has also been interest in understanding the limitations of these models (given their uninterpretability) BIBREF5, BIBREF6, as well as finding systematic biases in benchmark datasets BIBREF7, BIBREF8. In parallel to these efforts, there have also been recent logic-based approaches to NLI BIBREF9, BIBREF10, BIBREF11, BIBREF12, BIBREF13, which take inspiration from linguistics. In contrast to early attempts at using logic BIBREF14, these approaches have proven to be more robust.",
        "type": "Document"
      },
      {
        "id": "be2df9a6-561c-48fc-a09e-cc532c3941fd",
        "metadata": {
          "vector_store_key": "1905.13497-3",
          "chunk_id": 17,
          "document_id": "1905.13497",
          "start_idx": 10213,
          "end_idx": 10904
        },
        "page_content": "This is a drastic improvement considering the best system based on language models outperforms random guess by only 4% in accuracy. This task is more difficult than PDP-60. First, the overall performance of all competing systems are much lower than that of PDP-60. Second, incorporating supervised learning and expensive annotated KBs to USSM provides insignificant gain this time (+3%), comparing to the large gain on PDP-60 (+19%). Finally, for the sake of completeness, BIBREF13 report that their single language model trained on a customized dataset built from CommonCrawl based on questions used in comonsense reasoning achieves an higher accuracy than the proposed approach with 62.6%.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the metric that is measures in this paper?",
    "llm_answer": "Premise-Hypothesis Distance, ROUGE-L, METEOR, $\\Delta_s$, Accuracy (ACC), Normalized Mutual Information (NMI), BLEU, Time$_{full}$, Time$_{DP}$.\n",
    "context": [
      {
        "id": "dd65ae52-1193-4705-933e-02e417dc2dc1",
        "metadata": {
          "vector_store_key": "1607.06025-0",
          "chunk_id": 49,
          "document_id": "1607.06025",
          "start_idx": 28660,
          "end_idx": 29373
        },
        "page_content": "The main purpose of the figure is not show absolute values for each of the metrics, but to compare the metrics' curves to the curve of our main metric, the accuracy of the classifier. The first metric \u2013 Premise-Hypothesis Distance \u2013 represents the average Jaccard distance between the premise and the generated hypothesis. Datasets generated with low latent dimensions have hypotheses more similar to premises, which indicates that the generated hypotheses are more trivial and less diverse than hypothesis generated with higher latent dimensions. We also evaluated the models with standard language generation metrics ROUGE-L and METEOR. The metrics are negatively correlated with the accuracy of the classifier.",
        "type": "Document"
      },
      {
        "id": "4f8d45fa-8878-4a4b-a11f-e26b8409e169",
        "metadata": {
          "vector_store_key": "2001.02380-6",
          "chunk_id": 81,
          "document_id": "2001.02380",
          "start_idx": 43195,
          "end_idx": 43942
        },
        "page_content": "We call this metric ${\\Delta }_s$ (for delta-softmax), which can be written as: where $rel$ is the true relation of the EDU pair, $t_i$ represents the token at index $i$ of $N$ tokens, and $X_{mask=i}$ represents the input sequence with the masked position $i$ (for $i \\in 1 \\ldots N$ ignoring separators, or $\\phi $, the empty set). To visualize the model's predictions, we compare ${\\Delta }_s$ for a particular token to two numbers: the maximum ${\\Delta }_s$ achieved by any token in the current pair (a measure of relative importance for the current classification) and the maximum ${\\Delta }_s$ achieved by any token in the current document (a measure of how strongly the current relation is signaled compared to other relations in the text).",
        "type": "Document"
      },
      {
        "id": "94819e86-edf4-436d-97e7-7d6df937d3a3",
        "metadata": {
          "vector_store_key": "1701.00185-9",
          "chunk_id": 45,
          "document_id": "1701.00185",
          "start_idx": 24818,
          "end_idx": 25599
        },
        "page_content": "The clustering performance is evaluated by comparing the clustering results of texts with the tags/labels provided by the text corpus. Two metrics, the accuracy (ACC) and the normalized mutual information metric (NMI), are used to measure the clustering performance BIBREF38 , BIBREF48 . Given a text INLINEFORM0 , let INLINEFORM1 and INLINEFORM2 be the obtained cluster label and the label provided by the corpus, respectively. Accuracy is defined as: DISPLAYFORM0  where, INLINEFORM0 is the total number of texts, INLINEFORM1 is the indicator function that equals one if INLINEFORM2 and equals zero otherwise, and INLINEFORM3 is the permutation mapping function that maps each cluster label INLINEFORM4 to the equivalent label from the text data by Hungarian algorithm BIBREF49 .",
        "type": "Document"
      },
      {
        "id": "e5ca78f8-d5b5-47ab-8c8e-15dbff3bfb02",
        "metadata": {
          "vector_store_key": "1701.00185-9",
          "chunk_id": 46,
          "document_id": "1701.00185",
          "start_idx": 25113,
          "end_idx": 25879
        },
        "page_content": "Accuracy is defined as: DISPLAYFORM0  where, INLINEFORM0 is the total number of texts, INLINEFORM1 is the indicator function that equals one if INLINEFORM2 and equals zero otherwise, and INLINEFORM3 is the permutation mapping function that maps each cluster label INLINEFORM4 to the equivalent label from the text data by Hungarian algorithm BIBREF49 . Normalized mutual information BIBREF50 between tag/label set INLINEFORM0 and cluster set INLINEFORM1 is a popular metric used for evaluating clustering tasks. It is defined as follows: DISPLAYFORM0  where, INLINEFORM0 is the mutual information between INLINEFORM1 and INLINEFORM2 , INLINEFORM3 is entropy and the denominator INLINEFORM4 is used for normalizing the mutual information to be in the range of [0, 1].",
        "type": "Document"
      },
      {
        "id": "0c070008-5c07-4c13-ab5e-76daec86eb90",
        "metadata": {
          "vector_store_key": "1909.09484-4",
          "chunk_id": 31,
          "document_id": "1909.09484",
          "start_idx": 16716,
          "end_idx": 17421
        },
        "page_content": "But our model actually generates each individual token of actions, and we consider a prediction to be correct only if every token of the model output matches the corresponding token in the ground truth. BLEU BIBREF19: The metric evaluates the quality of the final response generated by natural language generator. The metric is usually used to measure the performance of the task-oriented dialogue system. We also choose the following metrics to evaluate the efficiency of training the model: $\\mathbf {Time_{full}}$: The time for training the whole model, which is important for industry settings. $\\mathbf {Time_{DP}}$: The time for training the dialogue policy maker in a task-oriented dialogue system.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Was evaluation metrics and criteria were used to evaluate the output of the cascaded multimodal speech translation?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "41be2b51-b337-440f-b4ac-3f9cab9fdac1",
        "metadata": {
          "vector_store_key": "1612.06897-3",
          "chunk_id": 25,
          "document_id": "1612.06897",
          "start_idx": 13490,
          "end_idx": 14195
        },
        "page_content": "In addition to evaluating on automatic metrics, we also performed a subjective human evaluation where a human annotator assigns a score based on the quality of the translation. The judgments are done by an experienced annotator (a native speaker of German and a fluent speaker of English). We ask our annotator to judge the translation output of different systems on a randomly selected in-domain sample of 50 sentences (maximum sentence length 50). Each source sentence is presented to the annotator with all 3 different translations (baseline/ continue/ ensemble). The translation are presented in a blind fashion (i.e., the annotator is not aware of which system is which) and shuffled in random order.",
        "type": "Document"
      },
      {
        "id": "767e481e-e48d-46ed-94d9-26ebbb68b21a",
        "metadata": {
          "vector_store_key": "2004.01694-8",
          "chunk_id": 33,
          "document_id": "2004.01694",
          "start_idx": 19262,
          "end_idx": 20031
        },
        "page_content": "When assessing the second sentence out of context (sentence-level evaluation), it is hard to penalise MT$_1$ for producing 2016 Python Cultural Festival, particularly for fluency raters without access to the corresponding source text. For further examples, see Section SECREF24 and Table TABREF34. Yet another relevant element in human evaluation is the reference translation used. This is the focus of this section, where we cover two aspects of reference translations that can have an impact on evaluation: quality and directionality. Because the translations are created by humans, a number of factors could lead to compromises in quality: If the translator is a non-native speaker of the source language, they may make mistakes in interpreting the original message.",
        "type": "Document"
      },
      {
        "id": "93ec7173-102e-46b2-afe4-4f57e0ae29c9",
        "metadata": {
          "vector_store_key": "2004.01694-8",
          "chunk_id": 32,
          "document_id": "2004.01694",
          "start_idx": 18464,
          "end_idx": 19262
        },
        "page_content": "Document-level evaluation exposes errors to raters which are hard or impossible to spot in a sentence-level evaluation, such as coherent translation of named entities. The example in Table TABREF23 shows the first two sentences of a Chinese news article as translated by a professional human translator (H$_A$) and BIBREF3's BIBREF3 NMT system (MT$_1$). When looking at both sentences (document-level evaluation), it can be seen that MT$_1$ uses two different translations to refer to a cultural festival, \u201c2016\u76c2\u5170\u6587\u5316\u8282\", whereas the human translation uses only one. When assessing the second sentence out of context (sentence-level evaluation), it is hard to penalise MT$_1$ for producing 2016 Python Cultural Festival, particularly for fluency raters without access to the corresponding source text.",
        "type": "Document"
      },
      {
        "id": "7a469e92-6989-422e-aeca-3147087e2c84",
        "metadata": {
          "vector_store_key": "1912.06813-1",
          "chunk_id": 35,
          "document_id": "1912.06813",
          "start_idx": 20822,
          "end_idx": 21502
        },
        "page_content": "We leave rigorous investigation for future work. Systemwise subjective tests on naturalness and conversion similarity were also conducted to evaluate the perceptual performance. For naturalness, participants were asked to evaluate the naturalness of the speech by the mean opinion score (MOS) test on a five-point scale. For conversion similarity, each listener was presented a natural speech of the target speaker and a converted speech, and asked to judge whether they were produced by the same speaker with the confidence of the decision, i.e., sure or not sure. Ten non-native English speakers were recruited. Table TABREF14 shows the subjective results on the evaluation set.",
        "type": "Document"
      },
      {
        "id": "fcf8be2f-41ef-4628-a599-a4f691aa1c11",
        "metadata": {
          "vector_store_key": "2001.08051-3",
          "chunk_id": 15,
          "document_id": "2001.08051",
          "start_idx": 8175,
          "end_idx": 8857
        },
        "page_content": "However, a detailed linguistic evaluation cannot be performed without allowing the students to express themselves in both written sentences and spoken utterances, which typically require the intervention of human experts to be scored. Tables and report some statistics extracted from both the written and spoken data collected so far in all the campaigns. Each written or spoken item received a total score by human experts, computed by summing up the scores related to 6 indicators in 2017/2018 (from 3 to 6 in the 2016 campaign, according to the proficiency levels and the type of test). Each indicator can assume a value 0, 1, 2, corresponding to bad, medium, good, respectively.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the two decoding functions?",
    "llm_answer": "A linear projection with a trainable weight matrix and bias term, and a decoder employing a GRU with an embedding function.\n",
    "context": [
      {
        "id": "fe8a5d2c-5126-41c8-aa35-a6fe03b185e0",
        "metadata": {
          "vector_store_key": "1809.02731-3",
          "chunk_id": 14,
          "document_id": "1809.02731",
          "start_idx": 7348,
          "end_idx": 8112
        },
        "page_content": "Here, two types of decoding functions are considered and explored. In this case, the decoding function is a linear projection, which is $= f_{\\text{de}}()=+ $ , where $\\in ^{d_\\times d_}$ is a trainable weight matrix and $\\in ^{d_\\times 1}$ is the bias term. As $f_\\text{de}$ is a linear projection, the simplest situation is when $$ is an orthogonal matrix and its inverse is equal to its transpose. Often, as the dimensionality of vector $$ doesn't necessarily need to match that of word vectors $$ , $$ is not a square matrix . To enforce invertibility on $$ , a row-wise orthonormal regularisation on $$ is applied during learning, which leads to $^\\top =$ , where $$0 is the identity matrix, thus the inverse function is simply $$1 , which is easily computed.",
        "type": "Document"
      },
      {
        "id": "1a6d8dca-1a74-4499-b090-84d2cddff17d",
        "metadata": {
          "vector_store_key": "1809.02731-3",
          "chunk_id": 13,
          "document_id": "1809.02731",
          "start_idx": 6879,
          "end_idx": 7566
        },
        "page_content": "During learning, in order to reduce the computation load, only the last hidden state serves as the sentence representation $\\in ^{d_}$ , where $d_=2d$ . As the goal is to reuse the decoding function $f_{\\text{de}}()$ as another plausible encoder for building sentence representations after learning rather than ignoring it, one possible solution is to find the inverse function of the decoder function during testing, which is noted as $f^{-1}_{\\text{de}}()$ . In order to reduce the complexity and the running time during both training and testing, the decoding function $f_{\\text{de}}()$ needs to be easily invertible. Here, two types of decoding functions are considered and explored.",
        "type": "Document"
      },
      {
        "id": "f355cd26-b15a-45b1-920b-e8f70e291f49",
        "metadata": {
          "vector_store_key": "1908.02402-6",
          "chunk_id": 19,
          "document_id": "1908.02402",
          "start_idx": 10145,
          "end_idx": 10758
        },
        "page_content": "It maps the input $A_{t-1} \\circ B_{t-1} \\circ U_{t}$ (where $\\circ $ denotes concatenation) to a sequence of hidden vectors $\\lbrace h^{E}_i| i = 1, \\dots , |A_{t-1} \\circ B_{t-1} \\circ U_{t}| \\rbrace $ so that $h^{E}_i = \\text{GRU}_H(e^{A_{t-1} \\circ B_{t-1} \\circ U_{t}})$ where $e$ is the embedding function that maps from words to vectors. The output of the input encoder is its last hidden state $h^{E}_{l}$ , which is served as the initial state for the belief state and response decoders as discussed next. The belief state is composed of informable slot values $I_{t}$ and the requestable slots $R_{t}$ .",
        "type": "Document"
      },
      {
        "id": "7ef71d24-b419-415e-8022-6e8a12aa02ec",
        "metadata": {
          "vector_store_key": "1809.02731-3",
          "chunk_id": 5,
          "document_id": "1809.02731",
          "start_idx": 2743,
          "end_idx": 3442
        },
        "page_content": "We are thus motivated to explore the use of sentence decoders after learning instead of ignoring them as most sentence encoder-decoder models do. Our approach is to invert the decoding function in order to use it as another encoder to assist the original encoder. In order to make computation of the inverse function well-posed and tractable, careful design of the decoder is needed. A simple instance of an invertible decoder is a linear projection with an orthonormal square matrix, whose transpose is its inverse. A family of bijective transformations with non-linear functions BIBREF17 , BIBREF18 , BIBREF19 can also be considered as it empowers the decoder to learn a complex data distribution.",
        "type": "Document"
      },
      {
        "id": "c3788726-cf97-4295-b4d0-8cb2cf6fee57",
        "metadata": {
          "vector_store_key": "2003.07758-5",
          "chunk_id": 28,
          "document_id": "2003.07758",
          "start_idx": 15792,
          "end_idx": 16482
        },
        "page_content": "At a position $t$, the decoder inputs a set of embedded words $\\mathbf {e}^j_{\\leqslant t}$ with the output of the encoder $\\mathbf {z}^j$ and sends the output to the next layer which is conditioned on this output and, again, the encoder output $\\mathbf {z}^j$. Eventually, the decoder producing its internal representation $\\mathbf {g}_{\\leqslant t}^j \\in \\mathbb {R}^{t \\times D_T}$. The decoder block is similar to the encoder but has an additional sub-layer that applies multi-headed attention on the encoder output and the output of its previous sub-layer. The decoder employs the layer normalization and residual connections at all three sub-layers in the same fashion as the encoder.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the domains covered in the dataset?",
    "llm_answer": "The 20 domains are listed in Table TABREF10, and 17 domains (excluding the \"Alarm\" domain) are listed in Table TABREF5.  The datasets also include domains from the New York Times.\n",
    "context": [
      {
        "id": "786eaa4f-6843-428f-b802-626e6014bcbf",
        "metadata": {
          "vector_store_key": "2002.01359-4",
          "chunk_id": 22,
          "document_id": "2002.01359",
          "start_idx": 12592,
          "end_idx": 13359
        },
        "page_content": "The 20 domains present across the train, dev and test datasets are listed in Table TABREF10, as are the details regarding which domains are present in each of the datasets. We create synthetic implementations of a total of 45 services or APIs over these domains. Our simulator framework interacts with these services to generate dialogue outlines, which are structured representations of dialogue semantics. We then use a crowd-sourcing procedure to paraphrase these outlines to natural language utterances. Our novel crowd-sourcing procedure preserves all annotations obtained from the simulator and does not require any extra annotations after dialogue collection. In this section, we describe these steps briefly and then present analyses of the collected dataset.",
        "type": "Document"
      },
      {
        "id": "2e76af14-b064-4501-b27a-103fd6bbbeaf",
        "metadata": {
          "vector_store_key": "1907.11499-3",
          "chunk_id": 12,
          "document_id": "1907.11499",
          "start_idx": 6293,
          "end_idx": 7203
        },
        "page_content": "For example, the definition of the \u201cLifestyle\u201d domain is \u201cthe interests, opinions, behaviors, and behavioral orientations of an individual, group, or culture\u201d. Figure FIGREF5 provides an overview of our Domain Detection Network, which we call DetNet. The model comprises two modules; an encoder learns representations for words and sentences whilst incorporating prior domain information; a detector generates domain scores for words, sentences, and documents by selectively attending to previously encoded information. We describe the two modules in more detail below. We learn representations for words and sentences using identical encoders with separate learning parameters. Given a document, the two encoders implement the following steps: INLINEFORM0   For each sentence INLINEFORM0 , the word-level encoder yields contextualized word representations INLINEFORM1 and their attention weights INLINEFORM2 .",
        "type": "Document"
      },
      {
        "id": "8bb1d945-1431-4e60-ac4b-7aeb34105f64",
        "metadata": {
          "vector_store_key": "1909.05855-1",
          "chunk_id": 9,
          "document_id": "1909.05855",
          "start_idx": 5540,
          "end_idx": 6328
        },
        "page_content": "Our Schema-Guided Dialogue (SGD) dataset exceeds other datasets in most of the metrics at scale. The especially larger number of domains, slots, and slot values, and the presence of multiple services per domain, are representative of these scale-related challenges. Furthermore, our evaluation sets contain many services, and consequently slots, which are not present in the training set, to help evaluate model performance on unseen services. The 17 domains (`Alarm' domain not included in training) present in our dataset are listed in Table TABREF5. We create synthetic implementations of a total of 34 services or APIs over these domains. Our simulator framework interacts with these services to generate dialogue outlines, which are a structured representation of dialogue semantics.",
        "type": "Document"
      },
      {
        "id": "63424b33-ce8f-4d5e-94ed-403d702844ba",
        "metadata": {
          "vector_store_key": "1907.11499-2",
          "chunk_id": 18,
          "document_id": "1907.11499",
          "start_idx": 10010,
          "end_idx": 10650
        },
        "page_content": "For this, we created a third test set from the New York Times, in addition to our Wikipedia-based English and Chinese datasets. For all three corpora, we randomly sampled two documents for each domain, and then from each document, we sampled one long paragraph or a few consecutive short paragraphs containing 8\u201312 sentences. Amazon Mechanical Turkers were asked to read these sentences and assign a domain based on the seven labels used in this paper (multiple labels were allowed). Participants were provided with domain definitions. We obtained five annotations per sentence and adopted the majority label as the sentence's domain label.",
        "type": "Document"
      },
      {
        "id": "302516a5-a841-4cfc-b165-1aee27acef77",
        "metadata": {
          "vector_store_key": "1909.03242-2",
          "chunk_id": 13,
          "document_id": "1909.03242",
          "start_idx": 7571,
          "end_idx": 8252
        },
        "page_content": "The number of instances, as well as labels per domain, are shown in Table TABREF34 and label names in Table TABREF43 in the appendix. The dataset is split into a training part (80%) and a development and testing part (10% each) in a label-stratified manner. Note that the domains vary in the number of labels, ranging from 2 to 27. Labels include both straight-forward ratings of veracity (`correct', `incorrect'), but also labels that would be more difficult to map onto a veracity scale (e.g. `grass roots movement!', `misattributed', `not the whole story'). We therefore do not postprocess label types across domains to map them onto the same scale, and rather treat them as is.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are the two different models trained?",
    "llm_answer": "Different models are trained with different methods, including fine-tuning with self-inference, question-generation, back-translation data, and sometimes with a quality controller and meta-learning.  Some models are initialized with rule-covered training data.  Baseline models use entity embeddings and Logistic Regression.  Other models use different features and EM algorithm.\n",
    "context": [
      {
        "id": "5a72856b-91f8-460a-a634-ef0826946058",
        "metadata": {
          "vector_store_key": "1910.03891-7",
          "chunk_id": 43,
          "document_id": "1910.03891",
          "start_idx": 24539,
          "end_idx": 25258
        },
        "page_content": "The training time on both data sets is limited to at most 400 epochs. The best models are selected by a grid search and early stopping on validation sets. In entity classification, the aim is to predicate the type of entity. For all baseline models, we first get the entity embedding in different datasets through default parameter settings as in their original papers or implementations. Then, Logistic Regression is used as classifier, which regards the entity's embeddings as feature of classifier. In evaluation, we random selected 10% of training set as validation set and accuracy as evaluation metric. Experimental results of entity classification on the test sets of all the datasets is shown in Table TABREF25.",
        "type": "Document"
      },
      {
        "id": "999d2a51-e9e0-4d91-ba58-c0c2ef43fa3b",
        "metadata": {
          "vector_store_key": "2002.05829-1",
          "chunk_id": 21,
          "document_id": "2002.05829",
          "start_idx": 12431,
          "end_idx": 13078
        },
        "page_content": "For example, we choose proper batchsize and learning rate for BERTBASE to make sure the model converges and can reach expected performance as soon as possible with parameter searching. As shown in Figure FIGREF15, the fine-tuning performance curve differs a lot among pretrained models. The x-axis denoting time consumed is shown in log-scale for better comparison of different models. None of the models acutally take the lead in all tasks. However, if two pretrained models are in the same family, such as BERTBASE and BERTLARGE, the model with smaller number of parameters tend to converge a bit faster than the other in the NER and SST-2 task.",
        "type": "Document"
      },
      {
        "id": "0fd13f41-5924-488b-9ebc-5d1addc65f1c",
        "metadata": {
          "vector_store_key": "1909.05438-2",
          "chunk_id": 28,
          "document_id": "1909.05438",
          "start_idx": 15449,
          "end_idx": 16062
        },
        "page_content": "Furthermore, we could get more training data by back translation, we refer to these data as question-generation training data. For all the settings, the Base Model is initialized with rule covered training data. In Base + Self Training Method, we finetune the Base model with self-inference training data. In Base + Question Generation Method, we use question-generation training data to finetune our model. In Base + BT Method, we use both self-inference and question-generation data to finetune our model. In Base + BT + QC, we add our quality controller. In Base + BT + QC + MAML, we further add meta-learning.",
        "type": "Document"
      },
      {
        "id": "52a5c7b9-197c-4c52-9a22-93a7f3f1bb7a",
        "metadata": {
          "vector_store_key": "1910.12618-2",
          "chunk_id": 45,
          "document_id": "1910.12618",
          "start_idx": 24114,
          "end_idx": 24750
        },
        "page_content": "In order to tone it down and to increase the consistency of our results, the different models are run $B=10$ times. The results presented hereafter correspond to the average and standard-deviation on those runs. The RF model denoted as \"sel\" is the one with the reduced number of features, whereas the other RF uses the full vocabulary. We also considered an aggregated forecaster (abridged Agg), consisting of the average of the two best individual ones in terms of RMSE. All the neural network methods have a reduced vocabulary size $V^*$. The results for the French and UK data are respectively given by tables TABREF26 and TABREF27.",
        "type": "Document"
      },
      {
        "id": "13484938-48e7-4311-873d-4dac5900a3c8",
        "metadata": {
          "vector_store_key": "1603.04553-4",
          "chunk_id": 14,
          "document_id": "1603.04553",
          "start_idx": 7746,
          "end_idx": 8356
        },
        "page_content": "Specifically, as shown in Table 1 , we use different features under different resolution modes. It should be noted that only the Distance feature is designed for parameter $q$ , all other features are designed for parameter $t$ . For model learning, we run EM algorithm BIBREF19 on our Model, treating $D$ as observed data and $C$ as latent variables. We run EM with 10 iterations and select the parameters achieving the best performance on the development data. Each iteration takes around 12 hours with 10 CPUs parallelly. The best parameters appear at around the 5th iteration, according to our experiments.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How long is the dataset?",
    "llm_answer": "The answer is not found in the context.  While various durations and sizes are mentioned, there's no single \"length\" for a single dataset.\n",
    "context": [
      {
        "id": "9ca5340d-587b-4e94-bcbe-160570788673",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 35,
          "document_id": "1910.12618",
          "start_idx": 19294,
          "end_idx": 19871
        },
        "page_content": "For instance for random forests it can correspond to the maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014 and 2015 serve as validation set (730 days).",
        "type": "Document"
      },
      {
        "id": "73a83f60-b2c9-406d-b4cd-5720639e5f06",
        "metadata": {
          "vector_store_key": "1812.07023-4",
          "chunk_id": 18,
          "document_id": "1812.07023",
          "start_idx": 10358,
          "end_idx": 10986
        },
        "page_content": "After 10 such qa turns, the questioner wraps up by writing a summary of the video based on the conversation. Dataset statistics such as the number of dialogues, turns, and words for the avsd dataset are presented in Table TABREF5 . For the initially released prototype dataset, the training set of the avsd dataset corresponds to videos taken from the training set of the Charades dataset while the validation and test sets of the avsd dataset correspond to videos taken from the validation set of the Charades dataset. For the official dataset, training, validation and test sets are drawn from the corresponding Charades sets.",
        "type": "Document"
      },
      {
        "id": "f57f61de-28fb-415a-97ae-9d7e1c849440",
        "metadata": {
          "vector_store_key": "1911.13066-4",
          "chunk_id": 7,
          "document_id": "1911.13066",
          "start_idx": 4428,
          "end_idx": 5163
        },
        "page_content": "The dataset consists of more than $0.3$ million records and has been made available for future research. The rest of the paper is organized as follows. Section SECREF2 defines the dataset acquiring process and provides an explanation of the class labels. In section SECREF3, the architecture of the proposed model, its hyperparameters, and the experimental setup is discussed. We discuss the results in section SECREF4 and finally, concluding remarks are presented in section SECREF5. . The dataset consists of SMS feedbacks of the citizens of Pakistan on different public services availed by them. The objective of collecting these responses is to measure the performance of government departments rendering different public services.",
        "type": "Document"
      },
      {
        "id": "731d76b9-6365-4a8c-872e-c14badc3b626",
        "metadata": {
          "vector_store_key": "1911.07228-0",
          "chunk_id": 12,
          "document_id": "1911.07228",
          "start_idx": 6115,
          "end_idx": 6702
        },
        "page_content": "The dataset contains four different types of label: Location (LOC), Person (PER), Organization (ORG) and Miscellaneous - Name of an entity that do not belong to 3 types above (Table TABREF15). Although the corpus has more information about the POS and chunks, but we do not use them as features in our model. There are two folders with 267 text files of training data and 45 text files of test data. They all have their own format. We take 21 first text files and 22 last text files and 22 sentences of the 22th text file and 55 sentences of the 245th text file to be a development data.",
        "type": "Document"
      },
      {
        "id": "c92bd6ba-08c0-4fbc-b92b-c509a6863d91",
        "metadata": {
          "vector_store_key": "1705.03151-5",
          "chunk_id": 45,
          "document_id": "1705.03151",
          "start_idx": 24956,
          "end_idx": 25631
        },
        "page_content": "The training data sets from the seven languages are as follows: Assamese 75 hours, Bengali 87 hours, Cantonese 175 hours, Georgian 64 hours, Pashto 111 hours, Tagalog 116 hours and Turkish 107 hours. The average duration of the test utterances is $4.15$ seconds, ranging from $0.19$ seconds to $30.85$ seconds. The AP16-OL7 database was originally created by Speechocean Inc., targeted towards various speech processing tasks (mainly speech recognition), and was used as the official data for the AP16-OLR LID challenge. The database contains seven datasets, each in a particular language. These are: Mandarin, Cantonese, Indonesian, Japanese, Russian, Korean and Vietnamese.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big are negative effects of proposed techniques on high-resource tasks?",
    "llm_answer": "The context does not contain information about negative effects on high-resource tasks.\n",
    "context": [
      {
        "id": "75b380a4-5902-4e28-9219-fc5eb419c1b2",
        "metadata": {
          "vector_store_key": "1912.09152-5",
          "chunk_id": 50,
          "document_id": "1912.09152",
          "start_idx": 25993,
          "end_idx": 26590
        },
        "page_content": "But, perhaps most importantly, the other key point brought to light in this contribution is that a resource-based approach also favours a more critical stance on the dataset(s) used to evaluate system performance. With these methods, system development can go hand in hand with dataset refinement in a virtuous circle that let us think that maybe next time we are planning to add a new gazetteer or word embedding to our system in order to try to improve system performance, we should first look at our data and, like King Midas, turn our Human Annotated Dataset into a true Gold Standard Dataset.",
        "type": "Document"
      },
      {
        "id": "8df0ecc3-7ece-40b6-b9da-3315f66d1b3e",
        "metadata": {
          "vector_store_key": "1912.09152-5",
          "chunk_id": 49,
          "document_id": "1912.09152",
          "start_idx": 25916,
          "end_idx": 26702
        },
        "page_content": "With this resource-based system developed for the PharmaCoNER shared task on ner of pharmacological, chemical and biomedical entities, we have demonstrated that, having a very limited knowledge of the domain, and, thus, making wrong choices many times in the creation of resources for the tasks at hand, but being more flexible with the matching mechanisms, a simple-design system can outperform a ner tagger for biomedical entities based on state-of-the-art artificial neural network technology. Thus, knowledge-based methods stand on their own merits in task resolution. But, perhaps most importantly, the other key point brought to light in this contribution is that a resource-based approach also favours a more critical stance on the dataset(s) used to evaluate system performance.",
        "type": "Document"
      },
      {
        "id": "7b39aa2d-358d-4b5d-bfec-c23c9c635138",
        "metadata": {
          "vector_store_key": "1909.06434-1",
          "chunk_id": 21,
          "document_id": "1909.06434",
          "start_idx": 12197,
          "end_idx": 12830
        },
        "page_content": "For future work, in order to increase the utility of adaptive schedulers, it would be beneficial to explore their use on a much larger number of simultaneous tasks. In this scenario, they may prove more useful as hyper-parameter search over fixed schedules would become cumbersome. In this appendix, we present the impact of various hyper-parameters for the different schedule types. Figure FIGREF11 illustrates the effect of sampling ratios in explicit constant scheduling. We vary the sampling ratio for a task from 10% to 90% and evaluated the development and test BLEU scores by using this fixed schedule throughout the training.",
        "type": "Document"
      },
      {
        "id": "cb355127-c6ba-457f-bc45-a28180fb1025",
        "metadata": {
          "vector_store_key": "1905.11901-0",
          "chunk_id": 19,
          "document_id": "1905.11901",
          "start_idx": 10984,
          "end_idx": 11819
        },
        "page_content": "Our results show that low-resource NMT is very sensitive to hyperparameters such as BPE vocabulary size, word dropout, and others, and by following a set of best practices, we can train competitive NMT systems without relying on auxiliary resources. This has practical relevance for languages where large amounts of monolingual data, or multilingual data involving related languages, are not available. Even though we focused on only using parallel data, our results are also relevant for work on using auxiliary data to improve low-resource MT. Supervised systems serve as an important baseline to judge the effectiveness of semisupervised or unsupervised approaches, and the quality of supervised systems trained on little data can directly impact semi-supervised workflows, for instance for the back-translation of monolingual data.",
        "type": "Document"
      },
      {
        "id": "dcc07c0c-e65e-42be-9172-97d8a02d0c6d",
        "metadata": {
          "vector_store_key": "1707.06939-2",
          "chunk_id": 4,
          "document_id": "1707.06939",
          "start_idx": 2154,
          "end_idx": 2850
        },
        "page_content": "Our results indicate that the AUI treatment did not affect quality, and did not help workers perform more quickly or achieve greater response consensus. Instead, workers with the AUI were significantly slower and their responses were more diverse than workers in the non-AUI control group. An important goal of crowdsourcing research is achieving efficient scalability of the crowd to very large sets of tasks. Efficiency in crowdsourcing manifests both in receiving more effective information per worker and in making individual workers faster and/or more accurate. The former problem is a significant area of interest BIBREF5 , BIBREF6 , BIBREF7 while less work has been put towards the latter.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Are this techniques used in training multilingual models, on what languages?",
    "llm_answer": "Multilingual BERT was trained on Wikipedia data for 104 languages.  XLM was also trained for machine translation.  ELMo models were trained for Japanese, Portuguese, German, and Basque. BERT models were trained for Simplified and Traditional Chinese and German.\n",
    "context": [
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      },
      {
        "id": "c4d056ee-28bf-4bef-9918-bace1aeac08e",
        "metadata": {
          "vector_store_key": "1809.04686-3",
          "chunk_id": 3,
          "document_id": "1809.04686",
          "start_idx": 1743,
          "end_idx": 2504
        },
        "page_content": "Cross-lingual or multilingual NLP, the task of transferring knowledge from one language to another, serves as a good test bed for evaluating various transfer learning approaches. For cross-lingual NLP, the most widely studied approach is to use multilingual embeddings as features in neural network models. However, research has shown that representations learned in context are more effective BIBREF5 , BIBREF6 ; therefore, we aim at doing better than just using multilingual embeddings in the cross-lingual tasks. Recent progress in multilingual NMT provides a compelling opportunity for obtaining contextualized multilingual representations, as multilingual NMT systems are capable of generalizing to an unseen language direction, i.e. zero-shot translation.",
        "type": "Document"
      },
      {
        "id": "f40dc553-ab7a-46e3-a36b-56d977dc273d",
        "metadata": {
          "vector_store_key": "1612.06897-4",
          "chunk_id": 6,
          "document_id": "1612.06897",
          "start_idx": 3181,
          "end_idx": 3863
        },
        "page_content": "Nevertheless, integrating the in-domain data with interpolation is faster than building a system from scratch. The third approach is called semi-supervised training, where a large in-domain monolingual data is first translated with a machine translation engine into a different language to generate parallel data. The automatic translations have been used for retraining the language model and/or the translation model BIBREF15 , BIBREF16 , BIBREF17 . Parallel data can be created also by back-translating monolingual target language into the source language creating additional parallel data BIBREF18 . The additional parallel training data can be used to train the NMT and obtain.",
        "type": "Document"
      },
      {
        "id": "8307bedd-b140-434b-a3df-d91aad4eef6f",
        "metadata": {
          "vector_store_key": "1911.03894-3",
          "chunk_id": 7,
          "document_id": "1911.03894",
          "start_idx": 4498,
          "end_idx": 5183
        },
        "page_content": "Following the success of large pretrained language models, they were extended to the multilingual setting with multilingual BERT , a single multilingual model for 104 different languages trained on Wikipedia data, and later XLM BIBREF12, which greatly improved unsupervised machine translation. A few monolingual models have been released: ELMo models for Japanese, Portuguese, German and Basque and BERT for Simplified and Traditional Chinese and German. However, to the best of our knowledge, no particular effort has been made toward training models for languages other than English, at a scale similar to the latest English models (e.g. RoBERTa trained on more than 100GB of data).",
        "type": "Document"
      },
      {
        "id": "538a8227-1a08-4ac1-be43-27a174e4f9a7",
        "metadata": {
          "vector_store_key": "1911.13066-2",
          "chunk_id": 5,
          "document_id": "1911.13066",
          "start_idx": 3216,
          "end_idx": 4080
        },
        "page_content": "Our goal is to achieve this without any prior knowledge of the language, code-switching indication, language translation, normalizing lexical variations, or language transliteration. In multilingual text classification, previous approaches employ a single deep learning architecture, such as CNN or Long Short Term Memory (LSTM) for feature learning and classification. McM, on the other hand, employs three cascades (aka feature learners) to learn rich textual representations from three perspectives. These representations are then forwarded to a small discriminator network for final prediction. We compare the performance of the proposed model with existing CNN-based model for multilingual text classification BIBREF3. We report a series of experiments using 3 kinds of embedding initialization approaches as well as the effect of attention mechanism BIBREF6.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What metric is used to measure performance?",
    "llm_answer": "Accuracy (Acc), weighted F1 score (F1-w), macro F1 score (F1-m), area under the ROC curve (AUC), and equality differences (ED).\n",
    "context": [
      {
        "id": "d13a761b-92fd-4d26-99f1-f829fcfed252",
        "metadata": {
          "vector_store_key": "2002.10361-3",
          "chunk_id": 31,
          "document_id": "2002.10361",
          "start_idx": 18881,
          "end_idx": 19583
        },
        "page_content": "To measure overall performance, we evaluate models by four metrics: accuracy (Acc), weighted F1 score (F1-w), macro F1 score (F1-m) and area under the ROC curve (AUC). The F1 score coherently combines both precision and recall by $2*\\frac{precision*recall}{precision+recall}$. We report F1-m considering that the datasets are imbalanced. To evaluate group fairness, we measure the equality differences (ED) of true positive/negative and false positive/negative rates for each demographic factor. ED is a standard metric to evaluate fairness and bias of document classifiers BIBREF0, BIBREF4, BIBREF5. This metric sums the differences between the rates within specific user groups and the overall rates.",
        "type": "Document"
      },
      {
        "id": "1c2721da-ef50-4f82-92a2-9f11a92306e0",
        "metadata": {
          "vector_store_key": "2002.05058-2",
          "chunk_id": 49,
          "document_id": "2002.05058",
          "start_idx": 27267,
          "end_idx": 27919
        },
        "page_content": "As described in the human evaluation procedure, we perform 10 runs to test the reliability of each metric when used to perform hyperparameter tuning and early-stopping respectively. In each run, we select the best hyperparameter combination or early-stopping checkpoint based on each of the five compared metrics. Human evaluation is then employed to identify the best choice. We evaluate the performance of each metric by how many times (out of 10) they succeeded in selecting the best hyperparameter combination or early-stopping checkpoint (out of 4) and the average human-annotated score for their selected models. The results are shown in Table 3.",
        "type": "Document"
      },
      {
        "id": "399b4770-7eb5-41f4-b6dc-87c168dadbcc",
        "metadata": {
          "vector_store_key": "1804.08186-8",
          "chunk_id": 173,
          "document_id": "1804.08186",
          "start_idx": 99525,
          "end_idx": 100377
        },
        "page_content": "Given a set of evaluation documents, each having a known correct label from a closed set of labels (often referred to as the \u201cgold-standard\u201d), and a predicted label for each document from the same set, the document-level accuracy is the proportion of documents that are correctly labeled over the entire evaluation collection. This is the most frequently reported metric and conveys the same information as the error rate, which is simply the proportion of documents that are incorrectly labeled (i.e. INLINEFORM0 ). Authors sometimes provide a per-language breakdown of results. There are two distinct ways in which results are generally summarized per-language: (1) precision, in which documents are grouped according to their predicted language; and (2) recall, in which documents are grouped according to what language they are actually written in.",
        "type": "Document"
      },
      {
        "id": "594882ee-b778-463a-8537-679b08ae453a",
        "metadata": {
          "vector_store_key": "1911.13066-1",
          "chunk_id": 24,
          "document_id": "1911.13066",
          "start_idx": 13995,
          "end_idx": 14818
        },
        "page_content": "These metrics are accuracy, precision, recall, and F1-score, where latter three can be computed using micro-average or macro-average strategies BIBREF15. In micro-average strategy, each instance holds equal weight and outcomes are aggregated across all classes to compute a particular metric. This essentially means that the outcome would be influenced by the frequent class, if class distribution is skewed. In macro-average however, metrics for each class are calculated separately and then averaged, irrespective of their class label occurrence ratio. This gives each class equal weight instead of each instance, consequently favoring the under-represented classes. In our particular dataset, it is more plausible to favor smaller classes (i.e., other than \u201cAppreciation\" and \u201cSatisfied\") to detect potential complaints.",
        "type": "Document"
      },
      {
        "id": "d511bb7a-2f5d-4402-b99f-9a52ecba3fd8",
        "metadata": {
          "vector_store_key": "1601.02543-3",
          "chunk_id": 1,
          "document_id": "1601.02543",
          "start_idx": 628,
          "end_idx": 1339
        },
        "page_content": "Before commercial deployment of a speech solution it is imperative to have a quantitative measure of the performance of the speech solution which is primarily based on the speech recognition accuracy of the speech engine used. Generally, the recognition performance of any speech recognition based solution is quantitatively evaluated by putting it to actual use by the people who are the intended users and then analyzing the logs to identify successful and unsuccessful transactions. This evaluation is then used to identifying any further improvement in the speech recognition based solution to better the overall transaction completion rates. This process of evaluation is both time consuming and expensive.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do Zipf and Herdan-Heap's laws differ?",
    "llm_answer": "The provided text states that Heaps-Herdan's law describes how vocabulary grows with text size (a power law relationship), while Zipf's law describes the rank-frequency distribution of words in a text (also a power law).  The text also suggests Heaps-Herdan's law might be a consequence of Zipf's law.\n",
    "context": [
      {
        "id": "adc36c7f-8a32-45d6-b1ad-6107a1f0461f",
        "metadata": {
          "vector_store_key": "1911.08915-3",
          "chunk_id": 1,
          "document_id": "1911.08915",
          "start_idx": 946,
          "end_idx": 1578
        },
        "page_content": "Indeed, a large amount of effort has been devoted to try to understand the origin of Zipf's law, in some cases arguing that it arises from the fact that texts carry information BIBREF15, all the way to arguing that it is the result of mere chance BIBREF16, BIBREF17. Another interesting characterization of texts is the Heaps-Herdan law, which describes how the vocabulary -that is, the set of different words- grows with the size of a text, the number of which, empirically, has been found to grow as a power of the text size BIBREF18, BIBREF19. It is worth noting that it has been argued that this law is a consequence Zipf's law.",
        "type": "Document"
      },
      {
        "id": "8514731a-0222-476f-95f8-2455a2a95795",
        "metadata": {
          "vector_store_key": "1911.08915-3",
          "chunk_id": 33,
          "document_id": "1911.08915",
          "start_idx": 16859,
          "end_idx": 17468
        },
        "page_content": "Further, Heaps-Herdan law and the degree distribution of the adjacency network, appear to be consequences of Zipf's law, and are, thus, as universal. In this work we studied 91 texts in seven different languages, as well as random texts constructed by randomizing the spacings between words without altering the order of the letters in the text. We find that they are all well described by the universal laws. However, we also found that the distribution of clustering coefficients of the networks of each text appears to vary from one language to another, and to distinguish random texts from real languages.",
        "type": "Document"
      },
      {
        "id": "93a6765f-d146-4491-9dc8-0c7d7fcc7771",
        "metadata": {
          "vector_store_key": "1911.08915-3",
          "chunk_id": 32,
          "document_id": "1911.08915",
          "start_idx": 16170,
          "end_idx": 16859
        },
        "page_content": "Note that for example the values are clearly different for Spanish and Turkish, similar for Spanish and French, and very different for all languages and random. Zipf's law is one of the most universal statistics of natural languages. However, it may be too universal. While it may not strictly apply to sequences of independent random symbols with random spacings BIBREF32, it appears to describe random texts that conserve most of the correlations between successive symbols, as accurately as it describes texts written in real languages. Further, Heaps-Herdan law and the degree distribution of the adjacency network, appear to be consequences of Zipf's law, and are, thus, as universal.",
        "type": "Document"
      },
      {
        "id": "f676de56-c9a1-4db6-b5d9-f615db30f0e0",
        "metadata": {
          "vector_store_key": "1911.08915-3",
          "chunk_id": 13,
          "document_id": "1911.08915",
          "start_idx": 7188,
          "end_idx": 7933
        },
        "page_content": "We also note that in all cases, hapax legomena represent approximately half of the vocabulary in each text. In figure (FIGREF2) we show Zipf plots for some of the texts, including the random texts constructed as described previously. It is clear that all the texts reproduce convincingly Zipf's law: $f(n)\\sim 1/n^\\alpha $ where $n=1,2,...N_{tot}$ is the word rank, $N_{tot}$ is the size of the vocabulary and $f(n)$ is its frequency. This is in contrast to previous work in which it is argued that there are differences between the Zipf plots of texts and random sequencesBIBREF32, this might be due to the fact that our random text construction preserves correlations between letters, whereas the letters in BIBREF32 were placed independently.",
        "type": "Document"
      },
      {
        "id": "cd514c1b-e212-4d41-a114-5e29100fa8ae",
        "metadata": {
          "vector_store_key": "1911.08915-3",
          "chunk_id": 0,
          "document_id": "1911.08915",
          "start_idx": 0,
          "end_idx": 946
        },
        "page_content": "Statistical characterization of languages has been a field of study for decadesBIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5. Even simple quantities, like letter frequency, can be used to decode simple substitution cryptogramsBIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10. However, probably the most surprising result in the field is Zipf's law, which states that if one ranks words by their frequency in a large text, the resulting rank frequency distribution is approximately a power law, for all languages BIBREF0, BIBREF11. These kind of universal results have long piqued the interest of physicists and mathematicians, as well as linguistsBIBREF12, BIBREF13, BIBREF14. Indeed, a large amount of effort has been devoted to try to understand the origin of Zipf's law, in some cases arguing that it arises from the fact that texts carry information BIBREF15, all the way to arguing that it is the result of mere chance BIBREF16, BIBREF17.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are the synthetic examples generated?",
    "llm_answer": "The premises and labels from the original dataset are used as input for the generative model.  The model then generates new hypotheses.\n",
    "context": [
      {
        "id": "84104c9f-f895-46f2-b0a6-d3f3451ac006",
        "metadata": {
          "vector_store_key": "1906.00180-3",
          "chunk_id": 3,
          "document_id": "1906.00180",
          "start_idx": 1879,
          "end_idx": 2547
        },
        "page_content": "First, we develop a protocol for automatically generating data that can be used in entailment recognition tasks. Second, we demonstrate that several deep learning architectures succeed at one such task. Third, we present and apply a number of experiments to test whether models are capable of compositional generalization. The data generation process is inspired by BIBREF13 : an artificial language is defined, sentences are generated according to its grammar and the entailment relation between pairs of such sentences is established according to a fixed background logic. However, our language is significantly more complex, and instead of natural logic we use FOL.",
        "type": "Document"
      },
      {
        "id": "fffe3f80-9435-409d-bad1-e4512d1dea31",
        "metadata": {
          "vector_store_key": "1910.09399-2",
          "chunk_id": 5,
          "document_id": "1910.09399",
          "start_idx": 3128,
          "end_idx": 4145
        },
        "page_content": "Most of the contributions from these papers rely on multimodal learning approaches that include generative adversarial networks and deep convolutional decoder networks as their main drivers to generate entrancing images from text BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11. First introduced by Ian Goodfellow et al. BIBREF9, generative adversarial networks (GANs) consist of two neural networks paired with a discriminator and a generator. These two models compete with one another, with the generator attempting to produce synthetic/fake samples that will fool the discriminator and the discriminator attempting to differentiate between real (genuine) and synthetic samples. Because GANs' adversarial training aims to cause generators to produce images similar to the real (training) images, GANs can naturally be used to generate synthetic images (image synthesis), and this process can even be customized further by using text descriptions to specify the types of images to generate, as shown in Figure FIGREF6.",
        "type": "Document"
      },
      {
        "id": "9cdc1117-68ba-4240-862d-a0a60a32d5ab",
        "metadata": {
          "vector_store_key": "1607.06025-8",
          "chunk_id": 2,
          "document_id": "1607.06025",
          "start_idx": 1254,
          "end_idx": 1984
        },
        "page_content": "The proposed generative networks are trained to generate a hypothesis given a premise and a label, which allow us to construct new, unseen examples. Some generative models are build to generate a single optimal response given the input. Such models have been applied to machine translation BIBREF5 , image caption generation BIBREF6 , or dialogue systems BIBREF7 . Another type of generative models are autoencoders that generate a stream of random samples from the original distribution. For instance, autoencoders have been used to generate text BIBREF8 , BIBREF9 , and images BIBREF10 . In our setting we combine both approaches to generate a stream of random responses (hypotheses) that comply with the input (premise, label).",
        "type": "Document"
      },
      {
        "id": "277b6532-3b00-493f-81d4-bf4bd8453d7b",
        "metadata": {
          "vector_store_key": "1607.06025-2",
          "chunk_id": 39,
          "document_id": "1607.06025",
          "start_idx": 22867,
          "end_idx": 23549
        },
        "page_content": "The premises and labels from the examples of the original dataset are taken as an input for the generative model. The new hypotheses replace the training hypotheses in the new dataset. Next, the classifier, presented in Section SECREF6 , is trained on the generated dataset. The accuracy of the new classifier is the main metric for evaluating the quality of the generated dataset. All the experiments are performed on the SNLI dataset. There are 549,367 examples in the dataset, divided into training, development and test set. Both the development and test set contain around 10.000 examples. Some examples are labeled with '-', which means there was not enough consensus on them.",
        "type": "Document"
      },
      {
        "id": "d69c261a-457d-48c6-9280-d01442f445f9",
        "metadata": {
          "vector_store_key": "1607.06025-2",
          "chunk_id": 7,
          "document_id": "1607.06025",
          "start_idx": 4257,
          "end_idx": 4858
        },
        "page_content": "The generated hypothesis is combined with the premise and the label to form a new unseen example. This is done for every example in the original dataset to construct a new dataset. Next, a classifier is trained on the new dataset. Finally, the classifier is evaluated on the original test set. The accuracy of the classifier is the proposed quality metric for the generative model. It can be compared to the accuracy of the classifier trained on the original training set and tested on the original test set. The generative models learn solely from the original training set to regenerate the dataset.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does the new parser outperform the current state-of-the-art?",
    "llm_answer": "The context states the new parser outperforms several other models, but does not give a single overall comparison against the current absolute state-of-the-art.\n",
    "context": [
      {
        "id": "e19039a4-35bf-4ee7-b491-67f2e3136dd1",
        "metadata": {
          "vector_store_key": "1911.08673-1",
          "chunk_id": 44,
          "document_id": "1911.08673",
          "start_idx": 23664,
          "end_idx": 24082
        },
        "page_content": "Compared to graph-based and transition-based parsers, our parser achieves a better tradeoff between parsing accuracy and efficiency by taking advantages of both graph-based models' training methods and transition-based models' linear time decoding strategies. Experimental results on 28 treebanks show the effectiveness of our parser by achieving good performance on 27 treebanks, including the PTB and CTB benchmarks.",
        "type": "Document"
      },
      {
        "id": "c34f7da4-1a15-4199-9ef4-c98fc6357b88",
        "metadata": {
          "vector_store_key": "1808.09409-1",
          "chunk_id": 25,
          "document_id": "1808.09409",
          "start_idx": 14324,
          "end_idx": 15062
        },
        "page_content": "Furthermore, the neural-parser-based system achieves the best overall performance on the L2 data. Though performing slightly worse than the neural syntax-agnostic one on the L1 data, it has much smaller INLINEFORM0 F, showing that as the syntactic analysis improves, the performances on both the L1 and L2 data grow, while the gap can be maintained. This demonstrates again the importance of syntax in semantic constructions, especially for learner texts. Table TABREF45 summarizes the SRL results of the baseline PCFGLA-parser-based model as well as its corresponding retrained models. Since both the syntactic parser and the SRL classifier can be retrained and thus enhanced, we report the individual impact as well as the combined one.",
        "type": "Document"
      },
      {
        "id": "d18ededb-8d4f-4a2f-8b0c-90bd0ec9b33f",
        "metadata": {
          "vector_store_key": "1911.08673-1",
          "chunk_id": 5,
          "document_id": "1911.08673",
          "start_idx": 3406,
          "end_idx": 4382
        },
        "page_content": "Just as with one-shot scoring in graph-based parsers, our proposed parser will perform arc-attachment scoring, parsing order scoring, and decoding simultaneously in an incremental, deterministic fashion just as transition-based parsers do. We evaluated our models on the common benchmark treebanks PTB and CTB, as well as on the multilingual CoNLL and the Universal Dependency treebanks. From the evaluation results on the benchmark treebanks, our proposed model gives significant improvements when compared to the baseline parser. In summary, our contributions are thus: $\\bullet $ We integrate the arc scoring mechanism of graph-based parsers and the linear time complexity inference approach of transition parsing models, which, by replacing stepwise local feature scoring, significantly alleviates the drawbacks of these models, improving their moderate performance caused by error propagation and increasing their training speeds resulting from their lack of parallelism.",
        "type": "Document"
      },
      {
        "id": "418f3800-222a-42ce-b5e5-9ec3333484ef",
        "metadata": {
          "vector_store_key": "1804.05868-3",
          "chunk_id": 44,
          "document_id": "1804.05868",
          "start_idx": 25228,
          "end_idx": 25740
        },
        "page_content": "The parser uses neural stacking architecture of zhang-weiss:2016:P16-1 and chen-zhang-liu:2016:EMNLP2016 for learning POS tagging and parsing and for knowledge transfer from Bilingual models trained on Hindi and English UD treebanks. We have also presented normalization and back-transliteration models with a decoding process tailored for CS data. Our neural stacking parser is 1.5% LAS points better than the augmented parsing model and 3.8% LAS points better than the one which uses first-best normalizations.",
        "type": "Document"
      },
      {
        "id": "55919d20-280e-44e2-b1b8-2e473ea151e4",
        "metadata": {
          "vector_store_key": "1710.09340-3",
          "chunk_id": 18,
          "document_id": "1710.09340",
          "start_idx": 9575,
          "end_idx": 10295
        },
        "page_content": "Apart from error exploration, this also opens the way to incorporating non-monotonicity BIBREF9 . While these approaches have shown to improve accuracy under online training settings, here we prioritize homogeneous comparability to BIBREF2 , so we use batch training and a static oracle, and still obtain state-of-the-art accuracy for a greedy parser. Table TABREF10 presents a comparison between the Covington parser and the novel variant developed here. The NL-Covington parser outperforms the original version in all datasets tested, with all improvements statistically significant ( INLINEFORM0 ). Table TABREF12 compares our novel system with other state-of-the-art transition-based dependency parsers on the PT-SD.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What experimental evaluation is used?",
    "llm_answer": "Human evaluation on QA4IE benchmark, using Triple Accuracy and Contextual Consistency metrics.  10 runs for hyperparameter tuning/early-stopping, using 5 compared metrics, with human evaluation to identify the best choice.\n",
    "context": [
      {
        "id": "b8ca75dd-84d9-4514-9a0b-ed8771d04dfc",
        "metadata": {
          "vector_store_key": "1605.08675-1",
          "chunk_id": 90,
          "document_id": "1605.08675",
          "start_idx": 49692,
          "end_idx": 50451
        },
        "page_content": "However, because of the small difference between the techniques including title, for the sake of simplicity, the single sentence is used in the final evaluation. To impose a realistic challenge to the system, the evaluation set, used at this stage, substantially differs from the one used during the development (see section SECREF80 ). A configuration for the final evaluation has been prepared based on results of the experiments. All of the tested versions share the following features: no question analysis corrections, question classification and query generation solutions which proved best in the previous experiments (see section SECREF18 ), a retrieved set of documents including 20 articles, no minimal confidence, singe sentence context with title.",
        "type": "Document"
      },
      {
        "id": "1aa58b63-7d45-4c30-8497-358be6b0b602",
        "metadata": {
          "vector_store_key": "1804.03396-5",
          "chunk_id": 56,
          "document_id": "1804.03396",
          "start_idx": 29719,
          "end_idx": 30662
        },
        "page_content": "Note that our framework takes the whole document as the input while the baseline systems take the individual sentence as the input, which means the experiment setting is much more difficult for our framework. Finally, we perform a human evaluation on our QA4IE benchmark to verify the reliability of former experiments. The evaluation metrics are as follows: Triple Accuracy is to check whether each ground truth triple is accurate (one cannot find conflicts between the ground truth triple and the corresponding article) because the ground truth triples from WikiData and DBpedia may be incorrect or incomplete. Contextual Consistency is to check whether the context of each answer location is consistent with the corresponding ground truth triple (one can infer from the context to obtain the ground truth triple) because we keep all matched answer locations as ground truths but some of them may be irrelevant with the corresponding triple.",
        "type": "Document"
      },
      {
        "id": "1c2721da-ef50-4f82-92a2-9f11a92306e0",
        "metadata": {
          "vector_store_key": "2002.05058-2",
          "chunk_id": 49,
          "document_id": "2002.05058",
          "start_idx": 27267,
          "end_idx": 27919
        },
        "page_content": "As described in the human evaluation procedure, we perform 10 runs to test the reliability of each metric when used to perform hyperparameter tuning and early-stopping respectively. In each run, we select the best hyperparameter combination or early-stopping checkpoint based on each of the five compared metrics. Human evaluation is then employed to identify the best choice. We evaluate the performance of each metric by how many times (out of 10) they succeeded in selecting the best hyperparameter combination or early-stopping checkpoint (out of 4) and the average human-annotated score for their selected models. The results are shown in Table 3.",
        "type": "Document"
      },
      {
        "id": "74f6d11f-dad7-42be-9e7a-785347faecaf",
        "metadata": {
          "vector_store_key": "1906.09774-1",
          "chunk_id": 34,
          "document_id": "1906.09774",
          "start_idx": 20144,
          "end_idx": 20930
        },
        "page_content": "In addition, we also investigate the available affective resources which usually use in the emotion classification task. In this part, we only focus on English resources and found several resources from the old one such as LIWC and Emolex to the new one, including DepecheMood and EmoWordNet. In the final part, we gather information about how to evaluate the performance of EAC, and we can classify the approach into two techniques, including qualitative and quantitative assessment. For qualitative assessment, most studies used ISO 9241, which covers several aspects such as efficiency, effectiveness, and satisfaction. While in quantitative analysis, two techniques can be used, including automatic evaluation (by using perplexity) and manual evaluation (involving human judgement).",
        "type": "Document"
      },
      {
        "id": "4be44773-b407-4314-abd0-d9c32e99174b",
        "metadata": {
          "vector_store_key": "1908.07816-1",
          "chunk_id": 7,
          "document_id": "1908.07816",
          "start_idx": 4025,
          "end_idx": 4656
        },
        "page_content": "We consider factors such as the balance of positive and negative sentiments in test dialogs, a well-chosen range of topics, and dialogs that our human evaluators can relate. It is the first time such an approach is designed with consideration for the human judges. Our main goal is to increase the objectivity of the results and reduce judges' mistakes due to out-of-context dialogs they have to evaluate. The rest of the paper unfolds as follows. Section SECREF2 discusses some related work. In Section SECREF3, we give detailed description of the methodology. We present experimental results and some analysis in Section SECREF4.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the architecture fault-tolerant?",
    "llm_answer": "The architecture is fault-tolerant because Spark RDDs store execution steps in a lineage graph, allowing recovery from faults like memory overload or system crashes.  Duplicate copies of RDDs in Apache Hive and the stored model state allow recovery to the previous steady state.\n",
    "context": [
      {
        "id": "43c23897-6e22-4c43-ad87-413904fda6f3",
        "metadata": {
          "vector_store_key": "2003.04967-8",
          "chunk_id": 24,
          "document_id": "2003.04967",
          "start_idx": 13308,
          "end_idx": 13946
        },
        "page_content": "An important property of this architecture is the persistence of the data and the model. The machine learning model persists itself by storing its weights to disk and loading from it while retraining or reinforcing itself to learn from mistakes. The tweets and cryptocurrency training data is also stored in Apache Hive which provides data warehousing support to read, write and manage distributed datasets directly from disk. This persistence technique helps the whole platform to reset itself without omissions in real time. Spark RDD has the innate capability to recover itself because it stores all execution steps in a lineage graph.",
        "type": "Document"
      },
      {
        "id": "6e93c216-dc98-4ccb-a7c9-6a127606c66d",
        "metadata": {
          "vector_store_key": "2003.04967-8",
          "chunk_id": 21,
          "document_id": "2003.04967",
          "start_idx": 11936,
          "end_idx": 12516
        },
        "page_content": "The growth of the volume of data inspired us to opt for a big data architecture which can not only handle the prediction algorithms but also the streaming and increasing volume of data in a fault tolerant way. Figure FIGREF2 gives an overview of the architecture design. Central to this design is Apache Spark which acts as an in-memory data store and allows us to perform computations in a scalable manner. This data is the input to our machine learning model for making predictions. To bootstrap our model, we first gather a few days of data and store that in Apache Spark RDDs.",
        "type": "Document"
      },
      {
        "id": "335c1ae9-c3bb-4a93-b2fe-597e5ee8f908",
        "metadata": {
          "vector_store_key": "2001.02380-2",
          "chunk_id": 12,
          "document_id": "2001.02380",
          "start_idx": 6659,
          "end_idx": 7541
        },
        "page_content": "In Section SECREF5, we combine the anchoring annotation data from Section SECREF3 with the model's predictions to evaluate how `human-like' its performance is, using an information retrieval approach measuring recall@k and assessing the stability of different signal types based on how the model scores them. We develop a visualization for tokenwise signaling strength and perform error analysis for some signals found by the model which were not flagged by humans and vice versa, and point out the strengths and weaknesses of the architecture. Section SECREF6 offers further discussion of what we can learn from the model, what kinds of additional features it might benefit from given the error analysis, and what the distributions of scores for individual signals can teach us about the ambiguity and reliability of different signal types, opening up avenues for further research.",
        "type": "Document"
      },
      {
        "id": "34f66d81-8034-4939-b12d-14056fa88e48",
        "metadata": {
          "vector_store_key": "2003.04967-4",
          "chunk_id": 25,
          "document_id": "2003.04967",
          "start_idx": 13946,
          "end_idx": 14638
        },
        "page_content": "Spark RDD has the innate capability to recover itself because it stores all execution steps in a lineage graph. In case of any faults in the system, Spark redoes all the previous executions from the built DAG and recovers itself to the previous steady state from any fault such as memory overload. Spark RDDs lie in the core of KryptoOracle and therefore make it easier for it to recover from faults. Moreover, faults like memory overload or system crashes may require for the whole system to hard reboot. However, due to the duplicate copies of the RDDs in Apache Hive and the stored previous state of the machine learning model, KryptoOracle can easily recover to the previous steady state.",
        "type": "Document"
      },
      {
        "id": "68c58226-9ede-4846-b3b2-f521360f1a31",
        "metadata": {
          "vector_store_key": "1905.13413-0",
          "chunk_id": 23,
          "document_id": "1905.13413",
          "start_idx": 13630,
          "end_idx": 13801
        },
        "page_content": "An error analysis is performed to shed light on possible future directions. This work was supported in part by gifts from Bosch Research, and the Carnegie Bosch Institute.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which elements of the platform are modular?",
    "llm_answer": "The dialog system, the platform for bitcoin price prediction, Katecheo, and Torch-Struct are modular.\n",
    "context": [
      {
        "id": "3c9f0541-d731-4e38-a036-27f7bc4e9bab",
        "metadata": {
          "vector_store_key": "1707.05853-1",
          "chunk_id": 1,
          "document_id": "1707.05853",
          "start_idx": 660,
          "end_idx": 1455
        },
        "page_content": "Task-oriented dialog systems are often implemented in a modular architecture to break up the complex task of conducting dialogs into more manageable subtasks. BIBREF6 describe the following prototypical set-up of such a modular architecture: First, an ASR system converts the spoken user utterance into text. Then, a spoken language understanding (SLU) module extracts the user's intent and coarse-grained semantic information. Next, a dialog state tracking (DST) component maintains a distribution over the state of the dialog, updating it in every turn. Given this information, the dialog policy manager decides on the next action of the system. Finally, a natural language generation (NLG) module forms the system reply that is converted into an audio signal via a text-to-speech synthesizer.",
        "type": "Document"
      },
      {
        "id": "d13b431e-0a4c-4f4c-9c90-4edc90cd7661",
        "metadata": {
          "vector_store_key": "2003.04967-1",
          "chunk_id": 5,
          "document_id": "2003.04967",
          "start_idx": 2916,
          "end_idx": 3670
        },
        "page_content": "Secondly, the proposed platform offers an approach that supports sentiment analysis based on VADER which can respond to large amounts of natural language processing queries in real time. Thirdly, the platform supports a predictive approach based on online learning in which a machine learning model adapts its weights to cope with new prices and sentiments. Finally, the platform is modular and integrative in the sense that it combines these different solutions to provide novel real-time tool support for bitcoin price prediction that is more scalable, data-rich, and proactive, and can help accelerate decision-making, uncover new opportunities and provide more timely insights based on the available and ever-larger financial data volume and variety.",
        "type": "Document"
      },
      {
        "id": "ab3d5731-d49b-4f7f-91d6-2ee3fd634667",
        "metadata": {
          "vector_store_key": "1907.00854-0",
          "chunk_id": 28,
          "document_id": "1907.00854",
          "start_idx": 14623,
          "end_idx": 15433
        },
        "page_content": "It is portable because it is built on cloud native technologies (i.e., Docker and Kubernetes) and can be deployed to any cloud or on-premise environment. It is modular because it is composed of four configurable modules that collectively enable identification of questions, classification of those questions into topics, a search of knowledge base articles, and reading comprehension. Initial usage of the system indicates that it provides a flexible and developer friendly way to enable question answering functionality for multiple topics or domains via REST API. That being said, the current configurations of Katecheo are limited to answering from knowledge bases constructed in a question and answer format, and the current topic classification relies on topical NER models that are compatible with spaCy.",
        "type": "Document"
      },
      {
        "id": "97ee039d-d1d1-4ff6-ae82-c47313626ec6",
        "metadata": {
          "vector_store_key": "2002.00876-1",
          "chunk_id": 3,
          "document_id": "2002.00876",
          "start_idx": 1999,
          "end_idx": 2779
        },
        "page_content": "With this challenge in mind, we introduce Torch-Struct with three specific contributions: Modularity: models are represented as distributions with a standard flexible API integrated into a deep learning framework. Completeness: a broad array of classical algorithms are implemented and new models can easily be added in Python. Efficiency: implementations target computational/memory efficiency for GPUs and the backend includes extensions for optimization. In this system description, we first motivate the approach taken by the library, then present a technical description of the methods used, and finally present several example use cases. Several software libraries target structured prediction. Optimization tools, such as SVM-struct BIBREF18, focus on parameter estimation.",
        "type": "Document"
      },
      {
        "id": "3cf1728c-0711-4400-a389-d0f3ab12da96",
        "metadata": {
          "vector_store_key": "1909.08859-0",
          "chunk_id": 8,
          "document_id": "1909.08859",
          "start_idx": 4401,
          "end_idx": 5098
        },
        "page_content": "Its architecture is based on a bi-directional attention flow (BiDAF) model BIBREF6, but also equipped with an explicit reasoning module that acts on entity-specific relational memory units. Fig. FIGREF4 shows an overview of the network architecture. It consists of five main modules: An input module, an attention module, a reasoning module, a modeling module, and an output module. Note that the question answering tasks we consider here are multimodal in that while the context is a procedural text, the question and the multiple choice answers are composed of images. Input Module extracts vector representations of inputs at different levels of granularity by using several different encoders.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Could you tell me more about the metrics used for performance evaluation?",
    "llm_answer": "The metrics used for performance evaluation include: accuracy (Acc), weighted F1 score (F1-w), macro F1 score (F1-m), area under the ROC curve (AUC), equality differences (ED) of true positive/negative and false positive/negative rates, BLEU, perplexity, ROUGE, METEOR, BPRA (Belief Per-Response Accuracy).  The F1 score combines precision and recall.  There are also task-based evaluations and manual evaluations.\n",
    "context": [
      {
        "id": "1c2721da-ef50-4f82-92a2-9f11a92306e0",
        "metadata": {
          "vector_store_key": "2002.05058-2",
          "chunk_id": 49,
          "document_id": "2002.05058",
          "start_idx": 27267,
          "end_idx": 27919
        },
        "page_content": "As described in the human evaluation procedure, we perform 10 runs to test the reliability of each metric when used to perform hyperparameter tuning and early-stopping respectively. In each run, we select the best hyperparameter combination or early-stopping checkpoint based on each of the five compared metrics. Human evaluation is then employed to identify the best choice. We evaluate the performance of each metric by how many times (out of 10) they succeeded in selecting the best hyperparameter combination or early-stopping checkpoint (out of 4) and the average human-annotated score for their selected models. The results are shown in Table 3.",
        "type": "Document"
      },
      {
        "id": "239cc6aa-01e0-4622-91f9-05e7083798ee",
        "metadata": {
          "vector_store_key": "2002.05058-2",
          "chunk_id": 48,
          "document_id": "2002.05058",
          "start_idx": 26480,
          "end_idx": 27267
        },
        "page_content": "We can see that the proposed comparative evaluator with skill rating significantly outperforms all compared baselines, including comparative evaluator with averaged sample-level scores. This demonstrates the effectiveness of the skill rating system for performing model-level comparison with pairwise sample-level evaluation. In addition, the poor correlation between conventional evaluation metrics including BLEU and perplexity demonstrates the necessity of better automated evaluation metrics in open domain NLG evaluation. We further investigate the impact of imperfect metrics on training NLG models. As described in the human evaluation procedure, we perform 10 runs to test the reliability of each metric when used to perform hyperparameter tuning and early-stopping respectively.",
        "type": "Document"
      },
      {
        "id": "d13a761b-92fd-4d26-99f1-f829fcfed252",
        "metadata": {
          "vector_store_key": "2002.10361-3",
          "chunk_id": 31,
          "document_id": "2002.10361",
          "start_idx": 18881,
          "end_idx": 19583
        },
        "page_content": "To measure overall performance, we evaluate models by four metrics: accuracy (Acc), weighted F1 score (F1-w), macro F1 score (F1-m) and area under the ROC curve (AUC). The F1 score coherently combines both precision and recall by $2*\\frac{precision*recall}{precision+recall}$. We report F1-m considering that the datasets are imbalanced. To evaluate group fairness, we measure the equality differences (ED) of true positive/negative and false positive/negative rates for each demographic factor. ED is a standard metric to evaluate fairness and bias of document classifiers BIBREF0, BIBREF4, BIBREF5. This metric sums the differences between the rates within specific user groups and the overall rates.",
        "type": "Document"
      },
      {
        "id": "61e54692-2da3-41fb-b1b0-c3469ffa76bf",
        "metadata": {
          "vector_store_key": "1607.06025-0",
          "chunk_id": 17,
          "document_id": "1607.06025",
          "start_idx": 10067,
          "end_idx": 10777
        },
        "page_content": "These attributes are conditional information that is fed to the models, like the discrete label is in our models. As recognized by BIBREF37 , the evaluation metrics of text-generating models fall into three categories: manual evaluation, automatic evaluation metrics, task-based evaluation. In evaluation based on human judgment each generated textual example is inspected manually. The automatic evaluation metrics, like ROUGE, BLEU and METEOR, compare human texts and generated texts. BIBREF38 shows METEOR has the strongest correlation with human judgments in image description evaluation. The last category is task-based evaluation, where the impact of the generated texts on a particular task is measured.",
        "type": "Document"
      },
      {
        "id": "fa230c6a-9ae2-4313-8be2-bde2493abce6",
        "metadata": {
          "vector_store_key": "1909.09484-4",
          "chunk_id": 29,
          "document_id": "1909.09484",
          "start_idx": 15790,
          "end_idx": 16412
        },
        "page_content": "We evaluate the performance of the proposed model in three aspects: (1) the accuracy of the dialogue state tracker, it aims to show the impact of the dialogue state tracker on the dialogue policy maker; (2) the accuracy of dialogue policy maker, it aims to explain the performance of different methods of constructing dialogue policy; (3) the quality of the final response, it aims to explain the impact of the dialogue policy on the final dialogue response. The evaluation metrics are listed as follows: BPRA: Belief Per-Response Accuracy (BPRA) tests the ability to generate the correct user intents during the dialogue.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the tasks that this method has shown improvements?",
    "llm_answer": "Sentiment analysis, language identification of tweets, predictors for renal failure, political affiliation classification, and hate-speech detection.\n",
    "context": [
      {
        "id": "4c1b12a8-7898-4344-8118-816aa40d4d2e",
        "metadata": {
          "vector_store_key": "2001.07263-4",
          "chunk_id": 23,
          "document_id": "2001.07263",
          "start_idx": 13335,
          "end_idx": 14047
        },
        "page_content": "Sorting the results by decreasing number of absolute errors on Hub5'00, Table TABREF7 indicates that each regularization method contributes to the improved WER. SpecAugment is by far the most important method, while using $\\Delta $ and $\\Delta \\Delta $ features or switching off the curriculum learning in the later stage of training have marginal but positive effects. Other direct input level perturbation steps (speed/tempo perturbation and sequence noise injection) are also key techniques that can be found in the upper half of the table. If we compare the worst and baseline models, we find that the relative performance difference between them is nearly unchanged by including the external LM in decoding.",
        "type": "Document"
      },
      {
        "id": "86a0dfe8-98c9-4041-95ed-e59f21788d62",
        "metadata": {
          "vector_store_key": "1707.06939-2",
          "chunk_id": 1,
          "document_id": "1707.06939",
          "start_idx": 667,
          "end_idx": 1411
        },
        "page_content": "Efficiency gains can be achieved either collectively at the level of the entire crowd or by helping individual workers. At the crowd level, efficiency can be gained by assigning tasks to workers in the best order BIBREF2 , by filtering out poor tasks or workers, or by best incentivizing workers BIBREF3 . At the individual worker level, efficiency gains can come from helping workers craft more accurate responses and complete tasks in less time. One way to make workers individually more efficient is to computationally augment their task interface with useful information. For example, an autocompletion user interface (AUI) BIBREF4 , such as used on Google's main search page, may speed up workers as they answer questions or propose ideas.",
        "type": "Document"
      },
      {
        "id": "203ac6fe-92e2-4afc-80c0-ae2cee0a5bf4",
        "metadata": {
          "vector_store_key": "1701.09123-9",
          "chunk_id": 10,
          "document_id": "1701.09123",
          "start_idx": 5259,
          "end_idx": 6064
        },
        "page_content": "Second, we empirically show how to effectively use various types of simple word representation features thereby providing a clear methodology for choosing and combining them. Third, we demonstrate that our system still obtains very competitive results even when the supervised data is reduced by half (even less in some cases), alleviating the dependency on costly hand annotated data. These three main contributions are based on: A simple and shallow robust set of features across languages and datasets, even in out-of-domain evaluations. The lack of linguistic motivated features, even for languages with agglutinative (e.g., Basque) and/or complex morphology (e.g., German). A clear methodology for using and combining various types of word representation features by leveraging public unlabeled data.",
        "type": "Document"
      },
      {
        "id": "d15b5d36-d658-49ad-ba38-d983b73bc319",
        "metadata": {
          "vector_store_key": "2001.07263-4",
          "chunk_id": 22,
          "document_id": "2001.07263",
          "start_idx": 12630,
          "end_idx": 13335
        },
        "page_content": "In a second set of experiments, we characterize the importance of each of the regularization methods described in Sec. SECREF2 for our model performance by switching off one training method at a time without re-optimizing the remaining settings. In these experiments, decoding is performed without an external language model. Curriculum learning is evaluated by either switching to randomized batches after 35 epochs or leaving the sorting on throughout training. We also test the importance of $\\Delta $ and $\\Delta \\Delta $ features BIBREF48. Sorting the results by decreasing number of absolute errors on Hub5'00, Table TABREF7 indicates that each regularization method contributes to the improved WER.",
        "type": "Document"
      },
      {
        "id": "8aa484d7-2f1b-4265-8d93-89735e7018b7",
        "metadata": {
          "vector_store_key": "1909.00430-0",
          "chunk_id": 41,
          "document_id": "1909.00430",
          "start_idx": 22623,
          "end_idx": 23877
        },
        "page_content": "The method achieves state-of-the-art results for the task, and is also effective for improving on top of a strong pre-trained Bert model. The proposed method provides an additional data-efficient tool in the modeling arsenal, which can be applied on its own or together with another training method, in situations where there is a conditional relations between the labels of a source task for which we have supervision, and a target task for which we don't. While we demonstrated the approach on the sentiment domain, the required conditional dependence between task labels is present in many situations. Other possible application of the method includes training language identification of tweets given geo-location supervision (knowing the geographical region gives a prior on languages spoken), training predictors for renal failure from textual medical records given classifier for diabetes (there is a strong correlation between the two conditions), training a political affiliation classifier from social media tweets based on age-group classifiers, zip-code information, or social-status classifiers (there are known correlations between all of these to political affiliation), training hate-speech detection based on emotion detection, and so on.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Why does the model improve in monolingual spaces as well? ",
    "llm_answer": "By averaging representations from different languages, the impact of language-specific phenomena and corpus-specific biases is reduced, resulting in more \"neutral\" monolingual embeddings.\n",
    "context": [
      {
        "id": "127bc491-2cf9-4a51-b583-3baff7687d2b",
        "metadata": {
          "vector_store_key": "1810.03459-0",
          "chunk_id": 11,
          "document_id": "1810.03459",
          "start_idx": 6418,
          "end_idx": 7213
        },
        "page_content": "Languages with limited data when trained with other languages allows them to be robust and helps in improving the recognition performance. In spite of being simple, the model has limitations in keeping the target language data unseen during training. Table TABREF16 shows the recognition performance of naive multilingual approach using BLSTMP and VGG model against a monolingual model trained with BLSTMP. The results clearly indicate that having a better architecture such as VGG-BLSTM helps in improving multilingual performance. Except Pashto, Georgian and Tokpisin, the multilingual VGG-BLSTM model gave 8.8 % absolute gain in average over monolingual model. In case of multilingual BLSTMP, except Pashto and Georgian an absolute gain of 5.0 % in average is observed over monolingual model.",
        "type": "Document"
      },
      {
        "id": "bdf57968-d746-42b0-b2b1-08db03db4f58",
        "metadata": {
          "vector_store_key": "1710.09589-2",
          "chunk_id": 19,
          "document_id": "1710.09589",
          "start_idx": 11202,
          "end_idx": 11956
        },
        "page_content": "However, the multilingual model does not consistently fare better than single models, for example on French a monolingual model would be more beneficial. Adding POS tags did not help (cf. Table TABREF31 ), actually dropped performance. We disregard this feature for the final official runs. We trained the final models on the concatenation of Train and Dev data. The results on the test set (using our internally used weighted F1 metric) are given in Table TABREF33 . There are two take-away points from the main results: First, we see a positive transfer for languages with little data, i.e., the single multilingual model outperforms the language-specific models on the two languages (Spanish and Japanese) which have the least amount of training data.",
        "type": "Document"
      },
      {
        "id": "2fead996-657c-4c63-aba7-9fab310f1fbe",
        "metadata": {
          "vector_store_key": "1808.08780-0",
          "chunk_id": 19,
          "document_id": "1808.08780",
          "start_idx": 10186,
          "end_idx": 10936
        },
        "page_content": "This is because the methods presented in the previous section apply constraints to ensure that the structure of the monolingual embeddings is largely preserved. As already mentioned in the introduction, conceptually this may not be optimal, as embeddings for different languages and trained from different corpora can be expected to be structured somewhat differently. Empirically, as we will see in the evaluation, after applying methods such as VecMap and MUSE there still tend to be significant gaps between the vector representations of words and their translations. Our method directly attempts to reduce these gaps by moving each word vector towards the middle point between its current representation and the representation of its translation.",
        "type": "Document"
      },
      {
        "id": "a0971d6e-1f82-47eb-9bad-fec93d1bab9b",
        "metadata": {
          "vector_store_key": "1808.08780-0",
          "chunk_id": 20,
          "document_id": "1808.08780",
          "start_idx": 10597,
          "end_idx": 11340
        },
        "page_content": "Our method directly attempts to reduce these gaps by moving each word vector towards the middle point between its current representation and the representation of its translation. In this way, by bringing the two monolingual fragments of the space closer to each other, we can expect to see an improved performance on cross-lingual evaluation tasks such as bilingual dictionary induction. Importantly, the internal structure of the two monolingual fragments themselves is also affected by this step. By averaging between the representations obtained from different languages, we hypothesize that the impact of language-specific phenomena and corpus specific biases will be reduced, thereby ending up with more \u201cneutral\u201d monolingual embeddings.",
        "type": "Document"
      },
      {
        "id": "d09b033e-4415-4169-9ed8-22734ae45862",
        "metadata": {
          "vector_store_key": "1710.09589-2",
          "chunk_id": 20,
          "document_id": "1710.09589",
          "start_idx": 11517,
          "end_idx": 12282
        },
        "page_content": "There are two take-away points from the main results: First, we see a positive transfer for languages with little data, i.e., the single multilingual model outperforms the language-specific models on the two languages (Spanish and Japanese) which have the least amount of training data. Overall results between the monolingual and multilingual model are close, but the advantage of our multilingual All-in-1 approach is that it is a single model that can be applied to all four languages. Second, automatic translation harms, the performance of the EN model on the translated data is substantially lower than the respective in-language model. We could investigate this as the organizers provided us with translations of French, Spanish and Japanese back to English.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is annotation projection done when languages have different word order?",
    "llm_answer": "The context states that translation divergences exist between languages, making projection sometimes difficult without loss of information.  However, it doesn't describe *how* the projection handles different word orders.  The answer is not found in the context.\n",
    "context": [
      {
        "id": "e2e4ba7c-01bf-4976-bcbf-c3e5201f645e",
        "metadata": {
          "vector_store_key": "1704.04539-2",
          "chunk_id": 23,
          "document_id": "1704.04539",
          "start_idx": 12110,
          "end_idx": 12817
        },
        "page_content": "There are several sources of noise in the annotation projection method, which affect the parsing results: 1) the parsers are trained on silver data obtained by an automatic parser for English; 2) the projection uses noisy word alignments; 3) the AMR alignments on the source side are also noisy; 4) translation divergences exist between the languages, making it sometimes difficult to project the annotation without loss of information. Figure 3 shows examples of output parses for all languages, including the AMR alignments by-product of the parsing process, that we use to discuss the mistakes made by the parsers. In the Italian example, the only evident error is that Infine (Lastly) should be ignored.",
        "type": "Document"
      },
      {
        "id": "1020c05a-5e37-4809-8519-a17a18db0501",
        "metadata": {
          "vector_store_key": "1704.04539-4",
          "chunk_id": 9,
          "document_id": "1704.04539",
          "start_idx": 4890,
          "end_idx": 5665
        },
        "page_content": "AMR is not grounded in the input sentence, therefore there is no need to change the AMR annotation when projecting to another language. We think of English labels for the graph nodes as ones from an independent language, which incidentally looks similar to English. However, in order to train state-of-the-art AMR parsers, we also need to project the alignments between AMR nodes and words in the sentence (henceforth called AMR alignments). We use word alignments, similarly to other annotation projection work, to project the AMR alignments to the target languages. Our approach depends on an underlying assumption that we make: if a source word is word-aligned to a target word and it is AMR aligned with an AMR node, then the target word is also aligned to that AMR node.",
        "type": "Document"
      },
      {
        "id": "ae4de082-cda5-4737-8143-05cf3630789e",
        "metadata": {
          "vector_store_key": "1808.08780-0",
          "chunk_id": 14,
          "document_id": "1808.08780",
          "start_idx": 7546,
          "end_idx": 8231
        },
        "page_content": "Second, a linear alignment strategy is used to map the monolingual embeddings to a common bilingual vector space (Section \"Aligning monolingual spaces\" ). Third, a final transformation is applied on the aligned embeddings so the word vectors from both languages are refined and further integrated with each other (Section \"Conclusions and Future Work\" ). This third step is the main contribution of our paper. Once the monolingual word embeddings have been obtained, a linear transformation is applied in order to integrate them into the same vector space. This linear transformation is generally carried out using a supervision signal, typically in the form of a bilingual dictionary.",
        "type": "Document"
      },
      {
        "id": "d7b58e09-4dd2-4da6-a15a-b72d3c0ccd0e",
        "metadata": {
          "vector_store_key": "1810.06743-0",
          "chunk_id": 20,
          "document_id": "1810.06743",
          "start_idx": 11861,
          "end_idx": 12643
        },
        "page_content": "Moreover, we contend that ambiguity in the schemata leave the door open to allow for such imperfections. With no strict guidance, it's natural that annotators would take different paths. Nevertheless, modulo annotator disagreement, we assume that within a particular corpus, one word form will always be consistently annotated. Three categories of annotation difficulty are missing values, language-specific attributes, and multiword expressions. In our work, the goal is not simply to translate one schema into the other, but to translate one resource (the imperfect manifestation of the schema) to match the other. The differences between the schemata and discrepancies in annotation mean that the transformation of annotations from one schema to the other is not straightforward.",
        "type": "Document"
      },
      {
        "id": "8e6126bf-230c-4c96-be0a-86f4b7c29547",
        "metadata": {
          "vector_store_key": "1704.04539-4",
          "chunk_id": 11,
          "document_id": "1704.04539",
          "start_idx": 5845,
          "end_idx": 6474
        },
        "page_content": "$A_t(\\cdot )$ be the same function for $T$ ; $v$ be a node in the AMR graph; and finally, $W(\\cdot )$ be an alignment that maps a word in $S$ to a subset of words in $T$ . Then, the AMR projection assumption is: $T = t_1 \\dots t_{\\vert t \\vert }$0  In the example of Figure 1 , Questa is word-aligned with This and therefore AMR-aligned with the node this, and the same logic applies to the other aligned words. The words is, the and of do not generate any AMR nodes, so we ignore their word alignments. We apply this method to project existing AMR annotations to other languages, which are then used to train the target parsers.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What's the precision of the system?",
    "llm_answer": "The precision of the system varies, ranging from 25-40% up to a confidence level of 0.75, with a drop to 0.35% in recall at that level.  A precision of 91% is also mentioned in the context, but is qualified as an estimate for a different aspect of the system.\n",
    "context": [
      {
        "id": "888896a4-8d3d-4593-a248-259955498527",
        "metadata": {
          "vector_store_key": "1802.05574-3",
          "chunk_id": 24,
          "document_id": "1802.05574",
          "start_idx": 13600,
          "end_idx": 14268
        },
        "page_content": "The Correct Triples column contains the number of triples that are labelled as being correct by all annotators. Total Triples are the total number of triples extracted by the given systems over the specified data. Precision is calculated as typical where Correct Triples are treated as true positives. On average, 3.1 triples were extracted per sentence. Figure 1 shows the performance of extractors in terms of precision as inter-annotator agreement decreases. In this figure, we look only at agreement on triples where the majority agree that the triple is correct. Furthermore, to ease comparison, we only consider triples with 5 judgements this excludes 9 triples.",
        "type": "Document"
      },
      {
        "id": "8dce665d-9665-4bef-8d03-212f5e982d92",
        "metadata": {
          "vector_store_key": "1605.08675-3",
          "chunk_id": 82,
          "document_id": "1605.08675",
          "start_idx": 45319,
          "end_idx": 46017
        },
        "page_content": "Of course, the final evaluation needs to be checked manually. As mentioned in previous section, the results consist of two groups: experiments, showing an influence of some aspects of algorithm on performance, and a final assessment. Both use the Polish Wikipedia as a knowledge base, whereas the questions asked belong to development and evaluation sets, respectively. In this section, recall measures percentage of questions, to which RAFAEL gave any answer, whereas precision denotes percentage of question answered correctly. When analysing results of different entity recognition techniques, we need to remember that they strongly rely on output of the question analysis, which is not perfect.",
        "type": "Document"
      },
      {
        "id": "553a7d46-e0bd-4570-9828-97ff77bb4e49",
        "metadata": {
          "vector_store_key": "1911.03243-2",
          "chunk_id": 18,
          "document_id": "1911.03243",
          "start_idx": 10598,
          "end_idx": 11351
        },
        "page_content": "The measured precision with respect to PropBank is low for adjuncts due to the fact that our annotators were capturing many correct arguments not covered in PropBank. To examine this, we analyzed 100 false positive arguments. Only 32 of those were due to wrong or incomplete QA annotations in our gold, while most others were outside of PropBank's scope, capturing either implied arguments or roles not covered in PropBank. Extrapolating from this manual analysis estimates our true precision (on all roles) to be about 91%, which is consistent with the 88% precision figure in Table TABREF19. Compared with 2015, our QA-SRL gold yielded 1593 annotations, with 989 core and 604 adjuncts, while theirs yielded 1315 annotations, 979 core and 336 adjuncts.",
        "type": "Document"
      },
      {
        "id": "70f5cfb2-3438-47c5-99aa-4d2b128bd531",
        "metadata": {
          "vector_store_key": "1809.08298-3",
          "chunk_id": 9,
          "document_id": "1809.08298",
          "start_idx": 4983,
          "end_idx": 5808
        },
        "page_content": "A correct judgment is where a run-on sentence is detected and a PERIOD is inserted in the right place. Across all datasets, roCRF has the highest precision. We speculate that roCRF consistently has the highest precision because it is the only model to use POS and syntactic features, which may restrict the occurrence of false positives by identifying longer distance, structural dependencies. roS2S is able to generalize better than roCRF, resulting in higher recall with only a moderate impact on precision. On all datasets except RealESL, roS2S consistently has the highest overall INLINEFORM0 score. In general, Punctuator has the highest recall, probably because it is trained for a more general purpose task and tries to predict punctuation at each possible position, resulting in lower precision than the other models.",
        "type": "Document"
      },
      {
        "id": "8ff94452-1294-4b43-b39d-14448e40b824",
        "metadata": {
          "vector_store_key": "1605.08675-1",
          "chunk_id": 88,
          "document_id": "1605.08675",
          "start_idx": 48599,
          "end_idx": 49271
        },
        "page_content": "Generally, the system behaves as expected, but the exact values disappoint. The precision remain at a level of 25-40 per cent up to confidence 0.75, where in turn recall drops to 0.35 per cent only. Values of F1 measure suggest that 0.2 is the highest sensible confidence rate. One more parameter worth testing, explained in section UID34 , is the context generation strategy. To find the entity with a context most similar to a question content, we could analyse a single sentence, where it appears, or a sequence of words of a predefined length. For both of these solutions, we could also add a document title, as it is likely to be referred to by anaphoric expressions.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which of the two ensembles yields the best performance?",
    "llm_answer": "The ensemble model for the synthesis task achieves the best performance (46.65 ROUGE-L and 44.78 BLEU-1).  The ensemble model for the language classification task (84.84% accuracy) also outperforms other models.\n",
    "context": [
      {
        "id": "23e74a78-129e-4d5d-aba0-1b1a84e8ce50",
        "metadata": {
          "vector_store_key": "1706.04815-2",
          "chunk_id": 37,
          "document_id": "1706.04815",
          "start_idx": 20797,
          "end_idx": 21485
        },
        "page_content": "Finally we select 13 models for ensemble, which achieves 42.92 and 44.97 in terms of ROUGE-L and BLEU-1, respectively, which achieves the state-of-the-art results of the extraction model. Then we test our synthesis model based on the extracted evidence. Our synthesis model achieves 3.78% and 3.73% improvement on the single model and ensemble model in terms of ROUGE-L, respectively. Our best result achieves 46.65 in terms of ROUGE-L and 44.78 in terms of BLEU-1, which outperforms all existing methods with a large margin and are very close to human performance. Moreover, we observe that our method only achieves significant improvement in terms of ROUGE-L compared with our baseline.",
        "type": "Document"
      },
      {
        "id": "fed6213a-1939-419a-8f57-90fc56335cce",
        "metadata": {
          "vector_store_key": "1905.07464-3",
          "chunk_id": 43,
          "document_id": "1905.07464",
          "start_idx": 24150,
          "end_idx": 25077
        },
        "page_content": "In addition to Training-22 and NLM-180, the team trained and validated their models on a set of 1148 sentences sampled from DailyMed labels that were manually annotated according to official annotation guidelines. Hence, strictly speaking, their method is not directly comparable to ours given the significant difference in available training data. While precision was similar between the three systems (with exceptions), we observed that our ensemble-based system benefited mostly from improved recall. This aligns with our initial expectation (based on prior experience with deep learning models) that an ensemble-based approach would improve stability and accuracy with deep neural models. Although including NLM-180 as training data resulted in significant performance gains during 11-fold cross validation, we find that the same improvements were not as dramatic on either test sets despite the 800% gain in training data.",
        "type": "Document"
      },
      {
        "id": "002cc73f-b05e-4dab-ab4e-923e260ed2a0",
        "metadata": {
          "vector_store_key": "1804.08186-4",
          "chunk_id": 166,
          "document_id": "1804.08186",
          "start_idx": 95379,
          "end_idx": 96124
        },
        "page_content": "BIBREF210 observed a small increase in average accuracy using the product ensemble over a majority voting ensemble. In a INLINEFORM0 -best ensemble, several models are created for each language INLINEFORM1 by partitioning the corpus INLINEFORM2 into separate samples. The score INLINEFORM3 is calculated for each model. For each language, plurality voting is then applied to the INLINEFORM4 models with the best scores to predict the language of the test document INLINEFORM5 . BIBREF349 evaluated INLINEFORM6 -best with INLINEFORM7 based on several similarity measures. BIBREF54 compared INLINEFORM8 and INLINEFORM9 and concluded that there was no major difference in accuracy when distinguishing between six languages (100 character test set).",
        "type": "Document"
      },
      {
        "id": "320f4c0c-143a-4d2c-872f-0a480e50c134",
        "metadata": {
          "vector_store_key": "1901.02257-0",
          "chunk_id": 31,
          "document_id": "1901.02257",
          "start_idx": 18545,
          "end_idx": 19174
        },
        "page_content": "The TriAN achieves 81.94% in terms of test accuracy, which is the best result of the single model. The best performing ensemble result is 84.13%, provided by HMA, which is the voting results of 7 single systems. Our single MPFN model achieves 83.52% in terms of accuracy, outperforming all the previous models. The model exceeds the HMA and TriAN by approximately 2.58% and 1.58% absolute respectively. Our ensemble model surpasses the current state-of-the-art model with an accuracy of 84.84%. We got the final ensemble result by voting on 4 single models. Every single model uses the same architecture but different parameters.",
        "type": "Document"
      },
      {
        "id": "9e4ffcc1-bed9-4948-992e-13cffb43bc05",
        "metadata": {
          "vector_store_key": "1911.03681-1",
          "chunk_id": 18,
          "document_id": "1911.03681",
          "start_idx": 10144,
          "end_idx": 10802
        },
        "page_content": "By contrast, E-BERT typically outperforms BERT on relations where such trivial answers are rare. The ensembles are able to mitigate the losses of E-BERT on almost all relations, while keeping most of its gains (purple and orange bars). This suggests that they successfully combine BERT's ability to reason about entity names with E-BERT's enhanced factual knowledge. Figure FIGREF17 shows that the lead of E-BERT and the ensembles over BERT and ERNIE in terms of mean P@k is especially salient for bigger k. We also evaluate on the FewRel relation classification dataset BIBREF9, using the setup and data split from zhang2019ernie (see Appendix for details).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the new initialization method proposed in this paper?",
    "llm_answer": "The provided text does not describe a new initialization method.  It discusses various existing methods, but does not propose its own.\n",
    "context": [
      {
        "id": "9ab10d6a-c180-4daa-8d43-5d6fccc9ff8a",
        "metadata": {
          "vector_store_key": "1603.00968-4",
          "chunk_id": 1,
          "document_id": "1603.00968",
          "start_idx": 778,
          "end_idx": 1580
        },
        "page_content": "For example, Kalchbrenner et al. kalchbrenner2014convolutional initialize word vectors to random low-dimensional vectors to be fit during training, while Johnson and Zhang johnson2014effective use fixed, one-hot encodings for each word. By contrast, Kim kim2014convolutional initializes word vectors to those estimated via the word2vec model trained on 100 billion words of Google News BIBREF5 ; these are then updated during training. Initializing embeddings to pre-trained word vectors is intuitively appealing because it allows transfer of learned distributional semantics. This has allowed a relatively simple CNN architecture to achieve remarkably strong results. Many pre-trained word embeddings are now readily available on the web, induced using different models, corpora, and processing steps.",
        "type": "Document"
      },
      {
        "id": "a4e6fdc2-6e9d-4f8a-aca0-13dc0a255327",
        "metadata": {
          "vector_store_key": "1909.13362-1",
          "chunk_id": 25,
          "document_id": "1909.13362",
          "start_idx": 14785,
          "end_idx": 15528
        },
        "page_content": "Adam was chosen because it adapts the learning rate on a parameter-to-parameter basis; strong convergence occurs at the end of optimization. Training is performed to a set number of epochs. Early stopping allows the network to conclude training if convergence is reached prior to reaching the epoch training limit BIBREF33. The materials for this research comprises the software described above and several syllabified datasets. The implementation of our model was adapted from an open source code library designed for general-purpose sequence tagging and made available by BIBREF37. The modifications to this code include adding data preparation scripts and changing the model architecture to reflect the network architecture described above.",
        "type": "Document"
      },
      {
        "id": "e22c963f-fe84-4a59-a341-6fce3efe0bf8",
        "metadata": {
          "vector_store_key": "1909.02480-5",
          "chunk_id": 21,
          "document_id": "1909.02480",
          "start_idx": 11328,
          "end_idx": 12155
        },
        "page_content": "It consists of a series of steps of flow, combined in a multi-scale architecture (see Figure FIGREF13.) Each step of flow consists three types of elementary flows \u2013 actnorm, invertible multi-head linear, and coupling. Note that all three functions are invertible and conducive to calculation of log determinants (details in Appendix SECREF6). The activation normalization layer (actnorm; BIBREF11) is an alternative for batch normalization BIBREF18, that has mainly been used in the context of image data to alleviate problems in model training. Actnorm performs an affine transformation of the activations using a scale and bias parameter per feature for sequences: Both $\\mathbf {z}$ and $\\mathbf {z}^{\\prime }$ are tensors of shape $[T\\times d_{\\mathrm {z}}]$ with time dimension $t$ and feature dimension $d_{\\mathrm {z}}$.",
        "type": "Document"
      },
      {
        "id": "69de7229-d28f-4b99-aa6c-ec52a2a841b3",
        "metadata": {
          "vector_store_key": "1706.04815-1",
          "chunk_id": 17,
          "document_id": "1706.04815",
          "start_idx": 9753,
          "end_idx": 10486
        },
        "page_content": "( $p^2$ ), which can be formulated as follows:  $$s_j^t &= \\mathrm {v}^\\mathrm {T}\\mathrm {tanh}(W_h^{P} v_j^P + W_{h}^{a} h_{t-1}^a) \\nonumber \\\\\na_i^t &= \\mathrm {exp}(s_i^t) / \\Sigma _{j=1}^N \\mathrm {exp}(s_j^t) \\nonumber \\\\\np^t &= \\mathrm {argmax}(a_1^t, \\dots , a_N^t)$$   (Eq. 16)   Here $h_{t-1}^a$ represents the last hidden state of the answer recurrent network (pointer network). The input of the answer recurrent network is the attention-pooling vector based on current predicted probability $a^t$ :  $$c_t &= \\Sigma _{i=1}^N a_i^t v_i^P \\nonumber \\\\\nh_t^a &= \\mathrm {GRU}(h_{t-1}^a, c_t)$$   (Eq. 17)   When predicting the start position, $h_{t-1}^a$ represents the initial hidden state of the answer recurrent network.",
        "type": "Document"
      },
      {
        "id": "1f7e4710-1099-4b7c-bb94-2b1ee2d1b0d4",
        "metadata": {
          "vector_store_key": "1706.08032-2",
          "chunk_id": 4,
          "document_id": "1706.08032",
          "start_idx": 2399,
          "end_idx": 3090
        },
        "page_content": "The paper makes the following contributions: The organization of the present paper is as follows: In section 2, we describe the model architecture which introduces the structure of the model. We explain the basic idea of model and the way of constructing the model. Section 3 show results and analysis and section 4 summarize this paper. Our proposed model consists of a deep learning classifier and a tweet processor. The deep learning classifier is a combination of DeepCNN and Bi-LSTM. The tweet processor standardizes tweets and then applies semantic rules on datasets. We construct a framework that treats the deep learning classifier and the tweet processor as two distinct components.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How was a quality control performed so that the text is noisy but the annotations are accurate?",
    "llm_answer": "Annotators were limited to 120 headlines per annotator, geographically restricted to the U.S., and required to pass a 95% test on 1,100/500 questions.  Annotations were excluded if made in less than 10/70 seconds (Phase 1/2).  Reliability was assessed by computing the probability of agreement with the majority.  Independent annotation of a subset of instances was followed by adjudication, and the accuracy was measured.  Instances with confidence scores \u2264 0.7 were discarded.  A subset was re-annotated by an expert, and agreement with crowdsourced workers was verified.\n",
    "context": [
      {
        "id": "01e061a4-fd26-44f2-b478-8168a8afc2fe",
        "metadata": {
          "vector_store_key": "1912.03184-3",
          "chunk_id": 30,
          "document_id": "1912.03184",
          "start_idx": 16874,
          "end_idx": 17668
        },
        "page_content": "To control the quality, we ensured that a single annotator annotates maximum 120 headlines (this protects the annotators from reading too many news headlines and from dominating the annotations). Secondly, we let only annotators who geographically reside in the U.S. contribute to the task. We test the annotators on a set of $1,100$ test questions for the first phase (about 10% of the data) and 500 for the second phase. Annotators were required to pass 95%. The questions were generated based on hand-picked non-ambiguous real headlines through swapping out relevant words from the headline in order to obtain a different annotation, for instance, for \u201cDjokovic happy to carry on cruising\u201d, we would swap \u201cDjokovic\u201d with a different entity, the cue \u201chappy\u201d to a different emotion expression.",
        "type": "Document"
      },
      {
        "id": "2d559d84-a43b-4334-a899-2901937fc2f9",
        "metadata": {
          "vector_store_key": "1912.03184-9",
          "chunk_id": 31,
          "document_id": "1912.03184",
          "start_idx": 17668,
          "end_idx": 18392
        },
        "page_content": "The questions were generated based on hand-picked non-ambiguous real headlines through swapping out relevant words from the headline in order to obtain a different annotation, for instance, for \u201cDjokovic happy to carry on cruising\u201d, we would swap \u201cDjokovic\u201d with a different entity, the cue \u201chappy\u201d to a different emotion expression. Further, we exclude Phase 1 annotations that were done in less than 10 seconds and Phase 2 annotations that were done in less than 70 seconds. After we collected all annotations, we found unreliable annotators for both phases in the following way: for each annotator and for each question, we compute the probability with which the annotator agrees with the response chosen by the majority.",
        "type": "Document"
      },
      {
        "id": "9157dbf7-6d4a-47a2-b243-c752ebd1231d",
        "metadata": {
          "vector_store_key": "1906.03538-3",
          "chunk_id": 16,
          "document_id": "1906.03538",
          "start_idx": 9500,
          "end_idx": 10225
        },
        "page_content": "To assess the quality of these annotations, two of the authors independently annotate a random subset of instances in the previous step (328 perspectives for 10 claims). Afterwards, the differences were adjudicated. We measure the accuracy adjudicated results with AMT annotations to estimate the quality of our annotation. This results in an accuracy of 94%, which shows high-agreement with the crowdsourced annotations. To enrich the ways the perspectives are phrased, we crowdsource paraphrases of our perspectives. We ask annotators to generate two paraphrases for each of the 15 perspectives in each HIT, for a reward of $1.50. Subsequently, we perform another round of crowdsourcing to verify the generated paraphrases.",
        "type": "Document"
      },
      {
        "id": "d0649c87-8163-45b9-96c7-88de0238b455",
        "metadata": {
          "vector_store_key": "1904.01608-4",
          "chunk_id": 23,
          "document_id": "1904.01608",
          "start_idx": 13392,
          "end_idx": 14157
        },
        "page_content": "Furthermore, crowdsource workers were required to remain on the annotation page (five annotations) for at least ten seconds before proceeding to the next page. Annotations were dynamically collected. The annotations were aggregated along with a confidence score describing the level of agreement between multiple crowdsource workers. The confidence score is the agreement on a single instance weighted by a trust score (accuracy of the annotator on the initial 50 test questions). To only collect high quality annotations, instances with confidence score of $\\le $ 0.7 were discarded. In addition, a subset of the dataset with 100 samples was re-annotated by a trained, expert annotator to check for quality, and the agreement rate with crowdsource workers was 86%.",
        "type": "Document"
      },
      {
        "id": "188012af-4dd7-4f42-9775-be8dcdd871ca",
        "metadata": {
          "vector_store_key": "2004.03744-2",
          "chunk_id": 48,
          "document_id": "2004.03744",
          "start_idx": 26393,
          "end_idx": 27002
        },
        "page_content": "2,060 workers participated in the annotation effort, with an average of 1.98 assignments per worker and a standard deviation of 5.54. We required the workers to have a previous approval rate above 90%. No restriction was put on the workers' location. Each assignment consisted of a set of 10 image-sentence pairs. For each pair, the participant was asked to (a) choose a label, (b) highlight words in the sentence that led to their decision, and (c) explain their decision in a comprehensive and concise manner, using a subset of the words that they highlighted. The instructions are shown in Figure FIGREF42.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is it a neural model? How is it trained?",
    "llm_answer": "Yes.  Trained end-to-end using CTC loss.\n",
    "context": [
      {
        "id": "142e5a83-d75e-4371-859e-9150adcef8e3",
        "metadata": {
          "vector_store_key": "1902.10525-1",
          "chunk_id": 12,
          "document_id": "1902.10525",
          "start_idx": 6378,
          "end_idx": 7045
        },
        "page_content": "(Figure FIGREF12 ). It is trained in an end-to-end manner using the CTC loss BIBREF24 . Our architecture is similar to what is often used in the context of acoustic modeling for speech recognition BIBREF19 , in which it is referred to as a CLDNN (Convolutions, LSTMs, and DNNs), yet we differ from it in four points. Firstly, we do not use convolution layers, which in our own experience do not add value for large networks trained on large datasets of relatively short (compared to speech input) sequences typically seen in handwriting recognition. Secondly, we use bidirectional LSTMs, which due to latency constraints is not feasible in speech recognition systems.",
        "type": "Document"
      },
      {
        "id": "6356b79c-b02c-4045-923d-10c84efa9c62",
        "metadata": {
          "vector_store_key": "1710.01507-2",
          "chunk_id": 13,
          "document_id": "1710.01507",
          "start_idx": 7711,
          "end_idx": 8430
        },
        "page_content": "The implicit learning of spatial layout and object semantics in the later layers of the network from very large datasets has contributed to the success of these features. We use a pre-trained network of VGG-19 architecture BIBREF14 trained over the ImageNet database (ILSVRC-2012) and extract CNN features. We use the output of the fully-connected layer (FC7), which has 4096 dimensions, as feature representations for our architecture. We now go into detail about the components of the model, individual and combined, and how the parameters are learned. Recurrent Neural Network (RNN) is a class of artificial neural networks which utilizes sequential information and maintains history through its intermediate layers.",
        "type": "Document"
      },
      {
        "id": "6bbd7a5c-e154-44ef-aa8d-26d57ff0fc1a",
        "metadata": {
          "vector_store_key": "1706.07179-3",
          "chunk_id": 5,
          "document_id": "1706.07179",
          "start_idx": 2979,
          "end_idx": 3608
        },
        "page_content": "The model is sequential in nature, consisting of the following steps: read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer. We model the dynamic memory in a fashion similar to Recurrent Entity Networks BIBREF17 and then equip it with an additional relational memory. There are three main components to the model: 1) input encoder 2) dynamic memory, and 3) output module. We will describe these three modules in details. The input encoder and output module implementations are similar to the Entity Network BIBREF17 and main novelty lies in the dynamic memory.",
        "type": "Document"
      },
      {
        "id": "66dd0d49-ddb9-4ed3-b98a-a3835e72cdfd",
        "metadata": {
          "vector_store_key": "1910.06748-1",
          "chunk_id": 14,
          "document_id": "1910.06748",
          "start_idx": 7845,
          "end_idx": 8646
        },
        "page_content": "Neural network models have been applied on many NLP problems in recent years with great success, achieving excellent performance on challenges ranging from text classification BIBREF12 to sequence labeling BIBREF13. In LID, the authors of BIBREF1 built a hierarchical system of two neural networks. The first level is a Convolutional Neural Network (CNN) that converts white-space-delimited words into a word vector. The second level is a Long-Short-Term Memory (LSTM) network (a type of recurrent neural network (RNN)) that takes in sequences of word vectors outputted by the first level and maps them to language labels. They trained and tested their network on Twitter's official Twitter70 dataset, and achieved an F-score of 0.912, compared to langid.py's performance of 0.879 on the same dataset.",
        "type": "Document"
      },
      {
        "id": "3575a107-b7da-4344-ad4e-91ea4380b9ca",
        "metadata": {
          "vector_store_key": "1809.01060-4",
          "chunk_id": 23,
          "document_id": "1809.01060",
          "start_idx": 12248,
          "end_idx": 12935
        },
        "page_content": "It has three main components: The encoder for each pair of sentences taken as input is composed of two parallel \"Atrous\" Convolutional Neural Networks (CNNs) and LSTM RNNs, feeding two sequenced fully connected layers. The encoder is preloaded with the lexical embeddings from Word2vec BIBREF6 . The sequences of word embeddings that we use as input provides the model with dense word-level information, while the model tries to generalize over these embedding patterns. The combination of a CNN and an LSTM allows us to capture both long-distance syntactic and semantic relations, best identified by a CNN, and the sequential nature of the input, most efficiently identified by an LSTM.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How they evaluate quality of generated output?",
    "llm_answer": "Human evaluation, automatic evaluation (summarization quality, language fluency), task-based evaluation, discriminative evaluation, and grading by experts and amateurs.\n",
    "context": [
      {
        "id": "190855e2-c769-4beb-9b68-b620fc9e8e55",
        "metadata": {
          "vector_store_key": "2004.01980-2",
          "chunk_id": 30,
          "document_id": "2004.01980",
          "start_idx": 17061,
          "end_idx": 17710
        },
        "page_content": "In addition, we have another independent human evaluation task about the style strength \u2013 we present the generated headlines from TitleStylist and baselines to the human judges and let them choose the one that most conforms to the target style such as humor. Then we define the style strength score as the proportion of choices. Apart from the comprehensive human evaluation, we use automatic evaluation to measure the generation quality through two conventional aspects: summarization quality and language fluency. Note that the purpose of this two-way automatic evaluation is to confirm that the performance of our model is in an acceptable range.",
        "type": "Document"
      },
      {
        "id": "4e761f78-ffd6-4ccb-9a33-daac9a9849b1",
        "metadata": {
          "vector_store_key": "1912.00864-7",
          "chunk_id": 46,
          "document_id": "1912.00864",
          "start_idx": 26721,
          "end_idx": 27480
        },
        "page_content": "Following evaluations made by crowdsourced evaluators BIBREF29, we conducted human evaluations to judge the outputs of CLSTM and those of NAGM. Different from BIBREF29, we hired human experts who had experience in Oshiete-goo QA community service. Thus, they were familiar with the sorts of answers provided by and to the QA community. The experts asked questions, which were not included in our training datasets, to the AI system and rated the answers; one answer per question. The experts rated the answers as follows: (1) the content of the answer matched the question, and the grammar was okay; (2) the content was suitable, but the grammar was poor; (3) the content was not suitable, but the grammar was okay; (4) both the content and grammar were poor.",
        "type": "Document"
      },
      {
        "id": "61e54692-2da3-41fb-b1b0-c3469ffa76bf",
        "metadata": {
          "vector_store_key": "1607.06025-0",
          "chunk_id": 17,
          "document_id": "1607.06025",
          "start_idx": 10067,
          "end_idx": 10777
        },
        "page_content": "These attributes are conditional information that is fed to the models, like the discrete label is in our models. As recognized by BIBREF37 , the evaluation metrics of text-generating models fall into three categories: manual evaluation, automatic evaluation metrics, task-based evaluation. In evaluation based on human judgment each generated textual example is inspected manually. The automatic evaluation metrics, like ROUGE, BLEU and METEOR, compare human texts and generated texts. BIBREF38 shows METEOR has the strongest correlation with human judgments in image description evaluation. The last category is task-based evaluation, where the impact of the generated texts on a particular task is measured.",
        "type": "Document"
      },
      {
        "id": "be49936c-0a0d-48bb-8e46-51cc6f59b6b9",
        "metadata": {
          "vector_store_key": "1607.06025-0",
          "chunk_id": 68,
          "document_id": "1607.06025",
          "start_idx": 39589,
          "end_idx": 40296
        },
        "page_content": "To further examine the quality of generated hypothesis, they were compared against the original human written hypotheses. The discriminative evaluation shows that in INLINEFORM2 of cases the human evaluator incorrectly distinguished between the original and the generated hypothesis. The discriminative model was actually better in distinguishing. We have also compared the accuracy of classifier to other metrics. The standard text generation metrics ROUGE and METEOR do not indicate if a generated dataset is good for training a classifier. To obtain higher accuracies of the generated datasets, they need to be filtered, because the generative models produce examples, whose label is not always accurate.",
        "type": "Document"
      },
      {
        "id": "f2ccbf11-92ca-4546-b7d7-d1243199c4a9",
        "metadata": {
          "vector_store_key": "1909.00279-3",
          "chunk_id": 26,
          "document_id": "1909.00279",
          "start_idx": 15254,
          "end_idx": 15992
        },
        "page_content": "The human evaluators were divided into two groups. The expert group contains 15 people who hold a bachelor degree in Chinese literature, and the amateur group contains 15 people who holds a bachelor degree in other fields. All 30 human evaluators are native Chinese speakers. We ask evaluators to grade each generated poem from four perspectives: 1) Fluency: Is the generated poem grammatically and rhythmically well formed, 2) Semantic coherence: Is the generated poem itself semantic coherent and meaningful, 3) Semantic preservability: Does the generated poem preserve the semantic of the modern Chinese translation, 4) Poeticness: Does the generated poem display the characteristic of a poem and does the poem build good poetic image.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the four forums the data comes from?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "786eaa4f-6843-428f-b802-626e6014bcbf",
        "metadata": {
          "vector_store_key": "2002.01359-4",
          "chunk_id": 22,
          "document_id": "2002.01359",
          "start_idx": 12592,
          "end_idx": 13359
        },
        "page_content": "The 20 domains present across the train, dev and test datasets are listed in Table TABREF10, as are the details regarding which domains are present in each of the datasets. We create synthetic implementations of a total of 45 services or APIs over these domains. Our simulator framework interacts with these services to generate dialogue outlines, which are structured representations of dialogue semantics. We then use a crowd-sourcing procedure to paraphrase these outlines to natural language utterances. Our novel crowd-sourcing procedure preserves all annotations obtained from the simulator and does not require any extra annotations after dialogue collection. In this section, we describe these steps briefly and then present analyses of the collected dataset.",
        "type": "Document"
      },
      {
        "id": "ada0c52d-5c4c-4087-8df6-931ef27e13d1",
        "metadata": {
          "vector_store_key": "1805.09959-1",
          "chunk_id": 43,
          "document_id": "1805.09959",
          "start_idx": 24782,
          "end_idx": 25486
        },
        "page_content": "The authors wish to acknowledge the Vermont Advanced Computing Core, which is supported by NASA (NNX-08AO96G) at the University of Vermont which provided High Performance Computing resources that contributed to the research results reported within this poster. EMC was supported by the Vermont Complex Systems Center. CMD and PSD were supported by an NSF BIGDATA grant IIS-1447634. There are three types of endpoints to access data from Twitter. The `spritzer' (1%) and `gardenhose' (10%) endpoints were both implemented to collect publicly posted relevant data for our analysis. The third type of endpoint is the `Firehose' feed, a full 100% sample, which can be purchased via subscription from Twitter.",
        "type": "Document"
      },
      {
        "id": "01c3a47b-6191-4f91-85ae-fc53dd0b5110",
        "metadata": {
          "vector_store_key": "1909.03242-1",
          "chunk_id": 8,
          "document_id": "1909.03242",
          "start_idx": 4951,
          "end_idx": 5544
        },
        "page_content": "This resulted in 38 websites in total (shown in Table TABREF1 ). Ten websites could not be crawled, as further detailed in Table TABREF40 . In the later experimental descriptions, we refer to the part of the dataset crawled from a specific fact checking website as a domain, and we refer to each website as source. From each source, we crawled the ID, claim, label, URL, reason for label, categories, person making the claim (speaker), person fact checking the claim (checker), tags, article title, publication date, claim date, as well as the full text that appears when the claim is clicked.",
        "type": "Document"
      },
      {
        "id": "276501ed-bda4-47b1-b60d-93e9fd9610b4",
        "metadata": {
          "vector_store_key": "1909.02265-3",
          "chunk_id": 29,
          "document_id": "1909.02265",
          "start_idx": 16212,
          "end_idx": 16476
        },
        "page_content": "We plan to extend this current research and integrate its fruitful results into a future version of the platform. The following is three example dialogues generated by our system. The first dialogue is in single-domain. The next two dialogues are in mixed-domains.",
        "type": "Document"
      },
      {
        "id": "51bf7ec8-05f0-4791-a232-eadb6cf43620",
        "metadata": {
          "vector_store_key": "1909.05855-3",
          "chunk_id": 6,
          "document_id": "1909.05855",
          "start_idx": 3587,
          "end_idx": 4269
        },
        "page_content": "These datasets have utilized a variety of data collection techniques, falling within two broad categories: Wizard-of-Oz This setup BIBREF12 connects two crowd workers playing the roles of the user and the system. The user is provided a goal to satisfy, and the system accesses a database of entities, which it queries as per the user's preferences. WOZ2.0, FRAMES and MultiWOZ, among others, have utilized such methods. Machine-machine Interaction A related line of work explores simulation-based dialogue generation, where the user and system roles are simulated to generate a complete conversation flow, which can then be converted to natural language using crowd workers BIBREF1.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are sentence embeddings incorporated into the speech recognition system?",
    "llm_answer": "External word and/or sentence embeddings (e.g., fastText, BERT) are used to create conversational-context representations.  A gating mechanism integrates these embeddings (along with speech embeddings) into the decoder network.\n",
    "context": [
      {
        "id": "66d2d371-dcae-4154-bf3d-d005e863fe97",
        "metadata": {
          "vector_store_key": "1906.11604-3",
          "chunk_id": 3,
          "document_id": "1906.11604",
          "start_idx": 2039,
          "end_idx": 2920
        },
        "page_content": "In this paper, we create a conversational-context aware end-to-end speech recognizer capable of incorporating a conversational-context to better process long conversations. Specifically, we propose to exploit external word and/or sentence embeddings which trained on massive amount of text resources, (i.e. fastText, BERT) so that the model can learn better conversational-context representations. So far, the use of such pre-trained embeddings have found limited success in the speech recognition task. We also add a gating mechanism to the decoder network that can integrate all the available embeddings (word, speech, conversational-context) efficiently with increase representational power using multiplicative interactions. Additionally, we explore a way to train our speech recognition model even with text-only data in the form of pre-training and joint-training approaches.",
        "type": "Document"
      },
      {
        "id": "ef564f77-d4d7-4198-83fb-83f3f1f33dfc",
        "metadata": {
          "vector_store_key": "2001.00137-2",
          "chunk_id": 16,
          "document_id": "2001.00137",
          "start_idx": 9859,
          "end_idx": 10606
        },
        "page_content": "The reconstructed hidden sentence embedding $h_{rec}$ is compared with the complete hidden sentence embedding $h_{comp}$ through a mean square error loss function, as shown in Eq. (DISPLAY_FORM7): After reconstructing the correct hidden embeddings from the incomplete sentences, the correct hidden embeddings are given to bidirectional transformers to generate input representations. The model is then fine-tuned in an end-to-end manner on the incomplete text classification corpus. Classification is done with a feedforward network and softmax activation function. Softmax $\\sigma $ is a discrete probability distribution function for $N_C$ classes, with the sum of the classes probability being 1 and the maximum value being the predicted class.",
        "type": "Document"
      },
      {
        "id": "e325926f-6952-4c23-ab25-af4eed197952",
        "metadata": {
          "vector_store_key": "1910.05608-2",
          "chunk_id": 7,
          "document_id": "1910.05608",
          "start_idx": 3964,
          "end_idx": 4626
        },
        "page_content": "Each sentence has a set of words corresponding to a set of word vectors, and that set of word vectors is a representation of a sentence. We also make a sentence embedding by using RoBERTa architecture BIBREF8. CBOW and RoBERTa models trained on text from some resources including VLSP 2016 Sentiment Analysis, VLSP 2018 Sentiment Analysis, VLSP 2019 HSD and text crawled from Facebook. After having sentence representation, we use some classification models to classify input sentences. Those models will be described in detail in the section SECREF13. With the multiply output results, we will use an ensemble method to combine them and output the final result.",
        "type": "Document"
      },
      {
        "id": "d7f15dac-e215-4e43-b7ee-2f54c601e72e",
        "metadata": {
          "vector_store_key": "1703.02507-0",
          "chunk_id": 24,
          "document_id": "1703.02507",
          "start_idx": 13872,
          "end_idx": 14688
        },
        "page_content": "The sentence discourse vector can hence be obtained by subtracting INLINEFORM6 estimated by the first principal component of INLINEFORM7 's on a set of sentences. In other words, the sentence embeddings are obtained by a weighted average of the word vectors stripping away the syntax by subtracting the common discourse vector and down-weighting frequent tokens. They generate sentence embeddings from diverse pre-trained word embeddings among which are unsupervised word embeddings such as GloVe BIBREF2 as well as supervised word embeddings such as paragram-SL999 (PSL) BIBREF18 trained on the Paraphrase Database BIBREF19 . In a very different line of work, C-PHRASE BIBREF20 relies on additional information from the syntactic parse tree of each sentence, which is incorporated into the C-BOW training objective.",
        "type": "Document"
      },
      {
        "id": "66cd9d70-2a00-4c22-894d-0600b95be2b4",
        "metadata": {
          "vector_store_key": "1906.11604-3",
          "chunk_id": 40,
          "document_id": "1906.11604",
          "start_idx": 23414,
          "end_idx": 24250
        },
        "page_content": "We have introduced a novel method for conversational-context aware end-to-end speech recognition based on a gated network that incorporates word/sentence/speech embeddings. Unlike prior work, our model is trained on conversational datasets to predict a word, conditioning on multiple preceding conversational-context representations, and consequently improves recognition accuracy of a long conversation. Moreover, our gated network can incorporate effectively with text-based external resources, word or sentence embeddings (i.e., fasttext, BERT) within an end-to-end framework and so that the whole system can be optimized towards our final objectives, speech recognition accuracy. By incorporating external embeddings with gating mechanism, our model can achieve further improvement with better conversational-context representation.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How different is the dataset size of source and target?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "8be62763-27f4-46ff-b2bb-edb6a6b2fa04",
        "metadata": {
          "vector_store_key": "1607.06025-3",
          "chunk_id": 42,
          "document_id": "1607.06025",
          "start_idx": 24463,
          "end_idx": 25198
        },
        "page_content": "The new dataset consists of training set, which is generated using examples from the original training set, and a development set, which is generated from the original development set. The beam size for beam search was set to 1. The details of the decision are presented in Section SECREF35 . Some datasets were constructed by filtering the generated datasets according to various thresholds. Thus, the generated datasets were constructed to contain enough examples, so that the filtered datasets had at least the number of examples as the original dataset. In the end, all the datasets were trimmed down to the size of the original dataset by selecting the samples sequentially from the beginning until the dataset had the right size.",
        "type": "Document"
      },
      {
        "id": "78e083f1-0666-4414-8066-d724fc284323",
        "metadata": {
          "vector_store_key": "1901.09755-0",
          "chunk_id": 25,
          "document_id": "1901.09755",
          "start_idx": 14080,
          "end_idx": 14670
        },
        "page_content": "For English, it should be noted that the size of the 2015 set is less than half with respect to the 2014 dataset in terms of tokens, and only one third in number of targets. The French, Spanish and Dutch datasets are quite similar in terms of tokens although the number of targets in the Dutch dataset is comparatively smaller, possibly due to the tendency to construct compound terms in that language. The Russian dataset is the largest whereas the Turkish set is by far the smallest one. Additionally, we think it is also interesting to note the low number of targets that are multiwords.",
        "type": "Document"
      },
      {
        "id": "0399d898-292f-435e-bcb6-f065e146ace9",
        "metadata": {
          "vector_store_key": "1910.10408-0",
          "chunk_id": 20,
          "document_id": "1910.10408",
          "start_idx": 10284,
          "end_idx": 10930
        },
        "page_content": "While our main goal is to verify our hypotheses on a large data condition, thus the need to include proprietary data, for the sake of reproducibility in both languages we also provide results with systems only trained on TED Talks (small data condition). When training on large scale data we use Transformer with layer size of 1024, hidden size of 4096 on feed forward layers, 16 heads in the multi-head attention, and 6 layers in both encoder and decoder. When training only on TED talks, we set layer size of 512, hidden size of 2048 for the feed forward layers, multi-head attention with 8 heads and again 6 layers in both encoder and decoder.",
        "type": "Document"
      },
      {
        "id": "4a420f91-0ffd-440c-b176-e1aa838223ea",
        "metadata": {
          "vector_store_key": "1709.05404-4",
          "chunk_id": 33,
          "document_id": "1709.05404",
          "start_idx": 17489,
          "end_idx": 18127
        },
        "page_content": "It is interesting to note that the overall results of the Hyp data are lower than those for Gen and RQs, likely due to the smaller size of the Hyp dataset. To examine the effect of dataset size, we compare F-measure (using the same 10-fold cross-validation setup) for each dataset while holding the number of posts per class constant. Figure FIGREF26 shows the performance of each of the Gen, RQ, and Hyp datasets at intervals of 100 posts per class (up to the maximum size of 582 posts per class for Hyp, and 851 posts per class for RQ). From the graph, we can see that as a general trend, the datasets benefit from larger dataset sizes.",
        "type": "Document"
      },
      {
        "id": "0fcc020b-3016-4745-902c-d73226505bf1",
        "metadata": {
          "vector_store_key": "1910.11949-3",
          "chunk_id": 17,
          "document_id": "1910.11949",
          "start_idx": 9318,
          "end_idx": 10010
        },
        "page_content": "Each source contains 5,000 images with 5 questions per image, adding a total of 15,000 images with 75,000 questions. COCO dataset includes images of complex everyday scenes containing common objects in their natural context, but it is limited in terms of the concepts it covers. Bing dataset contains more event related questions and has a wider range of questions longitudes (between 3 and 20 words), while Flickr questions are shorter (less than 6 words) and the images appear to be more casual. We use two datasets to train our chatbot model. The first one is the Persona-chat BIBREF15 which contains dialogues between two people with different profiles that are trying to know each other.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What type of documents are supported by the annotation platform?",
    "llm_answer": "PDFs and Microsoft Word.\n",
    "context": [
      {
        "id": "b3be9f3f-6aa3-4026-be1a-fa6e9c5a4ffd",
        "metadata": {
          "vector_store_key": "2002.01861-4",
          "chunk_id": 24,
          "document_id": "2002.01861",
          "start_idx": 12829,
          "end_idx": 13601
        },
        "page_content": "The platform has two main features: First, it provides an annotation interface that allows users to define content elements, upload documents, and annotate documents; a screenshot is shown in Figure FIGREF12. We have invested substantial effort in making the interface as easy to use as possible; for example, annotating content elements is as easy as selecting text from the document. Our platform is able to ingest documents in a variety of formats, including PDFs and Microsoft Word, and converts these formats into plain text before presenting them to the annotators. The second feature of the platform is the ability for users to upload new documents and apply inference on them using a fine-tuned BERT model; a screenshot of this feature is shown in Figure FIGREF13.",
        "type": "Document"
      },
      {
        "id": "b98f628e-079c-4a88-a864-851b3736d44a",
        "metadata": {
          "vector_store_key": "1912.07025-2",
          "chunk_id": 39,
          "document_id": "1912.07025",
          "start_idx": 23765,
          "end_idx": 24535
        },
        "page_content": "Given the general nature of our framework, advances in instance segmentation approaches can be leveraged thereby improving performance over time. Our proposed web-based annotator system, although designed for Indic manuscripts, is flexible, and could be reused for similar manuscripts from Asian subcontinent. We intend to expand the capabilities of our annotator system in many useful ways. For instance, the layout estimated by our deep-network could be provided to annotators for correction, thus reducing annotation efforts. Finally, we plan to have our dataset, instance segmentation system and annotator system publicly available. This would enable large-scale data collection and automated analysis efforts for Indic as well as other historical Asian manuscripts.",
        "type": "Document"
      },
      {
        "id": "ddf2f2e3-0ceb-4bbb-9dbb-c05e8bbf91a0",
        "metadata": {
          "vector_store_key": "1912.07025-2",
          "chunk_id": 17,
          "document_id": "1912.07025",
          "start_idx": 11170,
          "end_idx": 12027
        },
        "page_content": "Degradations: Historical Indic manuscripts tend to be inherently fragile and prone to damage due to various sources \u2013 wood-and-leaf-boring insects, humidity seepage, improper storage and handling etc. While some degradations cause the edges of the document to become frayed, others manifest as irregularly shaped perforations in the document interior. It may be important to identify such degradations before attempting lexically-focused tasks such as OCR or word-spotting. Keeping the aforementioned challenges in mind, we introduce a new browser-based annotation tool (see Figure FIGREF10). The tool is designed to operate both stand-alone and as a web-service. The web-service mode enables features such as distributed parallel sessions by registered annotators, dashboard-based live session monitoring and a wide variety of annotation-related analytics.",
        "type": "Document"
      },
      {
        "id": "ee784bfd-73f3-4a5d-afd2-cb5ae025f7e5",
        "metadata": {
          "vector_store_key": "2002.01861-4",
          "chunk_id": 23,
          "document_id": "2002.01861",
          "start_idx": 12530,
          "end_idx": 13179
        },
        "page_content": "Finally, in Table TABREF8 we show an excerpt from each type of document along with the content elements that are extracted by our BERT models. We provide both the original source Chinese texts as well as English translations to provide the reader with a general sense of the source documents and how well our models behave. All the capabilities described in this paper come together in an end-to-end cloud-based platform that we have built. The platform has two main features: First, it provides an annotation interface that allows users to define content elements, upload documents, and annotate documents; a screenshot is shown in Figure FIGREF12.",
        "type": "Document"
      },
      {
        "id": "6941e3c3-ed76-4d5f-86b8-66718faf1417",
        "metadata": {
          "vector_store_key": "1912.07025-2",
          "chunk_id": 13,
          "document_id": "1912.07025",
          "start_idx": 8701,
          "end_idx": 9444
        },
        "page_content": "As our results show, our approach can successfully handle such multi-page documents, thereby making it truly an end-to-end system. Overall, our dataset contains 508 annotated Indic manuscripts. Some salient aspects of the dataset can be viewed in Table TABREF5 and a pictorial illustration of layout regions can be viewed in Figure FIGREF13. Note that multiple regions can overlap, unlike existing historical document datasets which typically contain disjoint region annotations. For the rest of the section, we discuss the challenges associated with annotating Indic manuscripts (Section SECREF9) and our web-based annotation tool (Section SECREF11). A variety of unique challenges exist in the context of annotating Indic manuscript layouts.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the strong baselines you have?",
    "llm_answer": "The neural baseline with ELMo contextual vectors (F1 score of 82.6) and AoA Reader are strong baselines.\n",
    "context": [
      {
        "id": "fa88df39-033d-4e0f-891b-162e0f9b0424",
        "metadata": {
          "vector_store_key": "1709.08299-4",
          "chunk_id": 19,
          "document_id": "1709.08299",
          "start_idx": 11042,
          "end_idx": 11836
        },
        "page_content": "The neural baselines are built by the following steps. We first use the shared training data to build a general systems, and choose the best performing model (in terms of cloze validation set) as baseline. Use User Query validation data for further tuning the systems with 10-fold cross-validations. Increase dropout rate BIBREF9 to 0.5 for preventing over-fitting issue. All baseline systems are chosen according to the performance of the validation set. The participant system results are given in Table 2 and 3 . As we can see that two neural baselines are competitive among participant systems and AoA Reader successfully outperform AS Reader and all participant systems in single model condition, which proves that it is a strong baseline system even without further fine-tuning procedure.",
        "type": "Document"
      },
      {
        "id": "e7065ee9-867c-433d-ac3c-08844aac0090",
        "metadata": {
          "vector_store_key": "1904.01608-7",
          "chunk_id": 38,
          "document_id": "1904.01608",
          "start_idx": 21241,
          "end_idx": 21885
        },
        "page_content": "On this dataset, the best baseline is the neural baseline with addition of ELMo contextual vectors achieving an F1 score of 82.6 followed by BIBREF7 , which is expected because neural models generally achieve higher gains when more training data is available and because BIBREF7 was not designed with the SciCite dataset in mind. The breakdown of results by intent on ACL-ARC and SciCite datasets is respectively shown in Tables 5 and 6 . Generally we observe that results on categories with more number of instances are higher. For example on ACL-ARC, the results on the Background category are the highest as this category is the most common.",
        "type": "Document"
      },
      {
        "id": "5e76f24e-4c75-4a34-b69c-7fe1f8d12771",
        "metadata": {
          "vector_store_key": "1909.02635-2",
          "chunk_id": 15,
          "document_id": "1909.02635",
          "start_idx": 9069,
          "end_idx": 9719
        },
        "page_content": "This includes (i) Majority Class, (ii) Exact Match of an ingredient $e$ in recipe step $s_t$, and (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo$_{token}$ and ELMo$_{sent}$) BIBREF22 compared to GPT.",
        "type": "Document"
      },
      {
        "id": "9707c65e-4f23-4b79-9493-c320ff38b186",
        "metadata": {
          "vector_store_key": "1911.13066-2",
          "chunk_id": 26,
          "document_id": "1911.13066",
          "start_idx": 15335,
          "end_idx": 15950
        },
        "page_content": "It is observed from the results that baseline model performs worst among all the experiments. The reason behind this degradation in performance can be traced back to the nature of the texts in the datasets (i.e., datasets used in original paper of baseline model BIBREF3 and in our study). The approach in base model measure the performance of the model on multilingual dataset in which there is no code-switching involved. The complete text belongs to either one language or the other. However, in our case, the SMS text can have code-switching between two language, variation of spelling, or non-standard grammar.",
        "type": "Document"
      },
      {
        "id": "220ef063-b36b-4173-bb01-e93af23b3983",
        "metadata": {
          "vector_store_key": "1911.05153-4",
          "chunk_id": 22,
          "document_id": "1911.05153",
          "start_idx": 12331,
          "end_idx": 12964
        },
        "page_content": "We use dropout of $0.3$ and train the model for 20 epochs with learning rate of $0.01$ and weight decay of $0.001$. This model, our baseline, achieves $87.1\\%$ accuracy over the test set. The performance of the base model described in the previous section is shown in the first row of Table TABREF8 for the Nematus cs-en ($\\bar{cs}$), FB MT system cs-en (cs) and es-en (es), sequence autoencoder (seq2seq), and the average of the adversarial sets (avg). We also included the results for the ensemble model, which combines the decisions of five separate baseline models that differ in batch order, initialization, and dropout masking.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are causal attribution networks?",
    "llm_answer": "A causal attribution network is a collection of text pairs reflecting cause-effect relationships proposed by humans, where the nodes represent the concepts and directed edges represent the relationships (e.g., \"virus\" \u2192 \"sickness\").\n",
    "context": [
      {
        "id": "f9b0dd6d-98ba-4652-9b14-1e650f9f4b82",
        "metadata": {
          "vector_store_key": "1812.06038-1",
          "chunk_id": 0,
          "document_id": "1812.06038",
          "start_idx": 0,
          "end_idx": 943
        },
        "page_content": "In this work we compare causal attribution networks derived from three datasets. A causal attribution dataset is a collection of text pairs that reflect cause-effect relationships proposed by humans (for example, \u201cvirus causes sickness\u201d). These written statements identify the nodes of the network (see also our graph fusion algorithm for dealing with semantically equivalent statements) while cause-effect relationships form the directed edges (\u201cvirus\u201d $\\rightarrow $ \u201csickness\u201d) of the causal attribution network. We collected causal attribution networks from three sources of data: English Wikidata BIBREF11 , English ConceptNet BIBREF10 , and IPRnet BIBREF12 . Wikidata and ConceptNet, are large knowledge graphs that contain semantic links denoting many types of interactions, one of which is causal attribution, while IPRnet comes from an Amazon Mechanical Turk study in which crowd workers were prompted to provide causal relationships.",
        "type": "Document"
      },
      {
        "id": "8e9fe96a-a60f-4ecc-9888-33e31eb9bdff",
        "metadata": {
          "vector_store_key": "1812.06038-7",
          "chunk_id": 24,
          "document_id": "1812.06038",
          "start_idx": 12989,
          "end_idx": 13767
        },
        "page_content": "We perform a descriptive analysis of the three datasets, comparing and contrasting their features and properties. We focus on two aspects, the network structure and the text information (the written descriptions associated with each node in the network). Understanding these data at these levels can inform efforts to combine different causal attribution networks (Sec. \"Fusing causal networks\" ). Table 1 and Fig. 2 summarize network characteristics for the three causal attribution networks. We focus on standard measures of network structure, measuring the sizes, densities, motif structure, and connectedness of the three networks. Both Wikidata and ConceptNet, the two larger networks, are highly disconnected, amounting to collections of small components with low density.",
        "type": "Document"
      },
      {
        "id": "58275b9e-76a6-4212-8d8e-c84faa39dec7",
        "metadata": {
          "vector_store_key": "1812.06038-7",
          "chunk_id": 23,
          "document_id": "1812.06038",
          "start_idx": 12629,
          "end_idx": 13386
        },
        "page_content": "Thus, to understand how causal attribution networks can be combined, we introduce and analyze a method for fusing networks (Sec. \"Fusing causal networks\" ) that builds off both the network structure and associated text information and explicitly incorporates conceptual equivalencies. Lastly, in Sec. \"Inferring the size of the causal attribution network\" we use the degree of overlap in these networks as a means to infer the total size of the one underlying causal attribution network being explored by these data collection efforts, allowing us to better understand the size of collective space of cause-effect relationships held by humans. We perform a descriptive analysis of the three datasets, comparing and contrasting their features and properties.",
        "type": "Document"
      },
      {
        "id": "a80eddc9-4f28-4881-b858-e8bf34acbbc6",
        "metadata": {
          "vector_store_key": "1812.06038-7",
          "chunk_id": 49,
          "document_id": "1812.06038",
          "start_idx": 26206,
          "end_idx": 26887
        },
        "page_content": "This estimate possesses a relatively large confidence interval due to low observed overlap in the sets of edges. According to this estimate, $0.198\\% \\pm 0.174\\%$ of links have been explored. The construction of causal attribution networks generates important knowledge networks that may inform causal inference research and even help future AI systems to perform causal reasoning, but these networks are time-consuming and costly to generate, and to date no efforts have been made to combine different networks. Our work not only studies the potential for fusing different networks together, but also infers the overall size of the total causal attribution network being explored.",
        "type": "Document"
      },
      {
        "id": "073d53e8-d9c5-454d-a510-b3b9fe3b9ef6",
        "metadata": {
          "vector_store_key": "1812.06038-1",
          "chunk_id": 33,
          "document_id": "1812.06038",
          "start_idx": 18018,
          "end_idx": 18734
        },
        "page_content": "These proportions imply that causal attributions contain a higher frequency of objects committing actions than general speech. However, ConceptNet differs, with proportions of nouns and verbs closer to the baseline. The baseline also contains more adjectives than ConceptNet and IPRnet. Overall, shorter, noun-heavy sentences may either help or harm the ability to combine causal attribution networks, depending on their ambiguity relative to longer, typical written statements. These causal attributions networks are separate efforts to map out the underlying or latent causal attribution network held collectively by humans. It is natural to then ask if these different efforts can be combined in an effective way.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "how did they ask if a tweet was racist?",
    "llm_answer": "The context does not describe how participants were asked if a tweet was racist.  It discusses offensiveness in general, but not specific types of offenses.\n",
    "context": [
      {
        "id": "02778c89-b1e3-484f-9b07-77ea84df05b6",
        "metadata": {
          "vector_store_key": "1701.08118-1",
          "chunk_id": 15,
          "document_id": "1701.08118",
          "start_idx": 7924,
          "end_idx": 8546
        },
        "page_content": "Participants were finally asked (3) to answer how offensive they thought the tweet was on a 6-point Likert scale from 1 (Not offensive at all) to 6 (Very offensive). If they answered 4 or higher, the participants had the option to state which particular words they found offensive. After the annotation of the 20 tweets, participants were asked to voluntarily answer an open question regarding the definition of hate speech. In the survey with the definition, they were asked if the definition of Twitter was sufficient. In the survey without the definition, the participants were asked to suggest a definition themselves.",
        "type": "Document"
      },
      {
        "id": "1339527f-0d22-49ff-a867-6945112829bd",
        "metadata": {
          "vector_store_key": "1701.08118-1",
          "chunk_id": 11,
          "document_id": "1701.08118",
          "start_idx": 5835,
          "end_idx": 6474
        },
        "page_content": "After these filtering steps, our corpus consists of 541 tweets, none of which are duplicates, contain links or pictures, or are retweets or replies. As a first measurement of the frequency of hate speech in our corpus, we personally annotated them based on our previous expertise. The 541 tweets were split into six parts and each part was annotated by two out of six annotators in order to determine if hate speech was present or not. The annotators were rotated so that each pair of annotators only evaluated one part. Additionally the offensiveness of a tweet was rated on a 6-point Likert scale, the same scale used later in the study.",
        "type": "Document"
      },
      {
        "id": "17ee1f5e-1f9c-4af7-b355-37b59fdee355",
        "metadata": {
          "vector_store_key": "2002.10361-0",
          "chunk_id": 7,
          "document_id": "2002.10361",
          "start_idx": 4447,
          "end_idx": 5240
        },
        "page_content": "By using the Twitter streaming API, we collected the tweets annotated by hate speech labels and their corresponding user profiles in English BIBREF14, BIBREF19, BIBREF20, Italian BIBREF15, Polish BIBREF16, Portuguese BIBREF18, and Spanish BIBREF17. We binarize all tweets' labels (indicating whether a tweet has indications of hate speech), allowing to merge the different label sets and reduce the data sparsity. Whether a tweet is considered hate speech heavily depends on who the speaker is; for example, whether a racial slur is intended as hate speech depends in part on the speaker's race BIBREF14. Therefore, hate speech classifiers may not generalize well across all groups of people, and disparities in the detection offensive speech could lead to bias in content moderation BIBREF21.",
        "type": "Document"
      },
      {
        "id": "5d672edb-7fea-4781-b127-005fcb8b4695",
        "metadata": {
          "vector_store_key": "1811.02906-2",
          "chunk_id": 10,
          "document_id": "1811.02906",
          "start_idx": 5686,
          "end_idx": 6482
        },
        "page_content": "The training data contains 5,008 manually labeled tweets sampled from Twitter from selected accounts that are suspected to contain a high share of offensive language. Manual inspection reveals a high share of political tweets among those labeled as offensive. These tweets range from offending single Twitter users, politicians and parties to degradation of whole social groups such as Muslims, migrants or refugees. The test data contains 3,532 tweets. To create a realistic scenario of truly unseen test data, training and test set are sampled from disjoint user accounts. No standard validation set is provided for the task. To optimize hyper-parameters of our classification models and allow for early stopping to prevent the neural models from overfitting, we created our own validation set.",
        "type": "Document"
      },
      {
        "id": "440d5a40-6a8f-44dc-8769-4da8844bd421",
        "metadata": {
          "vector_store_key": "1701.08118-1",
          "chunk_id": 14,
          "document_id": "1701.08118",
          "start_idx": 7300,
          "end_idx": 7924
        },
        "page_content": "Participants were randomly assigned one of the two surveys. The surveys consisted of 20 tweets presented in a random order. For each tweet, each participant was asked three questions. Depending on the survey, participants were asked (1) to answer (yes/no) if they considered the tweet hate speech, either based on the definition or based on their personal opinion. Afterwards they were asked (2) to answer (yes/no) if the tweet should be banned from Twitter. Participants were finally asked (3) to answer how offensive they thought the tweet was on a 6-point Likert scale from 1 (Not offensive at all) to 6 (Very offensive).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does the model compute the likelihood of executing to the correction semantic denotation?",
    "llm_answer": "The model computes the reconstruction likelihood $p(x|y)$ using an inverse parser that recovers the utterance $x$ from its logical form $y$.  This measures how well the logical form reflects the utterance meaning.\n",
    "context": [
      {
        "id": "78261090-1b8c-4408-aea5-7614e92517f1",
        "metadata": {
          "vector_store_key": "1808.07625-0",
          "chunk_id": 23,
          "document_id": "1808.07625",
          "start_idx": 13903,
          "end_idx": 14597
        },
        "page_content": "The above objective can be thus reduced to:  $$\\hspace*{-9.38945pt}\\mathbb {E}_{q(y|x)} \\log p(x|y) - \\mathbb {E}_{q(y|x)} \\log q(y|x) = \\mathcal {L}(x)$$   (Eq. 27)  where the first term computes the reconstruction likelihood $p(x|y)$ ; and the second term is the entropy of the approximated posterior $q(y|x) $ for regularization. Specifically, we use the semantic parser to compute the approximated posterior $q(y|x)$ . The reconstruction likelihood $p(x|y)$ is computed with an inverse parser which recovers utterance $x$ from its logical form $y$ . We use $p(x|y)$ to measure how well the logical form reflects the utterance meaning; details of the inverse parser are described as follows.",
        "type": "Document"
      },
      {
        "id": "9bde4e81-6250-4962-9423-3c65d56ae94a",
        "metadata": {
          "vector_store_key": "1706.09147-4",
          "chunk_id": 39,
          "document_id": "1706.09147",
          "start_idx": 20782,
          "end_idx": 21404
        },
        "page_content": "In addition, $15.5\\%$ (31/200) were cases where a Wikipedia disambiguation-page was either the correct or predicted entity ( $2.5\\%$ and $14\\%$ , respectively). We considered the rest of the 130 errors as true semantic errors, and analyzed them in-depth. First, we noticed that in $31.5$ % of the true errors (41/130) our model selected an entity that can be understood as a specific ( $6.5$ %) or general (25%) realization of the correct solution. For example, instead of predicting 'Aroma of wine' for a text on the scent and flavor of Turkish wine, the model assigned the mention 'Aroma' with the general 'Odor' entity.",
        "type": "Document"
      },
      {
        "id": "a8f8d359-2ede-4f66-be9e-685d24434e56",
        "metadata": {
          "vector_store_key": "1901.05280-0",
          "chunk_id": 21,
          "document_id": "1901.05280",
          "start_idx": 12595,
          "end_idx": 13546
        },
        "page_content": "The model is trained to optimize the probability INLINEFORM0 of the predicate-argument-relation tuples INLINEFORM1 given the sentence INLINEFORM2 , which can be factorized as: DISPLAYFORM0   where INLINEFORM0 represents the model parameters, and INLINEFORM1 , is the score for the predicate-argument-relation tuple, including predicate score INLINEFORM2 , argument score INLINEFORM3 and relation score INLINEFORM4 . Our model adopts a biaffine scorer for semantic role label prediction, which is implemented as cross-entropy loss. Moreover, our model is trained to minimize the negative likehood of the golden structure INLINEFORM0 : INLINEFORM1 . The score of null labels are enforced into INLINEFORM2 . For predicates and arguments prediction, we train separated scorers ( INLINEFORM3 and INLINEFORM4 ) in parallel fed to the biaffine scorer for predicate and argument predication respectively, which helps to reduce the chance of error propagation.",
        "type": "Document"
      },
      {
        "id": "7013d005-4df1-433f-9478-f59db6932768",
        "metadata": {
          "vector_store_key": "1910.05456-5",
          "chunk_id": 13,
          "document_id": "1910.05456",
          "start_idx": 7205,
          "end_idx": 7890
        },
        "page_content": "The probability of a character $y_t$ at time step $t$ is computed as a sum of the probability of $y_t$ given by the decoder and the probability of copying $y_t$, weighted by the probabilities of generating and copying: $p_{\\textrm {dec}}(y_t)$ is calculated as an LSTM update and a projection of the decoder state to the vocabulary, followed by a softmax function. $p_{\\textrm {copy}}(y_t)$ corresponds to the attention weights for each input character. The model computes the probability $\\alpha $ with which it generates a new output character as for context vector $c_t$, decoder state $s_t$, embedding of the last output $y_{t-1}$, weights $w_c$, $w_s$, $w_y$, and bias vector $b$.",
        "type": "Document"
      },
      {
        "id": "0c42c397-2e8c-4f11-b5ca-6ce3e490e0bf",
        "metadata": {
          "vector_store_key": "1904.09708-1",
          "chunk_id": 19,
          "document_id": "1904.09708",
          "start_idx": 10352,
          "end_idx": 11216
        },
        "page_content": "The syntactic information (or \"annotations\" BIBREF12 ) of each word $x_j$ is determined by the two vectors $\\overrightarrow{h_{j-1}}$ , $\\overleftarrow{h_{j+1}}$ corresponding to the words surrounding it:  $$h_j = [\\overrightarrow{h_{j-1}};\\overleftarrow{h_{j+1}}]$$   (Eq. 9)  In all experiments, we used a bidirectional Long Short-Term Memory (LSTM) for this purpose. Note that because there is no sequence information in the semantic representations, all of the information required to parse (i.e. align) the input sequence correctly (e.g. phrase structure, modifying relationships, etc.) must be encoded by the biRNN. The decoder models the conditional probability of each target word given the input and the previous targets: $p(y_i | y_1, y_2, ..., y_{i-1}, \\mathbf {x})$ , where $y_i$ is the target translation and $\\mathbf {x}$ is the whole input sequence.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are state of the art methods authors compare their work with? ",
    "llm_answer": "Supervised systems learning CRF, SVM, Maximum Entropy or Averaged Perceptron models; and other state of the art methods.\n",
    "context": [
      {
        "id": "3f8abf9e-1146-4e91-9585-cb954eddcbe5",
        "metadata": {
          "vector_store_key": "1906.10551-8",
          "chunk_id": 9,
          "document_id": "1906.10551",
          "start_idx": 5173,
          "end_idx": 5825
        },
        "page_content": "These include simple synonym replacements, rule-based substitutions or word order permutations. In 2016, Potthast et al. BIBREF10 presented the first large-scale evaluation of three AO approaches that aim to attack 44 AV methods, which were submitted to the PAN-AV competitions during 2013-2015 BIBREF11 , BIBREF5 , BIBREF12 . One of their findings was that even basic AO approaches have a significant impact on many AV methods. More precisely, the best-performing AO approach was able to flip on average INLINEFORM0 % of an authorship verifier\u2019s decisions towards choosing N (\u201cdifferent author\u201d), while in fact Y (\u201csame author\u201d) was correct BIBREF10 .",
        "type": "Document"
      },
      {
        "id": "d099a987-e303-4026-a4b9-3c500ed1a4da",
        "metadata": {
          "vector_store_key": "1808.08850-4",
          "chunk_id": 11,
          "document_id": "1808.08850",
          "start_idx": 6584,
          "end_idx": 7312
        },
        "page_content": "With respect to the methods used for SBD, they mostly rely on statistical/neural machine translation BIBREF18 , BIBREF27 , language models BIBREF9 , BIBREF16 , BIBREF22 , BIBREF6 , conditional random fields BIBREF21 , BIBREF28 , BIBREF23 and deep neural networks BIBREF29 , BIBREF20 , BIBREF13 . Despite their differences in features and/or methodology, almost all previous cited research share a common element; the evaluation methodology. Metrics as Precision, Recall, F1-score, Classification Error Rate and Slot Error Rate (SER) are used to evaluate the proposed system against one reference. As discussed in Section SECREF1 , further NLP tasks rely on the result of SBD, meaning that is crucial to have a good segmentation.",
        "type": "Document"
      },
      {
        "id": "18eced3a-fbf6-40a7-b546-fe11d4346eb5",
        "metadata": {
          "vector_store_key": "1701.09123-9",
          "chunk_id": 27,
          "document_id": "1701.09123",
          "start_idx": 14490,
          "end_idx": 15257
        },
        "page_content": "Therefore, we will summarize those approaches that are most relevant to our work, especially those we will directly compared with in Section SECREF4 . Since CoNLL shared tasks, the most competitive approaches have been supervised systems learning CRF, SVM, Maximum Entropy or Averaged Perceptron models. In any case, while the machine learning method is important, it has also been demonstrated that good performance might largely be due to the feature set used BIBREF45 . Table TABREF13 provides an overview of the features used by previous best scoring approaches for each of the five languages we address in this paper. Traditionally, local features have included contextual and orthographic information, affixes, character-based features, prediction history, etc.",
        "type": "Document"
      },
      {
        "id": "f6397b49-baf9-4327-a11d-b7a9d35f947c",
        "metadata": {
          "vector_store_key": "1906.10551-8",
          "chunk_id": 2,
          "document_id": "1906.10551",
          "start_idx": 1022,
          "end_idx": 1707
        },
        "page_content": "In the past two decades, researchers from different fields including linguistics, psychology, computer science and mathematics proposed numerous techniques and concepts that aim to solve the AV task. Probably due to the interdisciplinary nature of this research field, AV approaches were becoming more and more diverse, as can be seen in the respective literature. In 2013, for example, Veenman and Li BIBREF2 presented an AV method based on compression, which has its roots in the field of information theory. In 2015, Bagnall BIBREF3 introduced the first deep learning approach that makes use of language modeling, an important key concept in statistical natural language processing.",
        "type": "Document"
      },
      {
        "id": "11bbd522-28ef-4580-9084-84720bfb9758",
        "metadata": {
          "vector_store_key": "2001.06888-0",
          "chunk_id": 42,
          "document_id": "2001.06888",
          "start_idx": 22556,
          "end_idx": 22770
        },
        "page_content": "Evaluation results of our proposed model compared to other state of the art methods show its superiority to these methods in overall while in two categories (Person and Miscellaneous) our model outperformed others.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much improvement do they get?",
    "llm_answer": "The context provides various improvements, including:  a 50% enhancement in decision tree accuracy,  significant gains for Naive Bayes and SVM, improvements for kNN, a 2.1 point gain in number handling, a 5.4 EM improvement in multi-span questions, a 1.5 EM improvement in single-span questions, a 12.2 absolute improvement in EM and 2.3 in F1, and a 29.7 EM and 15.1 F1 improvement.\n",
    "context": [
      {
        "id": "cae387d5-b5b1-44de-a443-42d90d1eb1c2",
        "metadata": {
          "vector_store_key": "2003.06279-7",
          "chunk_id": 32,
          "document_id": "2003.06279",
          "start_idx": 19100,
          "end_idx": 19861
        },
        "page_content": "The relative improvement in performance is given by $\\Gamma _+{(p)}/\\Gamma _0$, where $\\Gamma _+{(p)}$ is the accuracy rate obtained when $p\\%$ additional edges are included and $\\Gamma _0 = \\Gamma _+{(p=0)}$, i.e. $\\Gamma _0$ is the accuracy rate measured from the traditional co-occurrence model. We only show the highest relative improvements in performance for each classifier. In our analysis, we considered also samples of text with distinct length, since the performance of network-based methods is sensitive to text length BIBREF34. In this figure, we considered samples comprising $w=\\lbrace 1.0, 2.5, 5.0, 10.0\\rbrace $ thousand words. The results obtained for GloVe show that the highest relative improvements in performance occur for decision trees.",
        "type": "Document"
      },
      {
        "id": "4f72307a-0a25-4c37-9a12-004e692be9a5",
        "metadata": {
          "vector_store_key": "2003.06279-2",
          "chunk_id": 33,
          "document_id": "2003.06279",
          "start_idx": 19861,
          "end_idx": 20566
        },
        "page_content": "The results obtained for GloVe show that the highest relative improvements in performance occur for decision trees. This is apparent specially for the shortest samples. For $w=1,000$ words, the decision tree accuracy is enhanced by a factor of almost 50% when $p=20\\%$. An excellent gain in performance is also observed for both Naive Bayes and SVM classifiers, when $p=18\\%$ and $p=12\\%$, respectively. When $w=2,500$ words, the highest improvements was observed for the decision tree algorithm. A minor improvement was observed for the kNN method. A similar behavior occurred for $w=5,000$ words. Interestingly, SVM seems to benefit from the use of additional edges when larger documents are considered.",
        "type": "Document"
      },
      {
        "id": "6e9156a1-1bd2-4904-9fe1-0957c0410a9d",
        "metadata": {
          "vector_store_key": "1909.13375-3",
          "chunk_id": 43,
          "document_id": "1909.13375",
          "start_idx": 22785,
          "end_idx": 23407
        },
        "page_content": "Not using the simple preprocessing from Section SECREF17 resulted in a 2.5 point decrease in both EM and F1. The numeric questions were the most affected, with their performance dropping by 3.5 points. Given that number questions make up about 61% of the dataset, we can deduce that our improved number handling is responsible for about a 2.1 point gain, while the rest could be be attributed to the improved Wikipedia parsing. Although NER span cleaning (Section SECREF23) affected only 3% of the multi-span questions, it provided a solid improvement of 5.4 EM in multi-span questions and 1.5 EM in single-span questions.",
        "type": "Document"
      },
      {
        "id": "050432d2-b325-4282-ae97-1513abbfedc7",
        "metadata": {
          "vector_store_key": "2003.06279-7",
          "chunk_id": 36,
          "document_id": "2003.06279",
          "start_idx": 21531,
          "end_idx": 22144
        },
        "page_content": "While Figures FIGREF14 \u2013 FIGREF16 show the relative behavior in the accuracy, it still interesting to observe the absolute accuracy rate obtained with the classifiers. In Table TABREF17, we show the best accuracy rate (i.e. $\\max \\Gamma _+ = \\max _p \\Gamma _+(p)$) for GloVe. We also show the average difference in performance ($\\langle \\Gamma _+ - \\Gamma _0 \\rangle $) and the total number of cases in which an improvement in performance was observed ($N_+$). $N_+$ ranges in the interval $0 \\le N_+ \\le 20$. Table TABREF17 summarizes the results obtained for $w = \\lbrace 1.0, 5.0, 10.0\\rbrace $ thousand words.",
        "type": "Document"
      },
      {
        "id": "2e7cfe41-b41c-475c-83f6-e7d39903180d",
        "metadata": {
          "vector_store_key": "1909.13375-4",
          "chunk_id": 40,
          "document_id": "1909.13375",
          "start_idx": 21052,
          "end_idx": 21792
        },
        "page_content": "While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1. All that, while keeping in mind we compare a base model to a large model (for reference, note the 8 point improvement between MTMSNbase and MTMSNlarge in both EM and F1). Our best model, large-squad, exhibits a huge improvement of 29.7 in EM and 15.1 in F1 compared to MTMSNlarge. When comparing single-span performance, our best model exhibits slightly better results, but it should be noted that it retains the single-span heads from NABERT+, while in MTMSN they have one head to predict both single-span and multi-span answers.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which languages do they test on?",
    "llm_answer": "English, French, German, Spanish, Russian, Italian, Bengali, Hindi, Telegu, Chinese.\n",
    "context": [
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      },
      {
        "id": "24e7a576-ab3d-4f0a-8326-f14fe7d7f139",
        "metadata": {
          "vector_store_key": "1911.03894-4",
          "chunk_id": 26,
          "document_id": "1911.03894",
          "start_idx": 15178,
          "end_idx": 16033
        },
        "page_content": "We also evaluate our model on the Natural Language Inference (NLI) task, using the French part of the XNLI dataset BIBREF50. NLI consists in predicting whether a hypothesis sentence is entailed, neutral or contradicts a premise sentence. The XNLI dataset is the extension of the Multi-Genre NLI (MultiNLI) corpus BIBREF51 to 15 languages by translating the validation and test sets manually into each of those languages. The English training set is also machine translated for all languages. The dataset is composed of 122k train, 2490 valid and 5010 test examples. As usual, NLI performance is evaluated using accuracy. To evaluate a model on a language other than English (such as French), we consider the two following settings: TRANSLATE-TEST: The French test set is machine translated into English, and then used with an English classification model.",
        "type": "Document"
      },
      {
        "id": "fdaab8f5-0f71-47b8-8671-bdd41a9285b0",
        "metadata": {
          "vector_store_key": "1804.08186-3",
          "chunk_id": 247,
          "document_id": "1804.08186",
          "start_idx": 140050,
          "end_idx": 140712
        },
        "page_content": "(2) \u201cunseen\u201d languages that appear in the test data but not in training data; (3) short target sentences; and (4) (sometimes inconsistent) transliteration into Latin text. Their solution to this task is to take advantage of a domain-specific feature: they assume that the name of the language that they are extracting must appear in the document containing the IGT, and hence treat this as a co-reference resolution problem. They report that this approach significantly outperforms the text-based approach in this particular problem setting. An interesting area to explore is the trade-off between the number of languages supported and the accuracy per-language.",
        "type": "Document"
      },
      {
        "id": "56fe66b6-5e7f-4d68-a3e5-95871fe6ebb1",
        "metadata": {
          "vector_store_key": "1911.03894-1",
          "chunk_id": 27,
          "document_id": "1911.03894",
          "start_idx": 16033,
          "end_idx": 16717
        },
        "page_content": "To evaluate a model on a language other than English (such as French), we consider the two following settings: TRANSLATE-TEST: The French test set is machine translated into English, and then used with an English classification model. This setting provides a reasonable, although imperfect, way to circumvent the fact that no such data set exists for French, and results in very strong baseline scores. TRANSLATE-TRAIN: The French model is fine-tuned on the machine-translated English training set and then evaluated on the French test set. This is the setting that we used for CamemBERT. For the TRANSLATE-TEST setting, we report results of the English RoBERTa to act as a reference.",
        "type": "Document"
      },
      {
        "id": "c28fe057-2cfc-4978-9c0a-84fd286d4b98",
        "metadata": {
          "vector_store_key": "1612.06897-3",
          "chunk_id": 39,
          "document_id": "1612.06897",
          "start_idx": 20315,
          "end_idx": 20488
        },
        "page_content": "We presented results on two diverse language pairs German $\\rightarrow $ English and Chinese $\\rightarrow $ English (usually very challenging pairs for machine translation).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What limitations are mentioned?",
    "llm_answer": "Limited slot value sets for certain attributes, limited combinations of slot values supported by real-world services, and maximum length constraints for descriptions and video/sentence content.  Syntactic agreement across object relative clauses is also identified as a potential limitation for LSTM-LMs.\n",
    "context": [
      {
        "id": "46ef86e6-c75a-4cd0-b930-e855c4d48743",
        "metadata": {
          "vector_store_key": "2002.01359-0",
          "chunk_id": 16,
          "document_id": "2002.01359",
          "start_idx": 9091,
          "end_idx": 9830
        },
        "page_content": "The schema contains details like the name of the service, the list of tasks supported by the service (intents) and the attributes of the entities used by the service (slots). The schema also contains natural language descriptions of the service, intents and slots which can be used for developing models which can condition their predictions on the schema. To reflect the constraints present in real-world services and APIs, we impose a few constraints on the data. Our dataset does not expose the set of all possible values for certain slots. Having such a list is impractical for slots like date or time because they have infinitely many possible values or for slots like movie or song names, for which new values are periodically added.",
        "type": "Document"
      },
      {
        "id": "cfcb66e1-1f4e-46cf-925a-df9eaf6044d8",
        "metadata": {
          "vector_store_key": "1909.05855-2",
          "chunk_id": 14,
          "document_id": "1909.05855",
          "start_idx": 8300,
          "end_idx": 9018
        },
        "page_content": "Second, real-world services can only be invoked with a limited number of slot combinations: e.g. restaurant reservation APIs do not let the user search for restaurants by date without specifying a location. However, existing datasets simplistically allow service calls with any given combination of slot values, thus giving rise to flows unsupported by actual services or APIs. As in Figure FIGREF7, the different service calls supported by a service are listed as intents. Each intent specifies a set of required slots and the system is not allowed to call this intent without specifying values for these required slots. Each intent also lists a set of optional slots with default values, which the user can override.",
        "type": "Document"
      },
      {
        "id": "14487e93-e21d-42da-b2ec-16ff8d86455a",
        "metadata": {
          "vector_store_key": "1606.04631-4",
          "chunk_id": 20,
          "document_id": "1606.04631",
          "start_idx": 11331,
          "end_idx": 11959
        },
        "page_content": "We note that over 99% of the descriptions in MSVD and COCO 2014 contain no more than 40 words, and in BIBREF19 , Venugopalan et al. pointed out that 94% of the YouTube training videos satisfy our maximum length limit. To ensure sufficient visual content, we adopt two ways to truncate the videos and sentences adaptively when the sum of the number of frames and words exceed the limit. If the number of words is within 40, we arbitrarily truncate the frames to satisfy the maximum length. When the length of sentence is more than 40, we discard the words that beyond the length and take video frames with a maximum number of 40.",
        "type": "Document"
      },
      {
        "id": "cd7a1837-8e0f-4ff4-9dd6-4d795ef192a3",
        "metadata": {
          "vector_store_key": "1606.04631-2",
          "chunk_id": 32,
          "document_id": "1606.04631",
          "start_idx": 18254,
          "end_idx": 18394
        },
        "page_content": "In the future we will make more effort to fix these limitations and exploit the linguistic domain knowledge in visual content understanding.",
        "type": "Document"
      },
      {
        "id": "3c0460df-7cd9-4f2e-b803-875be6672a0c",
        "metadata": {
          "vector_store_key": "2004.02451-6",
          "chunk_id": 8,
          "document_id": "2004.02451",
          "start_idx": 4138,
          "end_idx": 4815
        },
        "page_content": "The latter part of this paper is a detailed analysis of the trained models and introduced losses. Our second question is about the true limitation of LSTM-LMs: are there still any syntactic constructions that the models cannot handle robustly even with our direct learning signals? This question can be seen as a fine-grained one raised by BIBREF5 with a stronger tool and improved evaluation metric. Among tested constructions, we find that syntactic agreement across an object relative clause (RC) is challenging. To inspect whether this is due to the architectural limitation, we train another LM on a dataset, on which we unnaturally augment sentences involving object RCs.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the baselines?",
    "llm_answer": "Neural baselines, ANRES_Sing_R, ANRES_Sing_Z, ANRES_Seq_R, ANRES_Seq_Z, an i-vector system, and an x-vector system.\n",
    "context": [
      {
        "id": "fa88df39-033d-4e0f-891b-162e0f9b0424",
        "metadata": {
          "vector_store_key": "1709.08299-4",
          "chunk_id": 19,
          "document_id": "1709.08299",
          "start_idx": 11042,
          "end_idx": 11836
        },
        "page_content": "The neural baselines are built by the following steps. We first use the shared training data to build a general systems, and choose the best performing model (in terms of cloze validation set) as baseline. Use User Query validation data for further tuning the systems with 10-fold cross-validations. Increase dropout rate BIBREF9 to 0.5 for preventing over-fitting issue. All baseline systems are chosen according to the performance of the validation set. The participant system results are given in Table 2 and 3 . As we can see that two neural baselines are competitive among participant systems and AoA Reader successfully outperform AS Reader and all participant systems in single model condition, which proves that it is a strong baseline system even without further fine-tuning procedure.",
        "type": "Document"
      },
      {
        "id": "348b5dbb-b442-4f57-9f6e-c2038d069875",
        "metadata": {
          "vector_store_key": "1709.08299-4",
          "chunk_id": 16,
          "document_id": "1709.08299",
          "start_idx": 9232,
          "end_idx": 9901
        },
        "page_content": "The paragraph should be read carefully and judged whether appropriate for asking questions No more than 5 questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models.",
        "type": "Document"
      },
      {
        "id": "64beee2a-a1a9-454b-917e-917a34317fa7",
        "metadata": {
          "vector_store_key": "2004.01878-3",
          "chunk_id": 27,
          "document_id": "2004.01878",
          "start_idx": 14569,
          "end_idx": 15289
        },
        "page_content": "In summary, the following four baselines are designed: ANRES_Sing_R: randomly initializing the states for each single trading day. ANRES_Sing_Z: initializing the states as zeros for each single trading day. ANRES_Seq_R: randomly initializing the first states for each trading sequence only. ANRES_Seq_Z: initializing the first states as zeros for each trading sequence only. Development set results on predicting S&P 500 index are shown in Table TABREF13. We can see that modeling recurrent value sequences performs better than treating each trading day separately, which shows that modeling trading sequences can capture the correlations between trading days and the non-linear compositional effects of multiple events.",
        "type": "Document"
      },
      {
        "id": "e7065ee9-867c-433d-ac3c-08844aac0090",
        "metadata": {
          "vector_store_key": "1904.01608-7",
          "chunk_id": 38,
          "document_id": "1904.01608",
          "start_idx": 21241,
          "end_idx": 21885
        },
        "page_content": "On this dataset, the best baseline is the neural baseline with addition of ELMo contextual vectors achieving an F1 score of 82.6 followed by BIBREF7 , which is expected because neural models generally achieve higher gains when more training data is available and because BIBREF7 was not designed with the SciCite dataset in mind. The breakdown of results by intent on ACL-ARC and SciCite datasets is respectively shown in Tables 5 and 6 . Generally we observe that results on categories with more number of instances are higher. For example on ACL-ARC, the results on the Background category are the highest as this category is the most common.",
        "type": "Document"
      },
      {
        "id": "02bef31f-a2b5-4b89-8063-62dc0b0f91eb",
        "metadata": {
          "vector_store_key": "1911.01799-4",
          "chunk_id": 19,
          "document_id": "1911.01799",
          "start_idx": 11188,
          "end_idx": 11946
        },
        "page_content": "Two state-of-the-art baseline systems were built following the Kaldi SITW recipe BIBREF22: an i-vector system BIBREF3 and an x-vector system BIBREF10. For the i-vector system, the acoustic feature involved 24-dimensional MFCCs plus the log energy, augmented by the first- and second-order derivatives. We also applied the cepstral mean normalization (CMN) and the energy-based voice active detection (VAD). The universal background model (UBM) consisted of $2,048$ Gaussian components, and the dimensionality of the i-vector space was 400. LDA was applied to reduce the dimensionality of the i-vectors to 150. The PLDA model was used for scoring BIBREF4. For the x-vector system, the feature-learning component was a 5-layer time-delay neural network (TDNN).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What semantic features help in detecting whether a piece of text is genuine or generated? of ",
    "llm_answer": "Semantic inconsistencies in different structural parts of a satirical news tweet, captured by decomposing the syntactic tree and using word vectors.\n",
    "context": [
      {
        "id": "9c40843f-3da5-4900-9389-7db31266d4c7",
        "metadata": {
          "vector_store_key": "2002.03438-2",
          "chunk_id": 3,
          "document_id": "2002.03438",
          "start_idx": 1938,
          "end_idx": 2659
        },
        "page_content": "Second, we consider not just a setting with a specific language model with given performance metrics, but rather consider a universal setting where we take a generic view of language models as empirical maximum likelihood $k$-order Markov approximations of stationary, ergodic random processes. Results on estimation of such random processes are revisited in the context of the error probability, using a conjectured extension of the reverse Pinsker inequality. In closing, we discuss how the semantics of generated text may be a form of side information in detection. Consider a language $L$ like English, which has tokens drawn from a finite alphabet $\\mathcal {A}$; tokens can be letters, words, or other such symbols.",
        "type": "Document"
      },
      {
        "id": "398aa8b3-4826-479b-83da-74ca4be7a9e8",
        "metadata": {
          "vector_store_key": "2004.03788-1",
          "chunk_id": 9,
          "document_id": "2004.03788",
          "start_idx": 5492,
          "end_idx": 6288
        },
        "page_content": "The detection of a certain type of sarcasm which contracts positive sentiment with a negative situation by analyzing the sentence pattern with a bootstrapped learning was also discussed BIBREF20. Although word level statistical features are widely used, with advanced word representations and state-of-the-art part-of-speech tagging and named entity recognition model, we observe that semantic features are more important than word level statistical features to model performance. Thus, we decompose the syntactic tree and use word vectors to more precisely capture the semantic inconsistencies in different structural parts of a satirical news tweet. Recently, with the success of deep learning in NLP, many researchers attempted to detect fake news with end-to-end neural nets based approaches.",
        "type": "Document"
      },
      {
        "id": "3bddc073-a8d8-4685-8b63-467ebd3639b4",
        "metadata": {
          "vector_store_key": "1804.08186-3",
          "chunk_id": 4,
          "document_id": "1804.08186",
          "start_idx": 2320,
          "end_idx": 3054
        },
        "page_content": "The ability to accurately detect the language that a document is written in is an enabling technology that increases accessibility of data and has a wide variety of applications. For example, presenting information in a user's native language has been found to be a critical factor in attracting website visitors BIBREF3 . Text processing techniques developed in natural language processing and Information Retrieval (\u201cIR\u201d) generally presuppose that the language of the input text is known, and many techniques assume that all documents are in the same language. In order to apply text processing techniques to real-world data, automatic is used to ensure that only documents in relevant languages are subjected to further processing.",
        "type": "Document"
      },
      {
        "id": "ab674a67-5069-4697-b94d-f23217a9a6f1",
        "metadata": {
          "vector_store_key": "1908.08419-2",
          "chunk_id": 5,
          "document_id": "1908.08419",
          "start_idx": 3007,
          "end_idx": 3728
        },
        "page_content": "Specifically, we combine information branch and gated neural network to determine if the segment is a legal word, i.e., word score. Meanwhile, we use the hidden layer output of the long short-term memory (LSTM) BIBREF11 to find out how the word is linked to its surroundings, i.e., link score. The final decision on the selection of labeling samples is made by calculating the average of word and link scores on the whole segmented sentence, i.e., sequence score. Besides, to capture coherence over characters, we additionally add K-means clustering features to the input of CRF-based word segmenter. To sum up, the main contributions of our work are summarized as follows: The rest of this paper is organized as follows.",
        "type": "Document"
      },
      {
        "id": "58b007fc-804b-4a22-9207-4ef619832e7e",
        "metadata": {
          "vector_store_key": "1910.09399-8",
          "chunk_id": 49,
          "document_id": "1910.09399",
          "start_idx": 26314,
          "end_idx": 27061
        },
        "page_content": "This goal mainly achieved though a two-step process which first generates images matching to the \u201cactions\u201d of the texts, followed by a mapping or alignment procedure to ensure that images are coherent in the temporal order. black In the following, we will introduce how these GAN frameworks evolve for text-to-image synthesis, and will also review some typical methods of each category. black Semantic relevance is one the of most important criteria of the text-to-image synthesis. For most GNAs discussed in this survey, they are required to generate images semantically related to the text descriptions. However, the semantic relevance is a rather subjective measure, and images are inherently rich in terms of its semantics and interpretations.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is the assumption that natural language is stationary and ergodic valid?",
    "llm_answer": "Manning and Sch\u00fctze argue that language can be modeled as a stationary, ergodic random process, an assumption the text follows.\n",
    "context": [
      {
        "id": "6939b4e6-29fa-4004-becc-41ad4efe80b8",
        "metadata": {
          "vector_store_key": "2002.03438-4",
          "chunk_id": 16,
          "document_id": "2002.03438",
          "start_idx": 9055,
          "end_idx": 9803
        },
        "page_content": "Moreover, given the diversity of language production, we assume this stationary ergodic random process with finite alphabet $\\mathcal {A}$ denoted $X = \\lbrace X_i, -\\infty < i < \\infty \\rbrace $ is non-null in the sense that always $P(x_{-m}^{-1}) > 0$ and This is sometimes called the smoothing requirement. We further introduce an additional property of random processes that we assume for language $L$. We define the continuity rate of the process $X$ as: We further let $\\gamma = \\sum _{k=1}^{\\infty } \\gamma (k)$, and If $\\gamma < \\infty $, then the process has summable continuity rate. These specific technical notions of smoothing and continuity are taken from the literature on estimation of stationary, ergodic random processes BIBREF30.",
        "type": "Document"
      },
      {
        "id": "b9ff99cf-cf27-4fb4-bc1d-0f3af50396a5",
        "metadata": {
          "vector_store_key": "2002.03438-4",
          "chunk_id": 15,
          "document_id": "2002.03438",
          "start_idx": 8882,
          "end_idx": 9543
        },
        "page_content": "We specifically consider the empirical ML model among the class of models that are $k$-order Markov approximations of language $L$, which is simply the empirical plug-in estimate. Manning and Sch\u00fctze argue that, even though not quite correct, language text can be modeled as stationary, ergodic random processes BIBREF29, an assumption that we follow. Moreover, given the diversity of language production, we assume this stationary ergodic random process with finite alphabet $\\mathcal {A}$ denoted $X = \\lbrace X_i, -\\infty < i < \\infty \\rbrace $ is non-null in the sense that always $P(x_{-m}^{-1}) > 0$ and This is sometimes called the smoothing requirement.",
        "type": "Document"
      },
      {
        "id": "4755f0fd-29d4-40fa-89f6-e9db9fe47902",
        "metadata": {
          "vector_store_key": "2002.03438-4",
          "chunk_id": 17,
          "document_id": "2002.03438",
          "start_idx": 9803,
          "end_idx": 10632
        },
        "page_content": "These specific technical notions of smoothing and continuity are taken from the literature on estimation of stationary, ergodic random processes BIBREF30. As such, the hypothesis test we aim to consider here is between a non-null, stationary, ergodic process with summable continuity rate (genuine language) and its empirical $k$-order Markov approximation based on training data (language model output). We think of the setting where the language model is trained on data with many tokens, a sequence of very long length $m$. For example, the CTRL language model was trained using 140 GB of text BIBREF7. We think of the Markov order $k$ as a large value and so the family of empirical $k$-order Markov approximations encompasses the class of neural language models like GPT-2 and CTRL, which are a fortiori Markov in structure.",
        "type": "Document"
      },
      {
        "id": "8f42aa03-889d-4c5e-a1ed-9e02154845e3",
        "metadata": {
          "vector_store_key": "1807.03367-1",
          "chunk_id": 54,
          "document_id": "1807.03367",
          "start_idx": 31377,
          "end_idx": 32219
        },
        "page_content": "However, their level of accuracy is slightly below the baseline of communicating a single observation, indicating that these models only learn to ground utterances in a single landmark observation. paragraph4 0.1ex plus0.1ex minus.1ex-1em Better grounding of generated utterances We analyze natural language samples in Table TABREF38 , and confirm that, unlike human utterances, the generated utterances are talking about the observed landmarks. This observation explains why the generated utterances obtain higher localization accuracy. The current language models are most successful when conditioned on a single landmark observation; We show in Appendix UID43 that performance quickly deteriorates when the model is conditioned on more observations, suggesting that it can not produce natural language utterances about multiple time steps.",
        "type": "Document"
      },
      {
        "id": "c2344818-80f4-40e6-922d-bebc469b1f90",
        "metadata": {
          "vector_store_key": "2002.08899-3",
          "chunk_id": 5,
          "document_id": "2002.08899",
          "start_idx": 2907,
          "end_idx": 3616
        },
        "page_content": "It is possible, for example, to know that the syntax for an utterance is a noun phrase that is composed of a determiner and a noun, followed by a verb phrase that is composed of a verb. One can know this without knowing the words. This also means that there are aspects of a word's meaning that the syntax does not determine; by definition, these aspects are invariant to word arrangement. In a natural language translation setting, suppose that an input word corresponds to a set of output tokens independently of its context. Even though this information might be useful to determine the syntax of the input utterance in the first place, the syntax does not determine this knowledge at all (by supposition).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which models do they try out?",
    "llm_answer": "The models mentioned are:  models, SVMs (SVM2 and SVM2-PV).\n",
    "context": [
      {
        "id": "c84c325e-5fb2-4708-89ca-f70d8b54bc71",
        "metadata": {
          "vector_store_key": "1908.05434-9",
          "chunk_id": 39,
          "document_id": "1908.05434",
          "start_idx": 20900,
          "end_idx": 21637
        },
        "page_content": "Below are the top three samples that the model considers likely: [itemsep=0pt] \u201camazing reviewed crystal only here till fri book now please check our site for the services the girls provide all updates specials photos rates reviews njfantasygirls ...look who s back amazing reviewed model samantha...brand new spinner jessica special rate today 250 hr 21 5 4 120 34b total gfe total anything goes no limits...\u201d \u201c2 hot toght 18y o spinners 4 amazing providers today specials...\u201d \u201casian college girl is visiting bellevue service type escort hair color brown eyes brown age 23 height 5 4 body type slim cup size c cup ethnicity asian service type escort i am here for you settle men i am a tiny asian girl who is waiting for a gentlemen...\u201d",
        "type": "Document"
      },
      {
        "id": "0ab61107-67bf-493c-aa5f-199d370caaa3",
        "metadata": {
          "vector_store_key": "1703.04617-2",
          "chunk_id": 26,
          "document_id": "1703.04617",
          "start_idx": 14460,
          "end_idx": 15206
        },
        "page_content": "The previous models are often trained for all questions without explicitly discriminating different question types; however, for a target question, both the common features shared by all questions and the specific features for a specific type of question are further considered in this paper, as they could potentially obey different distributions. In this paper we further explicitly model different types of questions in the end-to-end training. We start from a simple way to first analyze the word frequency of all questions, and obtain top-10 most frequent question types: what, how, who, when, which, where, why, be, whose, and whom, in which be stands for the questions beginning with different forms of the word be such as is, am, and are.",
        "type": "Document"
      },
      {
        "id": "3d80e22b-95de-45ee-854b-6dd9c24fc625",
        "metadata": {
          "vector_store_key": "1709.02271-3",
          "chunk_id": 27,
          "document_id": "1709.02271",
          "start_idx": 15339,
          "end_idx": 16061
        },
        "page_content": "To confirm that our models generalize, we pick the best models from the baseline-dataset experiments and evaluate on the novel-50 and IMDB62 datasets. For novel-50, the chunking size applied is 2000-word as per the baseline-dataset experiment results, and for IMDB62, texts are not chunked (i.e., we feed the models with the original reviews directly). For model comparison, we also run the SVMs (i.e., SVM2 and SVM2-PV) used in the baseline-dataset experiment. All the experiments conducted here are multi-class classification with macro-averaged F1 evaluation. Model configurations. Following F15, we perform 5-fold cross-validation. The embedding sizes are tuned on novel-9 (multi-class condition): 50 for char-bigrams;",
        "type": "Document"
      },
      {
        "id": "10d964d1-649c-4a21-a067-2a49fad2091f",
        "metadata": {
          "vector_store_key": "1711.06351-2",
          "chunk_id": 46,
          "document_id": "1711.06351",
          "start_idx": 24789,
          "end_idx": 25505
        },
        "page_content": "Figure 3 shows five novel questions that were sampled from the model, across four different game contexts. Questions were produced by taking five weighted samples from the set of programs produced in Section \"Optimization\" for approximate inference, with weights determined by their energy (Eq. 14 ). To ensure novelty, samples were rejected if they were equivalent to any human question in the training data set or to an already sampled question. Equivalence between any two questions was determined by the mutual information of their answer distributions (i.e., their partitions over possible hypotheses), and or if the programs differed only through their arguments (e.g. (size Blue) is equivalent to (size Red)).",
        "type": "Document"
      },
      {
        "id": "951de0ef-328c-4f84-83e7-f3d72ab2203d",
        "metadata": {
          "vector_store_key": "1909.00154-1",
          "chunk_id": 38,
          "document_id": "1909.00154",
          "start_idx": 20824,
          "end_idx": 21499
        },
        "page_content": "Encode choice models for train, development and test sets using the learned embeddings Estimate choice model accordingly using its train set Evaluate the new model using the test set Since there is stochasticity in the embeddings training model, we will repeat the above multiple times, for the different experiments in the paper (and report the respective mean and standard deviation statistics). Whenever we want to analyse a particular model (e.g. to check the coefficients of a choice model), we select the one with the highest likelihood at the development set (i.e. in practice, its out-of-sample generalization performance), and report its performance on the test set.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the competing models?",
    "llm_answer": "ESIM, 600D MIMN, 600D MIMN-memory, LSTMN, ESIM, Bilateral Multi-Perspective Matching Model, Enhanced Sequential Inference Model, Decomposable Attention Model, KIM, BERT (pretrained on Wikipedia and BookCorpus), BERT (trained with SNLI and MultiNLI).\n",
    "context": [
      {
        "id": "07b84dd0-f246-4c86-b216-a68029541a1a",
        "metadata": {
          "vector_store_key": "2002.05058-5",
          "chunk_id": 53,
          "document_id": "2002.05058",
          "start_idx": 30143,
          "end_idx": 30900
        },
        "page_content": "We evaluate these model variants on the Dailydialog dataset. Results are presented in Table 5. We can see that comparison-based evaluation is very effective as our model correlates much better than adversarial evaluator. The tie option is also very important as it can prevent the comparative evaluator from making uncertain decision and model the inductive bias that samples generated by the same model are generally of similar quality, which may help our model generalize better. As for different sources of training examples, we find that human preference annotation is the most important, which is not surprising. In addition, we find that the proposed weak supervision also helps, but is of smaller relative importance compared with strong supervision.",
        "type": "Document"
      },
      {
        "id": "724635fa-e20b-42d6-8825-7e5fbfbe5c95",
        "metadata": {
          "vector_store_key": "1901.02222-3",
          "chunk_id": 26,
          "document_id": "1901.02222",
          "start_idx": 15084,
          "end_idx": 15802
        },
        "page_content": "Besides above models, following variants of our model are designed for comparing: ESIM We choose the ESIM model as our baseline. It mixes all the matching feature together in the matching layer and then infers the matching result in a single-turn with a BiLSTM. 600D MIMN: This is our main model described in section SECREF3 . 600D MIMN-memory: This model removes the memory component. The motivation of this experiment is to verify whether the multiple turns inference can acquire more sufficient information than one-pass inference. In this model, we process one matching feature in one iteration. The three matching features are encoded by INLINEFORM0 in multi-turns iteratively without previous memory information.",
        "type": "Document"
      },
      {
        "id": "49603db5-bac7-4035-9ffd-070c82e78407",
        "metadata": {
          "vector_store_key": "1906.03338-2",
          "chunk_id": 6,
          "document_id": "1906.03338",
          "start_idx": 3141,
          "end_idx": 3963
        },
        "page_content": "In this section, we describe different formulations of the argumentative relation classification task and describe features used by our replicated model. In order to test our hypotheses, we propose to group all features into three distinct types. Now, we introduce a classification of three different prediction models used in the argumentative relation prediction literature. We will inspect all of them and show that all can suffer from severe issues when focusing (too much) on the context. The model INLINEFORM0 adopts a discourse parsing view on argumentative relation prediction and predicts one outgoing edge for an argumentative unit (one-outgoing edge). Model INLINEFORM1 assumes a connected graph with argumentative units and is tasked with predicting edge labels for unit tuples (labeling relations in a graph).",
        "type": "Document"
      },
      {
        "id": "42813885-f050-4bae-b640-828dfd6ae7bd",
        "metadata": {
          "vector_store_key": "1901.02222-3",
          "chunk_id": 25,
          "document_id": "1901.02222",
          "start_idx": 13947,
          "end_idx": 14693
        },
        "page_content": "The premises are created from relevant web sentences, while hypotheses are created from science questions and the corresponding answer candidates. We compare our model with \u201cmatching-aggregation\u201d related and attention-based memory related models. In addition, to verify the effectiveness of these major components in our model, we design the following model variations for comparison: ESIM is considered as a typical model of \u201cmatching-aggregation\u201d, so we choose ESIM as the principal comparison object. We choose the LSTMN model with deep attention fusion as a complement comparison, which is a memory related model. Besides above models, following variants of our model are designed for comparing: ESIM We choose the ESIM model as our baseline.",
        "type": "Document"
      },
      {
        "id": "dfcb060a-a7cd-4162-8c13-2229a797791e",
        "metadata": {
          "vector_store_key": "1906.06448-1",
          "chunk_id": 30,
          "document_id": "1906.06448",
          "start_idx": 17231,
          "end_idx": 18020
        },
        "page_content": "(Bilateral Multi-Perspective Matching Model; BIBREF31 , BIBREF31 ), ESIM (Enhanced Sequential Inference Model; BIBREF32 , BIBREF32 ), Decomposable Attention Model BIBREF33 , KIM (Knowledge-based Inference Model; BIBREF34 , BIBREF34 ), and BERT (Bidirectional Encoder Representations from Transformers model; BIBREF35 , BIBREF35 ). Regarding BERT, we checked the performance of a model pretrained on Wikipedia and BookCorpus for language modeling and trained with SNLI and MultiNLI. For other models, we checked the performance trained with SNLI. In agreement with our dataset, we regarded the prediction label contradiction as non-entailment. Table 6 shows that the accuracies of all models were better on upward inferences, in accordance with the reported results of the GLUE leaderboard.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the input triple translated to a slot-filling task?",
    "llm_answer": "The input triple is translated to a slot-filling task by querifying the relation for each entity, treating the entity as a variable, and instantiating a question template with the relevant entities.\n",
    "context": [
      {
        "id": "723157ee-3e59-4cef-9cab-e2b46c536c5c",
        "metadata": {
          "vector_store_key": "1909.06937-1",
          "chunk_id": 7,
          "document_id": "1909.06937",
          "start_idx": 4188,
          "end_idx": 4885
        },
        "page_content": "Formally, given an utterance $X = \\lbrace x_1, x_2, \\cdots , x_N \\rbrace $ with $N$ words and its corresponding slot tags $Y^{slot} = \\lbrace y_1, y_2, \\cdots , y_N \\rbrace $, the slot filling task aims to learn a parameterized mapping function $f_{\\theta } : X \\rightarrow Y $ from input words to slot tags. For the intent detection, it is designed to predict the intent label $\\hat{y}^{int}$ for the entire utterance $X$ from the predefined label set $S^{int}$. Typically, the input utterance is firstly encoded into a sequence of distributed representations $\\mathbf {X} = \\lbrace \\mathbf {x}_1, \\mathbf {x}_2, \\cdots , \\mathbf {x}_N\\rbrace $ by character-aware and pre-trained word embeddings.",
        "type": "Document"
      },
      {
        "id": "fbfde3d2-a805-45c7-996d-70c5f1976520",
        "metadata": {
          "vector_store_key": "1909.05855-2",
          "chunk_id": 37,
          "document_id": "1909.05855",
          "start_idx": 21436,
          "end_idx": 22039
        },
        "page_content": "Otherwise, a slot value is predicted and assigned to it in the second stage. In the second stage, equation is used to obtain a logit for each value taken by each categorical slot. Logits for a given categorical slot are normalized using softmax to get a distribution over all possible values. The value with the maximum mass is assigned to the slot. For each non-categorical slot, logits obtained using equations and are normalized using softmax to yield two distributions over all tokens. These two distributions respectively correspond to the start and end index of the span corresponding to the slot.",
        "type": "Document"
      },
      {
        "id": "43a5f763-4bed-4496-9867-2a96c64e8299",
        "metadata": {
          "vector_store_key": "1908.11546-2",
          "chunk_id": 9,
          "document_id": "1908.11546",
          "start_idx": 4885,
          "end_idx": 5504
        },
        "page_content": "The Slots unit maps the tuple $(c_t, a_t, s_{t-1})$ and the KB vector $k$ into $x_t^s$. The hidden state from the act cell $h_t^a$ and $x_t^s$ are inputs to a $\\text{GRU}^s$ unit that produces output $g_t^s$ and hidden state $h_t^s$. Finally, $g_t^a$ is used to predict $s_t$ through a linear projection and a $\\text{sigmoid}$. Let $z_t^i$ be the $i$-th slot's ground truth. The overall loss is the sum of the losses of the three units: $\\mathcal {L} = \\mathcal {L}^c + \\mathcal {L}^a + \\mathcal {L}^s$ The experiment dataset comes from Microsoft Research (MSR) . It contains three domains: movie, taxi, and restaurant.",
        "type": "Document"
      },
      {
        "id": "22554f50-d83d-4dea-9ef7-257c5ced98f6",
        "metadata": {
          "vector_store_key": "1706.04115-0",
          "chunk_id": 13,
          "document_id": "1706.04115",
          "start_idx": 7691,
          "end_idx": 8456
        },
        "page_content": "We observe that given a natural-language question $e$2 that expresses $e$3 (e.g. \u201cWhat did Steve Jobs do for a living?\u201d), solving the reading comprehension problem of answering $e$4 from $e$5 is equivalent to solving the slot-filling challenge. The challenge now becomes one of querification: translating $R(e,?)$ into $q$ . Rather than querify $R(e,?)$ for every entity $e$ , we propose a method of querifying the relation $R$ . We treat $e$ as a variable $x$ , querify the parametrized query $R(x,?)$ (e.g. $occupation(x,?)$ ) as a question template $q_x$ (\u201cWhat did $q$0 do for a living?\u201d), and then instantiate this template with the relevant entities, creating a tailored natural-language question for each entity $q$1 (\u201cWhat did Steve Jobs do for a living?\u201d).",
        "type": "Document"
      },
      {
        "id": "de5ca909-e597-4c8a-beea-fb6fe86829cc",
        "metadata": {
          "vector_store_key": "1911.01680-4",
          "chunk_id": 0,
          "document_id": "1911.01680",
          "start_idx": 0,
          "end_idx": 597
        },
        "page_content": "Slot Filling (SF) is the task of identifying the semantic concept expressed in natural language utterance. For instance, consider a request to edit an image expressed in natural language: \u201cRemove the blue ball on the table and change the color of the wall to brown\u201d. Here, the user asks for an \"Action\" (i.e., removing) on one \u201cObject\u201d (blue ball on the table) in the image and changing an \u201cAttribute\u201d (i.e., color) of the image to new \u201cValue\u201d (i.e., brown). Our goal in SF is to provide a sequence of labels for the given sentence to identify the semantic concept expressed in the given sentence.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is module that analyzes behavioral state trained?",
    "llm_answer": "The behavior model is pre-trained to identify the presence of each behavior from a sequence of words using a multi-label classification scheme.  The pre-trained portion used a single layer RNN with LSTM units with dimension size 50.\n",
    "context": [
      {
        "id": "841ca5e3-b85a-4caa-8287-74dc282024db",
        "metadata": {
          "vector_store_key": "1909.00107-3",
          "chunk_id": 6,
          "document_id": "1909.00107",
          "start_idx": 3648,
          "end_idx": 4333
        },
        "page_content": "Since our contribution is towards introducing behavior as a psycholinguistic feature for aiding the language modeling process, we stick with a reliable and simple LSTM-based RNN model and follow the recommendations from BIBREF1 for our baseline model. The analysis and processing of human behavior informatics is crucial in many psychotherapy settings such as observational studies and patient therapy BIBREF17. Prior work has proposed the application of neural networks in modeling human behavior in a variety of clinical settings BIBREF18, BIBREF19, BIBREF20. In this work we adopt a behavior model that predicts the likelihood of occurrence of various behaviors based on input text.",
        "type": "Document"
      },
      {
        "id": "4e5c08e9-442a-4025-9059-200472f60dd3",
        "metadata": {
          "vector_store_key": "1810.00663-5",
          "chunk_id": 53,
          "document_id": "1810.00663",
          "start_idx": 29997,
          "end_idx": 30775
        },
        "page_content": "As part of this effort, we contributed a new dataset of 11,051 pairs of user instructions and navigation plans from 100 different environments. Our model achieved the best performance in this dataset in comparison to a two-step baseline approach for interpreting navigation instructions, and a sequence-to-sequence model that does not consider the behavioral graph. Our quantitative and qualitative results suggest that attention mechanisms can help leverage the behavioral graph as a relevant knowledge base to facilitate the translation of free-form navigation instructions. Overall, our approach demonstrated practical form of learning for a complex and useful task. In future work, we are interested in investigating mechanisms to improve generalization to new environments.",
        "type": "Document"
      },
      {
        "id": "9def472d-14c2-4b80-a651-e2026e232f5f",
        "metadata": {
          "vector_store_key": "1910.02789-2",
          "chunk_id": 49,
          "document_id": "1910.02789",
          "start_idx": 27059,
          "end_idx": 27763
        },
        "page_content": "After the sentence describing the state is generated, it is transformed to an embedding vector. Words that were not found in the vocabulary were replaced with an \u201cOOV\" vector. All words were then concatenated to a NxDx1 matrix, representing the state. We experimented with both Word2Vec and GloVe pretrained embedding vectors. Eventually, we used the latter, as it consumes less memory and speeds up the training process. The length of the state sentence is one of the hyperparameters of the agents; shorter sentences are zero padded, where longer ones are trimmed. All of our models were implemented using PyTorch. The DQN agents used a single network that outputs the Q-Values of the available actions.",
        "type": "Document"
      },
      {
        "id": "ee881cb6-bb57-40e7-b2f0-3c503a02ef58",
        "metadata": {
          "vector_store_key": "1909.00107-4",
          "chunk_id": 14,
          "document_id": "1909.00107",
          "start_idx": 8481,
          "end_idx": 9169
        },
        "page_content": "This is motivated from previous works which showed good separability in these behaviors as well as being easy to interpret. The behavior model is pre-trained to identify the presence of each behavior from a sequence of words using a multi-label classification scheme. The pre-trained portion of the behavior model was implemented using a single layer RNN with LSTM units with dimension size 50. We augmented previous RNN language model architectures by BIBREF1 and BIBREF2 with our proposed behavior gates. We used the same architecture as in each work to maintain similar number of parameters and performed a grid search of hyperparameters such as learning rate, dropout, and batch size.",
        "type": "Document"
      },
      {
        "id": "e5d28ff5-89c8-4a07-8a46-8985674d8ab7",
        "metadata": {
          "vector_store_key": "1810.00663-5",
          "chunk_id": 52,
          "document_id": "1810.00663",
          "start_idx": 29198,
          "end_idx": 29997
        },
        "page_content": "This work introduced behavioral navigation through free-form natural language instructions as a challenging and a novel task that falls at the intersection of natural language processing and robotics. This problem has a range of interesting cross-domain applications, including information retrieval. We proposed an end-to-end system to translate user instructions to a high-level navigation plan. Our model utilized an attention mechanism to merge relevant information from the navigation instructions with a behavioral graph of the environment. The model then used a decoder to predict a sequence of navigation behaviors that matched the input commands. As part of this effort, we contributed a new dataset of 11,051 pairs of user instructions and navigation plans from 100 different environments.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Can the model add new relations to the knowledge graph, or just new entities?",
    "llm_answer": "The model can add a fourth type of relation (COMPLEMENT edge) to the knowledge graph.\n",
    "context": [
      {
        "id": "fef9cba7-aa12-4507-9212-a21ecc0ae11a",
        "metadata": {
          "vector_store_key": "1808.09920-2",
          "chunk_id": 17,
          "document_id": "1808.09920",
          "start_idx": 9227,
          "end_idx": 9963
        },
        "page_content": "In addition to that, and to prevent having disconnected graphs, we add a fourth type of relation (COMPLEMENT edge) between any two nodes that are not connected with any of the other relations. We can think of these edges as those in the complement set of the entity graph with respect to a fully connected graph. Our model then approaches multi-step reasoning by transforming node representations (Section \"Node annotations\" for details) with a differentiable message passing algorithm that propagates information through the entity graph. The algorithm is parameterized by a graph convolutional network (GCN) BIBREF13 , in particular, we employ relational-GCNs BIBREF17 , an extended version that accommodates edges of different types.",
        "type": "Document"
      },
      {
        "id": "de09cc14-6a6b-4073-a427-894aeaf89696",
        "metadata": {
          "vector_store_key": "1910.03891-4",
          "chunk_id": 0,
          "document_id": "1910.03891",
          "start_idx": 0,
          "end_idx": 841
        },
        "page_content": "In the past decade, many large-scale Knowledge Graphs (KGs), such as Freebase BIBREF0, DBpedia BIBREF1 and YAGO BIBREF2 have been built to represent human complex knowledge about the real-world in the machine-readable format. The facts in KGs are usually encoded in the form of triples $(\\textit {head entity}, relation, \\textit {tail entity})$ (denoted $(h, r, t)$ in this study) through the Resource Description Framework, e.g.,$(\\textit {Donald Trump}, Born In, \\textit {New York City})$. Figure FIGREF2 shows the subgraph of knowledge graph about the family of Donald Trump. In many KGs, we can observe that some relations indicate attributes of entities, such as the $\\textit {Born}$ and $\\textit {Abstract}$ in Figure FIGREF2, and others indicates the relations between entities (the head entity and tail entity are real world entity).",
        "type": "Document"
      },
      {
        "id": "93c31c63-c2b0-4113-861d-1df6e281c3c6",
        "metadata": {
          "vector_store_key": "1910.03891-4",
          "chunk_id": 20,
          "document_id": "1910.03891",
          "start_idx": 11779,
          "end_idx": 12617
        },
        "page_content": "Relation and Attribute Triples: A set of Relation triples $ T_{R} $ can be represented by $ T_{R} \\subset E \\times R \\times E $, where $E \\subset I \\cup B $ is set of entities, $R \\subset I$ is set of relations between entities. Similarly, $ T_{A} \\subset E \\times R \\times A $ is the set of attribute triples, where $ A \\subset I \\cup B \\cup L $ is the set of attribute values. Definition 2. Knowledge Graph: A KG consists of a combination of relation triples in the form of $ (h, r, t)\\in T_{R} $, and attribute triples in form of $ (h, r, a)\\in T_{A} $. Formally, we represent a KG as $G=(E,R,A,T_{R},T_{A})$, where $E=\\lbrace h,t|(h,r,t)\\in T_{R} \\cup (h,r,a)\\in T_{A}\\rbrace $ is set of entities, $R =\\lbrace r|(h,r,t)\\in T_{R} \\cup (h,r,a)\\in T_{A}\\rbrace $ is set of relations, $A=\\lbrace a|(h,r,a)\\in T_{A}\\rbrace $, respectively.",
        "type": "Document"
      },
      {
        "id": "10f70057-729d-4f3c-8366-cd743a026e8e",
        "metadata": {
          "vector_store_key": "1910.03891-4",
          "chunk_id": 2,
          "document_id": "1910.03891",
          "start_idx": 1015,
          "end_idx": 1899
        },
        "page_content": "A relation triples in KGs represents relationship between entities, e.g.,$(\\textit {Donald Trump},Father of, \\textit {Ivanka Trump})$, while attribute triples denote a literal attribute value of an entity, e.g.,$(\\textit {Donald Trump},Born, \\textit {\"June 14, 1946\"})$. Knowledge graphs have became important basis for many artificial intelligence applications, such as recommendation system BIBREF4, question answering BIBREF5 and information retrieval BIBREF6, which is attracting growing interests in both academia and industry communities. A common approach to apply KGs in these artificial intelligence applications is through embedding, which provide a simple method to encode both entities and relations into a continuous low-dimensional embedding spaces. Hence, learning distributional representation of knowledge graph has attracted many research attentions in recent years.",
        "type": "Document"
      },
      {
        "id": "91659831-18e4-49e4-a9e3-0dab6447783e",
        "metadata": {
          "vector_store_key": "1711.03438-6",
          "chunk_id": 9,
          "document_id": "1711.03438",
          "start_idx": 4615,
          "end_idx": 5418
        },
        "page_content": "\"Closed-World Knowledge Graph Completion\" we relax the constraint on the triple set $\\mathbf {T^\\prime }$ so that triples in $\\mathbf {T^\\prime }$ can contain entities that are absent from the original entity set $\\mathbf {E}$ . Closed-world KGC models learn entity and relationship embedding vectors by updating an initially random vector based on the KG's topology. Therefore, any triple $\\langle h,r,t\\rangle \\in \\mathbf {T^\\prime }$ such that $h\\notin \\mathbf {E}$ or $t\\notin \\mathbf {E}$ will only ever be represented by its initial random vector because its absence does not permit updates from any inference function. In order to predict the missing connections for unseen entities, it is necessary to develop alternative features to replace the topological features used by closed-world models.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Do twitter users tend to tweet about the DOS attack when it occurs? How much data supports this assumption?",
    "llm_answer": "Yes, the context states that legitimate users often reveal issues with network access on social media platforms, including Twitter, during a denial-of-service attack.  The paper describes collecting tweets about specific attacks from one week before the attack to the day of the attack, which implicitly assumes tweeting about the attack occurs.\n",
    "context": [
      {
        "id": "18463aa9-8964-4d6e-a2c8-a038d69e5080",
        "metadata": {
          "vector_store_key": "1909.05890-2",
          "chunk_id": 10,
          "document_id": "1909.05890",
          "start_idx": 4948,
          "end_idx": 5712
        },
        "page_content": "While the experiments show promising results, precision can be further increased by adding a layer of a supervised classifier trained with attack data at the expense of recall. Following are the contributions in this paper: A dataset of annotated tweets extracted from Twitter during DoS attacks on a variety organizations from differing domains such as banking (like Bank Of America) and technology. A weakly-supervised approach to identifying detect likely DoS service related events on twitter in real-time. A score to measure impact of the DoS attack based on the frequency of user complaints about the event. The rest of this paper is organized as follows: In section 2, previous work regarding DDoS attack detection and new event detection will be discussed.",
        "type": "Document"
      },
      {
        "id": "4ee09ef5-2cd8-4ac0-916e-c6f76f565238",
        "metadata": {
          "vector_store_key": "1909.05890-2",
          "chunk_id": 4,
          "document_id": "1909.05890",
          "start_idx": 1890,
          "end_idx": 2533
        },
        "page_content": "Thus we split this problem into two parts namely by first isolating the tweet stream that is likely related to a DoS attack and then measuring the impact of attack by analyzing the extracted tweets. A central challenge to measure the impact is how to figure out the scale of the effect on users as soon as possible so that appropriate action can be taken. Another difficulty is given the huge number of users of a service, how to effectively get and process the user feedback. With the development of Social Networks, especially micro blogs like Twitter, users post many life events in real time which can help with generating a fast response.",
        "type": "Document"
      },
      {
        "id": "cad48142-bbc4-4365-9179-485ab90e876c",
        "metadata": {
          "vector_store_key": "1909.05890-2",
          "chunk_id": 3,
          "document_id": "1909.05890",
          "start_idx": 1783,
          "end_idx": 2379
        },
        "page_content": "No matter how complex the network becomes or what methods the attackers use, a denial of service attack always results in legitimate users being unable to access the network system or slowing down their access and they are usually willing to reveal this information on social media plaforms. Thus legitimate user feedback can be a reliable indicator about the severity level of the service outage. Thus we split this problem into two parts namely by first isolating the tweet stream that is likely related to a DoS attack and then measuring the impact of attack by analyzing the extracted tweets.",
        "type": "Document"
      },
      {
        "id": "ca977e14-a654-4ea2-8b73-cfec201cb5eb",
        "metadata": {
          "vector_store_key": "1909.05890-2",
          "chunk_id": 19,
          "document_id": "1909.05890",
          "start_idx": 10123,
          "end_idx": 10720
        },
        "page_content": "Figure FIGREF4 outlines the entire pipeline of the model from preprocessing tweets to modeling them and finally detecting / ranking future tweets that are related to a DoS issue and measuring its severity. To collect the tweets, we first gathered a list of big DDoS attacks happened from 2012 to 2014. Then for each attack on the list, we collected all the tweets from one week before the attack to the attack day that contains the name of the entity attacked. The following preprocessing procedure were applied to the corpus of tweets: Remove all the meta-data like time stamp, author, and so on.",
        "type": "Document"
      },
      {
        "id": "0ff0ef61-e7c6-4e58-88d3-aab8cf173034",
        "metadata": {
          "vector_store_key": "1909.05890-0",
          "chunk_id": 6,
          "document_id": "1909.05890",
          "start_idx": 2971,
          "end_idx": 3614
        },
        "page_content": "However, because of the large number of users, a huge quantity of tweets will be generated in a short time, making it difficult to manually annotate the tweets, which makes unsupervised or weakly-supervised models much more desirable. In the Twitter data that we collected there are three kinds of tweets. Firstly are tweets that are actually about a cyberattack. For example, someone tweeted \"Can't sign into my account for bank of America after hackers infiltrated some accounts.\" on September 19, 2012 when a attack on the website happened. Secondly are tweets about some random complaints about an entity like \"Death to Bank of America!!!!",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the training and test data used?",
    "llm_answer": "The provided text describes multiple training and test data sets with different splits and augmentations.  No single, unified training and test set is described.\n",
    "context": [
      {
        "id": "36adf7ba-6ece-4ad5-84b3-0a38b137a4fc",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 16,
          "document_id": "1912.13109",
          "start_idx": 8070,
          "end_idx": 8831
        },
        "page_content": "The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we employed various operational techniques for optimizing on the learning process. These processes and its implementation details will be discussed later but they were learning rate decay, early stopping, model checkpointing and reducing learning rate on plateau.",
        "type": "Document"
      },
      {
        "id": "9ca5340d-587b-4e94-bcbe-160570788673",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 35,
          "document_id": "1910.12618",
          "start_idx": 19294,
          "end_idx": 19871
        },
        "page_content": "For instance for random forests it can correspond to the maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014 and 2015 serve as validation set (730 days).",
        "type": "Document"
      },
      {
        "id": "8c714d0f-b982-4837-aefc-3b59003e3a67",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 15,
          "document_id": "1912.13109",
          "start_idx": 7410,
          "end_idx": 8070
        },
        "page_content": "Train-test split: The labelled dataset that was available for this task was very limited in number of examples and thus as noted above few data augmentation techniques were applied to boost the learning of the network. Before applying augmentation, a train-test split of 78%-22% was done from the original, cleansed data set. Thus, 700 tweets/messages were held out for testing. All model evaluation were done in on the test set that got generated by this process. The results presented in this report are based on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data.",
        "type": "Document"
      },
      {
        "id": "ba8679a5-3b03-48ac-acef-0d2a8aacc9c4",
        "metadata": {
          "vector_store_key": "1801.09030-4",
          "chunk_id": 32,
          "document_id": "1801.09030",
          "start_idx": 16515,
          "end_idx": 17171
        },
        "page_content": "We randomly split the whole data into three parts, the training data (90%), the development data (5%) and the test data (5%). The second one is a set of symptom-prescription pairs we manually extracted from the modern text book of the course Formulaology of TCM (UTF8gbsn\u4e2d\u533b\u65b9\u5242\u5b66) that is popularly adopted by many TCM colleges in China. There are more cases in the first sampled test dataset (4,102 examples), but it suffers from lower quality, as this dataset was parsed with simple rules, which may not cover all exceptions. The second test dataset has been proofread and all of the prescriptions are the most classical and influential ones in the history.",
        "type": "Document"
      },
      {
        "id": "219e1d29-ce32-45bf-8ebf-2888f80e650e",
        "metadata": {
          "vector_store_key": "1909.04181-0",
          "chunk_id": 3,
          "document_id": "1909.04181",
          "start_idx": 1941,
          "end_idx": 2640
        },
        "page_content": "As such, in total, the distributed training data has 2,250 users, contributing a total of 225,000 tweets. The official task test set contains 720,00 tweets posted by 720 users. For our experiments, we split the training data released by organizers into 90% TRAIN set (202,500 tweets from 2,025 users) and 10% DEV set (22,500 tweets from 225 users). The age task labels come from the tagset {under-25, between-25 and 34, above-35}. For dialects, the data are labeled with 15 classes, from the set {Algeria, Egypt, Iraq, Kuwait, Lebanon-Syria, Lybia, Morocco, Oman, Palestine-Jordan, Qatar, Saudi Arabia, Sudan, Tunisia, UAE, Yemen}. The gender task involves binary labels from the set {male, female}.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What writing styles are present in the corpus?",
    "llm_answer": "News, letters, journal, non-fiction, poems, vernacular literature, prose, fiction, essay, short stories, sports news, historical writings, novels, stories, books, news columns, and tweets.\n",
    "context": [
      {
        "id": "e9a9a15f-d11c-469d-b682-6517dc1a88ce",
        "metadata": {
          "vector_store_key": "1809.08731-3",
          "chunk_id": 15,
          "document_id": "1809.08731",
          "start_idx": 8566,
          "end_idx": 9272
        },
        "page_content": "It contains single sentences and two-sentence paragraphs from the Open American National Corpus (OANC), which belong to 4 genres: newswire, letters, journal, and non-fiction. Gold references are manually created and the outputs of 4 compression systems (ILP (extractive), NAMAS (abstractive), SEQ2SEQ (extractive), and T3 (abstractive); cf. toutanova2016dataset for details) for the test data are provided. Each example has 3 to 5 independent human ratings for content and fluency. We are interested in the latter, which is rated on an ordinal scale from 1 (disfluent) through 3 (fluent). We experiment on the 2955 system outputs for the test split. Average fluency scores per system are shown in Table 2 .",
        "type": "Document"
      },
      {
        "id": "7a2a8880-c4a1-4201-b02d-f53aadce0d38",
        "metadata": {
          "vector_store_key": "1909.01247-0",
          "chunk_id": 8,
          "document_id": "1909.01247",
          "start_idx": 4599,
          "end_idx": 5315
        },
        "page_content": "The 16 classes are inspired by the OntoNotes5 corpus BIBREF7 as well as the ACE (Automatic Content Extraction) English Annotation Guidelines for Entities Version 6.6 2008.06.13 BIBREF8. Each class will be presented in detail, with examples, in the section SECREF3 A summary of available classes with word counts for each is available in table TABREF18. The corpus is available in two formats: BRAT and CoNLL-U Plus. As the corpus was developed in the BRAT environment, it was natural to keep this format as-is. BRAT is an online environment for collaborative text annotation - a web-based tool where several people can mark words, sub-word pieces, multiple word expressions, can link them together by relations, etc.",
        "type": "Document"
      },
      {
        "id": "de047afc-54bd-4a1e-9bc5-f09c79bcc782",
        "metadata": {
          "vector_store_key": "1909.01247-0",
          "chunk_id": 6,
          "document_id": "1909.01247",
          "start_idx": 3341,
          "end_idx": 4124
        },
        "page_content": "Furthermore, annotations were done automatically with a tokenizer/tagger/parser, and thus are of slightly lower quality than one would expect of a gold-standard corpus. The corpus, at its current version 1.0 is composed of 5127 sentences, annotated with 16 classes, for a total of 26377 annotated entities. The 16 classes are: PERSON, NAT_REL_POL, ORG, GPE, LOC, FACILITY, PRODUCT, EVENT, LANGUAGE, WORK_OF_ART, DATETIME, PERIOD, MONEY, QUANTITY, NUMERIC_VALUE and ORDINAL. It is based on copyright-free text extracted from Southeast European Times (SETimes). The news portal has published \u201cnews and views from Southeast Europe\u201d in ten languages, including Romanian. SETimes has been used in the past for several annotated corpora, including parallel corpora for machine translation.",
        "type": "Document"
      },
      {
        "id": "de2465b8-bc43-4557-b11e-b5ad1b2f0d72",
        "metadata": {
          "vector_store_key": "1909.00279-3",
          "chunk_id": 23,
          "document_id": "1909.00279",
          "start_idx": 13381,
          "end_idx": 14083
        },
        "page_content": "(Section SECREF27) To this end, we built a dataset as described in Section SECREF18. Evaluation metrics and baselines are described in Section SECREF21 and SECREF22. For the implementation details of building the dataset and models, please refer to supplementary materials. Training and Validation Sets We collected a corpus of poems and a corpus of vernacular literature from online resources. The poem corpus contains 163K quatrain poems from Tang Poems and Song Poems, the vernacular literature corpus contains 337K short paragraphs from 281 famous books, the corpus covers various literary forms including prose, fiction and essay. Note that our poem corpus and a vernacular corpus are not aligned.",
        "type": "Document"
      },
      {
        "id": "3f796a82-78d4-4d2f-8165-b133aeaab300",
        "metadata": {
          "vector_store_key": "1911.12579-0",
          "chunk_id": 22,
          "document_id": "1911.12579",
          "start_idx": 12899,
          "end_idx": 13803
        },
        "page_content": "The corpus is a collection of human language text BIBREF31 built with a specific purpose. However, the statistical analysis of the corpus provides quantitative, reusable data, and an opportunity to examine intuitions and ideas about language. Therefore, the corpus has great importance for the study of written language to examine the text. In fact, realizing the necessity of large text corpus for Sindhi, we started this research by collecting raw corpus from multiple web resource using web-scrappy framwork for extraction of news columns of daily Kawish and Awami Awaz Sindhi newspapers, Wikipedia dumps, short stories and sports news from Wichaar social blog, news from Focus Word press blog, historical writings, novels, stories, books from Sindh Salamat literary websites, novels, history and religious books from Sindhi Adabi Board and tweets regarding news and sports are collected from twitter.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What meta-information is being transferred?",
    "llm_answer": "Relation meta and gradient meta.\n",
    "context": [
      {
        "id": "3247664f-ceaf-4bbc-869d-c219f1f40a81",
        "metadata": {
          "vector_store_key": "1909.01515-0",
          "chunk_id": 4,
          "document_id": "1909.01515",
          "start_idx": 2108,
          "end_idx": 2865
        },
        "page_content": "For example, in Figure 1 , relation-specific meta information related to the relation CEOof or CountryCapital will be extracted and transferred by MetaR from a few existing instances to incomplete triples. The relation-specific meta information is helpful in the following two perspectives: 1) transferring common relation information from observed triples to incomplete triples, 2) accelerating the learning process within one task by observing only a few instances. Thus we propose two kinds of relation-specific meta information: relation meta and gradient meta corresponding to afore mentioned two perspectives respectively. In our proposed framework MetaR, relation meta is the high-order representation of a relation connecting head and tail entities.",
        "type": "Document"
      },
      {
        "id": "75614238-4ef3-4e8d-a7d5-8c454e96ab9d",
        "metadata": {
          "vector_store_key": "1909.01515-7",
          "chunk_id": 19,
          "document_id": "1909.01515",
          "start_idx": 10177,
          "end_idx": 11073
        },
        "page_content": "For question (1), within one task, all triples in support set and query set are about the same relation, thus it is naturally to suppose that relation is the key common part between support and query set. For question (2), the learning process is usually conducted by minimizing a loss function via gradient descending, thus gradients reveal how the model's parameters should be changed. Intuitively, we believe that gradients are valuable source to accelerate learning process. Based on these thoughts, we propose two kinds of meta information which are shared between support set and query set to deal with above problems: In order to extract relation meta and gradient mate and incorporate them with knowledge graph embedding to solve few-shot link prediction, our proposal, MetaR, mainly contains two modules: The overview and algorithm of MetaR are shown in Figure 2 and Algorithm \"Method\" .",
        "type": "Document"
      },
      {
        "id": "5c2c08bb-25fe-4273-b774-ed2ca9a8bb12",
        "metadata": {
          "vector_store_key": "1909.01515-0",
          "chunk_id": 6,
          "document_id": "1909.01515",
          "start_idx": 3352,
          "end_idx": 4122
        },
        "page_content": "MetaR achieves state-of-the-art results, indicating the success of transferring relation-specific meta information in few-shot link prediction tasks. In summary, main contributions of our work are three-folds: One target of MetaR is to learn the representation of entities fitting the few-shot link prediction task and the learning framework is inspired by knowledge graph embedding methods. Furthermore, using loss gradient as one kind of meta information is inspired by MetaNet BIBREF12 and MAML BIBREF13 which explore methods for few-shot learning by meta-learning. From these two points, we regard knowledge graph embedding and meta-learning as two main kinds of related work. Knowledge graph embedding models map relations and entities into continuous vector space.",
        "type": "Document"
      },
      {
        "id": "cb559ca6-922a-4c25-ae45-c9db2015922b",
        "metadata": {
          "vector_store_key": "1909.01515-7",
          "chunk_id": 20,
          "document_id": "1909.01515",
          "start_idx": 10452,
          "end_idx": 11107
        },
        "page_content": "Based on these thoughts, we propose two kinds of meta information which are shared between support set and query set to deal with above problems: In order to extract relation meta and gradient mate and incorporate them with knowledge graph embedding to solve few-shot link prediction, our proposal, MetaR, mainly contains two modules: The overview and algorithm of MetaR are shown in Figure 2 and Algorithm \"Method\" . Next, we introduce each module of MetaR via one few-shot link prediction task $\\mathcal {T}_r = \\lbrace  \\mathcal {S}_r, \\mathcal {Q}_r\\rbrace $ . [tb] 1 Learning of MetaR [1] Training tasks $\\mathcal {T}_{train}$ Embedding layer $emb$ ;",
        "type": "Document"
      },
      {
        "id": "2e6c9e37-52e0-461f-8bca-5d31771d6130",
        "metadata": {
          "vector_store_key": "1909.01515-0",
          "chunk_id": 3,
          "document_id": "1909.01515",
          "start_idx": 1956,
          "end_idx": 2759
        },
        "page_content": "To do few-shot link prediction, BIBREF11 made the first trial and proposed GMatching, learning a matching metric by considering both learned embeddings and one-hop graph structures, while we try to accomplish few-shot link prediction from another perspective based on the intuition that the most important information to be transferred from a few existing instances to incomplete triples should be the common and shared knowledge within one task. We call such information relation-specific meta information and propose a new framework Meta Relational Learning (MetaR) for few-shot link prediction. For example, in Figure 1 , relation-specific meta information related to the relation CEOof or CountryCapital will be extracted and transferred by MetaR from a few existing instances to incomplete triples.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What datasets are used to evaluate the approach?",
    "llm_answer": "Three self-compiled corpora, and a dataset called BIBREF31 with 3,778 training instances and 2,032 test instances.  Data from 2016 (974 days for France and 1,096 days for the UK) is also used as a test set.\n",
    "context": [
      {
        "id": "9a908f8f-2077-4b70-85b8-2941b3d7ba32",
        "metadata": {
          "vector_store_key": "1708.06022-2",
          "chunk_id": 33,
          "document_id": "1708.06022",
          "start_idx": 18725,
          "end_idx": 19404
        },
        "page_content": "In the following we introduce these datasets, provide implementation details for our model, describe the systems used for comparison, and present our results. Our model was trained on three datasets, representative of different types of QA tasks. The first two datasets focus on question answering over a structured knowledge base, whereas the third one is specific to answer sentence selection. This dataset BIBREF31 contains $3,778$ training instances and $2,032$ test instances. Questions were collected by querying the Google Suggest API. A breadth-first search beginning with wh- was conducted and the answers were crowd-sourced using Freebase as the backend knowledge base.",
        "type": "Document"
      },
      {
        "id": "1ae5bef7-39bb-401b-ae6c-c40e134e5bd9",
        "metadata": {
          "vector_store_key": "1906.10551-3",
          "chunk_id": 79,
          "document_id": "1906.10551",
          "start_idx": 43743,
          "end_idx": 44420
        },
        "page_content": "Additionally, we mentioned that determinism must be fulfilled such that an AV method can be rated as reliable. Moreover, we clarified a number of misunderstandings in previous research works and proposed three clear criteria that allow to classify the model category of an AV method, which in turn influences its design and the way how it should be evaluated. In regard to binary-extrinsic AV approaches, we explained which challenges exist and how they affect their applicability. In an experimental setup, we applied 12 existing AV methods on three self-compiled corpora, where the intention behind each corpus was to focus on a different aspect of the methods applicability.",
        "type": "Document"
      },
      {
        "id": "057b88b5-9315-4bb2-9055-9678e85081ba",
        "metadata": {
          "vector_store_key": "1807.09671-4",
          "chunk_id": 11,
          "document_id": "1807.09671",
          "start_idx": 6308,
          "end_idx": 7163
        },
        "page_content": "Various techniques have been explored, including maximal marginal relevance BIBREF22 , submodularity BIBREF23 , integer linear programming BIBREF24 , BIBREF25 , BIBREF26 , BIBREF27 , BIBREF4 , minimizing reconstruction error BIBREF28 , graph-based models BIBREF29 , BIBREF30 , BIBREF31 , BIBREF32 , determinantal point processes BIBREF33 , neural networks and reinforcement learning BIBREF34 , BIBREF35 among others. Nonetheless, most studies are bound to a single dataset and few approaches have been evaluated in a cross-domain setting. In this paper, we propose an enhanced ILP framework and evaluate it on a broad range of datasets. We present an in-depth analysis of the dataset characteristics derived from both source documents and reference summaries to understand how domain-specific factors may affect the applicability of the proposed approach.",
        "type": "Document"
      },
      {
        "id": "3be44322-a327-4b65-b36a-be0068af802a",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 36,
          "document_id": "1910.12618",
          "start_idx": 19551,
          "end_idx": 20124
        },
        "page_content": "The years 2014 and 2015 serve as validation set (730 days). It is used to tune the hyperparameters of the different approaches. All the data from January the 1st 2016 (974 days for France and 1,096 for the UK) is used as test set, on which the final results are presented. Grid search is applied to find the best combination of values: for each hyperparameter, a range of values is defined, and all the possible combinations are successively tested. The one yielding the lowest RMSE (see section SECREF4) on the validation set is used for the final results on the test one.",
        "type": "Document"
      },
      {
        "id": "5dd9016f-68b6-4759-aba2-3ab711c3da2d",
        "metadata": {
          "vector_store_key": "1711.06351-2",
          "chunk_id": 40,
          "document_id": "1711.06351",
          "start_idx": 21511,
          "end_idx": 22196
        },
        "page_content": "Second, it was tasked with generating genuinely novel questions that were not present in the data set, which we evaluate qualitatively. To make predictions, the different candidate models were fit to 15 contexts and asked to predict the remaining one (i.e., leave one out cross-validation). This results in 64 different model fits (i.e., 4 models $\\times $ 16 fits). First, we verify that compositionality is an essential ingredient in an account of human question asking. For any given context, about 15% of the human questions did not appear in any of the other contexts. Any model that attempts to simply reuse/reweight past questions will be unable to account for this productivity",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much is performance hurt when using too small amount of layers in encoder?",
    "llm_answer": "Reducing the $enc_{src \\rightarrow mt}$ encoder block's depth (Exp. 5.2) reduces performance.\n",
    "context": [
      {
        "id": "7bf18f08-6af8-4a07-ab09-4ba2eec4b59c",
        "metadata": {
          "vector_store_key": "1908.06151-2",
          "chunk_id": 45,
          "document_id": "1908.06151",
          "start_idx": 25446,
          "end_idx": 26197
        },
        "page_content": "The number of layers ($N_{src}$-$N_{mt}$-$N_{pe}$) in all encoders and the decoder for these results is fixed to 6-6-6. In Exp. 5.1, and 5.2 in Table TABREF5, we see the results of changing this setting to 6-6-4 and 6-4-6. This can be compared to the results of Exp. 2.3, since no fine-tuning or ensembling was performed for these three experiments. Exp. 5.1 shows that decreasing the number of layers on the decoder side does not hurt the performance. In fact, in the case of test2016, we got some improvement, while for test2017, the scores got slightly worse. In contrast, reducing the $enc_{src \\rightarrow mt}$ encoder block's depth (Exp. 5.2) does indeed reduce the performance for all four scores, showing the importance of this second encoder.",
        "type": "Document"
      },
      {
        "id": "b475a458-dd48-48d4-9dd6-3e794bc03f64",
        "metadata": {
          "vector_store_key": "1910.04269-4",
          "chunk_id": 21,
          "document_id": "1910.04269",
          "start_idx": 12185,
          "end_idx": 12820
        },
        "page_content": "This tends to have a regularizing effect during training of the network and hence gives better results. Thus, batch size of 32 works best for the model. Layers in Convolutional block 1 and 2: We varied the number of layers in both the convolutional blocks. If the number of layers is low, then the network does not have enough depth to capture patterns in the data whereas having large number of layers leads to overfitting on the data. In our network, two layers in the first block and one layer in the second block give optimal results. Log-Mel spectrogram is the most commonly used method for converting audio into the image domain.",
        "type": "Document"
      },
      {
        "id": "ca81c37b-14c2-44f4-99b9-0f0c5e70e810",
        "metadata": {
          "vector_store_key": "1910.08987-1",
          "chunk_id": 12,
          "document_id": "1910.08987",
          "start_idx": 6825,
          "end_idx": 7459
        },
        "page_content": "We use a low dimensional latent space so that the model learns to generate a representation that only captures the most important aspects of the input contour, and also because clustering algorithms tend to perform poorly in high dimensional spaces. Our encoder consists of three layers. The first layer applies 2 convolutional filters (kernel size 4, stride 1) followed by max pooling (kernel size 2) and a tanh activation. The second layer applies 4 convolutional filters (kernel size 4, stride 1), again with max pooling (kernel size 2) and a tanh activation. The third layer is a fully connected layer with two dimensional output.",
        "type": "Document"
      },
      {
        "id": "0399d898-292f-435e-bcb6-f065e146ace9",
        "metadata": {
          "vector_store_key": "1910.10408-0",
          "chunk_id": 20,
          "document_id": "1910.10408",
          "start_idx": 10284,
          "end_idx": 10930
        },
        "page_content": "While our main goal is to verify our hypotheses on a large data condition, thus the need to include proprietary data, for the sake of reproducibility in both languages we also provide results with systems only trained on TED Talks (small data condition). When training on large scale data we use Transformer with layer size of 1024, hidden size of 4096 on feed forward layers, 16 heads in the multi-head attention, and 6 layers in both encoder and decoder. When training only on TED talks, we set layer size of 512, hidden size of 2048 for the feed forward layers, multi-head attention with 8 heads and again 6 layers in both encoder and decoder.",
        "type": "Document"
      },
      {
        "id": "c1ad97ee-60dd-4fdc-b91e-97dd0075443b",
        "metadata": {
          "vector_store_key": "1806.00722-1",
          "chunk_id": 16,
          "document_id": "1806.00722",
          "start_idx": 9601,
          "end_idx": 10257
        },
        "page_content": "Since the number of features fed into nonlinear operation is accumulated along the path, the parameter size increases accordingly. For example, for the INLINEFORM0 -th encoder layer, the input dimension of features is INLINEFORM1 , where INLINEFORM2 is the feature dimension in previous layers, INLINEFORM3 is the embedding size. In order to avoid the calculation bottleneck for later layers due to large INLINEFORM4 , we introduce the summary layer for deeper models. It summarizes the features for all previous layers and projects back to the embedding size, so that later layers of both the encoder and the decoder side do not need to look back further.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What neural machine translation models can learn in terms of transfer learning?",
    "llm_answer": "Multilingual NMT systems can generalize to an unseen language direction (zero-shot translation).\n",
    "context": [
      {
        "id": "f2d4ab9b-c217-4698-9615-2baa1a01e52d",
        "metadata": {
          "vector_store_key": "1707.06945-0",
          "chunk_id": 26,
          "document_id": "1707.06945",
          "start_idx": 15501,
          "end_idx": 16327
        },
        "page_content": "Other potential research avenues include porting the approach to other typologically diverse languages and truly low-resource settings (e.g., with only limited amounts of parallel data), as well as experiments with other distributional spaces, e.g. BIBREF84 . Further refinements of the specialisation and clustering algorithms may also result in improved verb class induction. We have presented a novel cross-lingual transfer model which enables the automatic induction of VerbNet-style verb classifications across multiple languages. The transfer is based on a word vector space specialisation framework, utilised to directly model the assumption of cross-linguistic validity of VerbNet-style classifications. Our results indicate strong improvements in verb classification accuracy across all six target languages explored.",
        "type": "Document"
      },
      {
        "id": "e0a9b03b-75a7-408b-9d74-c9f16e83f9d1",
        "metadata": {
          "vector_store_key": "1811.02906-2",
          "chunk_id": 34,
          "document_id": "1811.02906",
          "start_idx": 19100,
          "end_idx": 19242
        },
        "page_content": "We also plan to evaluate more task-agnostic approaches for transfer learning, for instance employing language modeling as a pre-training task.",
        "type": "Document"
      },
      {
        "id": "0bb480e8-5651-4c33-922f-49dbe8b03042",
        "metadata": {
          "vector_store_key": "1910.05456-3",
          "chunk_id": 44,
          "document_id": "1910.05456",
          "start_idx": 24518,
          "end_idx": 25600
        },
        "page_content": "Cross-lingual transfer learning has been used for a large variety NLP of tasks, e.g., automatic speech recognition BIBREF25, entity recognition BIBREF26, language modeling BIBREF27, or parsing BIBREF28, BIBREF29, BIBREF30. Machine translation has been no exception BIBREF31, BIBREF32, BIBREF33. Recent research asked how to automatically select a suitable source language for a given target language BIBREF34. This is similar to our work in that our findings could potentially be leveraged to find good source languages. Finally, a lot of research has focused on human L1 and L2 acquisition of inflectional morphology BIBREF35, BIBREF36, BIBREF37, BIBREF38, BIBREF39, BIBREF40. To name some specific examples, marques2011study investigated the effect of a stay abroad on Spanish L2 acquisition, including learning of its verbal morphology in English speakers. jia2003acquisition studied how Mandarin Chinese-speaking children learned the English plural morpheme. nicoladis2012young studied the English past tense acquisition in Chinese\u2013English and French\u2013English bilingual children.",
        "type": "Document"
      },
      {
        "id": "c4d056ee-28bf-4bef-9918-bace1aeac08e",
        "metadata": {
          "vector_store_key": "1809.04686-3",
          "chunk_id": 3,
          "document_id": "1809.04686",
          "start_idx": 1743,
          "end_idx": 2504
        },
        "page_content": "Cross-lingual or multilingual NLP, the task of transferring knowledge from one language to another, serves as a good test bed for evaluating various transfer learning approaches. For cross-lingual NLP, the most widely studied approach is to use multilingual embeddings as features in neural network models. However, research has shown that representations learned in context are more effective BIBREF5 , BIBREF6 ; therefore, we aim at doing better than just using multilingual embeddings in the cross-lingual tasks. Recent progress in multilingual NMT provides a compelling opportunity for obtaining contextualized multilingual representations, as multilingual NMT systems are capable of generalizing to an unseen language direction, i.e. zero-shot translation.",
        "type": "Document"
      },
      {
        "id": "78ae53ba-f815-4e1c-83be-3a84e384b2f6",
        "metadata": {
          "vector_store_key": "1809.04686-3",
          "chunk_id": 2,
          "document_id": "1809.04686",
          "start_idx": 925,
          "end_idx": 1743
        },
        "page_content": "Recent work has further shown that we can improve over this approach significantly by considering representations in context, i.e. modeled depending on the sentences that contain them, either by taking the outputs of an encoder in MT BIBREF5 or by obtaining representations from the internal states of a bi-directional Language Model (LM) BIBREF6 . There has also been successful recent work in transferring sentence representations from resource-rich tasks to improve resource-poor tasks BIBREF7 , however, most of the above transfer learning examples have focused on transferring knowledge across tasks for a single language, in English. Cross-lingual or multilingual NLP, the task of transferring knowledge from one language to another, serves as a good test bed for evaluating various transfer learning approaches.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does the semi-automatic construction process work?",
    "llm_answer": "The context describes a process using a layout model and an execution model to transform sentences recursively based on hand-crafted grammar rules.  It also details data generation for entailment recognition tasks using a defined artificial language and FOL.  No semi-automatic construction process is explicitly described.\n",
    "context": [
      {
        "id": "ebf0e7b0-4846-4ef6-b9b2-e4d3eeaec596",
        "metadata": {
          "vector_store_key": "1601.01705-3",
          "chunk_id": 16,
          "document_id": "1601.01705",
          "start_idx": 8538,
          "end_idx": 9252
        },
        "page_content": "This process involves the following variables: Our model is built around two distributions: a layout model $p(z|x;\\theta _\\ell )$ which chooses a layout for a sentence, and a execution model $p_z(y|w;\\theta _e)$ which applies the network specified by $z$ to $w$ . For ease of presentation, we introduce these models in reverse order. We first imagine that $z$ is always observed, and in sec:model:modules describe how to evaluate and learn modules parameterized by $\\theta _e$ within fixed structures. In sec:model:assemblingNetworks, we move to the real scenario, where $z$ is unknown. We describe how to predict layouts from questions and learn $\\theta _e$ and $\\theta _\\ell $ jointly without layout supervision.",
        "type": "Document"
      },
      {
        "id": "c630b6be-cc52-4297-bad8-134c4951c3a8",
        "metadata": {
          "vector_store_key": "1909.12140-3",
          "chunk_id": 3,
          "document_id": "1909.12140",
          "start_idx": 2151,
          "end_idx": 2767
        },
        "page_content": "It takes a sentence as input and performs a recursive transformation process that is based upon a small set of 35 hand-crafted grammar rules for the English version and 29 rules for the German approach. These patterns were heuristically determined in a comprehensive linguistic analysis and encode syntactic and lexical features that can be derived from a sentence's parse tree. Each rule specifies (1) how to split up and rephrase the input into structurally simplified sentences and (2) how to set up a semantic hierarchy between them. They are recursively applied on a given source sentence in a top-down fashion.",
        "type": "Document"
      },
      {
        "id": "84104c9f-f895-46f2-b0a6-d3f3451ac006",
        "metadata": {
          "vector_store_key": "1906.00180-3",
          "chunk_id": 3,
          "document_id": "1906.00180",
          "start_idx": 1879,
          "end_idx": 2547
        },
        "page_content": "First, we develop a protocol for automatically generating data that can be used in entailment recognition tasks. Second, we demonstrate that several deep learning architectures succeed at one such task. Third, we present and apply a number of experiments to test whether models are capable of compositional generalization. The data generation process is inspired by BIBREF13 : an artificial language is defined, sentences are generated according to its grammar and the entailment relation between pairs of such sentences is established according to a fixed background logic. However, our language is significantly more complex, and instead of natural logic we use FOL.",
        "type": "Document"
      },
      {
        "id": "99ebe223-c5e6-4ce9-91b3-5067cb95893e",
        "metadata": {
          "vector_store_key": "1911.11744-4",
          "chunk_id": 20,
          "document_id": "1911.11744",
          "start_idx": 10494,
          "end_idx": 11263
        },
        "page_content": "In order to create the language generator, we conducted an human-subject study to collect sentence templates of a placement task as well as common words and synonyms for each of the used features. By utilising these data, we are able to generate over 180,000 unique sentences, depending on the generated scenario. The generated parameters of the low-level DMP controller \u2013 the weights and goal position \u2013 must be sufficiently accurate in order to successfully deliver the object to the specified bin. On the right side of Figure FIGREF4, the generated weights for the DMP are shown for two tasks in which the target is close and far away from the robot, located at different sides of the table, indicating the robots ability to generate differently shaped trajectories.",
        "type": "Document"
      },
      {
        "id": "19f970d2-4423-4e69-8b7b-5e6ddd9e6f54",
        "metadata": {
          "vector_store_key": "1603.04513-5",
          "chunk_id": 37,
          "document_id": "1603.04513",
          "start_idx": 21088,
          "end_idx": 21912
        },
        "page_content": "During pretraining, all the model parameters, including mutichannel input, convolution parameters and fully connected layer, will be updated until they are mature to extract the sentence features. Subsequently, the same sets of parameters will be fine-tuned for supervised classification tasks. In sum, this pretraining is designed to produce good initial values for both model parameters and word embeddings. It is especially helpful for pretraining the embeddings of unknown words. We test the network on four classification tasks. We begin by specifying aspects of the implementation and the training of the network. We then report the results of the experiments. In each of the experiments, the top of the network is a logistic regression that predicts the probability distribution over classes given the input sentence.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What does \"explicitly leverages their probabilistic correlation to guide the training process of both models\" mean?",
    "llm_answer": "The training objective minimizes loss functions for both QA and QG models while enforcing a constraint based on the probabilistic correlation between them.\n",
    "context": [
      {
        "id": "3e15a09a-233b-4be3-967a-b6464255ce9b",
        "metadata": {
          "vector_store_key": "1706.02027-0",
          "chunk_id": 4,
          "document_id": "1706.02027",
          "start_idx": 2305,
          "end_idx": 2989
        },
        "page_content": "In this work, we leverage the probabilistic correlation between QA and QG as the regularization term to influence the training process of both tasks. Specifically, the training objective of our framework is to jointly learn the QA model parameterized by $\\theta _{qa}$ and the QG model parameterized by $\\theta _{qg}$ by minimizing their loss functions subject to the following constraint. $$P_a(a) P(q|a;\\theta _{qg}) = P_q(q)P(a|q;\\theta _{qa})$$   (Eq. 3)   $P_a(a)$ and $P_q(q)$ are the language models for answer sentences and question sentences, respectively. We examine the effectiveness of our training criterion by applying it to strong neural network based QA and QG models.",
        "type": "Document"
      },
      {
        "id": "7fed8fd9-6d9b-4cc7-b29f-4803f579e77c",
        "metadata": {
          "vector_store_key": "1706.02027-0",
          "chunk_id": 3,
          "document_id": "1706.02027",
          "start_idx": 1595,
          "end_idx": 2305
        },
        "page_content": "$$P(q, a) = P(a) P(q|a) = P(q)P(a|q)$$   (Eq. 1)  The conditional distribution $P(q|a)$ is exactly the QG model, and the conditional distribution $P(a|q)$ is closely related to the QA model. Existing studies typically learn the QA model and the QG model separately by minimizing their own loss functions, while ignoring the probabilistic correlation between them. Based on these considerations, we introduce a training framework that exploits the duality of QA and QG to improve both tasks. There might be different ways of exploiting the duality of QA and QG. In this work, we leverage the probabilistic correlation between QA and QG as the regularization term to influence the training process of both tasks.",
        "type": "Document"
      },
      {
        "id": "4560f20e-b727-4b5b-bf18-ce109ef6cfe4",
        "metadata": {
          "vector_store_key": "1706.02027-0",
          "chunk_id": 58,
          "document_id": "1706.02027",
          "start_idx": 32047,
          "end_idx": 32511
        },
        "page_content": "We exploit the \u201cduality\u201d of QA and QG tasks, and introduce a training framework to leverage the probabilistic correlation between the two tasks. In our approach, the \u201cduality\u201d is used as a regularization term to influence the learning of QA and QG models. We implement simple yet effective QA and QG models, both of which are neural network based approaches. Experimental results show that the proposed training framework improves both QA and QG on three datasets.",
        "type": "Document"
      },
      {
        "id": "d1bc0c4f-4420-4945-80cf-d64d24c5b4c0",
        "metadata": {
          "vector_store_key": "2004.02393-0",
          "chunk_id": 16,
          "document_id": "2004.02393",
          "start_idx": 9611,
          "end_idx": 10352
        },
        "page_content": "Then the selected $p_m$ is only rewarded if it appears in a chain in $\\mathcal {C}$ that starts with $p_h$ and ends with $p_t$: To alleviate the noise in the distant supervision signal $\\mathcal {C}$, in addition to the conditional selection, we further propose a cooperative Reasoner model, also implemented with the MatchLSTM architecture (see Appendix SECREF6), to predict the linking entity from the selected passages. Intuitively, when the Ranker makes more accurate passage selections, the Reasoner will work with less noisy data and thus is easier to succeed. Specifically, the Reasoner learns to extract the linking entity from chains selected by a well-trained Ranker, and it benefits the Ranker training by providing extra rewards.",
        "type": "Document"
      },
      {
        "id": "0ed1c1ba-0642-4141-866b-3b7b3ea060b5",
        "metadata": {
          "vector_store_key": "1909.06937-0",
          "chunk_id": 10,
          "document_id": "1909.06937",
          "start_idx": 5601,
          "end_idx": 6481
        },
        "page_content": "$P_{i,j}$ indicates the score of the $j^{th}$ tag of the $i^{th}$ word in a sentence BIBREF15. When testing, the Viterbi algorithm BIBREF16 is used to search the sequence of slot tags with maximum score: As to the prediction of intent, the word-level hidden states $\\mathbf {H}$ are firstly summarized into a utterance-level representation $\\mathbf {v}^{int}$ via mean pooling (or max pooling or self-attention, etc.): The most probable intent label $\\hat{y}^{int}$ is predicted by softmax normalization over the intent label set: Generally, both tasks are trained jointly to minimize the sum of cross entropy from each individual task. Formally, the loss function of the join model is computed as follows: where $y^{int}_i$ and $y^{slot}_{i,j}$ are golden labels, and $\\lambda $ is hyperparameter, and $|S^{int}|$ is the size of intent label set, and similarly for $|S^{slot}|$ .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is WNGT 2019 shared task?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "d6ae924e-00aa-48ba-a89c-8bd114f9f4f8",
        "metadata": {
          "vector_store_key": "1910.11493-4",
          "chunk_id": 20,
          "document_id": "1910.11493",
          "start_idx": 11356,
          "end_idx": 12165
        },
        "page_content": "The model uses hard monotonic attention instead of standard soft attention, along with a dynamic programming based training scheme. The SIGMORPHON 2019 shared task received 30 submissions\u201414 for task 1 and 16 for task 2\u2014from 23 teams. In addition, the organizers' baseline systems were evaluated. Five teams participated in the first Task, with a variety of methods aimed at leveraging the cross-lingual data to improve system performance. The University of Alberta (UAlberta) performed a focused investigation on four language pairs, training cognate-projection systems from external cognate lists. Two methods were considered: one which trained a high-resource neural encoder-decoder, and projected the test data into the HRL, and one that projected the HRL data into the LRL, and trained a combined system.",
        "type": "Document"
      },
      {
        "id": "373735e0-db3e-4d08-889a-109b31d1f166",
        "metadata": {
          "vector_store_key": "1804.08186-5",
          "chunk_id": 200,
          "document_id": "1804.08186",
          "start_idx": 114468,
          "end_idx": 115191
        },
        "page_content": "Generally, datasets for shared tasks have been made publicly available after the conclusion of the task, and are a good source of standardized evaluation data. However, the shared tasks to date have tended to target specific sub-problems in , and no general, broad-coverage datasets have been compiled. Widespread interest in over closely-related languages has resulted in a number of shared tasks that specifically tackle the issue. Some tasks have focused on varieties of a specific language. For example, the DEFT2010 shared task BIBREF385 examined varieties of French, requiring participants to classify French documents with respect to their geographical source, in addition to the decade in which they were published.",
        "type": "Document"
      },
      {
        "id": "1d262be4-4dbc-447d-b93f-275422d87e7e",
        "metadata": {
          "vector_store_key": "1909.04181-0",
          "chunk_id": 2,
          "document_id": "1909.04181",
          "start_idx": 1249,
          "end_idx": 1941
        },
        "page_content": "For the purpose of our experiments, we use data released by the APDA shared task organizers. The dataset is divided into train and test by organizers. The training set is distributed with labels for the three tasks of age, dialect, and gender. Following the standard shared tasks set up, the test set is distributed without labels and participants were expected to submit their predictions on test. The shared task predictions are expected by organizers at the level of users. The distribution has 100 tweets for each user, and so each tweet is distributed with a corresponding user id. As such, in total, the distributed training data has 2,250 users, contributing a total of 225,000 tweets.",
        "type": "Document"
      },
      {
        "id": "dc608d7e-7189-4739-a5c7-70ea7f533d12",
        "metadata": {
          "vector_store_key": "1804.08186-5",
          "chunk_id": 199,
          "document_id": "1804.08186",
          "start_idx": 114262,
          "end_idx": 114976
        },
        "page_content": "To address specific sub-problems in , a number of shared tasks have been organized on problems such as in multilingual documents BIBREF378 , code-switched data BIBREF383 , discriminating between closely related languages BIBREF384 , and dialect and language variety identification in various languages BIBREF385 , BIBREF386 , BIBREF370 , BIBREF387 . Shared tasks are important for because they provide datasets and standardized evaluation methods that serve as benchmarks for the community. We summarize all shared tasks organized to date in sharedtasks. Generally, datasets for shared tasks have been made publicly available after the conclusion of the task, and are a good source of standardized evaluation data.",
        "type": "Document"
      },
      {
        "id": "8163cb94-31d1-4a19-b80c-a14c0abe6ee2",
        "metadata": {
          "vector_store_key": "1910.11493-4",
          "chunk_id": 36,
          "document_id": "1910.11493",
          "start_idx": 20674,
          "end_idx": 21576
        },
        "page_content": "The SIGMORPHON 2019 shared task provided a type-level evaluation on 100 language pairs in 79 languages and a token-level evaluation on 107 treebanks in 66 languages, of systems for inflection and analysis. On task 1 (low-resource inflection with cross-lingual transfer), 14 systems were submitted, while on task 2 (lemmatization and morphological feature analysis), 16 systems were submitted. All used neural network models, completing a trend in past years' shared tasks and other recent work on morphology. In task 1, gains from cross-lingual training were generally modest, with gains positively correlating with the linguistic similarity of the two languages. In the second task, several methods were implemented by multiple groups, with the most successful systems implementing variations of multi-headed attention, multi-level encoding, multiple decoders, and ELMo and BERT contextual embeddings.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Was any variation in results observed based on language typology?",
    "llm_answer": "No.\n",
    "context": [
      {
        "id": "d67d54b6-3a64-4e64-8965-111710b50cd8",
        "metadata": {
          "vector_store_key": "1910.05456-7",
          "chunk_id": 40,
          "document_id": "1910.05456",
          "start_idx": 21931,
          "end_idx": 22624
        },
        "page_content": "A limitation of our work is that we only include languages that are written in Latin script. An interesting question for future work might, thus, regard the effect of disjoint L1 and L2 alphabets. Furthermore, none of the languages included in our study exhibits a templatic morphology. We make this choice because data for templatic languages is currently mostly available in non-Latin alphabets. Future work could investigate languages with templatic morphology as source or target languages, if needed by mapping the language's alphabet to Latin characters. Finally, while we intend to choose a diverse set of languages for this study, our overall number of languages is still rather small.",
        "type": "Document"
      },
      {
        "id": "7d9578de-8a10-4c5e-91b2-4d926700c1a0",
        "metadata": {
          "vector_store_key": "1910.04269-0",
          "chunk_id": 11,
          "document_id": "1910.04269",
          "start_idx": 6789,
          "end_idx": 7492
        },
        "page_content": "It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",
        "type": "Document"
      },
      {
        "id": "9bbd7687-5d8f-4a9d-a3bb-07fdb4179658",
        "metadata": {
          "vector_store_key": "1911.08915-2",
          "chunk_id": 35,
          "document_id": "1911.08915",
          "start_idx": 17791,
          "end_idx": 18463
        },
        "page_content": "This method was very successful identifying the languages in which test were written, only failing to distinguish a couple of texts, confusing texts french and spanish, which have a strong overlap. In Table (TABREF12) we present the evidence that we can use the statistics of clustering coefficient to measure a sort of distance between languages. Though hapax legomena account for most of the value $\\nu (C)$ for $C=0$ and 1, we found that the fraction $\\nu ^{\\prime }_H(C)$ of hapax to other words is similar for French and Spanish, and different for Spanish and, say, Turkish. Further, $\\nu ^{\\prime }_H(C)$ is different between random texts and the languages we study.",
        "type": "Document"
      },
      {
        "id": "33263f7a-b742-485d-8662-a59178bf4ca3",
        "metadata": {
          "vector_store_key": "1909.13362-0",
          "chunk_id": 37,
          "document_id": "1909.13362",
          "start_idx": 21946,
          "end_idx": 22603
        },
        "page_content": "Of these three models, the Base model performed the best with the exception of the French and Manipuri datasets. The differences in the French results can be considered negligible because the accuracies are all near $100\\%$. The Small model performed best on Manipuri, which may suggest that reducing the number of parameters of the Base model leads to better accuracy on smaller datasets. When comparing our model with previous syllabifiers, we consider the Base model exclusively. In Table TABREF26, a side-by-side comparison of our Base model to a selection of published syllabifiers shows that Base is near state-of-the art performance on English CELEX.",
        "type": "Document"
      },
      {
        "id": "ce2442ac-3726-4f9f-918e-bdfe8a4caa08",
        "metadata": {
          "vector_store_key": "1710.09589-2",
          "chunk_id": 21,
          "document_id": "1710.09589",
          "start_idx": 12282,
          "end_idx": 13060
        },
        "page_content": "We could investigate this as the organizers provided us with translations of French, Spanish and Japanese back to English. Averaged over all languages our system ranked first, cf. Table TABREF34 for the results of the top 5 submissions. The multilingual model reaches the overall best exact accuracy, for two languages training a in-language model would be slightly more beneficial at the cost of maintaining a separate model. The similarity-based baseline provided by the organizers is considerably lower. Our system was outperformed on English by three teams, most of which focused only on English. Unfortunately at the time of writing there is no system description available for most other top systems, so that we cannot say whether they used more English-specific features.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Can the approach be generalized to other technical domains as well? ",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "f55e2ce0-29a7-4bcf-b957-983e61f8c107",
        "metadata": {
          "vector_store_key": "1804.08186-5",
          "chunk_id": 295,
          "document_id": "1804.08186",
          "start_idx": 167240,
          "end_idx": 167986
        },
        "page_content": "Existing work on serves to illustrate that the scope and depth of the problem are much greater than they may first appear. In openissues, we discussed open issues in , identifying the key challenges, and outlining opportunities for future research. Far from being a solved problem, aspects of make it an archetypal learning task with subtleties that could be tackled by future work on supervised learning, representation learning, multi-task learning, domain adaptation, multi-label classification and other subfields of machine learning. We hope that this paper can serve as a reference point for future work in the area, both for providing insight into work to date, as well as pointing towards the key aspects that merit further investigation.",
        "type": "Document"
      },
      {
        "id": "5c407105-b91e-465d-8eb1-9293760d2ef6",
        "metadata": {
          "vector_store_key": "1906.11180-8",
          "chunk_id": 69,
          "document_id": "1906.11180",
          "start_idx": 35942,
          "end_idx": 36666
        },
        "page_content": "A new technical framework is proposed with neural network and knowledge-based learning. It (i) extracts candidate classes as well as their positive and negative samples from the KB by lookup and query answering, with their quality improved using an external KB; (ii) trains classifiers that can effectively learn a literal's contextual features with BiRNNs and an attention mechanism; (iii) identifies types and matches entity for canonicalization. We use a real data set and a synthetic data set, both extracted from DBpedia, for evaluation. It achieves much higher performance than the baselines that include the state-of-the-art. We discuss below some more subjective observations and possible directions for future work.",
        "type": "Document"
      },
      {
        "id": "a0e7fdb9-ae87-4700-a7f4-76bf89caa336",
        "metadata": {
          "vector_store_key": "1907.11499-3",
          "chunk_id": 2,
          "document_id": "1907.11499",
          "start_idx": 957,
          "end_idx": 1759
        },
        "page_content": "The ability to handle a wide variety of domains has become more pertinent with the rise of data-hungry machine learning techniques like neural networks and their application to a plethora of textual media ranging from news articles to twitter, blog posts, medical journals, Reddit comments, and parliamentary debates BIBREF0 , BIBREF3 , BIBREF4 , BIBREF5 . The question of how to best deal with multiple domains when training data is available for one or few of them has met with much interest in the literature. The field of domain adaptation BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 aims at improving the learning of a predictive function in a target domain where there is little or no labeled data, using knowledge transferred from a source domain where sufficient labeled data is available.",
        "type": "Document"
      },
      {
        "id": "be697555-c588-42e2-8af8-3b36d3203554",
        "metadata": {
          "vector_store_key": "2001.11268-8",
          "chunk_id": 52,
          "document_id": "2001.11268",
          "start_idx": 30207,
          "end_idx": 30911
        },
        "page_content": "We contributed an approach to deal with ambiguity in whole-sentence predictions, and proposed the usage of a completely different approach to entity recognition in settings where training data are sparse. In conclusion we wish to emphasize our argument that for future applications, interoperability is important. Instead of developing yet another stand-alone organizational interface with a machine learning classifier that works on limited data only, the focus should be to develop and train cross-domain and neural models that can be integrated into the backend of existing platforms. The performance of these models should be comparable on standardized datasets, evaluation scripts and leader boards.",
        "type": "Document"
      },
      {
        "id": "31b0ecec-35af-48ee-ae22-70b80cc996bc",
        "metadata": {
          "vector_store_key": "1901.02262-1",
          "chunk_id": 45,
          "document_id": "1901.02262",
          "start_idx": 23457,
          "end_idx": 23647
        },
        "page_content": "Future work will involve exploring the potential of hybrid models combining extractive and abstractive approaches and improving the passage re-ranking and answerable question identification.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What dataset they use for evaluation?",
    "llm_answer": "Cochrane Schizophrenia Group data, CMRC-2017, MSCOCO, and Quora duplicate question dataset.\n",
    "context": [
      {
        "id": "64246149-5307-408a-9ede-714a10713549",
        "metadata": {
          "vector_store_key": "2001.11268-9",
          "chunk_id": 55,
          "document_id": "2001.11268",
          "start_idx": 31903,
          "end_idx": 32694
        },
        "page_content": "Training data for sentence classification and question answering are freely available from the cited sources. Additionally, the Cochrane Schizophrenia Group extracted, annotated and made available data from studies included in over 200 systematic reviews. This aims at supporting the development of methods for reviewing tasks, and to increase the re-use of their data. These data include risk-of-bias assessment, results including all clean and published outcome data extracted by reviewers, data on PICOs, methods, and identifiers such as PubMed ID and a link to their study-based register. Additionally, a senior reviewer recently carried out a manual analysis of all 33,000 outcome names in these reviews, parsed and allocated to 15,000 unique outcomes in eight main categories BIBREF27.",
        "type": "Document"
      },
      {
        "id": "7b9201e9-046a-40b6-83a7-6c89517f0792",
        "metadata": {
          "vector_store_key": "1709.08299-1",
          "chunk_id": 8,
          "document_id": "1709.08299",
          "start_idx": 4718,
          "end_idx": 5611
        },
        "page_content": "To add difficulties to the dataset, along with the automatically generated evaluation sets (validation/test), they also release a human-annotated evaluation set. The experimental results show that the human-annotated evaluation set is significantly harder than the automatically generated questions. The reason would be that the automatically generated data is accordance with the training data which is also automatically generated and they share many similar characteristics, which is not the case when it comes to human-annotated data. In this section, we will briefly introduce the evaluation tracks and then the generation method of our dataset will be illustrated in detail. The proposed dataset is typically used for the 1st Evaluation on Chinese Machine Reading Comprehension (CMRC-2017), which aims to provide a communication platform to the Chinese communities in the related fields.",
        "type": "Document"
      },
      {
        "id": "fcf8be2f-41ef-4628-a599-a4f691aa1c11",
        "metadata": {
          "vector_store_key": "2001.08051-3",
          "chunk_id": 15,
          "document_id": "2001.08051",
          "start_idx": 8175,
          "end_idx": 8857
        },
        "page_content": "However, a detailed linguistic evaluation cannot be performed without allowing the students to express themselves in both written sentences and spoken utterances, which typically require the intervention of human experts to be scored. Tables and report some statistics extracted from both the written and spoken data collected so far in all the campaigns. Each written or spoken item received a total score by human experts, computed by summing up the scores related to 6 indicators in 2017/2018 (from 3 to 6 in the 2016 campaign, according to the proficiency levels and the type of test). Each indicator can assume a value 0, 1, 2, corresponding to bad, medium, good, respectively.",
        "type": "Document"
      },
      {
        "id": "8c81c383-8e40-4094-8e18-ab0a2d32126a",
        "metadata": {
          "vector_store_key": "1905.08949-6",
          "chunk_id": 18,
          "document_id": "1905.08949",
          "start_idx": 10340,
          "end_idx": 11074
        },
        "page_content": "Although the datasets are commonly shared between QG and QA, it is not the case for evaluation: it is challenging to define a gold standard of proper questions to ask. Meaningful, syntactically correct, semantically sound and natural are all useful criteria, yet they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks. As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE BIBREF43 , are also widely used.",
        "type": "Document"
      },
      {
        "id": "5ab7a4fc-e033-435e-82b9-cdf588aa7d87",
        "metadata": {
          "vector_store_key": "1804.00079-9",
          "chunk_id": 45,
          "document_id": "1804.00079",
          "start_idx": 25714,
          "end_idx": 26342
        },
        "page_content": "We use 113k training images from MSCOCO with 5k images for validation and 5k for testing. Image features are extracted using a pre-trained 110 layer ResNet. The evaluation criterion is Recall@K and the median K across 5 different splits of the data. In addition to the above tasks which were considered by BIBREF9 , we also evaluate on the recently published Quora duplicate question dataset since it is an order of magnitude larger than the others (approximately 400,000 question pairs). The task is to correctly identify question pairs that are duplicates of one another, which we formulate as a binary classification problem.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which regions of the United States do they consider?",
    "llm_answer": "The Northeast, Southeast, Midwest, and West Coast.\n",
    "context": [
      {
        "id": "d82d6c29-8b8c-4059-95cf-3d56c77fbaa8",
        "metadata": {
          "vector_store_key": "1708.05873-1",
          "chunk_id": 31,
          "document_id": "1708.05873",
          "start_idx": 17790,
          "end_idx": 18388
        },
        "page_content": "The fact that there is no effect of conflict on Topic 2 is interesting in this regard. Finally, we consider regional effects in Figure FIGREF14 . We use the World Bank's classifications of regions: Latin America and the Caribbean (LCN), South Asia (SAS), Sub-Saharan Africa (SSA), Europe and Central Asia (ECS), Middle East and North Africa (MEA), East Asia and the Pacific (EAS), North America (NAC). The figure shows that states in South Asia, and Latin America and the Caribbean are likely to discuss Topic 2 the most. States in South Asia and East Asia and the Pacific discuss Topic 7 the most.",
        "type": "Document"
      },
      {
        "id": "a8799de9-f1e5-464f-b4b7-c9dc7b5af303",
        "metadata": {
          "vector_store_key": "1904.07342-1",
          "chunk_id": 13,
          "document_id": "1904.07342",
          "start_idx": 7276,
          "end_idx": 7980
        },
        "page_content": "The clusters correspond to four major regions of the U.S.: the Northeast (green), Southeast (yellow), Midwest (blue), and West Coast (purple); centroids are designated by crosses. Average sentiments within each cluster confirm prior knowledge BIBREF1 : the Southeast and Midwest have lower average sentiments ( INLINEFORM0 and INLINEFORM1 , respectively) than the West Coast and Northeast (0.22 and 0.09, respectively). In Figure FIGREF5 , we plot predicted sentiment averaged by U.S. city of event-related tweeters. The majority of positive tweets emanate from traditionally liberal hubs (e.g. San Francisco, Los Angeles, Austin), while most negative tweets come from the Philadelphia metropolitan area.",
        "type": "Document"
      },
      {
        "id": "fab2aa32-4598-4763-b3d4-d5c71b8b5883",
        "metadata": {
          "vector_store_key": "1803.09745-2",
          "chunk_id": 19,
          "document_id": "1803.09745",
          "start_idx": 9451,
          "end_idx": 10130
        },
        "page_content": "Many individuals provide location information, entered as free text, along with their biographical profile. We matched user specified locations of the form `city, state' to a U.S. county when possible, comprising a 2.26% subset of the decahose dataset. Details on this matching process can be found in #1. For general English, we counted the number of tokens in the decahose dataset for each verb. For AE, we used the tweets whose geotagged coordinates are located in the United States, and for BE we used the tweets whose geotagged coordinates are located in the United Kingdom. For the analysis of verbs by county, we used the tweets with the user entered location information.",
        "type": "Document"
      },
      {
        "id": "26beb8ca-1472-4bf0-8427-6732a5961018",
        "metadata": {
          "vector_store_key": "1909.03242-1",
          "chunk_id": 18,
          "document_id": "1909.03242",
          "start_idx": 10190,
          "end_idx": 11010
        },
        "page_content": "The top 30 most frequent entities are listed in Table TABREF14 . This clearly shows that most of the claims involve entities related to the United States, which is to be expected, as most of the fact checking websites are US-based. We train several models to predict the veracity of claims. Those fall into two categories: those that only consider the claims themselves, and those that encode evidence pages as well. In addition, claim metadata (speaker, checker, linked entities) is optionally encoded for both categories of models, and ablation studies with and without that metadata are shown. We first describe the base model used in Section SECREF16 , followed by introducing our novel evidence ranking and veracity prediction model in Section SECREF22 , and lastly the metadata encoding model in Section SECREF31 .",
        "type": "Document"
      },
      {
        "id": "e4e5ccf7-c52d-4467-af78-9c67e96d65b5",
        "metadata": {
          "vector_store_key": "1803.09745-2",
          "chunk_id": 44,
          "document_id": "1803.09745",
          "start_idx": 22366,
          "end_idx": 22946
        },
        "page_content": "Also, here we include counties that have at least 10 tokens. Fig.~ \"\" gives an example map showing the $Gi^*$ $z$ -scores for the verb `dream'. The maps showing local spatial autocorrelation for the complete list of verbs can be found in the Online Appendix A at . For many of the counties in the US, there is a small sample of Twitter data. We restrict our analysis to counties with a total token count of at least 40 for the verbs we consider. Even for the counties meeting this criteria, the volume of data varies, leading to drastically different sample sizes across counties.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is performance measured?",
    "llm_answer": "Performance is measured by success rate in selecting the best hyperparameter combination or early-stopping checkpoint (out of 10 runs), average human-annotated score, means and standard deviations of accuracy (with 5 runs and different random seeds, except for a specific split where 25 runs and median are used), and time and cost to reach a specific task-specific performance level.\n",
    "context": [
      {
        "id": "1c2721da-ef50-4f82-92a2-9f11a92306e0",
        "metadata": {
          "vector_store_key": "2002.05058-2",
          "chunk_id": 49,
          "document_id": "2002.05058",
          "start_idx": 27267,
          "end_idx": 27919
        },
        "page_content": "As described in the human evaluation procedure, we perform 10 runs to test the reliability of each metric when used to perform hyperparameter tuning and early-stopping respectively. In each run, we select the best hyperparameter combination or early-stopping checkpoint based on each of the five compared metrics. Human evaluation is then employed to identify the best choice. We evaluate the performance of each metric by how many times (out of 10) they succeeded in selecting the best hyperparameter combination or early-stopping checkpoint (out of 4) and the average human-annotated score for their selected models. The results are shown in Table 3.",
        "type": "Document"
      },
      {
        "id": "d511bb7a-2f5d-4402-b99f-9a52ecba3fd8",
        "metadata": {
          "vector_store_key": "1601.02543-3",
          "chunk_id": 1,
          "document_id": "1601.02543",
          "start_idx": 628,
          "end_idx": 1339
        },
        "page_content": "Before commercial deployment of a speech solution it is imperative to have a quantitative measure of the performance of the speech solution which is primarily based on the speech recognition accuracy of the speech engine used. Generally, the recognition performance of any speech recognition based solution is quantitatively evaluated by putting it to actual use by the people who are the intended users and then analyzing the logs to identify successful and unsuccessful transactions. This evaluation is then used to identifying any further improvement in the speech recognition based solution to better the overall transaction completion rates. This process of evaluation is both time consuming and expensive.",
        "type": "Document"
      },
      {
        "id": "4d3a2966-d599-49bc-bc57-9c03fe6d8dbf",
        "metadata": {
          "vector_store_key": "2002.05829-3",
          "chunk_id": 7,
          "document_id": "2002.05829",
          "start_idx": 4345,
          "end_idx": 5119
        },
        "page_content": "With the Hulk benchmark, we quantify the energy efficiency of model pretraining, fine-tuning and inference phase by comparing the time and cost they require to reach certain overall task-specific performance level on selected datasets. The design principle and benchmarking process are detailed in section SECREF2. We also explore the relation between model parameter and fine-tuning efficiency and demonstrate consistency of energy efficiency between tasks for different pretrained models. For pretraining phase, the benchmark is designed to favor energy efficient models in terms of time and cost that each model takes to reach certain multi-task performance pretrained from scratch. For example, we keep track of the time and cost of a BERT model pretrained from scratch.",
        "type": "Document"
      },
      {
        "id": "cb8b2f55-7e86-42a5-b1b0-e36b956e0b41",
        "metadata": {
          "vector_store_key": "1804.08186-8",
          "chunk_id": 188,
          "document_id": "1804.08186",
          "start_idx": 107727,
          "end_idx": 108473
        },
        "page_content": "From a practical perspective, knowing the rate at which a system can process and classify documents is useful as it allows a practitioner to predict the time required to process a document collection given certain computational resources. However, so many factors influence the rate at which documents are processed that comparison of absolute values across publications is largely meaningless. Instead, it is more valuable to consider publications that compare multiple systems under controlled conditions (same computer hardware, same evaluation data, etc.). The most common observations are that classification times between different algorithms can differ by orders of magnitude, and that the fastest methods are not always the most accurate.",
        "type": "Document"
      },
      {
        "id": "eaad0382-a031-4123-ba6f-9bd1afa901b7",
        "metadata": {
          "vector_store_key": "1904.09708-6",
          "chunk_id": 59,
          "document_id": "1904.09708",
          "start_idx": 31097,
          "end_idx": 31784
        },
        "page_content": "All evaluation runs are reported in the main text: for each evaluation except for the add-jump split, models were trained 5 times with different random seeds, and performance was measured with means and standard deviations of accuracy. For the add-jump split, we included 25 runs to get a more accurate assessment of performance. This revealed a strong skew in the distribution of results, so we included the median as the main measure of performance. Occasionally, the model did not train at all due to an unknown error (possibly very poor random initialization, high learning rate or numerical error). For this reason, we excluded runs in which training accuracy did not get above 10%.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is novel in author's approach?",
    "llm_answer": "Using recent advances in text representation learning to test a single literary theory about a novel, focusing on a single book, rather than large collections of stories.\n",
    "context": [
      {
        "id": "8e7dee96-21fc-454d-9bd0-3059f8946cc4",
        "metadata": {
          "vector_store_key": "1904.08386-2",
          "chunk_id": 16,
          "document_id": "1904.08386",
          "start_idx": 9219,
          "end_idx": 10140
        },
        "page_content": "Our work takes a first step towards computationally engaging with literary criticism on a single book using state-of-the-art text representation methods. While we demonstrate that NLP techniques can be used to support literary analyses and obtain new insights, they also have clear limitations (e.g., in understanding abstract themes). As text representation methods become more powerful, we hope that (1) computational tools will become useful for analyzing novels with more conventional structures, and (2) literary criticism will be used as a testbed for evaluating representations. We thank the anonymous reviewers for their insightful comments. Additionally, we thank Nader Akoury, Garrett Bernstein, Chenghao Lv, Ari Kobren, Kalpesh Krishna, Saumya Lal, Tu Vu, Zhichao Yang, Mengxue Zhang and the UMass NLP group for suggestions that improved the paper's clarity, coverage of related work, and analysis experiments.",
        "type": "Document"
      },
      {
        "id": "31b0ecec-35af-48ee-ae22-70b80cc996bc",
        "metadata": {
          "vector_store_key": "1901.02262-1",
          "chunk_id": 45,
          "document_id": "1901.02262",
          "start_idx": 23457,
          "end_idx": 23647
        },
        "page_content": "Future work will involve exploring the potential of hybrid models combining extractive and abstractive approaches and improving the passage re-ranking and answerable question identification.",
        "type": "Document"
      },
      {
        "id": "d4dc3feb-cee8-42bc-85f6-e17c96767ae4",
        "metadata": {
          "vector_store_key": "2003.06279-5",
          "chunk_id": 52,
          "document_id": "2003.06279",
          "start_idx": 31568,
          "end_idx": 32268
        },
        "page_content": "The list of books is shown in Tables TABREF25 and TABREF26. For each book we show the respective authors (Aut.) and the following quantities: total number of words ($N_W$), total number of sentences ($N_S$), total number of paragraphs ($N_P$) and the average sentence length ($\\langle S_L \\rangle $), measured in number of words. The following authors were considered: Hector Hugh (HH), Thomas Hardy (TH), Daniel Defoe (DD), Allan Poe (AP), Bram Stoker (BS), Mark Twain (MT), Charles Dickens (CD), Pelham Grenville (PG), Charles Darwin (CD), Arthur Doyle (AD), George Eliot (GE), Jane Austen (JA), and Joseph Conrad (JC). In this section we show additional results obtained for different text length.",
        "type": "Document"
      },
      {
        "id": "6995991c-93b8-46ca-9d39-9ae2ed25a898",
        "metadata": {
          "vector_store_key": "1904.08386-2",
          "chunk_id": 0,
          "document_id": "1904.08386",
          "start_idx": 0,
          "end_idx": 718
        },
        "page_content": "Literary critics form interpretations of meaning in works of literature. Building computational models that can help form and test these interpretations is a fundamental goal of digital humanities research BIBREF0 . Within natural language processing, most previous work that engages with literature relies on \u201cdistant reading\u201d BIBREF1 , which involves discovering high-level patterns from large collections of stories BIBREF2 , BIBREF3 . We depart from this trend by showing that computational techniques can also engage with literary criticism at a closer distance: concretely, we use recent advances in text representation learning to test a single literary theory about the novel Invisible Cities by Italo Calvino.",
        "type": "Document"
      },
      {
        "id": "520a8491-3d3f-4608-a6e9-11e7ce8e32bf",
        "metadata": {
          "vector_store_key": "1904.08386-2",
          "chunk_id": 15,
          "document_id": "1904.08386",
          "start_idx": 8875,
          "end_idx": 9656
        },
        "page_content": "There has been other computational work that focuses on just a single book or a small number of books, much of it focused on network analysis: BIBREF35 extract character social networks from Alice in Wonderland, while BIBREF36 recover social networks from 19th century British novels. BIBREF37 disentangles multiple narrative threads within the novel Infinite Jest, while BIBREF7 provides several automated statistical methods for close reading and test them on the award-winning novel Cloud Atlas (2004). Compared to this work, we push further on modeling the content of the narrative by leveraging pretrained language models. Our work takes a first step towards computationally engaging with literary criticism on a single book using state-of-the-art text representation methods.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How large is the Dialog State Tracking Dataset?",
    "llm_answer": "Over 16000 dialogues in the training set.\n",
    "context": [
      {
        "id": "fccbf5d9-bc06-4c46-8295-71bbc332c05c",
        "metadata": {
          "vector_store_key": "1908.11546-0",
          "chunk_id": 10,
          "document_id": "1908.11546",
          "start_idx": 5362,
          "end_idx": 6031
        },
        "page_content": "It contains three domains: movie, taxi, and restaurant. The total count of dialogues per domain and train/valid/test split is reported in Table TABREF11. At every turn both user and agent acts are annotated, we use only the agent side as targets in our experiment. The acts are ordered in the dataset (each output sentence aligns with one act). The size of the sets of acts, slots, and act-slot pairs are also listed in Table TABREF11. Table TABREF12 shows the count of turns with multiple act annotations, which amounts to 23% of the dataset. We use MSR's dialogue management code and knowledge base to obtain the state at each turn and use it as input to every model.",
        "type": "Document"
      },
      {
        "id": "7aa0d12d-f58e-4235-b208-700f667827a3",
        "metadata": {
          "vector_store_key": "1909.11833-0",
          "chunk_id": 5,
          "document_id": "1909.11833",
          "start_idx": 3207,
          "end_idx": 3888
        },
        "page_content": "For instance, on Wizard-of-Oz (WOZ) 2.0 dataset BIBREF8, the SIM model obtains a joint-accuracy score of 89.5%, 1.4% higher than the previously best model GLAD, with only 22% of the number of parameters. On DSTC2 dataset, SIM achieves comparable performance with previous best models with only 19% of the model size. As outlined in BIBREF9, the dialogue state tracking task is formulated as follows: at each turn of dialogue, the user's utterance is semantically decoded into a set of slot-value pairs. There are two types of slots. Goal slots indicate the category, e.g. area, food, and the values specify the constraint given by users for the category, e.g. South, Mediterranean.",
        "type": "Document"
      },
      {
        "id": "84ff5116-0ca0-46d5-b614-5b01dc4d9f20",
        "metadata": {
          "vector_store_key": "1909.05855-1",
          "chunk_id": 23,
          "document_id": "1909.05855",
          "start_idx": 13431,
          "end_idx": 14210
        },
        "page_content": "With over 16000 dialogues in the training set, the Schema-Guided Dialogue dataset is the largest publicly available annotated task-oriented dialogue dataset. The annotations include the active intents and dialogue states for each user utterance and the system actions for every system utterance. We have a few other annotations like the user actions but we withhold them from the public release. These annotations enable our dataset to be used as benchmark for tasks like intent detection, dialogue state tracking, imitation learning of dialogue policy, dialogue act to text generation etc. The schemas contain semantic information about the schema and the constituent intents and slots, in the form of natural language descriptions and other details (example in Figure FIGREF7).",
        "type": "Document"
      },
      {
        "id": "7d35aa25-51d2-485b-b327-92a5ce88fc5f",
        "metadata": {
          "vector_store_key": "1909.05016-3",
          "chunk_id": 10,
          "document_id": "1909.05016",
          "start_idx": 5361,
          "end_idx": 6092
        },
        "page_content": "Plan for operating at scale when you define the architecture. Our system architecture (Figure FIGREF1) is comprised of 3 main components: First the user input is processed through the myblueNLP component, then this data is sent to myyellowResponse Candidates, which contains a suite of neural and rule-based models, and finally the mygreenDialog Manager chooses the best response taking into account dialog history and current myblueNLP data. These components communicate with each other through the mygrayDialog State Manager, and a myredKnowledge Base component that can be queried by our components stores our knowledge bases. We build on top of CoBot BIBREF2, thus the system is scalable and new components can be added easily.",
        "type": "Document"
      },
      {
        "id": "6e9b0f65-63a1-42c5-9fbc-54943680835d",
        "metadata": {
          "vector_store_key": "1707.05853-2",
          "chunk_id": 12,
          "document_id": "1707.05853",
          "start_idx": 6484,
          "end_idx": 7198
        },
        "page_content": "The cnet GRU subsumes a standard GRU-based RNN if each token in the input is represented as a timestep with a single hypothesis. We adopt this method for the system dialog acts and the baseline model that encode only the best ASR hypothesis. In our experiments, we use the dataset provided for the second Dialog State Tracking Challenge (DSTC2) BIBREF22 that consists of user interactions with an SDS in the restaurant domain. It encompasses 1612, 506, 1117 dialogs for training, development and testing, respectively. Every dialog turn is annotated with its dialog state encompassing the three goals for area (7 values), food (93 values) and price range (5 values) and 8 requestable slots, e.g. phone and address.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What dataset is used for train/test of this method?",
    "llm_answer": "UXTD, UXSSD, UPX, and BIBREF31.\n",
    "context": [
      {
        "id": "c266479e-4dd5-481c-80c7-22f460a342f6",
        "metadata": {
          "vector_store_key": "1709.08299-1",
          "chunk_id": 9,
          "document_id": "1709.08299",
          "start_idx": 5611,
          "end_idx": 6386
        },
        "page_content": "The proposed dataset is typically used for the 1st Evaluation on Chinese Machine Reading Comprehension (CMRC-2017), which aims to provide a communication platform to the Chinese communities in the related fields. In this evaluation, we provide two tracks. We provide a shared training data for both tracks and separated evaluation data. Cloze Track: In this track, the participants are required to use the large-scale training data to train their cloze system and evaluate on the cloze evaluation track, where training and test set are exactly the same type. User Query Track: This track is designed for using transfer learning or domain adaptation to minimize the gap between cloze training data and user query evaluation data, i.e. training and testing is fairly different.",
        "type": "Document"
      },
      {
        "id": "6b8da48a-62bf-4779-a0c8-0c543a0bf17a",
        "metadata": {
          "vector_store_key": "1907.00758-2",
          "chunk_id": 23,
          "document_id": "1907.00758",
          "start_idx": 13439,
          "end_idx": 14036
        },
        "page_content": "We obtain 243,764 samples for UXTD (13.5hrs), 333,526 for UXSSD (18.5hrs), and 572,078 for UPX (31.8 hrs), or a total 1,149,368 samples (63.9hrs) which we divide into training, validation and test sets. We aim to test whether our model generalises to data from new speakers, and to data from new sessions recorded with known speakers. To simulate this, we select a group of speakers from each dataset, and hold out all of their data either for validation or for testing. Additionally, we hold out one entire session from each of the remaining speakers, and use the rest of their data for training.",
        "type": "Document"
      },
      {
        "id": "9a908f8f-2077-4b70-85b8-2941b3d7ba32",
        "metadata": {
          "vector_store_key": "1708.06022-2",
          "chunk_id": 33,
          "document_id": "1708.06022",
          "start_idx": 18725,
          "end_idx": 19404
        },
        "page_content": "In the following we introduce these datasets, provide implementation details for our model, describe the systems used for comparison, and present our results. Our model was trained on three datasets, representative of different types of QA tasks. The first two datasets focus on question answering over a structured knowledge base, whereas the third one is specific to answer sentence selection. This dataset BIBREF31 contains $3,778$ training instances and $2,032$ test instances. Questions were collected by querying the Google Suggest API. A breadth-first search beginning with wh- was conducted and the answers were crowd-sourced using Freebase as the backend knowledge base.",
        "type": "Document"
      },
      {
        "id": "e4fc706d-44a6-449c-b40f-897a083745cc",
        "metadata": {
          "vector_store_key": "1910.07181-5",
          "chunk_id": 29,
          "document_id": "1910.07181",
          "start_idx": 15945,
          "end_idx": 16597
        },
        "page_content": "Given these ingredients, our procedure consists of three steps: (i) splitting the dataset into a train set and a set of test candidates, (ii) training the baseline model on the train set and (iii) modifying a subset of the test candidates to generate the final test set. We partition $\\mathcal {D}$ into a train set $\\mathcal {D}_\\text{train}$ and a set of test candidates, $\\mathcal {D}_\\text{cand}$, with the latter containing all instances $(\\mathbf {x},y) \\in \\mathcal {D}$ such that for at least one word $w$ in $\\mathbf {x}$, $S(w) \\ne \\emptyset $. Additionally, we require that the training set consists of at least one third of the entire data.",
        "type": "Document"
      },
      {
        "id": "3be44322-a327-4b65-b36a-be0068af802a",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 36,
          "document_id": "1910.12618",
          "start_idx": 19551,
          "end_idx": 20124
        },
        "page_content": "The years 2014 and 2015 serve as validation set (730 days). It is used to tune the hyperparameters of the different approaches. All the data from January the 1st 2016 (974 days for France and 1,096 for the UK) is used as test set, on which the final results are presented. Grid search is applied to find the best combination of values: for each hyperparameter, a range of values is defined, and all the possible combinations are successively tested. The one yielding the lowest RMSE (see section SECREF4) on the validation set is used for the final results on the test one.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much is the gap between using the proposed objective and using only cross-entropy objective?",
    "llm_answer": "The context states that the proposed mixed objective significantly outperforms a purely cross-entropy model, but does not quantify the gap.\n",
    "context": [
      {
        "id": "d31215d7-2215-4657-8f9c-9bf5a77fb274",
        "metadata": {
          "vector_store_key": "1711.00106-3",
          "chunk_id": 26,
          "document_id": "1711.00106",
          "start_idx": 13808,
          "end_idx": 14594
        },
        "page_content": "The gradient of the cross entropy objective can be derived using straight-forward backpropagation. The gradient of the self-critical reinforcement learning objective is shown in equation 12 . Figure 2 illustrates how the mixed objective is computed. In practice, we find that adding the cross entropy task significantly facilitates policy learning by pruning the space of candidate trajectories - without the former, it is very difficult for policy learning to converge due to the large space of potential answers, documents, and questions. We train and evaluate our model on the Stanford Question Answering Dataset (). We show our test performance of our model against other published models, and demonstrate the importance of our proposals via ablation studies on the development set.",
        "type": "Document"
      },
      {
        "id": "7e9a56c1-4dbc-4698-bb6f-4a820d36425c",
        "metadata": {
          "vector_store_key": "1711.00106-3",
          "chunk_id": 3,
          "document_id": "1711.00106",
          "start_idx": 1922,
          "end_idx": 2647
        },
        "page_content": "Our mixed objective brings two benefits: (i) the reinforcement learning objective encourages answers that are textually similar to the ground truth answer and discourages those that are not; (ii) the cross entropy objective significantly facilitates policy learning by encouraging trajectories that are known to be correct. The resulting objective is one that is both faithful to the evaluation metric and converges quickly in practice. In addition to our mixed training objective, we extend the Dynamic Coattention Network (DCN) by BIBREF0 with a deep residual coattention encoder. This allows the network to build richer representations of the input by enabling each input sequence to attend to previous attention contexts.",
        "type": "Document"
      },
      {
        "id": "c92617af-a4d2-421e-828d-5dd8a1035dd5",
        "metadata": {
          "vector_store_key": "1711.00106-3",
          "chunk_id": 32,
          "document_id": "1711.00106",
          "start_idx": 17044,
          "end_idx": 17675
        },
        "page_content": "(shown in Figure 4 ). However, with the addition of cross entropy loss, the model quickly learns a reasonable policy and subsequently outperforms the purely cross entropy model (shown in Figure 4 ). Figure 5 compares predictions by and by the baseline on the development set. Both models retrieve answers that have sensible entity types. For example, the second example asks for \u201cwhat game\u201d and both models retrieve an American football game; the third example asks for \u201ctype of Turing machine\u201d and both models retrieve a type of turing machine. We find, however, that consistently make less mistakes on finding the correct entity.",
        "type": "Document"
      },
      {
        "id": "73ea9e21-e2b1-4295-86fd-65a8d5d1dc83",
        "metadata": {
          "vector_store_key": "1711.00106-3",
          "chunk_id": 31,
          "document_id": "1711.00106",
          "start_idx": 16745,
          "end_idx": 17393
        },
        "page_content": "The training curves for with reinforcement learning and without reinforcement learning are shown in Figure 4 to illustrate the effectiveness of our proposed mixed objective. In particular, we note that without mixing in the cross entropy loss, it is extremely difficult to learn the policy. When we combine the cross entropy loss with the reinforcement learning objective, we find that the model initially performs worse early on as it begins policy learning from scratch (shown in Figure 4 ). However, with the addition of cross entropy loss, the model quickly learns a reasonable policy and subsequently outperforms the purely cross entropy model",
        "type": "Document"
      },
      {
        "id": "338e4858-2cb4-4d9a-a60c-94b9540dc49d",
        "metadata": {
          "vector_store_key": "1711.00106-0",
          "chunk_id": 18,
          "document_id": "1711.00106",
          "start_idx": 9778,
          "end_idx": 10438
        },
        "page_content": "However, $A$ has an exact string match with the ground truth answer whereas $B$ does not. The cross entropy objective penalizes $A$ and $B$ equally, despite the former being correct under both evaluation metrics. In the less extreme case where $A$ does not have exact match but has some degree of word overlap with the ground truth, the F1 metric still prefers $A$ over $B$ despite its wrongly predicted positions. We encode this preference using reinforcement learning, using the F1 score as the reward function. Let $\\hat{s_t} \\sim _t$ and $\\hat{e_t} \\sim _t$ denote the sampled start and end positions from the estimated distributions at decoding step $t$ .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many domains of ontologies do they gather data from?",
    "llm_answer": "20\n",
    "context": [
      {
        "id": "31d601b1-1977-46b4-9511-2a2233bca8f0",
        "metadata": {
          "vector_store_key": "1811.09529-2",
          "chunk_id": 12,
          "document_id": "1811.09529",
          "start_idx": 5706,
          "end_idx": 6337
        },
        "page_content": "We describe and motivate the materials first and then proceed to the methods and motivations thereof. There are multiple ontologies available over internet with competency questions provided, but since the focus of our research is on SPARQL-OWL queries, we selected only those ontologies with CQs stated against ontology schema (T-Box). As a result we selected 5 ontologies with 234 competency questions in total. Table TABREF8 summarizes our dataset size and source of each ontology. The Software Ontology (SWO) BIBREF5 is included because its set of CQs is of substantial size and it was part of Ren et al.'s set of analysed CQs.",
        "type": "Document"
      },
      {
        "id": "786eaa4f-6843-428f-b802-626e6014bcbf",
        "metadata": {
          "vector_store_key": "2002.01359-4",
          "chunk_id": 22,
          "document_id": "2002.01359",
          "start_idx": 12592,
          "end_idx": 13359
        },
        "page_content": "The 20 domains present across the train, dev and test datasets are listed in Table TABREF10, as are the details regarding which domains are present in each of the datasets. We create synthetic implementations of a total of 45 services or APIs over these domains. Our simulator framework interacts with these services to generate dialogue outlines, which are structured representations of dialogue semantics. We then use a crowd-sourcing procedure to paraphrase these outlines to natural language utterances. Our novel crowd-sourcing procedure preserves all annotations obtained from the simulator and does not require any extra annotations after dialogue collection. In this section, we describe these steps briefly and then present analyses of the collected dataset.",
        "type": "Document"
      },
      {
        "id": "302516a5-a841-4cfc-b165-1aee27acef77",
        "metadata": {
          "vector_store_key": "1909.03242-2",
          "chunk_id": 13,
          "document_id": "1909.03242",
          "start_idx": 7571,
          "end_idx": 8252
        },
        "page_content": "The number of instances, as well as labels per domain, are shown in Table TABREF34 and label names in Table TABREF43 in the appendix. The dataset is split into a training part (80%) and a development and testing part (10% each) in a label-stratified manner. Note that the domains vary in the number of labels, ranging from 2 to 27. Labels include both straight-forward ratings of veracity (`correct', `incorrect'), but also labels that would be more difficult to map onto a veracity scale (e.g. `grass roots movement!', `misattributed', `not the whole story'). We therefore do not postprocess label types across domains to map them onto the same scale, and rather treat them as is.",
        "type": "Document"
      },
      {
        "id": "a0e7fdb9-ae87-4700-a7f4-76bf89caa336",
        "metadata": {
          "vector_store_key": "1907.11499-3",
          "chunk_id": 2,
          "document_id": "1907.11499",
          "start_idx": 957,
          "end_idx": 1759
        },
        "page_content": "The ability to handle a wide variety of domains has become more pertinent with the rise of data-hungry machine learning techniques like neural networks and their application to a plethora of textual media ranging from news articles to twitter, blog posts, medical journals, Reddit comments, and parliamentary debates BIBREF0 , BIBREF3 , BIBREF4 , BIBREF5 . The question of how to best deal with multiple domains when training data is available for one or few of them has met with much interest in the literature. The field of domain adaptation BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 aims at improving the learning of a predictive function in a target domain where there is little or no labeled data, using knowledge transferred from a source domain where sufficient labeled data is available.",
        "type": "Document"
      },
      {
        "id": "ec11c575-9cdc-40f2-b742-781c89922785",
        "metadata": {
          "vector_store_key": "1701.02877-9",
          "chunk_id": 11,
          "document_id": "1701.02877",
          "start_idx": 5586,
          "end_idx": 6230
        },
        "page_content": "Domain describes the dominant subject matter of text, which might give specialised vocabulary or specific, unusal word senses. For example, \u201cbroadcast news\" is a genre, describing the manner of use of language, whereas \u201cfinancial text\" or \u201cpopular culture\" are domains, describing the topic. One notable exception to this terminology is social media, which tends to be a blend of myriad domains and genres, with huge variation in both these dimensions BIBREF38 , BIBREF39 ; for simplicity, we also refer to this as a genre here. In chronological order, the first corpus included here is MUC 7, which is the last of the MUC challenges BIBREF31 .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what is the practical application for this paper?",
    "llm_answer": "Understanding different aspects of global governance; empowering rural communities; text generation tasks (e.g., natural language generation, sentence compression); language identification of tweets; predictors for renal failure; political affiliation classifiers; hate-speech detection.\n",
    "context": [
      {
        "id": "e900003c-e4a4-4760-af29-1415e5bd0e01",
        "metadata": {
          "vector_store_key": "1708.05873-0",
          "chunk_id": 35,
          "document_id": "1708.05873",
          "start_idx": 20206,
          "end_idx": 20343
        },
        "page_content": "The paper more broadly demonstrates how text analytic approaches can help us to better understand different aspects of global governance.",
        "type": "Document"
      },
      {
        "id": "1315e308-8ad4-4041-a70d-d08fd36d160a",
        "metadata": {
          "vector_store_key": "1912.03457-0",
          "chunk_id": 48,
          "document_id": "1912.03457",
          "start_idx": 27355,
          "end_idx": 27803
        },
        "page_content": "It is promising to observe increased innovation and steady progress in the empowerment of rural communities using language tools. Increased exposure to the challenges and works in this area can catalyse developments in improving inclusion and information dissemination. We hope that this paper will provide pointers in the right direction for potential ventures that plan on introducing disruptive language technologies to marginalized communities.",
        "type": "Document"
      },
      {
        "id": "2b0855e5-b36f-4314-b3e0-1f2da26facc8",
        "metadata": {
          "vector_store_key": "1909.00578-4",
          "chunk_id": 23,
          "document_id": "1909.00578",
          "start_idx": 12727,
          "end_idx": 13118
        },
        "page_content": "It could thus be applied to other text generation tasks, such as natural language generation and sentence compression. We would like to thank the anonymous reviewers for their helpful feedback on this work. The work has been partly supported by the Research Center of the Athens University of Economics and Business, and by the French National Research Agency under project ANR-16-CE33-0013.",
        "type": "Document"
      },
      {
        "id": "f7c53cf5-b313-4429-99e5-a3eef6decad0",
        "metadata": {
          "vector_store_key": "1909.00430-0",
          "chunk_id": 42,
          "document_id": "1909.00430",
          "start_idx": 23091,
          "end_idx": 23829
        },
        "page_content": "Other possible application of the method includes training language identification of tweets given geo-location supervision (knowing the geographical region gives a prior on languages spoken), training predictors for renal failure from textual medical records given classifier for diabetes (there is a strong correlation between the two conditions), training a political affiliation classifier from social media tweets based on age-group classifiers, zip-code information, or social-status classifiers (there are known correlations between all of these to political affiliation), training hate-speech detection based on emotion detection, and so on. The work was supported in part by The Israeli Science Foundation (grant number 1555/15).",
        "type": "Document"
      },
      {
        "id": "c7a7d520-334a-4d98-91ce-3e9926210935",
        "metadata": {
          "vector_store_key": "1909.03023-1",
          "chunk_id": 28,
          "document_id": "1909.03023",
          "start_idx": 15890,
          "end_idx": 16580
        },
        "page_content": "We demonstrated that the scheme can be annotated with high reliability and reported on scheme validity. Finally, we discussed some possible applications and challenges posed by the proposed annotation scheme for both the educational and NLP communities. We plan to extend our annotation scheme to label information about collaborative relations between different argument moves, and release a corpus annotated with the extended scheme. We want to thank Haoran Zhang, Tazin Afrin, and Annika Swallen for their contribution, and all the anonymous reviewers for their helpful suggestions. This work was supported by the Learning Research and Development Center at the University of Pittsburgh.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What's the method used here?",
    "llm_answer": "A method for question answering using specific rules, falling back to generic rules, and throwing out QA pairs if no output is found.  The appropriate sentence is then selected based on the index, and the tuple is aligned with a sentence from the input passage.\n",
    "context": [
      {
        "id": "70508894-a759-46ac-b6a6-32d3f6df2128",
        "metadata": {
          "vector_store_key": "1905.07471-2",
          "chunk_id": 13,
          "document_id": "1905.07471",
          "start_idx": 7241,
          "end_idx": 7940
        },
        "page_content": "It first tries very specific rules (e.g. a parser for how questions where no subject is identified), then falls down to more generic rules. If no output is returned after all the methods are tried we throw the QA pair out. Otherwise, we find the appropriate sentence in the passage based on the index. Following QA to tuple conversion, the tuple must be aligned with a sentence in the input passage. We segment the passage into sentences using periods as delimiters. The sentence containing the answer is taken as the input sentence for the tuple. Outputted sentences predominantly align with their tuple, but some exhibit partial misalignment in the case of some multi-sentence reasoning questions.",
        "type": "Document"
      },
      {
        "id": "7c585b62-b44b-418a-8e3b-f980dbe05e36",
        "metadata": {
          "vector_store_key": "1601.02543-0",
          "chunk_id": 4,
          "document_id": "1601.02543",
          "start_idx": 2135,
          "end_idx": 2687
        },
        "page_content": "We then show how this method was adopted to evaluate a speech recognition based solution as a case study. This is the main contribution of the paper. The rest of the paper is organized as follows. The method for evaluation without testing is described in Section SECREF2 . In Section SECREF3 we present a case study and conclude in Section SECREF4 . Fig. FIGREF1 shows the schematic of a typical menu based speech solution having 3 nodes. At each node there are a set of words that the user is expected to speak and the system is supposed to recognize.",
        "type": "Document"
      },
      {
        "id": "79853218-a120-4757-ad23-8cfca74bd91b",
        "metadata": {
          "vector_store_key": "1908.07218-2",
          "chunk_id": 6,
          "document_id": "1908.07218",
          "start_idx": 3602,
          "end_idx": 4390
        },
        "page_content": "Illustrated in Figure 2 , the extraction algorithm before refinement consists of five steps. Definition concept expansion. As many words are synonymous with some concepts, many word senses are defined trivially by one concept. For example, the definition of UTF8bkai\u99ff\u99ac (excellent steed) is simply {UTF8bkai\u99ff\u99ac $\\vert $ ExcellentSteed}. The triviality is resolved by expanding such definitions by one layer, e.g., replacing {UTF8bkai\u99ff\u99ac $\\vert $ ExcellentSteed} with {UTF8bkai\u99ac $\\vert $ horse:qualification={HighQuality $\\vert $ UTF8bkai\u512a\u8cea}}, i.e., the definition of UTF8bkai\u99ff\u99ac $\\vert $ ExcellentSteed. Definition string parsing. We parse each definition into a directed graph. Each node in the graph is either a word, a concept, or a function relation, e.g., or() at the bottom of Table 1 .",
        "type": "Document"
      },
      {
        "id": "988d515b-27b5-41d1-9f19-6206835c09bd",
        "metadata": {
          "vector_store_key": "1703.06492-5",
          "chunk_id": 30,
          "document_id": "1703.06492",
          "start_idx": 14772,
          "end_idx": 15374
        },
        "page_content": "Also, we encode the query question as a column vector, 4800 by 1 dimensions, by Skip-Thought Vectors, called $\\mathbf {b}$ . Now, we can solve the $LASSO$ optimization problem, mentioned in Section 5.3, to get the solution, $\\mathbf {x}$ . Here, we consider the elements, in solution vector $\\mathbf {x}$ , as the weights of the corresponding BQ in BQ matrix, $A$ . The first element of $\\mathbf {x}$ corresponds to the first column, i.e. the first BQ, of $A$ . Then, we rank the all weights in $\\mathbf {x}$ and pick up the top 3 large weights with corresponding BQ to be the BQ of the query question.",
        "type": "Document"
      },
      {
        "id": "bb7506d2-3899-48e2-8c8e-51c11908de9c",
        "metadata": {
          "vector_store_key": "1808.09920-2",
          "chunk_id": 18,
          "document_id": "1808.09920",
          "start_idx": 9575,
          "end_idx": 10205
        },
        "page_content": "The algorithm is parameterized by a graph convolutional network (GCN) BIBREF13 , in particular, we employ relational-GCNs BIBREF17 , an extended version that accommodates edges of different types. In Section \"Entity relational graph convolutional network\" we describe the propagation rule. Each step of the algorithm (also referred to as a hop) updates all node representations in parallel. In particular, a node is updated as a function of messages from its direct neighbours, and a message is possibly specific to a certain relation. At the end of the first step, every node is aware of every other node it connects directly to.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does their method outperform state-of-the-art OOD detection?",
    "llm_answer": "More than 17 points in Precision@K on average.\n",
    "context": [
      {
        "id": "284da564-31ef-44a4-ab4c-eef2110915fd",
        "metadata": {
          "vector_store_key": "1905.10247-2",
          "chunk_id": 22,
          "document_id": "1905.10247",
          "start_idx": 12072,
          "end_idx": 12783
        },
        "page_content": "This superior stability nicely contrasts with the high sensitivity of AE-HCN-Indep with regard to threshold values as shown in Table TABREF25 . We proposed a novel OOD detection method that does not require OOD data without any restrictions by utilizing counterfeit OOD turns in the context of a dialog. We also release new dialog datasets which are three publicly available dialog corpora augmented with natural OOD turns to foster further research. In the presence of OOD utterances, our method outperforms state-of-the-art dialog models equipped with an OOD detection mechanism by a large margin \u2014 more than 17 points in Precision@K on average \u2014 while minimizing performance trade-off on in-domain test data.",
        "type": "Document"
      },
      {
        "id": "5f988717-ce30-4ab1-8b83-431c9f77779e",
        "metadata": {
          "vector_store_key": "1905.10247-2",
          "chunk_id": 23,
          "document_id": "1905.10247",
          "start_idx": 12380,
          "end_idx": 13081
        },
        "page_content": "In the presence of OOD utterances, our method outperforms state-of-the-art dialog models equipped with an OOD detection mechanism by a large margin \u2014 more than 17 points in Precision@K on average \u2014 while minimizing performance trade-off on in-domain test data. The detailed analysis sheds light on the difficulty of optimizing context-independent OOD detection and justifies the necessity of context-aware OOD handling models. We plan to explore other ways of scoring OOD utterances than autoencoders. For example, variational autoencoders or generative adversarial networks have great potential. We are also interested in using generative models to produce more realistic counterfeit user utterances.",
        "type": "Document"
      },
      {
        "id": "9adc97ac-d984-46d5-a9b8-22cbd9afc165",
        "metadata": {
          "vector_store_key": "1905.10247-3",
          "chunk_id": 21,
          "document_id": "1905.10247",
          "start_idx": 11413,
          "end_idx": 12072
        },
        "page_content": "Note that the best performance achieved at 9 is still not as good as that of AE-HCN(-CNN). This implies that we can perform better OOD detection by jointly considering other context features. Finally, we conduct a sensitivity analysis by varying counterfeit OOD probabilities. Table TABREF26 shows performances of AE-HCN-CNN on bAbI6 Test-OOD with different INLINEFORM0 values, ranging from 5% to 30%. The result indicates that our method manages to produce good performance without regard to the INLINEFORM1 value. This superior stability nicely contrasts with the high sensitivity of AE-HCN-Indep with regard to threshold values as shown in Table TABREF25 .",
        "type": "Document"
      },
      {
        "id": "17768857-f173-49db-8a5a-db91b0d8368a",
        "metadata": {
          "vector_store_key": "1905.10247-3",
          "chunk_id": 18,
          "document_id": "1905.10247",
          "start_idx": 9890,
          "end_idx": 10652
        },
        "page_content": "Since there are multiple actions that are appropriate for a given dialog context, we use per-utterance Precision@K as performance metric. We also report f1-score for OOD detection to measure the balance between precision and recall. The performances of HCN on Test-OOD are about 15 points down on average from those on Test, showing the detrimental impact of OOD utterances to such models only trained on in-domain training data. AE-HCN(-CNN) outperforms HCN on Test-OOD by a large margin about 17(20) points on average while keeping the minimum performance trade-off compared to Test. Interestingly, AE-HCN-CNN has even better performance than HCN on Test, indicating that, with the CNN encoder, counterfeit OOD augmentation acts as an effective regularization.",
        "type": "Document"
      },
      {
        "id": "8047594f-acaa-42d5-a69a-0caa55a4f886",
        "metadata": {
          "vector_store_key": "1905.10247-2",
          "chunk_id": 1,
          "document_id": "1905.10247",
          "start_idx": 750,
          "end_idx": 1450
        },
        "page_content": "There have been a set of prior approaches for OOD detection which require both in-domain (IND) and OOD data BIBREF0 , BIBREF1 . However, it is a formidable task to collect sufficient data to cover in theory unbounded variety of OOD utterances. In contrast, BIBREF2 introduced an in-domain verification method that requires only IND utterances. Later, with the rise of deep neural networks, BIBREF3 proposed an autoencoder-based OOD detection method which surpasses prior approaches without access to OOD data. However, those approaches still have some restrictions such that there must be multiple sub-domains to learn utterance representation and one must set a decision threshold for OOD detection.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are dilated convolutions?",
    "llm_answer": "Dilated convolutions skip some input values so that the convolution kernel is applied over a larger area than its own, operating on a larger scale without increasing the number of parameters.\n",
    "context": [
      {
        "id": "36209a59-043f-4e70-8d79-f955e38846d2",
        "metadata": {
          "vector_store_key": "1811.07684-0",
          "chunk_id": 7,
          "document_id": "1811.07684",
          "start_idx": 4331,
          "end_idx": 4971
        },
        "page_content": "Standard convolutional networks cannot capture long temporal patterns with reasonably small models due to the increase in computational cost yielded by larger receptive fields. Dilated convolutions skip some input values so that the convolution kernel is applied over a larger area than its own. The network therefore operates on a larger scale, without the downside of increasing the number of parameters. The receptive field $r$ of a network made of stacked convolutions indeed reads: $r = \\sum _i d_i (s_i - 1),$  where $d_i$ refers to the dilation rate ( $d_i=1$ for normal convolutions) and $s_i$ the filter size of the $i^{th}$ layer.",
        "type": "Document"
      },
      {
        "id": "6b43fe2c-d41b-4253-939b-b741fe3dcf47",
        "metadata": {
          "vector_store_key": "1811.07684-0",
          "chunk_id": 10,
          "document_id": "1811.07684",
          "start_idx": 5591,
          "end_idx": 6315
        },
        "page_content": "A large temporal dependency, can therefore be achieved by stacking multiple dilated convolution layers. By inserting residual connections between each layer, we are able to train a network of 24 layers on relatively small amount of data, which corresponds to a receptive field of 182 frames or 1.83s. The importance of gating and residual connections is analyzed in Section 3.3.2. In addition to reducing the model size, dilated convolutions allow the network to run in a streaming fashion during inference, drastically reducing the computational cost. When receiving a new input frame, the corresponding posteriors are recovered using previous computations, kept in memory for efficiency purposes as described in Figure 2 .",
        "type": "Document"
      },
      {
        "id": "b6b74753-048b-4090-959b-21761e2ed81c",
        "metadata": {
          "vector_store_key": "1811.07684-0",
          "chunk_id": 8,
          "document_id": "1811.07684",
          "start_idx": 4562,
          "end_idx": 5207
        },
        "page_content": "The receptive field $r$ of a network made of stacked convolutions indeed reads: $r = \\sum _i d_i (s_i - 1),$  where $d_i$ refers to the dilation rate ( $d_i=1$ for normal convolutions) and $s_i$ the filter size of the $i^{th}$ layer. Additionally, causal convolutions kernels ensure a causal ordering of input frames: the prediction emitted at time $t$ only depends on previous time stamps. It allows to reduce the latency at inference time. As mentioned in BIBREF10 , gated activations units \u2013 a combination of tanh and sigmoid activations controlling the propagation of information to the next layer \u2013 prove to efficiently model audio signals.",
        "type": "Document"
      },
      {
        "id": "89f3e62d-44ec-4b64-8f61-a8ab45b63960",
        "metadata": {
          "vector_store_key": "1603.04513-2",
          "chunk_id": 16,
          "document_id": "1603.04513",
          "start_idx": 9510,
          "end_idx": 10349
        },
        "page_content": "$\\mathbf {w}_i\\in \\mathbb {R}^{d}$ denotes the embedding of word $w_i$ ; a convolution layer uses sliding filters to extract local features of that sentence. The filter width $l$ is a parameter. We first concatenate the initialized embeddings of $l$ consecutive words ( $\\mathbf {w}_{i-l+1},\n\\ldots , \\mathbf {w}_i$ ) as $\\mathbf {c}_i\\in \\mathbb {R}^{ld}$ $(1\\le i <s+l)$ , then generate the feature value of this phrase as $\\textbf {p}_i$ (the whole vector $w_1, w_2, \\ldots , w_s$0 contains all the local features) using a tanh activation function and a linear projection vector $w_1, w_2, \\ldots , w_s$1 as:  $$\\mathbf {p}_i=\\mathrm {tanh}(\\mathbf {v}^\\mathrm {T}\\mathbf {c}_i+b)$$   (Eq. 2)  More generally, convolution operation can deal with multiple input feature maps and can be stacked to yield feature maps of increasing layers.",
        "type": "Document"
      },
      {
        "id": "61165ee7-80b2-4757-8e55-827948b80b26",
        "metadata": {
          "vector_store_key": "1910.08210-3",
          "chunk_id": 21,
          "document_id": "1910.08210",
          "start_idx": 11636,
          "end_idx": 12317
        },
        "page_content": "Exact dimensions of these variables are shown in Table TABREF42 in appendix SECREF8. Let $_$ denote a fixed-length $_$-dimensional representation of the text and $_$ the representation of visual inputs with height $H$, width $W$, and $_$ channels. Let $$ denote a convolution layer. Let + and * symbols denote element-wise addition and multiplication operations that broadcast over spatial dimensions. We first modulate visual features using text features: Unlike FiLM, we additionally modulate text features using visual features: The output of the layer consists of the sum of the modulated features $$, as well as a max-pooled summary $$ over this sum across spatial dimensions.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what are the three methods presented in the paper?",
    "llm_answer": "An active learning method for CWS, a method for evaluation without testing, and a speech recognition based solution.\n",
    "context": [
      {
        "id": "94d7deb7-476c-4597-b766-63cad5a82f44",
        "metadata": {
          "vector_store_key": "1906.10551-8",
          "chunk_id": 5,
          "document_id": "1906.10551",
          "start_idx": 2763,
          "end_idx": 3519
        },
        "page_content": "The rest of this paper is structured as follows. Section SECREF2 discusses the related work that served as an inspiration for our analysis. Section SECREF3 comprises the proposed criteria and properties to characterize AV methods. Section SECREF4 describes the methodology, consisting of the used corpora, examined AV methods, selected performance measures and experiments. Finally, Section SECREF5 concludes the work and outlines future work. Over the years, researchers in the field of authorship analysis identified a number of challenges and limitations regarding existing studies and approaches. Azarbonyad et al. BIBREF8 , for example, focused on the questions if the writing styles of authors of short texts change over time and how this affects AA.",
        "type": "Document"
      },
      {
        "id": "482acd0f-398b-47d3-9b79-e01b3396adec",
        "metadata": {
          "vector_store_key": "1908.08419-1",
          "chunk_id": 6,
          "document_id": "1908.08419",
          "start_idx": 3477,
          "end_idx": 4155
        },
        "page_content": "To sum up, the main contributions of our work are summarized as follows: The rest of this paper is organized as follows. Section SECREF2 briefly reviews the related work on CWS and active learning. Section SECREF3 presents an active learning method for CWS. We experimentally evaluate our proposed method in Section SECREF4 . Finally, Section SECREF5 concludes the paper and envisions on future work. In past decades, researches on CWS have a long history and various methods have been proposed BIBREF13 , BIBREF14 , BIBREF15 , which is an important task for Chinese NLP BIBREF7 . These methods are mainly focus on two categories: supervised learning and deep learning BIBREF2 .",
        "type": "Document"
      },
      {
        "id": "7c585b62-b44b-418a-8e3b-f980dbe05e36",
        "metadata": {
          "vector_store_key": "1601.02543-0",
          "chunk_id": 4,
          "document_id": "1601.02543",
          "start_idx": 2135,
          "end_idx": 2687
        },
        "page_content": "We then show how this method was adopted to evaluate a speech recognition based solution as a case study. This is the main contribution of the paper. The rest of the paper is organized as follows. The method for evaluation without testing is described in Section SECREF2 . In Section SECREF3 we present a case study and conclude in Section SECREF4 . Fig. FIGREF1 shows the schematic of a typical menu based speech solution having 3 nodes. At each node there are a set of words that the user is expected to speak and the system is supposed to recognize.",
        "type": "Document"
      },
      {
        "id": "abde311f-db3e-44fd-96aa-882a0a2aeeb0",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 17,
          "document_id": "1912.13109",
          "start_idx": 8552,
          "end_idx": 9307
        },
        "page_content": "These processes and its implementation details will be discussed later but they were learning rate decay, early stopping, model checkpointing and reducing learning rate on plateau. For the loss function we chose categorical cross entropy loss in finding the most optimal weights/parameters of the model. Formally this loss function for the model is defined as below: The double sum is over the number of observations and the categories respectively. While the model probability is the probability that the observation i belongs to category c. Among the model architectures we experimented with and without data augmentation were: Fully Connected dense networks: Model hyperparameters were inspired from the previous work done by Vo et al and Mathur et al.",
        "type": "Document"
      },
      {
        "id": "a8e7fe2b-088f-4875-997d-c9f49a8bf8b3",
        "metadata": {
          "vector_store_key": "1906.10551-3",
          "chunk_id": 4,
          "document_id": "1906.10551",
          "start_idx": 2134,
          "end_idx": 2763
        },
        "page_content": "These, however, must be taken into account before AV methods can be applied in real forensic settings. The objective of this paper is to fill this gap and to propose important properties and criteria that are not only intended to characterize AV methods, but also allow their assessment in a more systematic manner. By this, we hope to contribute to the further development of this young research field. Based on the proposed properties, we investigate the applicability of 12 existing AV approaches on three self-compiled corpora, where each corpus involves a specific challenge. The rest of this paper is structured as follows.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what datasets did the authors use?",
    "llm_answer": "novel-9, novel-50, IMDB62, SCI, Stat2015, Stat2016, CS2016\n",
    "context": [
      {
        "id": "6d614157-50dc-4aaa-af03-adc7f0282a12",
        "metadata": {
          "vector_store_key": "1709.02271-3",
          "chunk_id": 20,
          "document_id": "1709.02271",
          "start_idx": 11344,
          "end_idx": 11969
        },
        "page_content": "(Section SECREF26 ). The statistics for the three datasets used in the experiments are summarized in Table TABREF16 . novel-9. This dataset was compiled by F&H14: a collection of 19 novels by 9 nineteenth century British and American authors in the Project Gutenberg. To compare to F&H14, we apply the same resampling method (F&H14, Section 4.2) to correct the imbalance in authors by oversampling the texts of less-represented authors. novel-50. This dataset extends novel-9, compiling the works of 50 randomly selected authors of the same period. For each author, we randomly select 5 novels for a total 250 novels. IMDB62.",
        "type": "Document"
      },
      {
        "id": "c5f16988-f9bb-43d0-a24e-a2d52ae2d42a",
        "metadata": {
          "vector_store_key": "1802.05574-4",
          "chunk_id": 11,
          "document_id": "1802.05574",
          "start_idx": 6530,
          "end_idx": 7227
        },
        "page_content": "This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available and the corpus provides both an XML and a simple text version of each article. We randomly selected 2 sentences with more than two words from each paper using the simple text version of the paper.",
        "type": "Document"
      },
      {
        "id": "711988d5-b6d5-455f-a849-15926d3a08fb",
        "metadata": {
          "vector_store_key": "1910.12618-2",
          "chunk_id": 12,
          "document_id": "1910.12618",
          "start_idx": 6819,
          "end_idx": 7418
        },
        "page_content": "The rest of this paper is organized as follows. The following section introduces the two data sets used to conduct our study. Section 3 presents the different machine learning approaches used and how they were tuned. Section 4 highlights the main results of our study, while section 5 concludes this paper and gives insight on future possible work. In order to prove the consistency of our work, experiments have been conducted on two data sets, one for France and the other for the UK. In this section details about the text and time series data are given, as well as the major preprocessing steps.",
        "type": "Document"
      },
      {
        "id": "f57f61de-28fb-415a-97ae-9d7e1c849440",
        "metadata": {
          "vector_store_key": "1911.13066-4",
          "chunk_id": 7,
          "document_id": "1911.13066",
          "start_idx": 4428,
          "end_idx": 5163
        },
        "page_content": "The dataset consists of more than $0.3$ million records and has been made available for future research. The rest of the paper is organized as follows. Section SECREF2 defines the dataset acquiring process and provides an explanation of the class labels. In section SECREF3, the architecture of the proposed model, its hyperparameters, and the experimental setup is discussed. We discuss the results in section SECREF4 and finally, concluding remarks are presented in section SECREF5. . The dataset consists of SMS feedbacks of the citizens of Pakistan on different public services availed by them. The objective of collecting these responses is to measure the performance of government departments rendering different public services.",
        "type": "Document"
      },
      {
        "id": "8e741c56-0352-4660-89e2-d995b2fada35",
        "metadata": {
          "vector_store_key": "1807.09671-7",
          "chunk_id": 34,
          "document_id": "1807.09671",
          "start_idx": 19653,
          "end_idx": 20306
        },
        "page_content": "While the 2nd and 3rd data sets are from the same course, Statistics for Industrial Engineers, they were taught in 2015 and 2016 respectively (henceforth Stat2015 and Stat2016), at the Bo\u01e7azi\u00e7i University in Turkey. The course was taught in English while the official language is Turkish. The last one is from a fundamental undergraduate Computer Science course (data structures) at a local U.S. university taught in 2016 (henceforth CS2016). Another reason we choose the student responses is that we have advanced annotation allowing us to perform an intrinsic evaluation to test whether the low-rank approximation does capture similar concepts or not.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much performance improvements they achieve on SQuAD?",
    "llm_answer": "A 0.72 F1 score on the original SQuAD development set, and an improvement across the board in different question types (with notable improvements for \"What\" questions).  The context also mentions an 8% F1 score increase for I entities when adding additional data.\n",
    "context": [
      {
        "id": "27c06054-f78a-4b54-8af0-9a5fdbc210ce",
        "metadata": {
          "vector_store_key": "1912.10435-1",
          "chunk_id": 23,
          "document_id": "1912.10435",
          "start_idx": 13269,
          "end_idx": 13990
        },
        "page_content": "Batch Size - Default: 12, We had to use a batch size of 6 for all our experiments due to resource constraints and out of memory issues on the GPU for any larger batch size. Number of epochs - Default: 2 On increasing the number of epochs we saw a significant degradation in performance (-3 F1 score), we attribute this to the fact that the model starts to overfit to the training data with high variance and since the batch size is smaller the gradient updates could be noisy not allowing it to optimally converge. Learning Rate - Default: 3e-5 We wrote a script to help us find the optimal learning rate using grid search and found the optimal learning rates for SQuAD 2.0 and SQuAD 2.Q respectively for batch size of 6.",
        "type": "Document"
      },
      {
        "id": "2728e40a-66bd-4fcd-95d8-fcf1f01e14cf",
        "metadata": {
          "vector_store_key": "2001.11268-0",
          "chunk_id": 44,
          "document_id": "2001.11268",
          "start_idx": 25385,
          "end_idx": 26008
        },
        "page_content": "We never used the full SQuAD data in order to reduce time for training but observed increased performance when adding additional data. For classifying I entities, an increase from 20 to 200 additional SQuAD domains resulted in an increase of 8% for the F1 score, whereas the increase for the O domain was less than 1%. After training a model with 200 additional SQuAD domains, we also evaluated it on the original SQuAD development set and obtained a F1 score of 0.72 for this general reading comprehension task. In this evaluation, the F1 scores represent the overlap of labelled and predicted answer spans on token level.",
        "type": "Document"
      },
      {
        "id": "f59ac0e1-2db3-40ef-845e-0602ffc3a8c6",
        "metadata": {
          "vector_store_key": "1912.10435-1",
          "chunk_id": 34,
          "document_id": "1912.10435",
          "start_idx": 18529,
          "end_idx": 19173
        },
        "page_content": "Note that questions that did not fit into the 7 bins were classified as \"Other\". An example of a question in the \"Other\" category would be an \"Is it?\" question which is a minority set in SQUAD 2.0. Over the baseline, our model pretty much presents an overall improvement across the board in the different type of questions in the SQuAD 2.0 dev set. In the case of \"Which\" questions, our model goes wrong 69 times where as the baseline model goes wrong 64 times, a very small numeric difference. However, for the \"What\" questions the baseline model produces incorrect outputs for 776 examples while our model produces 30 fewer incorrect outputs.",
        "type": "Document"
      },
      {
        "id": "f18b158c-e89a-4c79-8d29-c5a76dca187f",
        "metadata": {
          "vector_store_key": "1902.00821-0",
          "chunk_id": 44,
          "document_id": "1902.00821",
          "start_idx": 23195,
          "end_idx": 23845
        },
        "page_content": "We perform pre-tuning for 10k steps as further increasing the pre-tuning steps doesn't yield better results. We fine-tune 6 epochs, though most runs converged just within 3 epochs due to the pre-trained/tuned weights of BERT. Results are reported as averages of 3 runs of fine-tuning (3 different random seeds for tuning batch generation). To be consistent with existing research, we leverage the same evaluation script from CoQA. Similar to the evaluation of SQuAD 2.0, CoQA script reports turn-level Exact Match (EM) and F1 scores for all turns in all dialogues. EM requires the answers to have exact string match with human annotated answer spans.",
        "type": "Document"
      },
      {
        "id": "3b2db0e7-2215-431f-ba01-e66abad5053b",
        "metadata": {
          "vector_store_key": "1703.04617-0",
          "chunk_id": 38,
          "document_id": "1703.04617",
          "start_idx": 21181,
          "end_idx": 21823
        },
        "page_content": "Training data includes 87,599 instances and validation set has 10,570 instances. The test data is hidden and kept by the organizer. The evaluation of SQuAD is Exact Match (EM) and F1 score. We use pre-trained 300-D Glove 840B vectors BIBREF20 to initialize our word embeddings. Out-of-vocabulary (OOV) words are initialized randomly with Gaussian samples. CharCNN filter length is 1,3,5, each is 50 dimensions. All vectors including word embedding are updated during training. The cluster number K in discriminative block is 100. The Adam method BIBREF25 is used for optimization. And the first momentum is set to be 0.9 and the second 0.999.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the baseline?",
    "llm_answer": "Neural baselines, built using shared training data, chosen based on cloze validation set performance, and further tuned using user query validation data with 10-fold cross-validations.  Specific examples include a neural baseline with ELMo contextual vectors, BIBREF7, i-vector and x-vector systems.  Also, majority class, exact match, first occurrence, and LSTM baselines are mentioned.\n",
    "context": [
      {
        "id": "fa88df39-033d-4e0f-891b-162e0f9b0424",
        "metadata": {
          "vector_store_key": "1709.08299-4",
          "chunk_id": 19,
          "document_id": "1709.08299",
          "start_idx": 11042,
          "end_idx": 11836
        },
        "page_content": "The neural baselines are built by the following steps. We first use the shared training data to build a general systems, and choose the best performing model (in terms of cloze validation set) as baseline. Use User Query validation data for further tuning the systems with 10-fold cross-validations. Increase dropout rate BIBREF9 to 0.5 for preventing over-fitting issue. All baseline systems are chosen according to the performance of the validation set. The participant system results are given in Table 2 and 3 . As we can see that two neural baselines are competitive among participant systems and AoA Reader successfully outperform AS Reader and all participant systems in single model condition, which proves that it is a strong baseline system even without further fine-tuning procedure.",
        "type": "Document"
      },
      {
        "id": "e7065ee9-867c-433d-ac3c-08844aac0090",
        "metadata": {
          "vector_store_key": "1904.01608-7",
          "chunk_id": 38,
          "document_id": "1904.01608",
          "start_idx": 21241,
          "end_idx": 21885
        },
        "page_content": "On this dataset, the best baseline is the neural baseline with addition of ELMo contextual vectors achieving an F1 score of 82.6 followed by BIBREF7 , which is expected because neural models generally achieve higher gains when more training data is available and because BIBREF7 was not designed with the SciCite dataset in mind. The breakdown of results by intent on ACL-ARC and SciCite datasets is respectively shown in Tables 5 and 6 . Generally we observe that results on categories with more number of instances are higher. For example on ACL-ARC, the results on the Background category are the highest as this category is the most common.",
        "type": "Document"
      },
      {
        "id": "02bef31f-a2b5-4b89-8063-62dc0b0f91eb",
        "metadata": {
          "vector_store_key": "1911.01799-4",
          "chunk_id": 19,
          "document_id": "1911.01799",
          "start_idx": 11188,
          "end_idx": 11946
        },
        "page_content": "Two state-of-the-art baseline systems were built following the Kaldi SITW recipe BIBREF22: an i-vector system BIBREF3 and an x-vector system BIBREF10. For the i-vector system, the acoustic feature involved 24-dimensional MFCCs plus the log energy, augmented by the first- and second-order derivatives. We also applied the cepstral mean normalization (CMN) and the energy-based voice active detection (VAD). The universal background model (UBM) consisted of $2,048$ Gaussian components, and the dimensionality of the i-vector space was 400. LDA was applied to reduce the dimensionality of the i-vectors to 150. The PLDA model was used for scoring BIBREF4. For the x-vector system, the feature-learning component was a 5-layer time-delay neural network (TDNN).",
        "type": "Document"
      },
      {
        "id": "5e76f24e-4c75-4a34-b69c-7fe1f8d12771",
        "metadata": {
          "vector_store_key": "1909.02635-2",
          "chunk_id": 15,
          "document_id": "1909.02635",
          "start_idx": 9069,
          "end_idx": 9719
        },
        "page_content": "This includes (i) Majority Class, (ii) Exact Match of an ingredient $e$ in recipe step $s_t$, and (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo$_{token}$ and ELMo$_{sent}$) BIBREF22 compared to GPT.",
        "type": "Document"
      },
      {
        "id": "6b22fb3d-fcd5-44c9-bc40-359ccb0e34cc",
        "metadata": {
          "vector_store_key": "1805.04033-6",
          "chunk_id": 34,
          "document_id": "1805.04033",
          "start_idx": 18664,
          "end_idx": 19368
        },
        "page_content": "To gain a better understanding of the results, we analyze the summaries generated by the baseline model and our proposed model. Some of the summaries are listed in Table TABREF28 . As shown in the table, the summaries generated by the proposed method are much better than the baseline, and we believe they are more precise and informative than the references. In the first one, the baseline system generates a grammatical but unrelated summary, while the proposed method generates a more informative summary. In the second one, the baseline system generates a related but ungrammatical summary, while the proposed method generates a summary related to the source content but different from the reference.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the network architecture?",
    "llm_answer": "The architecture is based on BiDAF with an explicit reasoning module, five main modules (input, attention, reasoning, modeling, output), and multimodal input (procedural text, image question/answers).  Other architectures mentioned include a multi-view network, a DQN regression network, and a PPO agent with actor and critic networks.  RNN-based embedding models are also part of the architecture.\n",
    "context": [
      {
        "id": "3cf1728c-0711-4400-a389-d0f3ab12da96",
        "metadata": {
          "vector_store_key": "1909.08859-0",
          "chunk_id": 8,
          "document_id": "1909.08859",
          "start_idx": 4401,
          "end_idx": 5098
        },
        "page_content": "Its architecture is based on a bi-directional attention flow (BiDAF) model BIBREF6, but also equipped with an explicit reasoning module that acts on entity-specific relational memory units. Fig. FIGREF4 shows an overview of the network architecture. It consists of five main modules: An input module, an attention module, a reasoning module, a modeling module, and an output module. Note that the question answering tasks we consider here are multimodal in that while the context is a procedural text, the question and the multiple choice answers are composed of images. Input Module extracts vector representations of inputs at different levels of granularity by using several different encoders.",
        "type": "Document"
      },
      {
        "id": "b8324d9d-5365-47cd-926e-dc2f2a6c84a3",
        "metadata": {
          "vector_store_key": "1704.05907-1",
          "chunk_id": 22,
          "document_id": "1704.05907",
          "start_idx": 12004,
          "end_idx": 12496
        },
        "page_content": "Unlike many neural approaches to classification, our architecture emphasizes network width in addition to depth, enhancing gradient flow during training. We have used the multi-view network architecture to establish new state-of-the-art results on two benchmark text classification tasks. In the future, we wish to better understand the benefits of generating multiple views, explore new sources of base features, and apply this technique to other NLP problems such as translation or tagging.",
        "type": "Document"
      },
      {
        "id": "b90b5667-0046-4454-a0ea-8d4393be1165",
        "metadata": {
          "vector_store_key": "1910.02789-2",
          "chunk_id": 54,
          "document_id": "1910.02789",
          "start_idx": 29840,
          "end_idx": 30474
        },
        "page_content": "Following the convolution layer there is a ReLU activation and a max pool layer. Finally, there are two fully connected layers; The first layer has 32 units, and second one has 16 units. Both of them are followed by ReLU activation. All architectures have the same output, regardless of the input type. The DQN network is a regression network, with its output size the number of available actions. The PPO agent has 2 networks; actor and critic. The actor network has a Softmax activation with size equal to the available amount of actions. The critic network is a regression model with a single output representing the state's value.",
        "type": "Document"
      },
      {
        "id": "283f0f76-4b6e-44fa-9151-69f69aa90fbc",
        "metadata": {
          "vector_store_key": "1710.01507-2",
          "chunk_id": 9,
          "document_id": "1710.01507",
          "start_idx": 5189,
          "end_idx": 5946
        },
        "page_content": "These components are (1) BiLSTM with attention, (2) Siamese Network on Text Embeddings, and (3) Siamese Network on Visual Embeddings. An overview of the architecture can be seen in Figure 1. We start with an explanation of the features used in the first component of the model. Distributed Word Embeddings Considering the effectiveness of distributional semantics in modeling language data, we use a pre-trained 300 dimensional Word2Vec BIBREF9 model trained over 100 billion words in the Google News corpus using the Continuous Bag of Words architecture. These map the words in a language to a high dimensional real-valued vectors to capture hidden semantic and syntactic properties of words, and are typically learned from large, unannotated text corpora.",
        "type": "Document"
      },
      {
        "id": "0d92fa29-2dea-4477-a4ae-0801322a359e",
        "metadata": {
          "vector_store_key": "1611.02550-1",
          "chunk_id": 16,
          "document_id": "1611.02550",
          "start_idx": 8396,
          "end_idx": 9068
        },
        "page_content": "All of the above equations refer to single-layer networks. In a deep network, with multiple stacked layers, the same update equations are used in each layer, with the state, cell, and gate vectors replaced by layer-specific vectors INLINEFORM0 and so on for layer INLINEFORM1 . For all but the first layer, the input INLINEFORM2 is replaced by the hidden state vector from the previous layer INLINEFORM3 . For the fully connected layers, we use rectified linear unit (ReLU) BIBREF29 activation, except for the final layer which depends on the form of supervision and loss used in training. We train the RNN-based embedding models using a set of pre-segmented spoken words.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What does recurrent deep stacking network do?",
    "llm_answer": "Stacks and concatenates the outputs of previous frames into the input features of the current frame.\n",
    "context": [
      {
        "id": "d2e73142-d3df-4fc3-89f9-a807dac82799",
        "metadata": {
          "vector_store_key": "1612.04675-2",
          "chunk_id": 3,
          "document_id": "1612.04675",
          "start_idx": 1957,
          "end_idx": 2868
        },
        "page_content": "As indicated in its name, Recurrent Deep Stacking Network stacks and concatenates the outputs of previous frames into the input features of the current frame. If we view acoustic models in ASR systems as functions projecting input features to the probability density outputs, we can see the differences between conventional systems and RDSN clearer. Denote the input features at frame $t$ as $x_t$ , and the output as frame $t$ as $y_t$ . We can see that RDSN tries to model  $$P(y_t | x_{t+k}, ..., x_{t+1}, x_t, x_{t-1}, ..., x_{t-k}, y_t, y_{t-1}, ..., y_{t-k})$$   (Eq. 2)  while conventional DNNs try to model  $$P(y_t | x_{t+k}, ..., x_{t+1}, x_t, x_{t-1}, ..., x_{t-k})$$   (Eq. 3)  Note that if we want the RDSN to be causal, we can simplify it to  $$P(y_t | x_t, x_{t-1}, ..., x_{t-k}, y_t, y_{t-1}, ..., y_{t-k})$$   (Eq. 4)  where $k$ 's in the above formula represent the number of recurrent frames.",
        "type": "Document"
      },
      {
        "id": "972657a3-0112-4641-bbdf-f10a4bf9466c",
        "metadata": {
          "vector_store_key": "1612.04675-2",
          "chunk_id": 2,
          "document_id": "1612.04675",
          "start_idx": 1172,
          "end_idx": 1957
        },
        "page_content": "Inspired by recent progresses in Natural Language Processing BIBREF5 , we proposed the Recurrent Deep Stacking Network (RDSN) and successfully applied it to Speech Enhancement tasks. RDSN utilizes the phoneme information in previous frames as additional inputs to the raw features. From another perspective of view, this framework transformed the Acoustic Model into a hybrid model consisted of an Acoustic Model and a simple N-gram Language Model on phoneme level. In the next section, we will explain the framework of RDSN and tricks to compress the outputs. Then we will show the experimental results and make a conclusion. As indicated in its name, Recurrent Deep Stacking Network stacks and concatenates the outputs of previous frames into the input features of the current frame.",
        "type": "Document"
      },
      {
        "id": "0d92fa29-2dea-4477-a4ae-0801322a359e",
        "metadata": {
          "vector_store_key": "1611.02550-1",
          "chunk_id": 16,
          "document_id": "1611.02550",
          "start_idx": 8396,
          "end_idx": 9068
        },
        "page_content": "All of the above equations refer to single-layer networks. In a deep network, with multiple stacked layers, the same update equations are used in each layer, with the state, cell, and gate vectors replaced by layer-specific vectors INLINEFORM0 and so on for layer INLINEFORM1 . For all but the first layer, the input INLINEFORM2 is replaced by the hidden state vector from the previous layer INLINEFORM3 . For the fully connected layers, we use rectified linear unit (ReLU) BIBREF29 activation, except for the final layer which depends on the form of supervision and loss used in training. We train the RNN-based embedding models using a set of pre-segmented spoken words.",
        "type": "Document"
      },
      {
        "id": "efc68bf6-5a66-4052-bf1a-d2ae1f5b14e1",
        "metadata": {
          "vector_store_key": "1811.09786-0",
          "chunk_id": 2,
          "document_id": "1811.09786",
          "start_idx": 1055,
          "end_idx": 1737
        },
        "page_content": "Notably, the wide adoption of stacked architectures across many applications BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 signify the need for designing complex and expressive encoders. Unfortunately, these strategies may face limitations. For example, the former might run a risk of overfitting and/or hitting a wall in performance. On the other hand, the latter might be faced with the inherent difficulties of going deep such as vanishing gradients or difficulty in feature propagation across deep RNN layers BIBREF11 . This paper proposes Recurrently Controlled Recurrent Networks (RCRN), a new recurrent architecture and a general purpose neural building block for sequence modeling.",
        "type": "Document"
      },
      {
        "id": "ed8dcca1-5fda-42be-a97e-b2f33ed2ab01",
        "metadata": {
          "vector_store_key": "1612.04675-1",
          "chunk_id": 6,
          "document_id": "1612.04675",
          "start_idx": 3386,
          "end_idx": 4138
        },
        "page_content": "We use a compression method based on the correspondence of the DNN output and monophone states. For each output dimension, we find the corresponding monophone and add its value with the other output dimensions that have the same corresponding monophone. This process compresses the dimension from thousands to about forty. Compressing the output dimensions enables us to add multiple recurrent outputs and keep the input vector dense. Originated from a similar idea as of RDSN, a BiPass Stacking Network (BPSN) takes as input both conventional features and the outputs of previous frames. But the way BiPass Stacking Network generates the representations of previous frames is through a two-pass scheme similar to Deep Stacking Networks (DSN) BIBREF6 .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the reward model for the reinforcement learning appraoch?",
    "llm_answer": "The reward can be a QA-based metric (number of overlapping words between ground truth and predicted answer), or a balanced reward maximizing the probability of finding an answer to the generated question within the input paragraph but not inside the source sentence.\n",
    "context": [
      {
        "id": "25539b43-d601-4f6f-9e65-7df3af65b3cb",
        "metadata": {
          "vector_store_key": "1702.03274-8",
          "chunk_id": 37,
          "document_id": "1702.03274",
          "start_idx": 20570,
          "end_idx": 21198
        },
        "page_content": "Once a system operates at scale, interacting with a large number of users, it is desirable for the system to continue to learn autonomously using reinforcement learning (RL). With RL, each turn receives a measurement of goodness called a reward; the agent explores different sequences of actions in different situations, and makes adjustments so as to maximize the expected discounted sum of rewards, which is called the return, denoted $G$ . For optimization, we selected a policy gradient approach BIBREF34 , which has been successfully applied to dialog systems BIBREF35 , robotics BIBREF36 , and the board game Go BIBREF37 .",
        "type": "Document"
      },
      {
        "id": "f374243e-d247-40e5-9d52-fd272370c40b",
        "metadata": {
          "vector_store_key": "1911.03350-7",
          "chunk_id": 8,
          "document_id": "1911.03350",
          "start_idx": 3906,
          "end_idx": 4698
        },
        "page_content": "For instance, BIBREF15 proposed a reinforcement learning setup trained using a QA-based metric reward: given a paragraph and an answer, the model first generates questions; then, the paragraph and the corresponding generated questions are given to a pre-trained QA model which predicts an answer; finally, the reward is computed as the number of overlapping words between the ground truth answer and the predicted answer. For an extensive evalution of models trained with different rewards we refer the reader to BIBREF17. Most of these works followed BIBREF18, who applied reinforcement to neural machine translation. First, a sequence to sequence model is trained under teacher forcing BIBREF19 to optimize cross-entropy, hence helping to reduce the action space (i.e. the vocabulary size).",
        "type": "Document"
      },
      {
        "id": "60a7cd31-ac21-45fa-92c8-8b53940d1817",
        "metadata": {
          "vector_store_key": "1911.03350-7",
          "chunk_id": 34,
          "document_id": "1911.03350",
          "start_idx": 18170,
          "end_idx": 19152
        },
        "page_content": "Reinforcement Learning (RL) is an efficient technique to maximize discrete metrics for text generation. Previously, BIBREF18 used the REINFORCE algorithm BIBREF20 to train RNNs for several generation tasks, showing improvements over previous supervised approaches. Moreover, BIBREF29 combined supervised and reinforcement learning, demonstrating improvements over competing approaches both in terms of ROUGE and on human evaluation. However, the metrics used as reward are often overfit, leading to numerical improvements which do not translate to increased \u2013 and, rather, contribute to degrading \u2013 output quality, thus leading to reduced effectiveness of the trained models for practical applications. On this matter, and with a particular focus on QG, BIBREF17 performed a human evaluation on RL models trained with several metrics as reward, finding them to be indeed poorly aligned with human judgments: the models appear to learn to exploit the weaknesses of the reward source.",
        "type": "Document"
      },
      {
        "id": "96783f0d-9189-44ae-8559-b266c27f98dc",
        "metadata": {
          "vector_store_key": "1911.03350-7",
          "chunk_id": 35,
          "document_id": "1911.03350",
          "start_idx": 19152,
          "end_idx": 19999
        },
        "page_content": "On this matter, and with a particular focus on QG, BIBREF17 performed a human evaluation on RL models trained with several metrics as reward, finding them to be indeed poorly aligned with human judgments: the models appear to learn to exploit the weaknesses of the reward source. To overcome this issue, we propose to use a balanced reward: thus maximizing the probability of finding an answer to the generated question within the input paragraph but not inside the source sentence. In our experiments, we follow the approach proposed by BIBREF18, BIBREF29, considering a mixed loss $L_{ml+rl}$ which combines supervised and reinforcement learning schemes: where the maximum likelihood $L_{ml}$ is defined as where $X=[x_1,...,x_n]$ represents the source text of length $n$ and $Y=[y_1,...,y_m]$ the corresponding reference question of length $m$.",
        "type": "Document"
      },
      {
        "id": "ace7bf14-dcd7-4997-8709-763353110930",
        "metadata": {
          "vector_store_key": "1902.00330-9",
          "chunk_id": 61,
          "document_id": "1902.00330",
          "start_idx": 33845,
          "end_idx": 34482
        },
        "page_content": "Nguyen et al. BIBREF2 use the sequence model, but they simply encode the results of the greedy choice, and measure the similarities between the global encoding and the candidate entity representations. Their model does not consider the long-term impact of current decisions on subsequent choices, nor does they add the selected target entity information to the current state to help disambiguation. In the last few years, reinforcement learning has emerged as a powerful tool for solving complex sequential decision-making problems. It is well known for its great success in the game field, such as Go BIBREF37 and Atari games BIBREF38 .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Does this paper propose a new task that others can try to improve performance on?",
    "llm_answer": "Yes.\n",
    "context": [
      {
        "id": "ec2bc147-fa9d-4840-998e-3f68b7b8f0cb",
        "metadata": {
          "vector_store_key": "1810.12885-2",
          "chunk_id": 3,
          "document_id": "1810.12885",
          "start_idx": 2066,
          "end_idx": 2943
        },
        "page_content": "If a system performs as well as a human on a task, the conclusion shouldn't immediately be that the task is too easy, it should more subtly be that new datasets are then needed to see if the inference mechanisms hold up, where the creation of the datasets can be based based on an explicitly stated intuition that humans may rely on more than pattern matching. It is a hypothesis at this point in the Introduction, that systems doing well on earlier datasets won't also do well on yours. You expect they will fail, and even design the dataset specifically around their failure cases. [color=red!20,size=,fancyline,caption=,disable]ben:I would say: While systems may match or even outperform humans on these datasets, our intuition suggests that there are at least some instances in human reading comprehension that require more than what existing challenge tasks are stressing.",
        "type": "Document"
      },
      {
        "id": "11d9962a-2577-495d-917c-943ce90262c9",
        "metadata": {
          "vector_store_key": "2001.11268-0",
          "chunk_id": 51,
          "document_id": "2001.11268",
          "start_idx": 29476,
          "end_idx": 30207
        },
        "page_content": "This was done in order to save computing resources, as an addition of 100 SQuAD domains resulted in training time increases of two hours, depending on various other parameter settings. Adjusted parameters include increased batch size, and decreased maximal context length in order to reduce training time. With this paper we aimed to explore state-of-the-art NLP methods to advance systematic review (semi)automation. Both of the presented fine-tuning approaches for transformers demonstrated flexibility and high performance. We contributed an approach to deal with ambiguity in whole-sentence predictions, and proposed the usage of a completely different approach to entity recognition in settings where training data are sparse.",
        "type": "Document"
      },
      {
        "id": "afa454ca-7095-47ba-818b-e0e164bf1802",
        "metadata": {
          "vector_store_key": "1911.03350-2",
          "chunk_id": 5,
          "document_id": "1911.03350",
          "start_idx": 2774,
          "end_idx": 3486
        },
        "page_content": "The contributions of this paper can be summarized as follow: we propose a new natural language generation task: curiosity-driven question generation; we propose a method to derive data for the task from popular non-conversational QA datasets; we experiment using language model pre-training and reinforcement learning, on two different datasets; we report a human evaluation analysis to assess both the pertinence of the automatic metrics used and the efficacy of the proposed dataset-creation method above. Deep learning models have been widely applied to text generation tasks such as machine translation BIBREF5, abstractive summarization BIBREF6 or dialog BIBREF7, providing significant gains in performance.",
        "type": "Document"
      },
      {
        "id": "9f62e76e-3934-4e8d-8306-48f7fe68fdd1",
        "metadata": {
          "vector_store_key": "1910.06748-2",
          "chunk_id": 7,
          "document_id": "1910.06748",
          "start_idx": 4080,
          "end_idx": 4734
        },
        "page_content": "Compared with other benchmarks on our Twitter datasets, our proposed model consistently achieves new state-of-the-art results with an improvement of 5% in accuracy and F1 score and a competitive inference time. The rest of this paper is structured as follows. After a background review in the next section, we will present our Twitter dataset in Section SECREF3. Our novel LID algorithm will be the topic of Section SECREF4. We will then present and analyze some experiments we conducted with our algorithm in Section SECREF5, along with benchmarking tests of popular and literature LID systems, before drawing some concluding remarks in Section SECREF6.",
        "type": "Document"
      },
      {
        "id": "e900003c-e4a4-4760-af29-1415e5bd0e01",
        "metadata": {
          "vector_store_key": "1708.05873-0",
          "chunk_id": 35,
          "document_id": "1708.05873",
          "start_idx": 20206,
          "end_idx": 20343
        },
        "page_content": "The paper more broadly demonstrates how text analytic approaches can help us to better understand different aspects of global governance.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is their dataset?",
    "llm_answer": "More than 0.3 million records, 0.83 million words, 230,000 word tokens, 12,350 ads, and a Web-crawled corpus of 5 x 10^10 tokens.\n",
    "context": [
      {
        "id": "f57f61de-28fb-415a-97ae-9d7e1c849440",
        "metadata": {
          "vector_store_key": "1911.13066-4",
          "chunk_id": 7,
          "document_id": "1911.13066",
          "start_idx": 4428,
          "end_idx": 5163
        },
        "page_content": "The dataset consists of more than $0.3$ million records and has been made available for future research. The rest of the paper is organized as follows. Section SECREF2 defines the dataset acquiring process and provides an explanation of the class labels. In section SECREF3, the architecture of the proposed model, its hyperparameters, and the experimental setup is discussed. We discuss the results in section SECREF4 and finally, concluding remarks are presented in section SECREF5. . The dataset consists of SMS feedbacks of the citizens of Pakistan on different public services availed by them. The objective of collecting these responses is to measure the performance of government departments rendering different public services.",
        "type": "Document"
      },
      {
        "id": "4d972e4b-977f-4d29-8238-d7d4ccecec5c",
        "metadata": {
          "vector_store_key": "1909.00107-4",
          "chunk_id": 12,
          "document_id": "1909.00107",
          "start_idx": 7253,
          "end_idx": 8074
        },
        "page_content": "The dataset comprises of approximately 0.83 million words with 10,000 unique entries of which 0.5 million is used for training (0.24m for dev and 88k for test). Cancer Couples Interaction Dataset: This dataset was gathered as part of a observational study of couples coping with advanced cancer. Advanced cancer patients and their spouse caregivers were recruited from clinics and asked to interact with each other in two structured discussions: neutral discussion and cancer related. Interactions were audio-recorded using small digital recorders worn by each participant. Manually transcribed audio has approximately 230,000 word tokens with a vocabulary size of 8173. In order to evaluate our proposed model on more generic language modeling tasks, we employ Penn Tree bank (PTB) BIBREF23, as preprocessed by BIBREF24.",
        "type": "Document"
      },
      {
        "id": "d87d327c-2858-4b5d-ae8a-860d401f63bd",
        "metadata": {
          "vector_store_key": "1908.05434-1",
          "chunk_id": 27,
          "document_id": "1908.05434",
          "start_idx": 14432,
          "end_idx": 15064
        },
        "page_content": "We have ensured that the raw dataset has no overlap with the labeled dataset to avoid bias in test accuracy. While it is possible to scrape more raw data, we did not observe significant improvements in model performances when the size of raw data increased from INLINEFORM0 70,000 to INLINEFORM1 170,000, hence we assume that the current raw dataset is sufficiently large. The labeled dataset is called Trafficking-10k. It consists of 12,350 ads from Backpage labeled by experts in human trafficking detection BIBREF9 . Each label is one of seven ordered levels of likelihood that the corresponding ad comes from a human trafficker.",
        "type": "Document"
      },
      {
        "id": "78e083f1-0666-4414-8066-d724fc284323",
        "metadata": {
          "vector_store_key": "1901.09755-0",
          "chunk_id": 25,
          "document_id": "1901.09755",
          "start_idx": 14080,
          "end_idx": 14670
        },
        "page_content": "For English, it should be noted that the size of the 2015 set is less than half with respect to the 2014 dataset in terms of tokens, and only one third in number of targets. The French, Spanish and Dutch datasets are quite similar in terms of tokens although the number of targets in the Dutch dataset is comparatively smaller, possibly due to the tendency to construct compound terms in that language. The Russian dataset is the largest whereas the Turkish set is by far the smallest one. Additionally, we think it is also interesting to note the low number of targets that are multiwords.",
        "type": "Document"
      },
      {
        "id": "1c11a194-58b2-4cda-851d-6cf17f4aad89",
        "metadata": {
          "vector_store_key": "1909.01958-0",
          "chunk_id": 13,
          "document_id": "1909.01958",
          "start_idx": 7877,
          "end_idx": 8634
        },
        "page_content": "To study particular phenomena and develop solvers, the project has created larger datasets to amplify and study different problems, resulting in 10 new datasets and 5 large knowledge resources for the community. The solvers can be loosely grouped into: Statistical and information retrieval methods Reasoning methods Large-scale language model methods Over the life of the project, the relative importance of the methods has shifted towards large-scale language methods. Several methods make use of the Aristo Corpus, comprising a large Web-crawled corpus ($5 \\times 10^{10}$ tokens (280GB)) originally from the University of Waterloo, combined with targeted science content from Wikipedia, SimpleWikipedia, and several smaller online science texts (BID25).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What task do they evaluate on?",
    "llm_answer": "Task-oriented dialogue systems, specifically a 31-categories classification task (chit-chat and task-oriented dialogue).  A conceptualization task is also mentioned.\n",
    "context": [
      {
        "id": "8c0b614b-bf22-42bc-9131-ea972e221afa",
        "metadata": {
          "vector_store_key": "1709.10217-1",
          "chunk_id": 7,
          "document_id": "1709.10217",
          "start_idx": 4048,
          "end_idx": 4813
        },
        "page_content": "It is worth noting that besides the released data for training and developing, we also allow to collect external data for training and developing. To considering that, the task 1 is indeed includes two sub tasks. One is a closed evaluation, in which only the released data can be used for training and developing. The other is an open evaluation that allow to collect external data for training and developing. For task 1, we use F1-score as evaluation metric. For the task-oriented dialogue systems, the best way for evaluation is to use the online human-computer dialogue. After finishing an online human-computer dialogue with a dialogue system, the human then manually evaluate the system by using the metrics of user satisfaction degree, dialogue fluency, etc.",
        "type": "Document"
      },
      {
        "id": "4c765ce0-5ec3-4c4c-9649-99b400dea6dd",
        "metadata": {
          "vector_store_key": "1709.10217-1",
          "chunk_id": 13,
          "document_id": "1709.10217",
          "start_idx": 7300,
          "end_idx": 7984
        },
        "page_content": "In the evaluation, all the data for training, developing and test is provided by the iFLYTEK Corporation. For task 1, as the descriptions in Section SECREF10 , the two top categories are chit-chat (chat in Table TABREF13 ) and task-oriented dialogue. Meanwhile, the task-oriented dialogue also includes 30 sub categories. Actually, the task 1 is a 31 categories classification task. In task 1, besides the data we released for training and developing, we also allow the participants to extend the training and developing corpus. Hence, there are two sub tasks for the task 1. One is closed test, which means the participants can only use the released data for training and developing.",
        "type": "Document"
      },
      {
        "id": "61e54692-2da3-41fb-b1b0-c3469ffa76bf",
        "metadata": {
          "vector_store_key": "1607.06025-0",
          "chunk_id": 17,
          "document_id": "1607.06025",
          "start_idx": 10067,
          "end_idx": 10777
        },
        "page_content": "These attributes are conditional information that is fed to the models, like the discrete label is in our models. As recognized by BIBREF37 , the evaluation metrics of text-generating models fall into three categories: manual evaluation, automatic evaluation metrics, task-based evaluation. In evaluation based on human judgment each generated textual example is inspected manually. The automatic evaluation metrics, like ROUGE, BLEU and METEOR, compare human texts and generated texts. BIBREF38 shows METEOR has the strongest correlation with human judgments in image description evaluation. The last category is task-based evaluation, where the impact of the generated texts on a particular task is measured.",
        "type": "Document"
      },
      {
        "id": "ed298574-f43c-456e-8633-7fd4765dbce0",
        "metadata": {
          "vector_store_key": "1707.06939-0",
          "chunk_id": 10,
          "document_id": "1707.06939",
          "start_idx": 5469,
          "end_idx": 6122
        },
        "page_content": "Here we describe the task we studied and its input data, worker recruitment, the design of our experimental treatment and control, the \u201cinstrumentation\u201d we used to measure the speeds of workers as they performed our task, and our procedures to post-process and rate the worker responses to our task prior to subsequent analysis. We recruited 176 AMT workers to participate in our conceptualization task. Of these workers, 90 were randomly assigned to the Control group and 86 to the AUI group. These workers completed 1001 tasks: 496 tasks in the control and 505 in the AUI. All responses were gathered within a single 24-hour period during April, 2017.",
        "type": "Document"
      },
      {
        "id": "73fa9d24-b35d-4e01-85d3-6a04bf538c21",
        "metadata": {
          "vector_store_key": "2002.05058-8",
          "chunk_id": 39,
          "document_id": "2002.05058",
          "start_idx": 22071,
          "end_idx": 23097
        },
        "page_content": "As human evaluation is expensive, sample-level evaluation is performed jointly with model-level evaluation, which is also used for evaluating the ability of different metrics for performing hyperparameter search and early-stopping. Concretely, we perform 10 groups of evaluations for performing hyperparameter selecting and early-stopping with five compared automated metrics. In each evaluation, each of the five compared metrics is used to select the best hyperparameter combination or early-stopping checkpoint with other variants fixed. We choose to perform score-based human evaluation for four reasons: 1) the ADEM baseline requires human-annotated score as training examples, 2) we can construct up to $\\binom{2n}{2}$ training examples for our comparative evaluator with $n$ human-annotated scores, 3) score-based human evaluation facilitates the evaluation of correlation scores, and 4) as all other metrics do not perform pairwise comparison, using pairwise human evaluation will likely be biased toward our approach.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many feature maps are generated for a given triple?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "910217ff-7633-4743-b75b-1986083a7405",
        "metadata": {
          "vector_store_key": "1804.03396-7",
          "chunk_id": 22,
          "document_id": "1804.03396",
          "start_idx": 12141,
          "end_idx": 12844
        },
        "page_content": "Then we gather all the DBpedia triples with the first entity is corresponding to one of the above 3.5M articles and the relation is one of the projected 148 relations. After the same clipping process as above and removing the repetitive triples, we obtain 394K additional triples in 302K existing Wikipedia articles. Distillation. Since our benchmark is for IE, we prefer the articles with more golden triples involved by assuming that Wikipedia articles with more annotated triples are more informative and better annotated. Therefore, we figure out the distribution of the number of golden triples in articles and decide to discard the articles with less than 6 golden triples (account for about 80%).",
        "type": "Document"
      },
      {
        "id": "5325f02e-71ed-4027-8b25-4e2f9e4ce152",
        "metadata": {
          "vector_store_key": "1804.03396-7",
          "chunk_id": 23,
          "document_id": "1804.03396",
          "start_idx": 12844,
          "end_idx": 13525
        },
        "page_content": "Therefore, we figure out the distribution of the number of golden triples in articles and decide to discard the articles with less than 6 golden triples (account for about 80%). After this step, we obtain about 200K articles and 1.4M triples with 636 relation types. Query and Answer Assignment. For each golden triple $\\lbrace e_i, r_{ij}, e_j\\rbrace $ , we assign the relation/property $r_{ij}$ as the query and the entity $e_j$ as the answer because the Wikipedia article and its corresponding golden triples are all about the same entity $e_i$ which is unnecessary in the queries. Besides, we find the location of each $e_j$ in the corresponding article as the answer location.",
        "type": "Document"
      },
      {
        "id": "566e51ba-ebe3-4f1a-a251-8b3174166818",
        "metadata": {
          "vector_store_key": "1712.02121-2",
          "chunk_id": 8,
          "document_id": "1712.02121",
          "start_idx": 4547,
          "end_idx": 5246
        },
        "page_content": "That is, these filters are repeatedly operated over every row of the input matrix to produce different feature maps. The feature maps are concatenated into a single feature vector which is then computed with a weight vector via a dot product to produce a score for the triple (h, r, t). This score is used to infer whether the triple (h, r, t) is valid or not. Our contributions in this paper are as follows: A knowledge base $\\mathcal {G}$ is a collection of valid factual triples in the form of (head entity, relation, tail entity) denoted as $(h, r, t)$ such that $h, t \\in \\mathcal {E}$ and $r \\in \\mathcal {R}$ where $\\mathcal {E}$ is a set of entities and $\\mathcal {R}$ is a set of relations.",
        "type": "Document"
      },
      {
        "id": "ec67598b-df0e-4a14-b613-663c2358d701",
        "metadata": {
          "vector_store_key": "1802.05574-3",
          "chunk_id": 25,
          "document_id": "1802.05574",
          "start_idx": 14268,
          "end_idx": 15034
        },
        "page_content": "Furthermore, to ease comparison, we only consider triples with 5 judgements this excludes 9 triples. We indicate not only the pair-wise inter-annotator agreement but also the number of annotators who have judged a triple to be correct. For example, at the 40% agreement level at least 3 annotators have agreed that a triple is true. The figure separates the results by extractor and by data source. We see that as expected the amount of triples agreed to as correct grows larger as we relax the requirement for agreement. For example, analyzing Open IE's results, at the 100% agreement level we see a precision of 0.56 whereas at the 40% agreement level we see a precision of 0.78. Table 3 shows the total number of correct extractions at the three agreement levels.",
        "type": "Document"
      },
      {
        "id": "888896a4-8d3d-4593-a248-259955498527",
        "metadata": {
          "vector_store_key": "1802.05574-3",
          "chunk_id": 24,
          "document_id": "1802.05574",
          "start_idx": 13600,
          "end_idx": 14268
        },
        "page_content": "The Correct Triples column contains the number of triples that are labelled as being correct by all annotators. Total Triples are the total number of triples extracted by the given systems over the specified data. Precision is calculated as typical where Correct Triples are treated as true positives. On average, 3.1 triples were extracted per sentence. Figure 1 shows the performance of extractors in terms of precision as inter-annotator agreement decreases. In this figure, we look only at agreement on triples where the majority agree that the triple is correct. Furthermore, to ease comparison, we only consider triples with 5 judgements this excludes 9 triples.",
        "type": "Document"
      }
    ]
  }
]