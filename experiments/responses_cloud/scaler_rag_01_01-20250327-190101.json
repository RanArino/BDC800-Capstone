[
  {
    "query": "What is the seed lexicon?",
    "llm_answer": "A seed lexicon consisting of 15 positive words and 15 negative words.\n",
    "context": [
      {
        "id": "77d2b277-266a-4e3f-8be8-b374378b2528",
        "metadata": {
          "vector_store_key": "1909.00694-2",
          "chunk_id": 25,
          "document_id": "1909.00694",
          "start_idx": 8296,
          "end_idx": 8611
        },
        "page_content": "] made a serious mistake, [I] got fired. From this sentence, we extracted the event pair of \u201c\u91cd\u5927\u306a\u5931\u6557\u3092\u72af\u3059\u201d ([I] make a serious mistake) and \u201c\u4ed5\u4e8b\u3092\u30af\u30d3\u306b\u306a\u308b\u201d ([I] get fired), and tagged it with Cause. We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the",
        "type": "Document"
      },
      {
        "id": "7eac29aa-5034-4cf4-9cb6-7c0fa7c1a2d1",
        "metadata": {
          "vector_store_key": "1909.00694-2",
          "chunk_id": 35,
          "document_id": "1909.00694",
          "start_idx": 11244,
          "end_idx": 11751
        },
        "page_content": "or the reverse of it for negation) if the event's predicate is in the seed lexicon. We can see that the seed lexicon itself had practically no impact on prediction. The models in the top block performed considerably better than the random baselines. The performance gaps with their (semi-)supervised counterparts, shown in the middle block, were less than 7%. This demonstrates the effectiveness of discourse relation-based label propagation. Comparing the model variants, we obtained the highest score with",
        "type": "Document"
      },
      {
        "id": "0afbd71e-4570-4a7e-beee-fdec98c2a800",
        "metadata": {
          "vector_store_key": "1909.00694-2",
          "chunk_id": 15,
          "document_id": "1909.00694",
          "start_idx": 5168,
          "end_idx": 5658
        },
        "page_content": " As shown in Figure FIGREF1, we limit our scope to two discourse relations: Cause and Concession. The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the",
        "type": "Document"
      },
      {
        "id": "9a4ba835-75e6-49eb-847b-a41b8095a966",
        "metadata": {
          "vector_store_key": "1909.00694-2",
          "chunk_id": 16,
          "document_id": "1909.00694",
          "start_idx": 5454,
          "end_idx": 5949
        },
        "page_content": " to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.  The seed lexicon matches (1) the latter event but (2) not the former event, and (3) their discourse relation type is Cause or Concession. If the discourse relation type is Cause, the former event is given the same score as the latter. Likewise, if",
        "type": "Document"
      },
      {
        "id": "6cc33e0e-34ac-4baf-a514-d81367fb2fe1",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 34,
          "document_id": "1909.00694",
          "start_idx": 11000,
          "end_idx": 11367
        },
        "page_content": " + \\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$.   Table TABREF23 shows accuracy. As the Random baseline suggests, positive and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon and output the corresponding label (or the reverse of it for negation) if the event's predicate is in the seed lexicon",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the results?",
    "llm_answer": "Results are shown in Table TABREF16 and Table TABREF23.\n",
    "context": [
      {
        "id": "06e9a4b3-9db0-4e73-892a-9b9a4c094d48",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 27,
          "document_id": "1909.00694",
          "start_idx": 8898,
          "end_idx": 9330
        },
        "page_content": " CO such that it was five times larger than AL. The results are shown in Table TABREF16. We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively: . \u4f5c\u696d\u304c\u697d\u3060\u3002 The work",
        "type": "Document"
      },
      {
        "id": "9d60a1a1-cb26-4e1c-b917-daf265954545",
        "metadata": {
          "vector_store_key": "1909.00694-3",
          "chunk_id": 32,
          "document_id": "1909.00694",
          "start_idx": 10558,
          "end_idx": 10948
        },
        "page_content": " output is the final hidden state corresponding to the special classification tag ([CLS]). For the details of ${\\rm Encoder}$, see Sections SECREF30. We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\mathcal {L}_{\\rm AL}$, $\\math",
        "type": "Document"
      },
      {
        "id": "6cc33e0e-34ac-4baf-a514-d81367fb2fe1",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 34,
          "document_id": "1909.00694",
          "start_idx": 11000,
          "end_idx": 11367
        },
        "page_content": " + \\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$.   Table TABREF23 shows accuracy. As the Random baseline suggests, positive and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon and output the corresponding label (or the reverse of it for negation) if the event's predicate is in the seed lexicon",
        "type": "Document"
      },
      {
        "id": "a2813591-f4ca-44da-9e32-566c3bf358d8",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 26,
          "document_id": "1909.00694",
          "start_idx": 8611,
          "end_idx": 9077
        },
        "page_content": " consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF",
        "type": "Document"
      },
      {
        "id": "75132ab8-c010-48a6-b0da-bca658641529",
        "metadata": {
          "vector_store_key": "1909.00694-3",
          "chunk_id": 30,
          "document_id": "1909.00694",
          "start_idx": 9921,
          "end_idx": 10268
        },
        "page_content": " the ACP Corpus. To optimize the hyperparameters, we used the dev set of the ACP Corpus. For the evaluation, we used the test set of the ACP Corpus. The model output was classified as positive if $p(x) > 0$ and negative if $p(x) \\le 0$.  As for ${\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. GRU BIBREF16 is a recurrent",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are relations used to propagate polarity?",
    "llm_answer": "Discourse relations are exploited to efficiently propagate polarity from seed predicate structures, in addition to linguistic patterns.\n",
    "context": [
      {
        "id": "a7cf5823-7152-4dce-bfeb-2b6b1f132023",
        "metadata": {
          "vector_store_key": "1909.00694-0",
          "chunk_id": 5,
          "document_id": "1909.00694",
          "start_idx": 1786,
          "end_idx": 2082
        },
        "page_content": ".e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is",
        "type": "Document"
      },
      {
        "id": "15d860fa-44cb-477f-8600-2662b6c36305",
        "metadata": {
          "vector_store_key": "1909.00694-0",
          "chunk_id": 6,
          "document_id": "1909.00694",
          "start_idx": 1955,
          "end_idx": 2465
        },
        "page_content": " (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event. We trained the models using a Japanese web corpus. Given the minimum amount of supervision, they performed well. In addition, the combination of annotated and unannotated data yielded a gain over a purely supervised baseline when labeled data were small. Learning affect",
        "type": "Document"
      },
      {
        "id": "bcb2315b-84cd-48a4-b772-949e1f1df51e",
        "metadata": {
          "vector_store_key": "1909.00694-5",
          "chunk_id": 2,
          "document_id": "1909.00694",
          "start_idx": 780,
          "end_idx": 1273
        },
        "page_content": " combinatorial nature of language, the non-compositionality of affective polarity entails the need for large amounts of world knowledge, which can hardly be learned from small annotated data. In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed pred",
        "type": "Document"
      },
      {
        "id": "fd450194-b665-4259-baa7-c79fbaa6c8d9",
        "metadata": {
          "vector_store_key": "1909.00694-5",
          "chunk_id": 12,
          "document_id": "1909.00694",
          "start_idx": 4159,
          "end_idx": 4582
        },
        "page_content": " structures in addition to linguistic patterns. Our method depends only on raw texts and thus has wider applicability.    Our goal is to learn the polarity function $p(x)$, which predicts the sentiment polarity score of an event $x$. We approximate $p(x)$ by a neural network with the following form: ${\\rm Encoder}$ outputs a vector representation of the event $x$. ${\\rm Linear}$ is a fully-connected layer and transforms",
        "type": "Document"
      },
      {
        "id": "6f4845e4-1424-400c-a55f-fdbbf4b39ec8",
        "metadata": {
          "vector_store_key": "1909.00694-5",
          "chunk_id": 1,
          "document_id": "1909.00694",
          "start_idx": 483,
          "end_idx": 952
        },
        "page_content": "2, and humor recognition BIBREF3. In this paper, we work on recognizing the polarity of an affective event that is represented by a score ranging from $-1$ (negative) to 1 (positive). Learning affective events is challenging because, as the examples above suggest, the polarity of an event is not necessarily predictable from its constituent words. Combined with the unbounded combinatorial nature of language, the non-compositionality of affective polarity entails the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is the Japanese data?",
    "llm_answer": "About 100 million sentences.\n",
    "context": [
      {
        "id": "06e9a4b3-9db0-4e73-892a-9b9a4c094d48",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 27,
          "document_id": "1909.00694",
          "start_idx": 8898,
          "end_idx": 9330
        },
        "page_content": " CO such that it was five times larger than AL. The results are shown in Table TABREF16. We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively: . \u4f5c\u696d\u304c\u697d\u3060\u3002 The work",
        "type": "Document"
      },
      {
        "id": "6aeaaf25-8c14-4292-a470-733d3bd41bf9",
        "metadata": {
          "vector_store_key": "1909.00694-8",
          "chunk_id": 48,
          "document_id": "1909.00694",
          "start_idx": 15131,
          "end_idx": 15574
        },
        "page_content": " vocabulary size was 100,000. The number of hidden layers was 2. The dimension of hidden units was 256. The optimizer was Momentum SGD BIBREF21. The mini-batch size was 1024. We ran 100 epochs and selected the snapshot that achieved the highest score for the dev set. We used a Japanese BERT model pretrained with Japanese Wikipedia. The input sentences were segmented into words by Juman++, and words were broken into subwords by applying BPE",
        "type": "Document"
      },
      {
        "id": "3e1406ea-9836-4a54-8399-d1463333181f",
        "metadata": {
          "vector_store_key": "1909.00694-8",
          "chunk_id": 49,
          "document_id": "1909.00694",
          "start_idx": 15395,
          "end_idx": 15766
        },
        "page_content": " segmented into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam BIBREF19. The mini-batch size was 32. We ran 1 epoch.",
        "type": "Document"
      },
      {
        "id": "a2813591-f4ca-44da-9e32-566c3bf358d8",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 26,
          "document_id": "1909.00694",
          "start_idx": 8611,
          "end_idx": 9077
        },
        "page_content": " consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF",
        "type": "Document"
      },
      {
        "id": "1039861a-aff4-4d1e-9cc1-bddc74613237",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 43,
          "document_id": "1909.00694",
          "start_idx": 13637,
          "end_idx": 14013
        },
        "page_content": " useful, they nevertheless contain noises. Adding linguistically-motivated filtering rules would help improve the performance. We thank Nobuhiro Kaji for providing the ACP Corpus and Hirokazu Kiyomaru and Yudai Kishimoto for their help in extracting event pairs. This work was partially supported by Yahoo! Japan Corporation. \u559c\u3076 (rejoice), \u5b09\u3057\u3044 (be glad), \u697d\u3057\u3044 (be pleasant), \u5e78\ufffd",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?",
    "llm_answer": "The context states that the method is effective when labeled data are small, but does not provide a quantitative comparison of improvements.\n",
    "context": [
      {
        "id": "9b52a0c8-1a01-4579-a5b6-8214fdda19e4",
        "metadata": {
          "vector_store_key": "1909.00694-9",
          "chunk_id": 36,
          "document_id": "1909.00694",
          "start_idx": 11751,
          "end_idx": 12204
        },
        "page_content": " of discourse relation-based label propagation. Comparing the model variants, we obtained the highest score with the BiGRU encoder trained with the AL+CA+CO dataset. BERT was competitive but its performance went down if CA and CO were used in addition to AL. We conjecture that BERT was more sensitive to noises found more frequently in CA and CO. Contrary to our expectations, supervised models (ACP) outperformed semi-supervised models (ACP+AL+CA+CO).",
        "type": "Document"
      },
      {
        "id": "20380271-fe93-4506-b0a5-f728dffa9ec2",
        "metadata": {
          "vector_store_key": "1909.00694-9",
          "chunk_id": 37,
          "document_id": "1909.00694",
          "start_idx": 12027,
          "end_idx": 12460
        },
        "page_content": " models (ACP) outperformed semi-supervised models (ACP+AL+CA+CO). This suggests that the training set of 0.6 million events is sufficiently large for training the models. For comparison, we trained the models with a subset (6,000 events) of the ACP dataset. As the results shown in Table TABREF24 demonstrate, our method is effective when labeled data are small. The result of hyperparameter optimization for the BiGRU encoder was as",
        "type": "Document"
      },
      {
        "id": "e68e8d12-e6d9-4010-90a4-4f7a64692b79",
        "metadata": {
          "vector_store_key": "1909.00694-9",
          "chunk_id": 38,
          "document_id": "1909.00694",
          "start_idx": 12460,
          "end_idx": 12825
        },
        "page_content": " data are small. The result of hyperparameter optimization for the BiGRU encoder was as follows: As the CA and CO pairs were equal in size (Table TABREF16), $\\lambda _{\\rm CA}$ and $\\lambda _{\\rm CO}$ were comparable values. $\\lambda _{\\rm CA}$ was about one-third of $\\lambda _{\\rm CO}$, and this indicated that the CA pairs were noisier than the CO pairs. A major",
        "type": "Document"
      },
      {
        "id": "75132ab8-c010-48a6-b0da-bca658641529",
        "metadata": {
          "vector_store_key": "1909.00694-3",
          "chunk_id": 30,
          "document_id": "1909.00694",
          "start_idx": 9921,
          "end_idx": 10268
        },
        "page_content": " the ACP Corpus. To optimize the hyperparameters, we used the dev set of the ACP Corpus. For the evaluation, we used the test set of the ACP Corpus. The model output was classified as positive if $p(x) > 0$ and negative if $p(x) \\le 0$.  As for ${\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. GRU BIBREF16 is a recurrent",
        "type": "Document"
      },
      {
        "id": "7af90eaa-5869-4521-b4fb-77ce8a5cd37e",
        "metadata": {
          "vector_store_key": "1909.00694-3",
          "chunk_id": 31,
          "document_id": "1909.00694",
          "start_idx": 10125,
          "end_idx": 10558
        },
        "page_content": " of neural networks: BiGRU and BERT. GRU BIBREF16 is a recurrent neural network sequence encoder. BiGRU reads an input sequence forward and backward and the output is the concatenation of the final forward and backward hidden states. BERT BIBREF17 is a pre-trained multi-layer bidirectional Transformer BIBREF18 encoder. Its output is the final hidden state corresponding to the special classification tag ([CLS]). For the details of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does their model learn using mostly raw data?",
    "llm_answer": "Their method requires a very small seed lexicon and a large raw corpus.  They assume they can automatically extract features from the raw corpus.\n",
    "context": [
      {
        "id": "9d60a1a1-cb26-4e1c-b917-daf265954545",
        "metadata": {
          "vector_store_key": "1909.00694-3",
          "chunk_id": 32,
          "document_id": "1909.00694",
          "start_idx": 10558,
          "end_idx": 10948
        },
        "page_content": " output is the final hidden state corresponding to the special classification tag ([CLS]). For the details of ${\\rm Encoder}$, see Sections SECREF30. We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\mathcal {L}_{\\rm AL}$, $\\math",
        "type": "Document"
      },
      {
        "id": "75132ab8-c010-48a6-b0da-bca658641529",
        "metadata": {
          "vector_store_key": "1909.00694-3",
          "chunk_id": 30,
          "document_id": "1909.00694",
          "start_idx": 9921,
          "end_idx": 10268
        },
        "page_content": " the ACP Corpus. To optimize the hyperparameters, we used the dev set of the ACP Corpus. For the evaluation, we used the test set of the ACP Corpus. The model output was classified as positive if $p(x) > 0$ and negative if $p(x) \\le 0$.  As for ${\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. GRU BIBREF16 is a recurrent",
        "type": "Document"
      },
      {
        "id": "20380271-fe93-4506-b0a5-f728dffa9ec2",
        "metadata": {
          "vector_store_key": "1909.00694-9",
          "chunk_id": 37,
          "document_id": "1909.00694",
          "start_idx": 12027,
          "end_idx": 12460
        },
        "page_content": " models (ACP) outperformed semi-supervised models (ACP+AL+CA+CO). This suggests that the training set of 0.6 million events is sufficiently large for training the models. For comparison, we trained the models with a subset (6,000 events) of the ACP dataset. As the results shown in Table TABREF24 demonstrate, our method is effective when labeled data are small. The result of hyperparameter optimization for the BiGRU encoder was as",
        "type": "Document"
      },
      {
        "id": "7af90eaa-5869-4521-b4fb-77ce8a5cd37e",
        "metadata": {
          "vector_store_key": "1909.00694-3",
          "chunk_id": 31,
          "document_id": "1909.00694",
          "start_idx": 10125,
          "end_idx": 10558
        },
        "page_content": " of neural networks: BiGRU and BERT. GRU BIBREF16 is a recurrent neural network sequence encoder. BiGRU reads an input sequence forward and backward and the output is the concatenation of the final forward and backward hidden states. BERT BIBREF17 is a pre-trained multi-layer bidirectional Transformer BIBREF18 encoder. Its output is the final hidden state corresponding to the special classification tag ([CLS]). For the details of",
        "type": "Document"
      },
      {
        "id": "a037ac72-055b-4681-a33e-6283ec08a913",
        "metadata": {
          "vector_store_key": "1909.00694-3",
          "chunk_id": 13,
          "document_id": "1909.00694",
          "start_idx": 4582,
          "end_idx": 4990
        },
        "page_content": " of the event $x$. ${\\rm Linear}$ is a fully-connected layer and transforms the representation into a scalar. ${\\rm tanh}$ is the hyperbolic tangent and transforms the scalar into a score ranging from $-1$ to 1. In Section SECREF21, we consider two specific implementations of ${\\rm Encoder}$.  Our method requires a very small seed lexicon and a large raw corpus. We assume that we can automatically extract",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is seed lexicon used for training?",
    "llm_answer": "15 positive words and 15 negative words\n",
    "context": [
      {
        "id": "3e1406ea-9836-4a54-8399-d1463333181f",
        "metadata": {
          "vector_store_key": "1909.00694-8",
          "chunk_id": 49,
          "document_id": "1909.00694",
          "start_idx": 15395,
          "end_idx": 15766
        },
        "page_content": " segmented into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam BIBREF19. The mini-batch size was 32. We ran 1 epoch.",
        "type": "Document"
      },
      {
        "id": "7eac29aa-5034-4cf4-9cb6-7c0fa7c1a2d1",
        "metadata": {
          "vector_store_key": "1909.00694-2",
          "chunk_id": 35,
          "document_id": "1909.00694",
          "start_idx": 11244,
          "end_idx": 11751
        },
        "page_content": "or the reverse of it for negation) if the event's predicate is in the seed lexicon. We can see that the seed lexicon itself had practically no impact on prediction. The models in the top block performed considerably better than the random baselines. The performance gaps with their (semi-)supervised counterparts, shown in the middle block, were less than 7%. This demonstrates the effectiveness of discourse relation-based label propagation. Comparing the model variants, we obtained the highest score with",
        "type": "Document"
      },
      {
        "id": "6aeaaf25-8c14-4292-a470-733d3bd41bf9",
        "metadata": {
          "vector_store_key": "1909.00694-8",
          "chunk_id": 48,
          "document_id": "1909.00694",
          "start_idx": 15131,
          "end_idx": 15574
        },
        "page_content": " vocabulary size was 100,000. The number of hidden layers was 2. The dimension of hidden units was 256. The optimizer was Momentum SGD BIBREF21. The mini-batch size was 1024. We ran 100 epochs and selected the snapshot that achieved the highest score for the dev set. We used a Japanese BERT model pretrained with Japanese Wikipedia. The input sentences were segmented into words by Juman++, and words were broken into subwords by applying BPE",
        "type": "Document"
      },
      {
        "id": "77d2b277-266a-4e3f-8be8-b374378b2528",
        "metadata": {
          "vector_store_key": "1909.00694-2",
          "chunk_id": 25,
          "document_id": "1909.00694",
          "start_idx": 8296,
          "end_idx": 8611
        },
        "page_content": "] made a serious mistake, [I] got fired. From this sentence, we extracted the event pair of \u201c\u91cd\u5927\u306a\u5931\u6557\u3092\u72af\u3059\u201d ([I] make a serious mistake) and \u201c\u4ed5\u4e8b\u3092\u30af\u30d3\u306b\u306a\u308b\u201d ([I] get fired), and tagged it with Cause. We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the",
        "type": "Document"
      },
      {
        "id": "0afbd71e-4570-4a7e-beee-fdec98c2a800",
        "metadata": {
          "vector_store_key": "1909.00694-2",
          "chunk_id": 15,
          "document_id": "1909.00694",
          "start_idx": 5168,
          "end_idx": 5658
        },
        "page_content": " As shown in Figure FIGREF1, we limit our scope to two discourse relations: Cause and Concession. The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do the various social phenomena examined manifest in different types of communities?",
    "llm_answer": "Radically contrasting engagement patterns emerge in different parts of the multi-community space.  For example, in niche communities, established members engage more with volatile content than outsiders, while the opposite is true in generic communities.\n",
    "context": [
      {
        "id": "4ff81d18-900b-43eb-b1dd-1b55594cface",
        "metadata": {
          "vector_store_key": "1705.09665-4",
          "chunk_id": 11,
          "document_id": "1705.09665",
          "start_idx": 4847,
          "end_idx": 5438
        },
        "page_content": " stay). Such insights into the relation between collective identity and user engagement can be informative to community maintainers seeking to better understand growth patterns within their online communities. More generally, our methodology stands as an example of how sociological questions can be addressed in a multi-community setting. In performing our analyses across a rich variety of communities, we reveal both the diversity of phenomena that can occur, as well as the systematic nature of this diversity. A community's identity derives from its members' common interests and shared",
        "type": "Document"
      },
      {
        "id": "e400f4ea-fcd3-47a1-810c-d54997e2406d",
        "metadata": {
          "vector_store_key": "1705.09665-4",
          "chunk_id": 83,
          "document_id": "1705.09665",
          "start_idx": 34021,
          "end_idx": 34670
        },
        "page_content": " systematically analyze how the psychological manifestations and implications of collective identity vary across diverse sets of communities. Our current understanding of engagement patterns in online communities is patched up from glimpses offered by several disparate studies focusing on a few individual communities. This work calls into attention the need for a method to systematically reason about similarities and differences across communities. By proposing a way to structure the multi-community space, we find not only that radically contrasting engagement patterns emerge in different parts of this space, but also that this variation can",
        "type": "Document"
      },
      {
        "id": "046b442d-581a-4a47-9a4c-f89ccaffc6de",
        "metadata": {
          "vector_store_key": "1705.09665-4",
          "chunk_id": 85,
          "document_id": "1705.09665",
          "start_idx": 34950,
          "end_idx": 35527
        },
        "page_content": "\u2014retention, acculturation and content affinity\u2014and complements measures based on activity and size with additional interpretable information. For example, we find that in niche communities established members are more likely to engage with volatile content than outsiders, while the opposite is true in generic communities. Such insights can be useful for community maintainers seeking to understand engagement patterns in their own communities. One main area of future research is to examine the temporal dynamics in the multi-community landscape. By averaging our measures of",
        "type": "Document"
      },
      {
        "id": "8b9b562b-3bc4-4c09-aa6b-87ce024901e9",
        "metadata": {
          "vector_store_key": "1705.09665-8",
          "chunk_id": 14,
          "document_id": "1705.09665",
          "start_idx": 6067,
          "end_idx": 6609
        },
        "page_content": ", we first examine a few example communities and draw attention to some key social dynamics that occur within them. We consider four communities from Reddit: in Seahawks, fans of the Seahawks football team gather to discuss games and players; in BabyBumps, expecting mothers trade advice and updates on their pregnancy; Cooking consists of recipe ideas and general discussion about cooking; while in pics, users share various images of random things (like eels and hornets). We note that these communities are topically contrasting and foster",
        "type": "Document"
      },
      {
        "id": "ff830f7e-832d-45fa-828e-1724e189e482",
        "metadata": {
          "vector_store_key": "1705.09665-4",
          "chunk_id": 13,
          "document_id": "1705.09665",
          "start_idx": 5729,
          "end_idx": 6305
        },
        "page_content": " communities along these two dimensions. We start by providing an intuition through inspecting a few example communities. We then introduce a generalizable language-based methodology and use it to map a large set of Reddit communities onto the landscape defined by our typology of community identity. In order to illustrate the diversity within the multi-community space, and to provide an intuition for the underlying structure captured by the proposed typology, we first examine a few example communities and draw attention to some key social dynamics that occur within them",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How did the select the 300 Reddit communities for comparison?",
    "llm_answer": "The text states that the dataset consists of all subreddits on Reddit from January 2013 to December 2014, with at least 500 words in the vocabulary used to estimate measures in at least 4 months of the subreddit's history.  The text does not specify *how* the 300 were selected from this larger dataset.\n",
    "context": [
      {
        "id": "f8ce7737-b746-4573-8ae4-8d0781fc651a",
        "metadata": {
          "vector_store_key": "1705.09665-6",
          "chunk_id": 30,
          "document_id": "1705.09665",
          "start_idx": 12307,
          "end_idx": 12917
        },
        "page_content": " comment sections. The website contains many highly active subreddits with thousands of active subscribers. These communities span an extremely rich variety of topical interests, as represented by the examples described earlier. They also vary along a rich multitude of structural dimensions, such as the number of users, the amount of conversation and social interaction, and the social norms determining which types of content become popular. The diversity and scope of Reddit's multicommunity ecosystem make it an ideal landscape in which to closely examine the relation between varying community identities",
        "type": "Document"
      },
      {
        "id": "d4474f1c-563d-45d3-9f91-6c5b5da9c5cf",
        "metadata": {
          "vector_store_key": "1705.09665-6",
          "chunk_id": 7,
          "document_id": "1705.09665",
          "start_idx": 3016,
          "end_idx": 3592
        },
        "page_content": " is also convenient since it renders our framework immediately applicable to a wide variety of online communities, where communication is primarily recorded in a textual format. Using our framework, we map almost 300 Reddit communities onto the landscape defined by the two axes of our typology (Section SECREF2 ). We find that this mapping induces conceptually sound categorizations that effectively capture key aspects of community-level social dynamics. In particular, we quantitatively validate the effectiveness of our mapping by showing that our two-dimensional typology",
        "type": "Document"
      },
      {
        "id": "9961496e-448c-42a0-9968-46a12caab0cb",
        "metadata": {
          "vector_store_key": "1705.09665-6",
          "chunk_id": 38,
          "document_id": "1705.09665",
          "start_idx": 15656,
          "end_idx": 16151
        },
        "page_content": ", using only the top 5th percentile of words, by frequency within each INLINEFORM0 , to score comments and communities. Typology output on Reddit. The distribution of INLINEFORM0 and INLINEFORM1 across Reddit communities is shown in Figure FIGREF3 .B, along with examples of communities at the extremes of our typology. We find that interpretable groupings of communities emerge at various points within our axes. For instance, highly distinctive and dynamic communities tend to focus on rapidly",
        "type": "Document"
      },
      {
        "id": "a03e9d40-2fec-4988-a643-2aabec9e668a",
        "metadata": {
          "vector_store_key": "1705.09665-6",
          "chunk_id": 31,
          "document_id": "1705.09665",
          "start_idx": 12917,
          "end_idx": 13479
        },
        "page_content": " multicommunity ecosystem make it an ideal landscape in which to closely examine the relation between varying community identities and social dynamics. Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk",
        "type": "Document"
      },
      {
        "id": "c374edfb-0088-4960-9056-48662538cbf3",
        "metadata": {
          "vector_store_key": "1705.09665-6",
          "chunk_id": 29,
          "document_id": "1705.09665",
          "start_idx": 11974,
          "end_idx": 12541
        },
        "page_content": " We now explain how our typology can be applied to the particular setting of Reddit, and describe the overall behaviour of our linguistic axes in this context. Dataset description. Reddit is a popular website where users form and participate in discussion-based communities called subreddits. Within these communities, users post content\u2014such as images, URLs, or questions\u2014which often spark vibrant lengthy discussions in thread-based comment sections. The website contains many highly active subreddits with thousands of active subscribers. These communities span an",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the clinical text structuring task defined?",
    "llm_answer": "The clinical text structuring task aims to discover structured patient medical data, such as symptoms, diseases, tumor size, surgical details, and lab results, from clinical text.\n",
    "context": [
      {
        "id": "1f3ade0e-f018-4060-a094-08b139f3cfab",
        "metadata": {
          "vector_store_key": "1908.06606-9",
          "chunk_id": 9,
          "document_id": "1908.06606",
          "start_idx": 3526,
          "end_idx": 4037
        },
        "page_content": " work on clinical text structuring in Section SECREF2. Then, we present question answer based clinical text structuring task in Section SECREF3. In Section SECREF4, we present an effective model for this task. Section SECREF5 is devoted to computational studies and several investigations on the key issues of our proposed model. Finally, conclusions are given in Section SECREF6. Clinical text structuring is a final problem which is highly related to practical applications. Most of existing studies are case-",
        "type": "Document"
      },
      {
        "id": "c7426ce9-0ac3-4d5c-b385-35428c94934a",
        "metadata": {
          "vector_store_key": "1908.06606-9",
          "chunk_id": 0,
          "document_id": "1908.06606",
          "start_idx": 0,
          "end_idx": 498
        },
        "page_content": "Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained. It is important to extract structured data from clinical text because bio-medical systems or bio-medical researches greatly rely on",
        "type": "Document"
      },
      {
        "id": "3ceed48f-7077-4aa2-9256-fd83cfeba11a",
        "metadata": {
          "vector_store_key": "1908.06606-0",
          "chunk_id": 8,
          "document_id": "1908.06606",
          "start_idx": 3003,
          "end_idx": 3526
        },
        "page_content": " information into pre-trained language model. Experimental results show that QA-CTS task leads to significant improvement due to shared dataset. Our proposed model also achieves significantly better performance than the strong baseline methods. In addition, we also show that two-stage training mechanism has a great improvement on QA-CTS task. The rest of the paper is organized as follows. We briefly review the related work on clinical text structuring in Section SECREF2. Then, we present question answer based clinical",
        "type": "Document"
      },
      {
        "id": "76bbebf6-5aad-4199-9642-8687aaeb3871",
        "metadata": {
          "vector_store_key": "1908.06606-0",
          "chunk_id": 7,
          "document_id": "1908.06606",
          "start_idx": 2693,
          "end_idx": 3201
        },
        "page_content": "ifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data. The main contributions of this work can be summarized as follows. We first present a question answering based clinical text structuring (QA-CTS) task, which unifies different specific tasks and make dataset shareable. We also propose an effective model to integrate clinical named entity information into pre-trained language model. Experimental results show that QA-CTS task leads to",
        "type": "Document"
      },
      {
        "id": "67ec4e74-05a5-4d8c-b7bc-068cbd65bb22",
        "metadata": {
          "vector_store_key": "1908.06606-0",
          "chunk_id": 5,
          "document_id": "1908.06606",
          "start_idx": 1954,
          "end_idx": 2429
        },
        "page_content": " break down the entire process into several pieces which improves the performance and generality. However, when the pipeline depth grows, error propagation will have a greater impact on the performance. To reduce the pipeline depth and break the barrier of non-uniform output formats, we present a question answering based clinical text structuring (QA-CTS) task (see Fig. FIGREF1). Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is all text in this dataset a question, or are there unrelated sentences in between questions?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "a679e157-1e9a-4b70-a5c8-40afb866201f",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 28,
          "document_id": "1909.00694",
          "start_idx": 9330,
          "end_idx": 9716
        },
        "page_content": " positive and negative, respectively: . \u4f5c\u696d\u304c\u697d\u3060\u3002 The work is easy. . \u99d0\u8eca\u5834\u304c\u306a\u3044\u3002 There is no parking lot. Although the ACP corpus was originally constructed in the context of sentiment analysis, we found that it could roughly be regarded as a collection of affective events. We parsed each sentence and extracted the last clause in it. The train/dev/test split of the data is shown in Table T",
        "type": "Document"
      },
      {
        "id": "7087a12c-20dd-48db-a9aa-7483610488c2",
        "metadata": {
          "vector_store_key": "1908.06606-6",
          "chunk_id": 30,
          "document_id": "1908.06606",
          "start_idx": 10621,
          "end_idx": 11075
        },
        "page_content": " both text and query. Afterwards, $V_s$ and $I_n$ are integrated together and fed into a feed forward network to calculate the start and end index of answer-related text. Here we define this calculation problem as a classification for each word to be the start or end word. For any clinical free-text paragraph $X$ and query $Q$, contextualized representation is to generate the encoded vector of both of them. Here we use pre-trained language model BERT",
        "type": "Document"
      },
      {
        "id": "2aad297e-653a-407f-8112-cd84261bf66f",
        "metadata": {
          "vector_store_key": "1908.06606-6",
          "chunk_id": 32,
          "document_id": "1908.06606",
          "start_idx": 11284,
          "end_idx": 11795
        },
        "page_content": "i$. To tell the model $Q$ and $X$ is two different sentence, a sentence type input is generated which is a binary label sequence to denote what sentence each character in the input belongs to. Positional encoding and mask matrix is also constructed automatically to bring in absolute position information and eliminate the impact of zero padding respectively. Then a hidden vector $V_s$ which contains both query and text information is generated through BERT-base model. Since BERT is trained on general corpus",
        "type": "Document"
      },
      {
        "id": "8e603133-c60d-43d6-9ed1-f0511ceb6013",
        "metadata": {
          "vector_store_key": "1908.06606-6",
          "chunk_id": 29,
          "document_id": "1908.06606",
          "start_idx": 10348,
          "end_idx": 10728
        },
        "page_content": ", Single) tag scheme. $I_{nq}$ and $I_{nt}$ are then integrated together into $I_n$. Meanwhile, the paragraph text $X$ and query text $Q$ are organized and passed to contextualized representation model which is pre-trained language model BERT BIBREF26 here to obtain the contextualized representation vector $V_s$ of both text and query. Afterwards, $V_s$ and $I_n$ are integrated",
        "type": "Document"
      },
      {
        "id": "06e9a4b3-9db0-4e73-892a-9b9a4c094d48",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 27,
          "document_id": "1909.00694",
          "start_idx": 8898,
          "end_idx": 9330
        },
        "page_content": " CO such that it was five times larger than AL. The results are shown in Table TABREF16. We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively: . \u4f5c\u696d\u304c\u697d\u3060\u3002 The work",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many questions are in the dataset?",
    "llm_answer": "2,714\n",
    "context": [
      {
        "id": "a5ce310f-b3c9-4b0e-bc79-32634d5562ea",
        "metadata": {
          "vector_store_key": "1908.06606-7",
          "chunk_id": 44,
          "document_id": "1908.06606",
          "start_idx": 15529,
          "end_idx": 15988
        },
        "page_content": " Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences.",
        "type": "Document"
      },
      {
        "id": "06e9a4b3-9db0-4e73-892a-9b9a4c094d48",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 27,
          "document_id": "1909.00694",
          "start_idx": 8898,
          "end_idx": 9330
        },
        "page_content": " CO such that it was five times larger than AL. The results are shown in Table TABREF16. We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively: . \u4f5c\u696d\u304c\u697d\u3060\u3002 The work",
        "type": "Document"
      },
      {
        "id": "a03e9d40-2fec-4988-a643-2aabec9e668a",
        "metadata": {
          "vector_store_key": "1705.09665-6",
          "chunk_id": 31,
          "document_id": "1705.09665",
          "start_idx": 12917,
          "end_idx": 13479
        },
        "page_content": " multicommunity ecosystem make it an ideal landscape in which to closely examine the relation between varying community identities and social dynamics. Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk",
        "type": "Document"
      },
      {
        "id": "278a3d51-9b71-4e30-b8a5-f5c445d770a6",
        "metadata": {
          "vector_store_key": "1705.09665-6",
          "chunk_id": 32,
          "document_id": "1705.09665",
          "start_idx": 13229,
          "end_idx": 13749
        },
        "page_content": " community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities ( INLINEFORM0 ), for a total of 4,872 community-months ( INLINEFORM1 ). Estimating linguistic measures. We estimate word frequencies INLINEFORM0 , and by extension each downstream measure, in a carefully controlled manner in order to ensure we capture robust and meaningful linguistic behaviour. First, we only consider",
        "type": "Document"
      },
      {
        "id": "a2813591-f4ca-44da-9e32-566c3bf358d8",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 26,
          "document_id": "1909.00694",
          "start_idx": 8611,
          "end_idx": 9077
        },
        "page_content": " consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What aspects have been compared between various language models?",
    "llm_answer": "Model quality (e.g., lower perplexity) and performance (energy consumption, query latency, etc.).\n",
    "context": [
      {
        "id": "2682b1e1-0036-4627-9a76-e73ae22e4ff7",
        "metadata": {
          "vector_store_key": "1811.00942-0",
          "chunk_id": 0,
          "document_id": "1811.00942",
          "start_idx": 0,
          "end_idx": 514
        },
        "page_content": "Deep learning has unquestionably advanced the state of the art in many natural language processing tasks, from syntactic dependency parsing BIBREF0 to named-entity recognition BIBREF1 to machine translation BIBREF2 . The same certainly applies to language modeling, where recent advances in neural language models (NLMs) have led to dramatically better approaches as measured using standard metrics such as perplexity BIBREF3 , BIBREF4 . Specifically focused on language modeling, this paper examines an issue that",
        "type": "Document"
      },
      {
        "id": "f39a8edc-8f45-4b94-825e-4020c25a6a5e",
        "metadata": {
          "vector_store_key": "1908.06606-5",
          "chunk_id": 18,
          "document_id": "1908.06606",
          "start_idx": 6760,
          "end_idx": 7192
        },
        "page_content": " BIBREF26, BIBREF27 which makes language model a shared model to all natural language processing tasks. Radford et al. BIBREF24 proposed a framework for fine-tuning pre-trained language model. Peters et al. BIBREF25 proposed ELMo which concatenates forward and backward language models in a shallow manner. Devlin et al. BIBREF26 used bidirectional Transformers to model deep interactions between the two directions. Yang et al. BIB",
        "type": "Document"
      },
      {
        "id": "d7e0779f-33e5-41b8-b3d5-0127ad3f2f40",
        "metadata": {
          "vector_store_key": "1811.00942-0",
          "chunk_id": 1,
          "document_id": "1811.00942",
          "start_idx": 514,
          "end_idx": 1077
        },
        "page_content": "REF3 , BIBREF4 . Specifically focused on language modeling, this paper examines an issue that to our knowledge has not been explored: advances in neural language models have come at a significant cost in terms of increased computational complexity. Computing the probability of a token sequence using non-neural techniques requires a number of phrase lookups and perhaps a few arithmetic operations, whereas model inference with NLMs require large matrix multiplications consuming perhaps millions of floating point operations (FLOPs). These performance tradeoffs",
        "type": "Document"
      },
      {
        "id": "09a992b3-b3fa-421b-8dbe-a80ebbd6210b",
        "metadata": {
          "vector_store_key": "1811.00942-0",
          "chunk_id": 2,
          "document_id": "1811.00942",
          "start_idx": 862,
          "end_idx": 1397
        },
        "page_content": " large matrix multiplications consuming perhaps millions of floating point operations (FLOPs). These performance tradeoffs are worth discussing. In truth, language models exist in a quality\u2013performance tradeoff space. As model quality increases (e.g., lower perplexity), performance as measured in terms of energy consumption, query latency, etc. tends to decrease. For applications primarily running in the cloud\u2014say, machine translation\u2014practitioners often solely optimize for the lowest perplexity. This is because such applications",
        "type": "Document"
      },
      {
        "id": "3106e17b-6593-4e01-9186-625249139470",
        "metadata": {
          "vector_store_key": "1705.09665-1",
          "chunk_id": 6,
          "document_id": "1705.09665",
          "start_idx": 2493,
          "end_idx": 3016
        },
        "page_content": " and at scale is that the language used within a community can simultaneously capture how distinctive and dynamic its interests are. This language-based approach draws on a wealth of literature characterizing linguistic variation in online communities and its relationship to community and user identity BIBREF16 , BIBREF5 , BIBREF17 , BIBREF18 , BIBREF19 . Basing our typology on language is also convenient since it renders our framework immediately applicable to a wide variety of online communities, where communication",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many attention layers are there in their model?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "2908fb03-73e1-4dc5-99b9-1032bf35d653",
        "metadata": {
          "vector_store_key": "1907.05664-3",
          "chunk_id": 29,
          "document_id": "1907.05664",
          "start_idx": 10451,
          "end_idx": 10985
        },
        "page_content": " low. What seems to happen in this application is that most of the information used is transmitted from the encoder to the decoder and the attention mechanism at each decoding step just changes marginally how it is used. Quantifying the difference between attention distribution and saliency map across multiple tasks is a possible future work. The second observation we can make is that the saliency map doesn't seem to highlight the right things in the input for the summary it generates. The saliency maps on Figure 3 correspond to",
        "type": "Document"
      },
      {
        "id": "e8305d52-8e96-4a6b-a6c2-c68d53c60528",
        "metadata": {
          "vector_store_key": "1907.05664-3",
          "chunk_id": 28,
          "document_id": "1907.05664",
          "start_idx": 9953,
          "end_idx": 10451
        },
        "page_content": " in an experiment the relevance propagated through the attention mechanism to the encoder and didn't observe much changes in the saliency map. It can be seen as evidence that using the attention distribution as an \u201cexplanation\" of the prediction can be misleading. It is not the only information received by the decoder and the importance it \u201callocates\" to this attention state might be very low. What seems to happen in this application is that most of the information used is transmitted from the",
        "type": "Document"
      },
      {
        "id": "df83a74f-0dd6-4af7-a05f-7f853c26f0d5",
        "metadata": {
          "vector_store_key": "1907.05664-3",
          "chunk_id": 40,
          "document_id": "1907.05664",
          "start_idx": 14847,
          "end_idx": 15378
        },
        "page_content": " mechanism. We observed a peculiar behaviour of the saliency maps for the words in the output summary: they are almost all identical and seem uncorrelated with the attention distribution. We then proceeded to validate our attributions by averaging the absolute value of the relevance across the saliency maps. We obtain a ranking of the word from the most important to the least important and proceeded to delete one or another. We showed that in some cases the saliency maps are truthful to the network's computation, meaning that",
        "type": "Document"
      },
      {
        "id": "bd6d0dcb-2449-4ea5-8011-eb92c7e8830f",
        "metadata": {
          "vector_store_key": "1907.05664-2",
          "chunk_id": 23,
          "document_id": "1907.05664",
          "start_idx": 8019,
          "end_idx": 8498
        },
        "page_content": " we propagate relevance through an element-wise multiplication operation, we give all the upper-layer's relevance to the \u201cinformation\" vector and none to the \u201cgate\" vector. We use the same method to transmit relevance through the attention mechanism back to the encoder because Bahdanau's attention BIBREF9 uses element-wise multiplications as well. We depict in Figure 2 the transmission end-to-end from the output layer to the input through the decoder, attention mechanism and",
        "type": "Document"
      },
      {
        "id": "b05c1a09-345c-42fb-ac8c-c6f008a0a73b",
        "metadata": {
          "vector_store_key": "1811.00942-1",
          "chunk_id": 25,
          "document_id": "1811.00942",
          "start_idx": 8253,
          "end_idx": 8645
        },
        "page_content": " specific hyperparameters are the same as those in the repository. Our model architecture consists of 400-dimensional tied embedding weights BIBREF18 and four QRNN layers, with 1550 hidden units per layer on PTB and 2500 per layer on WT103. Both QRNN models have window sizes of $r=2$ for the first layer and $r=1$ for the rest. For the KN-5 model, we trained an off-the-shelf five-gram model",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the three measures of bias which are reduced in experiments?",
    "llm_answer": "Geometric bias mitigation,  measures of geometric bias mitigation, and measures of  probabilistic bias mitigation.\n",
    "context": [
      {
        "id": "91ee73d1-50cd-4e8a-a54d-daa94dd5f6f7",
        "metadata": {
          "vector_store_key": "1910.14497-7",
          "chunk_id": 2,
          "document_id": "1910.14497",
          "start_idx": 805,
          "end_idx": 1304
        },
        "page_content": ") and subtracting out the difference between the resulting distances BIBREF0. On the other hand, the most popular metric for measuring bias is the WEAT statistic BIBREF1, which compares the cosine similarities between groups of words. However, WEAT has been recently shown to overestimate bias as a result of implicitly relying on similar frequencies for the target words BIBREF4, and BIBREF5 demonstrated that evidence of bias can still be recovered after geometric bias mitigation by examining the",
        "type": "Document"
      },
      {
        "id": "700abba3-e05c-4eda-9d77-cfc4d7707032",
        "metadata": {
          "vector_store_key": "1910.14497-7",
          "chunk_id": 46,
          "document_id": "1910.14497",
          "start_idx": 15033,
          "end_idx": 15361
        },
        "page_content": "coon', 'violinist', 'vocalist', 'waiter', 'waitress', 'warden', 'warrior', 'worker', 'wrestler', 'writer' Words used for WEAT statistic, consisting of baseline bias tests and gender bias tests in the format X vs Y / A vs B Flowers vs Insects / Pleasant vs Unpleasant X: \"aster\", \"clover\", \"hyacinth\", \"marigold\", \"poppy\", \"azale",
        "type": "Document"
      },
      {
        "id": "95c23775-d185-4d96-b471-d4417a8e745a",
        "metadata": {
          "vector_store_key": "1910.14497-8",
          "chunk_id": 5,
          "document_id": "1910.14497",
          "start_idx": 2099,
          "end_idx": 2675
        },
        "page_content": " bias has also been approached (albeit in different contexts) in the fairness literature, where various intuitive notions of equity such as equalized odds have been formalized through probabilistic criteria. By considering analogous criteria for the word embedding setting, we seek to draw connections between these two bodies of work. We present experiments on various bias mitigation benchmarks and show that our framework is comparable to state-of-the-art alternatives according to measures of geometric bias mitigation and that it performs far better according to measures",
        "type": "Document"
      },
      {
        "id": "e1d8ab88-2303-4587-a0f4-ef76001e4feb",
        "metadata": {
          "vector_store_key": "1910.14497-8",
          "chunk_id": 4,
          "document_id": "1910.14497",
          "start_idx": 1623,
          "end_idx": 2099
        },
        "page_content": "abilistic at their core \u2013 i.e., they are trained (explicitly or implicitly BIBREF6) to minimize some form of word co-occurrence probabilities. Thus, we argue that a framework for measuring and treating bias in these embeddings should take into account, in addition to their geometric aspect, their probabilistic nature too. On the other hand, the issue of bias has also been approached (albeit in different contexts) in the fairness literature, where various intuitive notions",
        "type": "Document"
      },
      {
        "id": "42758d10-881d-42bd-b30b-aec03514e0c5",
        "metadata": {
          "vector_store_key": "1910.14497-7",
          "chunk_id": 9,
          "document_id": "1910.14497",
          "start_idx": 3464,
          "end_idx": 3761
        },
        "page_content": ",b_k}$. The WEAT statistic BIBREF1 demonstrates the presence of biases in word embeddings with an effect size defined as the mean test statistic across the two word sets: Where $s$, the test statistic, is defined as: $s(w,A,B) = mean_{a \\in A} cos(w,a) - mean_{b \\in B} cos(w,a)$, and $X$,$Y$,$A$,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is the dataset?",
    "llm_answer": "The answer is not found in the context.  While dataset sizes are mentioned in several examples, there is no single, overall dataset size provided.\n",
    "context": [
      {
        "id": "06e9a4b3-9db0-4e73-892a-9b9a4c094d48",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 27,
          "document_id": "1909.00694",
          "start_idx": 8898,
          "end_idx": 9330
        },
        "page_content": " CO such that it was five times larger than AL. The results are shown in Table TABREF16. We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively: . \u4f5c\u696d\u304c\u697d\u3060\u3002 The work",
        "type": "Document"
      },
      {
        "id": "a03e9d40-2fec-4988-a643-2aabec9e668a",
        "metadata": {
          "vector_store_key": "1705.09665-6",
          "chunk_id": 31,
          "document_id": "1705.09665",
          "start_idx": 12917,
          "end_idx": 13479
        },
        "page_content": " multicommunity ecosystem make it an ideal landscape in which to closely examine the relation between varying community identities and social dynamics. Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk",
        "type": "Document"
      },
      {
        "id": "4a909c01-ae7f-47e3-8c10-e6d1ec17b71e",
        "metadata": {
          "vector_store_key": "1908.06606-6",
          "chunk_id": 43,
          "document_id": "1908.06606",
          "start_idx": 15209,
          "end_idx": 15723
        },
        "page_content": " to achieve a better contextualized representation performance. Then we deploy the proposed model and load the fine tuned BERT weights, attach named entity information layers and retrain the model. In this section, we devote to experimentally evaluating our proposed task and approach. The best results in tables are in bold. Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,",
        "type": "Document"
      },
      {
        "id": "3e1406ea-9836-4a54-8399-d1463333181f",
        "metadata": {
          "vector_store_key": "1909.00694-8",
          "chunk_id": 49,
          "document_id": "1909.00694",
          "start_idx": 15395,
          "end_idx": 15766
        },
        "page_content": " segmented into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam BIBREF19. The mini-batch size was 32. We ran 1 epoch.",
        "type": "Document"
      },
      {
        "id": "610012c5-068f-4461-9da1-ecbc2ff26944",
        "metadata": {
          "vector_store_key": "1907.05664-0",
          "chunk_id": 10,
          "document_id": "1907.05664",
          "start_idx": 3794,
          "end_idx": 4271
        },
        "page_content": " articles coupled with summaries of about three sentences. These summaries are in fact \u201chighlights\" of the articles provided by the media themselves. Articles have an average length of 780 words and the summaries of 50 words. We had 287 000 training pairs and 11 500 test pairs. Similarly to See et al. See2017, we limit during training and prediction the input text to 400 words and generate summaries of 200 words. We pad the shorter texts using an UNKNOWN token and truncate",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the intensity of the PTSD established?",
    "llm_answer": "The intensity is estimated using a mean squared error of 1.2, with four intensity levels (No PTSD, Low Risk PTSD, Moderate Risk PTSD, and High Risk PTSD) assigned scores of 0, 1, 2, and 3 respectively.\n",
    "context": [
      {
        "id": "670a1451-a6ab-4a9f-960f-221c3933f8ae",
        "metadata": {
          "vector_store_key": "2003.07433-8",
          "chunk_id": 74,
          "document_id": "2003.07433",
          "start_idx": 29104,
          "end_idx": 29632
        },
        "page_content": " to detect Post Traumatic Stress Disorder (PTSD) sufferers in time before going out of control that may result catastrophic impacts on society, people around or even sufferers themselves. Although, psychiatrists invented several clinical diagnosis tools (i.e., surveys) by assessing symptoms, signs and impairment associated with PTSD, most of the times, the process of diagnosis happens at the severe stage of illness which may have already caused some irreversible damages of mental health of the sufferers. On the other hand,",
        "type": "Document"
      },
      {
        "id": "614d1971-06ce-408b-8e60-6127cc7bfc11",
        "metadata": {
          "vector_store_key": "2003.07433-9",
          "chunk_id": 9,
          "document_id": "2003.07433",
          "start_idx": 3968,
          "end_idx": 4431
        },
        "page_content": " ($\\approx 96\\%$) and its intensity ($\\approx 1.2$ mean squared error). Fig. FIGREF7 shows a schematic representation of our proposed model. It consists of the following logical steps: (i) Develop PTSD Detection System using twitter posts of war-veterans(ii) design real surveys from the popular symptoms based mental disease assessment surveys; (iii) define single category and create PTSD Linguistic Dictionary for each survey question and multiple aspect/words",
        "type": "Document"
      },
      {
        "id": "61bb70cb-66bc-42db-8769-0d527fbd50c6",
        "metadata": {
          "vector_store_key": "2003.07433-8",
          "chunk_id": 64,
          "document_id": "2003.07433",
          "start_idx": 24931,
          "end_idx": 25419
        },
        "page_content": " rest of 50% dataset. The distribution of this training-test dataset segmentation followed a 50% distribution of PTSD and No PTSD from the original dataset. Our final survey based classification results showed an accuracy of 96% in detecting PTSD and mean squared error of 1.2 in estimating its intensity given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively. Table TABREF29 shows the classification details",
        "type": "Document"
      },
      {
        "id": "9c8fc24f-2699-44f2-8b90-ef4e871b8201",
        "metadata": {
          "vector_store_key": "2003.07433-8",
          "chunk_id": 1,
          "document_id": "2003.07433",
          "start_idx": 566,
          "end_idx": 1079
        },
        "page_content": "REF1. As a result, the Veteran Administration's National Center for PTSD (NCPTSD) suggests to reconceptualize PTSD not just in terms of a psychiatric symptom cluster, but focusing instead on the specific high risk behaviors associated with it, as these may be directly addressed though behavioral change efforts BIBREF0. Consensus prevalence estimates suggest that PTSD impacts between 15-20% of the veteran population which is typically chronic and treatment resistant BIBREF0. The PTSD patients support programs",
        "type": "Document"
      },
      {
        "id": "be929710-007b-448a-a1a0-2bc9a6bfbb76",
        "metadata": {
          "vector_store_key": "2003.07433-9",
          "chunk_id": 72,
          "document_id": "2003.07433",
          "start_idx": 28243,
          "end_idx": 28766
        },
        "page_content": " collect more data and run not only nationwide but also international-wide data collection to establish our innovation into a real tool. Apart from that, as we achieved promising results in detecting PTSD and its intensity using only twitter data, we aim to develop Linguistic Dictionary for other mental health issues too. Moreover, we will apply our proposed method in other types of mental illness such as depression, bipolar disorder, suicidal ideation and seasonal affective disorder (SAD) etc. As we know, accuracy of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "how is quality measured?",
    "llm_answer": "EM-score (Exact Match) and (macro-averaged) F$_1$-score.  Reliability is also assessed by Cronbach's alpha.\n",
    "context": [
      {
        "id": "bb75c3fa-e2c5-412d-9db8-2f31cd178291",
        "metadata": {
          "vector_store_key": "1908.06606-4",
          "chunk_id": 45,
          "document_id": "1908.06606",
          "start_idx": 15988,
          "end_idx": 16383
        },
        "page_content": ") and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage",
        "type": "Document"
      },
      {
        "id": "2fb5a0a9-4e20-4413-913c-ca4bca980d55",
        "metadata": {
          "vector_store_key": "2003.07433-5",
          "chunk_id": 51,
          "document_id": "2003.07433",
          "start_idx": 20049,
          "end_idx": 20591
        },
        "page_content": " each of the category words within texts. After the PTSD Linguistic Dictionary has been created, we empirically evaluate its psychometric properties such as reliability and validity as per American Standards for educational and psychological testing guideline BIBREF24. In psychometrics, reliability is most commonly evaluated by Cronbach's alpha, which assesses internal consistency based on inter-correlations and the number of measured items. In the text analysis scenario, each word in our PTSD Linguistic dictionary is considered an item",
        "type": "Document"
      },
      {
        "id": "31743f95-f17a-4be8-9fac-2dea9f994302",
        "metadata": {
          "vector_store_key": "1907.05664-3",
          "chunk_id": 34,
          "document_id": "1907.05664",
          "start_idx": 12389,
          "end_idx": 12938
        },
        "page_content": " scaled down by a constant factor. The absolute value average seemed to deliver the best results. We delete incrementally the important words (words with the highest average) in the input and compared it to the control experiment that consists of deleting the least important word and compare the degradation of the resulting summaries. We obtain mitigated results: for some texts, we observe a quick degradation when deleting important words which are not observed when deleting unimportant words (see Figure 4 ), but for other test examples we don",
        "type": "Document"
      },
      {
        "id": "ec95bf06-caba-4a66-81ed-f978ee6d6b3f",
        "metadata": {
          "vector_store_key": "1907.05664-1",
          "chunk_id": 31,
          "document_id": "1907.05664",
          "start_idx": 11265,
          "end_idx": 11791
        },
        "page_content": " the input features. We will call that truthfulness of the attribution in regard to the computation, meaning that an attribution is truthful in regard to the computation if it actually highlights the important input features that the network attended to during prediction. We proceed to measure the truthfulness of the attributions by validating them quantitatively. We propose to validate the saliency maps in a similar way as Arras et al. Arras2017 by incrementally deleting \u201cimportant\" words from the input text and observe",
        "type": "Document"
      },
      {
        "id": "4447b73d-1db4-4dd0-ba23-1d60560daf55",
        "metadata": {
          "vector_store_key": "2003.07433-5",
          "chunk_id": 52,
          "document_id": "2003.07433",
          "start_idx": 20394,
          "end_idx": 20829
        },
        "page_content": ". In the text analysis scenario, each word in our PTSD Linguistic dictionary is considered an item, and reliability is calculated based on each text file's response to each word item, which forms an $N$(number of text files) $\\times $ $J$(number of words or stems in a dictionary) data matrix. There are two ways to quantify such responses: using percentage data (uncorrected method), or using \"present or not\" data (binary method) BIB",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the accuracy reported by state-of-the-art methods?",
    "llm_answer": "95.4%\n",
    "context": [
      {
        "id": "fc85b849-8b62-410f-8f70-2a1182aa13bc",
        "metadata": {
          "vector_store_key": "1910.04269-4",
          "chunk_id": 57,
          "document_id": "1910.04269",
          "start_idx": 21961,
          "end_idx": 22224
        },
        "page_content": " which is a recently developed technique to prevent over\ufb01tting on test data.This approach achieved an accuracy of 95.4%. We also analysed how attention mechanism and recurrent layers impact the performance of networks. This approach achieved an accuracy of 95.0%.",
        "type": "Document"
      },
      {
        "id": "36267cba-7883-408e-89df-2121b2539d41",
        "metadata": {
          "vector_store_key": "1908.06606-4",
          "chunk_id": 46,
          "document_id": "1908.06606",
          "start_idx": 16204,
          "end_idx": 16661
        },
        "page_content": " are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage of predictions that match any one of the ground truth answers exactly. The F$_1$-score metric is a looser metric measures the average overlap between the prediction and ground truth answer. To implement deep neural network models, we utilize the Keras library BIBREF36 with TensorFlow BIBREF37 backend. Each model is run on a single NVIDIA GeForce GTX 1080 Ti GPU.",
        "type": "Document"
      },
      {
        "id": "5c151c8e-9955-4c2b-be8b-2bc86da163e3",
        "metadata": {
          "vector_store_key": "2003.07433-0",
          "chunk_id": 8,
          "document_id": "2003.07433",
          "start_idx": 3481,
          "end_idx": 3968
        },
        "page_content": " surveys, but also obtain trusts of clinicians who are expected to see traditional survey results of the PTSD assessment. Finally, we evaluate the accuracy of LAXARY model performance and reliability-validity of generated PTSD Linguistic Dictionary using real twitter users' posts. Our results show that, given normal weekly messages posted in twitter, LAXARY can provide very high accuracy in filling up surveys towards identifying PTSD ($\\approx 96\\%$) and its intensity ($\\approx 1.2$",
        "type": "Document"
      },
      {
        "id": "bb75c3fa-e2c5-412d-9db8-2f31cd178291",
        "metadata": {
          "vector_store_key": "1908.06606-4",
          "chunk_id": 45,
          "document_id": "1908.06606",
          "start_idx": 15988,
          "end_idx": 16383
        },
        "page_content": ") and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage",
        "type": "Document"
      },
      {
        "id": "ec95bf06-caba-4a66-81ed-f978ee6d6b3f",
        "metadata": {
          "vector_store_key": "1907.05664-1",
          "chunk_id": 31,
          "document_id": "1907.05664",
          "start_idx": 11265,
          "end_idx": 11791
        },
        "page_content": " the input features. We will call that truthfulness of the attribution in regard to the computation, meaning that an attribution is truthful in regard to the computation if it actually highlights the important input features that the network attended to during prediction. We proceed to measure the truthfulness of the attributions by validating them quantitatively. We propose to validate the saliency maps in a similar way as Arras et al. Arras2017 by incrementally deleting \u201cimportant\" words from the input text and observe",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do the authors define or exemplify 'incorrect words'?",
    "llm_answer": "Missing or incorrect words, and sentences with grammatical errors.  Examples include tweets written in informal language, with disregard for sentence correctness.\n",
    "context": [
      {
        "id": "70784eca-500a-420f-8f1e-4385dced20fa",
        "metadata": {
          "vector_store_key": "2001.00137-3",
          "chunk_id": 1,
          "document_id": "2001.00137",
          "start_idx": 549,
          "end_idx": 1083
        },
        "page_content": " grammatically correct sentences. Panko BIBREF0 goes as far as claiming that human accuracy regarding research paper writing is none when considering the entire document. This has been aggravated with the advent of internet and social networks, which allowed language and modern communication to be been rapidly transformed BIBREF1, BIBREF2. Take Twitter for instance, where information is expected to be readily communicated in short and concise sentences with little to no regard to correct sentence grammar or word spelling BIBREF3",
        "type": "Document"
      },
      {
        "id": "1ae808ee-df37-40ff-be40-8d3527ed6834",
        "metadata": {
          "vector_store_key": "2001.00137-0",
          "chunk_id": 0,
          "document_id": "2001.00137",
          "start_idx": 0,
          "end_idx": 549
        },
        "page_content": "Understanding a user's intent and sentiment is of utmost importance for current intelligent chatbots to respond appropriately to human requests. However, current systems are not able to perform to their best capacity when presented with incomplete data, meaning sentences with missing or incorrect words. This scenario is likely to happen when one considers human error done in writing. In fact, it is rather naive to assume that users will always type fully grammatically correct sentences. Panko BIBREF0 goes as far as claiming that human accuracy",
        "type": "Document"
      },
      {
        "id": "7bf18101-52ba-4e9a-a1f8-9907d6859e4f",
        "metadata": {
          "vector_store_key": "1910.14497-4",
          "chunk_id": 0,
          "document_id": "1910.14497",
          "start_idx": 0,
          "end_idx": 500
        },
        "page_content": "Word embeddings, or vector representations of words, are an important component of Natural Language Processing (NLP) models and necessary for many downstream tasks. However, word embeddings, including embeddings commonly deployed for public use, have been shown to exhibit unwanted societal stereotypes and biases, raising concerns about disparate impact on axes of gender, race, ethnicity, and religion BIBREF0, BIBREF1. The impact of this bias has manifested in a range of downstream tasks, ranging",
        "type": "Document"
      },
      {
        "id": "0d0e209a-cee7-46e6-9d8e-a85408bb3078",
        "metadata": {
          "vector_store_key": "1705.09665-6",
          "chunk_id": 34,
          "document_id": "1705.09665",
          "start_idx": 14064,
          "end_idx": 14593
        },
        "page_content": ", ignoring successive uses of the same word by the same user. This ensures that our word-level characterizations are not skewed by a small subset of highly active contributors. In our subsequent analyses, we will only look at these measures computed over the nouns used in comments. In principle, our framework can be applied to any choice of vocabulary. However, in the case of Reddit using nouns provides a convenient degree of interpretability. We can easily understand the implication of a community preferentially mentioning",
        "type": "Document"
      },
      {
        "id": "d42beb36-87a2-4e71-a929-85f5a84df0ae",
        "metadata": {
          "vector_store_key": "2001.00137-0",
          "chunk_id": 5,
          "document_id": "2001.00137",
          "start_idx": 2222,
          "end_idx": 2741
        },
        "page_content": "REF8. Surprisingly, however, there is currently no research on incomplete text classification in the NLP community. Realizing the need of research in that area, we make it the focus of this paper. In this novel task, the model aims to identify the user's intent or sentiment by analyzing a sentence with missing and/or incorrect words. In the sentiment classification task, the model aims to identify the user's sentiment given a tweet, written in informal language and without regards for sentence correctness. Current",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much do they outperform other models in the sentiment in intent classification tasks?",
    "llm_answer": "6% to 8% better F1-micro scores.\n",
    "context": [
      {
        "id": "071cf9d4-4f10-4550-a4a4-cb8bbdc9f3de",
        "metadata": {
          "vector_store_key": "2001.00137-3",
          "chunk_id": 49,
          "document_id": "2001.00137",
          "start_idx": 18686,
          "end_idx": 19161
        },
        "page_content": " Chatbot Intent Classification Corpus). Experimental results for the Twitter Sentiment Classification task on Kaggle's Sentiment140 Corpus dataset, displayed in Table TABREF37, show that our model has better F1-micros scores, outperforming the baseline models by 6$\\%$ to 8$\\%$. We evaluate our model and baseline models on three versions of the dataset. The first one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80",
        "type": "Document"
      },
      {
        "id": "940d1529-1abd-445d-ae85-4f7625a4f746",
        "metadata": {
          "vector_store_key": "2001.00137-2",
          "chunk_id": 48,
          "document_id": "2001.00137",
          "start_idx": 18254,
          "end_idx": 18686
        },
        "page_content": " time depending on the size of the dataset and train batch size. The stack of multilayer perceptrons are trained for 100 and 1,000 epochs with Adam Optimizer, learning rate of $10^{-3}$, weight decay of $10^{-5}$, MSE loss criterion and batch size the same as BERT (4 for the Twitter Sentiment Corpus and 8 for the Chatbot Intent Classification Corpus). Experimental results for the Twitter Sentiment Classification task on Kaggle's",
        "type": "Document"
      },
      {
        "id": "72793fca-aeeb-4854-9abe-c93f5fc266e4",
        "metadata": {
          "vector_store_key": "2001.00137-2",
          "chunk_id": 47,
          "document_id": "2001.00137",
          "start_idx": 17991,
          "end_idx": 18416
        },
        "page_content": " GPUs for 3 epochs with Adam Optimizer, learning rate of $2*10^{-5}$, maximum sequence length of 128, and warm up proportion of $0.1$. The train batch size is 4 for the Twitter Sentiment Corpus and 8 for the Chatbot Intent Classification Corpus. Our proposed model is trained in end-to-end manner on 2 Titan X GPUs, with training time depending on the size of the dataset and train batch size. The stack of multilayer percept",
        "type": "Document"
      },
      {
        "id": "b6350b3a-decd-4ccf-8029-7d4a137b1ad3",
        "metadata": {
          "vector_store_key": "2001.00137-3",
          "chunk_id": 63,
          "document_id": "2001.00137",
          "start_idx": 23847,
          "end_idx": 24394
        },
        "page_content": " their accuracy while the lower achieving class saw improvement. Further evaluation on the F1-scores decay in the presence of noise demonstrated that our model is more robust than the baseline models when considering noisy data, be that in the form of incorrect sentences or sentences with STT error. Not only that, experiments on the Twitter dataset also showed improved accuracy in clean data, with complete sentences. We infer that this is due to our model being able to extract richer data representations from the input data regardless of the",
        "type": "Document"
      },
      {
        "id": "9b52a0c8-1a01-4579-a5b6-8214fdda19e4",
        "metadata": {
          "vector_store_key": "1909.00694-9",
          "chunk_id": 36,
          "document_id": "1909.00694",
          "start_idx": 11751,
          "end_idx": 12204
        },
        "page_content": " of discourse relation-based label propagation. Comparing the model variants, we obtained the highest score with the BiGRU encoder trained with the AL+CA+CO dataset. BERT was competitive but its performance went down if CA and CO were used in addition to AL. We conjecture that BERT was more sensitive to noises found more frequently in CA and CO. Contrary to our expectations, supervised models (ACP) outperformed semi-supervised models (ACP+AL+CA+CO).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which experiments are perfomed?",
    "llm_answer": "Experiments on the WNC dataset with variations of $RoBERTa_{large}$, $ALBERT_{xxlarge.v2}$, $DistilRoBERTa$ and $BERT$,  and weighted-average ensembling.\n",
    "context": [
      {
        "id": "bb75c3fa-e2c5-412d-9db8-2f31cd178291",
        "metadata": {
          "vector_store_key": "1908.06606-4",
          "chunk_id": 45,
          "document_id": "1908.06606",
          "start_idx": 15988,
          "end_idx": 16383
        },
        "page_content": ") and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage",
        "type": "Document"
      },
      {
        "id": "9727f2d1-d7d1-467c-8d99-8a138d1097c8",
        "metadata": {
          "vector_store_key": "2002.06644-1",
          "chunk_id": 13,
          "document_id": "2002.06644",
          "start_idx": 5099,
          "end_idx": 5495
        },
        "page_content": " variations of $RoBERTa_{large}$, $ALBERT_{xxlarge.v2}$, $DistilRoBERTa$ and $BERT$ and outline selected combinations in tab:experimental-results. We perform our experiments on the WNC dataset open-sourced by the authors of BIBREF2. It consists of aligned pre and post neutralized sentences made by Wikipedia editors under the neutral point of view. It contains $180k$ biased sentences, and their",
        "type": "Document"
      },
      {
        "id": "c270760a-3f97-41b4-84b6-68ddf24229ea",
        "metadata": {
          "vector_store_key": "1910.04269-4",
          "chunk_id": 54,
          "document_id": "1910.04269",
          "start_idx": 20617,
          "end_idx": 21192
        },
        "page_content": " robust to variations which might be present in real world scenarios. There can be further exploration of various feature extraction techniques like Constant-Q transform and Fast Fourier Transform and assessment of their impact on Language Identification. There can be further improvements in neural network architectures like concatenating the high level features obtained from 1D-ConvNet and 2D-ConvNet, before performing classification. There can be experiments using deeper networks with skip connections and Inception modules. These are known to have positively impacted",
        "type": "Document"
      },
      {
        "id": "24fa816c-569e-4b7b-be00-07596d80b873",
        "metadata": {
          "vector_store_key": "1908.06606-1",
          "chunk_id": 10,
          "document_id": "1908.06606",
          "start_idx": 3825,
          "end_idx": 4300
        },
        "page_content": "uring is a final problem which is highly related to practical applications. Most of existing studies are case-by-case. Few of them are developed for the general purpose structuring task. These studies can be roughly divided into three categories: rule and dictionary based methods, task-specific end-to-end methods and pipeline methods. Rule and dictionary based methods BIBREF0, BIBREF1, BIBREF2 rely extremely on heuristics and handcrafted extraction rules which is more of",
        "type": "Document"
      },
      {
        "id": "c55d429c-cd7d-43de-896b-4dbf0413bb27",
        "metadata": {
          "vector_store_key": "2002.06644-2",
          "chunk_id": 12,
          "document_id": "2002.06644",
          "start_idx": 4609,
          "end_idx": 5099
        },
        "page_content": "rained distilled models on the training corpus to efficiently detect subjectivity. BERT-based ensemble models: Lastly, we use the weighted-average ensembling technique to exploit the predictions made by different variations of the above models. Ensembling methodology entails engendering a predictive model by utilizing predictions from multiple models in order to improve Accuracy and F1, decrease variance, and bias. We experiment with variations of $RoBERTa_{large}$, $ALBERT_{xxlarge.v2",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is ROUGE their only baseline?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "02ce962c-daed-4951-8d22-156cdbab9755",
        "metadata": {
          "vector_store_key": "1809.08731-6",
          "chunk_id": 54,
          "document_id": "1809.08731",
          "start_idx": 17965,
          "end_idx": 18451
        },
        "page_content": " correlated. In fact, sometimes it is difficult to distinguish which one is problematic: to illustrate this, we show some extreme examples\u2014compressions which got the highest fluency rating and the lowest possible content rating by at least one rater, but the lowest fluency score and the highest content score by another\u2014in Table 6 . We, thus, hypothesize that ROUGE should contain information about fluency which is complementary to SLOR, and want to make use of references for fluency",
        "type": "Document"
      },
      {
        "id": "897bf0ba-8e9d-4cea-bf97-02d0cb8b1c05",
        "metadata": {
          "vector_store_key": "1809.08731-6",
          "chunk_id": 44,
          "document_id": "1809.08731",
          "start_idx": 14576,
          "end_idx": 14995
        },
        "page_content": " references and no annotated examples are available. In contrast to SVR (line 1), ROUGE-LM (line 2) has only the same requirements as conventional word-overlap metrics (besides a large corpus for training the LM, which is easy to obtain for most languages). Thus, it can be used in the same settings as other word-overlap metrics. Since ROUGE-LM\u2014an uninformed combination\u2014performs significantly better than both ROUGE-L",
        "type": "Document"
      },
      {
        "id": "c28c41a3-86f4-4a2f-9d06-2227ae14622c",
        "metadata": {
          "vector_store_key": "1809.08731-6",
          "chunk_id": 43,
          "document_id": "1809.08731",
          "start_idx": 14319,
          "end_idx": 14742
        },
        "page_content": ", respectively. Results are shown in Table 7 . First, we can see that using SVR (line 1) to combine ROUGE-L-mult and WPSLOR outperforms both individual scores (lines 3-4) by a large margin. This serves as a proof of concept: the information contained in the two approaches is indeed complementary. Next, we consider the setting where only references and no annotated examples are available. In contrast to SVR (line 1), ROU",
        "type": "Document"
      },
      {
        "id": "9d2bc9d9-d3b4-4e9d-acc5-d7aec88719bf",
        "metadata": {
          "vector_store_key": "1809.08731-6",
          "chunk_id": 53,
          "document_id": "1809.08731",
          "start_idx": 17672,
          "end_idx": 18191
        },
        "page_content": " performance for fluency evaluation. However, a domain-specific LM might additionally improve the metrics' correlation with human judgments. We leave a more detailed analysis of the importance of the training data's domain for future work. ROUGE was shown to correlate well with ratings of a generated text's content or meaning at the sentence level BIBREF2 . We further expect content and fluency ratings to be correlated. In fact, sometimes it is difficult to distinguish which one is problematic: to illustrate this,",
        "type": "Document"
      },
      {
        "id": "91f2824f-431a-4712-b492-89b0ab421124",
        "metadata": {
          "vector_store_key": "1809.08731-6",
          "chunk_id": 72,
          "document_id": "1809.08731",
          "start_idx": 24617,
          "end_idx": 25067
        },
        "page_content": " metrics on a benchmark dataset. Additionally, they can be applied even in settings where no references are available, or would be costly to obtain. Finally, for given references, we proposed the reference-based metric ROUGE-LM, which consists of a combination of WPSLOR and ROUGE. Thus, we were able to obtain an even more accurate fluency evaluation. We would like to thank Sebastian Ebert and Samuel Bowman for their detailed and helpful feedback.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does their system outperform the lexicon-based models?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "9b52a0c8-1a01-4579-a5b6-8214fdda19e4",
        "metadata": {
          "vector_store_key": "1909.00694-9",
          "chunk_id": 36,
          "document_id": "1909.00694",
          "start_idx": 11751,
          "end_idx": 12204
        },
        "page_content": " of discourse relation-based label propagation. Comparing the model variants, we obtained the highest score with the BiGRU encoder trained with the AL+CA+CO dataset. BERT was competitive but its performance went down if CA and CO were used in addition to AL. We conjecture that BERT was more sensitive to noises found more frequently in CA and CO. Contrary to our expectations, supervised models (ACP) outperformed semi-supervised models (ACP+AL+CA+CO).",
        "type": "Document"
      },
      {
        "id": "2682b1e1-0036-4627-9a76-e73ae22e4ff7",
        "metadata": {
          "vector_store_key": "1811.00942-0",
          "chunk_id": 0,
          "document_id": "1811.00942",
          "start_idx": 0,
          "end_idx": 514
        },
        "page_content": "Deep learning has unquestionably advanced the state of the art in many natural language processing tasks, from syntactic dependency parsing BIBREF0 to named-entity recognition BIBREF1 to machine translation BIBREF2 . The same certainly applies to language modeling, where recent advances in neural language models (NLMs) have led to dramatically better approaches as measured using standard metrics such as perplexity BIBREF3 , BIBREF4 . Specifically focused on language modeling, this paper examines an issue that",
        "type": "Document"
      },
      {
        "id": "ba07f67e-c8f0-48f4-aa84-0be888144992",
        "metadata": {
          "vector_store_key": "1811.00942-3",
          "chunk_id": 36,
          "document_id": "1811.00942",
          "start_idx": 11673,
          "end_idx": 12104
        },
        "page_content": " does. Overall, these findings agree with those from BIBREF21 , which explores the log perplexity\u2013word error rate scale in language modeling for speech recognition. Quality\u2013performance tradeoff. In Table 2 , from left to right, we report perplexity results on the validation and test sets, R@3 on test, and finally per-query latency and energy usage. On the RPi, KN-5 is both fast and power-efficient to run, using only about 7 ms/",
        "type": "Document"
      },
      {
        "id": "f5d66c45-e178-4464-9c16-0f5d62ac6638",
        "metadata": {
          "vector_store_key": "1809.08731-0",
          "chunk_id": 40,
          "document_id": "1809.08731",
          "start_idx": 13249,
          "end_idx": 13586
        },
        "page_content": "L-mult.  Comparing WordSLOR and WPSLOR, we find no significant differences: $0.017$ for Pearson and $0.006$ for MSE. However, WPSLOR uses a more compact LM and, hence, has a shorter training time, since the vocabulary is smaller ( $16,000$ vs. $128,000$ tokens). Next, we find that WordNCE and WPNCE perform roughly on par with word-over",
        "type": "Document"
      },
      {
        "id": "3b672db7-7288-4e2b-a7e5-f8ff93c82b22",
        "metadata": {
          "vector_store_key": "2003.07433-7",
          "chunk_id": 14,
          "document_id": "2003.07433",
          "start_idx": 5802,
          "end_idx": 6249
        },
        "page_content": " one character n-gram language model (CLM); and 3) one from the LIWC categories $\\alpha $-scores and found that last one gives more accuracy than other ones. BIBREF11 used two types of $s$-scores taking the ratio of negative and positive language models. Differences in language use have been observed in the personal writing of students who score highly on depression scales BIBREF2, forum posts for depression BIBREF3, self narratives for PTSD (",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are the main international development topics that states raise identified?",
    "llm_answer": "By examining the key words associated with topics discussed in UN General Debate statements, identifying topics like economic development and sustainable development as the principal ones.\n",
    "context": [
      {
        "id": "ae106540-3c67-40b6-9db4-3226ac0a7729",
        "metadata": {
          "vector_store_key": "1708.05873-3",
          "chunk_id": 16,
          "document_id": "1708.05873",
          "start_idx": 6744,
          "end_idx": 7219
        },
        "page_content": "', `growth', `financ-', and etc. suggests that Topic 2 represent a more traditional view of international development in that the emphasis is specifically on economic processes and relations. Topic 3 - Nuclear disarmament. This topic picks up the issue of nuclear weapons, which has been a major issue in the UN since its founding. Topic 4 - Post-conflict development. This topic relates to post-conflict development. The countries that feature in the key words (e.g. Rwanda,",
        "type": "Document"
      },
      {
        "id": "ae718934-86b2-469a-96a9-1d8471a30c4e",
        "metadata": {
          "vector_store_key": "1708.05873-1",
          "chunk_id": 44,
          "document_id": "1708.05873",
          "start_idx": 18219,
          "end_idx": 18780
        },
        "page_content": " most. States in South Asia and East Asia and the Pacific discuss Topic 7 the most. The figure shows that countries in North America are likely to speak about Topic 7 least. The analysis of discussion of international development in annual UN General Debate statements therefore uncovers two principle development topics: economic development and sustainable development. We find that discussion of Topic 2 is not significantly impacted by country-specific factors, such as wealth, population, democracy, levels of ODA, and conflict (although there are regional",
        "type": "Document"
      },
      {
        "id": "ce3a5412-0bd0-41bb-b0d8-cc7f85589d07",
        "metadata": {
          "vector_store_key": "1708.05873-3",
          "chunk_id": 24,
          "document_id": "1708.05873",
          "start_idx": 9933,
          "end_idx": 10425
        },
        "page_content": ", and the Helsinki Accords. Based on these topics, we examine Topic 2 and Topic 7 as the principal \u201cinternational development\u201d topics. While a number of other topics \u2013 for example post-conflict development, Africa, Latin America, etc. \u2013 are related to development issues, Topic 2 and Topic 7 most directly capture aspects of international development. We consider these two topics more closely by contrasting the main words linked to these two topics. In Figure FIGREF6 , the word clouds show",
        "type": "Document"
      },
      {
        "id": "ced7b6f2-d1a5-4854-aa95-b88c45f12950",
        "metadata": {
          "vector_store_key": "1708.05873-1",
          "chunk_id": 9,
          "document_id": "1708.05873",
          "start_idx": 4026,
          "end_idx": 4560
        },
        "page_content": " the key international development topics discussed in the GD. We model topic prevalence in the context of the structural covariates. In addition, we control for region fixed effects and time trend. The aim is to allow the observed metadata to affect the frequency with which a topic is discussed in General Debate speeches. This allows us to test the degree of association between covariates (and region/time effects) and the average proportion of a document discussing a topic. We assess the optimal number of topics that need to be",
        "type": "Document"
      },
      {
        "id": "b81d53aa-d5ff-416d-8421-552407342414",
        "metadata": {
          "vector_store_key": "1708.05873-3",
          "chunk_id": 20,
          "document_id": "1708.05873",
          "start_idx": 8352,
          "end_idx": 8830
        },
        "page_content": "solution', `effort', `general', etc. Topic 9 - War. This topic directly relates to issues of war. The key words appear to be linked to discussions around ongoing wars. Topic 10 - Conflict in the Middle East. This topic clearly picks up issues related to the Middle East \u2013 particularly around peace and conflict in the Middle East. Topic 11 - Latin America. This is another topic with a regional focus, picking up on issues related to Latin America. Topic 12 - Commonwealth. This",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is the semantic hierarchy representation used for any task?",
    "llm_answer": "Yes.\n",
    "context": [
      {
        "id": "3922b7cb-1493-418f-a32e-f0a3def1b908",
        "metadata": {
          "vector_store_key": "1909.12140-0",
          "chunk_id": 16,
          "document_id": "1909.12140",
          "start_idx": 6708,
          "end_idx": 7227
        },
        "page_content": " of Open IE systems can be easily enriched with contextual information that allows to restore the semantic relationship between a set of propositions and, hence, preserve their interpretability in downstream tasks. We developed and implemented a discourse-aware syntactic TS approach that recursively splits and rephrases complex English or German sentences into a semantic hierarchy of simplified sentences. The resulting lightweight semantic representation can be used to facilitate and improve a variety of AI tasks.",
        "type": "Document"
      },
      {
        "id": "0d34d1ba-80ce-40a4-bec4-2306b2ae932c",
        "metadata": {
          "vector_store_key": "1909.12140-0",
          "chunk_id": 3,
          "document_id": "1909.12140",
          "start_idx": 1335,
          "end_idx": 1894
        },
        "page_content": " approach based on Rhetorical Structure Theory (RST) BIBREF3. It establishes a contextual hierarchy between the split components, and identifies and classifies the semantic relationship that holds between them. In that way, a complex source sentence is turned into a so-called discourse tree, consisting of a set of hierarchically ordered and semantically interconnected sentences that present a simplified syntax which is easier to process for downstream semantic applications and may support a faster generalization in machine learning tasks. We present Dis",
        "type": "Document"
      },
      {
        "id": "45526f3c-f20d-4c0a-a79b-3f339ff65189",
        "metadata": {
          "vector_store_key": "1909.12140-0",
          "chunk_id": 8,
          "document_id": "1909.12140",
          "start_idx": 3417,
          "end_idx": 3981
        },
        "page_content": " sentences (satellites) that disclose additional information about it. To differentiate between those two types of constituents, the transformation patterns encode a simple syntax-based approach where subordinate clauses/phrases are classified as context sentences, while superordinate as well as coordinate clauses/phrases are labelled as core. Second, we aim to restore the semantic relationship between the disembedded components. For this purpose, we identify and classify the rhetorical relations that hold between the simplified sentences, making use of both",
        "type": "Document"
      },
      {
        "id": "b7f97ae4-10c4-4e1c-b7b5-1451a463ec18",
        "metadata": {
          "vector_store_key": "1909.12140-2",
          "chunk_id": 5,
          "document_id": "1909.12140",
          "start_idx": 2277,
          "end_idx": 2783
        },
        "page_content": " These patterns were heuristically determined in a comprehensive linguistic analysis and encode syntactic and lexical features that can be derived from a sentence's parse tree. Each rule specifies (1) how to split up and rephrase the input into structurally simplified sentences and (2) how to set up a semantic hierarchy between them. They are recursively applied on a given source sentence in a top-down fashion. When no more rule matches, the algorithm stops and returns the generated discourse tree. In",
        "type": "Document"
      },
      {
        "id": "6918b578-a7f7-485a-936d-484e3f9f3700",
        "metadata": {
          "vector_store_key": "1909.12140-0",
          "chunk_id": 7,
          "document_id": "1909.12140",
          "start_idx": 3098,
          "end_idx": 3645
        },
        "page_content": " more sentences with a simplified syntax. To establish a semantic hierarchy between them, two subtasks are carried out: First, we set up a contextual hierarchy between the split sentences by connecting them with information about their hierarchical level, similar to the concept of nuclearity in RST. For this purpose, we distinguish core sentences (nuclei), which carry the key information of the input, from accompanying contextual sentences (satellites) that disclose additional information about it. To differentiate between those two types of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is the model evaluated?",
    "llm_answer": "Yes.\n",
    "context": [
      {
        "id": "4a96115b-96a0-40c9-8804-17c559445d5a",
        "metadata": {
          "vector_store_key": "1809.04960-5",
          "chunk_id": 63,
          "document_id": "1809.04960",
          "start_idx": 24606,
          "end_idx": 25124
        },
        "page_content": " the analysis of the error types of our model, we still need to focus on avoiding predicting the plausible comments. There are few studies regarding machine commenting. Qin et al. QinEA2018 is the first to propose the article commenting task and a dataset, which is used to evaluate our model in this work. More studies about the comments aim to automatically evaluate the quality of the comments. Park et al. ParkSDE16 propose a system called CommentIQ, which assist the comment moderators in identifying high quality",
        "type": "Document"
      },
      {
        "id": "21806947-d88f-404f-b964-d7856915d5cb",
        "metadata": {
          "vector_store_key": "1809.04960-5",
          "chunk_id": 43,
          "document_id": "1809.04960",
          "start_idx": 16805,
          "end_idx": 17328
        },
        "page_content": " various, so it is intractable to find out all the possible references to be compared with the model outputs. Inspired by the evaluation methods of dialogue models, we formulate the evaluation as a ranking problem. Given a piece of news and a set of candidate comments, the comment model should return the rank of the candidate comments. The candidate comment set consists of the following parts: Correct: The ground-truth comments of the corresponding news provided by the human. Plausible: The 50 most similar comments to",
        "type": "Document"
      },
      {
        "id": "799bdc71-c27d-497e-8b27-f21853762889",
        "metadata": {
          "vector_store_key": "1907.05664-3",
          "chunk_id": 35,
          "document_id": "1907.05664",
          "start_idx": 12938,
          "end_idx": 13449
        },
        "page_content": " are not observed when deleting unimportant words (see Figure 4 ), but for other test examples we don't observe a significant difference between the two settings (see Figure 5 ). One might argue that the second summary in Figure 5 is better than the first one as it makes better sentences but as the model generates inaccurate summaries, we do not wish to make such a statement. This however allows us to say that the attribution generated for the text at the origin of the summaries in Figure 4 are truthful in",
        "type": "Document"
      },
      {
        "id": "77a901fa-dee6-4178-9ccf-9f69697895ca",
        "metadata": {
          "vector_store_key": "1809.08731-5",
          "chunk_id": 3,
          "document_id": "1809.08731",
          "start_idx": 1267,
          "end_idx": 1699
        },
        "page_content": " subject of a decade-long debate in cognitive science. In particular, the question has been raised whether the grammatical knowledge that underlies this ability is probabilistic or categorical in nature BIBREF3 , BIBREF4 , BIBREF5 . Within this context, lau2017grammaticality have recently shown that neural language models (LMs) can be used for modeling human ratings of acceptability. Namely, they found SLOR BIBREF6 \u2014sentence log",
        "type": "Document"
      },
      {
        "id": "c55d429c-cd7d-43de-896b-4dbf0413bb27",
        "metadata": {
          "vector_store_key": "2002.06644-2",
          "chunk_id": 12,
          "document_id": "2002.06644",
          "start_idx": 4609,
          "end_idx": 5099
        },
        "page_content": "rained distilled models on the training corpus to efficiently detect subjectivity. BERT-based ensemble models: Lastly, we use the weighted-average ensembling technique to exploit the predictions made by different variations of the above models. Ensembling methodology entails engendering a predictive model by utilizing predictions from multiple models in order to improve Accuracy and F1, decrease variance, and bias. We experiment with variations of $RoBERTa_{large}$, $ALBERT_{xxlarge.v2",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How better is accuracy of new model compared to previously reported models?",
    "llm_answer": "The new model achieved improvements in EM-score and F$_1$-score compared to BERT-Base, QANet, and other baseline models (TF-IDF, S2S, IR).  Specific improvements vary, but examples include 5.64% better EM-score and 3.69% better F$_1$-score than BERT-Base.\n",
    "context": [
      {
        "id": "f90151f1-17af-403b-a162-bbea1585c396",
        "metadata": {
          "vector_store_key": "1809.04960-4",
          "chunk_id": 57,
          "document_id": "1809.04960",
          "start_idx": 22226,
          "end_idx": 22753
        },
        "page_content": " further improve the performance of the supervised model. Although our proposed model can achieve better performance than previous models, there are still remaining two questions: why our model can outperform them, and how to further improve the performance. To address these queries, we perform error analysis to analyze the error types of our model and the baseline models. We select TF-IDF, S2S, and IR as the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types",
        "type": "Document"
      },
      {
        "id": "b367f47b-ea61-495e-8709-0724ed1aad7a",
        "metadata": {
          "vector_store_key": "1908.06606-4",
          "chunk_id": 52,
          "document_id": "1908.06606",
          "start_idx": 18241,
          "end_idx": 18595
        },
        "page_content": "formed it with 0.75% score in EM-score. Compared with BERT-Base, our model led to a 5.64% performance improvement in EM-score and 3.69% in F$_1$-score. Although our model didn't outperform much with QANet in F$_1$-score (only 0.13%), our model significantly outperformed it with 6.39% score in EM-score. To further investigate the effects of named entity",
        "type": "Document"
      },
      {
        "id": "2edde34b-120d-4687-b68f-1177eb03c8ad",
        "metadata": {
          "vector_store_key": "1908.06606-4",
          "chunk_id": 51,
          "document_id": "1908.06606",
          "start_idx": 18088,
          "end_idx": 18399
        },
        "page_content": " TABREF23. Table TABREF23 indicates that our proposed model achieved the best performance both in EM-score and F$_1$-score with EM-score of 91.84% and F$_1$-score of 93.75%. QANet outperformed BERT-Base with 3.56% score in F$_1$-score but underperformed it with 0.75% score in EM-score. Compared with BERT-Base,",
        "type": "Document"
      },
      {
        "id": "b6350b3a-decd-4ccf-8029-7d4a137b1ad3",
        "metadata": {
          "vector_store_key": "2001.00137-3",
          "chunk_id": 63,
          "document_id": "2001.00137",
          "start_idx": 23847,
          "end_idx": 24394
        },
        "page_content": " their accuracy while the lower achieving class saw improvement. Further evaluation on the F1-scores decay in the presence of noise demonstrated that our model is more robust than the baseline models when considering noisy data, be that in the form of incorrect sentences or sentences with STT error. Not only that, experiments on the Twitter dataset also showed improved accuracy in clean data, with complete sentences. We infer that this is due to our model being able to extract richer data representations from the input data regardless of the",
        "type": "Document"
      },
      {
        "id": "2682b1e1-0036-4627-9a76-e73ae22e4ff7",
        "metadata": {
          "vector_store_key": "1811.00942-0",
          "chunk_id": 0,
          "document_id": "1811.00942",
          "start_idx": 0,
          "end_idx": 514
        },
        "page_content": "Deep learning has unquestionably advanced the state of the art in many natural language processing tasks, from syntactic dependency parsing BIBREF0 to named-entity recognition BIBREF1 to machine translation BIBREF2 . The same certainly applies to language modeling, where recent advances in neural language models (NLMs) have led to dramatically better approaches as measured using standard metrics such as perplexity BIBREF3 , BIBREF4 . Specifically focused on language modeling, this paper examines an issue that",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does the active learning model work?",
    "llm_answer": "Active learning uses a learning engine (a classifier) and a selection engine (sampling strategy) to select and label a small subset of unlabeled data.  The relabeled samples are added to the training set, and the classifier is retrained, improving accuracy in iterative steps.\n",
    "context": [
      {
        "id": "0a955999-6c1a-407f-99c4-3dd87b603103",
        "metadata": {
          "vector_store_key": "1908.08419-0",
          "chunk_id": 6,
          "document_id": "1908.08419",
          "start_idx": 2078,
          "end_idx": 2590
        },
        "page_content": " in general domains, causing a major problem that a considerable amount of manually annotated data is required, when introducing the models into specific domains. One of the solutions for this obstacle is to use active learning, where only a small scale of samples are selected and labeled in an active manner. Active learning methods are favored by the researchers in many natural language processing (NLP) tasks, such as text classification BIBREF9 and named entity recognition (NER) BIBREF10 . However, only a",
        "type": "Document"
      },
      {
        "id": "8ce2edd8-044c-45e7-95fc-4066c08de13e",
        "metadata": {
          "vector_store_key": "1908.08419-0",
          "chunk_id": 22,
          "document_id": "1908.08419",
          "start_idx": 8307,
          "end_idx": 8763
        },
        "page_content": " which chooses samples that need to be relabeled by annotators from unlabeled data. Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, a CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively. Fig. FIGREF7 and Algorithm SECREF3 demonstrate the procedure of CWS based on active learning. First, we train",
        "type": "Document"
      },
      {
        "id": "82f1e249-7f2f-41e0-8cf6-8a11daed51d4",
        "metadata": {
          "vector_store_key": "1908.08419-8",
          "chunk_id": 21,
          "document_id": "1908.08419",
          "start_idx": 7803,
          "end_idx": 8307
        },
        "page_content": ". However, in some complicated tasks, such as CWS and NER, only considering the uncertainty of classifier is obviously not enough. Active learning methods can generally be described into two parts: a learning engine and a selection engine BIBREF28 . The learning engine is essentially a classifier, which is mainly used for training of classification problems. The selection engine is based on the sampling strategy, which chooses samples that need to be relabeled by annotators from unlabeled data. Then",
        "type": "Document"
      },
      {
        "id": "e326f683-f1c3-46c9-ba78-fd45c5c6a175",
        "metadata": {
          "vector_store_key": "1908.08419-8",
          "chunk_id": 64,
          "document_id": "1908.08419",
          "start_idx": 23290,
          "end_idx": 23799
        },
        "page_content": " to annotate the unlabeled set. Then, sampling strategy in active learning selects a part of samples for annotators to relabel. Finally, the relabeled samples are added to train set for segmenter to re-train. Our proposed scoring strategy selects samples according to the sequence scores of the segmented sentences, while uncertainty sampling suggests relabeling samples that are closest to the segmenter\u2019s decision boundary. Generally, two main parameters in active learning are the numbers of iterations and",
        "type": "Document"
      },
      {
        "id": "6e47e59e-c265-41e8-9d88-c5e568443819",
        "metadata": {
          "vector_store_key": "1908.08419-0",
          "chunk_id": 18,
          "document_id": "1908.08419",
          "start_idx": 6703,
          "end_idx": 7256
        },
        "page_content": " approach for CWS in medical text, which combines information entropy with neural network to effectively reduce annotation cost. Active learning BIBREF22 mainly aims to ease the data collection process by automatically deciding which instances should be labeled by annotators to train a model as quickly and effectively as possible BIBREF23 . The sampling strategy plays a key role in active learning. In the past decade, the rapid development of active learning has resulted in various sampling strategies, such as uncertainty sampling BIBREF24 , query",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Did the annotators agreed and how much?",
    "llm_answer": "Inter-annotator agreement was found to be sufficiently high, and moderate to substantial agreement was observed on a subset of cases.  A 90.5% agreement was considered good.\n",
    "context": [
      {
        "id": "ae777c6e-5f84-42f7-8843-5ba4994ac482",
        "metadata": {
          "vector_store_key": "1703.05260-8",
          "chunk_id": 49,
          "document_id": "1703.05260",
          "start_idx": 17551,
          "end_idx": 18087
        },
        "page_content": " results were discussed with the authors. Throughout the whole annotation phase, annotators could discuss any emerging issues with the authors. All annotations were done by undergraduate students of computational linguistics. The annotation was rather time-consuming due to the complexity of the task, and thus we decided for single annotation mode. To assess annotation quality, a small sample of texts was annotated by all four annotators and their inter-annotator agreement was measured (see Section \"Inter-Annotator Agreement\" ). It",
        "type": "Document"
      },
      {
        "id": "521b024e-758a-4249-af24-eed867d46c15",
        "metadata": {
          "vector_store_key": "1703.05260-8",
          "chunk_id": 50,
          "document_id": "1703.05260",
          "start_idx": 17876,
          "end_idx": 18350
        },
        "page_content": " inter-annotator agreement was measured (see Section \"Inter-Annotator Agreement\" ). It was found to be sufficiently high. Annotation of the corpus together with some pre- and post-processing of the data required about 500 hours of work. All stories were annotated with event and participant types (a total of 12,188 and 43,946 instances, respectively). On average there were 7 coreference chains per story with an average length of 6 tokens. After the first annotation round",
        "type": "Document"
      },
      {
        "id": "c0388cb3-a80e-489d-983a-717364e8883a",
        "metadata": {
          "vector_store_key": "1703.05260-8",
          "chunk_id": 65,
          "document_id": "1703.05260",
          "start_idx": 23142,
          "end_idx": 23652
        },
        "page_content": " by all 4 annotators after the first annotation phase. We checked the agreement on these data using Fleiss' Kappa BIBREF4 . The results are shown in Figure 4 and indicate moderate to substantial agreement BIBREF5 . Interestingly, if we calculated the Kappa only on the subset of cases that were annotated with script-specific event and participant labels by all annotators, results were better than those of the evaluation on all labeled instances (including also unrelated and related non-script events). This",
        "type": "Document"
      },
      {
        "id": "8fbba252-c464-4ae5-95a1-c11eccfb25ce",
        "metadata": {
          "vector_store_key": "1703.05260-8",
          "chunk_id": 64,
          "document_id": "1703.05260",
          "start_idx": 22746,
          "end_idx": 23142
        },
        "page_content": "dresser}}}$ is cutting my $_{\\textsc {\\scriptsize {ScrPart\\_customer}}}$ hair we $_{\\textsc {\\scriptsize Scr\\_Part\\_hairdresser\\_customer}}$ usually talk a bit. In order to calculate inter-annotator agreement, a total of 30 stories from 6 scenarios were randomly chosen for parallel annotation by all 4 annotators after the first annotation phase. We checked the agreement on these data using Fle",
        "type": "Document"
      },
      {
        "id": "5e5dfe45-f801-413b-ac8a-29f23d004928",
        "metadata": {
          "vector_store_key": "1703.05260-8",
          "chunk_id": 67,
          "document_id": "1703.05260",
          "start_idx": 23974,
          "end_idx": 24460
        },
        "page_content": " 3 annotators (qualified majority vote) compared to the set of those pairs annotated by at least one person (see Figure 4 ). We take the result of 90.5% between annotators to be a good agreement. Figure 5 gives an overview of the number of event and participant types provided in the templates. Taking a flight and getting a haircut stand out with a large number of both event and participant types, which is due to the inherent complexity of the scenarios. In contrast, planting a tree",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What datasets are used to evaluate this approach?",
    "llm_answer": "A large-scale Chinese dataset with millions of real comments and a human-annotated test set collected from Tencent News, consisting of 198,112 news articles.  Also, 815 test instances (5,421 sentences) are used.\n",
    "context": [
      {
        "id": "bb75c3fa-e2c5-412d-9db8-2f31cd178291",
        "metadata": {
          "vector_store_key": "1908.06606-4",
          "chunk_id": 45,
          "document_id": "1908.06606",
          "start_idx": 15988,
          "end_idx": 16383
        },
        "page_content": ") and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage",
        "type": "Document"
      },
      {
        "id": "77a2c284-62f0-43e0-8bf9-52a2d068ed04",
        "metadata": {
          "vector_store_key": "2003.07433-1",
          "chunk_id": 49,
          "document_id": "2003.07433",
          "start_idx": 19311,
          "end_idx": 19770
        },
        "page_content": " well as dimensions' text analysis scores. We chose three categories based on the three different surveys: 1) DOSPERT scale; 2) BSSS scale; and 3) VIAS scale. Dimension selection: We have to define the word sets (also called dimension) for each category. We chose one dimension for each of the questions under each category to reflect real survey system evaluation. Our chosen categories are state in Fig FIGREF20. Score calculation $\\alpha $-score: $\\alpha $",
        "type": "Document"
      },
      {
        "id": "ac08e6c0-ed7c-44a4-8569-c15f1a5e35d1",
        "metadata": {
          "vector_store_key": "1809.04960-5",
          "chunk_id": 58,
          "document_id": "1809.04960",
          "start_idx": 22497,
          "end_idx": 22960
        },
        "page_content": " the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types of comments as described in the above retrieval evaluation setting: Correct, Plausible, Popular, and Random. We rank the candidate comment set with four models (TF-IDF, S2S, IR, and Proposed+IR), and record the types of top-1 comments. Figure FIGREF40 shows the percentage of different types of top-1 comments generated by each model. It shows",
        "type": "Document"
      },
      {
        "id": "d97120a1-c318-4d6c-89f7-4bb4ee280a24",
        "metadata": {
          "vector_store_key": "1809.04960-8",
          "chunk_id": 34,
          "document_id": "1809.04960",
          "start_idx": 13507,
          "end_idx": 13972
        },
        "page_content": " and paired data INLINEFORM4 . We select a large-scale Chinese dataset BIBREF0 with millions of real comments and a human-annotated test set to evaluate our model. The dataset is collected from Tencent News, which is one of the most popular Chinese websites for news and opinion articles. The dataset consists of 198,112 news articles. Each piece of news contains a title, the content of the article, and a list of the users' comments. Following the previous work B",
        "type": "Document"
      },
      {
        "id": "af93d2c3-10cc-4265-a880-027f1992bae5",
        "metadata": {
          "vector_store_key": "2003.07433-1",
          "chunk_id": 11,
          "document_id": "2003.07433",
          "start_idx": 4701,
          "end_idx": 5182
        },
        "page_content": "ores and $s$-scores of each category based on the $s$-scores of its dimensions; (vi) rank features according to the contributions of achieving separation among categories associated with different $\\alpha $-scores and $s$-scores; and select feature sets that minimize the overlap among categories as associated with the target classifier (SGD); and finally (vii) estimate the quality of selected features-based classification for filling up surveys based on classified categories i",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How was the dataset collected?",
    "llm_answer": "Extracted from Japanese websites using HTML layouts and linguistic patterns, covering various genres.  For the paired worker dataset, 90 well-trained workers participated in data collection online, choosing tourist or system roles and completing dialogues.\n",
    "context": [
      {
        "id": "06e9a4b3-9db0-4e73-892a-9b9a4c094d48",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 27,
          "document_id": "1909.00694",
          "start_idx": 8898,
          "end_idx": 9330
        },
        "page_content": " CO such that it was five times larger than AL. The results are shown in Table TABREF16. We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively: . \u4f5c\u696d\u304c\u697d\u3060\u3002 The work",
        "type": "Document"
      },
      {
        "id": "7e10c4cb-bec9-4c30-85d2-b27e4ec52647",
        "metadata": {
          "vector_store_key": "2001.00137-6",
          "chunk_id": 35,
          "document_id": "2001.00137",
          "start_idx": 13822,
          "end_idx": 14323
        },
        "page_content": " transport connections. The dataset has two intents, namely Departure Time and Find Connection with 100 train and 106 test samples, shown in Table TABREF18. Even though English is the main language of the benchmark, this dataset contains a few German station and street names. The incomplete dataset used for training is composed of lower-cased incomplete data obtained by manipulating the original corpora. The incomplete sentences with STT error are obtained in a 2-step process shown in Fig. FIGREF",
        "type": "Document"
      },
      {
        "id": "b8fd882e-992b-4dc0-962b-6e98b5b00744",
        "metadata": {
          "vector_store_key": "2002.11893-2",
          "chunk_id": 37,
          "document_id": "2002.11893",
          "start_idx": 13977,
          "end_idx": 14497
        },
        "page_content": " online. On the website, workers are free to choose one of the two roles: tourist (user) or system (wizard). Then, two paired workers are sent to a chatroom. The user needs to accomplish the allocated goal through conversation while the wizard searches the database to provide the necessary information and gives responses. Before the formal data collection, we trained the workers to complete a small number of dialogues by giving them feedback. Finally, 90 well-trained workers are participating in the data collection",
        "type": "Document"
      },
      {
        "id": "38e182ee-fbda-4ecb-9bd3-29733e9a857d",
        "metadata": {
          "vector_store_key": "1907.05664-0",
          "chunk_id": 9,
          "document_id": "1907.05664",
          "start_idx": 3532,
          "end_idx": 3981
        },
        "page_content": " in this section the baseline model from See et al. See2017 trained on the CNN/Daily Mail dataset. We reproduce the results from See et al. See2017 to then apply LRP on it. The CNN/Daily mail dataset BIBREF12 is a text summarization dataset adapted from the Deepmind question-answering dataset BIBREF13 . It contains around three hundred thousand news articles coupled with summaries of about three sentences. These summaries are in fact \u201chighlights",
        "type": "Document"
      },
      {
        "id": "419119a3-f10f-40e2-ba27-f3de123fb008",
        "metadata": {
          "vector_store_key": "2002.02224-0",
          "chunk_id": 44,
          "document_id": "2002.02224",
          "start_idx": 18011,
          "end_idx": 18184
        },
        "page_content": " the institutional support of the Masaryk University. This paper was presented at CEILI Workshop on Legal Data Analysis held in conjunction with Jurix 2019 in Madrid, Spain.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What models other than standalone BERT is new model compared to?",
    "llm_answer": "RoBERTa<sub>large</sub>, ALBERT, BERT, and their ensemble models.\n",
    "context": [
      {
        "id": "5f118b77-c1bd-4e17-ade3-4d7aa28a31cf",
        "metadata": {
          "vector_store_key": "2002.06644-2",
          "chunk_id": 6,
          "document_id": "2002.06644",
          "start_idx": 2489,
          "end_idx": 2911
        },
        "page_content": " various BERT-based models, including BERT, RoBERTa, ALBERT, with their base and large specifications along with their native classifiers. We propose an ensemble model exploiting predictions from these models using multiple ensembling techniques. We show that our model outperforms the baselines by a margin of $5.6$ of F1 score and $5.95\\%$ of Accuracy. In this section, we outline baseline models like $BERT_{large}$. We",
        "type": "Document"
      },
      {
        "id": "a4a26585-8ef3-422a-9b62-1bba9a4ffee6",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 46,
          "document_id": "2001.00137",
          "start_idx": 17632,
          "end_idx": 17991
        },
        "page_content": "$ and regularization term of $1.0$. Most often, the best performing classifier was MLP. Conventional BERT is a BERT-base-uncased model, meaning that it has 12 transformer blocks $L$, hidden size $H$ of 768, and 12 self-attention heads $A$. The model is fine-tuned with our dataset on 2 Titan X GPUs for 3 epochs with Adam Optimizer, learning rate of $2*10^{-5",
        "type": "Document"
      },
      {
        "id": "efafe945-13f8-4908-87d2-a8b7daf2106c",
        "metadata": {
          "vector_store_key": "2002.06644-0",
          "chunk_id": 11,
          "document_id": "2002.06644",
          "start_idx": 4295,
          "end_idx": 4797
        },
        "page_content": "QuAD for Question Answering. Distilled BERT-based models: Secondly, we propose to use distilled BERT-based models, as introduced in BIBREF8. They are smaller general-purpose language representation model, pre-trained by leveraging distillation knowledge. This results in significantly smaller and faster models with performance comparable to their undistilled versions. We finetune these pretrained distilled models on the training corpus to efficiently detect subjectivity. BERT-based ensemble models:",
        "type": "Document"
      },
      {
        "id": "5c902bd0-c2c9-44a6-b540-f374184d32ef",
        "metadata": {
          "vector_store_key": "1910.07181-2",
          "chunk_id": 78,
          "document_id": "1910.07181",
          "start_idx": 25645,
          "end_idx": 26144
        },
        "page_content": " standalone BERT model and one combined with Attentive Mimicking across all tasks. While keeping the original BERT embeddings in addition to Bertram's representation brings no benefit, adding in-domain data clearly helps for two out of three datasets. This makes sense as for rare words, every single additional context can be crucial for gaining a deeper understanding. To further understand for which words using Bertram is helpful, in Figure FIGREF39 we look at the accuracy of BERT both with and",
        "type": "Document"
      },
      {
        "id": "fb01b160-a168-4070-9f5a-fd61cc41f56f",
        "metadata": {
          "vector_store_key": "2002.06644-2",
          "chunk_id": 17,
          "document_id": "2002.06644",
          "start_idx": 6467,
          "end_idx": 6878
        },
        "page_content": "ERT based models, and BERT-based ensemble models outperform the baselines for all the metrics. Among the optimized BERT based models, $RoBERTa_{large}$ outperforms all other non-ensemble models and the baselines for all metrics. It further achieves a maximum recall of $0.681$ for all the proposed models. We note that DistillRoBERTa, a distilled model, performs competitively, achieving $69.69\\%$ accuracy, and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is the performance difference between this method and the baseline?",
    "llm_answer": "0.244 and 0.097 Pearson correlation, and 0.012 and 0.012 MSE.\n",
    "context": [
      {
        "id": "19f51692-d91e-4963-901c-fa5b8254322c",
        "metadata": {
          "vector_store_key": "1811.00942-2",
          "chunk_id": 43,
          "document_id": "1811.00942",
          "start_idx": 14041,
          "end_idx": 14133
        },
        "page_content": " $\\times $ rise in latency and 32 $\\times $ increase in energy usage, when compared to KN-5.",
        "type": "Document"
      },
      {
        "id": "7b2c50e7-9624-4431-bb2d-4f1b83f6475e",
        "metadata": {
          "vector_store_key": "1809.08731-0",
          "chunk_id": 47,
          "document_id": "1809.08731",
          "start_idx": 15656,
          "end_idx": 15966
        },
        "page_content": "35$ vs. $\\text{Var}(Y) = 0.17$ for SEQ2SEQ). Comparing the metrics, the two SLOR approaches perform best for SEQ2SEQ and T3. In particular, they outperform the best word-overlap metric baseline by $0.244$ and $0.097$ Pearson correlation as well as $0.012$ and $0.012$ MSE, respectively. Since T3 is an abstract",
        "type": "Document"
      },
      {
        "id": "c7fedf32-b198-483e-b944-2f34c3b6e57f",
        "metadata": {
          "vector_store_key": "2001.00137-1",
          "chunk_id": 50,
          "document_id": "2001.00137",
          "start_idx": 18932,
          "end_idx": 19320
        },
        "page_content": " one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80$\\%$ against BERT's 72$\\%$. The second version (Corr) considers the corrected tweets, and shows higher accuracy given that it is less noisy. In that version, Stacked DeBERT achieves 82$\\%$ accuracy against BERT's 76$\\%$, an improvement of 6$\\%$. In the last case (Inc+Corr), we",
        "type": "Document"
      },
      {
        "id": "a14e00d5-68d3-4bea-b9d6-6ad0992afd02",
        "metadata": {
          "vector_store_key": "1811.00942-2",
          "chunk_id": 39,
          "document_id": "1811.00942",
          "start_idx": 12674,
          "end_idx": 13079
        },
        "page_content": " over 1.2 seconds per next-word prediction. Neural models achieve perplexity drops of 60\u201380% and R@3 increases of 22\u201334%, but these improvements come at a much higher cost in latency and energy usage. In Table 2 (last two columns), the desktop yields very different results: the neural models on PTB (rows 2\u20133) are 9 $\\times $ slower than KN-5, but the absolute latency is only 8 ms/q, which is still much",
        "type": "Document"
      },
      {
        "id": "896923b4-37f9-4b9d-846e-0a8c38e6f697",
        "metadata": {
          "vector_store_key": "1908.06606-2",
          "chunk_id": 65,
          "document_id": "1908.06606",
          "start_idx": 22920,
          "end_idx": 23336
        },
        "page_content": " performance on distal resection margin improved by 5.56% in EM-score and 6.32% in F$_1$-score. Other performances also usually improved a lot. This proves the usefulness of two-stage training and named entity information as well. Lastly, we fine tune the model for each task with a pre-trained parameter. Table TABREF32 summarizes the result. (Add some explanations for the Table TABREF32). Comparing Table TABREF32",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What approaches without reinforcement learning have been tried?",
    "llm_answer": "Classification-based and regression-based approaches.  Deep learning architectures.\n",
    "context": [
      {
        "id": "51efb65b-2c96-40aa-8bcb-81cc71a95e2f",
        "metadata": {
          "vector_store_key": "1902.00330-5",
          "chunk_id": 101,
          "document_id": "1902.00330",
          "start_idx": 34293,
          "end_idx": 34753
        },
        "page_content": " state to help disambiguation. In the last few years, reinforcement learning has emerged as a powerful tool for solving complex sequential decision-making problems. It is well known for its great success in the game field, such as Go BIBREF37 and Atari games BIBREF38 . Recently, reinforcement learning has also been successfully applied to many natural language processing tasks and achieved good performance BIBREF12 , BIBREF39 , BIBREF5 . Feng et al. BIBREF",
        "type": "Document"
      },
      {
        "id": "20b8ec83-cb62-4e9d-bd50-91b5234703f2",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 43,
          "document_id": "1909.00542",
          "start_idx": 16697,
          "end_idx": 17106
        },
        "page_content": " for the reward function used by a reinforcement learning system. Reinforcement learning gives promising results, especially in human evaluations made on the runs submitted to BioASQ 6b. This year we introduced very small changes to the runs using reinforcement learning, and will aim to explore more complex reinforcement learning strategies and more complex neural models in the policy and value estimators.",
        "type": "Document"
      },
      {
        "id": "644dd4da-698b-443e-aeae-137d54b264d9",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 23,
          "document_id": "1909.00542",
          "start_idx": 8896,
          "end_idx": 9426
        },
        "page_content": " the results are compatible with the top part of the table and prior work suggesting that classification-based approaches improve over regression-based approaches. We also experiment with the use of reinforcement learning techniques. Again these experiments are based on BIBREF2, who uses REINFORCE to train a global policy. The policy predictor uses a simple feedforward network with a hidden layer. The results reported by BIBREF2 used ROUGE Recall and indicated no improvement with respect to deep learning architectures. Human",
        "type": "Document"
      },
      {
        "id": "8107ca43-d71a-48a3-8565-8a2e1310853f",
        "metadata": {
          "vector_store_key": "1902.00330-5",
          "chunk_id": 102,
          "document_id": "1902.00330",
          "start_idx": 34613,
          "end_idx": 35109
        },
        "page_content": "REF12 , BIBREF39 , BIBREF5 . Feng et al. BIBREF5 used reinforcement learning for relation classification task by filtering out the noisy data from the sentence bag and they achieved huge improvements compared with traditional classifiers. Zhang et al. BIBREF40 applied the reinforcement learning on sentence representation by automatically discovering task-relevant structures. To automatic taxonomy induction from a set of terms, Han et al. BIBREF41 designed an end-to-end reinforcement learning",
        "type": "Document"
      },
      {
        "id": "65e861a7-84e0-4b29-a7b8-a2181a840f0d",
        "metadata": {
          "vector_store_key": "1902.00330-5",
          "chunk_id": 100,
          "document_id": "1902.00330",
          "start_idx": 33757,
          "end_idx": 34293
        },
        "page_content": " of previously referred entities for the subsequent entity disambiguation. Nguyen et al. BIBREF2 use the sequence model, but they simply encode the results of the greedy choice, and measure the similarities between the global encoding and the candidate entity representations. Their model does not consider the long-term impact of current decisions on subsequent choices, nor does they add the selected target entity information to the current state to help disambiguation. In the last few years, reinforcement learning has emerged as a",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which languages do they validate on?",
    "llm_answer": "Spanish (Es), French (Fr), Italian (It), English, German, and Russian.\n",
    "context": [
      {
        "id": "07344119-d726-4d67-a8fe-3e22de9ad73d",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 46,
          "document_id": "1910.04269",
          "start_idx": 17760,
          "end_idx": 18217
        },
        "page_content": " of 1,500 samples for each of six languages. Out of 1,500 samples for each language, 1,200 were randomly selected as training dataset for that language and rest 300 as validation dataset using k-fold cross-validation. To sum up, we trained our model on 7,200 samples and validated it on 1800 samples comprising six languages. The results are discussed in next section. This paper discusses two end-to-end approaches which achieve state-of-the-art results in",
        "type": "Document"
      },
      {
        "id": "e90f75a5-f326-4202-82c5-878736af8308",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 51,
          "document_id": "1910.04269",
          "start_idx": 19553,
          "end_idx": 20023
        },
        "page_content": ", the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Kat",
        "type": "Document"
      },
      {
        "id": "a546fe79-c67f-4ddc-bac3-fd6f2efe9a2f",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 50,
          "document_id": "1910.04269",
          "start_idx": 19085,
          "end_idx": 19553
        },
        "page_content": "fold cross validation for six languages when mixup was applied. In Fig FIGREF27 (b) we present the confusion matrix for the 2D-ConvNet model. 2D attention models focused on the important features extracted by convolutional layers and bi-directional GRU captured the temporal features. Several of the spoken languages in Europe belong to the Indo-European family. Within this family, the languages are divided into three phyla which are Romance, Germanic and Slavic. Of",
        "type": "Document"
      },
      {
        "id": "b0d78a46-0cfa-4367-a963-2a9a8d82c9a4",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 18,
          "document_id": "1910.04269",
          "start_idx": 6853,
          "end_idx": 7342
        },
        "page_content": " comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (",
        "type": "Document"
      },
      {
        "id": "920bf597-62c3-4b9e-9093-4f838581cb08",
        "metadata": {
          "vector_store_key": "2003.07433-5",
          "chunk_id": 55,
          "document_id": "2003.07433",
          "start_idx": 21602,
          "end_idx": 22174
        },
        "page_content": " and .96 based on the binary method, which confirm the high reliability of our PTSD Dictionary created PTSD survey based categories. After assessing the reliability of the PTSD Linguistic dictionary, we focus on the two most common forms of construct validity: convergent validity and discriminant validity BIBREF25. Convergent validity provides evidence that two measures designed to assess the same construct are indeed related; discriminate validity involves evidence that two measures designed to assess different constructs are not too strongly related. In theory, we",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the baseline method for the task?",
    "llm_answer": "A neural network model trained on the train set, using dropout rate of 0.3, Adam optimizer with a learning rate of $10^{-5}$, batch normalization, and gradually increasing the KL term from 0 to 1 after each epoch.\n",
    "context": [
      {
        "id": "0cd02404-842e-4686-aebd-9976a4de080d",
        "metadata": {
          "vector_store_key": "1910.07181-5",
          "chunk_id": 47,
          "document_id": "1910.07181",
          "start_idx": 15902,
          "end_idx": 16251
        },
        "page_content": "ii) training the baseline model on the train set and (iii) modifying a subset of the test candidates to generate the final test set. We partition $\\mathcal {D}$ into a train set $\\mathcal {D}_\\text{train}$ and a set of test candidates, $\\mathcal {D}_\\text{cand}$, with the latter containing all instances $(\\mathbf {x},y) \\in \\mathcal {D}$ such that",
        "type": "Document"
      },
      {
        "id": "bc8ac242-0b5e-4c71-be0b-48d167015fad",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 43,
          "document_id": "1909.02764",
          "start_idx": 17421,
          "end_idx": 17831
        },
        "page_content": " layer. This setup is inspired by BIBREF38. We use a dropout rate of 0.3 in all layers and optimize with Adam BIBREF39 with a learning rate of $10^{-5}$ (These parameters are the same for all further experiments). We build on top of the Keras library with the TensorFlow backend. We consider this setup our baseline model. We train models on a variety of corpora, namely the common format published by BIBREF27",
        "type": "Document"
      },
      {
        "id": "ace147d0-5bf0-4495-a2ea-94bcd97f86ed",
        "metadata": {
          "vector_store_key": "1809.04960-1",
          "chunk_id": 38,
          "document_id": "1809.04960",
          "start_idx": 14887,
          "end_idx": 15333
        },
        "page_content": " and use batch normalization BIBREF9 in each layer. We also gradually increase the KL term from 0 to 1 after each epoch. We compare our model with several unsupervised models and supervised models. Unsupervised baseline models are as follows: TF-IDF (Lexical, Non-Neural) is an important unsupervised baseline. We use the concatenation of the title and the body as the query to retrieve the candidate comment set by means of the similarity of the",
        "type": "Document"
      },
      {
        "id": "b76ee65f-bf0f-4d9a-82ff-4f1d9e8c0d01",
        "metadata": {
          "vector_store_key": "1910.07181-5",
          "chunk_id": 46,
          "document_id": "1910.07181",
          "start_idx": 15478,
          "end_idx": 15902
        },
        "page_content": " an arbitrary text classification dataset $\\mathcal {D}$ containing labelled instances $(\\mathbf {x}, y)$ and a substitution dictionary $S$, mapping each word $w$ to a set of rare synonyms $S(w)$. Given these ingredients, our procedure consists of three steps: (i) splitting the dataset into a train set and a set of test candidates, (ii) training the baseline model on the train set and (iii) modifying a subset of the test",
        "type": "Document"
      },
      {
        "id": "caf96a37-2de9-4e90-b08a-ca8a54bac59b",
        "metadata": {
          "vector_store_key": "1909.00542-2",
          "chunk_id": 40,
          "document_id": "1909.00542",
          "start_idx": 15344,
          "end_idx": 15845
        },
        "page_content": "$\u201d baseline produces the same results as the neural regressor. As mentioned in Section SECREF3, the labels used for the classification experiments are the 5 sentences with highest ROUGE-SU4 F1 score. Macquarie University's participation in BioASQ 7 focused on the task of generating the ideal answers. The runs use query-based extractive techniques and we experiment with classification, regression, and reinforcement learning approaches. At the time of writing there were no human evaluation results,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what amounts of size were used on german-english?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "0b8b67d2-016d-4abd-917c-89566cd0c1e6",
        "metadata": {
          "vector_store_key": "1905.11901-2",
          "chunk_id": 9,
          "document_id": "1905.11901",
          "start_idx": 3269,
          "end_idx": 3706
        },
        "page_content": " dropout BIBREF27 , word dropout BIBREF28 , layer normalization BIBREF29 and tied embeddings BIBREF30 . Subword representations such as BPE BIBREF31 have become a popular choice to achieve open-vocabulary translation. BPE has one hyperparameter, the number of merge operations, which determines the size of the final vocabulary. For high-resource settings, the effect of vocabulary size on translation quality is relatively small; BIBREF",
        "type": "Document"
      },
      {
        "id": "7182a7ff-5f40-47b2-8a55-8bbfdda0be11",
        "metadata": {
          "vector_store_key": "1905.11901-2",
          "chunk_id": 10,
          "document_id": "1905.11901",
          "start_idx": 3534,
          "end_idx": 4016
        },
        "page_content": " high-resource settings, the effect of vocabulary size on translation quality is relatively small; BIBREF32 report mixed results when comparing vocabularies of 30k and 90k subwords. In low-resource settings, large vocabularies result in low-frequency (sub)words being represented as atomic units at training time, and the ability to learn good high-dimensional representations of these is doubtful. BIBREF33 propose a minimum frequency threshold for subword units, and splitting any",
        "type": "Document"
      },
      {
        "id": "f8cf2cb1-3e66-48cd-95c5-2c39cdf76b33",
        "metadata": {
          "vector_store_key": "1902.00330-8",
          "chunk_id": 72,
          "document_id": "1902.00330",
          "start_idx": 22937,
          "end_idx": 23326
        },
        "page_content": " We set the dimensions of word embedding and entity embedding to 300, where the word embedding and entity embedding are released by BIBREF20 and BIBREF0 respectively. For parameters of the local LSTM network, the number of LSTM cell units is set to 512, the batch size is 64, and the rank margin $\\gamma $ is 0.1. Similarly, in global LSTM network, the number of LSTM cell units is 700 and",
        "type": "Document"
      },
      {
        "id": "b0d78a46-0cfa-4367-a963-2a9a8d82c9a4",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 18,
          "document_id": "1910.04269",
          "start_idx": 6853,
          "end_idx": 7342
        },
        "page_content": " comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (",
        "type": "Document"
      },
      {
        "id": "635e5e6c-9cc3-4b1a-9a80-ed3f4fc00b6f",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 17,
          "document_id": "1910.04269",
          "start_idx": 6552,
          "end_idx": 7071
        },
        "page_content": " for feature extraction. An elaborate analysis of how deep architectures outperform their shallow counterparts is presented in this reseacrch. The difficulties in classifying perceptually similar languages like German and English were also put forward in this work. It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is the dataset?",
    "llm_answer": "198,112 news articles\n",
    "context": [
      {
        "id": "0723454a-4062-463e-972a-81a58c6a504e",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "06e9a4b3-9db0-4e73-892a-9b9a4c094d48",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 27,
          "document_id": "1909.00694",
          "start_idx": 8898,
          "end_idx": 9330
        },
        "page_content": " CO such that it was five times larger than AL. The results are shown in Table TABREF16. We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively: . \u4f5c\u696d\u304c\u697d\u3060\u3002 The work",
        "type": "Document"
      },
      {
        "id": "ea2d0819-bb3b-4b92-b6e3-a01f571f29f1",
        "metadata": {
          "vector_store_key": "1809.04960-8",
          "chunk_id": 35,
          "document_id": "1809.04960",
          "start_idx": 13972,
          "end_idx": 14405
        },
        "page_content": " the content of the article, and a list of the users' comments. Following the previous work BIBREF0 , we tokenize all text with the popular python package Jieba, and filter out short articles with less than 30 words in content and those with less than 20 comments. The dataset is split into training/validation/test sets, and they contain 191,502/5,000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1",
        "type": "Document"
      },
      {
        "id": "dbacfe88-aa7f-443c-9f7c-f1cb22a31e5b",
        "metadata": {
          "vector_store_key": "1908.08419-2",
          "chunk_id": 53,
          "document_id": "1908.08419",
          "start_idx": 19170,
          "end_idx": 19642
        },
        "page_content": " from 3868 records as unlabeled set. Then, we manually annotate remaining 668 records as labeled set, which contains 1170 sentences. Finally, we divide labeled set into train set and test set with the ratio of 7:3 randomly. Statistics of datasets are listed in Table TABREF33 . To determine suitable parameters, we divide training set into two sets, the first 80% sentences as training set and the rest 20% sentences as validation set. Character embedding dimensions and K",
        "type": "Document"
      },
      {
        "id": "d97120a1-c318-4d6c-89f7-4bb4ee280a24",
        "metadata": {
          "vector_store_key": "1809.04960-8",
          "chunk_id": 34,
          "document_id": "1809.04960",
          "start_idx": 13507,
          "end_idx": 13972
        },
        "page_content": " and paired data INLINEFORM4 . We select a large-scale Chinese dataset BIBREF0 with millions of real comments and a human-annotated test set to evaluate our model. The dataset is collected from Tencent News, which is one of the most popular Chinese websites for news and opinion articles. The dataset consists of 198,112 news articles. Each piece of news contains a title, the content of the article, and a list of the users' comments. Following the previous work B",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What MC abbreviate for?",
    "llm_answer": "The context does not contain an abbreviation for MC.\n",
    "context": [
      {
        "id": "7d4a3b39-7f63-4583-abd9-ae2329e1e599",
        "metadata": {
          "vector_store_key": "1910.14497-3",
          "chunk_id": 59,
          "document_id": "1910.14497",
          "start_idx": 17746,
          "end_idx": 18046
        },
        "page_content": "salary\", \"office\", \"business\", \"career\", \"industry\", \"company\", \"promotion\", \"profession\", \"CEO\", \"manager\", \"coworker\", \"entrepreneur\" B: \"home\", \"parents\", \"children\", \"family\", \"cousins\", \"marriage\", \"wedding\", \"relatives\", \"grandparents\", \"grandchildren\", \"nurture\", \"child\", \"toddler\", \"infant\",",
        "type": "Document"
      },
      {
        "id": "a93757c2-98bf-4e87-ba8a-ab8ef041dcc8",
        "metadata": {
          "vector_store_key": "1910.14497-3",
          "chunk_id": 57,
          "document_id": "1910.14497",
          "start_idx": 17282,
          "end_idx": 17564
        },
        "page_content": "tten\", \"vomit\", \"agony\", \"prison\" Male vs Female / Career vs Family: X: \"brother\", \"father\", \"uncle\", \"grandfather\", \"son\", \"he\", \"his\", \"him\", \"man\", \"himself\", \"men\", \"husband\", \"boy\", \"uncle\", \"nephew\", \"boyfriend\", \"king\", \"actor\" Y: \"sister\", \"mother\", \"aunt\", \"grandmother\", \"",
        "type": "Document"
      },
      {
        "id": "4eff95a7-1eae-447f-84e8-c33ac2b23592",
        "metadata": {
          "vector_store_key": "1910.14497-3",
          "chunk_id": 34,
          "document_id": "1910.14497",
          "start_idx": 12004,
          "end_idx": 12213
        },
        "page_content": "\ufffd, \u2019pregnancy\u2019, \u2019eva\u2019, \u2019pageant\u2019, \u2019distress\u2019, \u2019cristina\u2019, \u2019ida\u2019, \u2019beauty\u2019, \u2019sexuality\u2019,\u2019fertility\u2019 'accountant', 'acquaintance', 'actor', 'actress', 'administrator', 'adventurer', 'advocate', 'aide', 'alderman",
        "type": "Document"
      },
      {
        "id": "b8127f4b-2a3c-41a9-b3c0-ec04a2694595",
        "metadata": {
          "vector_store_key": "1910.14497-3",
          "chunk_id": 65,
          "document_id": "1910.14497",
          "start_idx": 19137,
          "end_idx": 19387
        },
        "page_content": "\", \"son\", \"he\", \"his\", \"him\", \"man\", \"himself\", \"men\", \"husband\", \"boy\", \"uncle\", \"nephew\", \"boyfriend\" B: \"sister\", \"mother\", \"aunt\", \"grandmother\", \"daughter\", \"she\", \"hers\", \"her\", \"woman\", \"herself\", \"women\", \"wife\", \"aunt\", \"niece\", \"girlfriend\"",
        "type": "Document"
      },
      {
        "id": "ca5cbaa1-28fd-4d00-b08f-698146992d59",
        "metadata": {
          "vector_store_key": "1910.14497-3",
          "chunk_id": 58,
          "document_id": "1910.14497",
          "start_idx": 17564,
          "end_idx": 17865
        },
        "page_content": " \"actor\" Y: \"sister\", \"mother\", \"aunt\", \"grandmother\", \"daughter\", \"she\", \"hers\", \"her\", \"woman\", \"herself\", \"women\", \"wife\", \"aunt\", \"niece\", \"girlfriend\", \"queen\", \"actress\" A: \"executive\", \"management\", \"professional\", \"corporation\", \"salary\", \"office\", \"business\", \"career\", \"industry\", \"company\",",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are their correlation results?",
    "llm_answer": "ROUGE-2 and ROUGE-SU4 have similar correlation values, with ROUGE-SU4 being slightly better.  Precision and F1 are similar, but precision has better correlation. Recall shows poor correlation, and virtually no correlation using the revised Kendall measure.  Pearson correlation for a single reference increases to 0.048.  WordSLOR and WPSLOR have the lowest MSE, with WordSLOR having a slightly lower error than WPSLOR.\n",
    "context": [
      {
        "id": "037ce8ed-7a08-4079-8740-3214e76425f7",
        "metadata": {
          "vector_store_key": "1809.08731-6",
          "chunk_id": 50,
          "document_id": "1809.08731",
          "start_idx": 16548,
          "end_idx": 16964
        },
        "page_content": " correlations for all models but different domains (cf. Table 5 ), we first observe that the results across domains are similar, i.e., we do not observe the same effect as in Subsection \"Analysis I: Fluency Evaluation per Compression System\" . This is due to the distributions of scores being uniform ( $\\text{Var}(Y) \\in [0.28, 0.36]$ ). Next, we focus on an important question: How much does the performance of our",
        "type": "Document"
      },
      {
        "id": "02e257dd-d13a-4643-a6d4-986febc10750",
        "metadata": {
          "vector_store_key": "1909.00542-3",
          "chunk_id": 36,
          "document_id": "1909.00542",
          "start_idx": 13913,
          "end_idx": 14353
        },
        "page_content": "PLAY_FORM37, where $S$ is the list of different sets. Table TABREF38 shows the results of all correlation metrics. Overall, ROUGE-2 and ROUGE-SU4 give similar correlation values but ROUGE-SU4 is marginally better. Among precision, recall and F1, both precision and F1 are similar, but precision gives a better correlation. Recall shows poor correlation, and virtually no correlation when using the revised Kendall measure. For reporting the",
        "type": "Document"
      },
      {
        "id": "981f6a43-9af8-4010-9707-3e94e8ff4197",
        "metadata": {
          "vector_store_key": "1809.08731-0",
          "chunk_id": 39,
          "document_id": "1809.08731",
          "start_idx": 13017,
          "end_idx": 13402
        },
        "page_content": "OR increases to $0.048$ for Pearson correlation. Note that, since having a single reference is very common, this result is highly relevant for practical applications. Considering MSE, the top two metrics are still WordSLOR and WPSLOR, with a $0.008$ and, respectively, $0.002$ lower error than the third best metric, ROUGE-L-mult.  Comparing WordSLOR and WPSLOR, we find no significant",
        "type": "Document"
      },
      {
        "id": "1d002ad8-9494-4666-a1d5-98570a3691d3",
        "metadata": {
          "vector_store_key": "1909.00542-3",
          "chunk_id": 33,
          "document_id": "1909.00542",
          "start_idx": 12727,
          "end_idx": 13230
        },
        "page_content": ", F1 for ROUGE-2 and ROUGE-SU4) against the average of the human scores. The correlation metrics are Pearson, Kendall, and a revised Kendall correlation explained below. The Pearson correlation between two variables is computed as the covariance of the two variables divided by the product of their standard deviations. This correlation is a good indication of a linear relation between the two variables, but may not be very effective when there is non-linear correlation. The Spearman rank correlation",
        "type": "Document"
      },
      {
        "id": "a88a551c-ea8c-49f6-b5b6-47930084b730",
        "metadata": {
          "vector_store_key": "1809.08731-6",
          "chunk_id": 45,
          "document_id": "1809.08731",
          "start_idx": 14995,
          "end_idx": 15436
        },
        "page_content": "GE-LM\u2014an uninformed combination\u2014performs significantly better than both ROUGE-L-mult and WPSLOR on their own, it should be the metric of choice for evaluating fluency with given references. The results per compression system (cf. Table 4 ) look different from the correlations in Table 3 : Pearson and MSE are both lower. This is due to the outputs of each given system being of comparable quality. Therefore, the datapoints are similar and,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What simpler models do they look at?",
    "llm_answer": "Distilled BERT-based models, smaller general-purpose language representation models pre-trained by leveraging distillation knowledge.\n",
    "context": [
      {
        "id": "1cc830ac-f4e2-46d9-9e7d-f8704a5cf375",
        "metadata": {
          "vector_store_key": "1703.04617-0",
          "chunk_id": 9,
          "document_id": "1703.04617",
          "start_idx": 3555,
          "end_idx": 4085
        },
        "page_content": " has the potential to capture long-distance interactions over the structures. Different types of questions are often used to seek for different types of information. For example, a \"what\" question could have very different property from that of a \"why\" question, while they may share information and need to be trained together instead of separately. We view this as a \"adaptation\" problem to let different types of questions share a basic model but still discriminate them when needed. Specifically, we are motivated by the ideas",
        "type": "Document"
      },
      {
        "id": "1be8a42e-6463-4f55-9d8d-8c3ca77e7503",
        "metadata": {
          "vector_store_key": "1811.00942-0",
          "chunk_id": 8,
          "document_id": "1811.00942",
          "start_idx": 3335,
          "end_idx": 3833
        },
        "page_content": " however, their focus is not on the computational footprint of each model, but rather the perplexity. To further reduce perplexity, many neural language model extensions exist, such as continuous cache pointer BIBREF5 and mixture of softmaxes BIBREF6 . Since our focus is on comparing \u201ccore\u201d neural and non-neural approaches, we disregard these extra optimizations techniques in all of our models. Other work focus on designing lightweight models for resource-efficient inference on mobile devices.",
        "type": "Document"
      },
      {
        "id": "7f3acdc0-23e0-47b0-bb2c-d9cc36085ded",
        "metadata": {
          "vector_store_key": "1905.00563-8",
          "chunk_id": 57,
          "document_id": "1905.00563",
          "start_idx": 17255,
          "end_idx": 17740
        },
        "page_content": "-10 models are more robust than those for WN18. Looking at sample attacks (provided in Appendix \"Sample Adversarial Attacks\" ), mostly tries to change the type of the target object by associating it with a subject and a relation for a different entity type. Uncertain-Test To better understand the effect of attacks, we consider a subset of test triples that 1) the model predicts correctly, 2) difference between their scores and the negative sample with the highest score is minimum.",
        "type": "Document"
      },
      {
        "id": "efafe945-13f8-4908-87d2-a8b7daf2106c",
        "metadata": {
          "vector_store_key": "2002.06644-0",
          "chunk_id": 11,
          "document_id": "2002.06644",
          "start_idx": 4295,
          "end_idx": 4797
        },
        "page_content": "QuAD for Question Answering. Distilled BERT-based models: Secondly, we propose to use distilled BERT-based models, as introduced in BIBREF8. They are smaller general-purpose language representation model, pre-trained by leveraging distillation knowledge. This results in significantly smaller and faster models with performance comparable to their undistilled versions. We finetune these pretrained distilled models on the training corpus to efficiently detect subjectivity. BERT-based ensemble models:",
        "type": "Document"
      },
      {
        "id": "e115f3ad-cfb7-4243-b123-51a669484bbc",
        "metadata": {
          "vector_store_key": "1703.04617-0",
          "chunk_id": 55,
          "document_id": "1703.04617",
          "start_idx": 14584,
          "end_idx": 15140
        },
        "page_content": "ity. The previous models are often trained for all questions without explicitly discriminating different question types; however, for a target question, both the common features shared by all questions and the specific features for a specific type of question are further considered in this paper, as they could potentially obey different distributions. In this paper we further explicitly model different types of questions in the end-to-end training. We start from a simple way to first analyze the word frequency of all questions, and obtain top-10 most",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What linguistic quality aspects are addressed?",
    "llm_answer": "Fluency.\n",
    "context": [
      {
        "id": "075245f4-821b-4e4d-af5b-0ed66cb9441f",
        "metadata": {
          "vector_store_key": "1909.00578-1",
          "chunk_id": 2,
          "document_id": "1909.00578",
          "start_idx": 710,
          "end_idx": 1242
        },
        "page_content": " use a pre-trained BERT model adding just a task-specific layer, and fine-tune the entire model on the task of predicting linguistic quality scores manually assigned to summaries. The five criteria addressed are given in Figure FIGREF2. We provide a thorough evaluation on three publicly available summarization datasets from NIST shared tasks, and compare the performance of our model to a wide variety of baseline methods capturing different aspects of linguistic quality. Sum-QE achieves very high correlations with human ratings",
        "type": "Document"
      },
      {
        "id": "0aa4db32-cbac-4ffa-81bd-765ef03bfb78",
        "metadata": {
          "vector_store_key": "1809.08731-4",
          "chunk_id": 2,
          "document_id": "1809.08731",
          "start_idx": 812,
          "end_idx": 1267
        },
        "page_content": " evaluation, especially of fluency, causing word-overlap metrics like ROUGE BIBREF1 to correlate only weakly with human judgments BIBREF2 . As a result, fluency evaluation of NLG is often done manually, which is costly and time-consuming. Evaluating sentences on their fluency, on the other hand, is a linguistic ability of humans which has been the subject of a decade-long debate in cognitive science. In particular, the question has been raised whether",
        "type": "Document"
      },
      {
        "id": "09e500d0-9187-4b83-adea-dfdac744410a",
        "metadata": {
          "vector_store_key": "1809.08731-4",
          "chunk_id": 0,
          "document_id": "1809.08731",
          "start_idx": 0,
          "end_idx": 505
        },
        "page_content": "Producing sentences which are perceived as natural by a human addressee\u2014a property which we will denote as fluency throughout this paper \u2014is a crucial goal of all natural language generation (NLG) systems: it makes interactions more natural, avoids misunderstandings and, overall, leads to higher user satisfaction and user trust BIBREF0 . Thus, fluency evaluation is important, e.g., during system development, or for filtering unacceptable generations at application time. However, fluency evaluation of",
        "type": "Document"
      },
      {
        "id": "1d58a6eb-ffc4-483f-86f7-8346434717f3",
        "metadata": {
          "vector_store_key": "1809.08731-4",
          "chunk_id": 13,
          "document_id": "1809.08731",
          "start_idx": 5000,
          "end_idx": 5512
        },
        "page_content": " data they had created by translating and back-translating sentences with neural models. While they tried different types of LMs, best results were obtained for neural models, namely recurrent neural networks (RNNs). In this work, we investigate if approaches which have proven successful for modeling acceptability can be applied to the NLP problem of automatic fluency evaluation. In this section, we first describe SLOR and the intuition behind this score. Then, we introduce WordPieces, before explaining how",
        "type": "Document"
      },
      {
        "id": "819919e6-ce03-4836-aea8-ab4799d9c876",
        "metadata": {
          "vector_store_key": "1809.08731-4",
          "chunk_id": 61,
          "document_id": "1809.08731",
          "start_idx": 20536,
          "end_idx": 21023
        },
        "page_content": " fluency evaluation in NLP has been rare. heilman2014predicting predicted the fluency (which they called grammaticality) of sentences written by English language learners. In contrast to ours, their approach is supervised. stent2005evaluating and cahill2009correlating found only low correlation between automatic metrics and fluency ratings for system-generated English paraphrases and the output of a German surface realiser, respectively. Explicit fluency evaluation of NLG, including",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What dataset do they use?",
    "llm_answer": "AIDA-Train, Wikipedia, FigureEight, ISEAR, Twitter Emotion Corpus, MCTest, CNN/Daily Mail, CBT.\n",
    "context": [
      {
        "id": "1f337671-9234-400e-b57d-968d7d0216e0",
        "metadata": {
          "vector_store_key": "1902.00330-6",
          "chunk_id": 65,
          "document_id": "1902.00330",
          "start_idx": 20691,
          "end_idx": 21180
        },
        "page_content": " AIDA-Train and Wikipedia data in the training set. Furthermore, we compare the RLEL with some baseline methods, where our model achieves the state-of-the-art results. We implement our models in Tensorflow and run experiments on 4 Tesla V100 GPU. We conduct experiments on several different types of public datasets including news and encyclopedia corpus. The training set is AIDA-Train and Wikipedia datasets, where AIDA-Train contains 18448 mentions and Wikipedia contains 25995 mentions",
        "type": "Document"
      },
      {
        "id": "d4d1ac88-d4d1-46ca-89b4-a8de2fc908db",
        "metadata": {
          "vector_store_key": "1909.02764-8",
          "chunk_id": 44,
          "document_id": "1909.02764",
          "start_idx": 17673,
          "end_idx": 18041
        },
        "page_content": " We train models on a variety of corpora, namely the common format published by BIBREF27 of the FigureEight (formally known as Crowdflower) data set of social media, the ISEAR data BIBREF40 (self-reported emotional events), and, the Twitter Emotion Corpus (TEC, weakly annotated Tweets with #anger, #disgust, #fear, #happy, #sadness, and #surprise, Mohammad2012). From",
        "type": "Document"
      },
      {
        "id": "4cac7588-28e9-4b6e-ac0d-d763db64bc58",
        "metadata": {
          "vector_store_key": "1703.04617-6",
          "chunk_id": 3,
          "document_id": "1703.04617",
          "start_idx": 1343,
          "end_idx": 1868
        },
        "page_content": " can help attain better results on our competitive baselines. Recent advance on reading comprehension and question answering has been closely associated with the availability of various datasets. BIBREF0 released the MCTest data consisting of 500 short, fictional open-domain stories and 2000 questions. The CNN/Daily Mail dataset BIBREF1 contains news articles for close style machine comprehension, in which only entities are removed and tested for comprehension. Children's Book Test (CBT) BIBREF2 leverages named entities",
        "type": "Document"
      },
      {
        "id": "01441853-36b1-4322-a8b7-0bf4ed1aeec9",
        "metadata": {
          "vector_store_key": "1909.00542-2",
          "chunk_id": 6,
          "document_id": "1909.00542",
          "start_idx": 2427,
          "end_idx": 2993
        },
        "page_content": " relevant concepts from designated terminologies and ontologies, relevant articles from PubMed, relevant snippets from the relevant articles, and relevant RDF triples from designated ontologies. Phase B provides a biomedical question and a list of relevant articles and snippets, and participant systems are expected to return the exact answers and the ideal answers. The training data is composed of the test data from all previous years, and amounts to 2,747 samples. There has been considerable research on the use of machine learning approaches for tasks related",
        "type": "Document"
      },
      {
        "id": "77a2c284-62f0-43e0-8bf9-52a2d068ed04",
        "metadata": {
          "vector_store_key": "2003.07433-1",
          "chunk_id": 49,
          "document_id": "2003.07433",
          "start_idx": 19311,
          "end_idx": 19770
        },
        "page_content": " well as dimensions' text analysis scores. We chose three categories based on the three different surveys: 1) DOSPERT scale; 2) BSSS scale; and 3) VIAS scale. Dimension selection: We have to define the word sets (also called dimension) for each category. We chose one dimension for each of the questions under each category to reflect real survey system evaluation. Our chosen categories are state in Fig FIGREF20. Score calculation $\\alpha $-score: $\\alpha $",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is typical GAN architecture for each text-to-image synhesis group?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "3ffc6de5-ba11-4ec0-a869-93400f560e8d",
        "metadata": {
          "vector_store_key": "1910.09399-0",
          "chunk_id": 14,
          "document_id": "1910.09399",
          "start_idx": 5638,
          "end_idx": 6156
        },
        "page_content": ", as they are the engines that make text-to-image synthesis possible and are essential building blocks to achieve photo-realistic images from text descriptions. Section 4 proposes a taxonomy to summarize GAN based text-to-image synthesis, discusses models and architectures of novel works focused solely on text-to-image synthesis. This section will also draw key contributions from these works in relation to their applications. Section 5 reviews GAN based text-to-image synthesis benchmarks, performance metrics, and",
        "type": "Document"
      },
      {
        "id": "9d050f9e-ddcf-4c2f-a6a0-217984e86f87",
        "metadata": {
          "vector_store_key": "1910.09399-0",
          "chunk_id": 62,
          "document_id": "1910.09399",
          "start_idx": 24164,
          "end_idx": 24615
        },
        "page_content": ". The fourth user group adds a new dimension in image synthesis, and aims to generate sequences of images which are coherent in temporal order, i.e. capture the motion information. black Based on the above descriptions, we categorize GAN based Text-to-Image Synthesis into a taxonomy with four major categories, as shown in Fig. FIGREF24. Semantic Enhancement GANs: Semantic enhancement GANs represent pioneer works of GAN frameworks for text-to-image",
        "type": "Document"
      },
      {
        "id": "511408ef-7fdf-4ab1-8bbe-6f4fa86b5609",
        "metadata": {
          "vector_store_key": "1910.09399-0",
          "chunk_id": 56,
          "document_id": "1910.09399",
          "start_idx": 21820,
          "end_idx": 22241
        },
        "page_content": ", but in this paper we focus specifically on models for text-to-image synthesis, with the review emphasizing on the \u201cmodel\u201d and \u201ccontributions\u201d for text-to-image synthesis. At the end of this section, we also briefly review methods using GANs for other image-synthesis applications. black In this section, we propose a taxonomy to summarize advanced GAN based text-to-image synthesis frameworks, as shown in Figure FIGREF",
        "type": "Document"
      },
      {
        "id": "a2463ef8-76a0-40f2-a1c7-c9f062a77354",
        "metadata": {
          "vector_store_key": "1910.09399-0",
          "chunk_id": 165,
          "document_id": "1910.09399",
          "start_idx": 64023,
          "end_idx": 64502
        },
        "page_content": " paper, we reviewed novel methods that generate, in our opinion, the most visually-rich and photo-realistic images, from text-based natural language. These generated images often rely on generative adversarial networks (GANs), deep convolutional decoder networks, and multimodal learning methods. blackIn the paper, we first proposed a taxonomy to organize GAN based text-to-image synthesis frameworks into four major groups: semantic enhancement GANs, resolution enhancement GAN",
        "type": "Document"
      },
      {
        "id": "c79b26c6-5bb8-4ab0-84e0-ae1848d90e44",
        "metadata": {
          "vector_store_key": "1910.09399-0",
          "chunk_id": 27,
          "document_id": "1910.09399",
          "start_idx": 10880,
          "end_idx": 11367
        },
        "page_content": "), which is the building block for many GAN based text-to-image synthesis models. After that, we briefly separate GAN based text-to-image synthesis into two types, Simple GAN frameworks vs. Advanced GAN frameworks, and discuss why advanced GAN architecture for image synthesis. black Notice that the simple vs. advanced GAN framework separation is rather too brief, our taxonomy in the next section will propose a taxonomy to summarize advanced GAN frameworks into four categories, based",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What language do the agents talk in?",
    "llm_answer": "Natural language.\n",
    "context": [
      {
        "id": "feb6e6dd-d8e4-4e84-b7e2-826b4e0bbc23",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 8,
          "document_id": "1909.02764",
          "start_idx": 3194,
          "end_idx": 3734
        },
        "page_content": " speech interactions, the study was conducted in a driving simulator. Based on the collected data, we provide baseline predictions with off-the-shelf tools for face and speech emotion recognition and compare them to a neural network-based approach for emotion recognition from text. Our second contribution is the introduction of transfer learning to adapt models trained on established out-of-domain corpora to our use case. We work on German language, therefore the transfer consists of a domain and a language transfer. A common approach",
        "type": "Document"
      },
      {
        "id": "955cdc97-03de-4618-9fef-3b143b2cc227",
        "metadata": {
          "vector_store_key": "1807.03367-3",
          "chunk_id": 4,
          "document_id": "1807.03367",
          "start_idx": 1550,
          "end_idx": 2025
        },
        "page_content": " humans. Hence, we argue that we should incorporate all three fundamental aspects of human language acquisition\u2014perception, action and interactive communication\u2014and develop a task and dataset to that effect. We introduce the Talk the Walk dataset, where the aim is for two agents, a \u201cguide\u201d and a \u201ctourist\u201d, to interact with each other via natural language in order to achieve a common goal: having the tourist navigate towards the correct location. The guide has access to a",
        "type": "Document"
      },
      {
        "id": "e90f75a5-f326-4202-82c5-878736af8308",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 51,
          "document_id": "1910.04269",
          "start_idx": 19553,
          "end_idx": 20023
        },
        "page_content": ", the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Kat",
        "type": "Document"
      },
      {
        "id": "b0d78a46-0cfa-4367-a963-2a9a8d82c9a4",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 18,
          "document_id": "1910.04269",
          "start_idx": 6853,
          "end_idx": 7342
        },
        "page_content": " comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (",
        "type": "Document"
      },
      {
        "id": "c34712f4-5b61-44e3-af97-4ec1f326646a",
        "metadata": {
          "vector_store_key": "1910.04269-2",
          "chunk_id": 0,
          "document_id": "1910.04269",
          "start_idx": 0,
          "end_idx": 512
        },
        "page_content": "Language Identification (LI) is a problem which involves classifying the language being spoken by a speaker. LI systems can be used in call centers to route international calls to an operator who is fluent in that identified language BIBREF0. In speech-based assistants, LI acts as the first step which chooses the corresponding grammar from a list of available languages for its further semantic analysis BIBREF1. It can also be used in multi-lingual voice-controlled information retrieval systems, for example,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much better is performance of proposed method than state-of-the-art methods in experiments?",
    "llm_answer": "The context states that the proposed method outperforms Coppersmith et al.'s method, and that BERT versions outperform other methods.  It also shows that multi-task versions of BERT often perform better than single-task versions. However, it does not provide a specific quantitative comparison of the proposed method's performance against all state-of-the-art methods.  Thus, the answer is not found in the context.\n",
    "context": [
      {
        "id": "0c3dc473-e6e5-400c-b3a1-75c887dbe5e3",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 28,
          "document_id": "1909.00542",
          "start_idx": 10682,
          "end_idx": 11217
        },
        "page_content": " experiments. We ran the version trained on Python ROUGE once, and the version trained on Perl twice. The two Perl runs have different results, and one of them clearly outperforms the Python run. However, given the differences of results between the two Perl runs we advice to re-run the experiments multiple times and obtain the mean and standard deviation of the runs before concluding whether there is any statistical difference between the results. But it seems that there may be an improvement of the final evaluation results when",
        "type": "Document"
      },
      {
        "id": "7a3763d4-793b-483d-9bf2-cdefe870cd27",
        "metadata": {
          "vector_store_key": "2003.07433-0",
          "chunk_id": 66,
          "document_id": "2003.07433",
          "start_idx": 25731,
          "end_idx": 26161
        },
        "page_content": "REF28 illustrates the comparisons between LAXARY and Coppersmith et. al. proposed method. Here we can see, the outperformance of our proposed method as well as the importance of $s-score$ estimation. We also illustrates the importance of $\\alpha -score$ and $S-score$ in Fig FIGREF30. Fig FIGREF30 illustrates that if we change the number of training samples (%), LAXARY models outperforms Coppersmith et. al. proposed model under",
        "type": "Document"
      },
      {
        "id": "fc85b849-8b62-410f-8f70-2a1182aa13bc",
        "metadata": {
          "vector_store_key": "1910.04269-4",
          "chunk_id": 57,
          "document_id": "1910.04269",
          "start_idx": 21961,
          "end_idx": 22224
        },
        "page_content": " which is a recently developed technique to prevent over\ufb01tting on test data.This approach achieved an accuracy of 95.4%. We also analysed how attention mechanism and recurrent layers impact the performance of networks. This approach achieved an accuracy of 95.0%.",
        "type": "Document"
      },
      {
        "id": "01e2318c-01c0-4835-9235-b8d871aa205b",
        "metadata": {
          "vector_store_key": "1811.00942-0",
          "chunk_id": 42,
          "document_id": "1811.00942",
          "start_idx": 13604,
          "end_idx": 14041
        },
        "page_content": " for the task of language modeling. Specifically, we explore the quality\u2013performance tradeoffs between KN-5, a non-neural approach, and AWD-LSTM and QRNN, two neural language models. We find that with decreased perplexity comes vastly increased computational requirements: In one of the NLMs, a perplexity reduction by 2.5 $\\times $ results in a 49 $\\times $ rise in latency and 32 $\\times $ increase in energy usage, when compared to KN",
        "type": "Document"
      },
      {
        "id": "82ee60ef-c81c-4369-8134-d20ea8aa6aef",
        "metadata": {
          "vector_store_key": "1909.00578-2",
          "chunk_id": 27,
          "document_id": "1909.00578",
          "start_idx": 8968,
          "end_idx": 9354
        },
        "page_content": " the training set of each fold. Table TABREF23 shows Spearman's $\\rho $, Kendall's $\\tau $ and Pearson's $r$ for all datasets and models. The three fine-tuned BERT versions clearly outperform all other methods. Multi-task versions seem to perform better than single-task ones in most cases. Especially for $\\mathcal {Q}4$ and $\\mathcal {Q}5$, which are highly correlated, the multi-task",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What baseline model is used?",
    "llm_answer": "A baseline model composed of word embedding, input encoder, alignment, aggregation, and prediction, with dropout rate of 0.3, Adam optimizer with a learning rate of 10^-5, and batch normalization.\n",
    "context": [
      {
        "id": "bc8ac242-0b5e-4c71-be0b-48d167015fad",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 43,
          "document_id": "1909.02764",
          "start_idx": 17421,
          "end_idx": 17831
        },
        "page_content": " layer. This setup is inspired by BIBREF38. We use a dropout rate of 0.3 in all layers and optimize with Adam BIBREF39 with a learning rate of $10^{-5}$ (These parameters are the same for all further experiments). We build on top of the Keras library with the TensorFlow backend. We consider this setup our baseline model. We train models on a variety of corpora, namely the common format published by BIBREF27",
        "type": "Document"
      },
      {
        "id": "05f513f6-29b2-46d8-8ba4-af84d6b0b34d",
        "metadata": {
          "vector_store_key": "1703.04617-0",
          "chunk_id": 10,
          "document_id": "1703.04617",
          "start_idx": 3849,
          "end_idx": 4374
        },
        "page_content": " questions share a basic model but still discriminate them when needed. Specifically, we are motivated by the ideas \"i-vector\" BIBREF18 in speech recognition, where neural network based adaptation is performed among different (groups) of speakers and we focused instead on different types of questions here. Our baseline model is composed of the following typical components: word embedding, input encoder, alignment, aggregation, and prediction. Below we discuss these components in more details. We concatenate embedding at",
        "type": "Document"
      },
      {
        "id": "ace147d0-5bf0-4495-a2ea-94bcd97f86ed",
        "metadata": {
          "vector_store_key": "1809.04960-1",
          "chunk_id": 38,
          "document_id": "1809.04960",
          "start_idx": 14887,
          "end_idx": 15333
        },
        "page_content": " and use batch normalization BIBREF9 in each layer. We also gradually increase the KL term from 0 to 1 after each epoch. We compare our model with several unsupervised models and supervised models. Unsupervised baseline models are as follows: TF-IDF (Lexical, Non-Neural) is an important unsupervised baseline. We use the concatenation of the title and the body as the query to retrieve the candidate comment set by means of the similarity of the",
        "type": "Document"
      },
      {
        "id": "b0acb005-01ea-424f-b249-be4013d0e3eb",
        "metadata": {
          "vector_store_key": "1910.04269-6",
          "chunk_id": 34,
          "document_id": "1910.04269",
          "start_idx": 12971,
          "end_idx": 13505
        },
        "page_content": " its hyperparameter. We took some specific design choices for this model, which are as follows: We added residual connections with each convolutional layer. Residual connections in a way makes the model selective of the contributing layers, determines the optimal number of layers required for training and solves the problem of vanishing gradients. Residual connections or skip connections skip training of those layers that do not contribute much in the overall outcome of model. We added spatial attention BIBREF27 networks to help",
        "type": "Document"
      },
      {
        "id": "ed8aaf94-c5e8-4cf0-9b3c-4a4e591733f5",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 39,
          "document_id": "1909.00542",
          "start_idx": 15059,
          "end_idx": 15516
        },
        "page_content": " the results and details of the runs submitted to BioASQ. The table uses ROUGE-SU4 Recall since this is the metric available at the time of writing this paper. However, note that, as explained in Section SECREF6, these results might differ from the final human evaluation results. Therefore we do not comment on the results, other than observing that the \u201cfirst $n$\u201d baseline produces the same results as the neural regressor. As mentioned in Section SECREF",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What stylistic features are used to detect drunk texts?",
    "llm_answer": "Capitalization, spelling errors, etc.\n",
    "context": [
      {
        "id": "f26bad51-26d4-4d55-8389-ecf9c114b5f0",
        "metadata": {
          "vector_store_key": "1610.00879-1",
          "chunk_id": 20,
          "document_id": "1610.00879",
          "start_idx": 7309,
          "end_idx": 7708
        },
        "page_content": "ifier is 78.1% for all features, and 75.6% for stylistic features. This shows that text-based clues can indeed be used for drunk-texting prediction. Using held-out dataset H, we evaluate how our system performs in comparison to humans. Three annotators, A1-A3, mark each tweet in the Dataset H as drunk or sober. Table TABREF19 shows a moderate agreement between our annotators (for example, it is 0",
        "type": "Document"
      },
      {
        "id": "dbdce599-866f-4bac-aa7d-df1fdb505717",
        "metadata": {
          "vector_store_key": "1610.00879-3",
          "chunk_id": 3,
          "document_id": "1610.00879",
          "start_idx": 1216,
          "end_idx": 1654
        },
        "page_content": " (c) most discriminative stylistic features, and (d) an error analysis that points to future work. To the best of our knowledge, this is a first study that shows the feasibility of text-based analysis for drunk-texting prediction. Past studies show the relation between alcohol abuse and unsociable behaviour such as aggression BIBREF0 , crime BIBREF1 , suicide attempts BIBREF2 , drunk driving BIBREF3 , and risky sexual behaviour BIBREF",
        "type": "Document"
      },
      {
        "id": "79af10ae-a0f1-4a9c-9815-5ddd55066059",
        "metadata": {
          "vector_store_key": "1610.00879-3",
          "chunk_id": 6,
          "document_id": "1610.00879",
          "start_idx": 2308,
          "end_idx": 2814
        },
        "page_content": " of social media text and not explicitly seeking user input. Thus, automatic drunk-texting prediction will improve systems aimed to avoid regrettable drunk-texting. To the best of our knowledge, ours is the first study that does a quantitative analysis, in terms of prediction of the drunk state by using textual clues. Several studies have studied linguistic traits associated with emotion expression and mental health issues, suicidal nature, criminal status, etc. BIBREF5 , BIBREF6 . NLP techniques have",
        "type": "Document"
      },
      {
        "id": "0050a21c-caee-473f-bf69-07b68dc9422e",
        "metadata": {
          "vector_store_key": "1610.00879-1",
          "chunk_id": 25,
          "document_id": "1610.00879",
          "start_idx": 9169,
          "end_idx": 9660
        },
        "page_content": " for drunk-texting prediction as means of identifying risky social behavior arising out of alcohol abuse, and the need to build tools that avoid privacy leaks due to drunk-texting. We then highlight the challenges of drunk-texting prediction: one of the challenges is selection of negative examples (sober tweets). Using hashtag-based supervision, we create three datasets annotated with drunk or sober labels. We then present SVM-based classifiers which use two sets of features: N-gram and",
        "type": "Document"
      },
      {
        "id": "647b23c7-8fdc-40c0-b5ff-4ec974c14e97",
        "metadata": {
          "vector_store_key": "1610.00879-1",
          "chunk_id": 2,
          "document_id": "1610.00879",
          "start_idx": 758,
          "end_idx": 1216
        },
        "page_content": " datasets by using different strategies that are related to the use of hashtags. We then present SVM-based classifiers that use N-gram and stylistic features such as capitalisation, spelling errors, etc. Through our experiments, we make subtle points related to: (a) the performance of our features, (b) how our approach compares against human ability to detect drunk-texting, (c) most discriminative stylistic features, and (d) an error analysis that points",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the accuracy of the proposed technique?",
    "llm_answer": "95.4%\n",
    "context": [
      {
        "id": "fc85b849-8b62-410f-8f70-2a1182aa13bc",
        "metadata": {
          "vector_store_key": "1910.04269-4",
          "chunk_id": 57,
          "document_id": "1910.04269",
          "start_idx": 21961,
          "end_idx": 22224
        },
        "page_content": " which is a recently developed technique to prevent over\ufb01tting on test data.This approach achieved an accuracy of 95.4%. We also analysed how attention mechanism and recurrent layers impact the performance of networks. This approach achieved an accuracy of 95.0%.",
        "type": "Document"
      },
      {
        "id": "c7fedf32-b198-483e-b944-2f34c3b6e57f",
        "metadata": {
          "vector_store_key": "2001.00137-1",
          "chunk_id": 50,
          "document_id": "2001.00137",
          "start_idx": 18932,
          "end_idx": 19320
        },
        "page_content": " one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80$\\%$ against BERT's 72$\\%$. The second version (Corr) considers the corrected tweets, and shows higher accuracy given that it is less noisy. In that version, Stacked DeBERT achieves 82$\\%$ accuracy against BERT's 76$\\%$, an improvement of 6$\\%$. In the last case (Inc+Corr), we",
        "type": "Document"
      },
      {
        "id": "682d5cc3-d4df-4093-8a12-9c83039fe641",
        "metadata": {
          "vector_store_key": "1905.00563-4",
          "chunk_id": 48,
          "document_id": "1905.00563",
          "start_idx": 13739,
          "end_idx": 14244
        },
        "page_content": " evaluate the quality of our approximations and compare with influence function (IF), we conduct leave one out experiments. In this setup, we take all the neighbors of a random target triple as candidate modifications, remove them one at a time, retrain the model each time, and compute the exact change in the score of the target triple. We can use the magnitude of this change in score to rank the candidate triples, and compare this exact ranking with ranking as predicted by: , influence function with",
        "type": "Document"
      },
      {
        "id": "b6f57088-c4f6-4ad7-aadf-d76e7b7ddc85",
        "metadata": {
          "vector_store_key": "2003.07433-0",
          "chunk_id": 65,
          "document_id": "2003.07433",
          "start_idx": 25419,
          "end_idx": 25905
        },
        "page_content": " score of 0, 1, 2 and 3 respectively. Table TABREF29 shows the classification details of our experiment which provide the very good accuracy of our classification. To compare the outperformance of our method, we also implemented Coppersmith et. al. proposed method and achieved an 86% overall accuracy of detecting PTSD users BIBREF11 following the same training-test dataset distribution. Fig FIGREF28 illustrates the comparisons between LAXARY and Coppersmith et. al. proposed method.",
        "type": "Document"
      },
      {
        "id": "49294551-620b-4bea-a456-d8053165bced",
        "metadata": {
          "vector_store_key": "1905.00563-6",
          "chunk_id": 70,
          "document_id": "1905.00563",
          "start_idx": 21792,
          "end_idx": 22295
        },
        "page_content": " identify error triples through a search over the neighbors of the 100 facts. The result of choosing the neighbor with the least influence on the target is provided in the Table 7 . When compared with baselines that randomly choose one of the neighbors, or assume that the fact with the lowest score is incorrect, we see that outperforms both of these with a considerable gap, obtaining an accuracy of $42\\%$ and $55\\%$ in detecting errors. Learning relational knowledge representations has been a focus",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which retrieval system was used for baselines?",
    "llm_answer": "IR (information retrieval)\n",
    "context": [
      {
        "id": "ed8aaf94-c5e8-4cf0-9b3c-4a4e591733f5",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 39,
          "document_id": "1909.00542",
          "start_idx": 15059,
          "end_idx": 15516
        },
        "page_content": " the results and details of the runs submitted to BioASQ. The table uses ROUGE-SU4 Recall since this is the metric available at the time of writing this paper. However, note that, as explained in Section SECREF6, these results might differ from the final human evaluation results. Therefore we do not comment on the results, other than observing that the \u201cfirst $n$\u201d baseline produces the same results as the neural regressor. As mentioned in Section SECREF",
        "type": "Document"
      },
      {
        "id": "f189eac9-0435-44a6-90b3-1a428fecf8da",
        "metadata": {
          "vector_store_key": "1707.03904-4",
          "chunk_id": 53,
          "document_id": "1707.03904",
          "start_idx": 20086,
          "end_idx": 20524
        },
        "page_content": " \u2013 finding the few most relevant ones will allow the reader to work best. In Tables 2 and 3 we compare all baselines when the context size is tuned to maximize the overall accuracy on the validation set. For Quasar-S the best performing baseline is the BiRNN language model, which achieves $33.6\\%$ accuracy. The GA model achieves $48.3\\%$ accuracy on the set of instances for which the answer is in context, however, a search accuracy of",
        "type": "Document"
      },
      {
        "id": "77ec8f2d-42a2-40ab-9a81-f7a464e9caf9",
        "metadata": {
          "vector_store_key": "1704.05572-1",
          "chunk_id": 32,
          "document_id": "1704.05572",
          "start_idx": 10856,
          "end_idx": 11303
        },
        "page_content": " powerful information-retrieval baseline BIBREF6 that selects the answer option with the best matching sentence in a corpus. TableILP is the state-of-the-art structured inference baseline BIBREF9 developed for science questions. Table 2 shows that TupleInf, with no curated knowledge, outperforms TableILP on both question sets by more than 11%. The lower half of the table shows that even when both solvers are given the same knowledge (C+T), the",
        "type": "Document"
      },
      {
        "id": "ff466553-9428-492a-9e6d-41acb96284da",
        "metadata": {
          "vector_store_key": "1704.05572-1",
          "chunk_id": 31,
          "document_id": "1704.05572",
          "start_idx": 10656,
          "end_idx": 11029
        },
        "page_content": " the IR solver and also used to create the tuple KB T and on-the-fly tuples $T^{\\prime }_{qa}$ . Additionally, TableILP uses $\\sim $ 70 Curated tables (C) designed for 4th grade NY Regents exams. We compare TupleInf with two state-of-the-art baselines. IR is a simple yet powerful information-retrieval baseline BIBREF6 that selects the answer option with the best matching",
        "type": "Document"
      },
      {
        "id": "caf96a37-2de9-4e90-b08a-ca8a54bac59b",
        "metadata": {
          "vector_store_key": "1909.00542-2",
          "chunk_id": 40,
          "document_id": "1909.00542",
          "start_idx": 15344,
          "end_idx": 15845
        },
        "page_content": "$\u201d baseline produces the same results as the neural regressor. As mentioned in Section SECREF3, the labels used for the classification experiments are the 5 sentences with highest ROUGE-SU4 F1 score. Macquarie University's participation in BioASQ 7 focused on the task of generating the ideal answers. The runs use query-based extractive techniques and we experiment with classification, regression, and reinforcement learning approaches. At the time of writing there were no human evaluation results,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much better was the BLSTM-CNN-CRF than the BLSTM-CRF?",
    "llm_answer": "Answer not found in the context.\n",
    "context": [
      {
        "id": "f5d6daa9-1243-4073-b29b-46016020acab",
        "metadata": {
          "vector_store_key": "1911.07228-6",
          "chunk_id": 28,
          "document_id": "1911.07228",
          "start_idx": 7484,
          "end_idx": 7865
        },
        "page_content": " about 200k words Based on state-of-the-art methods for NER, BLSTM-CNN-CRF is the end-to-end deep neural network model that achieves the best result on F-score BIBREF9. Therefore, we decide to conduct the experiment on this model and analyze the errors. We run experiment with the Ma and Hovy (2016) model BIBREF8, source code provided by (Motoki Sato) and analysis the errors from",
        "type": "Document"
      },
      {
        "id": "777580e2-ab60-45cb-9dc7-41d1cfaf5a9d",
        "metadata": {
          "vector_store_key": "1911.07228-6",
          "chunk_id": 31,
          "document_id": "1911.07228",
          "start_idx": 8371,
          "end_idx": 8784
        },
        "page_content": " 3 shows perfomance of the BLSTM-CNN-CRF model. In our experiments, we use three evaluation parameters (precision, recall, and F1 score) to access our experimental result. They will be described as follow in Table 3. The \"correctNE\", the number of correct label for entity that the model can found. The \"goldNE\", number of the real label annotated by annotator in the gold data. The \"foundNE\", number of the label",
        "type": "Document"
      },
      {
        "id": "096ce7b9-7410-43d2-9ddf-95d41fcdfce8",
        "metadata": {
          "vector_store_key": "1911.07228-6",
          "chunk_id": 12,
          "document_id": "1911.07228",
          "start_idx": 2956,
          "end_idx": 3283
        },
        "page_content": "ard Hovy with BLSTM-CNN-CRF in BIBREF8 achieved F1 score of 91.21, Thai-Hoang Pham and Phuong Le-Hong with BLSTM-CNN-CRF in BIBREF9 got 88.59% F1 score. These DNN models are also the state-of-the-art models. The results of our analysis experiments are reported in precision and recall over all labels (name of person, location,",
        "type": "Document"
      },
      {
        "id": "dc53a89a-db93-47bd-9516-6005f031f336",
        "metadata": {
          "vector_store_key": "1911.07228-6",
          "chunk_id": 30,
          "document_id": "1911.07228",
          "start_idx": 8165,
          "end_idx": 8527
        },
        "page_content": " models with and without different pre-trained word embedding \u2013 KP means the Kyubyong Park\u2019s pre-trained word embeddings and EG means Edouard Grave\u2019s pre-trained word embeddings. We compare the outputs of BLSTM-CNN-CRF model (predicted) to the annotated data (gold) and analyzed the errors. Table 3 shows perfomance of the BLSTM-CNN-CRF model. In our experiments",
        "type": "Document"
      },
      {
        "id": "a24737a1-0cca-44dd-af66-1c4c45fe1397",
        "metadata": {
          "vector_store_key": "2002.06644-3",
          "chunk_id": 16,
          "document_id": "2002.06644",
          "start_idx": 6017,
          "end_idx": 6467
        },
        "page_content": " along with a recurrent dropout of $0.2$ in two 64 unit sized stacked BiLSTMs, using softmax activation layer as the final dense layer. tab:experimental-results shows the performance of different models on the WNC corpus evaluated on the following four metrics: Precision, Recall, F1, and Accuracy. Our proposed methodology, the use of finetuned optimized BERT based models, and BERT-based ensemble models outperform the baselines for all the metrics",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What supplemental tasks are used for multitask learning?",
    "llm_answer": "The supplemental tasks for multitask learning are predicting the relationships of tasks other than the main task (task C).  Specifically, the context mentions predicting the relationships of tasks not specified further.\n",
    "context": [
      {
        "id": "ee9e3449-30dd-41fb-8b70-4f92e0e3025c",
        "metadata": {
          "vector_store_key": "1603.07044-4",
          "chunk_id": 24,
          "document_id": "1603.07044",
          "start_idx": 8235,
          "end_idx": 8649
        },
        "page_content": " (e.g., the ranking score from the IR system) to enhance the classifier. So the final input to the classifier will be $h_N$ , $h^{\\prime }$ , as well as augmented features. For task C, in addition to an original question (oriQ) and an external comment (relC), the question which relC commented on is also given (relQ). To incorporate this extra information, we consider a multitask learning framework which jointly",
        "type": "Document"
      },
      {
        "id": "509ce503-e75d-4796-9f7d-f278e461c14e",
        "metadata": {
          "vector_store_key": "1603.07044-1",
          "chunk_id": 37,
          "document_id": "1603.07044",
          "start_idx": 12962,
          "end_idx": 13403
        },
        "page_content": "-External Comments\" , we also explored a multitask learning framework that jointly learns to predict the relationships of all three tasks. We set $0.8$ for the main task (task C) and $0.1$ for the other auxiliary tasks. The MAP score did not improve, but F1 increases to $0.1617$ . We believe this is because other tasks have more balanced labels, which improves the shared parameters for task C. There are many sources of external question-",
        "type": "Document"
      },
      {
        "id": "270630ad-16a4-4d78-9767-8ea17f811346",
        "metadata": {
          "vector_store_key": "1909.08859-5",
          "chunk_id": 51,
          "document_id": "1909.08859",
          "start_idx": 17614,
          "end_idx": 18113
        },
        "page_content": " We used Adam optimizer with a learning rate of 1e-4 with an early stopping criteria with the patience set to 10 indicating that the training procedure ends after 10 iterations if the performance would not improve. We considered a batch size of 32 due to our hardware constraints. In the multi-task setting, batches are sampled round-robin from all tasks, where each batch is solely composed of examples from one task. We performed our experiments on a system containing four NVIDIA GTX-1080Ti GPUs,",
        "type": "Document"
      },
      {
        "id": "15df84a6-175b-4fb6-8ff7-134a2eafb5d6",
        "metadata": {
          "vector_store_key": "1703.04617-6",
          "chunk_id": 0,
          "document_id": "1703.04617",
          "start_idx": 0,
          "end_idx": 493
        },
        "page_content": "Enabling computers to understand given documents and answer questions about their content has recently attracted intensive interest, including but not limited to the efforts as in BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 . Many specific problems such as machine comprehension and question answering often involve modeling such question-document pairs. The recent availability of relatively large training datasets (see Section \"Related Work\" for more details) has made it more",
        "type": "Document"
      },
      {
        "id": "823432aa-1fcb-4081-9406-138d470be964",
        "metadata": {
          "vector_store_key": "1908.08419-0",
          "chunk_id": 16,
          "document_id": "1908.08419",
          "start_idx": 5833,
          "end_idx": 6343
        },
        "page_content": " model. In the medical field, Xing et al. BIBREF8 proposed an adaptive multi-task transfer learning framework to fully leverage domain-invariant knowledge from high resource domain to medical domain. Meanwhile, transfer learning still greatly focuses on the corpus in general domain. When it comes to the specific domain, large amounts of manually-annotated data is necessary. Active learning can solve this problem to a certain extent. However, due to the challenges faced by performing active learning on CWS",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much performance gap between their approach and the strong handcrafted method?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "82ee60ef-c81c-4369-8134-d20ea8aa6aef",
        "metadata": {
          "vector_store_key": "1909.00578-2",
          "chunk_id": 27,
          "document_id": "1909.00578",
          "start_idx": 8968,
          "end_idx": 9354
        },
        "page_content": " the training set of each fold. Table TABREF23 shows Spearman's $\\rho $, Kendall's $\\tau $ and Pearson's $r$ for all datasets and models. The three fine-tuned BERT versions clearly outperform all other methods. Multi-task versions seem to perform better than single-task ones in most cases. Especially for $\\mathcal {Q}4$ and $\\mathcal {Q}5$, which are highly correlated, the multi-task",
        "type": "Document"
      },
      {
        "id": "7a3763d4-793b-483d-9bf2-cdefe870cd27",
        "metadata": {
          "vector_store_key": "2003.07433-0",
          "chunk_id": 66,
          "document_id": "2003.07433",
          "start_idx": 25731,
          "end_idx": 26161
        },
        "page_content": "REF28 illustrates the comparisons between LAXARY and Coppersmith et. al. proposed method. Here we can see, the outperformance of our proposed method as well as the importance of $s-score$ estimation. We also illustrates the importance of $\\alpha -score$ and $S-score$ in Fig FIGREF30. Fig FIGREF30 illustrates that if we change the number of training samples (%), LAXARY models outperforms Coppersmith et. al. proposed model under",
        "type": "Document"
      },
      {
        "id": "0c3dc473-e6e5-400c-b3a1-75c887dbe5e3",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 28,
          "document_id": "1909.00542",
          "start_idx": 10682,
          "end_idx": 11217
        },
        "page_content": " experiments. We ran the version trained on Python ROUGE once, and the version trained on Perl twice. The two Perl runs have different results, and one of them clearly outperforms the Python run. However, given the differences of results between the two Perl runs we advice to re-run the experiments multiple times and obtain the mean and standard deviation of the runs before concluding whether there is any statistical difference between the results. But it seems that there may be an improvement of the final evaluation results when",
        "type": "Document"
      },
      {
        "id": "8481d522-45c7-406a-8901-a11119c3e151",
        "metadata": {
          "vector_store_key": "2002.06644-2",
          "chunk_id": 18,
          "document_id": "2002.06644",
          "start_idx": 6707,
          "end_idx": 7139
        },
        "page_content": ", a distilled model, performs competitively, achieving $69.69\\%$ accuracy, and $0.672$ F1 score. This observation shows that distilled pretrained models can replace their undistilled counterparts in a low-computing environment. We further observe that ensemble models perform better than optimized BERT-based models and distilled pretrained models. Our proposed ensemble comprising of $RoBERTa_{large}$, $ALBERT_{xxlarge.v2}$, $Dist",
        "type": "Document"
      },
      {
        "id": "896923b4-37f9-4b9d-846e-0a8c38e6f697",
        "metadata": {
          "vector_store_key": "1908.06606-2",
          "chunk_id": 65,
          "document_id": "1908.06606",
          "start_idx": 22920,
          "end_idx": 23336
        },
        "page_content": " performance on distal resection margin improved by 5.56% in EM-score and 6.32% in F$_1$-score. Other performances also usually improved a lot. This proves the usefulness of two-stage training and named entity information as well. Lastly, we fine tune the model for each task with a pre-trained parameter. Table TABREF32 summarizes the result. (Add some explanations for the Table TABREF32). Comparing Table TABREF32",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is their model?",
    "llm_answer": "The context describes various models with different sizes, but does not give a single overall answer to \"how big is their model?\".\n",
    "context": [
      {
        "id": "1d080d26-4113-431d-9eb9-8c72c31e9b10",
        "metadata": {
          "vector_store_key": "1902.09314-4",
          "chunk_id": 46,
          "document_id": "1902.09314",
          "start_idx": 16362,
          "end_idx": 16777
        },
        "page_content": " is a lightweight alternative of recurrent models, we study the model size of each model on the Restaurant dataset. Statistical results are reported in Table TABREF37 . We implement all the compared models base on the same source code infrastructure, use the same hyperparameters, and run them on the same GPU . RNN-based and BERT-based models indeed have larger model size. ATAE-LSTM, IAN, RAM, and AEN-GloVe-BiLST",
        "type": "Document"
      },
      {
        "id": "3e1406ea-9836-4a54-8399-d1463333181f",
        "metadata": {
          "vector_store_key": "1909.00694-8",
          "chunk_id": 49,
          "document_id": "1909.00694",
          "start_idx": 15395,
          "end_idx": 15766
        },
        "page_content": " segmented into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam BIBREF19. The mini-batch size was 32. We ran 1 epoch.",
        "type": "Document"
      },
      {
        "id": "b05c1a09-345c-42fb-ac8c-c6f008a0a73b",
        "metadata": {
          "vector_store_key": "1811.00942-1",
          "chunk_id": 25,
          "document_id": "1811.00942",
          "start_idx": 8253,
          "end_idx": 8645
        },
        "page_content": " specific hyperparameters are the same as those in the repository. Our model architecture consists of 400-dimensional tied embedding weights BIBREF18 and four QRNN layers, with 1550 hidden units per layer on PTB and 2500 per layer on WT103. Both QRNN models have window sizes of $r=2$ for the first layer and $r=1$ for the rest. For the KN-5 model, we trained an off-the-shelf five-gram model",
        "type": "Document"
      },
      {
        "id": "b38cb564-d49e-4be5-b746-5465ce6b23b5",
        "metadata": {
          "vector_store_key": "1603.07044-5",
          "chunk_id": 48,
          "document_id": "1603.07044",
          "start_idx": 16888,
          "end_idx": 17326
        },
        "page_content": " contain 63 words. Interestingly, the distribution of weights does not become more uniform; the model still focuses attention on a small number of hot words, for example, \u201cpuppy dog for ... mall\u201d and \u201chectic driving in doha ... car insurance ... quite costly\u201d. Additionally, some words that appear frequently but carry little information for classification are assigned very small weights, such as I/we/my, is/am, like, and to. Due to the",
        "type": "Document"
      },
      {
        "id": "577fb06e-9340-4422-93f3-91bc89574aee",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 43,
          "document_id": "2001.00137",
          "start_idx": 16781,
          "end_idx": 17197
        },
        "page_content": " the online platforms. Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many emotions do they look at?",
    "llm_answer": "24\n",
    "context": [
      {
        "id": "55c2018c-0438-41a7-ba85-2f9333dcb8f9",
        "metadata": {
          "vector_store_key": "1909.02764-9",
          "chunk_id": 39,
          "document_id": "1909.02764",
          "start_idx": 15881,
          "end_idx": 16327
        },
        "page_content": " seconds, with the minimum at 2.2s and the maximum at 54.7s. We apply an off-the-shelf tool for emotion recognition (the manufacturer cannot be disclosed due to licensing restrictions). It delivers frame-by-frame scores ($\\in [0;100]$) for discrete emotional states of joy, anger and fear. While joy corresponds directly to our annotation, we map anger to our label annoyance and fear to our label insecurity. The maximal average score across all",
        "type": "Document"
      },
      {
        "id": "765c451a-4595-4157-9e64-316a3b340ff2",
        "metadata": {
          "vector_store_key": "1909.02764-9",
          "chunk_id": 41,
          "document_id": "1909.02764",
          "start_idx": 16704,
          "end_idx": 17205
        },
        "page_content": " set of 24 discrete emotions for the entire utterance. We consider the outputs for the states of joy, anger, and fear, mapping analogously to our classes as for facial expressions. Low-confidence predictions are interpreted as \u201cno emotion\u201d. We accept the emotion with the highest score as the discrete prediction otherwise. For the emotion recognition from text, we manually transcribe all utterances of our AMMER study. To exploit existing and available data sets which are larger than the AMMER data",
        "type": "Document"
      },
      {
        "id": "5a432bd5-9dd7-4069-9338-7ee798af6776",
        "metadata": {
          "vector_store_key": "1909.02764-9",
          "chunk_id": 40,
          "document_id": "1909.02764",
          "start_idx": 16157,
          "end_idx": 16704
        },
        "page_content": " we map anger to our label annoyance and fear to our label insecurity. The maximal average score across all frames constitutes the overall classification for the video sequence. Frames where the software is not able to detect the face are ignored. We extract the audio signal for the same sequence as described for facial expressions and apply an off-the-shelf tool for emotion recognition. The software delivers single classification scores for a set of 24 discrete emotions for the entire utterance. We consider the outputs for the states of joy",
        "type": "Document"
      },
      {
        "id": "1d303ce9-d776-440b-afa7-621de50764cc",
        "metadata": {
          "vector_store_key": "1909.02764-4",
          "chunk_id": 12,
          "document_id": "1909.02764",
          "start_idx": 4894,
          "end_idx": 5437
        },
        "page_content": " at detecting frustration in a simulator environment. They induce the emotion with specific scenarios and a demanding secondary task and are able to associate specific face movements according to FACS. Paschero2012 use OpenCV (https://opencv.org/) to detect the eyes and the mouth region and track facial movements. They simulate different lightning conditions and apply a multilayer perceptron for the classification task of Ekman's set of fundamental emotions. Overall, we found that studies using facial features usually focus on continuous",
        "type": "Document"
      },
      {
        "id": "0759e434-9fe8-45c1-aeb2-c2cc58e08c72",
        "metadata": {
          "vector_store_key": "1909.02764-4",
          "chunk_id": 1,
          "document_id": "1909.02764",
          "start_idx": 454,
          "end_idx": 915
        },
        "page_content": " includes anger, fear, disgust, sadness, joy, and surprise, or the extension by Plutchik Plutchik1980 who adds trust and anticipation. Most work in emotion detection is limited to one modality. Exceptions include Busso2004 and Sebe2005, who investigate multimodal approaches combining speech with facial information. Emotion recognition in speech can utilize semantic features as well BIBREF0. Note that the term \u201cmultimodal\u201d is also used beyond the combination",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What approach did previous models use for multi-span questions?",
    "llm_answer": "The previous model (MTMSN) trained a dedicated categorical variable to predict the number of spans to extract, and generalized the single-span head method by utilizing non-maximum suppression (NMS) to extract multi-span answers.\n",
    "context": [
      {
        "id": "65e5096a-bac8-4dc1-bf4a-77da9e9f8b1c",
        "metadata": {
          "vector_store_key": "1909.13375-7",
          "chunk_id": 65,
          "document_id": "1909.13375",
          "start_idx": 22213,
          "end_idx": 22660
        },
        "page_content": " and 0.6 in F1 compared to MTMSN. Therefore, it could prove beneficial to try and analyze the reasons behind each model's (ours and MTMSN) relative advantages, and perhaps try to combine them into a more holistic approach of tackling span questions. Table TABREF25 shows the results on DROP's test set, with our model being the best overall as of the time of writing, and not just on multi-span questions. In order to analyze the effect of each of",
        "type": "Document"
      },
      {
        "id": "ad608421-d2c1-4105-8303-502157b0f9f9",
        "metadata": {
          "vector_store_key": "1909.13375-1",
          "chunk_id": 72,
          "document_id": "1909.13375",
          "start_idx": 24838,
          "end_idx": 25324
        },
        "page_content": " has a clear advantage in handling multi-span questions, with a 29.7 absolute improvement in EM, and a 15.1 absolute improvement in F1. Furthermore, we show that our model slightly eclipses the current state-of-the-art results on the entire DROP dataeset. Finally, we present some ablation studies, analyzing the benefit gained from individual components of our model. We believe that combining our tag-based approach for handling multi-span questions with current successful techniques",
        "type": "Document"
      },
      {
        "id": "9e99aa57-3705-4987-8ed0-f9d2e51abb76",
        "metadata": {
          "vector_store_key": "1909.13375-1",
          "chunk_id": 3,
          "document_id": "1909.13375",
          "start_idx": 1322,
          "end_idx": 1782
        },
        "page_content": "REF4 directly targeted multi-span questions in a manner that wasn't just a by-product of the model's overall performance. In this paper, we propose a new method for tackling multi-span questions. Our method takes a different path from that of the aforementioned model. It does not try to generalize the existing approach for tackling single-span questions, but instead attempts to attack this issue with a new, tag-based, approach. Numerically-aware QANet (NAQ",
        "type": "Document"
      },
      {
        "id": "d5843041-f2c5-40d9-bfcc-06e6d806da54",
        "metadata": {
          "vector_store_key": "1909.13375-1",
          "chunk_id": 9,
          "document_id": "1909.13375",
          "start_idx": 3497,
          "end_idx": 3987
        },
        "page_content": " capabilities, by adding the ability of doing simple multiplications and divisions between up to three numbers. MTMSN BIBREF4 is the first, and only model so far, that specifically tried to tackle the multi-span questions of DROP. Their approach consisted of two parts. The first was to train a dedicated categorical variable to predict the number of spans to extract. The second was to generalize the single-span head method of extracting a span, by utilizing the non-maximum suppression (",
        "type": "Document"
      },
      {
        "id": "8e38a564-7c20-49ce-a90c-1bd0f411e8d8",
        "metadata": {
          "vector_store_key": "1909.13375-7",
          "chunk_id": 64,
          "document_id": "1909.13375",
          "start_idx": 21772,
          "end_idx": 22213
        },
        "page_content": " multi-span answers. For a fairer comparison, we trained our model with the single-span heads removed, where our multi-span head remained the only head aimed for handling span questions. With this no-single-span-heads setting, while our multi-span performance even improved a bit, our single-span performance suffered a slight drop, ending up trailing by 0.8 in EM and 0.6 in F1 compared to MTMSN. Therefore, it could prove beneficial to try",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is difference in peformance between proposed model and state-of-the art on other question types?",
    "llm_answer": "The provided text states the proposed model achieves the best performance and better accuracy in discriminating plausible comments from correct comments.  It does not offer a comparison of performance on other question types with state-of-the-art models.\n",
    "context": [
      {
        "id": "2c6791a6-0607-4b8e-adf9-d553f4e4757e",
        "metadata": {
          "vector_store_key": "1703.04617-0",
          "chunk_id": 54,
          "document_id": "1703.04617",
          "start_idx": 14070,
          "end_idx": 14584
        },
        "page_content": "Q}^{TL}$ for M times to fit with $\\mathbf {I}$ . Questions by nature are often composed to fulfill different types of information needs. For example, a \"when\" question seeks for different types of information (i.e., temporal information) than those for a \"why\" question. Different types of questions and the corresponding answers could potentially have different distributional regularity. The previous models are often trained for all questions without explicitly discriminating different question types; however,",
        "type": "Document"
      },
      {
        "id": "bd961ae0-24e4-41a6-88c0-3fb605e0bdf3",
        "metadata": {
          "vector_store_key": "1809.04960-4",
          "chunk_id": 62,
          "document_id": "1809.04960",
          "start_idx": 24031,
          "end_idx": 24606
        },
        "page_content": " get lower scores than the plausible comments which are more literally relevant with the articles. With the help of our proposed model, proposed+IR achieves the best performance, and achieves a better accuracy to discriminate the plausible comments and the correct comments. Our proposed model incorporates the topic information, so the correct comments which are more similar to the articles in topic obtain higher scores than the other types of comments. According to the analysis of the error types of our model, we still need to focus on avoiding predicting the plausible",
        "type": "Document"
      },
      {
        "id": "1cc830ac-f4e2-46d9-9e7d-f8704a5cf375",
        "metadata": {
          "vector_store_key": "1703.04617-0",
          "chunk_id": 9,
          "document_id": "1703.04617",
          "start_idx": 3555,
          "end_idx": 4085
        },
        "page_content": " has the potential to capture long-distance interactions over the structures. Different types of questions are often used to seek for different types of information. For example, a \"what\" question could have very different property from that of a \"why\" question, while they may share information and need to be trained together instead of separately. We view this as a \"adaptation\" problem to let different types of questions share a basic model but still discriminate them when needed. Specifically, we are motivated by the ideas",
        "type": "Document"
      },
      {
        "id": "e115f3ad-cfb7-4243-b123-51a669484bbc",
        "metadata": {
          "vector_store_key": "1703.04617-0",
          "chunk_id": 55,
          "document_id": "1703.04617",
          "start_idx": 14584,
          "end_idx": 15140
        },
        "page_content": "ity. The previous models are often trained for all questions without explicitly discriminating different question types; however, for a target question, both the common features shared by all questions and the specific features for a specific type of question are further considered in this paper, as they could potentially obey different distributions. In this paper we further explicitly model different types of questions in the end-to-end training. We start from a simple way to first analyze the word frequency of all questions, and obtain top-10 most",
        "type": "Document"
      },
      {
        "id": "568b42cb-5770-49cb-b313-b18db222dc85",
        "metadata": {
          "vector_store_key": "1703.04617-0",
          "chunk_id": 1,
          "document_id": "1703.04617",
          "start_idx": 493,
          "end_idx": 995
        },
        "page_content": " of relatively large training datasets (see Section \"Related Work\" for more details) has made it more feasible to train and estimate rather complex models in an end-to-end fashion for these problems, in which a whole model is fit directly with given question-answer tuples and the resulting model has shown to be rather effective. In this paper, we take a closer look at modeling questions in such an end-to-end neural network framework, since we regard question understanding is of importance for such",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance of proposed model on entire DROP dataset?",
    "llm_answer": "The context states that the model achieved 68.73% EM score and 77.39% F1 score on the SQuAD test set, but does not provide the performance on the entire DROP dataset.  Answer not found in the context.\n",
    "context": [
      {
        "id": "555a266e-0a08-43e5-b719-8cd2fd10ebb8",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "1234c8c3-853f-4c66-ba54-aa2c20dfe21f",
        "metadata": {
          "vector_store_key": "1703.04617-9",
          "chunk_id": 83,
          "document_id": "1703.04617",
          "start_idx": 22381,
          "end_idx": 22802
        },
        "page_content": ". We apply dropout to the Encoder layer and aggregation layer with a dropout rate of 0.5. Table 1 shows the official leaderboard on SQuAD test set when we submitted our system. Our model achieves a 68.73% EM score and 77.39% F1 score, which is ranked among the state of the art single models (without model ensembling). Table 2 shows the ablation performances of various Q-code on the development set. Note that since the",
        "type": "Document"
      },
      {
        "id": "3c3e7bd6-8955-4353-bdf3-c8d8c69b5e4d",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 59,
          "document_id": "1909.13375",
          "start_idx": 20052,
          "end_idx": 20444
        },
        "page_content": "s. For the large models we used a batch size of 3 with a learning rate of $5e^{-6}$ and trained for 5 epochs, except for the model without the single-span heads that was trained with a batch size of 2 for 7 epochs. F1 was used as our validation metric. All models were trained on a single GPU with 12-16GB of memory. Table TABREF24 shows the results on DROP's development set. Compared to our",
        "type": "Document"
      },
      {
        "id": "bc8ac242-0b5e-4c71-be0b-48d167015fad",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 43,
          "document_id": "1909.02764",
          "start_idx": 17421,
          "end_idx": 17831
        },
        "page_content": " layer. This setup is inspired by BIBREF38. We use a dropout rate of 0.3 in all layers and optimize with Adam BIBREF39 with a learning rate of $10^{-5}$ (These parameters are the same for all further experiments). We build on top of the Keras library with the TensorFlow backend. We consider this setup our baseline model. We train models on a variety of corpora, namely the common format published by BIBREF27",
        "type": "Document"
      },
      {
        "id": "c03315b1-7d9a-4a9a-a552-f7259ef1288c",
        "metadata": {
          "vector_store_key": "1909.02764-7",
          "chunk_id": 53,
          "document_id": "1909.02764",
          "start_idx": 20698,
          "end_idx": 21118
        },
        "page_content": " to our data set. The results are shown in column \u201cSimple\u201d in Table TABREF19. We observe a clear drop in performance, with an average of F$_1$=48 %. The best performing model is again the one trained on TEC, en par with the one trained on the Figure8 data. The model trained on ISEAR performs second best in Experiment 1, it performs worst in Experiment 2. To adapt models trained on previously existing data sets to our",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How accurate is the aspect based sentiment classifier trained only using the XR loss?",
    "llm_answer": "The context states that an unannotated corpus of sentences is sufficient to surpass the results of a sentence-level trained classifier, and more unannotated data further improves results.  It does *not* state the *accuracy* of the classifier trained only using the XR loss.\n",
    "context": [
      {
        "id": "853d41f8-0df7-4c32-93f7-e0ad4515f1e5",
        "metadata": {
          "vector_store_key": "1909.00430-0",
          "chunk_id": 6,
          "document_id": "1909.00430",
          "start_idx": 2288,
          "end_idx": 2765
        },
        "page_content": " not be fully accurate, allowing us to make use of noisy predictions which are assigned by an automatic classifier (Sections SECREF12 and SECREF4 ). In other words, given a medium-sized sentiment corpus with sentence-level labels, and a large collection of un-annotated text from the same distribution, we can train an accurate aspect-level sentiment classifier. The XR loss allows us to use task A labels for training task B predictors. This ability seamlessly integrates into",
        "type": "Document"
      },
      {
        "id": "b0528819-1f68-4672-a39d-84005e571358",
        "metadata": {
          "vector_store_key": "1909.00430-0",
          "chunk_id": 13,
          "document_id": "1909.00430",
          "start_idx": 4830,
          "end_idx": 5281
        },
        "page_content": " of NLP, XR was used by BIBREF20 to train twitter-user attribute prediction using hundreds of noisy distributional expectations based on census demographics. Here, we suggest using XR to train a target task (aspect-level sentiment) based on the output of a related source-task classifier (sentence-level sentiment). The main idea of XR is moving from a fully supervised situation in which each data-point INLINEFORM0 has an associated label INLINEFORM",
        "type": "Document"
      },
      {
        "id": "29287008-fdf2-4309-be34-bf44700c559d",
        "metadata": {
          "vector_store_key": "1909.00430-0",
          "chunk_id": 4,
          "document_id": "1909.00430",
          "start_idx": 1532,
          "end_idx": 2003
        },
        "page_content": "ed aspects within this set. For example, in a random set of positive sentences, we expect to find 90% positive aspects, while in a set of negative sentences, we expect to find 70% negative aspects. These proportions can be easily either guessed or estimated from a small set. We propose a novel application of the XR framework for transfer learning in this setup. We present an algorithm (Sec SECREF12 ) that, given a corpus labeled for task A (sentence-level sentiment),",
        "type": "Document"
      },
      {
        "id": "be7be861-b400-48f8-b2d5-be23a0b07061",
        "metadata": {
          "vector_store_key": "1909.00430-4",
          "chunk_id": 47,
          "document_id": "1909.00430",
          "start_idx": 16766,
          "end_idx": 17277
        },
        "page_content": " scores. We note that in contrast to XR, the competing models have complete access to the supervised aspect-based labels. The second condition, BiLSTM-XR, is similar but now the model is allowed to estimate the conditional label proportions based on the entire aspect-based training set (the classifier still does not have direct access to the labels beyond the aggregate proportion information). This improves results further, showing the importance of accurately estimating the proportions. Finally, in BiLSTM",
        "type": "Document"
      },
      {
        "id": "9c6b948c-5075-49c3-8d9f-e6335f2c468d",
        "metadata": {
          "vector_store_key": "1909.00430-0",
          "chunk_id": 51,
          "document_id": "1909.00430",
          "start_idx": 18284,
          "end_idx": 18752
        },
        "page_content": " An unannotated corpus of INLINEFORM0 sentences is sufficient to surpass the results of the INLINEFORM1 sentence-level trained classifier, and more unannotated data further improves the results. Our method requires a sentence level classifier INLINEFORM0 to label both the target-task corpus and the unlabeled corpus. How does the quality of this classifier affect the overall XR training? We vary the amount of supervision used to train INLINEFORM1 from 0 sentences (",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What were the non-neural baselines used for the task?",
    "llm_answer": "The \"first $n$\" baseline.\n",
    "context": [
      {
        "id": "ed8aaf94-c5e8-4cf0-9b3c-4a4e591733f5",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 39,
          "document_id": "1909.00542",
          "start_idx": 15059,
          "end_idx": 15516
        },
        "page_content": " the results and details of the runs submitted to BioASQ. The table uses ROUGE-SU4 Recall since this is the metric available at the time of writing this paper. However, note that, as explained in Section SECREF6, these results might differ from the final human evaluation results. Therefore we do not comment on the results, other than observing that the \u201cfirst $n$\u201d baseline produces the same results as the neural regressor. As mentioned in Section SECREF",
        "type": "Document"
      },
      {
        "id": "10ee7c8c-8492-4d26-9ee2-5244b2aa61f4",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 20,
          "document_id": "1909.00542",
          "start_idx": 7743,
          "end_idx": 8155
        },
        "page_content": " layer, and use cross-entropy as the loss function. The complete architecture is shown in Fig. FIGREF28. The bottom section of Table TABREF26 shows the results of several variants of the neural architecture. The table includes a neural regressor (NNR) and a neural classifier (NNC). The neural classifier is trained in two set ups: \u201cNNC top 5\u201d uses classification labels as described in Section SECREF3, and \u201cNNC",
        "type": "Document"
      },
      {
        "id": "caf96a37-2de9-4e90-b08a-ca8a54bac59b",
        "metadata": {
          "vector_store_key": "1909.00542-2",
          "chunk_id": 40,
          "document_id": "1909.00542",
          "start_idx": 15344,
          "end_idx": 15845
        },
        "page_content": "$\u201d baseline produces the same results as the neural regressor. As mentioned in Section SECREF3, the labels used for the classification experiments are the 5 sentences with highest ROUGE-SU4 F1 score. Macquarie University's participation in BioASQ 7 focused on the task of generating the ideal answers. The runs use query-based extractive techniques and we experiment with classification, regression, and reinforcement learning approaches. At the time of writing there were no human evaluation results,",
        "type": "Document"
      },
      {
        "id": "bc8ac242-0b5e-4c71-be0b-48d167015fad",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 43,
          "document_id": "1909.02764",
          "start_idx": 17421,
          "end_idx": 17831
        },
        "page_content": " layer. This setup is inspired by BIBREF38. We use a dropout rate of 0.3 in all layers and optimize with Adam BIBREF39 with a learning rate of $10^{-5}$ (These parameters are the same for all further experiments). We build on top of the Keras library with the TensorFlow backend. We consider this setup our baseline model. We train models on a variety of corpora, namely the common format published by BIBREF27",
        "type": "Document"
      },
      {
        "id": "c94d339e-36f5-4336-a21a-05be0b487cef",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the models evaluated on?",
    "llm_answer": "200 unique comments, ranked with four models (TF-IDF, S2S, IR, and Proposed+IR).\n",
    "context": [
      {
        "id": "ac08e6c0-ed7c-44a4-8569-c15f1a5e35d1",
        "metadata": {
          "vector_store_key": "1809.04960-5",
          "chunk_id": 58,
          "document_id": "1809.04960",
          "start_idx": 22497,
          "end_idx": 22960
        },
        "page_content": " the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types of comments as described in the above retrieval evaluation setting: Correct, Plausible, Popular, and Random. We rank the candidate comment set with four models (TF-IDF, S2S, IR, and Proposed+IR), and record the types of top-1 comments. Figure FIGREF40 shows the percentage of different types of top-1 comments generated by each model. It shows",
        "type": "Document"
      },
      {
        "id": "be8b40f0-56ba-4775-9ef5-8844908beaec",
        "metadata": {
          "vector_store_key": "1905.00563-4",
          "chunk_id": 47,
          "document_id": "1905.00563",
          "start_idx": 13447,
          "end_idx": 13944
        },
        "page_content": " evaluate by ( \"Influence Function vs \" ) comparing estimate with the actual effect of the attacks, ( \"Robustness of Link Prediction Models\" ) studying the effect of adversarial attacks on evaluation metrics, ( \"Interpretability of Models\" ) exploring its application to the interpretability of KG representations, and ( \"Finding Errors in Knowledge Graphs\" ) detecting incorrect triples. To evaluate the quality of our approximations and compare with influence function (IF), we conduct leave one",
        "type": "Document"
      },
      {
        "id": "21806947-d88f-404f-b964-d7856915d5cb",
        "metadata": {
          "vector_store_key": "1809.04960-5",
          "chunk_id": 43,
          "document_id": "1809.04960",
          "start_idx": 16805,
          "end_idx": 17328
        },
        "page_content": " various, so it is intractable to find out all the possible references to be compared with the model outputs. Inspired by the evaluation methods of dialogue models, we formulate the evaluation as a ranking problem. Given a piece of news and a set of candidate comments, the comment model should return the rank of the candidate comments. The candidate comment set consists of the following parts: Correct: The ground-truth comments of the corresponding news provided by the human. Plausible: The 50 most similar comments to",
        "type": "Document"
      },
      {
        "id": "4a96115b-96a0-40c9-8804-17c559445d5a",
        "metadata": {
          "vector_store_key": "1809.04960-5",
          "chunk_id": 63,
          "document_id": "1809.04960",
          "start_idx": 24606,
          "end_idx": 25124
        },
        "page_content": " the analysis of the error types of our model, we still need to focus on avoiding predicting the plausible comments. There are few studies regarding machine commenting. Qin et al. QinEA2018 is the first to propose the article commenting task and a dataset, which is used to evaluate our model in this work. More studies about the comments aim to automatically evaluate the quality of the comments. Park et al. ParkSDE16 propose a system called CommentIQ, which assist the comment moderators in identifying high quality",
        "type": "Document"
      },
      {
        "id": "30ee9692-a719-46ac-84d6-3dc7d3f0630b",
        "metadata": {
          "vector_store_key": "1910.03891-2",
          "chunk_id": 25,
          "document_id": "1910.03891",
          "start_idx": 9736,
          "end_idx": 10179
        },
        "page_content": "REF17, textual descriptions BIBREF11, BIBREF12 and logical rules BIBREF23; (ii) those which tries to design more complicated strategies, e.g., deep neural network models BIBREF24. Except for TransE and its extensions, some efforts measure plausibility by matching latent semantics of entities and relations. The basic idea behind these models is that the plausible triples of a KG is assigned low energies. For examples, Distant Model BIBREF25",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the results of multimodal compared to unimodal models?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "29ff2783-cf3f-4f9e-8313-77cadf4680ca",
        "metadata": {
          "vector_store_key": "1909.02764-4",
          "chunk_id": 2,
          "document_id": "1909.02764",
          "start_idx": 759,
          "end_idx": 1210
        },
        "page_content": "0. Note that the term \u201cmultimodal\u201d is also used beyond the combination of vision, audio, and text. For example, Soleymani2012 use it to refer to the combination of electroencephalogram, pupillary response and gaze distance. In this paper, we deal with the specific situation of car environments as a testbed for multimodal emotion recognition. This is an interesting environment since it is, to some degree, a controlled environment: Dialogue partners",
        "type": "Document"
      },
      {
        "id": "43129cc8-6f1d-4ffe-8bd1-39b990e0534d",
        "metadata": {
          "vector_store_key": "1910.09399-1",
          "chunk_id": 11,
          "document_id": "1910.09399",
          "start_idx": 4538,
          "end_idx": 4998
        },
        "page_content": "IBREF13. Researchers often describe multimodal learning as a method that incorporates characteristics from several methods, algorithms, and ideas. This can include ideas from two or more learning approaches in order to create a robust implementation to solve an uncommon problem or improve a solution BIBREF8, BIBREF14, BIBREF15, BIBREF16, BIBREF17. black In this survey, we focus primarily on reviewing recent works that aim to solve the challenge of text-to-",
        "type": "Document"
      },
      {
        "id": "e1ee028c-001e-44c3-976a-cfecb205f46a",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 63,
          "document_id": "1909.13375",
          "start_idx": 21507,
          "end_idx": 21934
        },
        "page_content": " Our best model, large-squad, exhibits a huge improvement of 29.7 in EM and 15.1 in F1 compared to MTMSNlarge. When comparing single-span performance, our best model exhibits slightly better results, but it should be noted that it retains the single-span heads from NABERT+, while in MTMSN they have one head to predict both single-span and multi-span answers. For a fairer comparison, we trained our model with the single-span",
        "type": "Document"
      },
      {
        "id": "b8463b23-0372-4422-ba8d-e82788fecd2b",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 61,
          "document_id": "1909.13375",
          "start_idx": 20788,
          "end_idx": 21299
        },
        "page_content": " For the other types, most of the improvement came from better preprocessing. A more detailed discussion could be found in Section (SECREF36). Notice that different BERTlarge models were used, so the comparison is less direct. Overall, our large models exhibits similar results to those of MTMSNlarge. For multi-span questions we achieve a significantly better performance. While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still",
        "type": "Document"
      },
      {
        "id": "65e5096a-bac8-4dc1-bf4a-77da9e9f8b1c",
        "metadata": {
          "vector_store_key": "1909.13375-7",
          "chunk_id": 65,
          "document_id": "1909.13375",
          "start_idx": 22213,
          "end_idx": 22660
        },
        "page_content": " and 0.6 in F1 compared to MTMSN. Therefore, it could prove beneficial to try and analyze the reasons behind each model's (ours and MTMSN) relative advantages, and perhaps try to combine them into a more holistic approach of tackling span questions. Table TABREF25 shows the results on DROP's test set, with our model being the best overall as of the time of writing, and not just on multi-span questions. In order to analyze the effect of each of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What were their performance results?",
    "llm_answer": "F1 score of 57% on AMMER corpus; 73% on TEC;  performance results on DROP's development set are shown in Table TABREF24;  model surpasses NABERT+ baseline on all metrics for DROP.\n",
    "context": [
      {
        "id": "638d3f9b-929b-4fa6-b0d4-efe44cd66255",
        "metadata": {
          "vector_store_key": "1912.13109-2",
          "chunk_id": 25,
          "document_id": "1912.13109",
          "start_idx": 10203,
          "end_idx": 10681
        },
        "page_content": " metrics - Recall rate, F1 score and Overall accuracy. Choice of model parameters were in the above models were inspired from previous work done but then were tuned to the best performance of the Test dataset. Following parameters were considered for tuning. Learning rate: Based on grid search the best performance was achieved when learning rate was set to 0.01. This value was arrived by a grid search on lr parameter. Number of Bidirectional LSTM units: A set of 32, 64, 128",
        "type": "Document"
      },
      {
        "id": "31e3f6a2-9e9b-4d6d-824a-22195518284a",
        "metadata": {
          "vector_store_key": "1909.02764-7",
          "chunk_id": 52,
          "document_id": "1909.02764",
          "start_idx": 20420,
          "end_idx": 20841
        },
        "page_content": "=73 % on TEC. The model trained on our AMMER corpus achieves an F1 score of 57%. This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set. These results are mostly in line with Bostan2018. Now we analyze how well the models trained in Experiment 1 perform when applied to our data set. The results are shown in column \u201cSimple\u201d in Table TAB",
        "type": "Document"
      },
      {
        "id": "894f4ffd-fbcb-43a7-ba0f-9cca665b9304",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 2,
          "document_id": "1909.00542",
          "start_idx": 734,
          "end_idx": 1323
        },
        "page_content": " the training procedure was based on a regression set up, this year we experiment with various classification set ups. The main contributions of this paper are: We compare classification and regression approaches and show that classification produces better results than regression but the quality of the results depends on the approach followed to annotate the data labels. We conduct correlation analysis between various ROUGE evaluation metrics and the human evaluations conducted at BioASQ and show that Precision and F1 correlate better than Recall. Section SECREF2 briefly introduces",
        "type": "Document"
      },
      {
        "id": "148ea921-3ae7-4687-bc70-85a9690fe58b",
        "metadata": {
          "vector_store_key": "1912.13109-4",
          "chunk_id": 20,
          "document_id": "1912.13109",
          "start_idx": 7921,
          "end_idx": 8471
        },
        "page_content": " on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we",
        "type": "Document"
      },
      {
        "id": "555a266e-0a08-43e5-b719-8cd2fd10ebb8",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much did they outperform the other methods?",
    "llm_answer": "The context states that fine-tuned BERT versions \"clearly outperform all other methods\", but does not give specific performance differences.\n",
    "context": [
      {
        "id": "82ee60ef-c81c-4369-8134-d20ea8aa6aef",
        "metadata": {
          "vector_store_key": "1909.00578-2",
          "chunk_id": 27,
          "document_id": "1909.00578",
          "start_idx": 8968,
          "end_idx": 9354
        },
        "page_content": " the training set of each fold. Table TABREF23 shows Spearman's $\\rho $, Kendall's $\\tau $ and Pearson's $r$ for all datasets and models. The three fine-tuned BERT versions clearly outperform all other methods. Multi-task versions seem to perform better than single-task ones in most cases. Especially for $\\mathcal {Q}4$ and $\\mathcal {Q}5$, which are highly correlated, the multi-task",
        "type": "Document"
      },
      {
        "id": "7a3763d4-793b-483d-9bf2-cdefe870cd27",
        "metadata": {
          "vector_store_key": "2003.07433-0",
          "chunk_id": 66,
          "document_id": "2003.07433",
          "start_idx": 25731,
          "end_idx": 26161
        },
        "page_content": "REF28 illustrates the comparisons between LAXARY and Coppersmith et. al. proposed method. Here we can see, the outperformance of our proposed method as well as the importance of $s-score$ estimation. We also illustrates the importance of $\\alpha -score$ and $S-score$ in Fig FIGREF30. Fig FIGREF30 illustrates that if we change the number of training samples (%), LAXARY models outperforms Coppersmith et. al. proposed model under",
        "type": "Document"
      },
      {
        "id": "97d0589b-102d-402c-bcb2-27484aa3c4fb",
        "metadata": {
          "vector_store_key": "1701.00185-8",
          "chunk_id": 76,
          "document_id": "1701.00185",
          "start_idx": 27376,
          "end_idx": 27736
        },
        "page_content": " and NMI performance of our proposed approaches and four baseline methods, K-means, SkipVec, RecNN and Para2vec based clustering methods. Intuitively, we get a general observation that (1) BoW based approaches, including K-means (TF) and K-means (TF-IDF), and SkipVec based approaches perform not well; (2) RecNN based approaches, both RecNN (Ave.) and RecNN (",
        "type": "Document"
      },
      {
        "id": "013111e3-e602-4c28-838b-cfeb288a76e2",
        "metadata": {
          "vector_store_key": "1910.09399-1",
          "chunk_id": 168,
          "document_id": "1910.09399",
          "start_idx": 65080,
          "end_idx": 65581
        },
        "page_content": ". In other words, beyond the work of BIBREF8 in which images were generated from text in 64$\\times $64 tiny swatches. Lastly, all methods were evaluated on datasets that included birds, flowers, humans, and other miscellaneous elements. We were also able to allocate some important papers that were as impressive as the papers we finally surveyed. Though, these notable papers have yet to contribute directly or indirectly to the expansion of the vast computer vision AI field. Looking into the future",
        "type": "Document"
      },
      {
        "id": "c8e7c7fe-d68d-489a-942a-7ed4241f5229",
        "metadata": {
          "vector_store_key": "1910.11769-2",
          "chunk_id": 20,
          "document_id": "1910.11769",
          "start_idx": 7771,
          "end_idx": 8258
        },
        "page_content": "-fold cross validation for each technique. We provide a brief overview of each benchmark experiment below. Among all of the benchmarks, Bidirectional Encoder Representations from Transformers (BERT) BIBREF11 achieved the best performance with a 0.604 micro-F1 score. Overall, we observed that deep-learning based techniques performed better than lexical based methods. This suggests that a method which attends to context and themes could do well on the dataset. We computed bag-of-words",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the state of the art?",
    "llm_answer": "The context discusses various machine learning methods, including deep learning, for tasks like named entity recognition (NER) and acoustic speech processing, but does not explicitly define a \"state of the art.\"\n",
    "context": [
      {
        "id": "577fb06e-9340-4422-93f3-91bc89574aee",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 43,
          "document_id": "2001.00137",
          "start_idx": 16781,
          "end_idx": 17197
        },
        "page_content": " the online platforms. Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector",
        "type": "Document"
      },
      {
        "id": "7ddbeceb-93d9-45f3-875f-c9f53efca5ae",
        "metadata": {
          "vector_store_key": "1909.02764-5",
          "chunk_id": 14,
          "document_id": "1909.02764",
          "start_idx": 5772,
          "end_idx": 6281
        },
        "page_content": "2004automatic compare local and global features in support vector machines. Next to such discriminative approaches, hidden Markov models are well-studied, however, there is no agreement on which feature-based classifier is most suitable BIBREF13. Similar to the facial expression modality, recent efforts on applying deep learning have been increased for acoustic speech processing. For instance, lee2015high use a recurrent neural network and palaz2015analysis apply a convolutional neural network to the raw",
        "type": "Document"
      },
      {
        "id": "2f779d71-2179-4ca0-89f3-60ca55105db9",
        "metadata": {
          "vector_store_key": "1911.03894-1",
          "chunk_id": 63,
          "document_id": "1911.03894",
          "start_idx": 22277,
          "end_idx": 22771
        },
        "page_content": " many other languages. We will publish an updated version in the near future where we will explore and release models trained for longer, with additional downstream tasks, baselines (e.g. XLM) and analysis, we will also train additional models with potentially cleaner corpora such as CCNet BIBREF56 for more accurate performance evaluation and more complete ablation. This work was partly funded by three French National grants from the Agence Nationale de la Recherche, namely projects PARSIT",
        "type": "Document"
      },
      {
        "id": "30fae7be-f340-4047-8bcf-82ec20a73ca2",
        "metadata": {
          "vector_store_key": "1911.07228-7",
          "chunk_id": 10,
          "document_id": "1911.07228",
          "start_idx": 2445,
          "end_idx": 2848
        },
        "page_content": " used traditional machine learning methods such as Maximum Entropy Markov Model (MEMM), Support Vector Machine (SVM) and Conditional Random Field (CRF). In particular, most of the toolkits for NER task attempted to use MEMM BIBREF4, and CRF BIBREF5 to solve this problem. Nowadays, because of the increase in data, DNN methods are used a lot. They have archived great results when it comes to NER tasks,",
        "type": "Document"
      },
      {
        "id": "03c6eff5-bcbd-464b-bcfb-8ddf8ef79561",
        "metadata": {
          "vector_store_key": "1912.13109-3",
          "chunk_id": 38,
          "document_id": "1912.13109",
          "start_idx": 15008,
          "end_idx": 15448
        },
        "page_content": "box user\u2019s guide The MathWorks Incs [10] Chollet, Fran\u00e7ois and others Keras: The python deep learning library Astrophysics Source Code Library [11] Wei, Jason and Zou, Kai EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What difficulties does sentiment analysis on Twitter have, compared to sentiment analysis in other domains?",
    "llm_answer": "The unavailability of suitable datasets and lexicons for system training, development, and testing, and initially small and proprietary Twitter-specific resources.\n",
    "context": [
      {
        "id": "93121e4e-5518-4a33-879a-53ce53061c68",
        "metadata": {
          "vector_store_key": "1710.01492-1",
          "chunk_id": 7,
          "document_id": "1710.01492",
          "start_idx": 3007,
          "end_idx": 3411
        },
        "page_content": "e., a sentence or a headline rather than a full document. How to handle such challenges has only recently been the subject of thorough research BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 . Sentiment analysis has a wide number of applications in areas such as market research, political and social sciences, and for studying public opinion in general, and Twitter is one",
        "type": "Document"
      },
      {
        "id": "535fed0e-2d6d-4dbb-a8d6-c49d08040a76",
        "metadata": {
          "vector_store_key": "1710.01492-1",
          "chunk_id": 8,
          "document_id": "1710.01492",
          "start_idx": 3204,
          "end_idx": 3704
        },
        "page_content": " market research, political and social sciences, and for studying public opinion in general, and Twitter is one of the most commonly-used platforms for this. This is due to its streaming nature, which allows for real-time analysis, to its social aspect, which encourages people to share opinions, and to the short size of the tweets, which simplifies linguistic analysis. There are several formulations of the task of Sentiment Analysis on Twitter that look at different sizes of the target (e.g., at",
        "type": "Document"
      },
      {
        "id": "68713581-9e68-4298-8e1e-0a9ef8d3f2c7",
        "metadata": {
          "vector_store_key": "1710.01492-1",
          "chunk_id": 0,
          "document_id": "1710.01492",
          "start_idx": 0,
          "end_idx": 547
        },
        "page_content": "Microblog sentiment analysis; Twitter opinion mining Sentiment Analysis: This is text analysis aiming to determine the attitude of a speaker or a writer with respect to some topic or the overall contextual polarity of a piece of text. Sentiment analysis on Twitter is the use of natural language processing techniques to identify and categorize opinions expressed in a tweet, in order to determine the author's attitude toward a particular topic or in general. Typically, discrete labels such as positive, negative, neutral, and objective are used",
        "type": "Document"
      },
      {
        "id": "b20e6013-593b-426b-8a86-8af2ad53f1fc",
        "metadata": {
          "vector_store_key": "1710.01492-1",
          "chunk_id": 17,
          "document_id": "1710.01492",
          "start_idx": 6828,
          "end_idx": 7363
        },
        "page_content": ", which have proven to be very valuable for their respective domains and types of texts, but less useful for short social media messages. Later, with the emergence of social media, sentiment analysis in Twitter became a hot research topic. Unfortunately, research in that direction was hindered by the unavailability of suitable datasets and lexicons for system training, development, and testing. While some Twitter-specific resources were developed, initially they were either small and proprietary, such as the i-sieve corpus BIBREF",
        "type": "Document"
      },
      {
        "id": "aba2bae8-95f3-4a32-864d-bb9fe1608e0e",
        "metadata": {
          "vector_store_key": "1710.01492-1",
          "chunk_id": 9,
          "document_id": "1710.01492",
          "start_idx": 3704,
          "end_idx": 4099
        },
        "page_content": " of Sentiment Analysis on Twitter that look at different sizes of the target (e.g., at the level of words vs. phrases vs. tweets vs. sets of tweets), at different types of semantic targets (e.g., aspect vs. topic vs. overall tweet), at the explicitness of the target (e.g., sentiment vs. stance detection), at the scale of the expected label (2-point vs. 3-point vs. ordinal), etc. All these are",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many sentence transformations on average are available per unique sentence in dataset?",
    "llm_answer": "15\n",
    "context": [
      {
        "id": "575dbf7c-f0aa-413b-841e-f1e8cc14f410",
        "metadata": {
          "vector_store_key": "1912.01673-1",
          "chunk_id": 29,
          "document_id": "1912.01673",
          "start_idx": 11172,
          "end_idx": 11741
        },
        "page_content": " most of the transformations are likely to lead to a large number possible outcomes. As documented in scratching2013, Czech sentence might have hundreds of thousand of paraphrases. To support some minimal exploration of this possible diversity, most of sentences were assigned to several annotators. The annotation is a challenging task and the annotators naturally make mistakes. Unfortunately, a single typo can significantly influence the resulting embedding BIBREF26. After collecting all the sentence variations, we applied the statistical spellchecker and grammar",
        "type": "Document"
      },
      {
        "id": "564e9fd0-6135-4b6f-90a5-e40b91754b5d",
        "metadata": {
          "vector_store_key": "1912.01673-1",
          "chunk_id": 32,
          "document_id": "1912.01673",
          "start_idx": 12286,
          "end_idx": 12785
        },
        "page_content": ". Out of the 4262 distinct sentences, only 188 was recorded more than once. In other words, the chance of two annotators producing the same output string is quite low. The most repeated transformations are by far past, future and ban. The least repeated is paraphrase with only single one repeated. multiple-annots documents this in another way. The 293 annotations are split into groups depending on how many annotators saw the same input sentence: 30 annotations were annotated by one person only,",
        "type": "Document"
      },
      {
        "id": "239466a2-6f12-43f3-9449-502aab04f3ee",
        "metadata": {
          "vector_store_key": "1912.01673-3",
          "chunk_id": 31,
          "document_id": "1912.01673",
          "start_idx": 12000,
          "end_idx": 12463
        },
        "page_content": "ktor, there are 4262 unique sentences (including 150 seed sentences) that form the COSTRA 1.0 dataset. Statistics of individual annotators are available in tab:statistics. The time needed to carry out one piece of annotation (i.e. to provide one seed sentence with all 15 transformations) was on average almost 20 minutes but some annotators easily needed even half an hour. Out of the 4262 distinct sentences, only 188 was recorded more than once. In other words",
        "type": "Document"
      },
      {
        "id": "c58455a9-02a9-4e22-b300-6cea99f09232",
        "metadata": {
          "vector_store_key": "1912.01673-3",
          "chunk_id": 5,
          "document_id": "1912.01673",
          "start_idx": 1987,
          "end_idx": 2499
        },
        "page_content": " in sentences. In this work, we present COSTRA, a new dataset of COmplex Sentence TRAnsformations. In its first version, the dataset is limited to sample sentences in Czech. The goal is to support studies of semantic and syntactic relations between sentences in the continuous space. Our dataset is the prerequisite for one of possible ways of exploring sentence meaning relatability: we envision that the continuous space of sentences induced by an ideal embedding method would exhibit topological similarity to",
        "type": "Document"
      },
      {
        "id": "a76c3840-6102-4c6a-a010-6b4e64f73a95",
        "metadata": {
          "vector_store_key": "1912.01673-3",
          "chunk_id": 34,
          "document_id": "1912.01673",
          "start_idx": 13062,
          "end_idx": 13535
        },
        "page_content": " only 2%). It was also a case of 7 annotators only; the remaining 5 annotators were capable of producing all requested transformations. The top three transformations considered unfeasible were different meaning (using the same set of words), past (esp. for sentences already in the past tense) and simple sentence. We embedded COSTRA sentences with LASER BIBREF15, the method that performed very well in revealing linear relations in BaBo2019. Having browsed a number of 2D",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What annotations are available in the dataset?",
    "llm_answer": "Participant labels, BIO format (Inside, Outside, Beginning) labels, user states, system states, and dialogue acts.\n",
    "context": [
      {
        "id": "9d180fa4-0178-4f9f-8997-37024ccf17f9",
        "metadata": {
          "vector_store_key": "1909.00430-8",
          "chunk_id": 0,
          "document_id": "1909.00430",
          "start_idx": 0,
          "end_idx": 440
        },
        "page_content": "Data annotation is a key bottleneck in many data driven algorithms. Specifically, deep learning models, which became a prominent tool in many data driven tasks in recent years, require large datasets to work well. However, many tasks require manual annotations which are relatively hard to obtain at scale. An attractive alternative is lightly supervised learning BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF",
        "type": "Document"
      },
      {
        "id": "d4c4849a-b0a6-4931-82a7-aed67b41eb03",
        "metadata": {
          "vector_store_key": "1703.05260-7",
          "chunk_id": 19,
          "document_id": "1703.05260",
          "start_idx": 7578,
          "end_idx": 8108
        },
        "page_content": ". This section deals with the annotation of the data. We first describe the final annotation schema. Then, we describe the iterative process of corpus annotation and the refinement of the schema. This refinement was necessary due to the complexity of the annotation. For each of the scenarios, we designed a specific annotation template. A script template consists of scenario-specific event and participant labels. An example of a template is shown in Table 1 . All NP heads in the corpus were annotated with a participant label;",
        "type": "Document"
      },
      {
        "id": "3b541d42-de69-4c8c-97f5-634e76f12a94",
        "metadata": {
          "vector_store_key": "1911.07228-1",
          "chunk_id": 54,
          "document_id": "1911.07228",
          "start_idx": 14818,
          "end_idx": 15154
        },
        "page_content": " BIO format (Inside, Outside, Beginning) BIBREF6 at the top layer (CRF). If we can unify the labeling of the annotators between the train, dev and test sets. We will improve data quality and classifier. It is better if there is a pre-trained word embeddings that overlays the data, and segmentation algorithm need to be more accurately.",
        "type": "Document"
      },
      {
        "id": "f9e9e3ec-e213-4cd5-ae8e-8d986c9fd2db",
        "metadata": {
          "vector_store_key": "2002.11893-1",
          "chunk_id": 22,
          "document_id": "2002.11893",
          "start_idx": 8390,
          "end_idx": 8922
        },
        "page_content": " before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states. Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three",
        "type": "Document"
      },
      {
        "id": "41b33e3c-7622-4a3a-bf41-e8b160505f06",
        "metadata": {
          "vector_store_key": "1910.11769-4",
          "chunk_id": 13,
          "document_id": "1910.11769",
          "start_idx": 4963,
          "end_idx": 5497
        },
        "page_content": " using in-house data annotators who are experts in narrative content. A passage is accepted as valid if the in-house annotator's label matched any one of the MTurk annotators' labels. The remaining passages are discarded. We provide the fraction of annotator agreement for each label in the dataset. Though passages may lose some emotional context when read independently of the complete narrative, we believe annotator agreement on our dataset supports the assertion that small excerpts can still convey coherent emotions. During the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are possible sentence transformations represented in dataset, as new sentences?",
    "llm_answer": "The context states that sentence variations, likely paraphrases, are collected and, after spell/grammar checking, are added to the dataset as new sentences.\n",
    "context": [
      {
        "id": "575dbf7c-f0aa-413b-841e-f1e8cc14f410",
        "metadata": {
          "vector_store_key": "1912.01673-1",
          "chunk_id": 29,
          "document_id": "1912.01673",
          "start_idx": 11172,
          "end_idx": 11741
        },
        "page_content": " most of the transformations are likely to lead to a large number possible outcomes. As documented in scratching2013, Czech sentence might have hundreds of thousand of paraphrases. To support some minimal exploration of this possible diversity, most of sentences were assigned to several annotators. The annotation is a challenging task and the annotators naturally make mistakes. Unfortunately, a single typo can significantly influence the resulting embedding BIBREF26. After collecting all the sentence variations, we applied the statistical spellchecker and grammar",
        "type": "Document"
      },
      {
        "id": "21e77724-8c95-46fd-b1c3-62b430f495c8",
        "metadata": {
          "vector_store_key": "1912.01673-0",
          "chunk_id": 8,
          "document_id": "1912.01673",
          "start_idx": 3172,
          "end_idx": 3674
        },
        "page_content": " behaviour to emerge, as it happened for word vector operations, but regardless if the behaviour is emergent or trained, we need a dataset of sentences illustrating these patterns. If large enough, such a dataset could serve for training. If it will be smaller, it will provide a test set. In either case, these sentences could provide a \u201cskeleton\u201d to the continuous space of sentence embeddings. The paper is structured as follows: related summarizes existing methods of sentence embeddings evaluation",
        "type": "Document"
      },
      {
        "id": "2aad297e-653a-407f-8112-cd84261bf66f",
        "metadata": {
          "vector_store_key": "1908.06606-6",
          "chunk_id": 32,
          "document_id": "1908.06606",
          "start_idx": 11284,
          "end_idx": 11795
        },
        "page_content": "i$. To tell the model $Q$ and $X$ is two different sentence, a sentence type input is generated which is a binary label sequence to denote what sentence each character in the input belongs to. Positional encoding and mask matrix is also constructed automatically to bring in absolute position information and eliminate the impact of zero padding respectively. Then a hidden vector $V_s$ which contains both query and text information is generated through BERT-base model. Since BERT is trained on general corpus",
        "type": "Document"
      },
      {
        "id": "cc1c57ec-477b-42f6-b7cb-09485dc0ba4f",
        "metadata": {
          "vector_store_key": "1912.01673-0",
          "chunk_id": 7,
          "document_id": "1912.01673",
          "start_idx": 2877,
          "end_idx": 3401
        },
        "page_content": " the previous ones in multiple sentences could be partially or linearly ordered according to the strength of the speakers confidence in the claim. Our long term goal is to search for an embedding method which exhibits this behaviour, i.e. that the topological map of the embedding space corresponds to meaningful operations or changes in the set of sentences of a language (or more languages at once). We prefer this behaviour to emerge, as it happened for word vector operations, but regardless if the behaviour is emergent",
        "type": "Document"
      },
      {
        "id": "b47eb24e-554c-49db-afc1-a49279bccaa0",
        "metadata": {
          "vector_store_key": "2001.00137-7",
          "chunk_id": 17,
          "document_id": "2001.00137",
          "start_idx": 7115,
          "end_idx": 7610
        },
        "page_content": " by an embedding layer used for input representation, with the final input embedding being a sum of token embedddings, segmentation embeddings and position embeddings. The first one, token embedding layer, uses a vocabulary dictionary to convert each token into a more representative embedding. The segmentation embedding layer indicates which tokens constitute a sentence by signaling either 1 or 0. In our case, since our data are formed of single sentences, the segment is 1 until the first `",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are all 15 types of modifications ilustrated in the dataset?",
    "llm_answer": "The text lists several modifications, but not a complete, numbered list of 15 types.  It mentions:  first round examples, change of word order, interrogative/imperative transformations, fairy-tale style, excessive use of diminutives/vulgarisms, dada writing.  The answer is not found in the context.\n",
    "context": [
      {
        "id": "a2203714-1e91-4b9e-8237-3e2afa02553d",
        "metadata": {
          "vector_store_key": "1912.01673-1",
          "chunk_id": 21,
          "document_id": "1912.01673",
          "start_idx": 8152,
          "end_idx": 8670
        },
        "page_content": " several of its possible alternations, see tab:firstroundexamples. Unfortunately, these examples turned out to be highly influential on the annotators' decisions and they correspond to almost two thirds of all of modifications gathered in the first round. Other very common transformations include change of a word order or transformation into a interrogative/imperative sentence. Other interesting modification were also proposed such as change into a fairy-tale style, excessive use of diminutives/vulgarisms or dada",
        "type": "Document"
      },
      {
        "id": "0ef3afdf-a68c-4d53-895f-0490c7b9672d",
        "metadata": {
          "vector_store_key": "1909.13375-1",
          "chunk_id": 66,
          "document_id": "1909.13375",
          "start_idx": 22494,
          "end_idx": 22947
        },
        "page_content": " writing, and not just on multi-span questions. In order to analyze the effect of each of our changes, we conduct ablation studies on the development set, depicted in Table TABREF26. Not using the simple preprocessing from Section SECREF17 resulted in a 2.5 point decrease in both EM and F1. The numeric questions were the most affected, with their performance dropping by 3.5 points. Given that number questions make up about 61% of the dataset, we can",
        "type": "Document"
      },
      {
        "id": "c023aeb7-e6d9-4a99-9070-1470fe069fd8",
        "metadata": {
          "vector_store_key": "1910.09399-1",
          "chunk_id": 112,
          "document_id": "1910.09399",
          "start_idx": 43485,
          "end_idx": 43906
        },
        "page_content": "REF8, BIBREF36, BIBREF39, BIBREF40, BIBREF41, BIBREF42, BIBREF22, BIBREF26. These datasets were first utilized on earlier works BIBREF8, and most of them sport modified features such image annotations, labels, or descriptions. The qualitative and quantitative results reported by researchers in this study were far superior of earlier works in this same field of computer vision AI. black In this subsection, we introduce",
        "type": "Document"
      },
      {
        "id": "1cc830ac-f4e2-46d9-9e7d-f8704a5cf375",
        "metadata": {
          "vector_store_key": "1703.04617-0",
          "chunk_id": 9,
          "document_id": "1703.04617",
          "start_idx": 3555,
          "end_idx": 4085
        },
        "page_content": " has the potential to capture long-distance interactions over the structures. Different types of questions are often used to seek for different types of information. For example, a \"what\" question could have very different property from that of a \"why\" question, while they may share information and need to be trained together instead of separately. We view this as a \"adaptation\" problem to let different types of questions share a basic model but still discriminate them when needed. Specifically, we are motivated by the ideas",
        "type": "Document"
      },
      {
        "id": "d381678e-e3fe-439f-addf-388c74d1df0c",
        "metadata": {
          "vector_store_key": "1905.00563-7",
          "chunk_id": 4,
          "document_id": "1905.00563",
          "start_idx": 1447,
          "end_idx": 1903
        },
        "page_content": " prediction. As an example, consider the excerpt from a KG in Figure 1 with two observed facts, and a target predicted fact that Princes Henriette is the parent of Violante Bavaria. Our proposed graph perturbation, shown in Figure 1 , identifies the existing fact that Ferdinal Maria is the father of Violante Bavaria as the one when removed and model retrained, will change the prediction of Princes Henriette's child. We also study attacks that green!50!",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What were their results on the three datasets?",
    "llm_answer": "The provided text describes results on various datasets but does not explicitly list results on three specific datasets.  Results are given for TEC, AMMER, Figure8, ISEAR, and DROP.\n",
    "context": [
      {
        "id": "6c83f86c-6174-4eb6-9420-b21f6379cd47",
        "metadata": {
          "vector_store_key": "1707.03904-4",
          "chunk_id": 37,
          "document_id": "1707.03904",
          "start_idx": 13913,
          "end_idx": 14370
        },
        "page_content": " human testers were penalized for entering \u201c0\u201d as answer instead of \u201czero\u201d. However, a comparison between systems may still be meaningful. To put the difficulty of the introduced datasets into perspective, we evaluated human performance on answering the questions. For each dataset, we recruited one domain expert (a developer with several years of programming experience for Quasar-S, and an avid trivia enthusiast for Quasar-T) and $1-3$ non-experts. Each",
        "type": "Document"
      },
      {
        "id": "b0d78a46-0cfa-4367-a963-2a9a8d82c9a4",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 18,
          "document_id": "1910.04269",
          "start_idx": 6853,
          "end_idx": 7342
        },
        "page_content": " comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (",
        "type": "Document"
      },
      {
        "id": "31e3f6a2-9e9b-4d6d-824a-22195518284a",
        "metadata": {
          "vector_store_key": "1909.02764-7",
          "chunk_id": 52,
          "document_id": "1909.02764",
          "start_idx": 20420,
          "end_idx": 20841
        },
        "page_content": "=73 % on TEC. The model trained on our AMMER corpus achieves an F1 score of 57%. This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set. These results are mostly in line with Bostan2018. Now we analyze how well the models trained in Experiment 1 perform when applied to our data set. The results are shown in column \u201cSimple\u201d in Table TAB",
        "type": "Document"
      },
      {
        "id": "c03315b1-7d9a-4a9a-a552-f7259ef1288c",
        "metadata": {
          "vector_store_key": "1909.02764-7",
          "chunk_id": 53,
          "document_id": "1909.02764",
          "start_idx": 20698,
          "end_idx": 21118
        },
        "page_content": " to our data set. The results are shown in column \u201cSimple\u201d in Table TABREF19. We observe a clear drop in performance, with an average of F$_1$=48 %. The best performing model is again the one trained on TEC, en par with the one trained on the Figure8 data. The model trained on ISEAR performs second best in Experiment 1, it performs worst in Experiment 2. To adapt models trained on previously existing data sets to our",
        "type": "Document"
      },
      {
        "id": "555a266e-0a08-43e5-b719-8cd2fd10ebb8",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What semantic rules are proposed?",
    "llm_answer": "Ten semantic rules are proposed, but only five are used in the presented method.\n",
    "context": [
      {
        "id": "257096b4-d047-4afa-905d-fad74659fcaa",
        "metadata": {
          "vector_store_key": "1706.08032-2",
          "chunk_id": 21,
          "document_id": "1706.08032",
          "start_idx": 8281,
          "end_idx": 8786
        },
        "page_content": " and devised ten semantic rules in their hybrid approach based on the presentation of BIBREF16 . We use five rules in the semantic rules set because other five rules are only used to compute polarity of words after POS tagging or Parsing steps. We follow the same naming convention for rules utilized by BIBREF11 to represent the rules utilized in our proposed method. The rules utilized in the proposed method are displayed in Table TABREF15 in which is included examples from STS Corpus and output after",
        "type": "Document"
      },
      {
        "id": "b7f97ae4-10c4-4e1c-b7b5-1451a463ec18",
        "metadata": {
          "vector_store_key": "1909.12140-2",
          "chunk_id": 5,
          "document_id": "1909.12140",
          "start_idx": 2277,
          "end_idx": 2783
        },
        "page_content": " These patterns were heuristically determined in a comprehensive linguistic analysis and encode syntactic and lexical features that can be derived from a sentence's parse tree. Each rule specifies (1) how to split up and rephrase the input into structurally simplified sentences and (2) how to set up a semantic hierarchy between them. They are recursively applied on a given source sentence in a top-down fashion. When no more rule matches, the algorithm stops and returns the generated discourse tree. In",
        "type": "Document"
      },
      {
        "id": "83cb2c89-48d8-4515-b35f-14bea3e3b95b",
        "metadata": {
          "vector_store_key": "1909.12140-2",
          "chunk_id": 6,
          "document_id": "1909.12140",
          "start_idx": 2552,
          "end_idx": 3098
        },
        "page_content": "down fashion. When no more rule matches, the algorithm stops and returns the generated discourse tree. In a first step, source sentences that present a complex linguistic form are turned into clean, compact structures by decomposing clausal and phrasal components. For this purpose, the transformation rules encode both the splitting points and rephrasing procedure for reconstructing proper sentences. Each split will create two or more sentences with a simplified syntax. To establish a semantic hierarchy between them, two subtasks are carried",
        "type": "Document"
      },
      {
        "id": "a04618f1-6174-4fb2-bd7a-ec74326b9c51",
        "metadata": {
          "vector_store_key": "1710.01492-8",
          "chunk_id": 10,
          "document_id": "1710.01492",
          "start_idx": 3953,
          "end_idx": 4458
        },
        "page_content": " label (2-point vs. 3-point vs. ordinal), etc. All these are explored at SemEval, the International Workshop on Semantic Evaluation, which has created a number of benchmark datasets and has enabled direct comparison between different systems and approaches, both as part of the competition and beyond. Traditionally, the task has been addressed using supervised and semi-supervised methods, as well as using distant supervision, with the most important resource being sentiment polarity lexicons, and with",
        "type": "Document"
      },
      {
        "id": "49a72940-6564-407a-b0f0-7bd606d37ddb",
        "metadata": {
          "vector_store_key": "1810.06743-0",
          "chunk_id": 11,
          "document_id": "1810.06743",
          "start_idx": 4304,
          "end_idx": 4751
        },
        "page_content": " in tab:ptb. A morphosyntactic schema prescribes how language can be annotated\u2014giving stricter categories than our simple labels for prove\u2014and can vary in the level of detail provided. Part of speech tags are an example of a very coarse schema, ignoring details of person, gender, and number. A slightly finer-grained schema for English is the Penn Treebank tagset BIBREF6 , which includes signals for English morphology. For instance, its VBZ tag",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance of the model?",
    "llm_answer": "The model trained on the AMMER corpus achieves an F1 score of 57%.  A different model achieves 68.73%/77.74% on EM and F1, respectively, and 69.10%/78.38% on EM and F1 for a larger value of K.\n",
    "context": [
      {
        "id": "31e3f6a2-9e9b-4d6d-824a-22195518284a",
        "metadata": {
          "vector_store_key": "1909.02764-7",
          "chunk_id": 52,
          "document_id": "1909.02764",
          "start_idx": 20420,
          "end_idx": 20841
        },
        "page_content": "=73 % on TEC. The model trained on our AMMER corpus achieves an F1 score of 57%. This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set. These results are mostly in line with Bostan2018. Now we analyze how well the models trained in Experiment 1 perform when applied to our data set. The results are shown in column \u201cSimple\u201d in Table TAB",
        "type": "Document"
      },
      {
        "id": "555a266e-0a08-43e5-b719-8cd2fd10ebb8",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "3c3e7bd6-8955-4353-bdf3-c8d8c69b5e4d",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 59,
          "document_id": "1909.13375",
          "start_idx": 20052,
          "end_idx": 20444
        },
        "page_content": "s. For the large models we used a batch size of 3 with a learning rate of $5e^{-6}$ and trained for 5 epochs, except for the model without the single-span heads that was trained with a batch size of 2 for 7 epochs. F1 was used as our validation metric. All models were trained on a single GPU with 12-16GB of memory. Table TABREF24 shows the results on DROP's development set. Compared to our",
        "type": "Document"
      },
      {
        "id": "638d3f9b-929b-4fa6-b0d4-efe44cd66255",
        "metadata": {
          "vector_store_key": "1912.13109-2",
          "chunk_id": 25,
          "document_id": "1912.13109",
          "start_idx": 10203,
          "end_idx": 10681
        },
        "page_content": " metrics - Recall rate, F1 score and Overall accuracy. Choice of model parameters were in the above models were inspired from previous work done but then were tuned to the best performance of the Test dataset. Following parameters were considered for tuning. Learning rate: Based on grid search the best performance was achieved when learning rate was set to 0.01. This value was arrived by a grid search on lr parameter. Number of Bidirectional LSTM units: A set of 32, 64, 128",
        "type": "Document"
      },
      {
        "id": "80c384ae-e7f9-42fa-aeb3-0ed89754c63e",
        "metadata": {
          "vector_store_key": "1703.04617-7",
          "chunk_id": 86,
          "document_id": "1703.04617",
          "start_idx": 23354,
          "end_idx": 23758
        },
        "page_content": " be 20, the performance improves to 68.73%/77.74% on EM and F1, respectively, which corresponds to the results of our model reported in Table 1 . Furthermore, after submitted our result, we have experimented with a large value of $K$ and found that when $K=100$ , we can achieve a better performance of 69.10%/78.38% on the development set. Figure UID61 shows the EM/F1 scores of different question types",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was the performance of both approaches on their dataset?",
    "llm_answer": "The context describes experiments with different models and datasets, but does not give overall performance results for all approaches on their datasets.  It discusses specific results for different model components and tasks, but does not provide a summary of how well each approach performed overall.\n",
    "context": [
      {
        "id": "0c3dc473-e6e5-400c-b3a1-75c887dbe5e3",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 28,
          "document_id": "1909.00542",
          "start_idx": 10682,
          "end_idx": 11217
        },
        "page_content": " experiments. We ran the version trained on Python ROUGE once, and the version trained on Perl twice. The two Perl runs have different results, and one of them clearly outperforms the Python run. However, given the differences of results between the two Perl runs we advice to re-run the experiments multiple times and obtain the mean and standard deviation of the runs before concluding whether there is any statistical difference between the results. But it seems that there may be an improvement of the final evaluation results when",
        "type": "Document"
      },
      {
        "id": "04fe1abe-a675-484e-963d-9d80c75d505e",
        "metadata": {
          "vector_store_key": "1908.10449-2",
          "chunk_id": 66,
          "document_id": "1908.10449",
          "start_idx": 22880,
          "end_idx": 23381
        },
        "page_content": " hard time learning in this setting. As shown in Table TABREF16, both datasets have a vocabulary size of more than 100k. This is much larger than in the other two settings, where on average the length of questions is around 10. This suggests that the methods with better sample efficiency are needed to act in more realistic problem settings with huge action spaces. Experiments also show that a larger memory slot size always helps. Intuitively, with a memory mechanism (either implicit or explicit),",
        "type": "Document"
      },
      {
        "id": "42150962-2d02-417b-b4fa-d948c244b2d5",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 58,
          "document_id": "1909.13375",
          "start_idx": 19611,
          "end_idx": 20052
        },
        "page_content": " to gain further insights on the effect of the components that we ourselves introduced. For ease of performance comparison, we followed NABERT+'s training settings: we used the BERT Adam optimizer from huggingface with default settings and a learning rate of $1e^{-5}$. The only difference was that we used a batch size of 12. We trained our base model for 20 epochs. For the large models we used a batch size of 3 with a learning rate of $5",
        "type": "Document"
      },
      {
        "id": "11d9b2e6-e20d-4ab9-8d7c-c35a7e2d27e6",
        "metadata": {
          "vector_store_key": "1603.07044-6",
          "chunk_id": 43,
          "document_id": "1603.07044",
          "start_idx": 14997,
          "end_idx": 15515
        },
        "page_content": " task A already had a substantial amount of training data. Table 4 gives the final comparison between different models (we only list the MAP score because it is the official score for the challenge). Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited",
        "type": "Document"
      },
      {
        "id": "8f0169f9-d118-414b-bb28-c12c53b30466",
        "metadata": {
          "vector_store_key": "2002.11893-2",
          "chunk_id": 72,
          "document_id": "2002.11893",
          "start_idx": 26889,
          "end_idx": 27376
        },
        "page_content": " system often records full user constraints, while the last one records relaxed constraints for recommendation. Thus the last one involves system policy, which is out of the scope of state tracking. We used the first query for these models and left state tracking with recommendation for future work. Result Analysis: We evaluated the joint state accuracy (percentage of exact matching) of these two models (Table TABREF31). TRADE, the state-of-the-art model on MultiWOZ, performs poorly",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What genres are covered?",
    "llm_answer": "mechanics, thermodynamics, poetry, art, Shakespeare, dance, literature, novel, symphony, drama, orchestra, music, ballet, arts, creative, sculpture,  coming-of-age, strong-female-lead, LGBTQ+\n",
    "context": [
      {
        "id": "574096df-94ad-4986-88e3-cb625ee57ab8",
        "metadata": {
          "vector_store_key": "1910.14497-3",
          "chunk_id": 64,
          "document_id": "1910.14497",
          "start_idx": 18963,
          "end_idx": 19236
        },
        "page_content": "mechanics\", \"thermodynamics\" Y: \"poetry\", \"art\", \"Shakespeare\", \"dance\", \"literature\", \"novel\", \"symphony\", \"drama\", \"orchestra\", \"music\", \"ballet\", \"arts\", \"creative\", \"sculpture\" A: \"brother\", \"father\", \"uncle\", \"grandfather\", \"son\", \"he\", \"his\", \"him\", \"man\", \"himself\",",
        "type": "Document"
      },
      {
        "id": "781dd9b5-a993-49a8-a07e-286f9dca7291",
        "metadata": {
          "vector_store_key": "1910.14497-2",
          "chunk_id": 52,
          "document_id": "1910.14497",
          "start_idx": 17148,
          "end_idx": 17415
        },
        "page_content": "agony\", \"prison\" Instruments vs Weapons / Pleasant vs Unpleasant: X: \"bagpipe\", \"cello\", \"guitar\", \"lute\", \"trombone\", \"banjo\", \"clarinet\", \"harmonica\", \"mandolin\", \"trumpet\", \"bassoon\", \"drum\", \"harp\", \"oboe\", \"tuba\", \"bell\", \"fiddle\", \"harpsichord\", \"piano\", \"viola",
        "type": "Document"
      },
      {
        "id": "5f66f4e7-c7a2-4244-bc1b-51f74abf9a6d",
        "metadata": {
          "vector_store_key": "1911.01799-0",
          "chunk_id": 8,
          "document_id": "1911.01799",
          "start_idx": 3245,
          "end_idx": 3755
        },
        "page_content": " advertisement. The speech of a particular speaker may be in more than 5 genres. As a comparison, most of the utterances in VoxCeleb were extracted from interview videos. The diversity in genres makes our database more representative for the true scenarios in unconstrained conditions, but also more challenging. CN-Celeb is not fully automated, but involves human check. We found that more complex the genre is, more errors the automated pipeline tends to produce. Ironically, the error-pron segments could be",
        "type": "Document"
      },
      {
        "id": "d52e3029-3fdd-41c9-803a-8e158380e00a",
        "metadata": {
          "vector_store_key": "1910.14497-2",
          "chunk_id": 53,
          "document_id": "1910.14497",
          "start_idx": 17148,
          "end_idx": 17391
        },
        "page_content": " \"bell\", \"fiddle\", \"harpsichord\", \"piano\", \"viola\", \"bongo\", \"flute\", \"horn\", \"saxophone\", \"violin\" Y: \"arrow\", \"club\", \"gun\", \"missile\", \"spear\", \"ax\", \"dagger\", \"harpoon\", \"pistol\", \"sword\", \"blade\", \"dynamite\", \"hatchet\", \"rifle\", \"tank\", \"",
        "type": "Document"
      },
      {
        "id": "fe060448-98fd-4d8a-92da-6caf9813de4c",
        "metadata": {
          "vector_store_key": "1910.11769-4",
          "chunk_id": 16,
          "document_id": "1910.11769",
          "start_idx": 6259,
          "end_idx": 6704
        },
        "page_content": " 1600 unique titles across multiple categories, including 88 titles (1520 passages) from Project Gutenberg. All of the modern narratives were written after the year 2000, with notable amount of themes in coming-of-age, strong-female-lead, and LGBTQ+. The genre distribution is listed in Table TABREF8. In the final dataset, 21.0% of the data has consensus between all annotators, 73.5% has majority agreement, and 5.48% has labels assigned after",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which of the two speech recognition models works better overall on CN-Celeb?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "568b59a1-333a-46c9-a7b3-102cd84cb190",
        "metadata": {
          "vector_store_key": "1911.01799-0",
          "chunk_id": 40,
          "document_id": "1911.01799",
          "start_idx": 14641,
          "end_idx": 15051
        },
        "page_content": " with re-training the back-end model with VoxCeleb(L) (4th row), the performance on SITW becomes better than the same-source result on CN-Celeb(E) (11.34% vs 14.24%). All these results reconfirmed the significant difference between the two datasets, and indicates that CN-Celeb is more challenging than VoxCeleb. We introduced a free dataset CN-Celeb for speaker recognition research. The dataset contains more",
        "type": "Document"
      },
      {
        "id": "3b10c46d-f296-425e-a85b-1bc18846c602",
        "metadata": {
          "vector_store_key": "1911.01799-0",
          "chunk_id": 42,
          "document_id": "1911.01799",
          "start_idx": 15300,
          "end_idx": 15674
        },
        "page_content": "art speaker recognition models. Experimental results demonstrated that CN-Celeb is significantly different from VoxCeleb, and it is more challenging for speaker recognition research. The EER performance we obtained in this paper suggests that in unconstrained conditions, the performance of the current speaker recognition techniques might be much worse than it was thought.",
        "type": "Document"
      },
      {
        "id": "1859002a-da9d-435c-aec3-71e0d857eb48",
        "metadata": {
          "vector_store_key": "1911.01799-0",
          "chunk_id": 41,
          "document_id": "1911.01799",
          "start_idx": 15051,
          "end_idx": 15520
        },
        "page_content": "b. We introduced a free dataset CN-Celeb for speaker recognition research. The dataset contains more than $130k$ utterances from $1,000$ Chinese celebrities, and covers 11 different genres in real world. We compared CN-Celeb and VoxCeleb, a widely used dataset in speaker recognition, by setting up a series of experiments based on two state-of-the-art speaker recognition models. Experimental results demonstrated that CN-Celeb is significantly different from VoxCeleb",
        "type": "Document"
      },
      {
        "id": "e9c293d1-55da-4a1f-aaf1-8e332a3f7ef7",
        "metadata": {
          "vector_store_key": "1911.01799-0",
          "chunk_id": 37,
          "document_id": "1911.01799",
          "start_idx": 13776,
          "end_idx": 14166
        },
        "page_content": " source (SITW). To further compare CN-Celeb and VoxCeleb in a quantitative way, we built systems based on CN-Celeb and VoxCeleb, respectively. For a fair comparison, we randomly sampled 800 speakers from VoxCeleb and built a new dataset VoxCeleb(L) whose size is comparable to CN-Celeb(T). This data set was used for back-end (LDA-PLDA) training. The experimental results are shown in Table",
        "type": "Document"
      },
      {
        "id": "c811eaec-b309-42c8-aa95-87c2a5a5f1f0",
        "metadata": {
          "vector_store_key": "1911.01799-0",
          "chunk_id": 13,
          "document_id": "1911.01799",
          "start_idx": 5298,
          "end_idx": 5793
        },
        "page_content": " between CN-Celeb and VoxCeleb. Compared to VoxCeleb, CN-Celeb is a more complex dataset and more challenging for speaker recognition research. More details of these challenges are as follows. Most of the utterances involve real-world noise, including ambient noise, background babbling, music, cheers and laugh. A certain amount of utterances involve strong and overlapped background speakers, especially in the dram and movie genres. Most of speakers have different genres of utterances, which",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much is performance on CN-Celeb inferior to performance on VoxCeleb?",
    "llm_answer": "14.24% (CN-Celeb) vs 11.34% (VoxCeleb)\n",
    "context": [
      {
        "id": "363731f8-e3b9-4353-b9dd-bf5a1b274ba8",
        "metadata": {
          "vector_store_key": "1911.01799-1",
          "chunk_id": 36,
          "document_id": "1911.01799",
          "start_idx": 13328,
          "end_idx": 13776
        },
        "page_content": "-vector system and the x-vector system, the performance on CN-Celeb(E) is much worse than the performance on SITW and SITW(S). This indicates that there is big difference between these two datasets. From another perspective, it demonstrates that the model trained with VoxCeleb does not generalize well, although it has achieved reasonable performance on data from a similar source (SITW). To further compare CN-Celeb and VoxCeleb in a quantitative",
        "type": "Document"
      },
      {
        "id": "e9c293d1-55da-4a1f-aaf1-8e332a3f7ef7",
        "metadata": {
          "vector_store_key": "1911.01799-0",
          "chunk_id": 37,
          "document_id": "1911.01799",
          "start_idx": 13776,
          "end_idx": 14166
        },
        "page_content": " source (SITW). To further compare CN-Celeb and VoxCeleb in a quantitative way, we built systems based on CN-Celeb and VoxCeleb, respectively. For a fair comparison, we randomly sampled 800 speakers from VoxCeleb and built a new dataset VoxCeleb(L) whose size is comparable to CN-Celeb(T). This data set was used for back-end (LDA-PLDA) training. The experimental results are shown in Table",
        "type": "Document"
      },
      {
        "id": "568b59a1-333a-46c9-a7b3-102cd84cb190",
        "metadata": {
          "vector_store_key": "1911.01799-0",
          "chunk_id": 40,
          "document_id": "1911.01799",
          "start_idx": 14641,
          "end_idx": 15051
        },
        "page_content": " with re-training the back-end model with VoxCeleb(L) (4th row), the performance on SITW becomes better than the same-source result on CN-Celeb(E) (11.34% vs 14.24%). All these results reconfirmed the significant difference between the two datasets, and indicates that CN-Celeb is more challenging than VoxCeleb. We introduced a free dataset CN-Celeb for speaker recognition research. The dataset contains more",
        "type": "Document"
      },
      {
        "id": "13134e54-18d9-43db-8a4b-c333dd5083f5",
        "metadata": {
          "vector_store_key": "1911.01799-1",
          "chunk_id": 39,
          "document_id": "1911.01799",
          "start_idx": 14437,
          "end_idx": 14763
        },
        "page_content": " SITW(S) (1st row). This is understandable as VoxCeleb and SITW(S) were collected from the same source. For the pure CN-Celeb system (2nd row), although CN-Celeb(T) and CN-Celeb(E) are from the same source, the performance is still poor (14.24%). More importantly, with re-training the back-end model with VoxCeleb(L) (4th row",
        "type": "Document"
      },
      {
        "id": "9e226d1a-270a-4ab4-84be-c115759d913d",
        "metadata": {
          "vector_store_key": "1911.01799-0",
          "chunk_id": 12,
          "document_id": "1911.01799",
          "start_idx": 4886,
          "end_idx": 5298
        },
        "page_content": " In summary, CN-Celeb contains over $130,000$ utterances from $1,000$ Chinese celebrities. It covers 11 genres and the total amount of speech waveforms is 274 hours. Table TABREF5 gives the data distribution over the genres, and Table TABREF6 presents the data distribution over the length of utterances. Table TABREF13 summarizes the main difference between CN-Celeb and VoxCeleb. Compared to VoxCeleb, CN-Celeb",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do the authors measure performance?",
    "llm_answer": "EM-score, F\u2081-score, F\u2081-score (question answering), Spearman's \u03c1, Kendall's \u03c4, Pearson's r, average of predicted and manual scores.\n",
    "context": [
      {
        "id": "bb75c3fa-e2c5-412d-9db8-2f31cd178291",
        "metadata": {
          "vector_store_key": "1908.06606-4",
          "chunk_id": 45,
          "document_id": "1908.06606",
          "start_idx": 15988,
          "end_idx": 16383
        },
        "page_content": ") and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage",
        "type": "Document"
      },
      {
        "id": "674affb1-9973-4581-bdda-e127f9f0533b",
        "metadata": {
          "vector_store_key": "1908.10449-2",
          "chunk_id": 31,
          "document_id": "1908.10449",
          "start_idx": 10837,
          "end_idx": 11300
        },
        "page_content": "RC involves both MRC and RL, we adopt evaluation metrics from both settings. First, as a question answering task, we use $\\text{F}_1$ score to compare predicted answers against ground-truth, as in previous works. When there exist multiple ground-truth answers, we report the max $\\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we evaluate an agent's performance during both its training and testing phases.",
        "type": "Document"
      },
      {
        "id": "7d0ea89a-6851-44d9-8693-20c3c7a05240",
        "metadata": {
          "vector_store_key": "1707.03904-4",
          "chunk_id": 12,
          "document_id": "1707.03904",
          "start_idx": 4820,
          "end_idx": 5304
        },
        "page_content": "5\\%$ on Quasar-S and Quasar-T, while human performance is $50\\%$ and $60.6\\%$ respectively. For the automatic systems, we see an interesting tension between searching and reading accuracies \u2013 retrieving more documents in the search phase leads to a higher coverage of answers, but makes the comprehension task more difficult. We also collect annotations on a subset of the development set questions to allow researchers to analyze the categories in which their system performs well or",
        "type": "Document"
      },
      {
        "id": "adb396d1-6bc8-404f-8233-37de657861fe",
        "metadata": {
          "vector_store_key": "1909.00578-1",
          "chunk_id": 25,
          "document_id": "1909.00578",
          "start_idx": 8378,
          "end_idx": 8820
        },
        "page_content": " calculate the average of the predicted scores for the summaries of each particular contestant, and the average of the corresponding manual scores assigned to the contestant's summaries. We measure the correlation between the two (predicted vs. manual) across all contestants using Spearman's $\\rho $, Kendall's $\\tau $ and Pearson's $r$. We train and test the Sum-QE and BiGRU-ATT versions using a 3-fold procedure. In each fold, we train on",
        "type": "Document"
      },
      {
        "id": "a9967996-abe0-401c-9bba-ff7d4e484220",
        "metadata": {
          "vector_store_key": "1909.00578-4",
          "chunk_id": 11,
          "document_id": "1909.00578",
          "start_idx": 4037,
          "end_idx": 4469
        },
        "page_content": "notators assigned scores on a five-point scale, with 1 and 5 indicating that the summary is bad or good with respect to a specific $\\mathcal {Q}$. The overall score for a contestant with respect to a specific $\\mathcal {Q}$ is the average of the manual scores assigned to the summaries generated by the contestant. Note that the DUC-04 shared task involved seven $\\mathcal {Q}$s, but some of them were found to be highly overlapping",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the latest paper covered by this survey?",
    "llm_answer": "The answer is not found in the context.  While dates are mentioned for some datasets, there's no indication of the *publication* dates of the referenced papers.\n",
    "context": [
      {
        "id": "58dc1ff0-2907-45c5-ad6d-c1eb15bf6e62",
        "metadata": {
          "vector_store_key": "1912.13109-3",
          "chunk_id": 37,
          "document_id": "1912.13109",
          "start_idx": 14747,
          "end_idx": 15180
        },
        "page_content": " sentiment analysis: A survey Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery [8] Caruana, Rich and Lawrence, Steve and Giles, C Lee Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping Advances in neural information processing systems [9] Beale, Mark Hudson and Hagan, Martin T and Demuth, Howard B Neural network toolbox user\u2019s guide The MathWorks Incs [10] Chollet, Fran\u00e7ois and",
        "type": "Document"
      },
      {
        "id": "cf0f5d00-a545-4d7f-8b2b-2faf44dd80da",
        "metadata": {
          "vector_store_key": "1912.13109-3",
          "chunk_id": 36,
          "document_id": "1912.13109",
          "start_idx": 14243,
          "end_idx": 14747
        },
        "page_content": "-CNN model for Vietnamese sentiment analysis 2017 9th international conference on knowledge and systems engineering (KSE) [6] Pennington, Jeffrey and Socher, Richard and Manning, Christopher Glove: Global vectors for word representation Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) [7] Zhang, Lei and Wang, Shuai and Liu, Bing Deep learning for sentiment analysis: A survey Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery [8] Caruana",
        "type": "Document"
      },
      {
        "id": "1eb879ee-df56-4263-afd5-19bb5ae493ff",
        "metadata": {
          "vector_store_key": "1701.00185-9",
          "chunk_id": 51,
          "document_id": "1701.00185",
          "start_idx": 18784,
          "end_idx": 19212
        },
        "page_content": " BIBREF41 . StackOverflow. We use the challenge data published in Kaggle.com. The raw dataset consists 3,370,528 samples through July 31st, 2012 to August 14, 2012. In our experiments, we randomly select 20,000 question titles from 20 different tags as in Table TABREF25 . Biomedical. We use the challenge data published in BioASQ's official website. In our experiments, we randomly select 20, 000 paper titles from 20 different",
        "type": "Document"
      },
      {
        "id": "c6433cf2-13ae-4a11-9859-c4fefa3e544c",
        "metadata": {
          "vector_store_key": "1710.01492-1",
          "chunk_id": 3,
          "document_id": "1710.01492",
          "start_idx": 1421,
          "end_idx": 1934
        },
        "page_content": " political science, and social studies, among many others, which are interested in questions like these: Do people like the new Apple Watch? What do they hate about iPhone6? Do Americans support ObamaCare? What do Europeans think of Pope's visit to Palestine? How do we recognize the emergence of health problems such as depression? Do Germans like how Angela Merkel is handling the refugee crisis in Europe? What do republican voters in USA like/hate about Donald Trump? How do Scottish feel about the Brexit? An",
        "type": "Document"
      },
      {
        "id": "419119a3-f10f-40e2-ba27-f3de123fb008",
        "metadata": {
          "vector_store_key": "2002.02224-0",
          "chunk_id": 44,
          "document_id": "2002.02224",
          "start_idx": 18011,
          "end_idx": 18184
        },
        "page_content": " the institutional support of the Masaryk University. This paper was presented at CEILI Workshop on Legal Data Analysis held in conjunction with Jurix 2019 in Madrid, Spain.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What learning paradigms do they cover in this survey?",
    "llm_answer": "in-domain, out-of-domain, and transfer learning\n",
    "context": [
      {
        "id": "c94d339e-36f5-4336-a21a-05be0b487cef",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      },
      {
        "id": "8b468250-4671-4d25-a1dc-6d93ea228371",
        "metadata": {
          "vector_store_key": "1703.04617-0",
          "chunk_id": 2,
          "document_id": "1703.04617",
          "start_idx": 789,
          "end_idx": 1343
        },
        "page_content": " an end-to-end neural network framework, since we regard question understanding is of importance for such problems. We first introduced syntactic information to help encode questions. We then viewed and modelled different types of questions and the information shared among them as an adaptation problem and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results on our competitive baselines. Recent advance on reading comprehension and question answering has",
        "type": "Document"
      },
      {
        "id": "1cc830ac-f4e2-46d9-9e7d-f8704a5cf375",
        "metadata": {
          "vector_store_key": "1703.04617-0",
          "chunk_id": 9,
          "document_id": "1703.04617",
          "start_idx": 3555,
          "end_idx": 4085
        },
        "page_content": " has the potential to capture long-distance interactions over the structures. Different types of questions are often used to seek for different types of information. For example, a \"what\" question could have very different property from that of a \"why\" question, while they may share information and need to be trained together instead of separately. We view this as a \"adaptation\" problem to let different types of questions share a basic model but still discriminate them when needed. Specifically, we are motivated by the ideas",
        "type": "Document"
      },
      {
        "id": "e115f3ad-cfb7-4243-b123-51a669484bbc",
        "metadata": {
          "vector_store_key": "1703.04617-0",
          "chunk_id": 55,
          "document_id": "1703.04617",
          "start_idx": 14584,
          "end_idx": 15140
        },
        "page_content": "ity. The previous models are often trained for all questions without explicitly discriminating different question types; however, for a target question, both the common features shared by all questions and the specific features for a specific type of question are further considered in this paper, as they could potentially obey different distributions. In this paper we further explicitly model different types of questions in the end-to-end training. We start from a simple way to first analyze the word frequency of all questions, and obtain top-10 most",
        "type": "Document"
      },
      {
        "id": "46e1405f-46a2-4c37-9349-ebcad1f79edd",
        "metadata": {
          "vector_store_key": "1909.00430-5",
          "chunk_id": 65,
          "document_id": "1909.00430",
          "start_idx": 23326,
          "end_idx": 23878
        },
        "page_content": " (knowing the geographical region gives a prior on languages spoken), training predictors for renal failure from textual medical records given classifier for diabetes (there is a strong correlation between the two conditions), training a political affiliation classifier from social media tweets based on age-group classifiers, zip-code information, or social-status classifiers (there are known correlations between all of these to political affiliation), training hate-speech detection based on emotion detection, and so on. The work was supported in",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are all the input modalities considered in prior work in question generation?",
    "llm_answer": "The provided text mentions adapting NQG models to other input modalities, but does not explicitly list all input modalities considered in prior work.\n",
    "context": [
      {
        "id": "bff946c5-5126-4112-bf5b-c1c28d201a25",
        "metadata": {
          "vector_store_key": "1905.08949-5",
          "chunk_id": 18,
          "document_id": "1905.08949",
          "start_idx": 6831,
          "end_idx": 7344
        },
        "page_content": " . We discuss adapting NQG models to other input modalities in Section \"Wider Input Modalities\" . Finally, we consider the required cognitive process behind question asking, a distinguishing factor for questions BIBREF32 . A typical framework that attempts to categorize the cognitive levels involved in question asking comes from Bloom's taxonomy BIBREF33 , which has undergone several revisions and currently has six cognitive levels: Remembering, Understanding, Applying, Analyzing, Evaluating and Creating BIB",
        "type": "Document"
      },
      {
        "id": "3dffe9dc-473a-4370-8a23-2fae2fcd8269",
        "metadata": {
          "vector_store_key": "1905.08949-5",
          "chunk_id": 9,
          "document_id": "1905.08949",
          "start_idx": 3484,
          "end_idx": 3942
        },
        "page_content": " Given a sentence or a paragraph as input, content selection selects a particular salient topic worthwhile to ask about and determines the question type (What, When, Who, etc.). Approaches either take a syntactic BIBREF11 , BIBREF12 , BIBREF13 or semantic BIBREF14 , BIBREF3 , BIBREF15 , BIBREF16 tack, both starting by applying syntactic or semantic parsing, respectively, to obtain intermediate symbolic representations. Question construction then converts",
        "type": "Document"
      },
      {
        "id": "892addc6-3d8f-4372-bbe9-51e76394ad15",
        "metadata": {
          "vector_store_key": "1703.04617-0",
          "chunk_id": 59,
          "document_id": "1703.04617",
          "start_idx": 16017,
          "end_idx": 16540
        },
        "page_content": " questions share a basic model but still discriminate them when needed. Specifically, we borrow ideas from speaker adaptation BIBREF18 in speech recognition, where neural-network-based adaptation is performed among different groups of speakers. Conceptually we regard a type of questions as a group of acoustically similar speakers. Specifically we propose a question discriminative block or simply called a discriminative block (Figure 3 ) below to perform question adaptation. The main idea is described below:  $$\\mathbf",
        "type": "Document"
      },
      {
        "id": "536d7b37-3d43-4781-beea-c00f9bec22dd",
        "metadata": {
          "vector_store_key": "1905.08949-5",
          "chunk_id": 10,
          "document_id": "1905.08949",
          "start_idx": 3681,
          "end_idx": 4181
        },
        "page_content": " by applying syntactic or semantic parsing, respectively, to obtain intermediate symbolic representations. Question construction then converts intermediate representations to a natural language question, taking either a tranformation- or template-based approach. The former BIBREF17 , BIBREF18 , BIBREF13 rearranges the surface form of the input sentence to produce the question; the latter BIBREF19 , BIBREF20 , BIBREF21 generates questions from pre-defined question templates. Unfortunately, such Q",
        "type": "Document"
      },
      {
        "id": "05f513f6-29b2-46d8-8ba4-af84d6b0b34d",
        "metadata": {
          "vector_store_key": "1703.04617-0",
          "chunk_id": 10,
          "document_id": "1703.04617",
          "start_idx": 3849,
          "end_idx": 4374
        },
        "page_content": " questions share a basic model but still discriminate them when needed. Specifically, we are motivated by the ideas \"i-vector\" BIBREF18 in speech recognition, where neural network based adaptation is performed among different (groups) of speakers and we focused instead on different types of questions here. Our baseline model is composed of the following typical components: word embedding, input encoder, alignment, aggregation, and prediction. Below we discuss these components in more details. We concatenate embedding at",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do this framework facilitate demographic inference from social media?",
    "llm_answer": "The framework leverages weighted lexicons of terms, profile descriptions with rules and patterns, homophily interaction information, and visual features (images) to predict age and gender from textual and visual content.\n",
    "context": [
      {
        "id": "36c16456-1e80-4317-9257-a6d5d4a8c99e",
        "metadata": {
          "vector_store_key": "1902.06843-7",
          "chunk_id": 69,
          "document_id": "1902.06843",
          "start_idx": 27195,
          "end_idx": 27674
        },
        "page_content": " leverage both the visual and textual content for predicting age and gender. Prediction with Textual Content: We employ BIBREF73 's weighted lexicon of terms that uses the dataset of 75,394 Facebook users who shared their status, age and gender. The predictive power of this lexica was evaluated on Twitter, blog, and Facebook, showing promising results BIBREF73 . Utilizing these two weighted lexicon of terms, we are predicting the demographic information (age or gender) of IN",
        "type": "Document"
      },
      {
        "id": "0311a696-4f51-4155-bc91-58a50c0d44b0",
        "metadata": {
          "vector_store_key": "1902.06843-7",
          "chunk_id": 28,
          "document_id": "1902.06843",
          "start_idx": 11357,
          "end_idx": 11906
        },
        "page_content": " Twitter users. Similarly, relying on profile descriptions while devising a set of rules and patterns, a novel model introduced for extracting age for Twitter users BIBREF41 . They also parse description for occupation by consulting the SOC2010 list of occupations and validating it through social surveys. A novel age inference model was developed while relying on homophily interaction information and content for predicting age of Twitter users BIBREF42 . The limitations of textual content for predicting age and gender was highlighted by BIBREF",
        "type": "Document"
      },
      {
        "id": "e9bdb5c5-6c87-42f5-952d-14aa081ffc8c",
        "metadata": {
          "vector_store_key": "1902.06843-3",
          "chunk_id": 25,
          "document_id": "1902.06843",
          "start_idx": 10268,
          "end_idx": 10791
        },
        "page_content": " visual features, emotional features BIBREF34 , topic-level features, and domain-specific features. Particularly, our choice of multi-model prediction framework is intended to improve upon the prior works involving use of images in multimodal depression analysis BIBREF33 and prior works on studying Instagram photos BIBREF6 , BIBREF35 . Demographic information inference on Social Media:  There is a growing interest in understanding online user's demographic information due to its numerous applications in healthcare BIB",
        "type": "Document"
      },
      {
        "id": "4ff81d18-900b-43eb-b1dd-1b55594cface",
        "metadata": {
          "vector_store_key": "1705.09665-4",
          "chunk_id": 11,
          "document_id": "1705.09665",
          "start_idx": 4847,
          "end_idx": 5438
        },
        "page_content": " stay). Such insights into the relation between collective identity and user engagement can be informative to community maintainers seeking to better understand growth patterns within their online communities. More generally, our methodology stands as an example of how sociological questions can be addressed in a multi-community setting. In performing our analyses across a rich variety of communities, we reveal both the diversity of phenomena that can occur, as well as the systematic nature of this diversity. A community's identity derives from its members' common interests and shared",
        "type": "Document"
      },
      {
        "id": "a2719932-7c91-4b04-b63e-41ede86c59db",
        "metadata": {
          "vector_store_key": "1705.09665-4",
          "chunk_id": 88,
          "document_id": "1705.09665",
          "start_idx": 36224,
          "end_idx": 36805
        },
        "page_content": " newcomers at certain points in their lifecycle? Another important avenue of future work is to explore other ways of mapping the landscape of online communities. For example, combining structural properties of communities BIBREF40 with topical information BIBREF35 and with our identity-based measures could further characterize and explain variations in user engagement patterns. Furthermore, extending the present analyses to even more diverse communities supported by different platforms (e.g., GitHub, StackExchange, Wikipedia) could enable the characterization of more complex",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the data annotated?",
    "llm_answer": "The data was annotated using a final annotation schema, with iterative refinement due to complexity.  Scenario-specific annotation templates were designed, containing event and participant labels.  NP heads were labeled with participant labels.  Workers were trained and paired to converse, annotating user and system states.  Dialogue acts were automatically annotated using rules based on user states, system states, and dialogue histories.  In-house annotators verified the labels of MTurk annotators.  BIO format (Inside, Outside, Beginning) was used at a top layer.\n",
    "context": [
      {
        "id": "d4c4849a-b0a6-4931-82a7-aed67b41eb03",
        "metadata": {
          "vector_store_key": "1703.05260-7",
          "chunk_id": 19,
          "document_id": "1703.05260",
          "start_idx": 7578,
          "end_idx": 8108
        },
        "page_content": ". This section deals with the annotation of the data. We first describe the final annotation schema. Then, we describe the iterative process of corpus annotation and the refinement of the schema. This refinement was necessary due to the complexity of the annotation. For each of the scenarios, we designed a specific annotation template. A script template consists of scenario-specific event and participant labels. An example of a template is shown in Table 1 . All NP heads in the corpus were annotated with a participant label;",
        "type": "Document"
      },
      {
        "id": "f9e9e3ec-e213-4cd5-ae8e-8d986c9fd2db",
        "metadata": {
          "vector_store_key": "2002.11893-1",
          "chunk_id": 22,
          "document_id": "2002.11893",
          "start_idx": 8390,
          "end_idx": 8922
        },
        "page_content": " before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states. Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three",
        "type": "Document"
      },
      {
        "id": "9d180fa4-0178-4f9f-8997-37024ccf17f9",
        "metadata": {
          "vector_store_key": "1909.00430-8",
          "chunk_id": 0,
          "document_id": "1909.00430",
          "start_idx": 0,
          "end_idx": 440
        },
        "page_content": "Data annotation is a key bottleneck in many data driven algorithms. Specifically, deep learning models, which became a prominent tool in many data driven tasks in recent years, require large datasets to work well. However, many tasks require manual annotations which are relatively hard to obtain at scale. An attractive alternative is lightly supervised learning BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF",
        "type": "Document"
      },
      {
        "id": "41b33e3c-7622-4a3a-bf41-e8b160505f06",
        "metadata": {
          "vector_store_key": "1910.11769-4",
          "chunk_id": 13,
          "document_id": "1910.11769",
          "start_idx": 4963,
          "end_idx": 5497
        },
        "page_content": " using in-house data annotators who are experts in narrative content. A passage is accepted as valid if the in-house annotator's label matched any one of the MTurk annotators' labels. The remaining passages are discarded. We provide the fraction of annotator agreement for each label in the dataset. Though passages may lose some emotional context when read independently of the complete narrative, we believe annotator agreement on our dataset supports the assertion that small excerpts can still convey coherent emotions. During the",
        "type": "Document"
      },
      {
        "id": "3b541d42-de69-4c8c-97f5-634e76f12a94",
        "metadata": {
          "vector_store_key": "1911.07228-1",
          "chunk_id": 54,
          "document_id": "1911.07228",
          "start_idx": 14818,
          "end_idx": 15154
        },
        "page_content": " BIO format (Inside, Outside, Beginning) BIBREF6 at the top layer (CRF). If we can unify the labeling of the annotators between the train, dev and test sets. We will improve data quality and classifier. It is better if there is a pre-trained word embeddings that overlays the data, and segmentation algorithm need to be more accurately.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Where does the information on individual-level demographics come from?",
    "llm_answer": "Facebook users who shared their status, age, and gender.  A subset of 1061 users from INLINEFORM0.\n",
    "context": [
      {
        "id": "4ff81d18-900b-43eb-b1dd-1b55594cface",
        "metadata": {
          "vector_store_key": "1705.09665-4",
          "chunk_id": 11,
          "document_id": "1705.09665",
          "start_idx": 4847,
          "end_idx": 5438
        },
        "page_content": " stay). Such insights into the relation between collective identity and user engagement can be informative to community maintainers seeking to better understand growth patterns within their online communities. More generally, our methodology stands as an example of how sociological questions can be addressed in a multi-community setting. In performing our analyses across a rich variety of communities, we reveal both the diversity of phenomena that can occur, as well as the systematic nature of this diversity. A community's identity derives from its members' common interests and shared",
        "type": "Document"
      },
      {
        "id": "36c16456-1e80-4317-9257-a6d5d4a8c99e",
        "metadata": {
          "vector_store_key": "1902.06843-7",
          "chunk_id": 69,
          "document_id": "1902.06843",
          "start_idx": 27195,
          "end_idx": 27674
        },
        "page_content": " leverage both the visual and textual content for predicting age and gender. Prediction with Textual Content: We employ BIBREF73 's weighted lexicon of terms that uses the dataset of 75,394 Facebook users who shared their status, age and gender. The predictive power of this lexica was evaluated on Twitter, blog, and Facebook, showing promising results BIBREF73 . Utilizing these two weighted lexicon of terms, we are predicting the demographic information (age or gender) of IN",
        "type": "Document"
      },
      {
        "id": "f0a4591c-0319-403c-bc1c-f226b27e5bfd",
        "metadata": {
          "vector_store_key": "1902.06843-7",
          "chunk_id": 26,
          "document_id": "1902.06843",
          "start_idx": 10565,
          "end_idx": 11055
        },
        "page_content": " is a growing interest in understanding online user's demographic information due to its numerous applications in healthcare BIBREF36 , BIBREF37 . A supervised model developed by BIBREF38 for determining users' gender by employing features such as screen-name, full-name, profile description and content on external resources (e.g., personal blog). Employing features including emoticons, acronyms, slangs, punctuations, capitalization, sentence length and included links/images, along with",
        "type": "Document"
      },
      {
        "id": "51da5779-20bb-4254-a399-b1644589e290",
        "metadata": {
          "vector_store_key": "1902.06843-7",
          "chunk_id": 34,
          "document_id": "1902.06843",
          "start_idx": 13693,
          "end_idx": 14044
        },
        "page_content": " profile descriptions (such as \"17 years old, self-harm, anxiety, depression\") BIBREF41 . We compile \"age prefixes\" and \"age suffixes\", and use three age-extraction rules: 1. I am X years old 2. Born in X 3. X years old, where X is a \"date\" or age (e.g., 1994). We selected a subset of 1061 users among INLINEFORM0 as gold standard dataset INLINEFORM1",
        "type": "Document"
      },
      {
        "id": "b8a611c4-55c6-4498-b062-fe6cf9ddf3e1",
        "metadata": {
          "vector_store_key": "1902.06843-2",
          "chunk_id": 73,
          "document_id": "1902.06843",
          "start_idx": 28628,
          "end_idx": 29139
        },
        "page_content": " we use their pretrained CNN consisting of convolutional layers, including unshared and fully-connected layers, to predict gender and age from both the profile and shared images. We evaluate the performance for gender and age prediction task on INLINEFORM0 and INLINEFORM1 respectively as shown in Table TABREF42 and Table TABREF44 . Demographic Prediction Analysis: We delve deeper into the benefits and drawbacks of each data modality for demographic information prediction. This is crucial as the differences",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the source of the user interaction data? ",
    "llm_answer": "Reddit and Twitter.\n",
    "context": [
      {
        "id": "a03e9d40-2fec-4988-a643-2aabec9e668a",
        "metadata": {
          "vector_store_key": "1705.09665-6",
          "chunk_id": 31,
          "document_id": "1705.09665",
          "start_idx": 12917,
          "end_idx": 13479
        },
        "page_content": " multicommunity ecosystem make it an ideal landscape in which to closely examine the relation between varying community identities and social dynamics. Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk",
        "type": "Document"
      },
      {
        "id": "0dbcc912-11d7-4322-b81c-0d8cc7a5b9fc",
        "metadata": {
          "vector_store_key": "1705.09665-4",
          "chunk_id": 75,
          "document_id": "1705.09665",
          "start_idx": 30548,
          "end_idx": 31135
        },
        "page_content": " these communities indicates that temporally ephemeral content might serve as an entry point into such a community, without necessarily engaging users in the long term. Our language-based typology and analysis of user engagement draws on and contributes to several distinct research threads, in addition to the many foundational studies cited in the previous sections. Multicommunity studies. Our investigation of user engagement in multicommunity settings follows prior literature which has examined differences in user and community dynamics across various online groups, such as email",
        "type": "Document"
      },
      {
        "id": "526902c1-507f-4edc-9d8d-f6dbf515d17c",
        "metadata": {
          "vector_store_key": "1705.09665-4",
          "chunk_id": 77,
          "document_id": "1705.09665",
          "start_idx": 31419,
          "end_idx": 31975
        },
        "page_content": " content-based framework through which user engagement can be examined. Reddit has been a particularly useful setting for studying multiple communities in prior work. Such studies have mostly focused on characterizing how individual users engage across a multi-community platform BIBREF34 , BIBREF35 , or on specific user engagement patterns such as loyalty to particular communities BIBREF22 . We complement these studies by seeking to understand how features of communities can mediate a broad array of user engagement patterns within them. Typologies of",
        "type": "Document"
      },
      {
        "id": "2ac70ed2-98f1-4e4a-be2a-c64629883c9c",
        "metadata": {
          "vector_store_key": "1909.02764-3",
          "chunk_id": 37,
          "document_id": "1909.02764",
          "start_idx": 15115,
          "end_idx": 15596
        },
        "page_content": " to 288 interactions, 180 between driver and the agent and 108 between driver and co-driver. The emotion self-ratings from the participants yielded 90 utterances labeled with joy, 26 with annoyance, 49 with insecurity, 9 with boredom, 111 with relaxation and 3 with no emotion. One example interaction per interaction type and emotion is shown in Table TABREF7. For further experiments, we only use joy, annoyance/anger, and insecurity/fear due to the small sample size for boredom",
        "type": "Document"
      },
      {
        "id": "8b62aae0-640e-4f20-8469-964531e81404",
        "metadata": {
          "vector_store_key": "1910.03814-1",
          "chunk_id": 23,
          "document_id": "1910.03814",
          "start_idx": 9128,
          "end_idx": 9607
        },
        "page_content": " image. We call the dataset MMHS150K, and made it available online . In this section, we explain the dataset creation steps. We used the Twitter API to gather real-time tweets from September 2018 until February 2019, selecting the ones containing any of the 51 Hatebase terms that are more common in hate speech tweets, as studied in BIBREF9. We filtered out retweets, tweets containing less than three words and tweets containing porn related terms. From that selection, we kept",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the source of the textual data? ",
    "llm_answer": "Biomedical dataset, InScript corpus.\n",
    "context": [
      {
        "id": "c5eaf502-a11a-40bd-84a8-78f62e8d8d02",
        "metadata": {
          "vector_store_key": "1701.00185-9",
          "chunk_id": 10,
          "document_id": "1701.00185",
          "start_idx": 3790,
          "end_idx": 4304
        },
        "page_content": " this paper we use a new short text dataset, Biomedical, in the experiment to verify the effectiveness of our approach. Third, we put much effort on studying the influence of various different semantic features integrated in our self-taught CNN framework, which is not involved in the conference paper. For the purpose of reproducibility, we make the datasets and software used in our experiments publicly available at the website. The remainder of this paper is organized as follows: In Section SECREF2 , we first",
        "type": "Document"
      },
      {
        "id": "e2f6e78c-bf7d-46ad-bd51-435c8758190a",
        "metadata": {
          "vector_store_key": "1707.03904-2",
          "chunk_id": 39,
          "document_id": "1707.03904",
          "start_idx": 14370,
          "end_idx": 14849
        },
        "page_content": "\ufffdopen-book\u201d setting, where they had access to a search engine over the short pseudo-documents extracted for each dataset (as described in Section \"Context Retrieval\" ). We decided to use short pseudo-documents for this exercise to reduce the burden of reading on the volunteers, though we note that the long pseudo-documents have greater coverage of answers. We also asked the volunteers to provide annotations to categorize the type of each question they were asked, and a label",
        "type": "Document"
      },
      {
        "id": "41b33e3c-7622-4a3a-bf41-e8b160505f06",
        "metadata": {
          "vector_store_key": "1910.11769-4",
          "chunk_id": 13,
          "document_id": "1910.11769",
          "start_idx": 4963,
          "end_idx": 5497
        },
        "page_content": " using in-house data annotators who are experts in narrative content. A passage is accepted as valid if the in-house annotator's label matched any one of the MTurk annotators' labels. The remaining passages are discarded. We provide the fraction of annotator agreement for each label in the dataset. Though passages may lose some emotional context when read independently of the complete narrative, we believe annotator agreement on our dataset supports the assertion that small excerpts can still convey coherent emotions. During the",
        "type": "Document"
      },
      {
        "id": "929e7741-cc1f-488b-83ee-ffbc7d6eda71",
        "metadata": {
          "vector_store_key": "1912.01673-0",
          "chunk_id": 9,
          "document_id": "1912.01673",
          "start_idx": 3674,
          "end_idx": 4204
        },
        "page_content": "dings. The paper is structured as follows: related summarizes existing methods of sentence embeddings evaluation and related work. annotation describes our methodology for constructing our dataset. data details the obtained dataset and some first observations. We conclude and provide the link to the dataset in conclusion As hinted above, there are many methods of converting a sequence of words into a vector in a highly dimensional space. To name a few: BiLSTM with the max-pooling trained for natural language inference BIBREF",
        "type": "Document"
      },
      {
        "id": "1ca1c9f4-8d96-46db-9ef0-8e0af1d5d86f",
        "metadata": {
          "vector_store_key": "1703.05260-2",
          "chunk_id": 7,
          "document_id": "1703.05260",
          "start_idx": 2773,
          "end_idx": 3335
        },
        "page_content": " words. Relevant verbs and noun phrases in all stories are annotated with event types and participant types respectively. Additionally, the texts have been annotated with coreference information in order to facilitate the study of the interdependence between script structure and coreference. The InScript corpus is a unique resource that provides a basis for studying various aspects of the role of script knowledge in language processing by humans. The acquisition of this corpus is part of a larger research effort that aims at using script knowledge to model",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the source of the visual data? ",
    "llm_answer": "Image data captured from photo-ready devices (like smartphones) and online social media services.  Video, speech, and biosignals (from Empatica E4).  Profile/shared images.\n",
    "context": [
      {
        "id": "73f2642a-268a-46a6-95be-8f467e06933b",
        "metadata": {
          "vector_store_key": "1910.09399-3",
          "chunk_id": 127,
          "document_id": "1910.09399",
          "start_idx": 49259,
          "end_idx": 49739
        },
        "page_content": " semantic text regeneration and alignment module (STREAM). The back to back Text-to-Image (T2I) and Image-to-Text (I2T) are combined to progressively enhance the diversity and semantic consistency of the generated images. black In order to enhance the diversity of the output image, Scene Graph GAN BIBREF61 proposes to use visual scene graphs to describe the layout of the objects, allowing users to precisely specific the relationships between objects in the images. In order to",
        "type": "Document"
      },
      {
        "id": "1e38d321-61f2-4999-bc5b-46ba0d3f6d3d",
        "metadata": {
          "vector_store_key": "1909.08859-7",
          "chunk_id": 16,
          "document_id": "1909.08859",
          "start_idx": 6453,
          "end_idx": 6882
        },
        "page_content": " single image or a series of $L$ images depending on the reasoning task. In particular, for the visual cloze and the visual coherence type questions, the answer contains a single image ($L=1$) and for the visual ordering task, it includes a sequence. We encode the input recipe $\\mathbf {R}$ at character, word, and step levels. Character-level embedding layer uses a convolutional neural network, namely CharCNN model by BIBREF7",
        "type": "Document"
      },
      {
        "id": "f8e77cdc-3b47-4927-8b62-40db578f28fa",
        "metadata": {
          "vector_store_key": "1910.09399-1",
          "chunk_id": 144,
          "document_id": "1910.09399",
          "start_idx": 55753,
          "end_idx": 56314
        },
        "page_content": " for computer vision AI and deep learning researchers and enthusiasts. The consistent advancement in hardware, software, and contemporaneous development of computer vision AI research disrupts multiple industries. These advances in technology allow for the extraction of several data types from a variety of sources. For example, image data captured from a variety of photo-ready devices, such as smart-phones, and online social media services opened the door to the analysis of large amounts of media datasets BIBREF70. The availability of large media datasets",
        "type": "Document"
      },
      {
        "id": "dc6fb8a7-e63e-4734-91b0-b62d42e28ebb",
        "metadata": {
          "vector_store_key": "1909.02764-6",
          "chunk_id": 26,
          "document_id": "1909.02764",
          "start_idx": 10529,
          "end_idx": 10937
        },
        "page_content": " v2.2.0) simulation software (https://vires.com/vtd-vires-virtual-test-drive/). The vehicle has an automatic transmission, a steering wheel and gas and brake pedals. We collect data from video, speech and biosignals (Empatica E4 to record heart rate, electrodermal activity, skin temperature, not further used in this paper) and questionnaires. Two RGB cameras are fixed in the vehicle to capture the drivers",
        "type": "Document"
      },
      {
        "id": "879424ad-57af-45d4-bdc4-576f84fed1cf",
        "metadata": {
          "vector_store_key": "1902.06843-3",
          "chunk_id": 46,
          "document_id": "1902.06843",
          "start_idx": 18182,
          "end_idx": 18705
        },
        "page_content": " for each user u in INLINEFORM0 , we process profile/shared images for both the depressed and the control groups with at least one face from the shared images (Table TABREF23 ). For the photos that contain multiple faces, we measure the average emotion. Figure FIGREF27 illustrates the inter-correlation of these features. Additionally, we observe that emotions gleaned from facial expressions correlated with emotional signals captured from textual content utilizing LIWC. This indicates visual imagery can be harnessed as",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What result from experiments suggest that natural language based agents are more robust?",
    "llm_answer": "The context states that experiments comparing natural language representations to other methods were conducted under similar conditions, but does not state that natural language representations were found to be more robust.\n",
    "context": [
      {
        "id": "a4226673-4fe8-4712-9c98-0b0f328488f0",
        "metadata": {
          "vector_store_key": "1910.02789-6",
          "chunk_id": 38,
          "document_id": "1910.02789",
          "start_idx": 14681,
          "end_idx": 15255
        },
        "page_content": " from all of the previous scenarios. Our agent was implemented using a Convolutional Neural Network as described in Section SECREF4. We converted the parsed state into embedded representations of fixed length. We tested both a DQN and a PPO based agent, and compared the natural language representation to the other representation techniques, namely the raw image, feature vector, and semantic segmentation representations. In order to effectively compare the performance of the different representation methods, we conducted our experiments under similar conditions for all",
        "type": "Document"
      },
      {
        "id": "c270760a-3f97-41b4-84b6-68ddf24229ea",
        "metadata": {
          "vector_store_key": "1910.04269-4",
          "chunk_id": 54,
          "document_id": "1910.04269",
          "start_idx": 20617,
          "end_idx": 21192
        },
        "page_content": " robust to variations which might be present in real world scenarios. There can be further exploration of various feature extraction techniques like Constant-Q transform and Fast Fourier Transform and assessment of their impact on Language Identification. There can be further improvements in neural network architectures like concatenating the high level features obtained from 1D-ConvNet and 2D-ConvNet, before performing classification. There can be experiments using deeper networks with skip connections and Inception modules. These are known to have positively impacted",
        "type": "Document"
      },
      {
        "id": "94dfda86-59fb-4f3f-8ae3-69e656aefc53",
        "metadata": {
          "vector_store_key": "1910.02789-8",
          "chunk_id": 7,
          "document_id": "1910.02789",
          "start_idx": 2954,
          "end_idx": 3540
        },
        "page_content": "15. A basic observation of our work is that natural language representations are also beneficial for solving problems in which natural language is not the underlying source of input. Moreover, our results indicate that natural language is a strong alternative to current complementary methods for semantic representations of a state. In this work we assume a state can be described using natural language sentences. We use distributional embedding methods in order to represent sentences, processed with a standard Convolutional Neural Network for feature extraction. In Section SECREF2",
        "type": "Document"
      },
      {
        "id": "1d58a6eb-ffc4-483f-86f7-8346434717f3",
        "metadata": {
          "vector_store_key": "1809.08731-4",
          "chunk_id": 13,
          "document_id": "1809.08731",
          "start_idx": 5000,
          "end_idx": 5512
        },
        "page_content": " data they had created by translating and back-translating sentences with neural models. While they tried different types of LMs, best results were obtained for neural models, namely recurrent neural networks (RNNs). In this work, we investigate if approaches which have proven successful for modeling acceptability can be applied to the NLP problem of automatic fluency evaluation. In this section, we first describe SLOR and the intuition behind this score. Then, we introduce WordPieces, before explaining how",
        "type": "Document"
      },
      {
        "id": "d7e0779f-33e5-41b8-b3d5-0127ad3f2f40",
        "metadata": {
          "vector_store_key": "1811.00942-0",
          "chunk_id": 1,
          "document_id": "1811.00942",
          "start_idx": 514,
          "end_idx": 1077
        },
        "page_content": "REF3 , BIBREF4 . Specifically focused on language modeling, this paper examines an issue that to our knowledge has not been explored: advances in neural language models have come at a significant cost in terms of increased computational complexity. Computing the probability of a token sequence using non-neural techniques requires a number of phrase lookups and perhaps a few arithmetic operations, whereas model inference with NLMs require large matrix multiplications consuming perhaps millions of floating point operations (FLOPs). These performance tradeoffs",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which datasets are used in the paper?",
    "llm_answer": "Biomedical, StackOverflow, CNN/Daily Mail\n",
    "context": [
      {
        "id": "c5eaf502-a11a-40bd-84a8-78f62e8d8d02",
        "metadata": {
          "vector_store_key": "1701.00185-9",
          "chunk_id": 10,
          "document_id": "1701.00185",
          "start_idx": 3790,
          "end_idx": 4304
        },
        "page_content": " this paper we use a new short text dataset, Biomedical, in the experiment to verify the effectiveness of our approach. Third, we put much effort on studying the influence of various different semantic features integrated in our self-taught CNN framework, which is not involved in the conference paper. For the purpose of reproducibility, we make the datasets and software used in our experiments publicly available at the website. The remainder of this paper is organized as follows: In Section SECREF2 , we first",
        "type": "Document"
      },
      {
        "id": "401df3ea-241b-476d-97f4-1276dc131769",
        "metadata": {
          "vector_store_key": "1701.00185-9",
          "chunk_id": 99,
          "document_id": "1701.00185",
          "start_idx": 35418,
          "end_idx": 35678
        },
        "page_content": "ASQ for making the datasets available. This work is supported by the National Natural Science Foundation of China (No. 61602479, No. 61303172, No. 61403385) and the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDB02070005).",
        "type": "Document"
      },
      {
        "id": "419119a3-f10f-40e2-ba27-f3de123fb008",
        "metadata": {
          "vector_store_key": "2002.02224-0",
          "chunk_id": 44,
          "document_id": "2002.02224",
          "start_idx": 18011,
          "end_idx": 18184
        },
        "page_content": " the institutional support of the Masaryk University. This paper was presented at CEILI Workshop on Legal Data Analysis held in conjunction with Jurix 2019 in Madrid, Spain.",
        "type": "Document"
      },
      {
        "id": "1eb879ee-df56-4263-afd5-19bb5ae493ff",
        "metadata": {
          "vector_store_key": "1701.00185-9",
          "chunk_id": 51,
          "document_id": "1701.00185",
          "start_idx": 18784,
          "end_idx": 19212
        },
        "page_content": " BIBREF41 . StackOverflow. We use the challenge data published in Kaggle.com. The raw dataset consists 3,370,528 samples through July 31st, 2012 to August 14, 2012. In our experiments, we randomly select 20,000 question titles from 20 different tags as in Table TABREF25 . Biomedical. We use the challenge data published in BioASQ's official website. In our experiments, we randomly select 20, 000 paper titles from 20 different",
        "type": "Document"
      },
      {
        "id": "38e182ee-fbda-4ecb-9bd3-29733e9a857d",
        "metadata": {
          "vector_store_key": "1907.05664-0",
          "chunk_id": 9,
          "document_id": "1907.05664",
          "start_idx": 3532,
          "end_idx": 3981
        },
        "page_content": " in this section the baseline model from See et al. See2017 trained on the CNN/Daily Mail dataset. We reproduce the results from See et al. See2017 to then apply LRP on it. The CNN/Daily mail dataset BIBREF12 is a text summarization dataset adapted from the Deepmind question-answering dataset BIBREF13 . It contains around three hundred thousand news articles coupled with summaries of about three sentences. These summaries are in fact \u201chighlights",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they quantify moral relevance?",
    "llm_answer": "They use the proportion of \"not a moral issue\" human responses paired with irrelevance predictions and the proportion of \"acceptable\" responses paired with positive predictions, performing linear regressions on these values.  Diachronic moral relevance scores (p(c\u2081|q)) are also computed for 20 decades.\n",
    "context": [
      {
        "id": "6ca15c62-1479-475c-8ee2-dbd58071e99e",
        "metadata": {
          "vector_store_key": "2001.07209-0",
          "chunk_id": 38,
          "document_id": "2001.07209",
          "start_idx": 14939,
          "end_idx": 15306
        },
        "page_content": " \u201cnot a moral issue\". We compared human ratings with model predictions at two tiers: for moral relevance, we paired the proportion of \u201cnot a moral issue\u201d human responses with irrelevance predictions $p(c_0\\,|\\,\\mathbf {q})$ for each topic, and for moral acceptability, we paired the proportion of \u201cacceptable\u201d responses with positive predictions $p(c_+\\,|\\,\\mathbf {q",
        "type": "Document"
      },
      {
        "id": "02eab0e4-4e10-4c1e-905b-efab087ceb40",
        "metadata": {
          "vector_store_key": "2001.07209-3",
          "chunk_id": 42,
          "document_id": "2001.07209",
          "start_idx": 16301,
          "end_idx": 16747
        },
        "page_content": " performed a linear regression of $R$ on $T = 1,\\ldots ,n$ and took the fitted slope as a measure of moral relevance change. We repeated the same procedure for moral polarity. Finally, we removed words with average relevance score below $0.5$ to focus on morally relevant retrievals. Table TABREF17 shows the words with steepest predicted change toward moral relevance, along with their predicted fine-grained moral categories in modern times (i.",
        "type": "Document"
      },
      {
        "id": "b236d37c-eaf2-4883-a181-1ecffd2e03bf",
        "metadata": {
          "vector_store_key": "2001.07209-3",
          "chunk_id": 41,
          "document_id": "2001.07209",
          "start_idx": 16142,
          "end_idx": 16447
        },
        "page_content": " restricted to words labelled as nouns in WordNet BIBREF34 for validation. For each such word $\\mathbf {q}$, we computed diachronic moral relevance scores $R_i = p(c_1\\,|\\,\\mathbf {q}), i=1,\\ldots ,20$ for the 20 decades in our time span. Then, we performed a linear regression of $R$ on $T = 1,\\ldots ,n$",
        "type": "Document"
      },
      {
        "id": "17fba4df-76ff-45d9-aaea-4af655cc5fe4",
        "metadata": {
          "vector_store_key": "2001.07209-0",
          "chunk_id": 37,
          "document_id": "2001.07209",
          "start_idx": 14631,
          "end_idx": 15157
        },
        "page_content": " into the public sentiment toward these concepts as evidenced by text. We explored the predictive potential of our framework by comparing model predictions with human judgments of moral relevance and acceptability. We used data from the Pew Research Center's 2013 Global Attitudes survey BIBREF33, in which participants from 40 countries judged 8 topics such as abortion and homosexuality as one of \u201cacceptable\", \u201cunacceptable\", and \u201cnot a moral issue\". We compared human ratings with model predictions at two tiers: for moral",
        "type": "Document"
      },
      {
        "id": "4e13d53d-0e6b-4e5f-90c6-17523b07373e",
        "metadata": {
          "vector_store_key": "2001.07209-3",
          "chunk_id": 44,
          "document_id": "2001.07209",
          "start_idx": 17042,
          "end_idx": 17617
        },
        "page_content": " time of predicted moral relevance and in modern times. Although we do not have access to ground truth for this application, these results offer initial insight into the historical moral landscape of the English language at scale. In this application, we investigated the hypothesis that concept concreteness is inversely related to change in moral relevance, i.e., that concepts considered more abstract might become morally relevant at a higher rate than concepts considered more concrete. To test this hypothesis, we performed a multiple linear regression analysis on rate",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much is proposed model better in perplexity and BLEU score than typical UMT models?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "9ecafa72-861c-465c-ab11-f5d45f864b70",
        "metadata": {
          "vector_store_key": "2002.11893-4",
          "chunk_id": 81,
          "document_id": "2002.11893",
          "start_idx": 30351,
          "end_idx": 30691
        },
        "page_content": " as references (100 references on average), which results in high BLEU score. For user-side utterances, the BLEU score for TemplateNLG is 0.5780, while the BLEU score for SC-LSTM is 0.7858. For system-side, the two scores are 0.6828 and 0.8595. As exemplified in Table TABREF39, the gap between the two models can be attributed to that SC-L",
        "type": "Document"
      },
      {
        "id": "2edde34b-120d-4687-b68f-1177eb03c8ad",
        "metadata": {
          "vector_store_key": "1908.06606-4",
          "chunk_id": 51,
          "document_id": "1908.06606",
          "start_idx": 18088,
          "end_idx": 18399
        },
        "page_content": " TABREF23. Table TABREF23 indicates that our proposed model achieved the best performance both in EM-score and F$_1$-score with EM-score of 91.84% and F$_1$-score of 93.75%. QANet outperformed BERT-Base with 3.56% score in F$_1$-score but underperformed it with 0.75% score in EM-score. Compared with BERT-Base,",
        "type": "Document"
      },
      {
        "id": "b367f47b-ea61-495e-8709-0724ed1aad7a",
        "metadata": {
          "vector_store_key": "1908.06606-4",
          "chunk_id": 52,
          "document_id": "1908.06606",
          "start_idx": 18241,
          "end_idx": 18595
        },
        "page_content": "formed it with 0.75% score in EM-score. Compared with BERT-Base, our model led to a 5.64% performance improvement in EM-score and 3.69% in F$_1$-score. Although our model didn't outperform much with QANet in F$_1$-score (only 0.13%), our model significantly outperformed it with 6.39% score in EM-score. To further investigate the effects of named entity",
        "type": "Document"
      },
      {
        "id": "9742138b-9c20-46f9-9af9-34b98db8feb1",
        "metadata": {
          "vector_store_key": "1905.11901-0",
          "chunk_id": 25,
          "document_id": "1905.11901",
          "start_idx": 8768,
          "end_idx": 9134
        },
        "page_content": " hyperparameters (+0.7 BLEU) has a stronger effect than the lexical model, and adding the lexical model (9) on top of the optimized configuration (8) does not improve performance. Together, the adaptations to the ultra-low data setting yield 9.4 BLEU (7.2 INLINEFORM2 16.6). The model trained on full IWSLT data is less sensitive to our changes (31.9 INLINEFORM3 32.",
        "type": "Document"
      },
      {
        "id": "c7d22b84-949d-4fb8-8d5b-b47904f1b0a7",
        "metadata": {
          "vector_store_key": "1811.00942-3",
          "chunk_id": 34,
          "document_id": "1811.00942",
          "start_idx": 11032,
          "end_idx": 11406
        },
        "page_content": "ity appears to have a moderate linear relationship with R@3 error on PTB ( $r=0.85$ ), and an even stronger relationship on WT103 ( $r=0.94$ ). This is partially explained by WT103 having much longer sentences, and thus less noisy statistics. From Figure 1 , we find that QRNN models yield strongly linear log perplexity\u2013recall plots as well, where $r=0.88$ and $r=0.93$ for",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Do they train a different training method except from scheduled sampling?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "e326f683-f1c3-46c9-ba78-fd45c5c6a175",
        "metadata": {
          "vector_store_key": "1908.08419-8",
          "chunk_id": 64,
          "document_id": "1908.08419",
          "start_idx": 23290,
          "end_idx": 23799
        },
        "page_content": " to annotate the unlabeled set. Then, sampling strategy in active learning selects a part of samples for annotators to relabel. Finally, the relabeled samples are added to train set for segmenter to re-train. Our proposed scoring strategy selects samples according to the sequence scores of the segmented sentences, while uncertainty sampling suggests relabeling samples that are closest to the segmenter\u2019s decision boundary. Generally, two main parameters in active learning are the numbers of iterations and",
        "type": "Document"
      },
      {
        "id": "270630ad-16a4-4d78-9767-8ea17f811346",
        "metadata": {
          "vector_store_key": "1909.08859-5",
          "chunk_id": 51,
          "document_id": "1909.08859",
          "start_idx": 17614,
          "end_idx": 18113
        },
        "page_content": " We used Adam optimizer with a learning rate of 1e-4 with an early stopping criteria with the patience set to 10 indicating that the training procedure ends after 10 iterations if the performance would not improve. We considered a batch size of 32 due to our hardware constraints. In the multi-task setting, batches are sampled round-robin from all tasks, where each batch is solely composed of examples from one task. We performed our experiments on a system containing four NVIDIA GTX-1080Ti GPUs,",
        "type": "Document"
      },
      {
        "id": "8ce2edd8-044c-45e7-95fc-4066c08de13e",
        "metadata": {
          "vector_store_key": "1908.08419-0",
          "chunk_id": 22,
          "document_id": "1908.08419",
          "start_idx": 8307,
          "end_idx": 8763
        },
        "page_content": " which chooses samples that need to be relabeled by annotators from unlabeled data. Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, a CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively. Fig. FIGREF7 and Algorithm SECREF3 demonstrate the procedure of CWS based on active learning. First, we train",
        "type": "Document"
      },
      {
        "id": "535a4ad4-3aff-4297-862c-94166676565b",
        "metadata": {
          "vector_store_key": "1911.03894-0",
          "chunk_id": 51,
          "document_id": "1911.03894",
          "start_idx": 18085,
          "end_idx": 18526
        },
        "page_content": " learning rates and batch sizes. We select the best model on the validation set out of the 30 first epochs. Although this might push the performances even further, for all tasks except NLI, we don't apply any regularisation techniques such as weight decay, learning rate warm-up or discriminative fine-tuning. We show that fine-tuning CamemBERT in a straight-forward manner leads to state-of-the-art results on most tasks and outperforms the",
        "type": "Document"
      },
      {
        "id": "3c70b509-3255-4930-bd99-7ff78126c809",
        "metadata": {
          "vector_store_key": "1908.08419-8",
          "chunk_id": 19,
          "document_id": "1908.08419",
          "start_idx": 7019,
          "end_idx": 7526
        },
        "page_content": " of active learning has resulted in various sampling strategies, such as uncertainty sampling BIBREF24 , query-by-committee BIBREF25 and information gain BIBREF26 . Currently, the most mainstream sampling strategy is uncertainty sampling. It focuses its selection on samples closest to the decision boundary of the classifier and then chooses these samples for annotators to relabel BIBREF27 . The formal definition of uncertainty sampling is to select a sample INLINEFORM0 that maximizes the entropy INLINE",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they define upward and downward reasoning?",
    "llm_answer": "The text describes upward/downward expressions related to arguments within sentences, but does not explicitly define upward and downward reasoning.\n",
    "context": [
      {
        "id": "bb58be07-d1a3-4d43-9aa8-46bcd2d37a8f",
        "metadata": {
          "vector_store_key": "1906.06448-3",
          "chunk_id": 19,
          "document_id": "1906.06448",
          "start_idx": 6603,
          "end_idx": 7096
        },
        "page_content": "c) replace the phrase in the argument position with a more general or specific phrase in natural and various ways (e.g., by using lexical knowledge or logical connectives). For (a) and (b), we first conduct polarity computation on a syntactic structure for each sentence, and then select premises involving upward/downward expressions. For (c), we use crowdsourcing to narrow or broaden the arguments. The motivation for using crowdsourcing is to collect naturally alike monotonicity inference",
        "type": "Document"
      },
      {
        "id": "8ec6d30c-9ba3-4287-abba-7f0ac6bcf122",
        "metadata": {
          "vector_store_key": "1906.06448-8",
          "chunk_id": 67,
          "document_id": "1906.06448",
          "start_idx": 25183,
          "end_idx": 25612
        },
        "page_content": "P$ : No racin' on the Range $H$ : No horse racing is allowed on the Range  One possible reason why there are few downward inferences is that certain pragmatic factors can block people to draw a downward inference. For instance, in the case of the inference problem in ( \"Discussion\" ), unless the added disjunct in $H$ , i.e., a small cat with green eyes, is salient in the context, it would be difficult to draw the conclusion $",
        "type": "Document"
      },
      {
        "id": "bf4f48a6-d5e0-4471-92cb-3ff632f7af6e",
        "metadata": {
          "vector_store_key": "1906.06448-3",
          "chunk_id": 14,
          "document_id": "1906.06448",
          "start_idx": 4926,
          "end_idx": 5370
        },
        "page_content": "subseteq $ bought a show ticket), or adding disjunction (bought a movie ticket $\\sqsubseteq $ bought or sold a movie ticket). Table 1 shows determiners modeled as binary operators and their polarities with respect to the first and second arguments. There are various types of downward operators, not limited to determiners (see Table 2 ). As shown in ( \"Monotonicity\" ), if a propositional object is embedded in a downward monotonic context (e.",
        "type": "Document"
      },
      {
        "id": "2381dd3d-6125-4ab2-abc2-d3f97289a0d7",
        "metadata": {
          "vector_store_key": "1906.06448-8",
          "chunk_id": 66,
          "document_id": "1906.06448",
          "start_idx": 24738,
          "end_idx": 25183
        },
        "page_content": " few and the upward operator a few. The results of crowdsourcing tasks in Section 3.1.3 showed that some downward inferences can naturally be performed in human reasoning. However, we also found that the MultiNLI training set BIBREF10 , which is one of the dataset created from naturally-occurring texts, contains only 77 downward inference problems, including the following one.  $P$ : No racin' on the Range $H$ : No horse racing is allowed on",
        "type": "Document"
      },
      {
        "id": "abd46636-b132-47bf-8389-56630ba5cd40",
        "metadata": {
          "vector_store_key": "1906.06448-8",
          "chunk_id": 26,
          "document_id": "1906.06448",
          "start_idx": 9125,
          "end_idx": 9648
        },
        "page_content": " difficult to rephrase them in a natural way (e.g., every one, no time). We removed examples involving such expressions. To collect more downward inference examples, we select examples involving determiners in Table 1 and downward operators in Table 2 . As a result, we selected 1,485 examples involving expressions having arguments with upward monotonicity and 1,982 examples involving expressions having arguments with downward monotonicity. We present crowdworkers with a sentence whose polarized part is underlined, and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Do they annotate their own dataset or use an existing one?",
    "llm_answer": "The context describes annotating a dataset.\n",
    "context": [
      {
        "id": "41b33e3c-7622-4a3a-bf41-e8b160505f06",
        "metadata": {
          "vector_store_key": "1910.11769-4",
          "chunk_id": 13,
          "document_id": "1910.11769",
          "start_idx": 4963,
          "end_idx": 5497
        },
        "page_content": " using in-house data annotators who are experts in narrative content. A passage is accepted as valid if the in-house annotator's label matched any one of the MTurk annotators' labels. The remaining passages are discarded. We provide the fraction of annotator agreement for each label in the dataset. Though passages may lose some emotional context when read independently of the complete narrative, we believe annotator agreement on our dataset supports the assertion that small excerpts can still convey coherent emotions. During the",
        "type": "Document"
      },
      {
        "id": "9d180fa4-0178-4f9f-8997-37024ccf17f9",
        "metadata": {
          "vector_store_key": "1909.00430-8",
          "chunk_id": 0,
          "document_id": "1909.00430",
          "start_idx": 0,
          "end_idx": 440
        },
        "page_content": "Data annotation is a key bottleneck in many data driven algorithms. Specifically, deep learning models, which became a prominent tool in many data driven tasks in recent years, require large datasets to work well. However, many tasks require manual annotations which are relatively hard to obtain at scale. An attractive alternative is lightly supervised learning BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF",
        "type": "Document"
      },
      {
        "id": "3b541d42-de69-4c8c-97f5-634e76f12a94",
        "metadata": {
          "vector_store_key": "1911.07228-1",
          "chunk_id": 54,
          "document_id": "1911.07228",
          "start_idx": 14818,
          "end_idx": 15154
        },
        "page_content": " BIO format (Inside, Outside, Beginning) BIBREF6 at the top layer (CRF). If we can unify the labeling of the annotators between the train, dev and test sets. We will improve data quality and classifier. It is better if there is a pre-trained word embeddings that overlays the data, and segmentation algorithm need to be more accurately.",
        "type": "Document"
      },
      {
        "id": "d4c4849a-b0a6-4931-82a7-aed67b41eb03",
        "metadata": {
          "vector_store_key": "1703.05260-7",
          "chunk_id": 19,
          "document_id": "1703.05260",
          "start_idx": 7578,
          "end_idx": 8108
        },
        "page_content": ". This section deals with the annotation of the data. We first describe the final annotation schema. Then, we describe the iterative process of corpus annotation and the refinement of the schema. This refinement was necessary due to the complexity of the annotation. For each of the scenarios, we designed a specific annotation template. A script template consists of scenario-specific event and participant labels. An example of a template is shown in Table 1 . All NP heads in the corpus were annotated with a participant label;",
        "type": "Document"
      },
      {
        "id": "f9e9e3ec-e213-4cd5-ae8e-8d986c9fd2db",
        "metadata": {
          "vector_store_key": "2002.11893-1",
          "chunk_id": 22,
          "document_id": "2002.11893",
          "start_idx": 8390,
          "end_idx": 8922
        },
        "page_content": " before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states. Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What kind of neural network architecture do they use?",
    "llm_answer": "Neural regressor (NNR) and neural classifier (NNC).  NNC is used in two setups: \"NNC top 5\" and \"NNC with an explicit reasoning module\".  BiLSTM and BERT are also used as baselines.\n",
    "context": [
      {
        "id": "10ee7c8c-8492-4d26-9ee2-5244b2aa61f4",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 20,
          "document_id": "1909.00542",
          "start_idx": 7743,
          "end_idx": 8155
        },
        "page_content": " layer, and use cross-entropy as the loss function. The complete architecture is shown in Fig. FIGREF28. The bottom section of Table TABREF26 shows the results of several variants of the neural architecture. The table includes a neural regressor (NNR) and a neural classifier (NNC). The neural classifier is trained in two set ups: \u201cNNC top 5\u201d uses classification labels as described in Section SECREF3, and \u201cNNC",
        "type": "Document"
      },
      {
        "id": "2e27377a-5499-4f47-9c90-c8a9a21ce330",
        "metadata": {
          "vector_store_key": "1909.08859-1",
          "chunk_id": 11,
          "document_id": "1909.08859",
          "start_idx": 4603,
          "end_idx": 5140
        },
        "page_content": " with an explicit reasoning module that acts on entity-specific relational memory units. Fig. FIGREF4 shows an overview of the network architecture. It consists of five main modules: An input module, an attention module, a reasoning module, a modeling module, and an output module. Note that the question answering tasks we consider here are multimodal in that while the context is a procedural text, the question and the multiple choice answers are composed of images. Input Module extracts vector representations of inputs at different",
        "type": "Document"
      },
      {
        "id": "218e5a42-45d3-4f65-821b-d5e2beeddb13",
        "metadata": {
          "vector_store_key": "2002.06644-0",
          "chunk_id": 8,
          "document_id": "2002.06644",
          "start_idx": 3205,
          "end_idx": 3642
        },
        "page_content": " the local word order efficiently. BiLSTM: Unlike feedforward neural networks, recurrent neural networks like BiLSTMs use memory based on history information to learn long-distance features and then predict the output. We use a two-layer BiLSTM architecture with GloVe word embeddings as a strong RNN baseline. BERT BIBREF5: It is a contextualized word representation model that uses bidirectional transformers, pretrained on a large $3.",
        "type": "Document"
      },
      {
        "id": "817a9fbd-0ad5-4350-9d90-c039e237a296",
        "metadata": {
          "vector_store_key": "1710.01492-5",
          "chunk_id": 53,
          "document_id": "1710.01492",
          "start_idx": 19472,
          "end_idx": 19941
        },
        "page_content": " particular, the best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks BIBREF53 , BIBREF54 . Conversely, kernel machines seem to be less frequently used than in the past, and the use of learning methods other than the ones mentioned above is at this point scarce. All these models are examples of supervised learning as they need labeled training data. Semi-supervised learning. We should note two things about the use of deep neural",
        "type": "Document"
      },
      {
        "id": "577fb06e-9340-4422-93f3-91bc89574aee",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 43,
          "document_id": "2001.00137",
          "start_idx": 16781,
          "end_idx": 17197
        },
        "page_content": " the online platforms. Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What web and user-generated NER datasets are used for the analysis?",
    "llm_answer": "UMBC corpus for Twitter NER, and a further Twitter NER corpus.  WEB and TWI.\n",
    "context": [
      {
        "id": "827a2859-9f04-4487-a2f1-14ee3cfe97ec",
        "metadata": {
          "vector_store_key": "1701.02877-7",
          "chunk_id": 12,
          "document_id": "1701.02877",
          "start_idx": 4315,
          "end_idx": 4755
        },
        "page_content": " NER approaches, namely Stanford NER BIBREF34 , SENNA BIBREF35 and CRFSuite BIBREF36 (see Section \"NER Models and Features\" for details). Since the goal of this study is to compare NER performance on corpora from diverse domains and genres, seven benchmark NER corpora are included, spanning newswire, broadcast conversation, Web content, and social media (see Table 1 for details). These datasets were chosen such that they have been annot",
        "type": "Document"
      },
      {
        "id": "88a6f2c9-fa7c-4999-bc75-792ef40761a8",
        "metadata": {
          "vector_store_key": "1701.02877-1",
          "chunk_id": 10,
          "document_id": "1701.02877",
          "start_idx": 3620,
          "end_idx": 4066
        },
        "page_content": " primarily by F1 score), and also to study the relationship between OOV NEs and features and F1. See Section \"Experiments\" for details. To ensure representativeness and comprehensiveness, our experimental findings are based on key benchmark NER corpora spanning multiple genres, time periods, and corpus annotation methodologies and guidelines. As detailed in Section \"Datasets\" , the corpora studied are OntoNotes BIBREF29 , ACE BIBREF30 , MUC 7",
        "type": "Document"
      },
      {
        "id": "2c1de9f3-1cd6-4cd9-8d12-f7fe403598f4",
        "metadata": {
          "vector_store_key": "1701.02877-7",
          "chunk_id": 21,
          "document_id": "1701.02877",
          "start_idx": 7791,
          "end_idx": 8297
        },
        "page_content": " studying NER performance on this genre gained momentum. So far, there have been no big evaluation efforts, such as ACE and OntoNotes, resulting in substantial amounts of gold standard data. Instead, benchmark corpora were created as part of smaller challenges or individual projects. The first such corpus is the UMBC corpus for Twitter NER BIBREF33 , where researchers used crowdsourcing to obtain annotations for persons, locations and organisations. A further Twitter NER corpus was created by BIBREF21",
        "type": "Document"
      },
      {
        "id": "4064abd7-9af9-48a4-a020-2ec5ba6d7dbb",
        "metadata": {
          "vector_store_key": "1701.02877-7",
          "chunk_id": 20,
          "document_id": "1701.02877",
          "start_idx": 7291,
          "end_idx": 7791
        },
        "page_content": " NER, but also for other tasks such as coreference resolution, relation and event extraction and word sense disambiguation. In this paper, however, we restrict ourselves purely to the English NER annotations, for consistency across datasets. The ACE corpus contains HEAD as well as EXTENT annotations for NE spans. For our experiments we use the EXTENT tags. With the emergence of social media, studying NER performance on this genre gained momentum. So far, there have been no big evaluation efforts",
        "type": "Document"
      },
      {
        "id": "80c182c0-5774-418d-8ba6-2ad1af11f899",
        "metadata": {
          "vector_store_key": "1701.02877-1",
          "chunk_id": 88,
          "document_id": "1701.02877",
          "start_idx": 31506,
          "end_idx": 31962
        },
        "page_content": " genres. On average, newswire corpora and OntoNotes MZ are the easiest corpora and ACE UN, WEB and TWI are the hardest corpora for NER methods to reach good performance on. Normalising corpora by size results in more noisy data such as TWI and WEB data achieving similar results to NW corpora. Increasing the amount of available in-domain training data will likely result in improved NERC performance. There is a strong positive correlation between high F1",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which unlabeled data do they pretrain with?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "337dbf0b-4024-42e2-8575-fa8615a5722a",
        "metadata": {
          "vector_store_key": "1710.01492-5",
          "chunk_id": 56,
          "document_id": "1710.01492",
          "start_idx": 20569,
          "end_idx": 20967
        },
        "page_content": " the network, e.g., with convolution or with LSTMs BIBREF55 , BIBREF54 , BIBREF56 , need to learn something like a language model, i.e., which word is likely to follow which one. Training these parts needs no labels. While these parts can be also pre-trained, it is easier, and often better, to use self training. Distantly-supervised learning. Another way to make use of large unannotated datasets",
        "type": "Document"
      },
      {
        "id": "50585b71-98c0-4fa7-a5cb-290234a35f4d",
        "metadata": {
          "vector_store_key": "1904.05862-2",
          "chunk_id": 0,
          "document_id": "1904.05862",
          "start_idx": 0,
          "end_idx": 568
        },
        "page_content": "Current state of the art models for speech recognition require large amounts of transcribed audio data to attain good performance BIBREF1 . Recently, pre-training of neural networks has emerged as an effective technique for settings where labeled data is scarce. The key idea is to learn general representations in a setup where substantial amounts of labeled or unlabeled data is available and to leverage the learned representations to improve performance on a downstream task for which the amount of data is limited. This is particularly interesting for tasks where",
        "type": "Document"
      },
      {
        "id": "07a9fe7e-c358-4f14-8b0e-d45d17c38785",
        "metadata": {
          "vector_store_key": "1909.00124-3",
          "chunk_id": 38,
          "document_id": "1909.00124",
          "start_idx": 11950,
          "end_idx": 12394
        },
        "page_content": ", the sentiment of each aspect gives the ground-truth as the sentiments of all aspects are the same. For each clean-labeled dataset, the sentences are randomly partitioned into training set and test set with $80\\%$ and $20\\%$, respectively. Following BIBREF25, We also randomly select $10\\%$ of the test data for validation to check the model during training. Summary statistics of the training, validation, and test data are shown in Table TAB",
        "type": "Document"
      },
      {
        "id": "faa265f1-ef3e-4ef9-a60b-c6f40ff9ef6c",
        "metadata": {
          "vector_store_key": "1909.00124-3",
          "chunk_id": 40,
          "document_id": "1909.00124",
          "start_idx": 12671,
          "end_idx": 13117
        },
        "page_content": " we treat 4 or 5 stars as positive and 1 or 2 stars as negative. The data is noisy because a positive (negative) review can contain negative (positive) sentences, and there are also neutral sentences. This gives us three noisy-labeled training datasets. We still use the same test sets as those for the clean-labeled datasets. Summary statistics of all the datasets are shown in Table TABREF9. Experiment 1: Here we use the clean-labeled data (i.",
        "type": "Document"
      },
      {
        "id": "c94d339e-36f5-4336-a21a-05be0b487cef",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many convolutional layers does their model have?",
    "llm_answer": "Two\n",
    "context": [
      {
        "id": "4f7009fe-338f-46f2-b7a6-b9a6f5dc6949",
        "metadata": {
          "vector_store_key": "1910.02789-9",
          "chunk_id": 71,
          "document_id": "1910.02789",
          "start_idx": 28472,
          "end_idx": 28930
        },
        "page_content": " has 8 6X6 filters with stride 3 and ReLU activation. The second convolutional layer has 16 3X3 filters with stride 2 and ReLU activation. The fully connected layers has 32 and 16 units, both of them are followed by ReLU activation. The output layer's size is the amount of actions the agent has available in the trained scenario. Used in the feature vector based agent. Naturally, some discretization is needed in order to build a feature vector, so some of",
        "type": "Document"
      },
      {
        "id": "959ad849-5186-4047-b196-10acf4254acc",
        "metadata": {
          "vector_store_key": "1701.00185-1",
          "chunk_id": 72,
          "document_id": "1701.00185",
          "start_idx": 26026,
          "end_idx": 26417
        },
        "page_content": " ) is fixed to 15 when constructing the graph structures for LE and LPI. For CNN model, the networks has two convolutional layers. The widths of the convolutional filters are both 3. The value of INLINEFORM0 for the top INLINEFORM1 -max pooling in Eqn. ( EQREF10 ) is 5. The number of feature maps at the first convolutional layer is 12, and 8 feature maps at the second convolutional layer.",
        "type": "Document"
      },
      {
        "id": "4928712b-9151-49db-bc4e-bb142e14a1ca",
        "metadata": {
          "vector_store_key": "1811.00942-4",
          "chunk_id": 12,
          "document_id": "1811.00942",
          "start_idx": 4741,
          "end_idx": 5020
        },
        "page_content": " comprises two separate parts: a convolution layer with three weights, and a recurrent pooling layer. Given an input $\\mathbf {X} \\in \\mathbb {R}^{k \\times n}$ , the convolution layer is $\n\\mathbf {Z} = \\tanh (\\mathbf {W}_z \\cdot \\mathbf {X})\\\\\n\\mathbf {F} = \\sigma (\\mathbf {W}_",
        "type": "Document"
      },
      {
        "id": "fb9afdd8-bd9b-430b-8402-e1f4401051bc",
        "metadata": {
          "vector_store_key": "1908.10449-1",
          "chunk_id": 39,
          "document_id": "1908.10449",
          "start_idx": 13757,
          "end_idx": 14232
        },
        "page_content": " the encoding transformer blocks. Each encoding transformer block consists of four convolutional layers (with shared weights), a self-attention layer, and an MLP. Each convolutional layer has 96 filters, each kernel's size is 7. In the self-attention layer, we use a block hidden size of 96 and a single head attention mechanism. Layer normalization and dropout are applied after each component inside the block. We add positional encoding into each block's input. We use one",
        "type": "Document"
      },
      {
        "id": "d2fef74b-c1bb-4ee7-8c73-3030941084a0",
        "metadata": {
          "vector_store_key": "1701.00185-1",
          "chunk_id": 73,
          "document_id": "1701.00185",
          "start_idx": 26417,
          "end_idx": 26850
        },
        "page_content": " first convolutional layer is 12, and 8 feature maps at the second convolutional layer. Both those two convolutional layers are followed by a folding layer. We further set the dimension of word embeddings INLINEFORM2 as 48. Finally, the dimension of the deep feature representation INLINEFORM3 is fixed to 480. Moreover, we set the learning rate INLINEFORM4 as 0.01 and the mini-batch training size as 200. The output size INLINEFORM",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big are the datasets?",
    "llm_answer": "000/1,610 pieces of news, vocabulary size of more than 100k.\n",
    "context": [
      {
        "id": "0723454a-4062-463e-972a-81a58c6a504e",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "04fe1abe-a675-484e-963d-9d80c75d505e",
        "metadata": {
          "vector_store_key": "1908.10449-2",
          "chunk_id": 66,
          "document_id": "1908.10449",
          "start_idx": 22880,
          "end_idx": 23381
        },
        "page_content": " hard time learning in this setting. As shown in Table TABREF16, both datasets have a vocabulary size of more than 100k. This is much larger than in the other two settings, where on average the length of questions is around 10. This suggests that the methods with better sample efficiency are needed to act in more realistic problem settings with huge action spaces. Experiments also show that a larger memory slot size always helps. Intuitively, with a memory mechanism (either implicit or explicit),",
        "type": "Document"
      },
      {
        "id": "84720de1-c988-45c6-bcbe-7a0435cdf6fb",
        "metadata": {
          "vector_store_key": "1706.08032-0",
          "chunk_id": 38,
          "document_id": "1706.08032",
          "start_idx": 14264,
          "end_idx": 14661
        },
        "page_content": " datasets in Table III. for all datasets, the filter window size ( INLINEFORM0 ) is 7 with 6 feature maps each for the first wide convolution layer, the second wide convolution layer has a filter window size of 5 with 14 feature maps each. Dropout rate ( INLINEFORM1 ) is 0.5, INLINEFORM2 constraint, learning rate is 0.1 and momentum of 0.9. Mini-batch size for STS Corpus is 100 and others are 4",
        "type": "Document"
      },
      {
        "id": "6a849beb-7f02-435c-9188-73e3f6b1d235",
        "metadata": {
          "vector_store_key": "1701.02877-1",
          "chunk_id": 27,
          "document_id": "1701.02877",
          "start_idx": 9957,
          "end_idx": 10406
        },
        "page_content": " training and testing data sizes measured in number of NEs are listed in Table 2 . Separate models are then trained on the training parts of each corpus and evaluated on the development (if available) and test parts of the same corpus. If development parts are available, as they are for CoNLL (CoNLL Test A) and MUC (MUC 7 Dev), they are not merged with the training corpora for testing, as it was permitted to do in the context of those evaluation",
        "type": "Document"
      },
      {
        "id": "4cac7588-28e9-4b6e-ac0d-d763db64bc58",
        "metadata": {
          "vector_store_key": "1703.04617-6",
          "chunk_id": 3,
          "document_id": "1703.04617",
          "start_idx": 1343,
          "end_idx": 1868
        },
        "page_content": " can help attain better results on our competitive baselines. Recent advance on reading comprehension and question answering has been closely associated with the availability of various datasets. BIBREF0 released the MCTest data consisting of 500 short, fictional open-domain stories and 2000 questions. The CNN/Daily Mail dataset BIBREF1 contains news articles for close style machine comprehension, in which only entities are removed and tested for comprehension. Children's Book Test (CBT) BIBREF2 leverages named entities",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What baselines did they compare their model with?",
    "llm_answer": "Baseline and Ablation models;  content, content-network, and image-based models; NABERT+ baseline.\n",
    "context": [
      {
        "id": "dc50a5cf-b755-41a0-a173-1b561db0ed05",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "a1974dac-f964-4aeb-a415-a2006317d90a",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 68,
          "document_id": "1810.00663",
          "start_idx": 26280,
          "end_idx": 26763
        },
        "page_content": " room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM. Note that the average number of actions in the ground truth output sequences is 7.07 for the Test-New set. Our model's predictions are just INLINEFORM1 edits off on average from the correct navigation plans. This",
        "type": "Document"
      },
      {
        "id": "25bb2afe-7835-48e9-a6f8-dbdbbde2116d",
        "metadata": {
          "vector_store_key": "1902.06843-8",
          "chunk_id": 91,
          "document_id": "1902.06843",
          "start_idx": 35139,
          "end_idx": 35416
        },
        "page_content": "s by -0.27. Baselines: To test the efficacy of our multi-modal framework for detecting depressed users, we compare it against existing content, content-network, and image-based models (based on the aforementioned general image feature, facial presence, and facial expressions.)",
        "type": "Document"
      },
      {
        "id": "555a266e-0a08-43e5-b719-8cd2fd10ebb8",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "f0b7e3cd-e054-4500-ae09-a0ffa726b77b",
        "metadata": {
          "vector_store_key": "1812.07023-3",
          "chunk_id": 46,
          "document_id": "1812.07023",
          "start_idx": 17433,
          "end_idx": 17962
        },
        "page_content": " set have been provided by the challenge organizers. For the prototype test set and for the ablation study presented in Table TABREF24 , we use the same code for evaluation metrics as used by BIBREF11 for fairness and comparability. We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below: Our primary architectural differences over the baseline model are: not concatenating the question, answer pairs before encoding them, the auxiliary decoder module,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was the performance of their model?",
    "llm_answer": "The model achieved an F1 score of 57% on the AMMER corpus.  Results on other datasets and metrics are also described.\n",
    "context": [
      {
        "id": "148ea921-3ae7-4687-bc70-85a9690fe58b",
        "metadata": {
          "vector_store_key": "1912.13109-4",
          "chunk_id": 20,
          "document_id": "1912.13109",
          "start_idx": 7921,
          "end_idx": 8471
        },
        "page_content": " on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we",
        "type": "Document"
      },
      {
        "id": "31e3f6a2-9e9b-4d6d-824a-22195518284a",
        "metadata": {
          "vector_store_key": "1909.02764-7",
          "chunk_id": 52,
          "document_id": "1909.02764",
          "start_idx": 20420,
          "end_idx": 20841
        },
        "page_content": "=73 % on TEC. The model trained on our AMMER corpus achieves an F1 score of 57%. This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set. These results are mostly in line with Bostan2018. Now we analyze how well the models trained in Experiment 1 perform when applied to our data set. The results are shown in column \u201cSimple\u201d in Table TAB",
        "type": "Document"
      },
      {
        "id": "555a266e-0a08-43e5-b719-8cd2fd10ebb8",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "08676cf7-5cf4-4a2c-b9d7-ddbe513c2062",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 66,
          "document_id": "1810.00663",
          "start_idx": 25415,
          "end_idx": 25983
        },
        "page_content": " In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models. The previous section evaluated model performance on new instructions (and corresponding navigation plans) for environments that were previously seen at training time. Here, we examine whether the trained models succeed on environments that are completely new. The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as",
        "type": "Document"
      },
      {
        "id": "dc50a5cf-b755-41a0-a173-1b561db0ed05",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What evaluation metrics are used?",
    "llm_answer": "BLEU, BIBREF4, METEOR, ROUGE B, F1 score, Recall rate, Overall accuracy.\n",
    "context": [
      {
        "id": "175d0a0b-77ff-450c-8f37-24f131b845e7",
        "metadata": {
          "vector_store_key": "1905.08949-9",
          "chunk_id": 28,
          "document_id": "1905.08949",
          "start_idx": 10602,
          "end_idx": 11043
        },
        "page_content": " they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks. As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE B",
        "type": "Document"
      },
      {
        "id": "cc46f099-eec3-4c72-b5ce-b58340f9e2ea",
        "metadata": {
          "vector_store_key": "1909.00578-4",
          "chunk_id": 7,
          "document_id": "1909.00578",
          "start_idx": 2625,
          "end_idx": 3083
        },
        "page_content": " MT evaluation metrics BIBREF4, BIBREF18. QE models for MT make use of large post-edited datasets, and apply machine learning methods to predict post-editing effort scores and quality (good/bad) labels. We apply QE to summarization, focusing on linguistic qualities that reflect the readability and fluency of the generated texts. Since no post-edited datasets \u2013 like the ones used in MT \u2013 are available for summarization, we use instead the ratings assigned",
        "type": "Document"
      },
      {
        "id": "8135afc2-1b5e-4ad4-968f-868afaac9ba6",
        "metadata": {
          "vector_store_key": "1905.08949-9",
          "chunk_id": 30,
          "document_id": "1905.08949",
          "start_idx": 11311,
          "end_idx": 11823
        },
        "page_content": ". To overcome this, BIBREF46 proposed a new metric to evaluate the \u201canswerability\u201d of a question by calculating the scores for several question-specific factors, including question type, content words, function words, and named entities. However, as it is newly proposed, it has not been applied to evaluate any NQG system yet. To accurately measure what makes a good question, especially deep questions, improved evaluation schemes are required to specifically investigate the mechanism of question asking. Many",
        "type": "Document"
      },
      {
        "id": "674affb1-9973-4581-bdda-e127f9f0533b",
        "metadata": {
          "vector_store_key": "1908.10449-2",
          "chunk_id": 31,
          "document_id": "1908.10449",
          "start_idx": 10837,
          "end_idx": 11300
        },
        "page_content": "RC involves both MRC and RL, we adopt evaluation metrics from both settings. First, as a question answering task, we use $\\text{F}_1$ score to compare predicted answers against ground-truth, as in previous works. When there exist multiple ground-truth answers, we report the max $\\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we evaluate an agent's performance during both its training and testing phases.",
        "type": "Document"
      },
      {
        "id": "638d3f9b-929b-4fa6-b0d4-efe44cd66255",
        "metadata": {
          "vector_store_key": "1912.13109-2",
          "chunk_id": 25,
          "document_id": "1912.13109",
          "start_idx": 10203,
          "end_idx": 10681
        },
        "page_content": " metrics - Recall rate, F1 score and Overall accuracy. Choice of model parameters were in the above models were inspired from previous work done but then were tuned to the best performance of the Test dataset. Following parameters were considered for tuning. Learning rate: Based on grid search the best performance was achieved when learning rate was set to 0.01. This value was arrived by a grid search on lr parameter. Number of Bidirectional LSTM units: A set of 32, 64, 128",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How were the navigation instructions collected?",
    "llm_answer": "Collected through Mechanical Turk using 100 simulated environments with corresponding topological maps.\n",
    "context": [
      {
        "id": "41b38377-693b-4e12-9695-c3a0c66dd38d",
        "metadata": {
          "vector_store_key": "1810.00663-4",
          "chunk_id": 9,
          "document_id": "1810.00663",
          "start_idx": 3631,
          "end_idx": 4203
        },
        "page_content": " of behaviors to reach the desired destination according to the instructions and the map (Fig. FIGREF4 (c)). Our main insight is that using attention mechanisms to correlate navigation instructions with the topological map of the environment can facilitate predicting correct navigation plans. This work also contributes a new dataset of INLINEFORM0 pairs of free-form natural language instructions and high-level navigation plans. This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of",
        "type": "Document"
      },
      {
        "id": "6eadfdd1-23f4-4edc-b199-7b76c7dc81ed",
        "metadata": {
          "vector_store_key": "1810.00663-4",
          "chunk_id": 48,
          "document_id": "1810.00663",
          "start_idx": 18696,
          "end_idx": 19261
        },
        "page_content": " instructions is not always the same. For instance, a person said \u201cturn right and advance\u201d to describe part of a route, while another person said \u201cgo straight after turning right\u201d in a similar situation. The high variability present in the natural language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort. This section describes our evaluation of the proposed approach for interpreting navigation commands in natural",
        "type": "Document"
      },
      {
        "id": "3eeb420a-9ba5-4f61-bea2-37c164636bc2",
        "metadata": {
          "vector_store_key": "1807.03367-8",
          "chunk_id": 29,
          "document_id": "1807.03367",
          "start_idx": 11628,
          "end_idx": 12165
        },
        "page_content": " guide predicts that the tourist is at target, we evaluate its location. If successful, the task ends, otherwise we continue until there have been three wrong evaluations. The protocol is given as pseudo-code in Appendix SECREF12 . The designed navigation protocol relies on a trained localization model that predicts the tourist's location from a communicated message. Before we formalize this localization sub-task in Section UID21 , we further introduce two simplifying assumptions\u2014perfect perception and orientation-agnosticism\u2014so as",
        "type": "Document"
      },
      {
        "id": "c46fab2f-a7c4-405e-ab3c-4ba9da6632ce",
        "metadata": {
          "vector_store_key": "1810.00663-4",
          "chunk_id": 47,
          "document_id": "1810.00663",
          "start_idx": 18361,
          "end_idx": 18910
        },
        "page_content": " was collected from 88 unique simulated environments, totaling 6064 distinct navigation plans (2002 plans have two different navigation instructions each; the rest has one). The dataset contains two test set variants: While the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said \u201cturn right and advance\ufffd",
        "type": "Document"
      },
      {
        "id": "cbee24eb-6c63-48e7-bf4a-9ac62a9715ce",
        "metadata": {
          "vector_store_key": "1810.00663-4",
          "chunk_id": 10,
          "document_id": "1810.00663",
          "start_idx": 3977,
          "end_idx": 4584
        },
        "page_content": " collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation. The dataset opens up opportunities to explore data-driven methods for grounding navigation commands into high-level motion plans. We conduct extensive experiments to study the generalization capabilities of the proposed model for following natural language instructions. We investigate both generalization to new instructions in known and in new environments. We conclude this paper by discussing the benefits of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What language is the experiment done in?",
    "llm_answer": "English.\n",
    "context": [
      {
        "id": "17f3676f-8f5c-4da9-b0c7-cede94c8171d",
        "metadata": {
          "vector_store_key": "1904.05862-2",
          "chunk_id": 42,
          "document_id": "1904.05862",
          "start_idx": 15343,
          "end_idx": 15454
        },
        "page_content": "letter++ experiments, and Tatiana Likhomanenko for providing convolutional language models for our experiments.",
        "type": "Document"
      },
      {
        "id": "e90f75a5-f326-4202-82c5-878736af8308",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 51,
          "document_id": "1910.04269",
          "start_idx": 19553,
          "end_idx": 20023
        },
        "page_content": ", the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Kat",
        "type": "Document"
      },
      {
        "id": "b0d78a46-0cfa-4367-a963-2a9a8d82c9a4",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 18,
          "document_id": "1910.04269",
          "start_idx": 6853,
          "end_idx": 7342
        },
        "page_content": " comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (",
        "type": "Document"
      },
      {
        "id": "a9eede7a-1197-4843-89f6-7bc97d563a53",
        "metadata": {
          "vector_store_key": "1911.00069-3",
          "chunk_id": 63,
          "document_id": "1911.00069",
          "start_idx": 19862,
          "end_idx": 20285
        },
        "page_content": " in the previous works was motivated by domain adaptation, the focus of this paper is on cross-lingual model transfer, and hence we apply a random data split as follows. For the source language English and each target language, we randomly select $80\\%$ of the data as the training set, $10\\%$ as the development set, and keep the remaining $10\\%$ as the test set. The sizes of the sets are summarized in Table TABREF41. We",
        "type": "Document"
      },
      {
        "id": "4199c59c-1927-47cb-857b-9c1bed5dae20",
        "metadata": {
          "vector_store_key": "1911.03894-7",
          "chunk_id": 44,
          "document_id": "1911.03894",
          "start_idx": 15623,
          "end_idx": 16121
        },
        "page_content": " is also machine translated for all languages. The dataset is composed of 122k train, 2490 valid and 5010 test examples. As usual, NLI performance is evaluated using accuracy. To evaluate a model on a language other than English (such as French), we consider the two following settings: TRANSLATE-TEST: The French test set is machine translated into English, and then used with an English classification model. This setting provides a reasonable, although imperfect, way to circumvent the fact that",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What additional features are proposed for future work?",
    "llm_answer": "Combining structural, topical, and identity-based measures to characterize user engagement; extending analyses to diverse communities on different platforms; exploring more interesting sentiment analysis formulations; improvements in neural network architectures (e.g., concatenating high-level features, deeper networks with skip connections and Inception modules); exploring various feature extraction techniques (e.g., Constant-Q transform, Fast Fourier Transform); and training models for longer, with additional downstream tasks, baselines and analysis on cleaner corpora.\n",
    "context": [
      {
        "id": "a2719932-7c91-4b04-b63e-41ede86c59db",
        "metadata": {
          "vector_store_key": "1705.09665-4",
          "chunk_id": 88,
          "document_id": "1705.09665",
          "start_idx": 36224,
          "end_idx": 36805
        },
        "page_content": " newcomers at certain points in their lifecycle? Another important avenue of future work is to explore other ways of mapping the landscape of online communities. For example, combining structural properties of communities BIBREF40 with topical information BIBREF35 and with our identity-based measures could further characterize and explain variations in user engagement patterns. Furthermore, extending the present analyses to even more diverse communities supported by different platforms (e.g., GitHub, StackExchange, Wikipedia) could enable the characterization of more complex",
        "type": "Document"
      },
      {
        "id": "8448f360-547f-4701-9e82-26ef1b354393",
        "metadata": {
          "vector_store_key": "1710.01492-0",
          "chunk_id": 78,
          "document_id": "1710.01492",
          "start_idx": 28162,
          "end_idx": 28625
        },
        "page_content": " etc. We expect the quest for more interesting formulations of the general sentiment analysis task to continue. We see competitions such as those at SemEval as the engine of this innovation, as they not only perform head-to-head comparisons, but also create databases and tools that enable follow-up research for many years afterward. In terms of methods, we believe that deep learning BIBREF55 , BIBREF54 , BIBREF56 , together with semi-supervised and distantly-",
        "type": "Document"
      },
      {
        "id": "c270760a-3f97-41b4-84b6-68ddf24229ea",
        "metadata": {
          "vector_store_key": "1910.04269-4",
          "chunk_id": 54,
          "document_id": "1910.04269",
          "start_idx": 20617,
          "end_idx": 21192
        },
        "page_content": " robust to variations which might be present in real world scenarios. There can be further exploration of various feature extraction techniques like Constant-Q transform and Fast Fourier Transform and assessment of their impact on Language Identification. There can be further improvements in neural network architectures like concatenating the high level features obtained from 1D-ConvNet and 2D-ConvNet, before performing classification. There can be experiments using deeper networks with skip connections and Inception modules. These are known to have positively impacted",
        "type": "Document"
      },
      {
        "id": "b8389e35-a880-4a63-995d-127ccddb32f4",
        "metadata": {
          "vector_store_key": "1905.08949-3",
          "chunk_id": 5,
          "document_id": "1905.08949",
          "start_idx": 1945,
          "end_idx": 2409
        },
        "page_content": " have been considerable advances made in NQG, the area lacks a comprehensive survey. This paper fills this gap by presenting a systematic survey on recent development of NQG, focusing on three emergent trends that deep learning has brought in QG: (1) the change of learning paradigm, (2) the broadening of the input spectrum, and (3) the generation of deep questions. For the sake of clean exposition, we first provide a broad overview of QG by conceptualizing the",
        "type": "Document"
      },
      {
        "id": "2f779d71-2179-4ca0-89f3-60ca55105db9",
        "metadata": {
          "vector_store_key": "1911.03894-1",
          "chunk_id": 63,
          "document_id": "1911.03894",
          "start_idx": 22277,
          "end_idx": 22771
        },
        "page_content": " many other languages. We will publish an updated version in the near future where we will explore and release models trained for longer, with additional downstream tasks, baselines (e.g. XLM) and analysis, we will also train additional models with potentially cleaner corpora such as CCNet BIBREF56 for more accurate performance evaluation and more complete ablation. This work was partly funded by three French National grants from the Agence Nationale de la Recherche, namely projects PARSIT",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are their initial results on this task?",
    "llm_answer": "The final model \"Ours with Mask and Ordered Triplets\" outperforms the Baseline and Ablation models on all metrics in previously seen environments.  The difference is particularly evident for Exact Match and Goal Match metrics.\n",
    "context": [
      {
        "id": "413c231b-c771-4c94-97a8-b20b290a953c",
        "metadata": {
          "vector_store_key": "1906.06448-6",
          "chunk_id": 32,
          "document_id": "1906.06448",
          "start_idx": 11534,
          "end_idx": 12014
        },
        "page_content": " the premise and/or the hypothesis is ungrammatical or does not make sense. Workers were paid US$0.04 for each question, and each question was assigned to three workers. To collect high-quality annotation results, we imposed ten test questions on each worker, and removed workers who gave more than three wrong answers. We also set the minimum time it should take to complete each question to 200 seconds. 1,237 workers contributed to this task, and we annotated gold labels of 15",
        "type": "Document"
      },
      {
        "id": "dc50a5cf-b755-41a0-a173-1b561db0ed05",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "154800a9-8cc0-479d-bdb3-8f3aa6520648",
        "metadata": {
          "vector_store_key": "1906.06448-6",
          "chunk_id": 28,
          "document_id": "1906.06448",
          "start_idx": 9932,
          "end_idx": 10429
        },
        "page_content": " each set of substitutions, and each set was assigned to three workers. To remove low-quality examples, we set the minimum time it should take to complete each set to 200 seconds. The entry in our task was restricted to workers from native speaking English countries. 128 workers contributed to the task, and we created 15,339 hypotheses (7,179 upward examples and 8,160 downward examples). The gold label of each premise-hypothesis pair created in the previous task is automatically determined by",
        "type": "Document"
      },
      {
        "id": "31e3f6a2-9e9b-4d6d-824a-22195518284a",
        "metadata": {
          "vector_store_key": "1909.02764-7",
          "chunk_id": 52,
          "document_id": "1909.02764",
          "start_idx": 20420,
          "end_idx": 20841
        },
        "page_content": "=73 % on TEC. The model trained on our AMMER corpus achieves an F1 score of 57%. This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set. These results are mostly in line with Bostan2018. Now we analyze how well the models trained in Experiment 1 perform when applied to our data set. The results are shown in column \u201cSimple\u201d in Table TAB",
        "type": "Document"
      },
      {
        "id": "ec651bfc-61bb-46ab-a708-6779938b6cdb",
        "metadata": {
          "vector_store_key": "1701.02877-6",
          "chunk_id": 62,
          "document_id": "1701.02877",
          "start_idx": 22362,
          "end_idx": 22802
        },
        "page_content": " data gives little to no improvement, whereas adding positive examples drastically improves performance. Table UID9 shows results with size normalised precision (P), recall (R), and F1-Score (F1). The five lowest P, R and F1 values per method (CRFSuite, Stanford NER, SENNA) are in bold to highlight underperformers. Results for all corpora are summed with macro average. Comparing the different methods, the highest F1 results are achieved",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is morphology knowledge implemented in the method?",
    "llm_answer": "A morphological word segmentation method is used on the source (lemmatization and morphological feature analysis).  A morphologically motivated segmentation strategy combining morpheme segmentation and BPE is also proposed.  Other methods, like using Morfessor and rule-based segmentation, are also mentioned.\n",
    "context": [
      {
        "id": "63af40ef-c7bb-41a1-8a2a-bb74c83cc443",
        "metadata": {
          "vector_store_key": "2001.01589-4",
          "chunk_id": 4,
          "document_id": "2001.01589",
          "start_idx": 1465,
          "end_idx": 2002
        },
        "page_content": ". The complex morpheme structure and relatively free constituent order can produce very large vocabulary because of the derivational morphology, so when translating from the agglutinative languages, many words are unseen at training time. Moreover, due to the semantic context, the same word generally has different segmentation forms in the training corpus. For the purpose of incorporating morphology knowledge of agglutinative languages into word segmentation for NMT, we propose a morphological word segmentation method on the source",
        "type": "Document"
      },
      {
        "id": "ce494760-6a34-464a-9b7c-19905261f07b",
        "metadata": {
          "vector_store_key": "1910.11493-1",
          "chunk_id": 55,
          "document_id": "1910.11493",
          "start_idx": 21090,
          "end_idx": 21632
        },
        "page_content": " (lemmatization and morphological feature analysis), 16 systems were submitted. All used neural network models, completing a trend in past years' shared tasks and other recent work on morphology. In task 1, gains from cross-lingual training were generally modest, with gains positively correlating with the linguistic similarity of the two languages. In the second task, several methods were implemented by multiple groups, with the most successful systems implementing variations of multi-headed attention, multi-level encoding, multiple dec",
        "type": "Document"
      },
      {
        "id": "3c500358-96a6-48f1-9ce4-7333cfb12879",
        "metadata": {
          "vector_store_key": "2001.01589-4",
          "chunk_id": 16,
          "document_id": "2001.01589",
          "start_idx": 5973,
          "end_idx": 6532
        },
        "page_content": " information. Hence, on the analyses of the above popular word segmentation methods, we propose the morphologically motivated segmentation strategy that combines the morpheme segmentation and BPE for further improving the translation performance of NMT. Compared with the sentence of word surface forms, the corresponding sentence of stem units only contains the structure information without considering morphological information, which can make better generalization over inflectional variants of the same word and reduce data sparseness BIBREF8. Therefore,",
        "type": "Document"
      },
      {
        "id": "bfd5bb21-4628-459a-bcc0-108149c8469e",
        "metadata": {
          "vector_store_key": "2001.01589-4",
          "chunk_id": 35,
          "document_id": "2001.01589",
          "start_idx": 13004,
          "end_idx": 13508
        },
        "page_content": "bury and Socher BIBREF16 employed the modified Morfessor to provide morphology knowledge into word segmentation, but they neglected the morphological varieties between subword units, which might result in ambiguous translation results. Sanchez-Cartagena and Toral BIBREF17 proposed a rule-based morphological word segmentation for Finnish, which applies BPE on all the morpheme units uniformly without distinguishing their inner morphological roles. Huck BIBREF18 explored target-side segmentation method",
        "type": "Document"
      },
      {
        "id": "eac026c3-552b-4d5d-bf31-661ec06e73b3",
        "metadata": {
          "vector_store_key": "1810.06743-6",
          "chunk_id": 4,
          "document_id": "1810.06743",
          "start_idx": 1487,
          "end_idx": 2017
        },
        "page_content": " each other. Further, we show similar downstream performance for each annotation scheme in the task of morphological tagging. This tool enables a synergistic use of UniMorph and Universal Dependencies, as well as teasing out the annotation discrepancies within and across projects. When one dataset disobeys its schema or disagrees with a related language, the flaws may not be noticed except by such a methodological dive into the resources. When the maintainers of the resources ameliorate these flaws, the resources move closer",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the performance on the task evaluated?",
    "llm_answer": "For question answering tasks, F1 score is used.  For RL tasks, agent performance is evaluated during training and testing.  For cross-multi-domain tasks, task finish rate is calculated over 1000 simulations.\n",
    "context": [
      {
        "id": "cbb016ce-46fa-4916-ab46-cafe71395ef1",
        "metadata": {
          "vector_store_key": "2002.11893-7",
          "chunk_id": 90,
          "document_id": "2002.11893",
          "start_idx": 33632,
          "end_idx": 34069
        },
        "page_content": " are filled by real values, the simulator terminates. This is regarded as \"task finish\". It's worth noting that \"task finish\" does not mean the task is success, because the system may provide wrong information. We calculated \"task finish rate\" on 1000 times simulations for each goal type (See Table TABREF31). Findings are summarized below: Cross multi-domain tasks (CM and CM+T) are much harder to finish. Comparing M and M+T, although",
        "type": "Document"
      },
      {
        "id": "674affb1-9973-4581-bdda-e127f9f0533b",
        "metadata": {
          "vector_store_key": "1908.10449-2",
          "chunk_id": 31,
          "document_id": "1908.10449",
          "start_idx": 10837,
          "end_idx": 11300
        },
        "page_content": "RC involves both MRC and RL, we adopt evaluation metrics from both settings. First, as a question answering task, we use $\\text{F}_1$ score to compare predicted answers against ground-truth, as in previous works. When there exist multiple ground-truth answers, we report the max $\\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we evaluate an agent's performance during both its training and testing phases.",
        "type": "Document"
      },
      {
        "id": "0d47b895-32d3-4d72-9c9b-d302a4a8e3ad",
        "metadata": {
          "vector_store_key": "1910.02789-3",
          "chunk_id": 10,
          "document_id": "1910.02789",
          "start_idx": 4168,
          "end_idx": 4554
        },
        "page_content": "pi (s)$, which is a mapping from state $s$ to a probability distribution over actions $\\mathcal {A}$, with the objective to maximize a reward $r(s)$ that is provided by the environment. This is often solved by formulating the problem as a Markov Decision Process (MDP) BIBREF19. Two common quantities used to estimate the performance in MDPs are the value $v (s)$ and action-value $Q (s",
        "type": "Document"
      },
      {
        "id": "a9967996-abe0-401c-9bba-ff7d4e484220",
        "metadata": {
          "vector_store_key": "1909.00578-4",
          "chunk_id": 11,
          "document_id": "1909.00578",
          "start_idx": 4037,
          "end_idx": 4469
        },
        "page_content": "notators assigned scores on a five-point scale, with 1 and 5 indicating that the summary is bad or good with respect to a specific $\\mathcal {Q}$. The overall score for a contestant with respect to a specific $\\mathcal {Q}$ is the average of the manual scores assigned to the summaries generated by the contestant. Note that the DUC-04 shared task involved seven $\\mathcal {Q}$s, but some of them were found to be highly overlapping",
        "type": "Document"
      },
      {
        "id": "638d3f9b-929b-4fa6-b0d4-efe44cd66255",
        "metadata": {
          "vector_store_key": "1912.13109-2",
          "chunk_id": 25,
          "document_id": "1912.13109",
          "start_idx": 10203,
          "end_idx": 10681
        },
        "page_content": " metrics - Recall rate, F1 score and Overall accuracy. Choice of model parameters were in the above models were inspired from previous work done but then were tuned to the best performance of the Test dataset. Following parameters were considered for tuning. Learning rate: Based on grid search the best performance was achieved when learning rate was set to 0.01. This value was arrived by a grid search on lr parameter. Number of Bidirectional LSTM units: A set of 32, 64, 128",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what datasets were used?",
    "llm_answer": "Amazon Mechanical Turk dataset of simulated indoor environments, MCTest, CNN/Daily Mail, Children's Book Test, StackOverflow, and BioASQ.\n",
    "context": [
      {
        "id": "f8061e5e-74f5-49d0-8dbe-90e4db84d4eb",
        "metadata": {
          "vector_store_key": "1810.00663-7",
          "chunk_id": 46,
          "document_id": "1810.00663",
          "start_idx": 17810,
          "end_idx": 18361
        },
        "page_content": " . This dataset was created using Amazon Mechanical Turk and 100 maps of simulated indoor environments, each with 6 to 65 rooms. To the best of our knowledge, this is the first benchmark for comparing translation models in the context of behavioral robot navigation. As shown in Table TABREF16 , the dataset consists of 8066 pairs of free-form natural language instructions and navigation plans for training. This training data was collected from 88 unique simulated environments, totaling 6064 distinct navigation plans (2002 plans have two different",
        "type": "Document"
      },
      {
        "id": "4cac7588-28e9-4b6e-ac0d-d763db64bc58",
        "metadata": {
          "vector_store_key": "1703.04617-6",
          "chunk_id": 3,
          "document_id": "1703.04617",
          "start_idx": 1343,
          "end_idx": 1868
        },
        "page_content": " can help attain better results on our competitive baselines. Recent advance on reading comprehension and question answering has been closely associated with the availability of various datasets. BIBREF0 released the MCTest data consisting of 500 short, fictional open-domain stories and 2000 questions. The CNN/Daily Mail dataset BIBREF1 contains news articles for close style machine comprehension, in which only entities are removed and tested for comprehension. Children's Book Test (CBT) BIBREF2 leverages named entities",
        "type": "Document"
      },
      {
        "id": "1c380a0c-1037-4628-8ada-168d9c8247bc",
        "metadata": {
          "vector_store_key": "1707.03904-9",
          "chunk_id": 13,
          "document_id": "1707.03904",
          "start_idx": 5304,
          "end_idx": 5850
        },
        "page_content": " subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short. We plan to release these annotations along with the datasets, and our retrieved documents for each question. Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated",
        "type": "Document"
      },
      {
        "id": "401df3ea-241b-476d-97f4-1276dc131769",
        "metadata": {
          "vector_store_key": "1701.00185-9",
          "chunk_id": 99,
          "document_id": "1701.00185",
          "start_idx": 35418,
          "end_idx": 35678
        },
        "page_content": "ASQ for making the datasets available. This work is supported by the National Natural Science Foundation of China (No. 61602479, No. 61303172, No. 61403385) and the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDB02070005).",
        "type": "Document"
      },
      {
        "id": "1eb879ee-df56-4263-afd5-19bb5ae493ff",
        "metadata": {
          "vector_store_key": "1701.00185-9",
          "chunk_id": 51,
          "document_id": "1701.00185",
          "start_idx": 18784,
          "end_idx": 19212
        },
        "page_content": " BIBREF41 . StackOverflow. We use the challenge data published in Kaggle.com. The raw dataset consists 3,370,528 samples through July 31st, 2012 to August 14, 2012. In our experiments, we randomly select 20,000 question titles from 20 different tags as in Table TABREF25 . Biomedical. We use the challenge data published in BioASQ's official website. In our experiments, we randomly select 20, 000 paper titles from 20 different",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they obtain human judgements?",
    "llm_answer": "The provided text describes using human judgments for evaluating model performance in predicting time courses of moral relevance, moral polarity, and fine-grained moral categories.  However, it does not specify how those human judgments were obtained.\n",
    "context": [
      {
        "id": "6cf2612b-beb0-47d2-8ff9-a4a2c799cba1",
        "metadata": {
          "vector_store_key": "2001.07209-1",
          "chunk_id": 9,
          "document_id": "2001.07209",
          "start_idx": 3691,
          "end_idx": 4290
        },
        "page_content": " framework that we propose consists of a parameter-free approach that facilitates the prediction of public moral sentiment toward individual concepts, automated retrieval of morally changing concepts, and broad-scale psycholinguistic analyses of historical rates of moral sentiment change. We provide a description of the probabilistic models and data used, followed by comprehensive evaluations of our methodology. An emerging body of work in natural language processing and computational social science has investigated how NLP systems can detect moral sentiment in online text. For example, moral",
        "type": "Document"
      },
      {
        "id": "bb643fc7-89e8-4620-b5b0-404514fa7ea7",
        "metadata": {
          "vector_store_key": "2001.07209-1",
          "chunk_id": 34,
          "document_id": "2001.07209",
          "start_idx": 13176,
          "end_idx": 13731
        },
        "page_content": "s corpus, which enabled superior model performance throughout our evaluations. We applied our framework in three ways: 1) evaluation of selected concepts in historical time courses and prediction of human judgments; 2) automatic detection of moral sentiment change; and 3) broad-scale study of the relations between psycholinguistic variables and historical change of moral sentiment toward concepts. We applied our models diachronically to predict time courses of moral relevance, moral polarity, and fine-grained moral categories toward two historically",
        "type": "Document"
      },
      {
        "id": "8cfdeee9-9a89-46b9-a497-510c2f156b06",
        "metadata": {
          "vector_store_key": "1809.08731-2",
          "chunk_id": 69,
          "document_id": "1809.08731",
          "start_idx": 23501,
          "end_idx": 23955
        },
        "page_content": ":EMNLP20163 obtained close to no correlation with human judgements for BLEU, ROUGE and METEOR. They contributed this in a large part to the unrestrictedness of dialogue answers, which makes it hard to match given references. They emphasized that the community should move away from these metrics for dialogue generation tasks, and develop metrics that correlate more strongly with human judgments. elliott-keller:2014:P14-2 reported the same for BLEU and",
        "type": "Document"
      },
      {
        "id": "fa1ada7e-d06d-459a-bd7b-58a700ff0f06",
        "metadata": {
          "vector_store_key": "1809.04960-5",
          "chunk_id": 44,
          "document_id": "1809.04960",
          "start_idx": 17128,
          "end_idx": 17625
        },
        "page_content": "truth comments of the corresponding news provided by the human. Plausible: The 50 most similar comments to the news. We use the news as the query to retrieve the comments that appear in the training set based on the cosine similarity of their tf-idf values. We select the top 50 comments that are not the correct comments as the plausible comments. Popular: The 50 most popular comments from the dataset. We count the frequency of each comments in the training set, and select the 50 most frequent",
        "type": "Document"
      },
      {
        "id": "89e25d9c-0156-4dca-88df-7123c3cc7aad",
        "metadata": {
          "vector_store_key": "1809.08731-5",
          "chunk_id": 12,
          "document_id": "1809.08731",
          "start_idx": 4451,
          "end_idx": 5000
        },
        "page_content": " this research question, lau2017grammaticality aimed at modeling human acceptability judgments automatically, with the goal to gain insight into the nature of human perception of acceptability. In particular, they tried to answer the question: Do humans judge acceptability on a gradient scale? Their experiments showed a strong correlation between human judgments and normalized sentence log-probabilities under a variety of LMs for artificial data they had created by translating and back-translating sentences with neural models. While they tried",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Does this approach perform better in the multi-domain or single-domain setting?",
    "llm_answer": "The approach performs better in the multi-span question setting, with a significant improvement in both EM and F1 scores compared to other models.\n",
    "context": [
      {
        "id": "a200aee7-df1a-4bda-bac6-1975946bbe85",
        "metadata": {
          "vector_store_key": "2002.11893-9",
          "chunk_id": 15,
          "document_id": "2002.11893",
          "start_idx": 5632,
          "end_idx": 6105
        },
        "page_content": " turns for 16 domains in the training set. However, the multi-domain dependency in these two datasets is only embodied in imposing the same pre-specified constraints on different domains, such as requiring a restaurant and an attraction to locate in the same area, or the city of a hotel and the destination of a flight to be the same (Table TABREF6). Table TABREF5 presents a comparison between our dataset with other task-oriented datasets. In comparison to MultiWOZ, our",
        "type": "Document"
      },
      {
        "id": "6780ec62-3fd2-49af-910f-ae74c2f58bcc",
        "metadata": {
          "vector_store_key": "1909.13375-7",
          "chunk_id": 68,
          "document_id": "1909.13375",
          "start_idx": 23233,
          "end_idx": 23710
        },
        "page_content": ".4 EM in multi-span questions and 1.5 EM in single-span questions. The single-span improvement is probably due to the combination of better multi-span head learning as a result of fixing multi-span questions and the fact that the multi-span head can answer single-span questions as well. Not using the single-span heads results in a slight drop in multi-span performance, and a noticeable drop in single-span performance. However when performing the same comparison between our",
        "type": "Document"
      },
      {
        "id": "ad608421-d2c1-4105-8303-502157b0f9f9",
        "metadata": {
          "vector_store_key": "1909.13375-1",
          "chunk_id": 72,
          "document_id": "1909.13375",
          "start_idx": 24838,
          "end_idx": 25324
        },
        "page_content": " has a clear advantage in handling multi-span questions, with a 29.7 absolute improvement in EM, and a 15.1 absolute improvement in F1. Furthermore, we show that our model slightly eclipses the current state-of-the-art results on the entire DROP dataeset. Finally, we present some ablation studies, analyzing the benefit gained from individual components of our model. We believe that combining our tag-based approach for handling multi-span questions with current successful techniques",
        "type": "Document"
      },
      {
        "id": "11412592-faac-4798-a593-02d9617adf0d",
        "metadata": {
          "vector_store_key": "2002.11893-5",
          "chunk_id": 56,
          "document_id": "2002.11893",
          "start_idx": 20810,
          "end_idx": 21322
        },
        "page_content": " challenging than other tasks because additional cross-domain constraints in HAR domains are strict and will result in more \"NoOffer\" situations (i.e., the wizard finds no result that satisfies the current constraints). In this situation, the wizard will try to relax some constraints and issue multiple queries to find some results for a recommendation while the user will compromise and change the original goal. The negotiation process is captured by \"NoOffer rate\", \"Multi-query rate\", and \"Goal change rate\"",
        "type": "Document"
      },
      {
        "id": "fff77626-6222-4c51-9703-537fb806f641",
        "metadata": {
          "vector_store_key": "1701.02877-4",
          "chunk_id": 144,
          "document_id": "1701.02877",
          "start_idx": 52003,
          "end_idx": 52496
        },
        "page_content": ", and was it created with similar annotation guidelines. Yet it is very difficult to explain high/low out-of-domain performance compared to in-domain performance with those factors. A consistent trend is that, if out-of-domain memorisation is better in-domain memorisation, out-of-domain NERC performance with supervised learning is better than in-domain NERC performance with supervised learning too. This reinforces discussions in previous sections: an overlap in NEs is a good predictor for",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many samples did they generate for the artificial language?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "07344119-d726-4d67-a8fe-3e22de9ad73d",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 46,
          "document_id": "1910.04269",
          "start_idx": 17760,
          "end_idx": 18217
        },
        "page_content": " of 1,500 samples for each of six languages. Out of 1,500 samples for each language, 1,200 were randomly selected as training dataset for that language and rest 300 as validation dataset using k-fold cross-validation. To sum up, we trained our model on 7,200 samples and validated it on 1800 samples comprising six languages. The results are discussed in next section. This paper discusses two end-to-end approaches which achieve state-of-the-art results in",
        "type": "Document"
      },
      {
        "id": "3e1406ea-9836-4a54-8399-d1463333181f",
        "metadata": {
          "vector_store_key": "1909.00694-8",
          "chunk_id": 49,
          "document_id": "1909.00694",
          "start_idx": 15395,
          "end_idx": 15766
        },
        "page_content": " segmented into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam BIBREF19. The mini-batch size was 32. We ran 1 epoch.",
        "type": "Document"
      },
      {
        "id": "fd033831-b148-43b3-a2cf-5018e2dda42c",
        "metadata": {
          "vector_store_key": "1911.01799-1",
          "chunk_id": 26,
          "document_id": "1911.01799",
          "start_idx": 10361,
          "end_idx": 10700
        },
        "page_content": " two parts: VoxCeleb1 and VoxCeleb2. We used SITW BIBREF21, a subset of VoxCeleb1 as the evaluation set. The rest of VoxCeleb1 was merged with VoxCeleb2 to form the training set (simply denoted by VoxCeleb). The training set involves $1,236,567$ utterances from $7,185$ speakers, and the evaluation set involves $6,445$ utterances from 299",
        "type": "Document"
      },
      {
        "id": "17f3676f-8f5c-4da9-b0c7-cede94c8171d",
        "metadata": {
          "vector_store_key": "1904.05862-2",
          "chunk_id": 42,
          "document_id": "1904.05862",
          "start_idx": 15343,
          "end_idx": 15454
        },
        "page_content": "letter++ experiments, and Tatiana Likhomanenko for providing convolutional language models for our experiments.",
        "type": "Document"
      },
      {
        "id": "f8061e5e-74f5-49d0-8dbe-90e4db84d4eb",
        "metadata": {
          "vector_store_key": "1810.00663-7",
          "chunk_id": 46,
          "document_id": "1810.00663",
          "start_idx": 17810,
          "end_idx": 18361
        },
        "page_content": " . This dataset was created using Amazon Mechanical Turk and 100 maps of simulated indoor environments, each with 6 to 65 rooms. To the best of our knowledge, this is the first benchmark for comparing translation models in the context of behavioral robot navigation. As shown in Table TABREF16 , the dataset consists of 8066 pairs of free-form natural language instructions and navigation plans for training. This training data was collected from 88 unique simulated environments, totaling 6064 distinct navigation plans (2002 plans have two different",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Why does not the approach from English work on other languages?",
    "llm_answer": "The approach, while successful with English, French, and German, is less robust to new speakers in the test dataset.  Also,  difficulties arise with perceptually similar languages (like German and English), and issues exist with annotation discrepancies and differences in the choice of attributes to annotate across languages.  Different morphological structures (e.g., suffixing vs. prefixing) between source and target languages affect learning.\n",
    "context": [
      {
        "id": "1920df44-401c-4a6d-a53c-a400f8cc87f0",
        "metadata": {
          "vector_store_key": "1910.05456-2",
          "chunk_id": 27,
          "document_id": "1910.05456",
          "start_idx": 9884,
          "end_idx": 10340
        },
        "page_content": " additionally makes use of internal stem changes (e.g., o $\\rightarrow $ ue). Since English and Spanish are both Indo-European languages, and, thus, relatively similar, we further add a third, unrelated target language. We choose Zulu (ZUL), a Bantoid language. In contrast to the first two, it is strongly prefixing. For pretraining, we choose languages with different degrees of relatedness and varying morphological similarity to English, Spanish, and Z",
        "type": "Document"
      },
      {
        "id": "2ab7d9b7-a4c1-4aed-af29-7f8272995583",
        "metadata": {
          "vector_store_key": "1910.05456-5",
          "chunk_id": 6,
          "document_id": "1910.05456",
          "start_idx": 2256,
          "end_idx": 2734
        },
        "page_content": " on each of the source languages and then fine-tune the resulting models on small datasets in each of the target languages. Analyzing the errors made by the systems, we find that (i) source and target language being closely related simplifies the successful learning of inflection in the target language, (ii) the task is harder to learn in a prefixing language if the source language is suffixing \u2013 as well as the other way around, and (iii) a source language which exhibits an",
        "type": "Document"
      },
      {
        "id": "635e5e6c-9cc3-4b1a-9a80-ed3f4fc00b6f",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 17,
          "document_id": "1910.04269",
          "start_idx": 6552,
          "end_idx": 7071
        },
        "page_content": " for feature extraction. An elaborate analysis of how deep architectures outperform their shallow counterparts is presented in this reseacrch. The difficulties in classifying perceptually similar languages like German and English were also put forward in this work. It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize",
        "type": "Document"
      },
      {
        "id": "05212baa-0d7d-46a8-b43b-4f45fb3fbf2c",
        "metadata": {
          "vector_store_key": "1810.06743-2",
          "chunk_id": 49,
          "document_id": "1810.06743",
          "start_idx": 18569,
          "end_idx": 19061
        },
        "page_content": " from irremediable annotation discrepancies. Largely, we are hamstrung by differences in choice of attributes to annotate. When one resource marks gender and the other marks case, we can't infer the gender of the word purely from its surface form. The resources themselves would need updating to encode the relevant morphosyntactic information. Some languages had a very low number of overlapping forms, and no tag matches or near-matches between them: Arabic, Hindi, Lithuanian, Persian, and",
        "type": "Document"
      },
      {
        "id": "e7003dc7-f174-43b3-b750-34e756cc8d7c",
        "metadata": {
          "vector_store_key": "1911.00069-4",
          "chunk_id": 76,
          "document_id": "1911.00069",
          "start_idx": 24530,
          "end_idx": 24998
        },
        "page_content": ".9 $F_1$ points, except for Arabic. Since our approach projects the target-language word embeddings to the source-language embedding space preserving the word order, it is expected to work better for a target language that has more similar word order as the source language. This has been verified by our experiments. The source language, English, belongs to the SVO (Subject, Verb, Object) language family where in a sentence the subject comes first, the verb second,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they measure grammaticality?",
    "llm_answer": "Speakers' intuitions about sentence well-formedness, using \"acceptability\" or \"grammaticality\" judgments.  A log ratio of grammatical to ungrammatical phrases is also used.\n",
    "context": [
      {
        "id": "0aa4db32-cbac-4ffa-81bd-765ef03bfb78",
        "metadata": {
          "vector_store_key": "1809.08731-4",
          "chunk_id": 2,
          "document_id": "1809.08731",
          "start_idx": 812,
          "end_idx": 1267
        },
        "page_content": " evaluation, especially of fluency, causing word-overlap metrics like ROUGE BIBREF1 to correlate only weakly with human judgments BIBREF2 . As a result, fluency evaluation of NLG is often done manually, which is costly and time-consuming. Evaluating sentences on their fluency, on the other hand, is a linguistic ability of humans which has been the subject of a decade-long debate in cognitive science. In particular, the question has been raised whether",
        "type": "Document"
      },
      {
        "id": "819919e6-ce03-4836-aea8-ab4799d9c876",
        "metadata": {
          "vector_store_key": "1809.08731-4",
          "chunk_id": 61,
          "document_id": "1809.08731",
          "start_idx": 20536,
          "end_idx": 21023
        },
        "page_content": " fluency evaluation in NLP has been rare. heilman2014predicting predicted the fluency (which they called grammaticality) of sentences written by English language learners. In contrast to ours, their approach is supervised. stent2005evaluating and cahill2009correlating found only low correlation between automatic metrics and fluency ratings for system-generated English paraphrases and the output of a German surface realiser, respectively. Explicit fluency evaluation of NLG, including",
        "type": "Document"
      },
      {
        "id": "ec9e54ea-eeff-4eca-ade3-890e02aabf82",
        "metadata": {
          "vector_store_key": "1809.08731-5",
          "chunk_id": 9,
          "document_id": "1809.08731",
          "start_idx": 3495,
          "end_idx": 3943
        },
        "page_content": "' judgments of the well-formedness of sentences, have been the basis of much linguistics research BIBREF10 , BIBREF11 : a speakers intuition about a sentence is used to draw conclusions about a language's rules. Commonly, \u201cacceptability\u201d is used synonymously with \u201cgrammaticality\u201d, and speakers are in practice asked for grammaticality judgments or acceptability judgments interchangeably. Strictly speaking, however, a sentence can be unacceptable",
        "type": "Document"
      },
      {
        "id": "155700d7-8f16-4a86-94fe-c6fd91b1b068",
        "metadata": {
          "vector_store_key": "1906.04571-0",
          "chunk_id": 43,
          "document_id": "1906.04571",
          "start_idx": 15701,
          "end_idx": 16142
        },
        "page_content": " that are grammatical, we additionally consider the following log ratio (i.e., the grammatical phrase over the ungrammatical phrase): DISPLAYFORM0  We trained the linear parameterization using UD treebanks for Spanish, Hebrew, French, and Italian (see tab:data). For each of the four languages, we parsed one million sentences from Wikipedia (May 2018 dump) using BIBREF24 's parser and extracted taggings and lemmata using the method of BIB",
        "type": "Document"
      },
      {
        "id": "2fb5a0a9-4e20-4413-913c-ca4bca980d55",
        "metadata": {
          "vector_store_key": "2003.07433-5",
          "chunk_id": 51,
          "document_id": "2003.07433",
          "start_idx": 20049,
          "end_idx": 20591
        },
        "page_content": " each of the category words within texts. After the PTSD Linguistic Dictionary has been created, we empirically evaluate its psychometric properties such as reliability and validity as per American Standards for educational and psychological testing guideline BIBREF24. In psychometrics, reliability is most commonly evaluated by Cronbach's alpha, which assesses internal consistency based on inter-correlations and the number of measured items. In the text analysis scenario, each word in our PTSD Linguistic dictionary is considered an item",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the difference in recall score between the systems?",
    "llm_answer": "The context states that recall is consistently significantly lower than precision for the proposed method, but does not give specific recall scores for different systems.  Therefore, the difference is not found in the context.\n",
    "context": [
      {
        "id": "c0a36e83-6a5f-4829-b712-316da2b2e885",
        "metadata": {
          "vector_store_key": "1910.03814-8",
          "chunk_id": 51,
          "document_id": "1910.03814",
          "start_idx": 19558,
          "end_idx": 20033
        },
        "page_content": " results for the LSTM, for the Davison method proposed in BIBREF7 trained with MMHS150K, and for random scores. Fig. FIGREF32 shows the Precision vs Recall plot and the ROC curve (which plots the True Positive Rate vs the False Positive Rate) of the different models. First, notice that given the subjectivity of the task and the discrepancies between annotators, getting optimal scores in the evaluation metrics is virtually impossible. However, a system with relatively low",
        "type": "Document"
      },
      {
        "id": "f0f8938b-eb92-48ef-a865-1fa0909f1b35",
        "metadata": {
          "vector_store_key": "1906.04571-6",
          "chunk_id": 35,
          "document_id": "1906.04571",
          "start_idx": 12581,
          "end_idx": 13071
        },
        "page_content": " intervened. We present the results in tab:intrinsic. Recall is consistently significantly lower than precision. As expected, the baselines have the highest precision (though not by much). This is because they reflect well-known rules for each language. That said, they have lower recall than our approach because they fail to capture more subtle relationships. For both languages, our approach struggles with conjunctions. For example, consider the phrase \u00e9l es un ingeniero y escritor (he",
        "type": "Document"
      },
      {
        "id": "4753738d-6218-44f4-a43e-ba45e2f9a239",
        "metadata": {
          "vector_store_key": "1809.04960-7",
          "chunk_id": 55,
          "document_id": "1809.04960",
          "start_idx": 21435,
          "end_idx": 21923
        },
        "page_content": ". With the help of our proposed model, both IR and S2S achieve an improvement under the semi-supervised scenarios. We analyze the performance of the proposed method under the semi-supervised setting. We train the supervised IR model with different numbers of paired data. Figure FIGREF39 shows the curve (blue) of the recall1 score. As expected, the performance grows as the paired dataset becomes larger. We further combine the supervised IR with our unsupervised model, which is trained",
        "type": "Document"
      },
      {
        "id": "ec651bfc-61bb-46ab-a708-6779938b6cdb",
        "metadata": {
          "vector_store_key": "1701.02877-6",
          "chunk_id": 62,
          "document_id": "1701.02877",
          "start_idx": 22362,
          "end_idx": 22802
        },
        "page_content": " data gives little to no improvement, whereas adding positive examples drastically improves performance. Table UID9 shows results with size normalised precision (P), recall (R), and F1-Score (F1). The five lowest P, R and F1 values per method (CRFSuite, Stanford NER, SENNA) are in bold to highlight underperformers. Results for all corpora are summed with macro average. Comparing the different methods, the highest F1 results are achieved",
        "type": "Document"
      },
      {
        "id": "41b247c9-4efd-4c72-a969-d19320c333f1",
        "metadata": {
          "vector_store_key": "1909.00542-3",
          "chunk_id": 37,
          "document_id": "1909.00542",
          "start_idx": 14353,
          "end_idx": 14835
        },
        "page_content": ". Recall shows poor correlation, and virtually no correlation when using the revised Kendall measure. For reporting the evaluation of results, it will be therefore more useful to use precision or F1. However, given the small difference between precision and F1, and given that precision may favour short summaries when used as a function to optimise in a machine learning setting (e.g. using reinforcement learning), it may be best to use F1 as the metric to optimise. Fig. FIGREF40",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is their f1 score and recall?",
    "llm_answer": "F1 score: 78.38%\nRecall: Not found in the context.\n",
    "context": [
      {
        "id": "638d3f9b-929b-4fa6-b0d4-efe44cd66255",
        "metadata": {
          "vector_store_key": "1912.13109-2",
          "chunk_id": 25,
          "document_id": "1912.13109",
          "start_idx": 10203,
          "end_idx": 10681
        },
        "page_content": " metrics - Recall rate, F1 score and Overall accuracy. Choice of model parameters were in the above models were inspired from previous work done but then were tuned to the best performance of the Test dataset. Following parameters were considered for tuning. Learning rate: Based on grid search the best performance was achieved when learning rate was set to 0.01. This value was arrived by a grid search on lr parameter. Number of Bidirectional LSTM units: A set of 32, 64, 128",
        "type": "Document"
      },
      {
        "id": "31bcf066-a57a-491a-adf3-189cc3d3ebb7",
        "metadata": {
          "vector_store_key": "1703.04617-7",
          "chunk_id": 88,
          "document_id": "1703.04617",
          "start_idx": 24013,
          "end_idx": 24428
        },
        "page_content": " question is the major class. Figure 5 shows the composition of F1 score. Take our best model as an example, we observed a 78.38% F1 score on the whole development set, which can be separated into two parts: one is where F1 score equals to 100%, which means an exact match. This part accounts for 69.10% of the entire development set. And the other part accounts for 30.90%, of which the average F1 score is 30.03%.",
        "type": "Document"
      },
      {
        "id": "ec651bfc-61bb-46ab-a708-6779938b6cdb",
        "metadata": {
          "vector_store_key": "1701.02877-6",
          "chunk_id": 62,
          "document_id": "1701.02877",
          "start_idx": 22362,
          "end_idx": 22802
        },
        "page_content": " data gives little to no improvement, whereas adding positive examples drastically improves performance. Table UID9 shows results with size normalised precision (P), recall (R), and F1-Score (F1). The five lowest P, R and F1 values per method (CRFSuite, Stanford NER, SENNA) are in bold to highlight underperformers. Results for all corpora are summed with macro average. Comparing the different methods, the highest F1 results are achieved",
        "type": "Document"
      },
      {
        "id": "be0a5336-6975-40ba-9b09-178b713b3c09",
        "metadata": {
          "vector_store_key": "1701.02877-2",
          "chunk_id": 100,
          "document_id": "1701.02877",
          "start_idx": 35714,
          "end_idx": 36084
        },
        "page_content": " higher at 76.42; the less diverse data is, on average, easier to tag. Interestingly, average F1 on seen NEs in the Twitter corpora (MSM and Ritter) is around 80, whereas average F1 on the ACE corpora, which are of similar size, is lower, at around 70. To summarise, our findings are: [noitemsep] F1 on unseen NEs is significantly lower than F1 on seen NEs for all three",
        "type": "Document"
      },
      {
        "id": "d352816e-f99d-4e4e-b632-b5b1026e33d8",
        "metadata": {
          "vector_store_key": "1703.04617-7",
          "chunk_id": 89,
          "document_id": "1703.04617",
          "start_idx": 24428,
          "end_idx": 24839
        },
        "page_content": " other part accounts for 30.90%, of which the average F1 score is 30.03%. For the latter, we can further divide it into two sub-parts: one is where the F1 score equals to 0%, which means that predict answer is totally wrong. This part occupies 14.89% of the total development set. The other part accounts for 16.01% of the development set, of which average F1 score is 57.96%. From this analysis we can see that",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many layers does their system have?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "10ee7c8c-8492-4d26-9ee2-5244b2aa61f4",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 20,
          "document_id": "1909.00542",
          "start_idx": 7743,
          "end_idx": 8155
        },
        "page_content": " layer, and use cross-entropy as the loss function. The complete architecture is shown in Fig. FIGREF28. The bottom section of Table TABREF26 shows the results of several variants of the neural architecture. The table includes a neural regressor (NNR) and a neural classifier (NNC). The neural classifier is trained in two set ups: \u201cNNC top 5\u201d uses classification labels as described in Section SECREF3, and \u201cNNC",
        "type": "Document"
      },
      {
        "id": "4f7009fe-338f-46f2-b7a6-b9a6f5dc6949",
        "metadata": {
          "vector_store_key": "1910.02789-9",
          "chunk_id": 71,
          "document_id": "1910.02789",
          "start_idx": 28472,
          "end_idx": 28930
        },
        "page_content": " has 8 6X6 filters with stride 3 and ReLU activation. The second convolutional layer has 16 3X3 filters with stride 2 and ReLU activation. The fully connected layers has 32 and 16 units, both of them are followed by ReLU activation. The output layer's size is the amount of actions the agent has available in the trained scenario. Used in the feature vector based agent. Naturally, some discretization is needed in order to build a feature vector, so some of",
        "type": "Document"
      },
      {
        "id": "915d80b7-1854-4cc1-87bf-e52b8aed5645",
        "metadata": {
          "vector_store_key": "2001.00137-2",
          "chunk_id": 22,
          "document_id": "2001.00137",
          "start_idx": 8966,
          "end_idx": 9386
        },
        "page_content": "ERT embedding size for a single token, and 128 is the maximum sequence length in a sentence. The stacks of multilayer perceptrons are structured as two sets of three layers with two hidden layers each. The first set is responsible for compressing the $h_{inc}$ into a latent-space representation, extracting more abstract features into lower dimension vectors $z_1$, $z_2$ and $\\mathbf {z}$ with shape $(N_{bs}, 128, 128",
        "type": "Document"
      },
      {
        "id": "cd5a4227-02bf-481a-ba8d-794746daf743",
        "metadata": {
          "vector_store_key": "1806.00722-3",
          "chunk_id": 39,
          "document_id": "1806.00722",
          "start_idx": 14702,
          "end_idx": 15086
        },
        "page_content": " upper layer structure of the model. We also introduce three additional models we use for ablation study, all using 4-layer structure. Based on the residual connected BASE-4L model, (1) DenseENC-4L only makes encoder side dense, (2) DenseDEC-4L only makes decoder side dense, and (3) DenseAtt-4L only makes the attention dense using DenseAtt-2. There is no summary layer in the models",
        "type": "Document"
      },
      {
        "id": "fb9afdd8-bd9b-430b-8402-e1f4401051bc",
        "metadata": {
          "vector_store_key": "1908.10449-1",
          "chunk_id": 39,
          "document_id": "1908.10449",
          "start_idx": 13757,
          "end_idx": 14232
        },
        "page_content": " the encoding transformer blocks. Each encoding transformer block consists of four convolutional layers (with shared weights), a self-attention layer, and an MLP. Each convolutional layer has 96 filters, each kernel's size is 7. In the self-attention layer, we use a block hidden size of 96 and a single head attention mechanism. Layer normalization and dropout are applied after each component inside the block. We add positional encoding into each block's input. We use one",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What context modelling methods are evaluated?",
    "llm_answer": "Different context modeling methods are evaluated, as shown in Table TABREF39, with a focus on three representative models.  A simple concatenation method is also included for comparison.\n",
    "context": [
      {
        "id": "1705dfbb-7a02-4197-ac72-d022905b0bbc",
        "metadata": {
          "vector_store_key": "2002.00652-4",
          "chunk_id": 63,
          "document_id": "2002.00652",
          "start_idx": 20343,
          "end_idx": 20834
        },
        "page_content": " locate the substitution target, while implicit does not. On most cases, the target is schema or operator. In order to study the effect of context modeling methods on various phenomena, as shown in Table TABREF39, we take the development set of SParC as an example to perform our analysis. The analysis begins by presenting Ques.Match of three representative models on above fine-grained types in Figure FIGREF42. As shown, though different methods have different strengths, they all perform",
        "type": "Document"
      },
      {
        "id": "89c4b13e-2aa6-4de2-9819-446385c9ad66",
        "metadata": {
          "vector_store_key": "2002.00652-4",
          "chunk_id": 58,
          "document_id": "2002.00652",
          "start_idx": 18202,
          "end_idx": 18702
        },
        "page_content": " modeling methods, they do not significantly improve the performance as we expected. As mentioned in Section SECREF1, intuitively, methods which only use the precedent SQL enjoys better generalizability. To validate it, we further conduct an out-of-distribution experiment to assess the generalizability of different context modeling methods. Concretely, we select three representative methods and train them on questions at turn 1 and 2, whereas test them at turn 3, 4 and beyond. As shown in Figure",
        "type": "Document"
      },
      {
        "id": "51095457-b7c6-4ce1-af66-529cbf450f0d",
        "metadata": {
          "vector_store_key": "2002.00652-4",
          "chunk_id": 60,
          "document_id": "2002.00652",
          "start_idx": 19012,
          "end_idx": 19544
        },
        "page_content": " context modeling methods in the task of SPC are not as effective as expected, since they do not show a significant advantage over the simple concatenation method. By a careful investigation on contextual phenomena, we summarize them in multiple hierarchies. Roughly, there are three kinds of contextual phenomena in questions: semantically complete, coreference and ellipsis. Semantically complete means a question can reflect all the meaning of its corresponding SQL. Coreference means a question contains pronouns, while ellipsis",
        "type": "Document"
      },
      {
        "id": "e1501e6b-b5d9-4ffe-8d77-bdcf1e218cad",
        "metadata": {
          "vector_store_key": "2002.00652-2",
          "chunk_id": 73,
          "document_id": "2002.00652",
          "start_idx": 24143,
          "end_idx": 24692
        },
        "page_content": " methods. This work conducts an exploratory study on semantic parsing in context, to realize how far we are from effective context modeling. Through a thorough comparison, we find that existing context modeling methods are not as effective as expected. A simple concatenation method can be much competitive. Furthermore, by performing a fine-grained analysis, we summarize two potential directions as our future work: incorporating common sense for better pronouns inference, and modeling contextual clues in a more explicit manner. By open-sourcing",
        "type": "Document"
      },
      {
        "id": "b91e509e-130b-4ff4-a594-2599d7df2841",
        "metadata": {
          "vector_store_key": "2002.00652-2",
          "chunk_id": 9,
          "document_id": "2002.00652",
          "start_idx": 3418,
          "end_idx": 3964
        },
        "page_content": " of different context modeling methods. Our best model achieves state-of-the-art (SOTA) performances on both datasets with significant improvements. Furthermore, we summarize and generalize the most frequent contextual phenomena, with a fine-grained analysis on representative models. Through the analysis, we obtain some interesting findings, which may benefit the community on the potential research directions. We will open-source our code and materials to facilitate future work upon acceptance. In the task of semantic parsing in context, we",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is the baseline a non-heirarchical model like BERT?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "5f118b77-c1bd-4e17-ade3-4d7aa28a31cf",
        "metadata": {
          "vector_store_key": "2002.06644-2",
          "chunk_id": 6,
          "document_id": "2002.06644",
          "start_idx": 2489,
          "end_idx": 2911
        },
        "page_content": " various BERT-based models, including BERT, RoBERTa, ALBERT, with their base and large specifications along with their native classifiers. We propose an ensemble model exploiting predictions from these models using multiple ensembling techniques. We show that our model outperforms the baselines by a margin of $5.6$ of F1 score and $5.95\\%$ of Accuracy. In this section, we outline baseline models like $BERT_{large}$. We",
        "type": "Document"
      },
      {
        "id": "fb01b160-a168-4070-9f5a-fd61cc41f56f",
        "metadata": {
          "vector_store_key": "2002.06644-2",
          "chunk_id": 17,
          "document_id": "2002.06644",
          "start_idx": 6467,
          "end_idx": 6878
        },
        "page_content": "ERT based models, and BERT-based ensemble models outperform the baselines for all the metrics. Among the optimized BERT based models, $RoBERTa_{large}$ outperforms all other non-ensemble models and the baselines for all metrics. It further achieves a maximum recall of $0.681$ for all the proposed models. We note that DistillRoBERTa, a distilled model, performs competitively, achieving $69.69\\%$ accuracy, and",
        "type": "Document"
      },
      {
        "id": "1170937a-4f29-41ae-a6a6-dfb1ad2b5dbe",
        "metadata": {
          "vector_store_key": "1906.06448-4",
          "chunk_id": 62,
          "document_id": "1906.06448",
          "start_idx": 23037,
          "end_idx": 23574
        },
        "page_content": " was better on upward inferences than that on downward inferences. This indicates that BERT might fail to capture the monotonicity property that conditionals create a downward entailing context in their scope while they create an upward entailing context out of their scope. Regarding lexical knowledge, the data augmentation technique improved the performance much better on downward inferences which do not require lexical knowledge. However, among the 394 problems for which all models provided wrong answers, 244 problems are non-lex",
        "type": "Document"
      },
      {
        "id": "e4b78e80-4fff-4772-93bb-c8a348109001",
        "metadata": {
          "vector_store_key": "1906.06448-0",
          "chunk_id": 50,
          "document_id": "1906.06448",
          "start_idx": 18127,
          "end_idx": 18651
        },
        "page_content": " having rich lexical knowledge from a knowledge base (KIM) or pretraining (BERT). This indicates that downward inferences are difficult to perform even with the expansion of lexical knowledge. In addition, it is interesting to see that if a model performed better on upward inferences, it performed worse on downward inferences. We will investigate these results in detail below. To explore whether the performance of models on monotonicity reasoning depends on the training set or the model themselves, we conducted further",
        "type": "Document"
      },
      {
        "id": "05f513f6-29b2-46d8-8ba4-af84d6b0b34d",
        "metadata": {
          "vector_store_key": "1703.04617-0",
          "chunk_id": 10,
          "document_id": "1703.04617",
          "start_idx": 3849,
          "end_idx": 4374
        },
        "page_content": " questions share a basic model but still discriminate them when needed. Specifically, we are motivated by the ideas \"i-vector\" BIBREF18 in speech recognition, where neural network based adaptation is performed among different (groups) of speakers and we focused instead on different types of questions here. Our baseline model is composed of the following typical components: word embedding, input encoder, alignment, aggregation, and prediction. Below we discuss these components in more details. We concatenate embedding at",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How better are results compared to baseline models?",
    "llm_answer": "Our model outperforms the baseline and ablation models on all metrics in previously seen environments, with increases in accuracy of 35% and 25% for Exact Match and Goal Match, respectively.  For task A, our system performs better than a strong feature-rich based system when enough training data is available.  Our base model surpasses the NABERT+ baseline in every metric for task B.\n",
    "context": [
      {
        "id": "a1974dac-f964-4aeb-a415-a2006317d90a",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 68,
          "document_id": "1810.00663",
          "start_idx": 26280,
          "end_idx": 26763
        },
        "page_content": " room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM. Note that the average number of actions in the ground truth output sequences is 7.07 for the Test-New set. Our model's predictions are just INLINEFORM1 edits off on average from the correct navigation plans. This",
        "type": "Document"
      },
      {
        "id": "dc50a5cf-b755-41a0-a173-1b561db0ed05",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "11d9b2e6-e20d-4ab9-8d7c-c35a7e2d27e6",
        "metadata": {
          "vector_store_key": "1603.07044-6",
          "chunk_id": 43,
          "document_id": "1603.07044",
          "start_idx": 14997,
          "end_idx": 15515
        },
        "page_content": " task A already had a substantial amount of training data. Table 4 gives the final comparison between different models (we only list the MAP score because it is the official score for the challenge). Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited",
        "type": "Document"
      },
      {
        "id": "555a266e-0a08-43e5-b719-8cd2fd10ebb8",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "08676cf7-5cf4-4a2c-b9d7-ddbe513c2062",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 66,
          "document_id": "1810.00663",
          "start_idx": 25415,
          "end_idx": 25983
        },
        "page_content": " In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models. The previous section evaluated model performance on new instructions (and corresponding navigation plans) for environments that were previously seen at training time. Here, we examine whether the trained models succeed on environments that are completely new. The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is dataset used for training/testing?",
    "llm_answer": "The answer is not found in the context.  While various datasets and their splits are mentioned, there's no single, unified answer to the size of the training/testing datasets across all examples.\n",
    "context": [
      {
        "id": "3fd0d403-b663-4f73-b8f1-cfbe94ff3a61",
        "metadata": {
          "vector_store_key": "1910.12618-4",
          "chunk_id": 48,
          "document_id": "1910.12618",
          "start_idx": 19089,
          "end_idx": 19565
        },
        "page_content": " maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014",
        "type": "Document"
      },
      {
        "id": "9270562a-8e53-4e1d-b760-74d8fd0bc872",
        "metadata": {
          "vector_store_key": "1905.06566-7",
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "148ea921-3ae7-4687-bc70-85a9690fe58b",
        "metadata": {
          "vector_store_key": "1912.13109-4",
          "chunk_id": 20,
          "document_id": "1912.13109",
          "start_idx": 7921,
          "end_idx": 8471
        },
        "page_content": " on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we",
        "type": "Document"
      },
      {
        "id": "a872be77-b483-4ddf-9cee-412fb23e61cf",
        "metadata": {
          "vector_store_key": "1912.13109-4",
          "chunk_id": 19,
          "document_id": "1912.13109",
          "start_idx": 7657,
          "end_idx": 8136
        },
        "page_content": " above few data augmentation techniques were applied to boost the learning of the network. Before applying augmentation, a train-test split of 78%-22% was done from the original, cleansed data set. Thus, 700 tweets/messages were held out for testing. All model evaluation were done in on the test set that got generated by this process. The results presented in this report are based on the performance of the model on the test set. The training set of 2489 messages were however",
        "type": "Document"
      },
      {
        "id": "f4274fc8-8af1-4f7e-a32d-82ff390e6bcc",
        "metadata": {
          "vector_store_key": "1907.00758-4",
          "chunk_id": 35,
          "document_id": "1907.00758",
          "start_idx": 13403,
          "end_idx": 13866
        },
        "page_content": " or a total 1,149,368 samples (63.9hrs) which we divide into training, validation and test sets. We aim to test whether our model generalises to data from new speakers, and to data from new sessions recorded with known speakers. To simulate this, we select a group of speakers from each dataset, and hold out all of their data either for validation or for testing. Additionally, we hold out one entire session from each of the remaining speakers, and use the rest",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What geometric properties do embeddings display?",
    "llm_answer": "Distances between word vectors, specifically cosine distance.\n",
    "context": [
      {
        "id": "c734eecf-3695-412d-85f5-74431d45380a",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 81,
          "document_id": "1910.12618",
          "start_idx": 32047,
          "end_idx": 32551
        },
        "page_content": "ial properties translating linguistic ones. However considering the objective function of our problem, there was no obvious reason for such attributes to appear in our own. Nevertheless for both languages we conducted an analysis of the geometric properties of our embedding matrix. We investigated the distances between word vectors, the relevant metric being the cosine distance given by: where $\\overrightarrow{w_1}$ and $\\overrightarrow{w_2}$ are given word vectors. Thus a cosine distance lower than",
        "type": "Document"
      },
      {
        "id": "e5ed5882-f63a-46fa-ad24-5731c10cbe89",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 35,
          "document_id": "1910.12618",
          "start_idx": 14094,
          "end_idx": 14564
        },
        "page_content": ". In any case, the very nature of the objective function allows the embedding models to learn to translate linguistic similarities into geometric properties in the vector space. For instance the vector $\\overrightarrow{king} - \\overrightarrow{man} + \\overrightarrow{woman}$ is expected to be very close to the vector $\\overrightarrow{queen}$. However in our case we want a vector encoding which is tailored for the technical vocabulary of our weather reports and for the",
        "type": "Document"
      },
      {
        "id": "7bf18101-52ba-4e9a-a1f8-9907d6859e4f",
        "metadata": {
          "vector_store_key": "1910.14497-4",
          "chunk_id": 0,
          "document_id": "1910.14497",
          "start_idx": 0,
          "end_idx": 500
        },
        "page_content": "Word embeddings, or vector representations of words, are an important component of Natural Language Processing (NLP) models and necessary for many downstream tasks. However, word embeddings, including embeddings commonly deployed for public use, have been shown to exhibit unwanted societal stereotypes and biases, raising concerns about disparate impact on axes of gender, race, ethnicity, and religion BIBREF0, BIBREF1. The impact of this bias has manifested in a range of downstream tasks, ranging",
        "type": "Document"
      },
      {
        "id": "41ee45a7-4278-43ab-8f75-0c8e8398d23d",
        "metadata": {
          "vector_store_key": "1912.01673-0",
          "chunk_id": 3,
          "document_id": "1912.01673",
          "start_idx": 1159,
          "end_idx": 1704
        },
        "page_content": ", with novel representations appearing almost every other week. With an overwhelming number of methods to compute sentence vector representations, the study of their general properties becomes difficult. Furthermore, it is not so clear in which way the embeddings should be evaluated. In an attempt to bring together more traditional representations of sentence meanings and the emerging vector representations, bojar:etal:jnle:representations:2019 introduce a number of aspects or desirable properties of sentence embeddings. One of them is den",
        "type": "Document"
      },
      {
        "id": "a4abf318-3857-49a0-b3e8-d7ed84f86ddf",
        "metadata": {
          "vector_store_key": "1912.01673-0",
          "chunk_id": 6,
          "document_id": "1912.01673",
          "start_idx": 2302,
          "end_idx": 2877
        },
        "page_content": " envision that the continuous space of sentences induced by an ideal embedding method would exhibit topological similarity to the graph of sentence variations. For instance, one could argue that a subset of sentences could be organized along a linear scale reflecting the formalness of the language used. Another set of sentences could form a partially ordered set of gradually less and less concrete statements. And yet another set, intersecting both of the previous ones in multiple sentences could be partially or linearly ordered according to the strength of the speakers",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How accurate is model trained on text exclusively?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "bf6e594b-aef4-47b7-8211-04b14e0b13df",
        "metadata": {
          "vector_store_key": "1907.00758-5",
          "chunk_id": 46,
          "document_id": "1907.00758",
          "start_idx": 17344,
          "end_idx": 17855
        },
        "page_content": " across different conditions. Table shows the model accuracy broken down by utterance type. The model achieves 91.2% accuracy on utterances containing words, sentences, and conversations, all of which exhibit natural variation in speech. The model is less successful with Articulatory utterances, which contain isolated phones occurring once or repeated (e.g., \u201csh sh sh\"). Such utterances contain subtle tongue movement, making it more challenging to correlate the visual signal with the audio. And indeed, the",
        "type": "Document"
      },
      {
        "id": "880307b7-bfea-456d-bc6c-2e68f6ad0730",
        "metadata": {
          "vector_store_key": "2001.00137-3",
          "chunk_id": 51,
          "document_id": "2001.00137",
          "start_idx": 19320,
          "end_idx": 19801
        },
        "page_content": "$, an improvement of 6$\\%$. In the last case (Inc+Corr), we consider both incorrect and correct tweets as input to the models in hopes of improving performance. However, the accuracy was similar to the first aforementioned version, 80$\\%$ for our model and 74$\\%$ for the second highest performing model. Since the first and last corpus gave similar performances with our model, we conclude that the Twitter dataset does not require complete sentences to be given as training input",
        "type": "Document"
      },
      {
        "id": "509e375b-0631-47a8-a2da-5c8072f97f28",
        "metadata": {
          "vector_store_key": "1610.00879-3",
          "chunk_id": 27,
          "document_id": "1610.00879",
          "start_idx": 9948,
          "end_idx": 10164
        },
        "page_content": ".8%, our system reaches reasonably close and performs with a best accuracy of 64%. Our analysis of the task and experimental findings make a case for drunk-texting prediction as a useful and feasible NLP application.",
        "type": "Document"
      },
      {
        "id": "b6350b3a-decd-4ccf-8029-7d4a137b1ad3",
        "metadata": {
          "vector_store_key": "2001.00137-3",
          "chunk_id": 63,
          "document_id": "2001.00137",
          "start_idx": 23847,
          "end_idx": 24394
        },
        "page_content": " their accuracy while the lower achieving class saw improvement. Further evaluation on the F1-scores decay in the presence of noise demonstrated that our model is more robust than the baseline models when considering noisy data, be that in the form of incorrect sentences or sentences with STT error. Not only that, experiments on the Twitter dataset also showed improved accuracy in clean data, with complete sentences. We infer that this is due to our model being able to extract richer data representations from the input data regardless of the",
        "type": "Document"
      },
      {
        "id": "d24f28df-7b64-4a7a-9c45-f6e13849d9ab",
        "metadata": {
          "vector_store_key": "1909.00430-4",
          "chunk_id": 53,
          "document_id": "1909.00430",
          "start_idx": 19021,
          "end_idx": 19456
        },
        "page_content": " from this signal, it learns mostly to predict the majority class, and hence reaches very low F1 scores of 35.28. The more data given to the sentence level classifier, the better the potential results will be when training with our method using the classifier labels, with a classifiers trained on 100,1000,5000 and 10000 labeled sentences, we get a F1 scores of 53.81, 58.84, 61.81, 65.58 respectively. Improvements in the source task",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was their result on Stance Sentiment Emotion Corpus?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "f035d65e-5b1f-40af-a1d5-915e2771fb6d",
        "metadata": {
          "vector_store_key": "1910.11769-4",
          "chunk_id": 11,
          "document_id": "1910.11769",
          "start_idx": 4371,
          "end_idx": 4765
        },
        "page_content": " sentiment template and instructed the crowd annotators to `pick the best/major emotion embodied in the passage'. We further provided instructions to clarify the intensity of an emotion, such as: \u201cRage/Annoyance is a form of Anger\u201d, \u201cSerenity/Ecstasy is a form of Joy\u201d, and \u201cLove includes Romantic/Family/Friendship\u201d, along with sample passages. We required all annotators have a `master' MTurk",
        "type": "Document"
      },
      {
        "id": "d7abf518-e7a7-4416-a205-629b27dffd15",
        "metadata": {
          "vector_store_key": "1710.01492-6",
          "chunk_id": 36,
          "document_id": "1710.01492",
          "start_idx": 13584,
          "end_idx": 14049
        },
        "page_content": " respect from sentiment analysis, is that of stance detection. The goal here is to determine whether the author of a piece of text is in favor of, against, or neutral toward a proposition or a target BIBREF36 . For example, in (8) the author has a negative stance toward the proposition w\u200bomen have the right to abortion, even though the target is not mentioned at all. Similarly, in (9\u00a7) the author expresses a negative sentiment toward Mitt Romney, from which one",
        "type": "Document"
      },
      {
        "id": "089aaca6-86d9-48b7-987f-de48c59fcc61",
        "metadata": {
          "vector_store_key": "1909.00694-5",
          "chunk_id": 11,
          "document_id": "1909.00694",
          "start_idx": 3871,
          "end_idx": 4400
        },
        "page_content": " events is much simpler and more intuitive. Some previous studies made use of document structure to understand the sentiment. BIBREF11 proposed a sentiment-specific pre-training strategy using unlabeled dialog data (tweet-reply pairs). BIBREF12 proposed a method of building a polarity-tagged corpus (ACP Corpus). They automatically gathered sentences that had positive or negative opinions utilizing HTML layout structures in addition to linguistic patterns. Our method depends only on raw texts and thus has wider applicability",
        "type": "Document"
      },
      {
        "id": "1a5952b6-7d20-4e51-a2f5-114f7a916906",
        "metadata": {
          "vector_store_key": "1911.12569-6",
          "chunk_id": 2,
          "document_id": "1911.12569",
          "start_idx": 923,
          "end_idx": 1418
        },
        "page_content": " perception, predict stock prices and model public opinions BIBREF1. Sentiment analysis BIBREF2 is an important area of research in natural language processing (NLP) where we automatically determine the sentiments (positive, negative, neutral). Emotion analysis focuses on the extraction of predefined emotion from documents. Discrete emotions BIBREF3, BIBREF4 are often classified into anger, anticipation, disgust, fear, joy, sadness, surprise and trust. Sentiments and emotions are subjective",
        "type": "Document"
      },
      {
        "id": "bb643fc7-89e8-4620-b5b0-404514fa7ea7",
        "metadata": {
          "vector_store_key": "2001.07209-1",
          "chunk_id": 34,
          "document_id": "2001.07209",
          "start_idx": 13176,
          "end_idx": 13731
        },
        "page_content": "s corpus, which enabled superior model performance throughout our evaluations. We applied our framework in three ways: 1) evaluation of selected concepts in historical time courses and prediction of human judgments; 2) automatic detection of moral sentiment change; and 3) broad-scale study of the relations between psycholinguistic variables and historical change of moral sentiment toward concepts. We applied our models diachronically to predict time courses of moral relevance, moral polarity, and fine-grained moral categories toward two historically",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What performance did they obtain on the SemEval dataset?",
    "llm_answer": "Improved the existing best system of SemEval 2016 task 6 by 3.2 F-score points.\n",
    "context": [
      {
        "id": "fb4f0221-42a3-405a-b862-2fc00d671e42",
        "metadata": {
          "vector_store_key": "1911.12569-2",
          "chunk_id": 50,
          "document_id": "1911.12569",
          "start_idx": 19389,
          "end_idx": 19799
        },
        "page_content": " features. Our system comfortably surpasses the existing best system at SemEval. Our system manages to improve the existing best system of SemEval 2016 task 6 by 3.2 F-score points for sentiment analysis. We also compare our system with the state-of-the-art systems proposed by BIBREF15 on the emotion dataset. The comparison is demonstrated in Table TABREF22. Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN were",
        "type": "Document"
      },
      {
        "id": "52e6e463-52f1-4d65-9153-b767b95c73f9",
        "metadata": {
          "vector_store_key": "1911.12569-2",
          "chunk_id": 48,
          "document_id": "1911.12569",
          "start_idx": 18757,
          "end_idx": 19135
        },
        "page_content": " anticipation, fear, disgust, joy, sadness, surprise and trust are 0.000002, 0.000143, 0.00403, 0.000015, 0.004607, 0.069, 0.000001 and 0.000001, respectively. These results provide a good indication of statistical significance. Table TABREF19 shows the comparison of our proposed system with the existing state-of-the-art system of SemEval 2016 Task 6 for the sentiment dataset",
        "type": "Document"
      },
      {
        "id": "148ea921-3ae7-4687-bc70-85a9690fe58b",
        "metadata": {
          "vector_store_key": "1912.13109-4",
          "chunk_id": 20,
          "document_id": "1912.13109",
          "start_idx": 7921,
          "end_idx": 8471
        },
        "page_content": " on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we",
        "type": "Document"
      },
      {
        "id": "555a266e-0a08-43e5-b719-8cd2fd10ebb8",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "29e8ab25-ed43-4748-b632-9466561b6b77",
        "metadata": {
          "vector_store_key": "1905.06566-2",
          "chunk_id": 91,
          "document_id": "1905.06566",
          "start_idx": 25555,
          "end_idx": 25950
        },
        "page_content": " comparison ( $p < 0.05$ ), which indicates our model still lags behind Human, but is better than all other systems. As mentioned earlier, our pre-training includes two stages. The first stage is the open-domain pre-training stage on the GIGA-CM dataset and the following stage is the in-domain pre-training on the CNNDM (or NYT50) dataset. As shown in Table 3 , we pretrained $\\text{\\sc Hibert}",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the state-of-the-art systems?",
    "llm_answer": "Deep convolutional networks.\n",
    "context": [
      {
        "id": "577fb06e-9340-4422-93f3-91bc89574aee",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 43,
          "document_id": "2001.00137",
          "start_idx": 16781,
          "end_idx": 17197
        },
        "page_content": " the online platforms. Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector",
        "type": "Document"
      },
      {
        "id": "817a9fbd-0ad5-4350-9d90-c039e237a296",
        "metadata": {
          "vector_store_key": "1710.01492-5",
          "chunk_id": 53,
          "document_id": "1710.01492",
          "start_idx": 19472,
          "end_idx": 19941
        },
        "page_content": " particular, the best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks BIBREF53 , BIBREF54 . Conversely, kernel machines seem to be less frequently used than in the past, and the use of learning methods other than the ones mentioned above is at this point scarce. All these models are examples of supervised learning as they need labeled training data. Semi-supervised learning. We should note two things about the use of deep neural",
        "type": "Document"
      },
      {
        "id": "2479a645-a808-4a2b-bd3c-d81c6ee08cf8",
        "metadata": {
          "vector_store_key": "1912.13109-4",
          "chunk_id": 23,
          "document_id": "1912.13109",
          "start_idx": 9357,
          "end_idx": 9891
        },
        "page_content": " Vo et al and Mathur et al. This was also used as a baseline model but we did not get appreciable performance on such architecture due to FC networks not being able to capture local and long term dependencies. Convolution based architectures: Architecture and hyperparameter choices were chosen from the past study Deon on the subject. We were able to boost the performance as compared to only FC based network but we noticed better performance from architectures that are suitable to sequences such as text messages or any timeseries",
        "type": "Document"
      },
      {
        "id": "7c4f8f12-2d93-4d6c-856c-fba6554a8acb",
        "metadata": {
          "vector_store_key": "1710.01492-0",
          "chunk_id": 52,
          "document_id": "1710.01492",
          "start_idx": 19007,
          "end_idx": 19472
        },
        "page_content": " of all negative scores, sum of all scores, etc. Supervised learning. Traditionally, the above features were fed into classifiers such as Maximum Entropy (MaxEnt) and Support Vector Machines (SVM) with various kernels. However, observation over the SemEval Twitter sentiment task in recent years shows growing interest in, and by now clear dominance of methods based on deep learning. In particular, the best-performing systems at SemEval-2015 and SemEval-2016 used",
        "type": "Document"
      },
      {
        "id": "20e42f0b-cdfc-44c9-bfef-413df2507cdc",
        "metadata": {
          "vector_store_key": "1812.07023-1",
          "chunk_id": 23,
          "document_id": "1812.07023",
          "start_idx": 9153,
          "end_idx": 9631
        },
        "page_content": " are mostly word-overlap-based metrics such as BLEU, METEOR, ROUGE, and CIDEr, borrowed from the machine translation literature. Due to the diverse nature of possible responses, world-overlap metrics are not highly suitable for evaluating these tasks. Human evaluation of generated responses is considered the most reliable metric for such tasks but it is cost prohibitive and hence the dialogue system literature continues to rely widely on word-overlap-based metrics. The avsd",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the size of their collected dataset?",
    "llm_answer": "287,226 training documents, 13,368 validation, and 11,490 test documents for CNNDM; 137,778 training, 17,222 validation, and 17,223 test for NYT50.\n",
    "context": [
      {
        "id": "9270562a-8e53-4e1d-b760-74d8fd0bc872",
        "metadata": {
          "vector_store_key": "1905.06566-7",
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "0723454a-4062-463e-972a-81a58c6a504e",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "84720de1-c988-45c6-bcbe-7a0435cdf6fb",
        "metadata": {
          "vector_store_key": "1706.08032-0",
          "chunk_id": 38,
          "document_id": "1706.08032",
          "start_idx": 14264,
          "end_idx": 14661
        },
        "page_content": " datasets in Table III. for all datasets, the filter window size ( INLINEFORM0 ) is 7 with 6 feature maps each for the first wide convolution layer, the second wide convolution layer has a filter window size of 5 with 14 feature maps each. Dropout rate ( INLINEFORM1 ) is 0.5, INLINEFORM2 constraint, learning rate is 0.1 and momentum of 0.9. Mini-batch size for STS Corpus is 100 and others are 4",
        "type": "Document"
      },
      {
        "id": "651080a7-bc23-40d8-97e8-7f2495d09cdf",
        "metadata": {
          "vector_store_key": "1905.06566-7",
          "chunk_id": 64,
          "document_id": "1905.06566",
          "start_idx": 17240,
          "end_idx": 17663
        },
        "page_content": " , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from New York Times dataset. We used the same training/validation/test splits as in xu:2019:arxiv, which contain 137,778 documents for training, 17,222 for validation and 17,223 for test. To create sentence level labels for extractive summarization, we used a strategy similar to nallapati:2017:aaai. We label the subset of",
        "type": "Document"
      },
      {
        "id": "6a849beb-7f02-435c-9188-73e3f6b1d235",
        "metadata": {
          "vector_store_key": "1701.02877-1",
          "chunk_id": 27,
          "document_id": "1701.02877",
          "start_idx": 9957,
          "end_idx": 10406
        },
        "page_content": " training and testing data sizes measured in number of NEs are listed in Table 2 . Separate models are then trained on the training parts of each corpus and evaluated on the development (if available) and test parts of the same corpus. If development parts are available, as they are for CoNLL (CoNLL Test A) and MUC (MUC 7 Dev), they are not merged with the training corpora for testing, as it was permitted to do in the context of those evaluation",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What kind of features are used by the HMM models, and how interpretable are those?",
    "llm_answer": "HMM models pick up on spaces, indentation, and special characters (such as comment symbols in Linux data).  The provided text indicates that these features are interpretable.\n",
    "context": [
      {
        "id": "6ab18921-ea18-46f8-b43b-fcab485b40b7",
        "metadata": {
          "vector_store_key": "1606.05320-0",
          "chunk_id": 5,
          "document_id": "1606.05320",
          "start_idx": 2248,
          "end_idx": 2783
        },
        "page_content": " quotes, brackets and other clearly identifiable aspects of the text. Building on these results, we take a somewhat more systematic approach to looking for interpretable hidden state dimensions, by using decision trees to predict individual hidden state dimensions (Figure 2 ). We visualize the overall dynamics of the hidden states by coloring the training data with the k-means clusters on the state vectors (Figures 3 , 3 ). We explore several methods for building interpretable models by combining LSTMs and HMMs. The existing body",
        "type": "Document"
      },
      {
        "id": "abf75930-a719-471e-927d-cca2a4ef4c00",
        "metadata": {
          "vector_store_key": "1606.05320-3",
          "chunk_id": 24,
          "document_id": "1606.05320",
          "start_idx": 8305,
          "end_idx": 8784
        },
        "page_content": "such as comment symbols in Linux data). We see some examples where the HMM and LSTM complement each other, such as learning different things about spaces and comments on Linux data, or punctuation on the Shakespeare data. In Figure 2 , we see that some individual LSTM hidden state dimensions identify similar features, such as comment symbols in the Linux data. Hybrid HMM-RNN approaches combine the interpretability of HMMs with the predictive power of RNNs. Sometimes, a small",
        "type": "Document"
      },
      {
        "id": "a4975b67-2cd9-4a00-b733-a89f688c57bd",
        "metadata": {
          "vector_store_key": "1909.08859-1",
          "chunk_id": 12,
          "document_id": "1909.08859",
          "start_idx": 4906,
          "end_idx": 5447
        },
        "page_content": " question and the multiple choice answers are composed of images. Input Module extracts vector representations of inputs at different levels of granularity by using several different encoders. Reasoning Module scans the procedural text and tracks the states of the entities and their relations through a recurrent relational memory core unit BIBREF5. Attention Module computes context-aware query vectors and query-aware context vectors as well as query-aware memory vectors. Modeling Module employs two multi-layered RNNs to encode previous",
        "type": "Document"
      },
      {
        "id": "40283964-9405-4fe7-86ac-b70fe4ae9e3d",
        "metadata": {
          "vector_store_key": "1606.05320-0",
          "chunk_id": 23,
          "document_id": "1606.05320",
          "start_idx": 8072,
          "end_idx": 8468
        },
        "page_content": "STM components. In Figures 3 and 3 , we color-code the training data with the 10 HMM states. In Figures 3 and 3 , we apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters. The HMM and LSTM states pick up on spaces, indentation, and special characters in the data (such as comment symbols in Linux data). We see some examples where the HMM and LSTM",
        "type": "Document"
      },
      {
        "id": "0aeaa416-672d-4feb-b638-ca0f459cc4eb",
        "metadata": {
          "vector_store_key": "1901.04899-4",
          "chunk_id": 3,
          "document_id": "1901.04899",
          "start_idx": 1293,
          "end_idx": 1671
        },
        "page_content": " identified and annotated as: Set/Change Destination, Set/Change Route (including turn-by-turn instructions), Go Faster, Go Slower, Stop, Park, Pull Over, Drop Off, Open Door, and Other (turn music/radio on/off, open/close window/trunk, change AC/temp, show map, etc.). Relevant slots are identified and annotated as: Location, Position/Direction, Object, Time-Guidance, Person,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What kind of information do the HMMs learn that the LSTMs don't?",
    "llm_answer": "HMMs learn information about spaces, comments, and punctuation.\n",
    "context": [
      {
        "id": "abf75930-a719-471e-927d-cca2a4ef4c00",
        "metadata": {
          "vector_store_key": "1606.05320-3",
          "chunk_id": 24,
          "document_id": "1606.05320",
          "start_idx": 8305,
          "end_idx": 8784
        },
        "page_content": "such as comment symbols in Linux data). We see some examples where the HMM and LSTM complement each other, such as learning different things about spaces and comments on Linux data, or punctuation on the Shakespeare data. In Figure 2 , we see that some individual LSTM hidden state dimensions identify similar features, such as comment symbols in the Linux data. Hybrid HMM-RNN approaches combine the interpretability of HMMs with the predictive power of RNNs. Sometimes, a small",
        "type": "Document"
      },
      {
        "id": "e49f867f-df01-467b-8bcd-8a9f502577f5",
        "metadata": {
          "vector_store_key": "1606.05320-3",
          "chunk_id": 25,
          "document_id": "1606.05320",
          "start_idx": 8784,
          "end_idx": 9098
        },
        "page_content": " combine the interpretability of HMMs with the predictive power of RNNs. Sometimes, a small hybrid model can perform better than a standalone LSTM of the same size. We use visualizations to show how the LSTM and HMM components of the hybrid algorithm complement each other in terms of features learned in the data.",
        "type": "Document"
      },
      {
        "id": "21bb2de7-3cf5-4a7d-a6b2-04c3808a99b5",
        "metadata": {
          "vector_store_key": "1606.05320-3",
          "chunk_id": 20,
          "document_id": "1606.05320",
          "start_idx": 6884,
          "end_idx": 7353
        },
        "page_content": " HMM based on a PCA analysis of the LSTM states, as the first 20 components captured almost all the variance. Table 1 shows the predictive log likelihood of the next text character for each method. On all text data sets, the hybrid algorithm performs a bit better than the standalone LSTM with the same LSTM state dimension. This effect gets smaller as we increase the LSTM size and the HMM makes less difference to the prediction (though it can still make a difference",
        "type": "Document"
      },
      {
        "id": "e93dd5c9-fa9e-4cfc-bf59-3ad4b5704058",
        "metadata": {
          "vector_store_key": "1606.05320-4",
          "chunk_id": 8,
          "document_id": "1606.05320",
          "start_idx": 3337,
          "end_idx": 3764
        },
        "page_content": " then make use of the information from the HMM, and fill in the gaps when the HMM is not performing well, resulting in an LSTM with a smaller number of hidden state dimensions that could be interpreted individually (Figures 3 , 3 ). We compare a hybrid HMM-LSTM approach with a continuous emission HMM (trained on the hidden states of a 2-layer LSTM), and a discrete emission HMM (trained directly on data). We use a character-",
        "type": "Document"
      },
      {
        "id": "b6cb5d9c-a9e0-4b5a-8ba4-37989fb9d1cd",
        "metadata": {
          "vector_store_key": "1606.05320-4",
          "chunk_id": 17,
          "document_id": "1606.05320",
          "start_idx": 5921,
          "end_idx": 6363
        },
        "page_content": " HMM on the data, outputting the hidden state distributions obtained by the HMM's forward pass, and then add this information to the architecture in parallel with a 1-layer LSTM. The linear layer between the LSTM and the prediction layer is augmented with an extra column for each HMM state. The LSTM component of this architecture can be smaller than a standalone LSTM, since it only needs to fill in the gaps in the HMM's predictions. The H",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How large is the gap in performance between the HMMs and the LSTMs?",
    "llm_answer": "The hybrid algorithm (combining HMM and LSTM) performs a bit better than the standalone LSTM with the same LSTM state dimension on all text datasets.  The difference diminishes as the LSTM size increases.\n",
    "context": [
      {
        "id": "21bb2de7-3cf5-4a7d-a6b2-04c3808a99b5",
        "metadata": {
          "vector_store_key": "1606.05320-3",
          "chunk_id": 20,
          "document_id": "1606.05320",
          "start_idx": 6884,
          "end_idx": 7353
        },
        "page_content": " HMM based on a PCA analysis of the LSTM states, as the first 20 components captured almost all the variance. Table 1 shows the predictive log likelihood of the next text character for each method. On all text data sets, the hybrid algorithm performs a bit better than the standalone LSTM with the same LSTM state dimension. This effect gets smaller as we increase the LSTM size and the HMM makes less difference to the prediction (though it can still make a difference",
        "type": "Document"
      },
      {
        "id": "e49f867f-df01-467b-8bcd-8a9f502577f5",
        "metadata": {
          "vector_store_key": "1606.05320-3",
          "chunk_id": 25,
          "document_id": "1606.05320",
          "start_idx": 8784,
          "end_idx": 9098
        },
        "page_content": " combine the interpretability of HMMs with the predictive power of RNNs. Sometimes, a small hybrid model can perform better than a standalone LSTM of the same size. We use visualizations to show how the LSTM and HMM components of the hybrid algorithm complement each other in terms of features learned in the data.",
        "type": "Document"
      },
      {
        "id": "4f55a25d-bf0c-4f57-9204-936c782c5ecb",
        "metadata": {
          "vector_store_key": "1606.05320-4",
          "chunk_id": 18,
          "document_id": "1606.05320",
          "start_idx": 6195,
          "end_idx": 6640
        },
        "page_content": "M, since it only needs to fill in the gaps in the HMM's predictions. The HMM is written in Python, and the rest of the architecture is in Torch. We also build a joint hybrid model, where the LSTM and HMM are simultaneously trained in Torch. We implemented an HMM Torch module, optimized using stochastic gradient descent rather than FFBS. Similarly to the sequential hybrid model, we concatenate the LSTM outputs with the HMM state probabilities",
        "type": "Document"
      },
      {
        "id": "ed33e4ee-4f27-4733-b3d6-3e062a2f815f",
        "metadata": {
          "vector_store_key": "1606.05320-3",
          "chunk_id": 22,
          "document_id": "1606.05320",
          "start_idx": 7659,
          "end_idx": 8072
        },
        "page_content": " sets. The joint hybrid is an order of magnitude slower than the sequential hybrid, as the SGD-based HMM is slower to train than the FFBS-based HMM. We interpret the HMM and LSTM states in the hybrid algorithm with 10 LSTM state dimensions and 10 HMM states in Figures 3 and 3 , showing which features are identified by the HMM and LSTM components. In Figures 3 and 3 , we color-code the training data with the 10",
        "type": "Document"
      },
      {
        "id": "439e15bb-4608-47c1-84bf-d30ecc52c819",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 62,
          "document_id": "1909.13375",
          "start_idx": 21089,
          "end_idx": 21507
        },
        "page_content": " available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1. All that, while keeping in mind we compare a base model to a large model (for reference, note the 8 point improvement between MTMSNbase and MTMSNlarge in both EM and F1). Our best model, large-squad, exhibits a huge improvement of 29.7 in EM and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what was their system's f1 performance?",
    "llm_answer": "The system achieved an F1 score of 78.38% on the whole development set.\n",
    "context": [
      {
        "id": "638d3f9b-929b-4fa6-b0d4-efe44cd66255",
        "metadata": {
          "vector_store_key": "1912.13109-2",
          "chunk_id": 25,
          "document_id": "1912.13109",
          "start_idx": 10203,
          "end_idx": 10681
        },
        "page_content": " metrics - Recall rate, F1 score and Overall accuracy. Choice of model parameters were in the above models were inspired from previous work done but then were tuned to the best performance of the Test dataset. Following parameters were considered for tuning. Learning rate: Based on grid search the best performance was achieved when learning rate was set to 0.01. This value was arrived by a grid search on lr parameter. Number of Bidirectional LSTM units: A set of 32, 64, 128",
        "type": "Document"
      },
      {
        "id": "31e3f6a2-9e9b-4d6d-824a-22195518284a",
        "metadata": {
          "vector_store_key": "1909.02764-7",
          "chunk_id": 52,
          "document_id": "1909.02764",
          "start_idx": 20420,
          "end_idx": 20841
        },
        "page_content": "=73 % on TEC. The model trained on our AMMER corpus achieves an F1 score of 57%. This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set. These results are mostly in line with Bostan2018. Now we analyze how well the models trained in Experiment 1 perform when applied to our data set. The results are shown in column \u201cSimple\u201d in Table TAB",
        "type": "Document"
      },
      {
        "id": "be0a5336-6975-40ba-9b09-178b713b3c09",
        "metadata": {
          "vector_store_key": "1701.02877-2",
          "chunk_id": 100,
          "document_id": "1701.02877",
          "start_idx": 35714,
          "end_idx": 36084
        },
        "page_content": " higher at 76.42; the less diverse data is, on average, easier to tag. Interestingly, average F1 on seen NEs in the Twitter corpora (MSM and Ritter) is around 80, whereas average F1 on the ACE corpora, which are of similar size, is lower, at around 70. To summarise, our findings are: [noitemsep] F1 on unseen NEs is significantly lower than F1 on seen NEs for all three",
        "type": "Document"
      },
      {
        "id": "31bcf066-a57a-491a-adf3-189cc3d3ebb7",
        "metadata": {
          "vector_store_key": "1703.04617-7",
          "chunk_id": 88,
          "document_id": "1703.04617",
          "start_idx": 24013,
          "end_idx": 24428
        },
        "page_content": " question is the major class. Figure 5 shows the composition of F1 score. Take our best model as an example, we observed a 78.38% F1 score on the whole development set, which can be separated into two parts: one is where F1 score equals to 100%, which means an exact match. This part accounts for 69.10% of the entire development set. And the other part accounts for 30.90%, of which the average F1 score is 30.03%.",
        "type": "Document"
      },
      {
        "id": "dc8788d2-0422-4d4d-8da6-c8b37e254661",
        "metadata": {
          "vector_store_key": "1603.07044-7",
          "chunk_id": 42,
          "document_id": "1603.07044",
          "start_idx": 14511,
          "end_idx": 14997
        },
        "page_content": " $0.2064$ . To further enhance the system, we incorporate a one hot vector of the original IR ranking as an additional feature into the FNN classifier. Table 3 shows the results. In comparing the models with and without augmented features, we can see large improvement for task B and C. The F1 score for task A degrades slightly but MAP improves. This might be because task A already had a substantial amount of training data. Table 4 gives the final comparison between different models",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much more coverage is in the new dataset?",
    "llm_answer": "Answer not found in the context.\n",
    "context": [
      {
        "id": "08676cf7-5cf4-4a2c-b9d7-ddbe513c2062",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 66,
          "document_id": "1810.00663",
          "start_idx": 25415,
          "end_idx": 25983
        },
        "page_content": " In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models. The previous section evaluated model performance on new instructions (and corresponding navigation plans) for environments that were previously seen at training time. Here, we examine whether the trained models succeed on environments that are completely new. The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as",
        "type": "Document"
      },
      {
        "id": "cb88a1c3-f404-4104-ae5b-f68cd7d3e059",
        "metadata": {
          "vector_store_key": "1909.13375-1",
          "chunk_id": 67,
          "document_id": "1909.13375",
          "start_idx": 22947,
          "end_idx": 23384
        },
        "page_content": " 3.5 points. Given that number questions make up about 61% of the dataset, we can deduce that our improved number handling is responsible for about a 2.1 point gain, while the rest could be be attributed to the improved Wikipedia parsing. Although NER span cleaning (Section SECREF23) affected only 3% of the multi-span questions, it provided a solid improvement of 5.4 EM in multi-span questions and 1.5 EM in single-span questions. The",
        "type": "Document"
      },
      {
        "id": "a872be77-b483-4ddf-9cee-412fb23e61cf",
        "metadata": {
          "vector_store_key": "1912.13109-4",
          "chunk_id": 19,
          "document_id": "1912.13109",
          "start_idx": 7657,
          "end_idx": 8136
        },
        "page_content": " above few data augmentation techniques were applied to boost the learning of the network. Before applying augmentation, a train-test split of 78%-22% was done from the original, cleansed data set. Thus, 700 tweets/messages were held out for testing. All model evaluation were done in on the test set that got generated by this process. The results presented in this report are based on the performance of the model on the test set. The training set of 2489 messages were however",
        "type": "Document"
      },
      {
        "id": "a982cc1e-0dc1-47ba-b4e9-e78baae67bef",
        "metadata": {
          "vector_store_key": "1610.00879-0",
          "chunk_id": 19,
          "document_id": "1610.00879",
          "start_idx": 7038,
          "end_idx": 7487
        },
        "page_content": "2%. This degradation implies that our features capture a subset of drunk tweets and that there are properties of drunk tweets that may be more subtle. When both N-gram and stylistic features are used, there is negligible improvement. The accuracy for Dataset 2 increases from 77.9% to 78.1%. Precision/Recall metrics do not change significantly either. The best accuracy of our classifier is 78.1% for all features, and 75.6% for stylistic features.",
        "type": "Document"
      },
      {
        "id": "780ebc60-7580-490f-8abc-9e16b266a9cc",
        "metadata": {
          "vector_store_key": "1904.05862-3",
          "chunk_id": 37,
          "document_id": "1904.05862",
          "start_idx": 13422,
          "end_idx": 13926
        },
        "page_content": "training and the best accuracy is achieved when we use the largest amount of data for pre-training. In this section we analyze some of the design choices we made for . We pre-train on the 80 hour subset of clean Librispeech and evaluate on TIMIT. Table shows that increasing the number of negative samples only helps up to ten samples. Thereafter, performance plateaus while training time increases. We suspect that this is because the training signal from the positive samples decreases as the number of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How was quality measured?",
    "llm_answer": "Human evaluation, often by randomly sampling generated questions and asking annotators to rate them on a 5-point Likert scale; average rank or percentage of best-ranked questions are used.  For poems, evaluators graded fluency, semantic coherence, imagery, and originality.  For other tasks, macro-averaging of precision, recall, and F1 scores was used.\n",
    "context": [
      {
        "id": "175d0a0b-77ff-450c-8f37-24f131b845e7",
        "metadata": {
          "vector_store_key": "1905.08949-9",
          "chunk_id": 28,
          "document_id": "1905.08949",
          "start_idx": 10602,
          "end_idx": 11043
        },
        "page_content": " they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks. As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE B",
        "type": "Document"
      },
      {
        "id": "c16614f7-b4ab-495b-9659-e6980b3de746",
        "metadata": {
          "vector_store_key": "1909.00578-4",
          "chunk_id": 6,
          "document_id": "1909.00578",
          "start_idx": 2193,
          "end_idx": 2625
        },
        "page_content": " without references. The scores of their model correlate well with Pyramid and Responsiveness, but text quality is only addressed indirectly. Quality Estimation is well established in MT BIBREF15, BIBREF0, BIBREF1, BIBREF16, BIBREF17. QE methods provide a quality indicator for translation output at run-time without relying on human references, typically needed by MT evaluation metrics BIBREF4, BIBREF18. QE models for MT make use",
        "type": "Document"
      },
      {
        "id": "0d8ae1ba-1b03-4ba1-8ecf-3eb758fc662b",
        "metadata": {
          "vector_store_key": "1809.05752-2",
          "chunk_id": 44,
          "document_id": "1809.05752",
          "start_idx": 17771,
          "end_idx": 18259
        },
        "page_content": " the paragraphs in our gold standard. To assess relative performance of feature representations, we also include performance metrics of our models without MWEs. Because this is a multilabel classification task we use macro-averaging to compute precision, recall, and F1 scores for each paragraph in the testing set. In identifying domains individually, our models achieved the highest per-domain scores on Substance (F1 INLINEFORM0 0.8) and the lowest scores on Interpersonal and Mood (F1",
        "type": "Document"
      },
      {
        "id": "6e6ace1d-41d7-457e-a210-1e7d19864073",
        "metadata": {
          "vector_store_key": "1909.00124-3",
          "chunk_id": 39,
          "document_id": "1909.00124",
          "start_idx": 12394,
          "end_idx": 12855
        },
        "page_content": " during training. Summary statistics of the training, validation, and test data are shown in Table TABREF9. Noisy-labeled Training Datasets. For the above three domains (movie, laptop, and restaurant), we collected 2,000 reviews for each domain from the same review source. We extracted sentences from each review and assigned review's label to its sentences. Like previous work, we treat 4 or 5 stars as positive and 1 or 2 stars as negative. The data is noisy",
        "type": "Document"
      },
      {
        "id": "88a343cd-5733-4a91-9fbb-0158597a7c9b",
        "metadata": {
          "vector_store_key": "1909.00279-9",
          "chunk_id": 41,
          "document_id": "1909.00279",
          "start_idx": 15265,
          "end_idx": 15755
        },
        "page_content": "ators to join our human evaluation. The human evaluators were divided into two groups. The expert group contains 15 people who hold a bachelor degree in Chinese literature, and the amateur group contains 15 people who holds a bachelor degree in other fields. All 30 human evaluators are native Chinese speakers. We ask evaluators to grade each generated poem from four perspectives: 1) Fluency: Is the generated poem grammatically and rhythmically well formed, 2) Semantic coherence: Is the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is different in the improved annotation protocol?",
    "llm_answer": "The improved annotation protocol has a new gold standard for evaluations with higher coverage of core and implied roles than the previous Dense evaluation dataset.  It also has three layers of annotation: event types, participant types and coreference annotation.\n",
    "context": [
      {
        "id": "ae777c6e-5f84-42f7-8843-5ba4994ac482",
        "metadata": {
          "vector_store_key": "1703.05260-8",
          "chunk_id": 49,
          "document_id": "1703.05260",
          "start_idx": 17551,
          "end_idx": 18087
        },
        "page_content": " results were discussed with the authors. Throughout the whole annotation phase, annotators could discuss any emerging issues with the authors. All annotations were done by undergraduate students of computational linguistics. The annotation was rather time-consuming due to the complexity of the task, and thus we decided for single annotation mode. To assess annotation quality, a small sample of texts was annotated by all four annotators and their inter-annotator agreement was measured (see Section \"Inter-Annotator Agreement\" ). It",
        "type": "Document"
      },
      {
        "id": "d4c4849a-b0a6-4931-82a7-aed67b41eb03",
        "metadata": {
          "vector_store_key": "1703.05260-7",
          "chunk_id": 19,
          "document_id": "1703.05260",
          "start_idx": 7578,
          "end_idx": 8108
        },
        "page_content": ". This section deals with the annotation of the data. We first describe the final annotation schema. Then, we describe the iterative process of corpus annotation and the refinement of the schema. This refinement was necessary due to the complexity of the annotation. For each of the scenarios, we designed a specific annotation template. A script template consists of scenario-specific event and participant labels. An example of a template is shown in Table 1 . All NP heads in the corpus were annotated with a participant label;",
        "type": "Document"
      },
      {
        "id": "f4340342-0f08-4ff5-a57d-ad7fae35a1e9",
        "metadata": {
          "vector_store_key": "1911.03243-0",
          "chunk_id": 32,
          "document_id": "1911.03243",
          "start_idx": 12706,
          "end_idx": 13226
        },
        "page_content": "A-SRL. It enabled us to release a new gold standard for evaluations, notably of much higher coverage of core and implied roles than the previous Dense evaluation dataset. We believe that our annotation methodology and dataset would facilitate future research on natural semantic annotations and QA-SRL parsing. For completeness, we include several examples with some questions restructured into its 7 template slots in Table TABREF26 As described in section 3 The consolidator receives two sets of QA annotations and mer",
        "type": "Document"
      },
      {
        "id": "e174e690-8f1a-468b-882f-910b6d1610e2",
        "metadata": {
          "vector_store_key": "1810.06743-8",
          "chunk_id": 32,
          "document_id": "1810.06743",
          "start_idx": 11922,
          "end_idx": 12457
        },
        "page_content": " door open to allow for such imperfections. With no strict guidance, it's natural that annotators would take different paths. Nevertheless, modulo annotator disagreement, we assume that within a particular corpus, one word form will always be consistently annotated. Three categories of annotation difficulty are missing values, language-specific attributes, and multiword expressions. In our work, the goal is not simply to translate one schema into the other, but to translate one resource (the imperfect manifestation of the schema)",
        "type": "Document"
      },
      {
        "id": "0244524c-28a2-4bb3-896d-92150da0219d",
        "metadata": {
          "vector_store_key": "1703.05260-7",
          "chunk_id": 20,
          "document_id": "1703.05260",
          "start_idx": 7913,
          "end_idx": 8415
        },
        "page_content": " is shown in Table 1 . All NP heads in the corpus were annotated with a participant label; all verbs were annotated with an event label. For both participants and events, we also offered the label unclear if the annotator could not assign another label. We additionally annotated coreference chains between NPs. Thus, the process resulted in three layers of annotation: event types, participant types and coreference annotation. These are described in detail below. As a first layer, we annotated event",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What data were they used to train the multilingual encoder?",
    "llm_answer": "The context does not specify the data used to train the multilingual encoder.\n",
    "context": [
      {
        "id": "d8c128ab-1d1a-42c9-bce1-912fb128e14a",
        "metadata": {
          "vector_store_key": "1809.04686-0",
          "chunk_id": 31,
          "document_id": "1809.04686",
          "start_idx": 11259,
          "end_idx": 11672
        },
        "page_content": "rightarrow $ Fr and Fr $\\rightarrow $ En examples are interleaved. We picked the best model based on the best average development set BLEU score on both of the language pairs. The Encoder-Classifier model here uses the encoder defined previously. With regards to the classifier, the pre- and post-pooling networks ( $f_{pre}$ , $f_{post}$ ) are both one-layer feed forward networks to cast the dimension size from",
        "type": "Document"
      },
      {
        "id": "8b151bba-98fd-4115-b551-1a195e930878",
        "metadata": {
          "vector_store_key": "1809.04686-0",
          "chunk_id": 38,
          "document_id": "1809.04686",
          "start_idx": 13320,
          "end_idx": 13809
        },
        "page_content": " tasks - Amazon Reviews (English and French), SST, and SNLI. For each task, we first build a baseline system using the proposed Encoder-Classifier architecture described in Section \"Proposed Method\" where the encoder is initialized randomly. Next, we experiment with using the pre-trained multilingual NMT encoder to initialize the system as described in Section \"Analyses\" . Finally, we perform an experiment where we freeze the encoder after initialization and only update the classifier",
        "type": "Document"
      },
      {
        "id": "01385c57-6fa1-4d4e-acc5-fd272f77b9a5",
        "metadata": {
          "vector_store_key": "1809.04686-2",
          "chunk_id": 10,
          "document_id": "1809.04686",
          "start_idx": 3867,
          "end_idx": 4351
        },
        "page_content": " language BIBREF30 . After training such a multilingual NMT model, the decoder and the corresponding attention mechanisms (which are target-language specific) are discarded, while the multilingual encoder is used to initialize the encoder of our proposed Encoder-Classifier model. In order to leverage pre-trained multilingual representations introduced in Section \"Analyses\" , our encoder strictly follows the structure of a regular Recurrent Neural Network (RNN) based NMT encoder B",
        "type": "Document"
      },
      {
        "id": "5526fa0a-3d8f-45c2-8ebf-63698e3c3cf2",
        "metadata": {
          "vector_store_key": "1809.04686-2",
          "chunk_id": 40,
          "document_id": "1809.04686",
          "start_idx": 14114,
          "end_idx": 14616
        },
        "page_content": "izing with a pre-trained multilingual NMT encoder. It can be seen that this provides a significant improvement in accuracy, an average of 4.63%, across all the tasks. This illustrates that the multilingual NMT encoder has successfully learned transferable contextualized representations that are leveraged by the classifier component of our proposed system. These results are in line with the results in BIBREF5 where the authors used the representations from the top NMT encoder layer as an additional",
        "type": "Document"
      },
      {
        "id": "54574acb-3736-487b-baf9-15bc02bc1ac7",
        "metadata": {
          "vector_store_key": "1809.04686-2",
          "chunk_id": 28,
          "document_id": "1809.04686",
          "start_idx": 10090,
          "end_idx": 10545
        },
        "page_content": " sizes, learning rate of optimizers and initialization ranges of weights are tuned on the development sets provided for each task separately. Our multilingual NMT model consists of a shared multilingual encoder and two decoders, one for English and the other for French. The multilingual encoder uses one bi-directional LSTM, followed by three stacked layers of uni-directional LSTMs in the encoder. Each decoder consists of four stacked LSTM layers, with",
        "type": "Document"
      }
    ]
  },
  {
    "query": "From when are many VQA datasets collected?",
    "llm_answer": "2014\u20132016\n",
    "context": [
      {
        "id": "a8f7520c-c6ca-4824-be2d-21251f3f833a",
        "metadata": {
          "vector_store_key": "1703.09684-0",
          "chunk_id": 1,
          "document_id": "1703.09684",
          "start_idx": 479,
          "end_idx": 906
        },
        "page_content": "A datasets must test a wide range of abilities for progress to be adequately measured. VQA research began in earnest in late 2014 when the DAQUAR dataset was released BIBREF0 . Including DAQUAR, six major VQA datasets have been released, and algorithms have rapidly improved. On the most popular dataset, `The VQA Dataset' BIBREF1 , the best algorithms are now approaching 70% accuracy BIBREF2 (human performance is 83%). While",
        "type": "Document"
      },
      {
        "id": "2c3fa041-c673-48b3-8f69-6825d9da3a7d",
        "metadata": {
          "vector_store_key": "1703.09684-0",
          "chunk_id": 6,
          "document_id": "1703.09684",
          "start_idx": 2425,
          "end_idx": 2802
        },
        "page_content": "?', `Does the use of spatial attention help answer specific question-types?', `How successful are the VQA algorithms in answering less-common questions?', and 'Can the VQA algorithms differentiate between real and absurd questions?' Six datasets for VQA with natural images have been released between 2014\u20132016: DAQUAR BIBREF0 , COCO-QA BIBREF3 , FM-IQA BIBREF4 , The VQA Datas",
        "type": "Document"
      },
      {
        "id": "eb6b3da2-73c7-4512-888e-a35424000377",
        "metadata": {
          "vector_store_key": "1703.09684-0",
          "chunk_id": 23,
          "document_id": "1703.09684",
          "start_idx": 8700,
          "end_idx": 9279
        },
        "page_content": " contains natural images and questions. To improve algorithm analysis and comparison, our dataset has more (12) explicitly defined question-types and new evaluation metrics. In the past two years, multiple publicly released datasets have spurred the VQA research. However, due to the biases and issues with evaluation metrics, interpreting and comparing the performance of VQA systems can be opaque. We propose a new benchmark dataset that explicitly assigns questions into 12 distinct categories. This enables measuring performance within each category and understand which kind",
        "type": "Document"
      },
      {
        "id": "96658579-1a56-43e3-8ef2-b21526816310",
        "metadata": {
          "vector_store_key": "1703.09684-0",
          "chunk_id": 8,
          "document_id": "1703.09684",
          "start_idx": 3024,
          "end_idx": 3354
        },
        "page_content": " , BIBREF8 , BIBREF9 , we refer to the portion of The VQA Dataset containing natural images as COCO-VQA. Detailed dataset reviews can be found in BIBREF10 and BIBREF11 . All of the aforementioned VQA datasets are biased. DAQUAR and COCO-QA are small and have a limited variety of question-types. Visual Genome, Visual7W, and COCO-",
        "type": "Document"
      },
      {
        "id": "95dc4449-2b21-43a1-9de5-f3e660b83f01",
        "metadata": {
          "vector_store_key": "1703.09684-0",
          "chunk_id": 50,
          "document_id": "1703.09684",
          "start_idx": 19286,
          "end_idx": 19586
        },
        "page_content": " majority of VQA systems formulate it as a classification problem in which the system is given an image and a question, with the answers as categories. BIBREF1 , BIBREF3 , BIBREF2 , BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26 , BIBREF9 , BIBREF27 , BIBREF28 , BIBREF8 ,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is task success rate achieved? ",
    "llm_answer": "The context discusses \"task finish rate\" calculated over 1000 simulations, but not a success rate.  The answer is not found in the context.\n",
    "context": [
      {
        "id": "cbb016ce-46fa-4916-ab46-cafe71395ef1",
        "metadata": {
          "vector_store_key": "2002.11893-7",
          "chunk_id": 90,
          "document_id": "2002.11893",
          "start_idx": 33632,
          "end_idx": 34069
        },
        "page_content": " are filled by real values, the simulator terminates. This is regarded as \"task finish\". It's worth noting that \"task finish\" does not mean the task is success, because the system may provide wrong information. We calculated \"task finish rate\" on 1000 times simulations for each goal type (See Table TABREF31). Findings are summarized below: Cross multi-domain tasks (CM and CM+T) are much harder to finish. Comparing M and M+T, although",
        "type": "Document"
      },
      {
        "id": "86b81fb3-64d5-46ec-b0f3-4f031e736763",
        "metadata": {
          "vector_store_key": "2002.11893-7",
          "chunk_id": 54,
          "document_id": "2002.11893",
          "start_idx": 20211,
          "end_idx": 20604
        },
        "page_content": " we generate independent multi-domain, cross multi-domain, and traffic domain sub-goals one by one. Thus in terms of the task complexity, we have S<M<CM and M<M+T<CM+T, which is supported by the average number of sub-goals, semantic tuples, and turns per dialogue in Table TABREF26. The average number of tokens also becomes larger when the goal becomes more complex. About 60% of dialogues (M",
        "type": "Document"
      },
      {
        "id": "9a20d306-b0ea-4dad-9565-9cc06e29256c",
        "metadata": {
          "vector_store_key": "1909.00754-0",
          "chunk_id": 60,
          "document_id": "1909.00754",
          "start_idx": 16549,
          "end_idx": 16980
        },
        "page_content": ".15x, while for an ITC of $O(n)$ , it will be a multiplier of 25.1, and 1,143 for $O(mn)$ . As a convention, the metric of joint goal accuracy is used to compare our model to previous work. The joint goal accuracy only regards the model making a successful belief state prediction if all of the slots and values predicted are exactly matched with the labels provided. This metric gives a strict measurement that tells how often the",
        "type": "Document"
      },
      {
        "id": "508d5f3a-9bd7-4f38-bbe0-ba152ea120dd",
        "metadata": {
          "vector_store_key": "2002.11893-7",
          "chunk_id": 57,
          "document_id": "2002.11893",
          "start_idx": 21322,
          "end_idx": 21715
        },
        "page_content": " captured by \"NoOffer rate\", \"Multi-query rate\", and \"Goal change rate\" in Table TABREF26. In addition, \"Multi-query rate\" suggests that each sub-goal in M and M+T is as easy to finish as the goal in S. The distribution of dialogue length is shown in Figure FIGREF27, which is an indicator of the task complexity. Most single-domain dialogues terminate within 10 turns. The curves of M and M+T",
        "type": "Document"
      },
      {
        "id": "487c7366-83d4-4b58-ab24-21748e8fbc90",
        "metadata": {
          "vector_store_key": "1703.09684-3",
          "chunk_id": 48,
          "document_id": "1703.09684",
          "start_idx": 18525,
          "end_idx": 19045
        },
        "page_content": " all question-types and is skewed towards lowest performing categories. We also use normalized metrics that compensate for bias in the form of imbalance in the distribution of answers within each question-type, e.g., the most repeated answer `two' covers over 35% of all the counting-type questions. To do this, we compute the accuracy for each unique answer separately within a question-type and then average them together for the question-type. To compute overall performance, we compute the arithmetic normalized mean",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Does proposed end-to-end approach learn in reinforcement or supervised learning manner?",
    "llm_answer": "Reinforcement learning.\n",
    "context": [
      {
        "id": "9942eee4-2d4e-4ef6-8cc9-7c5ae4f82171",
        "metadata": {
          "vector_store_key": "1902.00330-5",
          "chunk_id": 103,
          "document_id": "1902.00330",
          "start_idx": 35109,
          "end_idx": 35645
        },
        "page_content": " of terms, Han et al. BIBREF41 designed an end-to-end reinforcement learning model to determine which term to select and where to place it on the taxonomy, which effectively reduced the error propagation between two phases. Inspired by the above works, we also add reinforcement learning to our framework. In this paper we consider entity linking as a sequence decision problem and present a reinforcement learning based model. Our model learns the policy on selecting target entities in a sequential manner and makes decisions based on",
        "type": "Document"
      },
      {
        "id": "1e75cb48-72fa-445d-8a8b-3727eabb15e3",
        "metadata": {
          "vector_store_key": "1911.11744-0",
          "chunk_id": 3,
          "document_id": "1911.11744",
          "start_idx": 1380,
          "end_idx": 1908
        },
        "page_content": "REF4, and human-robot interaction BIBREF5 an important question is how to incorporate language and vision into a differentiable end-to-end system for complex robot control. In this paper, we present an imitation learning approach that combines language, vision, and motion in order to synthesize natural language-conditioned control policies that have strong generalization capabilities while also capturing the semantics of the task. We argue that such a multi-modal teaching approach enables robots to acquire complex policies",
        "type": "Document"
      },
      {
        "id": "c0d6d384-55a2-4aa5-a1fb-55dada08099b",
        "metadata": {
          "vector_store_key": "1810.00663-4",
          "chunk_id": 76,
          "document_id": "1810.00663",
          "start_idx": 29565,
          "end_idx": 30130
        },
        "page_content": " applications, including information retrieval. We proposed an end-to-end system to translate user instructions to a high-level navigation plan. Our model utilized an attention mechanism to merge relevant information from the navigation instructions with a behavioral graph of the environment. The model then used a decoder to predict a sequence of navigation behaviors that matched the input commands. As part of this effort, we contributed a new dataset of 11,051 pairs of user instructions and navigation plans from 100 different environments. Our model achieved",
        "type": "Document"
      },
      {
        "id": "c5bb7f9c-b812-40e4-a925-6e9dcd252f55",
        "metadata": {
          "vector_store_key": "1810.00663-4",
          "chunk_id": 75,
          "document_id": "1810.00663",
          "start_idx": 28993,
          "end_idx": 29565
        },
        "page_content": " the model is indeed using the input instructions and is not just approximating shortest paths in the behavioral graph. Other examples on the prediction of sub-obtimal paths are described in the Appendix. This work introduced behavioral navigation through free-form natural language instructions as a challenging and a novel task that falls at the intersection of natural language processing and robotics. This problem has a range of interesting cross-domain applications, including information retrieval. We proposed an end-to-end system to translate user instructions to",
        "type": "Document"
      },
      {
        "id": "51efb65b-2c96-40aa-8bcb-81cc71a95e2f",
        "metadata": {
          "vector_store_key": "1902.00330-5",
          "chunk_id": 101,
          "document_id": "1902.00330",
          "start_idx": 34293,
          "end_idx": 34753
        },
        "page_content": " state to help disambiguation. In the last few years, reinforcement learning has emerged as a powerful tool for solving complex sequential decision-making problems. It is well known for its great success in the game field, such as Go BIBREF37 and Atari games BIBREF38 . Recently, reinforcement learning has also been successfully applied to many natural language processing tasks and achieved good performance BIBREF12 , BIBREF39 , BIBREF5 . Feng et al. BIBREF",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is performance of this system measured?",
    "llm_answer": "Performance is measured using EM-score, macro-averaged F$_1$-score, recall rate, F1 score, and overall accuracy.  Normalized metrics are used to compensate for answer distribution imbalances.  Human evaluation using a Likert scale is also used for some systems.\n",
    "context": [
      {
        "id": "487c7366-83d4-4b58-ab24-21748e8fbc90",
        "metadata": {
          "vector_store_key": "1703.09684-3",
          "chunk_id": 48,
          "document_id": "1703.09684",
          "start_idx": 18525,
          "end_idx": 19045
        },
        "page_content": " all question-types and is skewed towards lowest performing categories. We also use normalized metrics that compensate for bias in the form of imbalance in the distribution of answers within each question-type, e.g., the most repeated answer `two' covers over 35% of all the counting-type questions. To do this, we compute the accuracy for each unique answer separately within a question-type and then average them together for the question-type. To compute overall performance, we compute the arithmetic normalized mean",
        "type": "Document"
      },
      {
        "id": "bb75c3fa-e2c5-412d-9db8-2f31cd178291",
        "metadata": {
          "vector_store_key": "1908.06606-4",
          "chunk_id": 45,
          "document_id": "1908.06606",
          "start_idx": 15988,
          "end_idx": 16383
        },
        "page_content": ") and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage",
        "type": "Document"
      },
      {
        "id": "2d6d7c97-0fa2-456b-a7e4-2f9004d31f72",
        "metadata": {
          "vector_store_key": "1811.00942-2",
          "chunk_id": 28,
          "document_id": "1811.00942",
          "start_idx": 9060,
          "end_idx": 9476
        },
        "page_content": " same frameworks for evaluation. We plugged the RPi into a Watts Up Pro meter, a power meter that can be read programatically over USB at a frequency of 1 Hz. For the QRNNs, we used the first 350 words of the test set, and averaged the ms/query and mJ/query. For KN-5, we used the entire test set for evaluation, since the latency was much lower. To adjust for the base power load, we subtracted idle power draw from",
        "type": "Document"
      },
      {
        "id": "175d0a0b-77ff-450c-8f37-24f131b845e7",
        "metadata": {
          "vector_store_key": "1905.08949-9",
          "chunk_id": 28,
          "document_id": "1905.08949",
          "start_idx": 10602,
          "end_idx": 11043
        },
        "page_content": " they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks. As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE B",
        "type": "Document"
      },
      {
        "id": "638d3f9b-929b-4fa6-b0d4-efe44cd66255",
        "metadata": {
          "vector_store_key": "1912.13109-2",
          "chunk_id": 25,
          "document_id": "1912.13109",
          "start_idx": 10203,
          "end_idx": 10681
        },
        "page_content": " metrics - Recall rate, F1 score and Overall accuracy. Choice of model parameters were in the above models were inspired from previous work done but then were tuned to the best performance of the Test dataset. Following parameters were considered for tuning. Learning rate: Based on grid search the best performance was achieved when learning rate was set to 0.01. This value was arrived by a grid search on lr parameter. Number of Bidirectional LSTM units: A set of 32, 64, 128",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big dataset is used for training this system?",
    "llm_answer": "The context mentions various datasets, but doesn't specify a single, overall training dataset size.\n",
    "context": [
      {
        "id": "3c3e7bd6-8955-4353-bdf3-c8d8c69b5e4d",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 59,
          "document_id": "1909.13375",
          "start_idx": 20052,
          "end_idx": 20444
        },
        "page_content": "s. For the large models we used a batch size of 3 with a learning rate of $5e^{-6}$ and trained for 5 epochs, except for the model without the single-span heads that was trained with a batch size of 2 for 7 epochs. F1 was used as our validation metric. All models were trained on a single GPU with 12-16GB of memory. Table TABREF24 shows the results on DROP's development set. Compared to our",
        "type": "Document"
      },
      {
        "id": "3fd0d403-b663-4f73-b8f1-cfbe94ff3a61",
        "metadata": {
          "vector_store_key": "1910.12618-4",
          "chunk_id": 48,
          "document_id": "1910.12618",
          "start_idx": 19089,
          "end_idx": 19565
        },
        "page_content": " maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014",
        "type": "Document"
      },
      {
        "id": "148ea921-3ae7-4687-bc70-85a9690fe58b",
        "metadata": {
          "vector_store_key": "1912.13109-4",
          "chunk_id": 20,
          "document_id": "1912.13109",
          "start_idx": 7921,
          "end_idx": 8471
        },
        "page_content": " on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we",
        "type": "Document"
      },
      {
        "id": "912fbbf9-4132-4a1d-b32a-172795b7e15d",
        "metadata": {
          "vector_store_key": "1905.06566-7",
          "chunk_id": 67,
          "document_id": "1905.06566",
          "start_idx": 18293,
          "end_idx": 18730
        },
        "page_content": "DM, NYT50 and GIGA-CM are all segmented and tokenized using Stanford CoreNLP toolkit BIBREF40 . To reduce the vocabulary size, we applied byte pair encoding (BPE; BIBREF41 ) to all of our datasets. To limit the memory consumption during training, we limit the length of each sentence to be 50 words (51th word and onwards are removed) and split documents with more than 30 sentences into smaller documents with each containing at most 30",
        "type": "Document"
      },
      {
        "id": "9270562a-8e53-4e1d-b760-74d8fd0bc872",
        "metadata": {
          "vector_store_key": "1905.06566-7",
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they obtain word lattices from words?",
    "llm_answer": "For a sentence in Chinese, all possible substrings that can be considered as words are treated as vertices.  Neighboring words are connected by directed edges.  Out-of-vocabulary characters are replaced with labels to connect the graph.\n",
    "context": [
      {
        "id": "7c99799e-f48e-411b-86ef-46ebfebbb48f",
        "metadata": {
          "vector_store_key": "1902.09087-2",
          "chunk_id": 23,
          "document_id": "1902.09087",
          "start_idx": 8933,
          "end_idx": 9429
        },
        "page_content": ". The inherent graph structure of a word lattice allows all possible words represented explicitly, no matter the overlapping and nesting cases, and all of them can contribute directly to the sentence representations. As we mentioned in previous section, we can not directly apply standard CNNs to take word lattice as input, since there could be multiple feature vectors produced for a given word. Inspired by previous lattice LSTM models BIBREF10 , BIBREF11 , here we propose a lattice based CNN",
        "type": "Document"
      },
      {
        "id": "59dfffa1-e0e4-4be3-96ba-56708a75bc6b",
        "metadata": {
          "vector_store_key": "1902.09087-8",
          "chunk_id": 19,
          "document_id": "1902.09087",
          "start_idx": 7413,
          "end_idx": 7832
        },
        "page_content": "\u201d. As shown in Figure FIGREF4 , a word lattice is a directed graph INLINEFORM0 , where INLINEFORM1 represents a node set and INLINEFORM2 represents a edge set. For a sentence in Chinese, which is a sequence of Chinese characters INLINEFORM3 , all of its possible substrings that can be considered as words are treated as vertexes, i.e. INLINEFORM4 . Then, all neighbor words are connected by directed edges according to",
        "type": "Document"
      },
      {
        "id": "22042c24-6957-4062-84c6-b0adde3f1857",
        "metadata": {
          "vector_store_key": "1902.09087-2",
          "chunk_id": 22,
          "document_id": "1902.09087",
          "start_idx": 8403,
          "end_idx": 8933
        },
        "page_content": " be disconnected because of a few out-of-vocabulary characters. Thus, we append INLINEFORM0 labels to replace those characters to connect the graph. Obviously, word lattices are collections of characters and all possible words. Therefore, it is not necessary to make explicit decisions regarding specific word segmentations, but just embed all possible information into the lattice and take them to the next CNN layers. The inherent graph structure of a word lattice allows all possible words represented explicitly, no matter the",
        "type": "Document"
      },
      {
        "id": "bf1b8fa6-8311-4873-8180-947c0595342f",
        "metadata": {
          "vector_store_key": "1902.09087-2",
          "chunk_id": 17,
          "document_id": "1902.09087",
          "start_idx": 6782,
          "end_idx": 7248
        },
        "page_content": " each word may have different context words in different granularities in a lattice and may be treated as the center in various kernel spans with same length. Therefore, different from the original CNNs, there could be several feature vectors produced for a given word, which is the key challenge to apply the standard CNNs directly to a lattice input. For the example shown in Figure FIGREF6 , the word \u201ccitizen\u201d is the center word of four text spans with length 3:",
        "type": "Document"
      },
      {
        "id": "3f680f51-2a24-48ec-ae36-9d97df45d0e4",
        "metadata": {
          "vector_store_key": "1902.09087-2",
          "chunk_id": 68,
          "document_id": "1902.09087",
          "start_idx": 25087,
          "end_idx": 25615
        },
        "page_content": " construct from various word segmentations with different strategies to investigate the balance between the noisy words and additional information introduced by word lattice. We only use the DBQA dataset because word lattices here are more complex, so the construction strategies have more influence. Pilot experiments show that word lattices constructed based on character sequence perform better, so the strategies in Table TABREF33 are based on CNN-char. From Table TABREF33 , it is shown that all kinds of lattice are better",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How better is proposed method than baselines perpexity wise?",
    "llm_answer": "Answer not found in the context.\n",
    "context": [
      {
        "id": "70ea7d85-857e-480e-8d2e-39733427c7cd",
        "metadata": {
          "vector_store_key": "1603.07044-6",
          "chunk_id": 35,
          "document_id": "1603.07044",
          "start_idx": 12277,
          "end_idx": 12632
        },
        "page_content": ": for example, we could only use the parameter for embedding layers etc. For task B, the SNLI yields a slight improvement on MAP ( $0.2\\%$ ), and Task A could give ( $1.2\\%$ ) on top of that. No improvement was observed on F1. For task C, pretraining by task A is also better than using SNLI (task A is $1\\%$ better than the baseline, while SNLI is almost",
        "type": "Document"
      },
      {
        "id": "8e595a15-617e-47fa-8f20-8208066c69dc",
        "metadata": {
          "vector_store_key": "1905.06566-2",
          "chunk_id": 83,
          "document_id": "1905.06566",
          "start_idx": 23289,
          "end_idx": 23633
        },
        "page_content": " by 0.5 ROUGE. We also implemented two baselines. One is the hierarchical transformer summarization model (HeriTransfomer; described in \"Extractive Summarization\" ) without pre-training. Note the setting for HeriTransfomer is ( $L=4$ , $H=300$ and $A=4$ ) . We can see that the pre-training (details in Section \"Pre-training\" ) leads to a +1.25",
        "type": "Document"
      },
      {
        "id": "11d9b2e6-e20d-4ab9-8d7c-c35a7e2d27e6",
        "metadata": {
          "vector_store_key": "1603.07044-6",
          "chunk_id": 43,
          "document_id": "1603.07044",
          "start_idx": 14997,
          "end_idx": 15515
        },
        "page_content": " task A already had a substantial amount of training data. Table 4 gives the final comparison between different models (we only list the MAP score because it is the official score for the challenge). Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited",
        "type": "Document"
      },
      {
        "id": "25bb2afe-7835-48e9-a6f8-dbdbbde2116d",
        "metadata": {
          "vector_store_key": "1902.06843-8",
          "chunk_id": 91,
          "document_id": "1902.06843",
          "start_idx": 35139,
          "end_idx": 35416
        },
        "page_content": "s by -0.27. Baselines: To test the efficacy of our multi-modal framework for detecting depressed users, we compare it against existing content, content-network, and image-based models (based on the aforementioned general image feature, facial presence, and facial expressions.)",
        "type": "Document"
      },
      {
        "id": "f189eac9-0435-44a6-90b3-1a428fecf8da",
        "metadata": {
          "vector_store_key": "1707.03904-4",
          "chunk_id": 53,
          "document_id": "1707.03904",
          "start_idx": 20086,
          "end_idx": 20524
        },
        "page_content": " \u2013 finding the few most relevant ones will allow the reader to work best. In Tables 2 and 3 we compare all baselines when the context size is tuned to maximize the overall accuracy on the validation set. For Quasar-S the best performing baseline is the BiRNN language model, which achieves $33.6\\%$ accuracy. The GA model achieves $48.3\\%$ accuracy on the set of instances for which the answer is in context, however, a search accuracy of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Who manually annotated the semantic roles for the set of learner texts?",
    "llm_answer": "Undergraduate students of computational linguistics.\n",
    "context": [
      {
        "id": "f7d27bfd-8ade-4a9f-a940-08dbdb036495",
        "metadata": {
          "vector_store_key": "1808.09409-6",
          "chunk_id": 65,
          "document_id": "1808.09409",
          "start_idx": 24257,
          "end_idx": 24791
        },
        "page_content": ", we use additional 156,520 sentences to enhance the Berkeley parser. Statistical models of annotating learner texts are making rapid progress. Although there have been some initial studies on defining annotation specification as well as corpora for syntactic analysis, there is almost no work on semantic parsing for interlanguages. This paper discusses this topic, taking Semantic Role Labeling as a case task and learner Chinese as a case language. We reveal three unknown facts that are important towards a deeper analysis of lear",
        "type": "Document"
      },
      {
        "id": "6fb40b64-df26-4164-a2e6-75d80333f0bb",
        "metadata": {
          "vector_store_key": "1808.09409-7",
          "chunk_id": 23,
          "document_id": "1808.09409",
          "start_idx": 8985,
          "end_idx": 9407
        },
        "page_content": ". As for role labels, we mainly apply representations defined by CPB BIBREF1 . The predicate in a sentence was labeled as rel, the core semantic roles were labeled as AN and the adjuncts were labeled as AM. For inter-annotator agreement, we evaluate the precision (P), recall (R), and F1-score (F) of the semantic labels given by the two annotators. Table TABREF5 shows that our inter-annotator agreement is promising. All",
        "type": "Document"
      },
      {
        "id": "2b0e3271-d80b-48a3-8261-0d30cfa38bb2",
        "metadata": {
          "vector_store_key": "1808.09409-6",
          "chunk_id": 18,
          "document_id": "1808.09409",
          "start_idx": 6851,
          "end_idx": 7401
        },
        "page_content": " sentence pairs. We take the mother languages of the learners into consideration, which have a great impact on grammatical errors and hence automatic semantic analysis. We hope that four selected mother tongues guarantee a good coverage of typologies. The annotated corpus can be used both for linguistic investigation and as test data for NLP systems. Semantic role labeling (SRL) is the process of assigning semantic roles to constituents or their head words in a sentence according to their relationship to the predicates expressed in the sentence",
        "type": "Document"
      },
      {
        "id": "7900cb90-aa68-4928-b55d-04d9b3ec7099",
        "metadata": {
          "vector_store_key": "1910.11769-4",
          "chunk_id": 12,
          "document_id": "1910.11769",
          "start_idx": 4371,
          "end_idx": 4859
        },
        "page_content": "\ufffd, along with sample passages. We required all annotators have a `master' MTurk qualification. Each passage was labelled by 3 unique annotators. Only passages with a majority agreement between annotators were accepted as valid. This is equivalent to a Fleiss's $\\kappa $ score of greater than $0.4$. For passages without majority agreement between annotators, we consolidated their labels using in-house data annotators who are experts in narrative content. A passage is accepted as valid",
        "type": "Document"
      },
      {
        "id": "ae777c6e-5f84-42f7-8843-5ba4994ac482",
        "metadata": {
          "vector_store_key": "1703.05260-8",
          "chunk_id": 49,
          "document_id": "1703.05260",
          "start_idx": 17551,
          "end_idx": 18087
        },
        "page_content": " results were discussed with the authors. Throughout the whole annotation phase, annotators could discuss any emerging issues with the authors. All annotations were done by undergraduate students of computational linguistics. The annotation was rather time-consuming due to the complexity of the task, and thus we decided for single annotation mode. To assess annotation quality, a small sample of texts was annotated by all four annotators and their inter-annotator agreement was measured (see Section \"Inter-Annotator Agreement\" ). It",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they obtain region descriptions and object annotations?",
    "llm_answer": "The text describes a method to mine visual groundings for region annotations, and algorithms to generate questions from COCO and Visual Genome annotations.  Human annotators were used for certain question types.\n",
    "context": [
      {
        "id": "2f9f7733-4646-4068-8569-50437df6861e",
        "metadata": {
          "vector_store_key": "1808.00265-8",
          "chunk_id": 37,
          "document_id": "1808.00265",
          "start_idx": 14833,
          "end_idx": 15411
        },
        "page_content": " of visual grounding, these region and object annotations provide complementary information. As an example, as shown in Figure FIGREF3 , for questions related to interaction between objects, region annotations result highly relevant. In contrast, for questions related to properties of specific objects, object annotations result more valuable. Consequently, in this section we present a method to automatically select region and object annotations from VG that can be used as labels to implement visual grounding as an auxiliary task for VQA. For region annotations, we propose",
        "type": "Document"
      },
      {
        "id": "d4c4849a-b0a6-4931-82a7-aed67b41eb03",
        "metadata": {
          "vector_store_key": "1703.05260-7",
          "chunk_id": 19,
          "document_id": "1703.05260",
          "start_idx": 7578,
          "end_idx": 8108
        },
        "page_content": ". This section deals with the annotation of the data. We first describe the final annotation schema. Then, we describe the iterative process of corpus annotation and the refinement of the schema. This refinement was necessary due to the complexity of the annotation. For each of the scenarios, we designed a specific annotation template. A script template consists of scenario-specific event and participant labels. An example of a template is shown in Table 1 . All NP heads in the corpus were annotated with a participant label;",
        "type": "Document"
      },
      {
        "id": "3f963457-b7a1-4317-a5ce-a34a44518790",
        "metadata": {
          "vector_store_key": "1808.00265-8",
          "chunk_id": 38,
          "document_id": "1808.00265",
          "start_idx": 15190,
          "end_idx": 15648
        },
        "page_content": " labels to implement visual grounding as an auxiliary task for VQA. For region annotations, we propose a simple heuristic to mine visual groundings: for each INLINEFORM0 we enumerate all the region descriptions of INLINEFORM1 and pick the description INLINEFORM2 that has the most (at least two) overlapped informative words with INLINEFORM3 and INLINEFORM4 . Informative words are all nouns and verbs, where two informative words are matched if at least one",
        "type": "Document"
      },
      {
        "id": "c4166a62-accf-4d75-b270-17f4949c67ce",
        "metadata": {
          "vector_store_key": "1703.09684-6",
          "chunk_id": 26,
          "document_id": "1703.09684",
          "start_idx": 9860,
          "end_idx": 10324
        },
        "page_content": "QA and Visual Genome. Second, we created algorithms that generated questions from COCO's semantic segmentation annotations BIBREF17 , and Visual Genome's objects and attributes annotations BIBREF6 . Third, we used human annotators for certain question-types. In the following sections, we briefly describe each of these methods. We imported questions from COCO-VQA and Visual Genome belonging to all question-types except `object utilities and affordances'. We did",
        "type": "Document"
      },
      {
        "id": "318360a8-e1ec-4db8-8265-87747a8f5795",
        "metadata": {
          "vector_store_key": "1902.00330-7",
          "chunk_id": 21,
          "document_id": "1902.00330",
          "start_idx": 8140,
          "end_idx": 8466
        },
        "page_content": "IBREF11 . Then, we use Long Short-Term Memory (LSTM) networks to encode the contextual word sequence $\\lbrace w_c^1, w_c^2,..., w_c^n\\rbrace $ as a fixed-size vector $V_{m_t}$ . The description of entity is encoded as $D_{e_t^i}$ in the same way. Apart from the description of entity, there are many other valuable information",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which training dataset allowed for the best generalization to benchmark sets?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "12cc43a7-f833-4eb4-91b8-855b86d2e510",
        "metadata": {
          "vector_store_key": "1812.06705-0",
          "chunk_id": 43,
          "document_id": "1812.06705",
          "start_idx": 15717,
          "end_idx": 16178
        },
        "page_content": "-trained one. Six benchmark classification datasets are listed in table 1 . Following Kim BIBREF24 , for a dataset without validation data, we use 10% of its training set for the validation set. Summary statistics of six classification datasets are shown in table 1. SST BIBREF25 SST (Stanford Sentiment Treebank) is a dataset for sentiment classification on movie reviews, which are annotated with five labels (SST5: very positive, positive, neutral, negative,",
        "type": "Document"
      },
      {
        "id": "f8061e5e-74f5-49d0-8dbe-90e4db84d4eb",
        "metadata": {
          "vector_store_key": "1810.00663-7",
          "chunk_id": 46,
          "document_id": "1810.00663",
          "start_idx": 17810,
          "end_idx": 18361
        },
        "page_content": " . This dataset was created using Amazon Mechanical Turk and 100 maps of simulated indoor environments, each with 6 to 65 rooms. To the best of our knowledge, this is the first benchmark for comparing translation models in the context of behavioral robot navigation. As shown in Table TABREF16 , the dataset consists of 8066 pairs of free-form natural language instructions and navigation plans for training. This training data was collected from 88 unique simulated environments, totaling 6064 distinct navigation plans (2002 plans have two different",
        "type": "Document"
      },
      {
        "id": "8cf9b7c9-3579-41ac-9e68-ebbe040f34c6",
        "metadata": {
          "vector_store_key": "1703.09684-3",
          "chunk_id": 16,
          "document_id": "1703.09684",
          "start_idx": 5866,
          "end_idx": 6368
        },
        "page_content": " address the fact that some categories were much easier to classify than others (e.g., faces and planes were easy and also had the largest number of test images). Mean per-class accuracy compensates for this by requiring a system to do well on each category, even when the amount of test instances in categories vary considerably. Existing benchmarks do not require reporting accuracies across different question-types. Even when they are reported, the question-types can be too coarse to be useful, e.",
        "type": "Document"
      },
      {
        "id": "4cac7588-28e9-4b6e-ac0d-d763db64bc58",
        "metadata": {
          "vector_store_key": "1703.04617-6",
          "chunk_id": 3,
          "document_id": "1703.04617",
          "start_idx": 1343,
          "end_idx": 1868
        },
        "page_content": " can help attain better results on our competitive baselines. Recent advance on reading comprehension and question answering has been closely associated with the availability of various datasets. BIBREF0 released the MCTest data consisting of 500 short, fictional open-domain stories and 2000 questions. The CNN/Daily Mail dataset BIBREF1 contains news articles for close style machine comprehension, in which only entities are removed and tested for comprehension. Children's Book Test (CBT) BIBREF2 leverages named entities",
        "type": "Document"
      },
      {
        "id": "b94a7b18-5553-4078-aca0-d726c254ab70",
        "metadata": {
          "vector_store_key": "1810.09774-9",
          "chunk_id": 3,
          "document_id": "1810.09774",
          "start_idx": 1229,
          "end_idx": 1716
        },
        "page_content": " in accuracy was the system by BIBREF2 which utilizes external knowledge taken from WordNet BIBREF3 . In this paper we show that NLI systems that have been very successful in specific NLI benchmarks, fail to generalize when trained on a specific NLI dataset and then these trained models are tested across test sets taken from different NLI benchmarks. The results we get are in line with BIBREF1 , showing that the generalization capability of the individual NLI systems is very limited",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which models were compared?",
    "llm_answer": "Final model \"Ours with Mask and Ordered Triplets,\" Baseline, and Ablation models;  MEED;  and various RF models (with and without reduced features) and an aggregated forecaster (Agg).\n",
    "context": [
      {
        "id": "dc50a5cf-b755-41a0-a173-1b561db0ed05",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "08676cf7-5cf4-4a2c-b9d7-ddbe513c2062",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 66,
          "document_id": "1810.00663",
          "start_idx": 25415,
          "end_idx": 25983
        },
        "page_content": " In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models. The previous section evaluated model performance on new instructions (and corresponding navigation plans) for environments that were previously seen at training time. Here, we examine whether the trained models succeed on environments that are completely new. The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as",
        "type": "Document"
      },
      {
        "id": "28e8d703-f3c3-4cae-a595-da4bfcad4522",
        "metadata": {
          "vector_store_key": "1908.07816-4",
          "chunk_id": 58,
          "document_id": "1908.07816",
          "start_idx": 20270,
          "end_idx": 20712
        },
        "page_content": " the perplexity scores obtained by the three models on the two validation sets and the test set. As shown in the table, MEED achieves the lowest perplexity score on all three sets. We also conducted t-test on the perplexity obtained, and results show significant improvements (with $p$-value $<0.05$). Table TABREF34, TABREF35 and TABREF35 summarize the human evaluation results on the responses' grammatical correctness, contextual coherence",
        "type": "Document"
      },
      {
        "id": "11d9b2e6-e20d-4ab9-8d7c-c35a7e2d27e6",
        "metadata": {
          "vector_store_key": "1603.07044-6",
          "chunk_id": 43,
          "document_id": "1603.07044",
          "start_idx": 14997,
          "end_idx": 15515
        },
        "page_content": " task A already had a substantial amount of training data. Table 4 gives the final comparison between different models (we only list the MAP score because it is the official score for the challenge). Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited",
        "type": "Document"
      },
      {
        "id": "4eac1735-025c-4156-adb1-6423efb4e079",
        "metadata": {
          "vector_store_key": "1910.12618-9",
          "chunk_id": 61,
          "document_id": "1910.12618",
          "start_idx": 24043,
          "end_idx": 24512
        },
        "page_content": " the gradient descent in the NNs for instance. In order to tone it down and to increase the consistency of our results, the different models are run $B=10$ times. The results presented hereafter correspond to the average and standard-deviation on those runs. The RF model denoted as \"sel\" is the one with the reduced number of features, whereas the other RF uses the full vocabulary. We also considered an aggregated forecaster (abridged Agg), consisting of the average",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is private dashboard?",
    "llm_answer": "Answer not found in the context.\n",
    "context": [
      {
        "id": "c0f48f9f-4c03-41f2-8ee0-8f6475dbdc56",
        "metadata": {
          "vector_store_key": "1909.02764-6",
          "chunk_id": 27,
          "document_id": "1909.02764",
          "start_idx": 10937,
          "end_idx": 11442
        },
        "page_content": " in this paper) and questionnaires. Two RGB cameras are fixed in the vehicle to capture the drivers face, one at the sun shield above the drivers seat and one in the middle of the dashboard. A microphone is placed on the center console. One experimenter sits next to the driver, the other behind the simulator. The virtual agent accompanying the drive is realized as Wizard-of-Oz prototype which enables the experimenter to manually trigger prerecorded voice samples playing trough the in-car speakers and",
        "type": "Document"
      },
      {
        "id": "1a99cfaf-b7f5-4592-8b2f-86a20331b0d2",
        "metadata": {
          "vector_store_key": "1909.02764-3",
          "chunk_id": 32,
          "document_id": "1909.02764",
          "start_idx": 12958,
          "end_idx": 13486
        },
        "page_content": " drive, where participants are asked to describe their mood and thoughts regarding the (upcoming) drive. This results in five interactions between the driver and the virtual agent. Furthermore, the co-driver asks three different questions during sessions with light traffic and low cognitive demand (Driver-Co-Driver Interactions, D\u2013Co). These questions are more general and non-traffic-related and aim at triggering the participants' memory and fantasy. Participants are asked to describe their last vacation, their dream house",
        "type": "Document"
      },
      {
        "id": "0aeaa416-672d-4feb-b638-ca0f459cc4eb",
        "metadata": {
          "vector_store_key": "1901.04899-4",
          "chunk_id": 3,
          "document_id": "1901.04899",
          "start_idx": 1293,
          "end_idx": 1671
        },
        "page_content": " identified and annotated as: Set/Change Destination, Set/Change Route (including turn-by-turn instructions), Go Faster, Go Slower, Stop, Park, Pull Over, Drop Off, Open Door, and Other (turn music/radio on/off, open/close window/trunk, change AC/temp, show map, etc.). Relevant slots are identified and annotated as: Location, Position/Direction, Object, Time-Guidance, Person,",
        "type": "Document"
      },
      {
        "id": "b8fd882e-992b-4dc0-962b-6e98b5b00744",
        "metadata": {
          "vector_store_key": "2002.11893-2",
          "chunk_id": 37,
          "document_id": "2002.11893",
          "start_idx": 13977,
          "end_idx": 14497
        },
        "page_content": " online. On the website, workers are free to choose one of the two roles: tourist (user) or system (wizard). Then, two paired workers are sent to a chatroom. The user needs to accomplish the allocated goal through conversation while the wizard searches the database to provide the necessary information and gives responses. Before the formal data collection, we trained the workers to complete a small number of dialogues by giving them feedback. Finally, 90 well-trained workers are participating in the data collection",
        "type": "Document"
      },
      {
        "id": "9d80ec7c-c0b1-49d8-acb0-4f24defa5b65",
        "metadata": {
          "vector_store_key": "1901.04899-3",
          "chunk_id": 0,
          "document_id": "1901.04899",
          "start_idx": 0,
          "end_idx": 573
        },
        "page_content": "Understanding passenger intents and extracting relevant slots are important building blocks towards developing a contextual dialogue system responsible for handling certain vehicle-passenger interactions in autonomous vehicles (AV). When the passengers give instructions to AMIE (Automated-vehicle Multimodal In-cabin Experience), the agent should parse such commands properly and trigger the appropriate functionality of the AV system. In our AMIE scenarios, we describe usages and support various natural commands for interacting with the vehicle. We collected a multimod",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is public dashboard?",
    "llm_answer": "Answer not found in the context.\n",
    "context": [
      {
        "id": "b8fd882e-992b-4dc0-962b-6e98b5b00744",
        "metadata": {
          "vector_store_key": "2002.11893-2",
          "chunk_id": 37,
          "document_id": "2002.11893",
          "start_idx": 13977,
          "end_idx": 14497
        },
        "page_content": " online. On the website, workers are free to choose one of the two roles: tourist (user) or system (wizard). Then, two paired workers are sent to a chatroom. The user needs to accomplish the allocated goal through conversation while the wizard searches the database to provide the necessary information and gives responses. Before the formal data collection, we trained the workers to complete a small number of dialogues by giving them feedback. Finally, 90 well-trained workers are participating in the data collection",
        "type": "Document"
      },
      {
        "id": "c0f48f9f-4c03-41f2-8ee0-8f6475dbdc56",
        "metadata": {
          "vector_store_key": "1909.02764-6",
          "chunk_id": 27,
          "document_id": "1909.02764",
          "start_idx": 10937,
          "end_idx": 11442
        },
        "page_content": " in this paper) and questionnaires. Two RGB cameras are fixed in the vehicle to capture the drivers face, one at the sun shield above the drivers seat and one in the middle of the dashboard. A microphone is placed on the center console. One experimenter sits next to the driver, the other behind the simulator. The virtual agent accompanying the drive is realized as Wizard-of-Oz prototype which enables the experimenter to manually trigger prerecorded voice samples playing trough the in-car speakers and",
        "type": "Document"
      },
      {
        "id": "0aeaa416-672d-4feb-b638-ca0f459cc4eb",
        "metadata": {
          "vector_store_key": "1901.04899-4",
          "chunk_id": 3,
          "document_id": "1901.04899",
          "start_idx": 1293,
          "end_idx": 1671
        },
        "page_content": " identified and annotated as: Set/Change Destination, Set/Change Route (including turn-by-turn instructions), Go Faster, Go Slower, Stop, Park, Pull Over, Drop Off, Open Door, and Other (turn music/radio on/off, open/close window/trunk, change AC/temp, show map, etc.). Relevant slots are identified and annotated as: Location, Position/Direction, Object, Time-Guidance, Person,",
        "type": "Document"
      },
      {
        "id": "913be39a-9737-408f-bb77-0f14d82109ef",
        "metadata": {
          "vector_store_key": "1708.05873-2",
          "chunk_id": 6,
          "document_id": "1708.05873",
          "start_idx": 2700,
          "end_idx": 3184
        },
        "page_content": " GD is usually the first item for each new session of the UNGA, and as such it provides a forum for governments to identify like-minded members, and to put on the record the issues they feel the UNGA should address. Therefore, the GD can be viewed as a key forum for governments to put different policy issues on international agenda. We use a new dataset of GD statements from 1970 to 2016, the UN General Debate Corpus (UNGDC), to examine the international development agenda in the",
        "type": "Document"
      },
      {
        "id": "c374edfb-0088-4960-9056-48662538cbf3",
        "metadata": {
          "vector_store_key": "1705.09665-6",
          "chunk_id": 29,
          "document_id": "1705.09665",
          "start_idx": 11974,
          "end_idx": 12541
        },
        "page_content": " We now explain how our typology can be applied to the particular setting of Reddit, and describe the overall behaviour of our linguistic axes in this context. Dataset description. Reddit is a popular website where users form and participate in discussion-based communities called subreddits. Within these communities, users post content\u2014such as images, URLs, or questions\u2014which often spark vibrant lengthy discussions in thread-based comment sections. The website contains many highly active subreddits with thousands of active subscribers. These communities span an",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What dataset do they use?",
    "llm_answer": "StackOverflow, Biomedical, UltraSuite, AIDA-Train and Wikipedia.\n",
    "context": [
      {
        "id": "1c380a0c-1037-4628-8ada-168d9c8247bc",
        "metadata": {
          "vector_store_key": "1707.03904-9",
          "chunk_id": 13,
          "document_id": "1707.03904",
          "start_idx": 5304,
          "end_idx": 5850
        },
        "page_content": " subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short. We plan to release these annotations along with the datasets, and our retrieved documents for each question. Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated",
        "type": "Document"
      },
      {
        "id": "1eb879ee-df56-4263-afd5-19bb5ae493ff",
        "metadata": {
          "vector_store_key": "1701.00185-9",
          "chunk_id": 51,
          "document_id": "1701.00185",
          "start_idx": 18784,
          "end_idx": 19212
        },
        "page_content": " BIBREF41 . StackOverflow. We use the challenge data published in Kaggle.com. The raw dataset consists 3,370,528 samples through July 31st, 2012 to August 14, 2012. In our experiments, we randomly select 20,000 question titles from 20 different tags as in Table TABREF25 . Biomedical. We use the challenge data published in BioASQ's official website. In our experiments, we randomly select 20, 000 paper titles from 20 different",
        "type": "Document"
      },
      {
        "id": "3aa1cf3a-828c-47e8-b3b8-f674251a240b",
        "metadata": {
          "vector_store_key": "1907.00758-2",
          "chunk_id": 23,
          "document_id": "1907.00758",
          "start_idx": 9034,
          "end_idx": 9578
        },
        "page_content": " distance for each across utterance segments, and select the one with the minimum average distance. The candidate set is independent of the model, and is chosen based on task knowledge (Section SECREF5 ). For our experiments, we select a dataset whose utterances have been correctly synchronised at recording time. This allows us to control how the model is trained and verify its performance using ground truth synchronisation offsets. We use UltraSuite: a repository of ultrasound and acoustic data gathered from child speech therapy sessions",
        "type": "Document"
      },
      {
        "id": "04a66d23-bc0a-4d01-adba-8eeeb15d6b5d",
        "metadata": {
          "vector_store_key": "1907.00758-8",
          "chunk_id": 24,
          "document_id": "1907.00758",
          "start_idx": 9356,
          "end_idx": 9842
        },
        "page_content": ". We use UltraSuite: a repository of ultrasound and acoustic data gathered from child speech therapy sessions BIBREF15 . We used all three datasets from the repository: UXTD (recorded with typically developing children), and UXSSD and UPX (recorded with children with speech sound disorders). In total, the dataset contains 13,815 spoken utterances from 86 speakers, corresponding to 35.9 hours of recordings. The utterances have been categorised by the type of task the child was given",
        "type": "Document"
      },
      {
        "id": "1f337671-9234-400e-b57d-968d7d0216e0",
        "metadata": {
          "vector_store_key": "1902.00330-6",
          "chunk_id": 65,
          "document_id": "1902.00330",
          "start_idx": 20691,
          "end_idx": 21180
        },
        "page_content": " AIDA-Train and Wikipedia data in the training set. Furthermore, we compare the RLEL with some baseline methods, where our model achieves the state-of-the-art results. We implement our models in Tensorflow and run experiments on 4 Tesla V100 GPU. We conduct experiments on several different types of public datasets including news and encyclopedia corpus. The training set is AIDA-Train and Wikipedia datasets, where AIDA-Train contains 18448 mentions and Wikipedia contains 25995 mentions",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Do the use word embeddings alone or they replace some previous features of the model with word embeddings?",
    "llm_answer": "The context describes the use of word embeddings *in addition* to other features (character embeddings), not as a replacement.\n",
    "context": [
      {
        "id": "f6fc9332-5228-477d-92d3-0ccfe5f9d68f",
        "metadata": {
          "vector_store_key": "1909.08859-8",
          "chunk_id": 17,
          "document_id": "1909.08859",
          "start_idx": 6882,
          "end_idx": 7285
        },
        "page_content": " embedding layer uses a convolutional neural network, namely CharCNN model by BIBREF7, which outputs character level embeddings for each word and alleviates the issue of out-of-vocabulary (OOV) words. In word embedding layer, we use a pretrained GloVe model BIBREF8 and extract word-level embeddings. The concatenation of the character and the word embeddings are then fed to a two-layer highway network",
        "type": "Document"
      },
      {
        "id": "15c054b0-6e1a-4bc5-899f-ea509177dd1a",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 34,
          "document_id": "1910.12618",
          "start_idx": 13663,
          "end_idx": 14094
        },
        "page_content": " articles for example). The two most popular embeddings are probably Google's Word2Vec BIBREF19 and Standford's GloVe BIBREF20. In the former, a neural network is trained to predict a word given its context (continuous bag of word model), whereas in the latter a matrix factorization scheme on the log co-occurences of words is applied. In any case, the very nature of the objective function allows the embedding models to learn to",
        "type": "Document"
      },
      {
        "id": "11179f33-32a3-4130-b163-c0b70fe8dfcb",
        "metadata": {
          "vector_store_key": "1910.14497-8",
          "chunk_id": 26,
          "document_id": "1910.14497",
          "start_idx": 9536,
          "end_idx": 10043
        },
        "page_content": "000 words are used in all embeddings, though preliminary results indicate the findings extend to the full corpus. For our novel methods of mitigating bias, a shallow neural network is used to adjust the embedding. The single layer of the model is an embedding layer with weights initialized to those of the original embedding. For the composite method, these weights are initialized to those of the embedding after probabilistic bias mitigation. A batch of word indices is fed into the model, which are then",
        "type": "Document"
      },
      {
        "id": "baacad85-d735-4b9e-9f51-eb1c8f3a3c3a",
        "metadata": {
          "vector_store_key": "1809.10644-2",
          "chunk_id": 36,
          "document_id": "1809.10644",
          "start_idx": 13665,
          "end_idx": 14179
        },
        "page_content": " information about the user. For example, saying something in all CAPS is a useful signal that the model can take advantage of. Since our method was a simple word embedding based model, we explored the learned embedding space to analyze results. For this analysis, we only use the max pooling part of our architecture to help analyze the learned embedding space because it encourages salient words to increase their values to be selected. We projected the original pre-trained embeddings to the learned space using",
        "type": "Document"
      },
      {
        "id": "3bb4ec56-631c-4f73-8635-0f211f568905",
        "metadata": {
          "vector_store_key": "2003.06279-3",
          "chunk_id": 18,
          "document_id": "2003.06279",
          "start_idx": 7372,
          "end_idx": 7852
        },
        "page_content": " via embeddings information modify the underlying structure of networks and, most importantly, how it can improve the model to provide improved classification performance in the stylometry task. To represent texts as networks, we used the so-called word adjacency network representation BIBREF35, BIBREF28, BIBREF32. Typically, before creating the networks, the text is pre-processed. An optional pre-processing step is the removal of stopwords. This step is optional because such",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many natural language explanations are human-written?",
    "llm_answer": "100\n",
    "context": [
      {
        "id": "04a7c820-4b07-4645-8083-c6796ebf47ad",
        "metadata": {
          "vector_store_key": "2004.03744-3",
          "chunk_id": 54,
          "document_id": "2004.03744",
          "start_idx": 19461,
          "end_idx": 20038
        },
        "page_content": " explanations obtains 72.52%. This is encouraging, since it shows that one can obtain additional natural language explanations without sacrificing performance (and eventually even improving the label performance, however, future work is needed to conclude whether the difference $0.48\\%$ improvement in performance is statistically significant). Camburu BIBREF6 mentioned that the BLEU score was not an appropriate measure for the quality of explanations and suggested human evaluation instead. We therefore manually scored the relevance of 100 explanations that were generated",
        "type": "Document"
      },
      {
        "id": "7dc570ab-3e7f-45ea-8639-7a1e58bda7c6",
        "metadata": {
          "vector_store_key": "1807.03367-4",
          "chunk_id": 3,
          "document_id": "1807.03367",
          "start_idx": 1227,
          "end_idx": 1800
        },
        "page_content": " for natural language understanding, such as in machine translation and reading comprehension, have progressed at an unprecedented rate in recent years, but still rely on static, large-scale, text-only datasets that lack crucial aspects of how humans understand and produce natural language. Namely, humans develop language capabilities by being embodied in an environment which they can perceive, manipulate and move around in; and by interacting with other humans. Hence, we argue that we should incorporate all three fundamental aspects of human language acquisition\u2014per",
        "type": "Document"
      },
      {
        "id": "8e92ccf8-5454-4527-9ca9-4993ca3ab76a",
        "metadata": {
          "vector_store_key": "1808.09409-5",
          "chunk_id": 14,
          "document_id": "1808.09409",
          "start_idx": 5300,
          "end_idx": 5801
        },
        "page_content": "1) ruling out redundant content, (2) excluding sentences containing foreign words or Chinese phonetic alphabet by checking the Unicode values, (3) dropping overly simple sentences which may not be informative, and (4) utilizing a rule-based classifier to determine whether to include the sentence into the corpus. The final corpus consists of 717,241 learner sentences from writers of 61 different native languages, in which English and Japanese constitute the majority. As for completeness, 82.78% of",
        "type": "Document"
      },
      {
        "id": "6aefbbc5-ede4-49b7-91c7-5e40f10f6df0",
        "metadata": {
          "vector_store_key": "1809.04686-0",
          "chunk_id": 24,
          "document_id": "1809.04686",
          "start_idx": 8648,
          "end_idx": 9144
        },
        "page_content": " largest datasets for a natural language inference task in English and contains multiple sentence pairs with a sentence-level entailment label. Each pair of sentences can have one of three labels - entailment, contradiction, and neutral, which are annotated by multiple humans. The dataset contains 550K training, 10K validation, and 10K testing examples. To enable research on multilingual SNLI, BIBREF40 chose a subset of the SNLI test set (1332 sentences) and professionally translated it into",
        "type": "Document"
      },
      {
        "id": "fc5919ca-0cbc-4f15-ae37-e5ddad3c0b78",
        "metadata": {
          "vector_store_key": "1910.02789-8",
          "chunk_id": 4,
          "document_id": "1910.02789",
          "start_idx": 1610,
          "end_idx": 2211
        },
        "page_content": " summarize and transfer information efficiently from one to the other BIBREF5. The ability to associate states with natural language sentences that describe them is a hallmark of understanding representations for reinforcement learning. Humans use rich natural language to describe and communicate their visual perceptions, feelings, beliefs, strategies, and more. The semantics inherent to natural language carry knowledge and cues of complex types of content, including: events, spatial relations, temporal relations, semantic roles, logical structures, support for inference and entailment, as well",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the dataset used as input to the Word2Vec algorithm?",
    "llm_answer": "100 billion words from Google and Twitter, aggregated global word-word co-occurrence statistics from a corpus, Wikipedia dumps, the whole corpus of the StackOverflow dataset (including question titles and post contents), all titles and abstracts of Biomedical datasets, and BookCorpus datasets.\n",
    "context": [
      {
        "id": "fdfc34d9-5cdc-4261-8de1-a91b5d3e181c",
        "metadata": {
          "vector_store_key": "1910.11471-3",
          "chunk_id": 18,
          "document_id": "1910.11471",
          "start_idx": 7565,
          "end_idx": 8068
        },
        "page_content": " data, the code is written in Python programming language. To train the neural model, the texts should be converted to a computational entity. To do that, two separate vocabulary files are created - one for the source texts and another for the code. Vocabulary generation is done by tokenization of words. Afterwards, the words are put into their contextual vector space using the popular word2vec BIBREF10 method to make the words computational. In order to train the translation model between text-to-",
        "type": "Document"
      },
      {
        "id": "830432cf-0e24-4bc0-ba12-2eb0702d5765",
        "metadata": {
          "vector_store_key": "1706.08032-3",
          "chunk_id": 39,
          "document_id": "1706.08032",
          "start_idx": 14661,
          "end_idx": 15094
        },
        "page_content": " momentum of 0.9. Mini-batch size for STS Corpus is 100 and others are 4. In addition, training is done through stochastic gradient descent over shuffled mini-batches with Adadelta update rule BIBREF19 . we use the publicly available Word2Vec trained from 100 billion words from Google and TwitterGlove of Stanford is performed on aggregated global word-word co-occurrence statistics from a corpus. Word2Vec has dimensionality of 300",
        "type": "Document"
      },
      {
        "id": "a2220b6a-dcde-4031-9be7-f8c8f50fd007",
        "metadata": {
          "vector_store_key": "2003.06279-3",
          "chunk_id": 28,
          "document_id": "2003.06279",
          "start_idx": 11324,
          "end_idx": 11766
        },
        "page_content": " each word follows the reference word are obtained. Once the neural network is trained, the weights of the hidden layer are used as vectors of each corpus word. FastText: this method is another extension of the Word2Vec model BIBREF41. Unlike Word2Vec, FastText represents each word as a bag of character n-grams. Therefore, the neural network not only trains individual words, but also several n-grams of such words. The vector for a word is",
        "type": "Document"
      },
      {
        "id": "82047783-0d08-4d2b-a54b-4398abbdb1cb",
        "metadata": {
          "vector_store_key": "1701.00185-4",
          "chunk_id": 54,
          "document_id": "1701.00185",
          "start_idx": 19748,
          "end_idx": 20221
        },
        "page_content": " and the most parameters are set as same as Mikolov et al. BIBREF23 to train word vectors on Google News setting, except of vector dimensionality using 48 and minimize count using 5. For SearchSnippets, we train word vectors on Wikipedia dumps. For StackOverflow, we train word vectors on the whole corpus of the StackOverflow dataset described above which includes the question titles and post contents. For Biomedical, we train word vectors on all titles and abstracts of",
        "type": "Document"
      },
      {
        "id": "9432e055-6313-47e7-95dd-e58400dcf159",
        "metadata": {
          "vector_store_key": "1701.00185-4",
          "chunk_id": 78,
          "document_id": "1701.00185",
          "start_idx": 27988,
          "end_idx": 28389
        },
        "page_content": " off-the-shelf encoders are trained on the BookCorpus datasets BIBREF51 , and then applied to our datasets to extract the sentence representations. The SkipVec encoders can produce generic sentence representations but may not perform well for specific datasets, in our experiments, StackOverflow and Biomedical datasets consist of many computer terms and medical terms, such as \u201cASP.NET\u201d, \u201cXML\u201d, \u201cC#\u201d,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What methodology is used to compensate for limited labelled data?",
    "llm_answer": "Lightly supervised learning, self-training, and distantly-supervised learning.\n",
    "context": [
      {
        "id": "bef09354-c66c-4cb3-aa39-172d0ef2a56d",
        "metadata": {
          "vector_store_key": "1908.06606-9",
          "chunk_id": 2,
          "document_id": "1908.06606",
          "start_idx": 806,
          "end_idx": 1288
        },
        "page_content": " often have non-uniform output formats, such as specific-class classifications (e.g. tumor stage), strings in the original text (e.g. result for a laboratory test) and inferred values from part of the original text (e.g. calculated tumor size). Researchers have to construct different models for it, which is already costly, and hence it calls for a lot of labeled data for each model. Moreover, labeling necessary amount of data for training neural network requires expensive labor",
        "type": "Document"
      },
      {
        "id": "a52aeef2-2538-4a7c-b229-b7b55983246a",
        "metadata": {
          "vector_store_key": "1909.00124-0",
          "chunk_id": 12,
          "document_id": "1909.00124",
          "start_idx": 4084,
          "end_idx": 4496
        },
        "page_content": " or users' information (we only use single-labeled data, and we do not use any additional information), they have different settings than ours. Our work is closely related to DNNs based approaches to learning with noisy labels. DNNs based approaches explored three main directions: (1) training DNNs on selected samples BIBREF33, BIBREF34, BIBREF35, BIBREF17, (2) modifying the loss function of DNNs with regular",
        "type": "Document"
      },
      {
        "id": "78171176-4041-40f2-a8dc-94a9829642e4",
        "metadata": {
          "vector_store_key": "1909.00430-8",
          "chunk_id": 9,
          "document_id": "1909.00430",
          "start_idx": 3451,
          "end_idx": 3911
        },
        "page_content": " SECREF19 ). Source code is available at https://github.com/MatanBN/XRTransfer. An effective way to supplement small annotated datasets is to use lightly supervised learning, in which the objective function is supplemented by a set of domain-specific soft-constraints over the model's predictions on unlabeled data. Previous work in lightly-supervised learning focused on training classifiers by using prior knowledge of label proportions BIBREF2 , BIBREF3 , B",
        "type": "Document"
      },
      {
        "id": "337dbf0b-4024-42e2-8575-fa8615a5722a",
        "metadata": {
          "vector_store_key": "1710.01492-5",
          "chunk_id": 56,
          "document_id": "1710.01492",
          "start_idx": 20569,
          "end_idx": 20967
        },
        "page_content": " the network, e.g., with convolution or with LSTMs BIBREF55 , BIBREF54 , BIBREF56 , need to learn something like a language model, i.e., which word is likely to follow which one. Training these parts needs no labels. While these parts can be also pre-trained, it is easier, and often better, to use self training. Distantly-supervised learning. Another way to make use of large unannotated datasets",
        "type": "Document"
      },
      {
        "id": "e8008d61-c81e-4ad1-8941-345e1751e73f",
        "metadata": {
          "vector_store_key": "1701.00185-5",
          "chunk_id": 12,
          "document_id": "1701.00185",
          "start_idx": 4615,
          "end_idx": 5118
        },
        "page_content": " following two perspectives: short text clustering and deep neural networks. There have been several studies that attempted to overcome the sparseness of short text representation. One way is to expand and enrich the context of data. For example, Banerjee et al. BIBREF3 proposed a method of improving the accuracy of short text clustering by enriching their representation with additional features from Wikipedia, and Fodeh et al. BIBREF4 incorporate semantic knowledge from an ontology into text clust",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the baseline state of the art models?",
    "llm_answer": "BiMPM, ESIM, Decomposable Attention Model, KIM, and BERT.\n",
    "context": [
      {
        "id": "577fb06e-9340-4422-93f3-91bc89574aee",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 43,
          "document_id": "2001.00137",
          "start_idx": 16781,
          "end_idx": 17197
        },
        "page_content": " the online platforms. Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector",
        "type": "Document"
      },
      {
        "id": "7b6a405f-eb7a-4d59-8247-20fd3864055f",
        "metadata": {
          "vector_store_key": "1906.06448-5",
          "chunk_id": 47,
          "document_id": "1906.06448",
          "start_idx": 17189,
          "end_idx": 17518
        },
        "page_content": " five state-of-the-art NLI models adopting different approaches: BiMPM (Bilateral Multi-Perspective Matching Model; BIBREF31 , BIBREF31 ), ESIM (Enhanced Sequential Inference Model; BIBREF32 , BIBREF32 ), Decomposable Attention Model BIBREF33 , KIM (Knowledge-based Inference Model; BIBREF34 , BIBREF34 ), and BERT (Bidirectional",
        "type": "Document"
      },
      {
        "id": "bc8ac242-0b5e-4c71-be0b-48d167015fad",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 43,
          "document_id": "1909.02764",
          "start_idx": 17421,
          "end_idx": 17831
        },
        "page_content": " layer. This setup is inspired by BIBREF38. We use a dropout rate of 0.3 in all layers and optimize with Adam BIBREF39 with a learning rate of $10^{-5}$ (These parameters are the same for all further experiments). We build on top of the Keras library with the TensorFlow backend. We consider this setup our baseline model. We train models on a variety of corpora, namely the common format published by BIBREF27",
        "type": "Document"
      },
      {
        "id": "023a760d-81e9-48e4-8692-88260707e423",
        "metadata": {
          "vector_store_key": "2002.11402-1",
          "chunk_id": 24,
          "document_id": "2002.11402",
          "start_idx": 8819,
          "end_idx": 9288
        },
        "page_content": "es other models in all scores. Spacy results are good despite not being trained for case-less data. In terms of F1 and overall stability Spacy did better than Stanford NER, on our News Validation set. Similarly, Stanford did well in Precision but could not catch up with Spacy and our model in terms of Recall. Flair overall performed poorly, but as said before these open-source models are not trained for our particular use-case. Lets check some examples for detailed",
        "type": "Document"
      },
      {
        "id": "555a266e-0a08-43e5-b719-8cd2fd10ebb8",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they extract causality from text?",
    "llm_answer": "Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'.\n",
    "context": [
      {
        "id": "9c5f2f91-f2bf-4778-8ee5-d77a93cac444",
        "metadata": {
          "vector_store_key": "1604.05781-8",
          "chunk_id": 13,
          "document_id": "1604.05781",
          "start_idx": 5120,
          "end_idx": 5547
        },
        "page_content": " not performed. Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'. The word `cause' was not included due to its use as a popular contraction for `because'. One `cause-word' per document restricted the analysis to single relationships between two relata. Documents that contain bidirectional words (`associate', `relate', `connect', `correlate', and any of their",
        "type": "Document"
      },
      {
        "id": "6ce728ca-8b2c-43af-bb28-a85e1b4dfbeb",
        "metadata": {
          "vector_store_key": "1604.05781-0",
          "chunk_id": 6,
          "document_id": "1604.05781",
          "start_idx": 2254,
          "end_idx": 2805
        },
        "page_content": "REF27 . Despite these long lines of research, causal attributions made via social media or online social networks have not been well studied. The goal of this paper is to explore the language and topics of causal statements in a large corpus of social media taken from Twitter. We hypothesize that language and sentiment biases play a significant role in these statements, and that tools from natural language processing and computational linguistics can be used to study them. We do not attempt to study the factual correctness of these statements or",
        "type": "Document"
      },
      {
        "id": "00fc47dc-6959-4ba5-8da7-972c75b9f96f",
        "metadata": {
          "vector_store_key": "1604.05781-0",
          "chunk_id": 7,
          "document_id": "1604.05781",
          "start_idx": 2805,
          "end_idx": 3363
        },
        "page_content": " can be used to study them. We do not attempt to study the factual correctness of these statements or offer any degree of verification, nor do we exhaustively identify and extract all causal statements from these data. Instead, here we focus on statements that are with high certainty causal statements, with the goal to better understand key characteristics about causal statements that differ from everyday online communication. The rest of this paper is organized as follows: In Sec. \"Materials and Methods\" we discuss our materials and methods, including",
        "type": "Document"
      },
      {
        "id": "5de0eb9f-1544-4ff0-9a90-f05dd384f281",
        "metadata": {
          "vector_store_key": "1909.08859-0",
          "chunk_id": 1,
          "document_id": "1909.08859",
          "start_idx": 475,
          "end_idx": 936
        },
        "page_content": " is, one must be aware of the entities present in the text, infer relations among them and even anticipate changes in the states of the entities after each action. For example, consider the cheeseburger recipe presented in Fig. FIGREF2. The instruction \u201csalt and pepper each patty and cook for 2 to 3 minutes on the first side\u201d in Step 5 entails mixing three basic ingredients, the ground beef, salt and pepper, together and then applying heat to the mix, which",
        "type": "Document"
      },
      {
        "id": "c54fc9c0-1b13-432b-9e78-6f92f600f70f",
        "metadata": {
          "vector_store_key": "1604.05781-0",
          "chunk_id": 71,
          "document_id": "1604.05781",
          "start_idx": 25272,
          "end_idx": 25869
        },
        "page_content": " causal statements are made? Our work here focused on a limited subset of causal statements, but more generally, these results may inform new methods for automatically detecting causal statements from unstructured, natural language text BIBREF17 . Better computational tools focused on causal statements are an important step towards further understanding misinformation campaigns and other online activities. Lastly, an important but deeply challenging open question is how, if it is even possible, to validate the accuracy of causal statements. Can causal statements be ranked by some confidence",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the source of the \"control\" corpus?",
    "llm_answer": "Control documents were selected randomly, matched temporally to the causal documents, and did not contain specific words or bidirectional words.\n",
    "context": [
      {
        "id": "a24ffc65-584b-45d5-8dfb-81c335b6cc82",
        "metadata": {
          "vector_store_key": "1604.05781-6",
          "chunk_id": 16,
          "document_id": "1604.05781",
          "start_idx": 6146,
          "end_idx": 6498
        },
        "page_content": "560 control documents. Each of the three \u201ccause-words\u201d, `caused', `causes', and `causing' appeared in 38.2%, 35.0%, and 26.8% of causal documents, respectively. Documents were further studied by annotating their unigrams with Parts-of-Speech (POS) and Named Entities (NE) tags. POS tagging was done using NLTK v3.1 BIBREF29 which implements an averaged",
        "type": "Document"
      },
      {
        "id": "fd295212-42bb-4dfb-949c-1921b96a606f",
        "metadata": {
          "vector_store_key": "1604.05781-8",
          "chunk_id": 21,
          "document_id": "1604.05781",
          "start_idx": 7823,
          "end_idx": 8087
        },
        "page_content": "))}{p_N(x) / (1-p_N(x))},$$   (Eq. 1)   where $p_C(x)$ and $p_N(x)$ are the probabilities that a unigram, POS, or NE $x$ occurs in the causal and control corpus, respectively. These probabilities were computed for each corpus separately as $p(x) = f(x) / \\sum _{x^",
        "type": "Document"
      },
      {
        "id": "425bc893-08e9-4be5-a0d9-da3a54b2078f",
        "metadata": {
          "vector_store_key": "1604.05781-8",
          "chunk_id": 8,
          "document_id": "1604.05781",
          "start_idx": 3160,
          "end_idx": 3653
        },
        "page_content": " organized as follows: In Sec. \"Materials and Methods\" we discuss our materials and methods, including the dataset we studied, how we preprocessed that data and extracted a `causal' corpus and a corresponding `control' corpus, and the details of the statistical and language analysis tools we studied these corpora with. In Sec. \"Results\" we present results using these tools to compare the causal statements to control statements. We conclude with a discussion in Sec. \"Discussion\" . Data was",
        "type": "Document"
      },
      {
        "id": "8270f85e-7747-4bf2-b632-cf277c50cb48",
        "metadata": {
          "vector_store_key": "1910.11949-4",
          "chunk_id": 7,
          "document_id": "1910.11949",
          "start_idx": 2984,
          "end_idx": 3467
        },
        "page_content": "REF8 by Joseph Weizenbaum at MIT. Its implementation consisted in pattern matching and substitution methodology. Recently, data driven approaches have drawn significant attention. Existing work along this line includes retrieval-based methods BIBREF9BIBREF10 and generation-based methodsBIBREF11BIBREF12. In this work we focus on generative models, where sequence-to-sequence algorithm that uses RNNs to encode and decode inputs into responses is a current best practice. Our convers",
        "type": "Document"
      },
      {
        "id": "2aa37164-5538-442f-9e79-5de8d3fdd932",
        "metadata": {
          "vector_store_key": "1604.05781-8",
          "chunk_id": 15,
          "document_id": "1604.05781",
          "start_idx": 5832,
          "end_idx": 6276
        },
        "page_content": " not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present. The end result of this procedure identified 965,560 causal and 965,560 control documents. Each of the three \u201ccause-words\u201d, `caused',",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the selection criteria for \"causal statements\"?",
    "llm_answer": "Statements with high certainty of being causal statements.\n",
    "context": [
      {
        "id": "c54fc9c0-1b13-432b-9e78-6f92f600f70f",
        "metadata": {
          "vector_store_key": "1604.05781-0",
          "chunk_id": 71,
          "document_id": "1604.05781",
          "start_idx": 25272,
          "end_idx": 25869
        },
        "page_content": " causal statements are made? Our work here focused on a limited subset of causal statements, but more generally, these results may inform new methods for automatically detecting causal statements from unstructured, natural language text BIBREF17 . Better computational tools focused on causal statements are an important step towards further understanding misinformation campaigns and other online activities. Lastly, an important but deeply challenging open question is how, if it is even possible, to validate the accuracy of causal statements. Can causal statements be ranked by some confidence",
        "type": "Document"
      },
      {
        "id": "1890dfdb-f1d3-47ca-bd18-14422aa864ca",
        "metadata": {
          "vector_store_key": "1604.05781-0",
          "chunk_id": 70,
          "document_id": "1604.05781",
          "start_idx": 24966,
          "end_idx": 25505
        },
        "page_content": " causal statements from other users? The topics of causal statements also bring forth important questions to be addressed: how timely are causal statements? Are certain topics always being discussed in causal statements? Are there causal topics that are very popular for only brief periods and then forgotten? Temporal dynamics of causal statements are also interesting: do time-of-day or time-of-year factors play a role in how causal statements are made? Our work here focused on a limited subset of causal statements, but more generally",
        "type": "Document"
      },
      {
        "id": "0354c67e-d383-470a-97fb-439df54da247",
        "metadata": {
          "vector_store_key": "1604.05781-0",
          "chunk_id": 60,
          "document_id": "1604.05781",
          "start_idx": 21180,
          "end_idx": 21680
        },
        "page_content": "Figs. 1 and 2 ) and sentiment (Fig. 3 ) differences between causal statements made on social media compared with other social media statements. But what is being discussed? What are the topical foci of causal statements? To study this, for our last analysis we applied topic models to the causal statements. Topic modeling finds groups of related terms (unigrams) by considering similarities between how those terms co-occur across a set of documents. We used the popular topic modeling method Latent",
        "type": "Document"
      },
      {
        "id": "e361c034-c844-4b6f-95dc-5fbb054e8e49",
        "metadata": {
          "vector_store_key": "1604.05781-0",
          "chunk_id": 69,
          "document_id": "1604.05781",
          "start_idx": 24428,
          "end_idx": 24966
        },
        "page_content": ". Given the central role of causal statements, further studies of the interplay of information propagation and online causal attributions are crucial. Are causal statements more likely to spread online and, if so, in which ways? What types of social media users are more or less likely to make causal statements? Will a user be more likely to make a causal statement if they have recently been exposed to one or more causal statements from other users? The topics of causal statements also bring forth important questions to be addressed:",
        "type": "Document"
      },
      {
        "id": "00fc47dc-6959-4ba5-8da7-972c75b9f96f",
        "metadata": {
          "vector_store_key": "1604.05781-0",
          "chunk_id": 7,
          "document_id": "1604.05781",
          "start_idx": 2805,
          "end_idx": 3363
        },
        "page_content": " can be used to study them. We do not attempt to study the factual correctness of these statements or offer any degree of verification, nor do we exhaustively identify and extract all causal statements from these data. Instead, here we focus on statements that are with high certainty causal statements, with the goal to better understand key characteristics about causal statements that differ from everyday online communication. The rest of this paper is organized as follows: In Sec. \"Materials and Methods\" we discuss our materials and methods, including",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Do they use expert annotations, crowdsourcing, or only automatic methods to analyze the corpora?",
    "llm_answer": "Expert annotations, crowdsourcing, and automatic methods are used.\n",
    "context": [
      {
        "id": "c4166a62-accf-4d75-b270-17f4949c67ce",
        "metadata": {
          "vector_store_key": "1703.09684-6",
          "chunk_id": 26,
          "document_id": "1703.09684",
          "start_idx": 9860,
          "end_idx": 10324
        },
        "page_content": "QA and Visual Genome. Second, we created algorithms that generated questions from COCO's semantic segmentation annotations BIBREF17 , and Visual Genome's objects and attributes annotations BIBREF6 . Third, we used human annotators for certain question-types. In the following sections, we briefly describe each of these methods. We imported questions from COCO-VQA and Visual Genome belonging to all question-types except `object utilities and affordances'. We did",
        "type": "Document"
      },
      {
        "id": "529e234a-2c0f-4eee-af64-992ee8488ac4",
        "metadata": {
          "vector_store_key": "1703.09684-6",
          "chunk_id": 42,
          "document_id": "1703.09684",
          "start_idx": 16154,
          "end_idx": 16695
        },
        "page_content": "-exhaustive and duplicated, which makes using them to automatically make QA pairs difficult. We only use Visual Genome to make color and positional reasoning questions. The methods we used are similar to those used with COCO, but additional precautions were needed due to quirks in their annotations. Additional details are provided in the Appendix. Creating sentiment understanding and object utility/affordance questions cannot be readily done using templates, so we used manual annotation to create these. Twelve volunteer annotators were",
        "type": "Document"
      },
      {
        "id": "88a6f2c9-fa7c-4999-bc75-792ef40761a8",
        "metadata": {
          "vector_store_key": "1701.02877-1",
          "chunk_id": 10,
          "document_id": "1701.02877",
          "start_idx": 3620,
          "end_idx": 4066
        },
        "page_content": " primarily by F1 score), and also to study the relationship between OOV NEs and features and F1. See Section \"Experiments\" for details. To ensure representativeness and comprehensiveness, our experimental findings are based on key benchmark NER corpora spanning multiple genres, time periods, and corpus annotation methodologies and guidelines. As detailed in Section \"Datasets\" , the corpora studied are OntoNotes BIBREF29 , ACE BIBREF30 , MUC 7",
        "type": "Document"
      },
      {
        "id": "d8532faa-597d-4677-bdf8-45d435b74b9c",
        "metadata": {
          "vector_store_key": "1703.09684-6",
          "chunk_id": 31,
          "document_id": "1703.09684",
          "start_idx": 11857,
          "end_idx": 12339
        },
        "page_content": " with semantic knowledge attached to them. We exploit this information to generate new questions using question templates. To introduce variety, we define multiple templates for each question-type and use the annotations to populate them. For example, for counting we use 8 templates, e.g., `How many <objects> are there?', `How many <objects> are in the photo?', etc. Since the COCO and Visual Genome use different annotation formats, we discuss them separately. Sport recognition,",
        "type": "Document"
      },
      {
        "id": "f9e9e3ec-e213-4cd5-ae8e-8d986c9fd2db",
        "metadata": {
          "vector_store_key": "2002.11893-1",
          "chunk_id": 22,
          "document_id": "2002.11893",
          "start_idx": 8390,
          "end_idx": 8922
        },
        "page_content": " before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states. Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three",
        "type": "Document"
      }
    ]
  },
  {
    "query": "how do they collect the comparable corpus?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "4e9f1d4e-9690-473d-ba91-34edbd3b07aa",
        "metadata": {
          "vector_store_key": "1604.05781-9",
          "chunk_id": 22,
          "document_id": "1604.05781",
          "start_idx": 8014,
          "end_idx": 8330
        },
        "page_content": " corpus separately as $p(x) = f(x) / \\sum _{x^{\\prime } \\in V} f(x^{\\prime })$ , where $f(x)$ is the total number of occurrences of $x$ in the corpus and $V$ is the relevant set of unigrams, POS, or NEs. Confidence intervals for the ORs were computed using Wald's methodology BIBREF36 . As there are many unique unig",
        "type": "Document"
      },
      {
        "id": "bcda11da-68e2-42e7-abce-85740ed52515",
        "metadata": {
          "vector_store_key": "1902.00330-1",
          "chunk_id": 99,
          "document_id": "1902.00330",
          "start_idx": 33470,
          "end_idx": 33958
        },
        "page_content": " the pairs of mentions at most once, they assume each mention only needs to be consistent with one another mention in the document. The limitation of their method is that the consistency information is too sparse, resulting in low confidence. Similar to us, Guo et al. BIBREF18 also sort mentions according to the difficulty of disambiguation, but they did not make full use of the information of previously referred entities for the subsequent entity disambiguation. Nguyen et al. BIBREF",
        "type": "Document"
      },
      {
        "id": "d7e88a3a-9f2c-444d-a064-4fc4e2448cb3",
        "metadata": {
          "vector_store_key": "1701.02877-7",
          "chunk_id": 121,
          "document_id": "1701.02877",
          "start_idx": 43692,
          "end_idx": 44145
        },
        "page_content": " the CoNLL 2003 corpus. The seed domain for the experiments here is newswire, where we use the classifier trained on the biggest NW corpus investigated in this study, i.e. OntoNotes NW. That classifier is then applied to all other corpora. The rationale is to test how suitable such a big corpus would be for improving Twitter NER, for which only small training corpora are available. Results for out-of-domain performance are reported in Table 11 . The",
        "type": "Document"
      },
      {
        "id": "dc42843a-3848-4cab-ac67-e7192ece79f9",
        "metadata": {
          "vector_store_key": "1902.09087-0",
          "chunk_id": 13,
          "document_id": "1902.09087",
          "start_idx": 5293,
          "end_idx": 5820
        },
        "page_content": " from different input channels, which share parameters and map inputs to the same vector space. Then, the sentence representations are merged and compared to output the similarities. For our models, we use multi-layer CNNs for sentence representation. Residual connections BIBREF9 are used between convolutional layers to enrich features and make it easier to train. Then, max-pooling summarizes the global features to get the sentence level representations, which are merged via element-wise multiplication. The matching score",
        "type": "Document"
      },
      {
        "id": "280a977e-e326-45d6-9256-81eb5c21ed23",
        "metadata": {
          "vector_store_key": "1809.05752-4",
          "chunk_id": 22,
          "document_id": "1809.05752",
          "start_idx": 9188,
          "end_idx": 9738
        },
        "page_content": " set. The corpus consists of discharge summaries, encounter notes, and visit notes from approximately 30,000 patients admitted to the system's hospitals with psychiatric diagnoses and symptoms. This breadth of data captures a wide range of clinical narratives, creating a comprehensive foundation for topic extraction. After using the RPDR query tool to extract EHR paragraphs from the RPDR database, we created a training corpus by categorizing the extracted paragraphs according to their risk factor domain using a lexicon of 120 keywords that were",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they collect the control corpus?",
    "llm_answer": "The context describes how a \"control\" corpus was created alongside a \"causal\" corpus, but it doesn't describe how the control corpus was specifically collected.\n",
    "context": [
      {
        "id": "fd295212-42bb-4dfb-949c-1921b96a606f",
        "metadata": {
          "vector_store_key": "1604.05781-8",
          "chunk_id": 21,
          "document_id": "1604.05781",
          "start_idx": 7823,
          "end_idx": 8087
        },
        "page_content": "))}{p_N(x) / (1-p_N(x))},$$   (Eq. 1)   where $p_C(x)$ and $p_N(x)$ are the probabilities that a unigram, POS, or NE $x$ occurs in the causal and control corpus, respectively. These probabilities were computed for each corpus separately as $p(x) = f(x) / \\sum _{x^",
        "type": "Document"
      },
      {
        "id": "d7e88a3a-9f2c-444d-a064-4fc4e2448cb3",
        "metadata": {
          "vector_store_key": "1701.02877-7",
          "chunk_id": 121,
          "document_id": "1701.02877",
          "start_idx": 43692,
          "end_idx": 44145
        },
        "page_content": " the CoNLL 2003 corpus. The seed domain for the experiments here is newswire, where we use the classifier trained on the biggest NW corpus investigated in this study, i.e. OntoNotes NW. That classifier is then applied to all other corpora. The rationale is to test how suitable such a big corpus would be for improving Twitter NER, for which only small training corpora are available. Results for out-of-domain performance are reported in Table 11 . The",
        "type": "Document"
      },
      {
        "id": "280a977e-e326-45d6-9256-81eb5c21ed23",
        "metadata": {
          "vector_store_key": "1809.05752-4",
          "chunk_id": 22,
          "document_id": "1809.05752",
          "start_idx": 9188,
          "end_idx": 9738
        },
        "page_content": " set. The corpus consists of discharge summaries, encounter notes, and visit notes from approximately 30,000 patients admitted to the system's hospitals with psychiatric diagnoses and symptoms. This breadth of data captures a wide range of clinical narratives, creating a comprehensive foundation for topic extraction. After using the RPDR query tool to extract EHR paragraphs from the RPDR database, we created a training corpus by categorizing the extracted paragraphs according to their risk factor domain using a lexicon of 120 keywords that were",
        "type": "Document"
      },
      {
        "id": "2f7250eb-40f2-4bed-b674-39ea8310195d",
        "metadata": {
          "vector_store_key": "2002.02224-1",
          "chunk_id": 31,
          "document_id": "2002.02224",
          "start_idx": 12632,
          "end_idx": 13146
        },
        "page_content": " Figure FIGREF10. As the first step, every document in the CzCDC 1.0 was segmented using the text segmentation model. This allowed us to treat different parts of processed court documents differently in the further text processing. Specifically, it allowed us to subject only the specific part of a court decision, in this case the court argumentation, to further the reference recognition and extraction. A textual segment recognised as the court argumentation is then processed further. As the second step, parts",
        "type": "Document"
      },
      {
        "id": "425bc893-08e9-4be5-a0d9-da3a54b2078f",
        "metadata": {
          "vector_store_key": "1604.05781-8",
          "chunk_id": 8,
          "document_id": "1604.05781",
          "start_idx": 3160,
          "end_idx": 3653
        },
        "page_content": " organized as follows: In Sec. \"Materials and Methods\" we discuss our materials and methods, including the dataset we studied, how we preprocessed that data and extracted a `causal' corpus and a corresponding `control' corpus, and the details of the statistical and language analysis tools we studied these corpora with. In Sec. \"Results\" we present results using these tools to compare the causal statements to control statements. We conclude with a discussion in Sec. \"Discussion\" . Data was",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are resolution model variables?",
    "llm_answer": "Resolution mode variables $\\Pi = \\lbrace \\pi _1, \\ldots , \\pi _n\\rbrace $, where for each mention $j$ the variable $\\pi _j \\in \\lbrace str, prec, attr\\rbrace $ indicates in which mode the mention should be resolved.\n",
    "context": [
      {
        "id": "adfe93c1-0c6b-4ff5-a263-9e33374f62df",
        "metadata": {
          "vector_store_key": "1603.04553-3",
          "chunk_id": 15,
          "document_id": "1603.04553",
          "start_idx": 4984,
          "end_idx": 5354
        },
        "page_content": " as apposition or acronym. Motivated by this, we introduce resolution mode variables $\\Pi = \\lbrace \\pi _1, \\ldots , \\pi _n\\rbrace $ , where for each mention $j$ the variable $\\pi _j \\in \\lbrace str, prec, attr\\rbrace $ indicates in which mode the mention should be resolved. In our model, we define three resolution modes \u2014 string-matching (str), precise-construct (pre",
        "type": "Document"
      },
      {
        "id": "83ecbb67-425a-4628-ad62-21a449fbccbf",
        "metadata": {
          "vector_store_key": "1603.04553-5",
          "chunk_id": 42,
          "document_id": "1603.04553",
          "start_idx": 12601,
          "end_idx": 13098
        },
        "page_content": " differentiate more resolution modes. Another one is to add more precise or even event-based features to improve the model's performance. This research was supported in part by DARPA grant FA8750-12-2-0342 funded under the DEFT program. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA. Appendix A. Derivation of Model Learning Formally, we iteratively estimate the model parameters $\\",
        "type": "Document"
      },
      {
        "id": "e9d22107-6c10-4d03-bee9-db27d532c4ac",
        "metadata": {
          "vector_store_key": "1603.04553-5",
          "chunk_id": 59,
          "document_id": "1603.04553",
          "start_idx": 15461,
          "end_idx": 15606
        },
        "page_content": "_{j-1}, c_{j+1}, \\ldots , c_{n}\\rbrace $ . The above derivations correspond to the learning algorithm in Algorithm \"Resolution Mode Variables\" . ",
        "type": "Document"
      },
      {
        "id": "9b4a9f0e-5c4e-4b26-928c-0afd7a54ce95",
        "metadata": {
          "vector_store_key": "1603.04553-3",
          "chunk_id": 14,
          "document_id": "1603.04553",
          "start_idx": 4512,
          "end_idx": 4984
        },
        "page_content": " to be resolved. According to previous work BIBREF17 , BIBREF18 , BIBREF1 , antecedents are resolved by different categories of information for different mentions. For example, the Stanford system BIBREF1 uses string-matching sieves to link two mentions with similar text and precise-construct sieve to link two mentions which satisfy special syntactic or semantic relations such as apposition or acronym. Motivated by this, we introduce resolution mode variables $\\Pi = \\",
        "type": "Document"
      },
      {
        "id": "3403420d-7275-4475-96c4-892e57999916",
        "metadata": {
          "vector_store_key": "1910.09399-9",
          "chunk_id": 31,
          "document_id": "1910.09399",
          "start_idx": 12524,
          "end_idx": 12961
        },
        "page_content": " to become increasingly adept at its specific task until a threshold is reached. This is analogous to a min-max game between the two models, according to the following equation: In Eq. (DISPLAY_FORM10), $x$ denotes a multi-dimensional sample, e.g., an image, and $z$ denotes a multi-dimensional latent space vector, e.g., a multidimensional data point following a predefined distribution function such as that of normal distributions. $D",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is the model presented in the paper state of the art?",
    "llm_answer": "Yes.\n",
    "context": [
      {
        "id": "a1974dac-f964-4aeb-a415-a2006317d90a",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 68,
          "document_id": "1810.00663",
          "start_idx": 26280,
          "end_idx": 26763
        },
        "page_content": " room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM. Note that the average number of actions in the ground truth output sequences is 7.07 for the Test-New set. Our model's predictions are just INLINEFORM1 edits off on average from the correct navigation plans. This",
        "type": "Document"
      },
      {
        "id": "2f779d71-2179-4ca0-89f3-60ca55105db9",
        "metadata": {
          "vector_store_key": "1911.03894-1",
          "chunk_id": 63,
          "document_id": "1911.03894",
          "start_idx": 22277,
          "end_idx": 22771
        },
        "page_content": " many other languages. We will publish an updated version in the near future where we will explore and release models trained for longer, with additional downstream tasks, baselines (e.g. XLM) and analysis, we will also train additional models with potentially cleaner corpora such as CCNet BIBREF56 for more accurate performance evaluation and more complete ablation. This work was partly funded by three French National grants from the Agence Nationale de la Recherche, namely projects PARSIT",
        "type": "Document"
      },
      {
        "id": "47b152dd-73e8-40eb-a927-52d5511639b3",
        "metadata": {
          "vector_store_key": "1905.06566-2",
          "chunk_id": 89,
          "document_id": "1905.06566",
          "start_idx": 24955,
          "end_idx": 25353
        },
        "page_content": " We also conducted human experiment with 20 randomly sampled documents from the CNNDM test set. We compared our model $\\text{\\sc Hibert}_M$ against Lead3, DCA, Latent, BERT and the human reference (Human). We asked the subjects to rank the outputs of these systems from best to worst. As shown in Table 4 , the output of $\\text{\\sc Hibert}_M$ is selected as the best in 30% of cases and we obtained",
        "type": "Document"
      },
      {
        "id": "b210fc09-73ae-44c8-9eb9-75abd198f668",
        "metadata": {
          "vector_store_key": "1905.00563-0",
          "chunk_id": 2,
          "document_id": "1905.00563",
          "start_idx": 659,
          "end_idx": 1149
        },
        "page_content": " However, there are only a few studies BIBREF12 , BIBREF13 that investigate the quality of the different KG models. There is a need to go beyond just the accuracy on link prediction, and instead focus on whether these representations are robust and stable, and what facts they make use of for their predictions. In this paper, our goal is to design approaches that minimally change the graph structure such that the prediction of a target fact changes the most after the embeddings are rele",
        "type": "Document"
      },
      {
        "id": "08676cf7-5cf4-4a2c-b9d7-ddbe513c2062",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 66,
          "document_id": "1810.00663",
          "start_idx": 25415,
          "end_idx": 25983
        },
        "page_content": " In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models. The previous section evaluated model performance on new instructions (and corresponding navigation plans) for environments that were previously seen at training time. Here, we examine whether the trained models succeed on environments that are completely new. The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was the result of the highest performing system?",
    "llm_answer": "Human\n",
    "context": [
      {
        "id": "ef4233aa-38ad-4233-9de2-ee7f1ad4b9d3",
        "metadata": {
          "vector_store_key": "1905.06566-2",
          "chunk_id": 90,
          "document_id": "1905.06566",
          "start_idx": 25175,
          "end_idx": 25555
        },
        "page_content": " Hibert}_M$ is selected as the best in 30% of cases and we obtained lower mean rank than all systems except for Human. We also converted the rank numbers into ratings (rank $i$ to $7-i$ ) and applied student $t$ -test on the ratings. $\\text{\\sc Hibert}_M$ is significantly different from all systems in comparison ( $p < 0.05$ ), which indicates our model still lags behind Human,",
        "type": "Document"
      },
      {
        "id": "cbe1ae0c-a3e6-408d-b599-2f1e42c2205e",
        "metadata": {
          "vector_store_key": "1603.04553-1",
          "chunk_id": 36,
          "document_id": "1603.04553",
          "start_idx": 10414,
          "end_idx": 10861
        },
        "page_content": " baseline with two deterministic systems, namely Stanford: the Stanford system BIBREF10 and Multigraph: the unsupervised multigraph system BIBREF26 , and one unsupervised system, namely MIR: the unsupervised system using most informative relations BIBREF27 . Our model outperforms the three baseline systems on all the evaluation metrics. Specifically, our model achieves improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford system,",
        "type": "Document"
      },
      {
        "id": "d9c817c3-a211-4b55-9b45-d9c4f63cece2",
        "metadata": {
          "vector_store_key": "1701.02877-4",
          "chunk_id": 141,
          "document_id": "1701.02877",
          "start_idx": 50827,
          "end_idx": 51373
        },
        "page_content": " The other genre that was particularly difficult for the systems was ACE Usenet. This is a form of user-generated content, not intended for publication but rather discussion among communities. In this sense, it is social media, and so it is not surprising that system performance on ACE UN resembles performance on social media more than other genres. Crucially, the computationally-cheap memorisation method actually acts as a reasonable predictor of the performance of other methods. This suggests that high entity diversity predicts difficulty",
        "type": "Document"
      },
      {
        "id": "264f9077-b7c7-4b7e-a679-f6cf30b20b02",
        "metadata": {
          "vector_store_key": "1603.07044-7",
          "chunk_id": 44,
          "document_id": "1603.07044",
          "start_idx": 15296,
          "end_idx": 15790
        },
        "page_content": " already performs better than a very strong feature-rich based system. For task B, since only limited training data is given, both feature-rich based system and our system are worse than the IR system. For task C, our system also got comparable results with the feature-rich based system. If we do a simple system combination (average the rank score) between our system and the IR system, the combined system will give large gains on tasks B and C. This implies that our system is complimentary",
        "type": "Document"
      },
      {
        "id": "47b152dd-73e8-40eb-a927-52d5511639b3",
        "metadata": {
          "vector_store_key": "1905.06566-2",
          "chunk_id": 89,
          "document_id": "1905.06566",
          "start_idx": 24955,
          "end_idx": 25353
        },
        "page_content": " We also conducted human experiment with 20 randomly sampled documents from the CNNDM test set. We compared our model $\\text{\\sc Hibert}_M$ against Lead3, DCA, Latent, BERT and the human reference (Human). We asked the subjects to rank the outputs of these systems from best to worst. As shown in Table 4 , the output of $\\text{\\sc Hibert}_M$ is selected as the best in 30% of cases and we obtained",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What do they mean by answer styles?",
    "llm_answer": "Answer styles are related to the context of the question in the answer sentence, and can control other styles such as length and speaking styles.  An artificial token corresponding to the target style is introduced at the beginning of the answer sentence.\n",
    "context": [
      {
        "id": "3c7174c3-de4d-4e92-958b-592e901c1fe6",
        "metadata": {
          "vector_store_key": "1901.02262-0",
          "chunk_id": 6,
          "document_id": "1901.02262",
          "start_idx": 2098,
          "end_idx": 2374
        },
        "page_content": " x^{p_k}_{L}\\rbrace $ , and an answer style $s$ , an RC system outputs an answer $y = \\lbrace y_1, \\ldots , y_T \\rbrace $ conditioned on the style. In short, for inference, given a set of 3-tuples $(x^q, \\lbrace x^{p_k}\\rbrace , s)$ , the system predicts $P(y)$ . The training",
        "type": "Document"
      },
      {
        "id": "6c137ee6-68bd-4f03-b273-d4e5f1e3f781",
        "metadata": {
          "vector_store_key": "1901.02262-2",
          "chunk_id": 83,
          "document_id": "1901.02262",
          "start_idx": 22902,
          "end_idx": 23374
        },
        "page_content": " The key strength of our model is its high accuracy of generating abstractive summaries from the question and passages; our model achieved state-of-the-art performance in terms of Rouge-L on the Q&A and NLG tasks of MS MARCO 2.1 that have different answer styles BIBREF5 . The styles considered in this paper are only related to the context of the question in the answer sentence; our model will be promising for controlling other styles such as length and speaking styles",
        "type": "Document"
      },
      {
        "id": "aa6b9d07-79b5-4a7c-b853-574c8b27071f",
        "metadata": {
          "vector_store_key": "1901.02262-2",
          "chunk_id": 36,
          "document_id": "1901.02262",
          "start_idx": 9817,
          "end_idx": 10299
        },
        "page_content": " $t$ depend only on the known outputs at positions less than $t$ . Moreover, to be able to make use of multiple answer styles within a single system, our model introduces an artificial token corresponding to the target style at the beginning of the answer sentence ( $y_1$ ), like BIBREF14 . At test time, the user can specify the first token to control the answer styles. This modification does not require any changes to the model architecture. Note that introducing the tokens on",
        "type": "Document"
      },
      {
        "id": "424601cb-2fd3-4682-890f-397aae85ad3b",
        "metadata": {
          "vector_store_key": "1901.02262-2",
          "chunk_id": 80,
          "document_id": "1901.02262",
          "start_idx": 21638,
          "end_idx": 22164
        },
        "page_content": " 5 shows the lengths of the answers generated by our model, which are broken down by answer style and query type. The generated answers were relatively shorter than the reference answers but well controlled with the target style in every query type. Also, we should note that our model does not guarantee the consistency in terms of meaning across the answer styles. We randomly selected 100 questions and compared the answers our model generated with the NLG and Q&A styles. The consistency ratio was 0.81, where major errors",
        "type": "Document"
      },
      {
        "id": "76d6e267-c660-4afe-a59f-598d2908f809",
        "metadata": {
          "vector_store_key": "1905.08949-5",
          "chunk_id": 24,
          "document_id": "1905.08949",
          "start_idx": 9103,
          "end_idx": 9529
        },
        "page_content": " on deep question datasets, such as LearningQ BIBREF8 , shown in Section \"Generation of Deep Questions\" . The second factor is the answer type, i.e., the expected form of the answer, typically having four settings: (1) the answer is a text span in the passage, which is usually the case for factoid questions, (2) human-generated, abstractive answer that may not appear in the passage, usually the case for deep questions, (3)",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the baselines that Masque is compared against?",
    "llm_answer": "Baseline and Ablation models.\n",
    "context": [
      {
        "id": "62b30720-eda9-42ac-8a68-dd324c575513",
        "metadata": {
          "vector_store_key": "1901.02262-0",
          "chunk_id": 4,
          "document_id": "1901.02262",
          "start_idx": 1583,
          "end_idx": 1960
        },
        "page_content": ", for multi-passage RC. On the MS MARCO 2.1 dataset, Masque achieves state-of-the-art performance on the dataset's two tasks, Q&A and NLG, with different answer styles. The main contributions of this study are that our model enables the following two abilities. The task considered in this paper, is defined as: Problem 1 Given a question with $J$ words $x^q = \\lbrace x^q_1, \\",
        "type": "Document"
      },
      {
        "id": "dc50a5cf-b755-41a0-a173-1b561db0ed05",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "2e0ed67a-0c1c-48c2-8e46-cda2ed174823",
        "metadata": {
          "vector_store_key": "1901.02262-2",
          "chunk_id": 81,
          "document_id": "1901.02262",
          "start_idx": 22164,
          "end_idx": 22610
        },
        "page_content": " the NLG and Q&A styles. The consistency ratio was 0.81, where major errors were due to copying words from different parts of the passages and generating different words, especially yes/no, from a fixed vocabulary. Appendix \"Reading Comprehension Examples generated by Masque from MS MARCO 2.1\" shows examples of generated answers. We found (d) style errors; (e) yes/no classification errors; (f) copy errors with respect to numerical values; and",
        "type": "Document"
      },
      {
        "id": "8836cac5-0770-4b24-b87a-3d1fa58536ca",
        "metadata": {
          "vector_store_key": "1703.09684-5",
          "chunk_id": 65,
          "document_id": "1703.09684",
          "start_idx": 24411,
          "end_idx": 24828
        },
        "page_content": " simple baseline methods routinely exceed more complex methods using simple accuracy BIBREF7 , BIBREF8 , BIBREF19 . On TDIUC, MLP surpasses MCB and NMN in terms of simple accuracy, but a closer inspection reveals that MLP's score is highly determined by performance on categories with a large number of examples, such as `absurd' and `object presence.' Using MPT, we find that both NMN and MCB outperform MLP. Inspect",
        "type": "Document"
      },
      {
        "id": "8e595a15-617e-47fa-8f20-8208066c69dc",
        "metadata": {
          "vector_store_key": "1905.06566-2",
          "chunk_id": 83,
          "document_id": "1905.06566",
          "start_idx": 23289,
          "end_idx": 23633
        },
        "page_content": " by 0.5 ROUGE. We also implemented two baselines. One is the hierarchical transformer summarization model (HeriTransfomer; described in \"Extractive Summarization\" ) without pre-training. Note the setting for HeriTransfomer is ( $L=4$ , $H=300$ and $A=4$ ) . We can see that the pre-training (details in Section \"Pre-training\" ) leads to a +1.25",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance achieved on NarrativeQA?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "5e276034-cead-4394-8daa-927f30ae4e79",
        "metadata": {
          "vector_store_key": "1909.08859-1",
          "chunk_id": 67,
          "document_id": "1909.08859",
          "start_idx": 24277,
          "end_idx": 24845
        },
        "page_content": " to keep track of the changes in the states of the entities over the course of the procedure. Our experimental analysis on visual reasoning tasks in the RecipeQA dataset shows that the model significantly improves the results of the previous models, indicating that it better understands the procedural text and the accompanying images. Additionally, we carefully analyze our results and find that our approach learns meaningful dynamic representations of entities without any entity-level supervision. Although we achieve state-of-the-art results on RecipeQA, clearly",
        "type": "Document"
      },
      {
        "id": "89d02914-167f-4e63-a7c5-7dd0e8ff6692",
        "metadata": {
          "vector_store_key": "1908.10449-5",
          "chunk_id": 68,
          "document_id": "1908.10449",
          "start_idx": 23672,
          "end_idx": 24120
        },
        "page_content": " the purpose of our study of partially observable text environments. Not surprisingly, QA-DQN performs worse in general on iNewsQA, in all experiments. As shown in Table TABREF16, the average number of sentences per document in iNewsQA is about 6 times more than in iSQuAD. This is analogous to games with larger maps in the RL literature, where the environment is partially observable. A better exploration (in our case, jumping) strategy may help",
        "type": "Document"
      },
      {
        "id": "d2a4c5a1-497c-4122-84b3-f1cde2501aa5",
        "metadata": {
          "vector_store_key": "1812.07023-2",
          "chunk_id": 47,
          "document_id": "1812.07023",
          "start_idx": 17962,
          "end_idx": 18472
        },
        "page_content": ": not concatenating the question, answer pairs before encoding them, the auxiliary decoder module, and using the Time-Extended FiLM module for feature extraction. These, combined with using scheduled sampling and running hyperparameter optimization over the validation set to select hyperparameters, give us the observed performance boost. We observe that our models generate fairly relevant responses to questions in the dialogues, and models with audio-visual inputs respond to audio-visual questions (e.g. \u201c",
        "type": "Document"
      },
      {
        "id": "77271be3-870d-4cb5-b3fc-f539b8f75af9",
        "metadata": {
          "vector_store_key": "1901.02262-2",
          "chunk_id": 73,
          "document_id": "1901.02262",
          "start_idx": 19166,
          "end_idx": 19597
        },
        "page_content": " positive labels to 0.9. Table 2 shows that our ensemble model, controlled with the NLG and Q&A styles, achieved state-of-the-art performance on the NLG and Q&A tasks in terms of Rouge-L. In particular, for the NLG task, our single model outperformed competing models in terms of both Rouge-L and Bleu-1. The capability of creating abstractive summaries from the question and passages contributed to its improvements over the state",
        "type": "Document"
      },
      {
        "id": "54f462e2-16c3-4ac7-bf99-656e4b25df2d",
        "metadata": {
          "vector_store_key": "1703.09684-0",
          "chunk_id": 5,
          "document_id": "1703.09684",
          "start_idx": 2106,
          "end_idx": 2596
        },
        "page_content": " evaluate both baseline and state-of-the-art VQA algorithms. We found that our proposed approach enables more nuanced comparisons of VQA algorithms, and helps us understand the benefits of specific techniques better. In addition, it also allowed us to answer several key questions about VQA algorithms, such as, `Is the generalization capacity of the algorithms hindered by the bias in the dataset?', `Does the use of spatial attention help answer specific question-types?', `How successful",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is an \"answer style\"?",
    "llm_answer": "An \"answer style\" is related to the context of the question in the answer sentence, and can also control other styles such as length and speaking styles.  It also refers to the expected form of the answer,  typically having four settings: a text span in the passage, human-generated abstractive answer, etc.\n",
    "context": [
      {
        "id": "3c7174c3-de4d-4e92-958b-592e901c1fe6",
        "metadata": {
          "vector_store_key": "1901.02262-0",
          "chunk_id": 6,
          "document_id": "1901.02262",
          "start_idx": 2098,
          "end_idx": 2374
        },
        "page_content": " x^{p_k}_{L}\\rbrace $ , and an answer style $s$ , an RC system outputs an answer $y = \\lbrace y_1, \\ldots , y_T \\rbrace $ conditioned on the style. In short, for inference, given a set of 3-tuples $(x^q, \\lbrace x^{p_k}\\rbrace , s)$ , the system predicts $P(y)$ . The training",
        "type": "Document"
      },
      {
        "id": "aa6b9d07-79b5-4a7c-b853-574c8b27071f",
        "metadata": {
          "vector_store_key": "1901.02262-2",
          "chunk_id": 36,
          "document_id": "1901.02262",
          "start_idx": 9817,
          "end_idx": 10299
        },
        "page_content": " $t$ depend only on the known outputs at positions less than $t$ . Moreover, to be able to make use of multiple answer styles within a single system, our model introduces an artificial token corresponding to the target style at the beginning of the answer sentence ( $y_1$ ), like BIBREF14 . At test time, the user can specify the first token to control the answer styles. This modification does not require any changes to the model architecture. Note that introducing the tokens on",
        "type": "Document"
      },
      {
        "id": "6c137ee6-68bd-4f03-b273-d4e5f1e3f781",
        "metadata": {
          "vector_store_key": "1901.02262-2",
          "chunk_id": 83,
          "document_id": "1901.02262",
          "start_idx": 22902,
          "end_idx": 23374
        },
        "page_content": " The key strength of our model is its high accuracy of generating abstractive summaries from the question and passages; our model achieved state-of-the-art performance in terms of Rouge-L on the Q&A and NLG tasks of MS MARCO 2.1 that have different answer styles BIBREF5 . The styles considered in this paper are only related to the context of the question in the answer sentence; our model will be promising for controlling other styles such as length and speaking styles",
        "type": "Document"
      },
      {
        "id": "76d6e267-c660-4afe-a59f-598d2908f809",
        "metadata": {
          "vector_store_key": "1905.08949-5",
          "chunk_id": 24,
          "document_id": "1905.08949",
          "start_idx": 9103,
          "end_idx": 9529
        },
        "page_content": " on deep question datasets, such as LearningQ BIBREF8 , shown in Section \"Generation of Deep Questions\" . The second factor is the answer type, i.e., the expected form of the answer, typically having four settings: (1) the answer is a text span in the passage, which is usually the case for factoid questions, (2) human-generated, abstractive answer that may not appear in the passage, usually the case for deep questions, (3)",
        "type": "Document"
      },
      {
        "id": "424601cb-2fd3-4682-890f-397aae85ad3b",
        "metadata": {
          "vector_store_key": "1901.02262-2",
          "chunk_id": 80,
          "document_id": "1901.02262",
          "start_idx": 21638,
          "end_idx": 22164
        },
        "page_content": " 5 shows the lengths of the answers generated by our model, which are broken down by answer style and query type. The generated answers were relatively shorter than the reference answers but well controlled with the target style in every query type. Also, we should note that our model does not guarantee the consistency in terms of meaning across the answer styles. We randomly selected 100 questions and compared the answers our model generated with the NLG and Q&A styles. The consistency ratio was 0.81, where major errors",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are the EAU text spans annotated?",
    "llm_answer": "The context describes calculating the element-wise sum of GloVe vectors for words within the EAU span and surrounding context, and computing element-wise subtraction of source and target EAU vectors.  It also mentions using a corrected dataset (SNLI-VE-2.0) created through Amazon Mechanical Turk annotations.  However, the *method* of annotating the EAU text spans themselves isn't described.\n",
    "context": [
      {
        "id": "0205c514-3c6e-4ed1-a0a2-34e47ccc25a8",
        "metadata": {
          "vector_store_key": "1906.03338-6",
          "chunk_id": 26,
          "document_id": "1906.03338",
          "start_idx": 9744,
          "end_idx": 10174
        },
        "page_content": " the EAU span ( INLINEFORM0 ) and the words of the EAU-surrounding context ( INLINEFORM1 ). Additionally, we compute the element-wise subtraction of the source EAU vector from the target EAU vector, with the aim of modelling directions in distributional space, similarly to BIBREF25 . Words with no corresponding pre-trained word vector and empty sequences (e.g., no preceding context available) are treated as a zero-vector. Tree",
        "type": "Document"
      },
      {
        "id": "74cfb20b-b81e-4bdb-998d-1c7b188616c4",
        "metadata": {
          "vector_store_key": "1906.03338-6",
          "chunk_id": 25,
          "document_id": "1906.03338",
          "start_idx": 9472,
          "end_idx": 9933
        },
        "page_content": " content-based representation we retrieve only discourse relations that are confined within the span of the argumentative unit. In the very frequent case that discourse features cross the boundaries of embedding context and EAU span, we only take them into account for INLINEFORM0 . We use the element-wise sum of 300-dimensional pre-trained GloVe vectors BIBREF24 corresponding to the words within the EAU span ( INLINEFORM0 ) and the words of the EAU-surround",
        "type": "Document"
      },
      {
        "id": "fdac43e2-015b-402f-af33-b9cbe4928509",
        "metadata": {
          "vector_store_key": "1906.03338-6",
          "chunk_id": 28,
          "document_id": "1906.03338",
          "start_idx": 10472,
          "end_idx": 10892
        },
        "page_content": " node of the context which does not contain the EAU (ADVP in Figure FIGREF26 ). The sentiment for an EAU span ( INLINEFORM1 ) is assigned to the highest possible node covering the EAU span which does not contain the context sub-tree (S in Figure FIGREF26 ). The full-access ( INLINEFORM2 ) score is assigned to the lowest possible node which covers both the EAU span and its surrounding context (S' in Figure FIGREF26 ).",
        "type": "Document"
      },
      {
        "id": "3e2a2348-b186-4a50-b788-6b8c30a69b41",
        "metadata": {
          "vector_store_key": "1906.03338-6",
          "chunk_id": 29,
          "document_id": "1906.03338",
          "start_idx": 10892,
          "end_idx": 11374
        },
        "page_content": " which covers both the EAU span and its surrounding context (S' in Figure FIGREF26 ). Next to the sentiment scores for the selected tree nodes and analogously to the word embeddings, we also calculate the element-wise subtraction of the one-hot sentiment source vectors from the one-hot sentiment target vectors. This results in three additional vectors corresponding to INLINEFORM3 , INLINEFORM4 and INLINEFORM5 difference vectors. Our first step towards our main experiments is to",
        "type": "Document"
      },
      {
        "id": "0bc787e4-cf9a-4641-beaf-9f9228b8c98e",
        "metadata": {
          "vector_store_key": "2004.03744-8",
          "chunk_id": 5,
          "document_id": "2004.03744",
          "start_idx": 1873,
          "end_idx": 2305
        },
        "page_content": " sets of SNLI-VE, using Amazon Mechanical Turk (MTurk). To ensure high quality annotations, we used a series of quality control measures, such as in-browser checks, inserting trusted examples, and collecting three annotations per instance. Secondly, we re-evaluate current image-text understanding systems, such as the bottom-up top-down attention network (BUTD) BIBREF5 on VTE using our corrected dataset, which we call SNLI-VE-2.0",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which Twitter corpus was used to train the word vectors?",
    "llm_answer": "1, 7, and 15 days of Twitter data.\n",
    "context": [
      {
        "id": "e3b776f8-742c-4c5b-8f20-258f954bc03e",
        "metadata": {
          "vector_store_key": "1602.08741-3",
          "chunk_id": 51,
          "document_id": "1602.08741",
          "start_idx": 18542,
          "end_idx": 19007
        },
        "page_content": " We achieve results comparable with results obtained while training Word2Vec on traditional corpora, like Wikipedia and Web pages BIBREF3 , BIBREF11 . This is especially important because Twitter data is highly dynamic, and traditional sources are mostly static (rarely change over time). Thus verbal data acquired from Twitter may be used to estimate word vectors for neologisms, or determine other changes in word semantic, as soon as they appear in human speech.",
        "type": "Document"
      },
      {
        "id": "1e2295d5-0a0e-4776-b1d8-45dd4a1cd342",
        "metadata": {
          "vector_store_key": "1602.08741-3",
          "chunk_id": 47,
          "document_id": "1602.08741",
          "start_idx": 16975,
          "end_idx": 17486
        },
        "page_content": " with algebraic operations over these vectors. Authors of BIBREF2 propose to assess quality of these vectors on task of exact prediction of these word relations. However, word vectors learned from Twitter seem to perform poorly on this task. We don\u2019t make systematic research on this subject here because it goes outside of the scope of the current paper, though it is an important direction of future studies. Twitter post often contains three special types of words: user mentions, hashtags and hyperlinks. It",
        "type": "Document"
      },
      {
        "id": "82047783-0d08-4d2b-a54b-4398abbdb1cb",
        "metadata": {
          "vector_store_key": "1701.00185-4",
          "chunk_id": 54,
          "document_id": "1701.00185",
          "start_idx": 19748,
          "end_idx": 20221
        },
        "page_content": " and the most parameters are set as same as Mikolov et al. BIBREF23 to train word vectors on Google News setting, except of vector dimensionality using 48 and minimize count using 5. For SearchSnippets, we train word vectors on Wikipedia dumps. For StackOverflow, we train word vectors on the whole corpus of the StackOverflow dataset described above which includes the question titles and post contents. For Biomedical, we train word vectors on all titles and abstracts of",
        "type": "Document"
      },
      {
        "id": "3ca3ff47-6cd1-425d-920f-277f0b0465bf",
        "metadata": {
          "vector_store_key": "1602.08741-0",
          "chunk_id": 38,
          "document_id": "1602.08741",
          "start_idx": 13648,
          "end_idx": 14083
        },
        "page_content": "REF2 . It was mentioned that accuracy of estimated word vectors improves with size of corpus. Twitter provides an enormous amount of data, thus it is a perfect job for Word2Vec. We fix parameters for the model with following values: vector size of 300, min-freq of 40, context size of 5 and downsampling of 1e-3. We train our model subsequently with 1, 7 and 15 days of Twitter data (each starting with 07/21 and followed by subsequent",
        "type": "Document"
      },
      {
        "id": "743befa5-b4c9-402a-aa10-e9462ae22f80",
        "metadata": {
          "vector_store_key": "1604.05781-6",
          "chunk_id": 12,
          "document_id": "1604.05781",
          "start_idx": 4670,
          "end_idx": 5120
        },
        "page_content": " either not natural language text, or it is Twitter-specific, or both. Documents were broken into individual words (unigrams) on whitespace. Casing information was retained, as we will use it for our Named Entity analysis, but otherwise all words were considered lowercase only (see also the Appendix). Stemming BIBREF30 and lemmatization BIBREF31 were not performed. Causal documents were chosen to contain one occurrence only of the exact unigrams:",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does proposed word embeddings compare to Sindhi fastText word representations?",
    "llm_answer": "The proposed Sindhi word embeddings have surpassed SdfastText in intrinsic evaluation matrices.\n",
    "context": [
      {
        "id": "8c2a6488-0183-4053-916a-7e07cb822237",
        "metadata": {
          "vector_store_key": "1911.12579-0",
          "chunk_id": 125,
          "document_id": "1911.12579",
          "start_idx": 46126,
          "end_idx": 46618
        },
        "page_content": " same vocabulary because of character-level learning of word representations and sub-sampling approaches in SG and CBoW. Our proposed Sindhi word embeddings have surpassed SdfastText in the intrinsic evaluation matrix. Also, the vocabulary of SdfastText is limited because they are trained on a small Wikipedia corpus of Sindhi Persian-Arabic. We will further investigate the extrinsic performance of proposed word embeddings on the Sindhi text classification task in the future. The proposed",
        "type": "Document"
      },
      {
        "id": "c56b854c-a0e3-41ee-bc89-44ae53b7d7a8",
        "metadata": {
          "vector_store_key": "1911.12579-0",
          "chunk_id": 17,
          "document_id": "1911.12579",
          "start_idx": 6379,
          "end_idx": 6878
        },
        "page_content": " corpus. Generate word embeddings using GloVe, CBoW, and SG Word2Vec algorithms also evaluate and compare them using the intrinsic evaluation approaches of cosine similarity matrix and WordSim353. We are the first to evaluate SdfastText word representations and compare them with our proposed Sindhi word embeddings. The remaining sections of the paper are organized as; Section SECREF2 presents the literature survey regarding computational resources, Sindhi corpus construction, and word embedding",
        "type": "Document"
      },
      {
        "id": "be5e7c54-6ca4-44ec-b966-977e403c54dd",
        "metadata": {
          "vector_store_key": "1911.12579-0",
          "chunk_id": 126,
          "document_id": "1911.12579",
          "start_idx": 46397,
          "end_idx": 46655
        },
        "page_content": " performance of proposed word embeddings on the Sindhi text classification task in the future. The proposed resources along with systematic evaluation will be a sophisticated addition to the computational resources for statistical Sindhi language processing.",
        "type": "Document"
      },
      {
        "id": "64dd3f48-21bd-4dd6-8b45-d82409896fa1",
        "metadata": {
          "vector_store_key": "1911.12579-0",
          "chunk_id": 117,
          "document_id": "1911.12579",
          "start_idx": 42864,
          "end_idx": 43393
        },
        "page_content": ". The large corpus obtained from multiple web resources is utilized for the training of word embeddings using SG, CBoW and Glove models. The intrinsic evaluation along with comparative results demonstrates that the proposed Sindhi word embeddings have accurately captured the semantic information as compare to recently revealed SdfastText word vectors. The SG yield best results in nearest neighbors, word pair relationship and semantic similarity. The performance of CBoW is also close to SG in all the evaluation matrices. The",
        "type": "Document"
      },
      {
        "id": "e9c117ad-589f-4b14-95f8-223926c0ba2a",
        "metadata": {
          "vector_store_key": "1911.12579-0",
          "chunk_id": 121,
          "document_id": "1911.12579",
          "start_idx": 44514,
          "end_idx": 45073
        },
        "page_content": " refined further by creating custom benchmarks and the extrinsic evaluation approach will be employed for the performance analysis of proposed word embeddings. Moreover, we will also utilize the corpus using Bi-directional Encoder Representation Transformer BIBREF13 for learning deep contextualized Sindhi word representations. Furthermore, the generated word embeddings will be utilized for the automatic construction of Sindhi WordNet. In this paper, we mainly present three novel contributions of large corpus development contains large vocabulary of more",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many uniue words are in the dataset?",
    "llm_answer": "618,224\n",
    "context": [
      {
        "id": "8cc6285d-61e9-447e-8717-3a3846b0f702",
        "metadata": {
          "vector_store_key": "1912.13109-4",
          "chunk_id": 6,
          "document_id": "1912.13109",
          "start_idx": 2336,
          "end_idx": 2800
        },
        "page_content": " large variation. Spelling variation: There is no agreement on the spellings of the words which are mixed with English. For example to express love, a code mixed spelling, specially when used social platforms might be pyaar, pyar or pyr. Dataset: Based on some earlier work, only available labelled dataset had 3189 rows of text messages of average length of 116 words and with a range of 1, 1295. Prior work addresses this concern by using Transfer Learning on an",
        "type": "Document"
      },
      {
        "id": "16dd4533-d27e-4942-9eeb-f8782a281713",
        "metadata": {
          "vector_store_key": "2001.09332-2",
          "chunk_id": 21,
          "document_id": "2001.09332",
          "start_idx": 7980,
          "end_idx": 8493
        },
        "page_content": " it was also decided to replace every number present in the text with the particular $\\langle NUM \\rangle $ token; which probably also assumes a better representation in the embedding space (not separating into the various possible values). All the words were then transformed to lowercase (to avoid a double presence) finally producing a vocabulary of $618\\,224$ words. Note that among the special characters are also included punctuation marks, which therefore do not appear within the vocabulary. However, some",
        "type": "Document"
      },
      {
        "id": "8adfa466-610f-43a1-b792-c02a0bedf911",
        "metadata": {
          "vector_store_key": "1910.12618-2",
          "chunk_id": 32,
          "document_id": "1910.12618",
          "start_idx": 12926,
          "end_idx": 13347
        },
        "page_content": " and $\\#\\lbrace d: w \\in d \\rbrace $ is the number of documents in which the word $w$ appears. In our work we considered only individual words, also commonly referred as 1-grams in the field of natural language processing (NLP). The methodology can be easily extended to $n$-grams (groups of $n$ consecutive words), but initial experiments showed that it did not bring any significant improvement over 1-grams. The second",
        "type": "Document"
      },
      {
        "id": "df39ff13-16b3-498b-8399-9a9fba479cc1",
        "metadata": {
          "vector_store_key": "1602.08741-0",
          "chunk_id": 41,
          "document_id": "1602.08741",
          "start_idx": 14742,
          "end_idx": 15167
        },
        "page_content": "and pair) is \"russian\u0439\u0435\u043b\u044c\", Yale, the name of university in the USA. There are no \"missing\" words in 15-days corpus. Training the model on 15-days corpus took 8 hours on our machine with 2 cores and 4Gb of RAM. We have an intuition that further improvements are possible with larger corpus. Comparing our results to ones reported by RUSSE participants, we conclude that our best result of 0.598 is comparable to other results",
        "type": "Document"
      },
      {
        "id": "546e67c8-5b73-4c27-9417-d9dcac9f5b2d",
        "metadata": {
          "vector_store_key": "1809.04686-3",
          "chunk_id": 18,
          "document_id": "1809.04686",
          "start_idx": 6349,
          "end_idx": 6815
        },
        "page_content": " sub-word vocabulary BIBREF37 , BIBREF38 of 32K units from all source and target training data. We use this sub-word vocabulary for all of our experiments below. The Amazon reviews dataset BIBREF39 is a multilingual sentiment classification dataset, providing data for four languages - English (En), French (Fr), German (De), and Japanese. We use the English and French datasets in our experiments. The dataset contains 6,000 documents in the train and test portions",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which baseline methods are used?",
    "llm_answer": "K-means (TF), K-means (TF-IDF), SkipVec, RecNN (Ave.), RecNN (SNLI + MultiNLI), Para2vec.  BiRNN language model.  Linear or multi-layer perceptron (MLP) classifiers.\n",
    "context": [
      {
        "id": "5052ba39-69fe-4041-a83e-e233fd3408c9",
        "metadata": {
          "vector_store_key": "1701.00185-8",
          "chunk_id": 56,
          "document_id": "1701.00185",
          "start_idx": 20521,
          "end_idx": 21012
        },
        "page_content": "ans, Skip-thought Vectors, Recursive Neural Network and Paragraph Vector based clustering methods, four baseline clustering methods are directly based on the popular unsupervised dimensionality reduction methods as described in Section SECREF11 . We further compare our approach with some other non-biased neural networks, such as bidirectional RNN. More details are listed as follows: K-means K-means BIBREF42 on original keyword features which are respectively weighted with term frequency",
        "type": "Document"
      },
      {
        "id": "97d0589b-102d-402c-bcb2-27484aa3c4fb",
        "metadata": {
          "vector_store_key": "1701.00185-8",
          "chunk_id": 76,
          "document_id": "1701.00185",
          "start_idx": 27376,
          "end_idx": 27736
        },
        "page_content": " and NMI performance of our proposed approaches and four baseline methods, K-means, SkipVec, RecNN and Para2vec based clustering methods. Intuitively, we get a general observation that (1) BoW based approaches, including K-means (TF) and K-means (TF-IDF), and SkipVec based approaches perform not well; (2) RecNN based approaches, both RecNN (Ave.) and RecNN (",
        "type": "Document"
      },
      {
        "id": "824971d7-63ea-4037-9e29-269aae646ddf",
        "metadata": {
          "vector_store_key": "1810.09774-5",
          "chunk_id": 18,
          "document_id": "1810.09774",
          "start_idx": 6998,
          "end_idx": 7473
        },
        "page_content": " SNLI + MultiNLI we report the baseline using the SNLI test data. All the experimental combinations are listed in Table 1 . Examples from the selected datasets are provided in Table 2 . To be more precise, we vary three things: training dataset, model and testing dataset. We should qualify this though, since the three datasets we look at, can also be grouped by text domain/genre and type of data collection, with MultiNLI and SNLI using the same data collection style, and",
        "type": "Document"
      },
      {
        "id": "f189eac9-0435-44a6-90b3-1a428fecf8da",
        "metadata": {
          "vector_store_key": "1707.03904-4",
          "chunk_id": 53,
          "document_id": "1707.03904",
          "start_idx": 20086,
          "end_idx": 20524
        },
        "page_content": " \u2013 finding the few most relevant ones will allow the reader to work best. In Tables 2 and 3 we compare all baselines when the context size is tuned to maximize the overall accuracy on the validation set. For Quasar-S the best performing baseline is the BiRNN language model, which achieves $33.6\\%$ accuracy. The GA model achieves $48.3\\%$ accuracy on the set of instances for which the answer is in context, however, a search accuracy of",
        "type": "Document"
      },
      {
        "id": "d3d778d5-f01f-475d-bce0-1cee3d135bf5",
        "metadata": {
          "vector_store_key": "1703.09684-4",
          "chunk_id": 52,
          "document_id": "1703.09684",
          "start_idx": 19848,
          "end_idx": 20262
        },
        "page_content": " a more comprehensive review, see BIBREF10 and BIBREF11 . Two simple VQA baselines are linear or multi-layer perceptron (MLP) classifiers that take as input the question and image embeddings concatenated to each other BIBREF1 , BIBREF7 , BIBREF8 , where the image features come from the last hidden layer of a CNN. These simple approaches often work well and can be competitive with complex attentive models BIBREF",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much is the BLEU score?",
    "llm_answer": "The context provides several BLEU scores, but no single \"the\" BLEU score.\n",
    "context": [
      {
        "id": "3181f7bc-a5fe-42a5-9e5e-43ba22e96929",
        "metadata": {
          "vector_store_key": "1707.00110-2",
          "chunk_id": 35,
          "document_id": "1707.00110",
          "start_idx": 10538,
          "end_idx": 10965
        },
        "page_content": " size 128. We train for at most 200,000 steps (see Figure 3 for sample learning curves). BLEU scores are calculated on tokenized data using the multi-bleu.perl script in Moses. We decode using beam search with a beam size of 10 BIBREF18 . Table 1 shows the BLEU scores of our model on different sequence lengths while varying $K$ . This is a study of the trade-off between computational time and representational power. A large",
        "type": "Document"
      },
      {
        "id": "cae13da9-4e4f-4eb8-ad3a-97706c183af9",
        "metadata": {
          "vector_store_key": "1910.11949-7",
          "chunk_id": 31,
          "document_id": "1910.11949",
          "start_idx": 12034,
          "end_idx": 12516
        },
        "page_content": "U BIBREF30 metric on the validation set for the VQG model training. BLEU is a measure of similitude between generated and target sequences of words, widely used in natural language processing. It assumes that valid generated responses have significant word overlap with the ground truth responses. We use it because in this case we have five different references for each of the generated questions. We obtain a BLEU score of 2.07. Our chatbot model instead, only have one reference",
        "type": "Document"
      },
      {
        "id": "69b5e75f-a2dc-4c10-a308-8db66f1ce14c",
        "metadata": {
          "vector_store_key": "1809.08731-2",
          "chunk_id": 66,
          "document_id": "1809.08731",
          "start_idx": 22358,
          "end_idx": 22769
        },
        "page_content": " expressed, e.g., by callison2006re, who doubted the meaningfulness of BLEU, and claimed that a higher BLEU score was neither a necessary precondition nor a proof of improved translation quality. Similarly, song2013bleu discussed BLEU being unreliable at the sentence or sub-sentence level (in contrast to the system-level), or for only one single reference. This was supported by isabelle-cherry-foster:2017:EM",
        "type": "Document"
      },
      {
        "id": "9ecafa72-861c-465c-ab11-f5d45f864b70",
        "metadata": {
          "vector_store_key": "2002.11893-4",
          "chunk_id": 81,
          "document_id": "2002.11893",
          "start_idx": 30351,
          "end_idx": 30691
        },
        "page_content": " as references (100 references on average), which results in high BLEU score. For user-side utterances, the BLEU score for TemplateNLG is 0.5780, while the BLEU score for SC-LSTM is 0.7858. For system-side, the two scores are 0.6828 and 0.8595. As exemplified in Table TABREF39, the gap between the two models can be attributed to that SC-L",
        "type": "Document"
      },
      {
        "id": "9968dbd3-30dd-4981-ba96-f0354ff57946",
        "metadata": {
          "vector_store_key": "2001.00137-3",
          "chunk_id": 38,
          "document_id": "2001.00137",
          "start_idx": 14872,
          "end_idx": 15265
        },
        "page_content": "-STT combinations, thus varying rates of missing and incorrect words. The level of noise in the STT imbued sentences is denoted by a inverted BLEU (iBLEU) score ranging from 0 to 1. The inverted BLEU score is denoted in Eq. (DISPLAY_FORM23): where BLEU is a common metric usually used in machine translation tasks BIBREF21. We decide to showcase that instead of regular BLEU because it is more",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which datasets are used in experiments?",
    "llm_answer": "MCTest, CNN/Daily Mail, Children's Book Test (CBT), and Biomedical.\n",
    "context": [
      {
        "id": "4cac7588-28e9-4b6e-ac0d-d763db64bc58",
        "metadata": {
          "vector_store_key": "1703.04617-6",
          "chunk_id": 3,
          "document_id": "1703.04617",
          "start_idx": 1343,
          "end_idx": 1868
        },
        "page_content": " can help attain better results on our competitive baselines. Recent advance on reading comprehension and question answering has been closely associated with the availability of various datasets. BIBREF0 released the MCTest data consisting of 500 short, fictional open-domain stories and 2000 questions. The CNN/Daily Mail dataset BIBREF1 contains news articles for close style machine comprehension, in which only entities are removed and tested for comprehension. Children's Book Test (CBT) BIBREF2 leverages named entities",
        "type": "Document"
      },
      {
        "id": "c94d339e-36f5-4336-a21a-05be0b487cef",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      },
      {
        "id": "c5eaf502-a11a-40bd-84a8-78f62e8d8d02",
        "metadata": {
          "vector_store_key": "1701.00185-9",
          "chunk_id": 10,
          "document_id": "1701.00185",
          "start_idx": 3790,
          "end_idx": 4304
        },
        "page_content": " this paper we use a new short text dataset, Biomedical, in the experiment to verify the effectiveness of our approach. Third, we put much effort on studying the influence of various different semantic features integrated in our self-taught CNN framework, which is not involved in the conference paper. For the purpose of reproducibility, we make the datasets and software used in our experiments publicly available at the website. The remainder of this paper is organized as follows: In Section SECREF2 , we first",
        "type": "Document"
      },
      {
        "id": "0e97c7e7-33a8-40dd-9b79-3c5ebe85d772",
        "metadata": {
          "vector_store_key": "1812.07023-1",
          "chunk_id": 27,
          "document_id": "1812.07023",
          "start_idx": 10725,
          "end_idx": 11271
        },
        "page_content": " prototype dataset, the training set of the avsd dataset corresponds to videos taken from the training set of the Charades dataset while the validation and test sets of the avsd dataset correspond to videos taken from the validation set of the Charades dataset. For the official dataset, training, validation and test sets are drawn from the corresponding Charades sets. The Charades dataset also provides additional annotations for the videos such as action, scene, and object annotations, which are considered to be external data sources by the",
        "type": "Document"
      },
      {
        "id": "1c380a0c-1037-4628-8ada-168d9c8247bc",
        "metadata": {
          "vector_store_key": "1707.03904-9",
          "chunk_id": 13,
          "document_id": "1707.03904",
          "start_idx": 5304,
          "end_idx": 5850
        },
        "page_content": " subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short. We plan to release these annotations along with the datasets, and our retrieved documents for each question. Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are new best results on standard benchmark?",
    "llm_answer": "\"Ours with Mask and Ordered Triplets\" outperforms Baseline and Ablation models on all metrics in previously seen environments.\n",
    "context": [
      {
        "id": "555a266e-0a08-43e5-b719-8cd2fd10ebb8",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "dc50a5cf-b755-41a0-a173-1b561db0ed05",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "c0a36e83-6a5f-4829-b712-316da2b2e885",
        "metadata": {
          "vector_store_key": "1910.03814-8",
          "chunk_id": 51,
          "document_id": "1910.03814",
          "start_idx": 19558,
          "end_idx": 20033
        },
        "page_content": " results for the LSTM, for the Davison method proposed in BIBREF7 trained with MMHS150K, and for random scores. Fig. FIGREF32 shows the Precision vs Recall plot and the ROC curve (which plots the True Positive Rate vs the False Positive Rate) of the different models. First, notice that given the subjectivity of the task and the discrepancies between annotators, getting optimal scores in the evaluation metrics is virtually impossible. However, a system with relatively low",
        "type": "Document"
      },
      {
        "id": "9d9ab555-416a-4bc9-a24d-f0230333ee94",
        "metadata": {
          "vector_store_key": "1910.12618-4",
          "chunk_id": 49,
          "document_id": "1910.12618",
          "start_idx": 19565,
          "end_idx": 20023
        },
        "page_content": " the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014 and 2015 serve as validation set (730 days). It is used to tune the hyperparameters of the different approaches. All the data from January the 1st 2016 (974 days for France and 1,096 for the UK) is used as test set, on which the final results are presented. Grid search is applied to find the best combination of values: for each hyperparameter,",
        "type": "Document"
      },
      {
        "id": "e152b4ae-6c68-42bc-89fe-2987a9bd857d",
        "metadata": {
          "vector_store_key": "1906.00180-3",
          "chunk_id": 8,
          "document_id": "1906.00180",
          "start_idx": 3251,
          "end_idx": 3690
        },
        "page_content": "IBREF27 and LSTM BIBREF28 . Training and testing accuracies after 50 training epochs, averaged over five different model runs, are shown in Table UID18 . All recurrent models outperform the summing baseline. Even the simplest recurrent network, the SRN, achieves higher training and testing accuracy scores than the tree-shaped matrix model. The GRU and LSTM even beat the tensor model. The LSTM obtains slightly lower scores than the GRU,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How better is performance compared to competitive baselines?",
    "llm_answer": "The final model \"Ours with Mask and Ordered Triplets\" outperforms baselines on all metrics in previously seen environments, increasing accuracy by 35% and 25% for Exact Match and Goal Match respectively.  Our model also outperforms baselines by a clear margin in new environments.\n",
    "context": [
      {
        "id": "24bf2271-5c8e-4680-b893-d10c22b99f5e",
        "metadata": {
          "vector_store_key": "1707.03904-4",
          "chunk_id": 55,
          "document_id": "1707.03904",
          "start_idx": 20754,
          "end_idx": 21226
        },
        "page_content": " baselines, however, lag behind human performance by $16.4\\%$ and $32.1\\%$ for Quasar-S and Quasar-T respectively, indicating the strong potential for improvement. Interestingly, for human performance we observe that non-experts are able to match or beat the performance of experts when given access to the background corpus for searching the answers. We also emphasize that the human performance is limited by either the knowledge of the experts, or the usefulness of the",
        "type": "Document"
      },
      {
        "id": "dc50a5cf-b755-41a0-a173-1b561db0ed05",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "11d9b2e6-e20d-4ab9-8d7c-c35a7e2d27e6",
        "metadata": {
          "vector_store_key": "1603.07044-6",
          "chunk_id": 43,
          "document_id": "1603.07044",
          "start_idx": 14997,
          "end_idx": 15515
        },
        "page_content": " task A already had a substantial amount of training data. Table 4 gives the final comparison between different models (we only list the MAP score because it is the official score for the challenge). Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited",
        "type": "Document"
      },
      {
        "id": "e4a8d053-1aed-406f-b1bc-3355dbcf6c7b",
        "metadata": {
          "vector_store_key": "1707.03904-4",
          "chunk_id": 54,
          "document_id": "1707.03904",
          "start_idx": 20334,
          "end_idx": 20754
        },
        "page_content": " accuracy on the set of instances for which the answer is in context, however, a search accuracy of only $65\\%$ means its overall performance is lower. This can improve with improved retrieval. For Quasar-T, both the neural models significantly outperform the heuristic models, with BiDAF getting the highest F1 score of $28.5\\%$ . The best performing baselines, however, lag behind human performance by $16.4\\%$ and $32",
        "type": "Document"
      },
      {
        "id": "a1974dac-f964-4aeb-a415-a2006317d90a",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 68,
          "document_id": "1810.00663",
          "start_idx": 26280,
          "end_idx": 26763
        },
        "page_content": " room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM. Note that the average number of actions in the ground truth output sequences is 7.07 for the Test-New set. Our model's predictions are just INLINEFORM1 edits off on average from the correct navigation plans. This",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What 6 language pairs is experimented on?",
    "llm_answer": "English, French, German, Spanish, Italian, and Russian.\n",
    "context": [
      {
        "id": "e90f75a5-f326-4202-82c5-878736af8308",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 51,
          "document_id": "1910.04269",
          "start_idx": 19553,
          "end_idx": 20023
        },
        "page_content": ", the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Kat",
        "type": "Document"
      },
      {
        "id": "ef088dd9-ab26-43d4-a236-5cdf234c79f7",
        "metadata": {
          "vector_store_key": "1910.11493-6",
          "chunk_id": 13,
          "document_id": "1910.11493",
          "start_idx": 5173,
          "end_idx": 5630
        },
        "page_content": " languages. Data for all but four languages (Basque, Kurmanji, Murrinhpatha, and Sorani) are extracted from English Wiktionary, a large multi-lingual crowd-sourced dictionary with morphological paradigms for many lemmata. 20 of the 100 language pairs are either distantly related or unrelated; this allows speculation into the relative importance of data quantity and linguistic relatedness. For each language, the basic data consists of triples of the form",
        "type": "Document"
      },
      {
        "id": "b0d78a46-0cfa-4367-a963-2a9a8d82c9a4",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 18,
          "document_id": "1910.04269",
          "start_idx": 6853,
          "end_idx": 7342
        },
        "page_content": " comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (",
        "type": "Document"
      },
      {
        "id": "17f3676f-8f5c-4da9-b0c7-cede94c8171d",
        "metadata": {
          "vector_store_key": "1904.05862-2",
          "chunk_id": 42,
          "document_id": "1904.05862",
          "start_idx": 15343,
          "end_idx": 15454
        },
        "page_content": "letter++ experiments, and Tatiana Likhomanenko for providing convolutional language models for our experiments.",
        "type": "Document"
      },
      {
        "id": "91a0009e-f9ac-4ecb-b3fa-9af2d4b0ebb9",
        "metadata": {
          "vector_store_key": "1910.05456-2",
          "chunk_id": 28,
          "document_id": "1910.05456",
          "start_idx": 10144,
          "end_idx": 10648
        },
        "page_content": " choose languages with different degrees of relatedness and varying morphological similarity to English, Spanish, and Zulu. We limit our experiments to languages which are written in Latin script. As an estimate for morphological similarity we look at the features from the Morphology category mentioned in The World Atlas of Language Structures (WALS). An overview of the available features as well as the respective values for our set of languages is shown in Table TABREF13. We decide on Basque (EUS),",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they enrich the positional embedding with length information",
    "llm_answer": "The provided text describes how a segment of 1 is assigned to a sentence until a [SEP] character appears, then it becomes 0.  A position embedding layer adds information related to the token's position in the sentence.  No specific enrichment of the positional embedding with length information is described.\n",
    "context": [
      {
        "id": "44dde58c-8dd3-4321-9a56-8f8a5002947e",
        "metadata": {
          "vector_store_key": "2001.00137-7",
          "chunk_id": 18,
          "document_id": "2001.00137",
          "start_idx": 7419,
          "end_idx": 7883
        },
        "page_content": " our case, since our data are formed of single sentences, the segment is 1 until the first `[SEP]' character appears (indicating segment A) and then it becomes 0 (segment B). The position embedding layer, as the name indicates, adds information related to the token's position in the sentence. This prepares the data to be considered by the layers of vanilla bidirectional transformers, which outputs a hidden embedding that can be used by our novel layers of deno",
        "type": "Document"
      },
      {
        "id": "941d08a3-5b52-43de-ac1e-174b93890a55",
        "metadata": {
          "vector_store_key": "1910.10408-7",
          "chunk_id": 25,
          "document_id": "1910.10408",
          "start_idx": 9446,
          "end_idx": 9953
        },
        "page_content": "ing strategy, we specifically aim; i) to decouple the performance of the baseline NMT model from that of the additional length information, ii) control the level of aggressiveness that can come from the data (length token) and the model (length encoding), and iii) make the approaches versatile to any pre-trained model. More importantly, it will allow to transform any NMT model to an output length aware version, while getting better improvements on the quality of the generated sequences. Our experiments",
        "type": "Document"
      },
      {
        "id": "326617a7-bff5-4ef2-a244-395be56e2b57",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 40,
          "document_id": "1910.12618",
          "start_idx": 16003,
          "end_idx": 16518
        },
        "page_content": " concatenate all the vector representations of a sentence together beforehand and result in a network with too many parameters to be trained correctly with our number of available documents. Recall that we decided to train our own vector representation of words instead of using an already available one. In order to obtain the embedding, the texts are first converted into a sequence of integers: each word is given a number ranging from 1 to $V$, where $V$ is the vocabulary size (0 is used for padding or unknown",
        "type": "Document"
      },
      {
        "id": "e23972ab-e08e-4c15-ad2f-f99a408bf0a6",
        "metadata": {
          "vector_store_key": "1807.03367-1",
          "chunk_id": 46,
          "document_id": "1807.03367",
          "start_idx": 18085,
          "end_idx": 18551
        },
        "page_content": "1ex minus.1ex-1em Continuous vectors The tourist has access to observations of several time steps, whose order is important for accurate localization. Because summing embeddings is order-invariant, we introduce a sum over positionally-gated embeddings, which, conditioned on time step INLINEFORM0 , pushes embedding information into the appropriate dimensions. More specifically, we generate an observation message INLINEFORM1 , where INLINEFORM2 is a learned gating",
        "type": "Document"
      },
      {
        "id": "1735f229-f46b-4c42-88dc-faada0d3d093",
        "metadata": {
          "vector_store_key": "1911.12579-9",
          "chunk_id": 112,
          "document_id": "1911.12579",
          "start_idx": 40753,
          "end_idx": 41248
        },
        "page_content": " both the local and global levels. We visualize the embeddings using PPL=20 on 5000-iterations of 300-D models. We use the same query words (see Table TABREF74) by retrieving the top 20 nearest neighboring word clusters for a better understanding of the distance between similar words. Every query word has a distinct color for the clear visualization of a similar group of words. The closer word clusters show the high similarity between the query and retrieved word clusters. The word clusters",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they condition the output to a given target-source class?",
    "llm_answer": "The condition vector, a text string representing the class label (e.g., \"Red bird\"), is fed to both the generator and discriminator.  It is important that the condition vector is related to the real data.\n",
    "context": [
      {
        "id": "907cfc7c-fb76-48c7-82e7-5416528bfc2d",
        "metadata": {
          "vector_store_key": "1910.09399-6",
          "chunk_id": 47,
          "document_id": "1910.09399",
          "start_idx": 18297,
          "end_idx": 18765
        },
        "page_content": " tailored outputs. In Figure FIGREF14, the condition vector is the class label (text string) \"Red bird\", which is fed to both the generator and discriminator. It is important, however, that the condition vector is related to the real data. If the model in Figure FIGREF14 was trained with the same set of real data (red birds) but the condition text was \"Yellow fish\", the generator would learn to create images of red birds when conditioned with the text \"Yellow fish",
        "type": "Document"
      },
      {
        "id": "9d60a1a1-cb26-4e1c-b917-daf265954545",
        "metadata": {
          "vector_store_key": "1909.00694-3",
          "chunk_id": 32,
          "document_id": "1909.00694",
          "start_idx": 10558,
          "end_idx": 10948
        },
        "page_content": " output is the final hidden state corresponding to the special classification tag ([CLS]). For the details of ${\\rm Encoder}$, see Sections SECREF30. We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\mathcal {L}_{\\rm AL}$, $\\math",
        "type": "Document"
      },
      {
        "id": "aa66a727-b140-4417-9188-4dccddb560e2",
        "metadata": {
          "vector_store_key": "1911.00069-1",
          "chunk_id": 28,
          "document_id": "1911.00069",
          "start_idx": 9323,
          "end_idx": 9589
        },
        "page_content": "}^{\\prime }= \\frac{\\mathbf {y}}{||\\mathbf {y}||}$ for each target-language word embedding $\\mathbf {y}$. Next, we add an orthogonality constraint to (DISPLAY_FORM14) such that $\\mathbf {M}$ is an orthogonal matrix, i.e., $\\mathbf {M}^\\mathrm {T}\\mathbf {M} = \\mathbf",
        "type": "Document"
      },
      {
        "id": "d4ac03e3-fbd8-46d2-8cbe-30f8a11b5592",
        "metadata": {
          "vector_store_key": "1902.00330-4",
          "chunk_id": 37,
          "document_id": "1902.00330",
          "start_idx": 12947,
          "end_idx": 13482
        },
        "page_content": " indicates the output of our model. After pre-training the global encoder, we start using the entity selector to choose the target entity for each mention and encode these selections. In the entity selector module, we choose the target entity from candidate set based on the results of local and global encoder. In the process of sequence disambiguation, each selection result will have an impact on subsequent decisions. Therefore, we transform the choice of the target entity into a reinforcement learning problem and view the entity",
        "type": "Document"
      },
      {
        "id": "099ca1cf-16c5-458f-9a74-a5559d5a976c",
        "metadata": {
          "vector_store_key": "1910.10408-5",
          "chunk_id": 4,
          "document_id": "1910.10408",
          "start_idx": 1526,
          "end_idx": 2024
        },
        "page_content": " and target depends on the two languages and can significantly vary from one sentence pair to another due to stylistic decisions of the translator and linguistic constraints (e.g. idiomatic expressions). In this work, we propose two approaches to control the output length of a transformer NMT model. In the first approach, we augment the source side with a token representing a specific length-ratio class, i.e. short, normal, and long, which at training time corresponds to the observed ratio and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is this library implemented into Torch or is framework agnostic?",
    "llm_answer": "PyTorch.\n",
    "context": [
      {
        "id": "0ad24ac6-7d65-4d5f-ab18-557027aa50ac",
        "metadata": {
          "vector_store_key": "2002.00876-4",
          "chunk_id": 4,
          "document_id": "2002.00876",
          "start_idx": 1722,
          "end_idx": 2323
        },
        "page_content": " with the complexity of research in deep structured prediction. With this challenge in mind, we introduce Torch-Struct with three specific contributions: Modularity: models are represented as distributions with a standard flexible API integrated into a deep learning framework. Completeness: a broad array of classical algorithms are implemented and new models can easily be added in Python. Efficiency: implementations target computational/memory efficiency for GPUs and the backend includes extensions for optimization. In this system description, we first motivate the approach taken by the library",
        "type": "Document"
      },
      {
        "id": "32d46af1-0a9d-4718-b24e-2b3e58e9dafe",
        "metadata": {
          "vector_store_key": "2002.00876-4",
          "chunk_id": 36,
          "document_id": "2002.00876",
          "start_idx": 12589,
          "end_idx": 13120
        },
        "page_content": " naively, dynamic programming algorithms in Python are prohibitively slow. As such Torch-Struct provides key primitives to help batch and vectorize these algorithms to take advantage of GPU computation and to minimize the overhead of backpropagating through chart-based dynamic programmming. Figure FIGREF17 shows the impact of these optimizations on the core algorithms. The commutative properties of semiring algorithms allow flexibility in the order in which we compute $A(\\ell )$. Typical implementations of dynamic programming",
        "type": "Document"
      },
      {
        "id": "949f4ced-8c92-4011-83bb-b3f21db88672",
        "metadata": {
          "vector_store_key": "2002.00876-4",
          "chunk_id": 15,
          "document_id": "2002.00876",
          "start_idx": 5684,
          "end_idx": 6163
        },
        "page_content": " amount of user code, the implementation achieves near perfect accuracy on the ListOps dataset. The library design of Torch-Struct follows the distributions API used by both TensorFlow and PyTorch BIBREF29. For each structured model in the library, we define a conditional random field (CRF) distribution object. From a user's standpoint, this object provides all necessary distributional properties. Given log-potentials (scores) output from a deep network $\\ell $, the user can",
        "type": "Document"
      },
      {
        "id": "48cdd012-b4e6-43d2-b457-aa87300adb67",
        "metadata": {
          "vector_store_key": "2002.00876-4",
          "chunk_id": 7,
          "document_id": "2002.00876",
          "start_idx": 3079,
          "end_idx": 3655
        },
        "page_content": " integrate with deep learning such as Pyro BIBREF23, allow for specification and inference over some discrete domains. Most ambitiously, inference libraries such as Dyna BIBREF24 allow for declarative specifications of dynamic programming algorithms to support inference for generic algorithms. Torch-Struct takes a different approach and integrates a library of optimized structured distributions into a vectorized deep learning system. We begin by motivating this approach with a case study. While structured prediction is traditionally presented at the output layer, recent",
        "type": "Document"
      },
      {
        "id": "2650da4c-bf38-4ece-a3b8-e08f34355f19",
        "metadata": {
          "vector_store_key": "2002.00876-4",
          "chunk_id": 46,
          "document_id": "2002.00876",
          "start_idx": 15991,
          "end_idx": 16529
        },
        "page_content": " vectorize important dynamic programming steps. In addition to the problems discussed so far, Torch-Struct also includes several other example implementations including supervised dependency parsing with BERT, unsupervised tagging, structured attention, and connectionist temporal classification (CTC) for speech. The full library is available at https://github.com/harvardnlp/pytorch-struct. In the future, we hope to support research and production applications employing structured models. We also believe the library provides a strong",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does this compare to traditional calibration methods like Platt Scaling?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "8a0d1b76-f511-404a-abf3-0286b8865590",
        "metadata": {
          "vector_store_key": "1909.01013-0",
          "chunk_id": 35,
          "document_id": "1909.01013",
          "start_idx": 11517,
          "end_idx": 11713
        },
        "page_content": ". Results show that strengthening bi-directional mapping consistency significantly improves the effectiveness over the state-of-the-art method, leading to the best results on a standard benchmark.",
        "type": "Document"
      },
      {
        "id": "9d4d31c9-5a30-4eee-8a6e-979131c73f64",
        "metadata": {
          "vector_store_key": "1910.04269-9",
          "chunk_id": 56,
          "document_id": "1910.04269",
          "start_idx": 21494,
          "end_idx": 21961
        },
        "page_content": "ConvNet and evaluated their effect on validation accuracy. This method is able to bypass the computational overhead of conventional approaches which depend on generation of spectrograms as a necessary pre-procesing step. We were able to achieve an accauracy of 93.7% using this technique. Next, we discussed the enhancement in performance of 2D-ConvNet using mixup augmentation, which is a recently developed technique to prevent over\ufb01tting on test data.This approach",
        "type": "Document"
      },
      {
        "id": "a8e9deae-4f9a-4ef2-ae81-43a853be9d6a",
        "metadata": {
          "vector_store_key": "1703.09684-3",
          "chunk_id": 15,
          "document_id": "1703.09684",
          "start_idx": 5561,
          "end_idx": 6066
        },
        "page_content": " that work poorly when deployed in the real-world. For related reasons, major benchmarks released in the last decade do not use simple accuracy for evaluating image recognition and related computer vision tasks, but instead use metrics such as mean-per-class accuracy that compensates for unbalanced categories. For example, on Caltech-101 BIBREF13 , even with balanced training data, simple accuracy fails to address the fact that some categories were much easier to classify than others (e.g., faces and",
        "type": "Document"
      },
      {
        "id": "6349e4ac-1f1a-47af-bad3-f0f403e00178",
        "metadata": {
          "vector_store_key": "1908.08419-1",
          "chunk_id": 67,
          "document_id": "1908.08419",
          "start_idx": 24426,
          "end_idx": 24948
        },
        "page_content": " of relabeled data are shown in Fig. FIGREF45 , where x-axis represents the number of iterations and y-axis denotes the INLINEFORM0 -score of the segmenter. Scoring strategy shows consistent improvements over uncertainty sampling in the early iterations, indicating that scoring strategy is more capable of selecting representative samples. Furthermore, we also investigate the relations between the best INLINEFORM0 -score and corresponding number of iteration on two sampling strategies, which is depicted in Fig. FIGREF",
        "type": "Document"
      },
      {
        "id": "6a332e39-1381-42ec-a71c-c89cc5d3ed8f",
        "metadata": {
          "vector_store_key": "1909.00430-4",
          "chunk_id": 49,
          "document_id": "1909.00430",
          "start_idx": 17515,
          "end_idx": 18046
        },
        "page_content": " accuracy, and matching it on F1. We report significance tests for the robustness of the method under random parameter initialization. Our reported numbers are averaged over five random initialization. Since the datasets are unbalanced w.r.t the label distribution, we report both accuracy and macro-F1. The XR training is also more stable than the other semi-supervised baselines, achieving substantially lower standard deviations across different runs. In each experiment in this section we estimate the proportions using the Sem",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What's the input representation of OpenIE tuples into the model?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "67555858-e955-441a-a620-377d82840d64",
        "metadata": {
          "vector_store_key": "1905.13413-3",
          "chunk_id": 0,
          "document_id": "1905.13413",
          "start_idx": 0,
          "end_idx": 383
        },
        "page_content": "Open information extraction (IE, sekine2006demand, Banko:2007:OIE) aims to extract open-domain assertions represented in the form of $n$ -tuples (e.g., was born in; Barack Obama; Hawaii) from natural language sentences (e.g., Barack Obama was born in Hawaii). Open IE started from rule-based BIBREF0 and syntax-driven systems BIBREF1 , BIBREF2 , and recently has used neural networks",
        "type": "Document"
      },
      {
        "id": "7b567107-7a0d-4787-98ce-8b4885f0e678",
        "metadata": {
          "vector_store_key": "1704.05572-2",
          "chunk_id": 27,
          "document_id": "1704.05572",
          "start_idx": 9274,
          "end_idx": 9770
        },
        "page_content": " ordering constraint (third group in Table 1 ). Its worth mentioning that TupleInf only combines parallel evidence i.e. each tuple must connect words in the question to the answer choice. For reliable multi-hop reasoning using OpenIE tuples, we can add inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules for the Science domain is an open problem and potential avenue of future work. Comparing our method with",
        "type": "Document"
      },
      {
        "id": "d43d52f5-4cfa-4c1d-9db4-789f4742803e",
        "metadata": {
          "vector_store_key": "1810.00663-1",
          "chunk_id": 31,
          "document_id": "1810.00663",
          "start_idx": 12567,
          "end_idx": 13009
        },
        "page_content": "hot encoded into vectors of dimensionality INLINEFORM7 , where INLINEFORM8 and INLINEFORM9 are the number of nodes and edges in INLINEFORM10 , respectively. Encoder layer: The model then uses two bidirectional Gated Recurrent Units (GRUs) BIBREF25 to independently process the information from INLINEFORM0 and INLINEFORM1 , and incorporate contextual cues from the surrounding embeddings in each sequence. The outputs of the encoder layer are",
        "type": "Document"
      },
      {
        "id": "37c034a5-1a58-4350-afb6-fad2013e7b35",
        "metadata": {
          "vector_store_key": "1905.00563-9",
          "chunk_id": 42,
          "document_id": "1905.00563",
          "start_idx": 12060,
          "end_idx": 12389
        },
        "page_content": " $(s^{\\prime },r^{\\prime })$ . In particular, we train an auto-encoder where the encoder is fixed to receive the $s$ and $\\langle s^{\\prime }, r^{\\prime }, o \\rangle $0 as one-hot inputs, and calculates $\\langle s^{\\prime }, r^{\\prime }, o \\rangle $1 in the same way as the DistMult and ConvE encoders respectively (using trained",
        "type": "Document"
      },
      {
        "id": "2401d1f0-8a85-4877-a390-c6c2703ccdae",
        "metadata": {
          "vector_store_key": "1909.08859-3",
          "chunk_id": 46,
          "document_id": "1909.08859",
          "start_idx": 15768,
          "end_idx": 16158
        },
        "page_content": " \\mathbb {R}^{2d \\times 1}$ where $\\varphi _o$ is a multilayer perceptron with $\\operatorname{tanh}$ activation function. The output module takes the output of the modeling module, encoding vector embeddings of the question-aware recipe and the entities $\\mathbf {Y}$, and the embedding of the answer $\\mathbf {A}$, and returns a similarity score which is used while determining the correct",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is CRWIZ already used for data collection, what are the results?",
    "llm_answer": "Yes, CRWIZ is used for data collection.  The context describes a comparison of the CRWIZ dataset to one collected in a lab setting, but no results are presented.\n",
    "context": [
      {
        "id": "779ac96c-00cb-45c3-a15b-95aca0785652",
        "metadata": {
          "vector_store_key": "2003.05995-4",
          "chunk_id": 9,
          "document_id": "2003.05995",
          "start_idx": 3904,
          "end_idx": 4417
        },
        "page_content": "REF4 and discuss the advantages/disadvantages of both approaches. Finally, we present future work. Our contributions are as follows: The release of a platform for the CRWIZ Intelligent Wizard Interface to allow for the collection of dialogue data for longer complex tasks, by providing a dynamic selection of relevant dialogue acts. A survey of existing datasets and data collection platforms, with a comparison to the CRWIZ data collection for Wizarded crowdsourced data in task-based interactions. Table TABREF3",
        "type": "Document"
      },
      {
        "id": "d411bc2c-c361-4eff-be78-4226caa5fe81",
        "metadata": {
          "vector_store_key": "2003.05995-4",
          "chunk_id": 10,
          "document_id": "2003.05995",
          "start_idx": 4229,
          "end_idx": 4714
        },
        "page_content": "IZ data collection for Wizarded crowdsourced data in task-based interactions. Table TABREF3 gives an overview of prior work and datasets. We report various factors to compare to the CRWIZ dataset corresponding to columns in Table TABREF3: whether or not the person was aware they were talking to a bot; whether each dialogue had a single or multiple participants per role; whether the data collection was crowdsourced; and the modality of the interaction and the domain. As we see from",
        "type": "Document"
      },
      {
        "id": "37f433ca-a4bc-43c8-8cdc-1b6523820bc3",
        "metadata": {
          "vector_store_key": "2003.05995-4",
          "chunk_id": 3,
          "document_id": "2003.05995",
          "start_idx": 1353,
          "end_idx": 1907
        },
        "page_content": " have traditionally not been scalable because of the scarcity of Wizard experts or the expense to train up workers. This was the situation with an initial study reported in BIBREF4, which was conducted in a traditional lab setting and where the Wizard (an academic researcher) had to learn, through training and reading manuals, how best to perform operations in our domain of emergency response. We present the CRWIZ Intelligent Wizard Interface that enables a crowdsourced Wizard to make intelligent, relevant choices without such intensive training by",
        "type": "Document"
      },
      {
        "id": "2452875f-843f-4997-9cb2-4661689df57a",
        "metadata": {
          "vector_store_key": "2003.05995-9",
          "chunk_id": 7,
          "document_id": "2003.05995",
          "start_idx": 3074,
          "end_idx": 3590
        },
        "page_content": " to automate a conversational Intelligent Assistant (Fred), who acts as an intermediary between the operator and the multiple robotic systems BIBREF6, BIBREF7. Emergency response is clearly a high-stakes situation, which is difficult to emulate in a lab or crowdsourced data collection environment. Therefore, in order to foster engagement and collaboration, the scenario was gamified with a monetary reward given for task success. In this paper, we provide a brief survey of existing datasets and describe the CRWIZ",
        "type": "Document"
      },
      {
        "id": "542f712c-d5f8-4199-b2e8-fe9d353df252",
        "metadata": {
          "vector_store_key": "2003.05995-4",
          "chunk_id": 8,
          "document_id": "2003.05995",
          "start_idx": 3380,
          "end_idx": 3904
        },
        "page_content": " success. In this paper, we provide a brief survey of existing datasets and describe the CRWIZ framework for pairing crowdworkers and having half of them acting as Wizards by limiting their dialogue options only to relevant and plausible ones, at any one point in the interaction. We then perform a data collection and compare our dataset to a similar dataset collected in a more controlled lab setting with a single Wizard BIBREF4 and discuss the advantages/disadvantages of both approaches. Finally, we present future work",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What contextual features are used?",
    "llm_answer": "Automatically extracted contextual keywords, jointly learned during training.\n",
    "context": [
      {
        "id": "d2c5139d-1197-422c-be2b-c03481424b3e",
        "metadata": {
          "vector_store_key": "1907.02636-5",
          "chunk_id": 56,
          "document_id": "1907.02636",
          "start_idx": 20055,
          "end_idx": 20644
        },
        "page_content": "14 , we initialize the contextual features of each given token using the automatically extracted contextual keywords and jointly learn them during the process of training with the whole ANN model. To prove the effectiveness of the contextual features, we visualize the learned weights martix of each contextual keyword of contextual feature and show several examples in Fig. FIGREF28 . Each row of the matrix in each plot indicates the weights of contextual keywords for the given tokens. From this we see which contextual keyword were considered more important to represent the contextual",
        "type": "Document"
      },
      {
        "id": "51095457-b7c6-4ce1-af66-529cbf450f0d",
        "metadata": {
          "vector_store_key": "2002.00652-4",
          "chunk_id": 60,
          "document_id": "2002.00652",
          "start_idx": 19012,
          "end_idx": 19544
        },
        "page_content": " context modeling methods in the task of SPC are not as effective as expected, since they do not show a significant advantage over the simple concatenation method. By a careful investigation on contextual phenomena, we summarize them in multiple hierarchies. Roughly, there are three kinds of contextual phenomena in questions: semantically complete, coreference and ellipsis. Semantically complete means a question can reflect all the meaning of its corresponding SQL. Coreference means a question contains pronouns, while ellipsis",
        "type": "Document"
      },
      {
        "id": "09d21daf-f500-4b2f-adb0-e19c2170d7f7",
        "metadata": {
          "vector_store_key": "1710.01492-4",
          "chunk_id": 48,
          "document_id": "1710.01492",
          "start_idx": 17786,
          "end_idx": 18151
        },
        "page_content": "52 . Features. Traditionally, systems for Sentiment Analysis on Twitter have relied on handcrafted features derived from word-level (e.g., great, freshly roasted coffee, becoming president) and character-level INLINEFORM0 -grams (e.g., bec, beco, comin, oming), stems (e.g., becom), lemmata (e.g., become, roast), punctuation (e.g., exclamation and question marks),",
        "type": "Document"
      },
      {
        "id": "a1f68157-644d-414a-a756-12980327ead3",
        "metadata": {
          "vector_store_key": "1910.07181-9",
          "chunk_id": 38,
          "document_id": "1910.07181",
          "start_idx": 12787,
          "end_idx": 13094
        },
        "page_content": " contexts $\\mathcal {C} = \\lbrace C_1, \\ldots , C_m\\rbrace $ and the corresponding embeddings $v_{(w,C_1)}, \\ldots , v_{(w,C_m)}$, we apply a self-attention mechanism to all embeddings, allowing the model to distinguish informative contexts from uninformative ones. The final embedding $v_{(w, \\mathcal {C})",
        "type": "Document"
      },
      {
        "id": "3f563cd5-d442-40de-a17f-ada9182eebc1",
        "metadata": {
          "vector_store_key": "1910.09399-6",
          "chunk_id": 3,
          "document_id": "1910.09399",
          "start_idx": 1262,
          "end_idx": 1819
        },
        "page_content": " shown in Figure FIGREF4. In order to connect text descriptions to images, one could use correlation between keywords (or keyphrase) & images that identifies informative and \u201cpicturable\u201d text units; then, these units would search for the most likely image parts conditioned on the text, eventually optimizing the picture layout conditioned on both the text and the image parts. Such methods often integrated multiple artificial intelligence key components, including natural language processing, computer vision, computer graphics, and machine learning. The",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the data in RAFAEL labelled?",
    "llm_answer": "The authors projected word-sense annotations of publicly available corpora to supersenses and applied perceptron-trained Hidden Markov Model for sequence classification.  They also use labels such as plant, animal, or shape.\n",
    "context": [
      {
        "id": "3c270ff7-d313-4bf7-b299-0c2fc0546eb3",
        "metadata": {
          "vector_store_key": "1605.08675-9",
          "chunk_id": 13,
          "document_id": "1605.08675",
          "start_idx": 5272,
          "end_idx": 5770
        },
        "page_content": " a separate test set, previously unused in development, and is checked manually. The results are shown in section SECREF93 and discussed in chapter SECREF6 . Finally, chapter SECREF7 concludes the paper. As stated in previous chapter, RAFAEL is a computer system solving a task of Polish text-based, open-domain, factoid question answering. It means that provided questions, knowledge base and returned answers are expressed in Polish and may belong to any domain. The system analyses the knowledge",
        "type": "Document"
      },
      {
        "id": "6a6ae44e-bdd4-43d1-8f95-a4e99b4c0d0c",
        "metadata": {
          "vector_store_key": "1605.08675-9",
          "chunk_id": 36,
          "document_id": "1605.08675",
          "start_idx": 14177,
          "end_idx": 14672
        },
        "page_content": ", but also other labels, such as plant, animal or shape. The authors projected word-sense annotations of publicly available corpora to supersenses and applied perceptron-trained Hidden Markov Model for sequence classification, obtaining precision and recall around 77 per cent. A general architectural scheme of RAFAEL (figure FIGREF11 ) has been inspired by similar systems developed for English; for examples see works by BIBREF22 and BIBREF23 . Two of the steps in the diagram concern offline",
        "type": "Document"
      },
      {
        "id": "b5549316-ec79-4936-912b-69fc3a2b61bc",
        "metadata": {
          "vector_store_key": "1605.08675-9",
          "chunk_id": 64,
          "document_id": "1605.08675",
          "start_idx": 24641,
          "end_idx": 25068
        },
        "page_content": " named entity recognition in RAFAEL, including a description of Quant and a mapping between question types and named entity types available in NERF and Liner2. An alternative being in focus of this work, i.e. DeepER approach, is thorougly discussed in chapter SECREF3 . RAFAEL may use any of the two approaches to entity recognition: NER (via NERF, Liner2 and Quant) or novel DeepER; this choice affects its overall performance",
        "type": "Document"
      },
      {
        "id": "de9670f5-5780-4823-b501-04079e15748a",
        "metadata": {
          "vector_store_key": "1605.08675-9",
          "chunk_id": 113,
          "document_id": "1605.08675",
          "start_idx": 41672,
          "end_idx": 42162
        },
        "page_content": "per-synset recall 88.15 per cent), which instead includes 14 false synsets. Evaluation of RAFAEL is typical for factoid QA systems: given a knowledge base and and questions, its responses are compared to the expected ones, prepared in advance. Section SECREF80 describes data used in this procedure, whereas section SECREF87 explains how an automatic evaluation is possible without human labour. The Polish Wikipedia serves as a knowledge base. It has been downloaded from a project site as",
        "type": "Document"
      },
      {
        "id": "7df48f8d-fc66-42c3-afdf-c71888dfcae4",
        "metadata": {
          "vector_store_key": "1810.06743-4",
          "chunk_id": 17,
          "document_id": "1810.06743",
          "start_idx": 6580,
          "end_idx": 7020
        },
        "page_content": "ational in some languages. For example, the Czech UD guidance uses a Coll value for the Number feature to denote mass nouns (for example, \"lidstvo\" \"humankind\" from the root \"lid\" \"people\"). UD represents a confederation of datasets BIBREF8 annotated with dependency relationships (which are not the focus of this work) and morphosyntactic descriptions. Each dataset is an annotated treebank, making it a resource of token-level annotations",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the fluctuation in the sense of the word and its neighbors measured?",
    "llm_answer": "Using the concept of surrounding uniformity, which roughly corresponds to statistical fluctuation in the vectors of neighboring words.\n",
    "context": [
      {
        "id": "b191133e-818d-43e8-aaab-2b1811922c18",
        "metadata": {
          "vector_store_key": "1709.08858-5",
          "chunk_id": 2,
          "document_id": "1709.08858",
          "start_idx": 759,
          "end_idx": 1292
        },
        "page_content": " the major sense, which is most frequently used. The information about a word's minor sense is subtle, and the effect of a minor sense is difficult to distinguish from statistical fluctuation. To measure the effect of a minor sense, this paper proposes to use the concept of surrounding uniformity. The surrounding uniformity roughly corresponds to statistical fluctuation in the vectors that correspond to the words in the neighbor. We have found that there is a difference in the surrounding uniformity between a monosemic word and",
        "type": "Document"
      },
      {
        "id": "98c3ce53-6b99-4a52-9869-07def543fafa",
        "metadata": {
          "vector_store_key": "1709.08858-5",
          "chunk_id": 46,
          "document_id": "1709.08858",
          "start_idx": 15039,
          "end_idx": 15552
        },
        "page_content": " the accuracy of the test. To make the statistical test more accurate, we need more samples from the neighbors. At the same time, since we assume that we can measure the statistical fluctuation from the neighbors, we need to exclude words of a different nature from the neighbors. It is natural that the right number for a neighbor may be different according to the word. The number that we choose is the minimum value for the statistical test, and has room to adjust for improvement. We computed the neighbor and",
        "type": "Document"
      },
      {
        "id": "c16aa4ca-699b-4d17-a2af-904688a6f12d",
        "metadata": {
          "vector_store_key": "1709.08858-0",
          "chunk_id": 0,
          "document_id": "1709.08858",
          "start_idx": 0,
          "end_idx": 460
        },
        "page_content": "Distributed representation of word sense provides us with the ability to perform several operations on the word. One of the most important operations on a word is to obtain the set of words whose meaning is similar to the word, or whose usage in text is similar to the word. We call this set the neighbor of the word. When a word has several senses, it is called a polysemic word. When a word has only one sense, it is called a monosemic word. We have observed",
        "type": "Document"
      },
      {
        "id": "a8f2463a-5bc0-4fe2-9c16-37943e455c4c",
        "metadata": {
          "vector_store_key": "1910.12618-2",
          "chunk_id": 84,
          "document_id": "1910.12618",
          "start_idx": 33113,
          "end_idx": 33548
        },
        "page_content": " sense) and on all the available data (i.e. everything is used as training) to compensate for the increased size of the vocabulary. We then calculated the distance of a few prominent words to the others. The analysis of the average cosine distance over $B=10$ runs for three major words is given by tables TABREF38 and TABREF39, and three other examples are given in the appendix tables TABREF57 and TABREF58. The first row corresponds",
        "type": "Document"
      },
      {
        "id": "3d93021c-51df-439b-b8b6-6bfddcb0fa0a",
        "metadata": {
          "vector_store_key": "1911.12579-3",
          "chunk_id": 75,
          "document_id": "1911.12579",
          "start_idx": 27139,
          "end_idx": 27618
        },
        "page_content": "s in words along with their percentage in the developed corpus (see Table TABREF57). The bi-gram words are most frequent, mostly consists of stop words and secondly, 4-gram words have a higher frequency. The word frequency count is an observation of word occurrences in the text. The commonly used words are considered to be with higher frequency, such as the word \u201cthe\" in English. Similarly, the frequency of rarely used words to be lower. Such frequencies can be calculated at",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does SPNet outperforms state-of-the-art abstractive summarization methods on evaluation metrics?",
    "llm_answer": "SPNet outperforms Pointer-Generator, Transformer, and other baseline methods by a significant margin.\n",
    "context": [
      {
        "id": "4da4ebe4-bd6a-43f4-9aaf-a2148c90d115",
        "metadata": {
          "vector_store_key": "1910.00825-4",
          "chunk_id": 46,
          "document_id": "1910.00825",
          "start_idx": 16549,
          "end_idx": 17119
        },
        "page_content": " over Pointer-Generator but is having a higher cost on training time and computing resources. We observe that SPNet outperforms other methods in all the automatic evaluation metrics with a big margin, as it incorporates all the three semantic scaffolds. Semantic slot contributes the most to SPNet's increased performance, bringing the largest increase on all automatic evaluation metrics. We also perform human evaluation to verify if our method's increased performance on automatic evaluation metrics entails better human perceived quality. We randomly select 100 test",
        "type": "Document"
      },
      {
        "id": "9170b5f1-a175-49e8-a9d5-a1353a026d76",
        "metadata": {
          "vector_store_key": "1910.00825-1",
          "chunk_id": 56,
          "document_id": "1910.00825",
          "start_idx": 20655,
          "end_idx": 21208
        },
        "page_content": " as the semantic scaffolds to improve abstractive summary quality. We also propose an automatic evaluation metric CIC that considers semantic slot relevance to serve as a complementary metric to ROUGE. SPNet outperforms baseline methods in both automatic and human evaluation metrics. It suggests that involving semantic scaffolds efficiently improves abstractive summarization quality in the dialog scene. Moreover, we can easily extend SPNet to other summarization tasks. We plan to apply semantic slot scaffold to news summarization. Specifically, we",
        "type": "Document"
      },
      {
        "id": "2f6bacce-bebd-41c2-b12b-afb3df3001f9",
        "metadata": {
          "vector_store_key": "1910.00825-4",
          "chunk_id": 50,
          "document_id": "1910.00825",
          "start_idx": 18207,
          "end_idx": 18733
        },
        "page_content": " This motivates us to collect a dialog summarization dataset with high-quality human-written summaries in the future. Results in the ranking evaluation show more differences between different summaries. SPNet outperforms Pointer-Generator with a large margin. Its performance is relatively close to the ground truth summary. Table TABREF25 shows an example summary from all models along with ground truth summary. We observe that Pointer-Generator ignores some essential fragments, such as the restaurant booking information (",
        "type": "Document"
      },
      {
        "id": "36c5a020-5324-4007-a357-3257f036e993",
        "metadata": {
          "vector_store_key": "1910.00825-1",
          "chunk_id": 6,
          "document_id": "1910.00825",
          "start_idx": 2432,
          "end_idx": 2913
        },
        "page_content": " delexicalized utterances for producing delexicalized summary, and fills in slot values to generate complete summary. Finally, we incorporate dialog domain scaffold by jointly optimizing dialog domain classification task along with the summarization task. We evaluate SPNet with both automatic and human evaluation metrics on MultiWOZ. SPNet outperforms Pointer-Generator BIBREF5 and Transformer BIBREF6 on all the metrics. BIBREF7 first applied modern neural models to abstractive",
        "type": "Document"
      },
      {
        "id": "6c111de0-4fbf-4297-aeaa-6b0974409310",
        "metadata": {
          "vector_store_key": "1910.00825-4",
          "chunk_id": 48,
          "document_id": "1910.00825",
          "start_idx": 17413,
          "end_idx": 17884
        },
        "page_content": " summary on three indicators: relevance, conciseness and readability on a 1 to 5 scale, and rank the summary pair (tie allowed). We present human evaluation results in Table TABREF27. In the scoring part, our model outperforms Pointer-Generator in all three evaluation metrics. SPNet scored better than Pointer-Generator on relevance and readability. All generated summaries are relatively concise; therefore, they score very similar in conciseness. Ground truth is still",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is it expected to have speaker role, semantic slot and dialog domain annotations in real world datasets?",
    "llm_answer": "Yes.\n",
    "context": [
      {
        "id": "9fd2a84a-5fc2-4574-ad01-5d631d4c8208",
        "metadata": {
          "vector_store_key": "1910.00825-1",
          "chunk_id": 54,
          "document_id": "1910.00825",
          "start_idx": 19824,
          "end_idx": 20350
        },
        "page_content": " of SPNet still needs extra annotations for semantic scaffolds. For a dialog dataset, speaker role scaffold is a natural pattern for modeling. Most multi-domain dialog corpus has the domain annotation. While for texts, for example news, its topic categorization such as sports or entertainment can be used as domain annotation. We find that semantic slot scaffold brings the most significant improvement, but it is seldom explicitly annotated. However, the semantic slot scaffold can be relaxed to any critical entities in the",
        "type": "Document"
      },
      {
        "id": "51523ef3-93a3-4b8a-824b-2a8ff4851c86",
        "metadata": {
          "vector_store_key": "1910.00825-1",
          "chunk_id": 55,
          "document_id": "1910.00825",
          "start_idx": 20141,
          "end_idx": 20655
        },
        "page_content": " explicitly annotated. However, the semantic slot scaffold can be relaxed to any critical entities in the corpus, such as team name in sports news or professional terminology in a technical meeting. We adapt a dialog generation dataset, MultiWOZ to an abstractive dialog summarization dataset. We propose SPNet, an end-to-end model that incorporates the speaker role, semantic slot and dialog domain as the semantic scaffolds to improve abstractive summary quality. We also propose an automatic evaluation metric C",
        "type": "Document"
      },
      {
        "id": "951915de-69d3-4157-8e5a-1af168ea552e",
        "metadata": {
          "vector_store_key": "2002.11893-1",
          "chunk_id": 47,
          "document_id": "2002.11893",
          "start_idx": 17952,
          "end_idx": 18486
        },
        "page_content": " We also obtained a binary label for each semantic tuple in the user state, which indicates whether this semantic tuple has been selected to be expressed by the user. This annotation directly illustrates the progress of the conversation. To evaluate the quality of the annotation of dialogue acts and states (both user and system states), three experts were employed to manually annotate dialogue acts and states for the same 50 dialogues (806 utterances), 10 for each goal type (see Section SECREF4). Since dialogue act annotation is",
        "type": "Document"
      },
      {
        "id": "f9e9e3ec-e213-4cd5-ae8e-8d986c9fd2db",
        "metadata": {
          "vector_store_key": "2002.11893-1",
          "chunk_id": 22,
          "document_id": "2002.11893",
          "start_idx": 8390,
          "end_idx": 8922
        },
        "page_content": " before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states. Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three",
        "type": "Document"
      },
      {
        "id": "e9bdf522-3b89-4c8e-8e6f-8c96d13bd00f",
        "metadata": {
          "vector_store_key": "1910.00825-9",
          "chunk_id": 21,
          "document_id": "1910.00825",
          "start_idx": 7742,
          "end_idx": 8159
        },
        "page_content": " roles, incorporating semantic slot scaffold and dialog domain scaffold. Our encoder-decoder framework employs separate encoding for different speakers in the dialog. User utterances $x_t^{usr}$ and system utterances $x_t^{sys}$ are fed into a user encoder and a system encoder separately to obtain encoder hidden states $h_{i}^{usr}$ and $h_{i}^{sys}$ . The attention distributions and context vectors are calculated",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does new evaluation metric considers critical informative entities?",
    "llm_answer": "The new metric, Critical Information Completeness (CIC), is a recall of semantic slot information between a candidate and reference summary.  It considers delexicalized values in the reference summary.\n",
    "context": [
      {
        "id": "e936f641-074f-4efc-a3ad-87d3c7cee81b",
        "metadata": {
          "vector_store_key": "1910.00825-7",
          "chunk_id": 37,
          "document_id": "1910.00825",
          "start_idx": 13254,
          "end_idx": 13678
        },
        "page_content": " However, ROUGE did not take this into consideration. To address this drawback in ROUGE, we propose a new evaluation metric: Critical Information Completeness (CIC). Formally, CIC is a recall of semantic slot information between a candidate summary and a reference summary. CIC is defined as follows: where $V$ stands for a set of delexicalized values in the reference summary, $Count_{match}(v)$ is the number of values co-",
        "type": "Document"
      },
      {
        "id": "8135afc2-1b5e-4ad4-968f-868afaac9ba6",
        "metadata": {
          "vector_store_key": "1905.08949-9",
          "chunk_id": 30,
          "document_id": "1905.08949",
          "start_idx": 11311,
          "end_idx": 11823
        },
        "page_content": ". To overcome this, BIBREF46 proposed a new metric to evaluate the \u201canswerability\u201d of a question by calculating the scores for several question-specific factors, including question type, content words, function words, and named entities. However, as it is newly proposed, it has not been applied to evaluate any NQG system yet. To accurately measure what makes a good question, especially deep questions, improved evaluation schemes are required to specifically investigate the mechanism of question asking. Many",
        "type": "Document"
      },
      {
        "id": "cc46f099-eec3-4c72-b5ce-b58340f9e2ea",
        "metadata": {
          "vector_store_key": "1909.00578-4",
          "chunk_id": 7,
          "document_id": "1909.00578",
          "start_idx": 2625,
          "end_idx": 3083
        },
        "page_content": " MT evaluation metrics BIBREF4, BIBREF18. QE models for MT make use of large post-edited datasets, and apply machine learning methods to predict post-editing effort scores and quality (good/bad) labels. We apply QE to summarization, focusing on linguistic qualities that reflect the readability and fluency of the generated texts. Since no post-edited datasets \u2013 like the ones used in MT \u2013 are available for summarization, we use instead the ratings assigned",
        "type": "Document"
      },
      {
        "id": "b3b0115c-601b-4949-87fc-6d1dd9619260",
        "metadata": {
          "vector_store_key": "1911.03894-2",
          "chunk_id": 37,
          "document_id": "1911.03894",
          "start_idx": 13285,
          "end_idx": 13679
        },
        "page_content": "\ufffd, 3357 of \u201cCompany\u201d, 67 of \u201cProduct\u201d, 15 of \u201cPOI\u201d (Point of Interest) and 29 of \u201cFictional Character\u201d. A large proportion of the entity mentions in the treebank are multi-word entities. For NER we therefore report the 3 metrics that are commonly used to evaluate models: precision, recall, and F1 score. Here precision measures the percentage of entities found by the system that are correctly",
        "type": "Document"
      },
      {
        "id": "dc58289f-90f2-4f31-9a31-296d00bb01a9",
        "metadata": {
          "vector_store_key": "1709.10217-2",
          "chunk_id": 11,
          "document_id": "1709.10217",
          "start_idx": 4141,
          "end_idx": 4657
        },
        "page_content": ", in which only the released data can be used for training and developing. The other is an open evaluation that allow to collect external data for training and developing. For task 1, we use F1-score as evaluation metric. For the task-oriented dialogue systems, the best way for evaluation is to use the online human-computer dialogue. After finishing an online human-computer dialogue with a dialogue system, the human then manually evaluate the system by using the metrics of user satisfaction degree, dialogue flu",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are state of the art methods MMM is compared to?",
    "llm_answer": "MAN classifier, FCNN classifier, MCB, NMN, MLP, MTMSNlarge, MTMSNbase, large-squad, MC160, MC500.\n",
    "context": [
      {
        "id": "fa5e0d38-94cf-4624-b800-8cb3a874bb5f",
        "metadata": {
          "vector_store_key": "1910.00458-5",
          "chunk_id": 65,
          "document_id": "1910.00458",
          "start_idx": 22388,
          "end_idx": 22877
        },
        "page_content": " control experiments for investigation. The results in Table TABREF27, show that casting the fine-tuning process on three datasets into separate stages performs better, indicating that multi-stage training is also necessary. This verifies our MMM framework with coarse-tuning on out-of-domain datasets and fine-tuning on in-domain datesets. Previous results show that the MAN classifier shows improvement compared with the FCNN classifier, but we are also interested in how the performance",
        "type": "Document"
      },
      {
        "id": "b8463b23-0372-4422-ba8d-e82788fecd2b",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 61,
          "document_id": "1909.13375",
          "start_idx": 20788,
          "end_idx": 21299
        },
        "page_content": " For the other types, most of the improvement came from better preprocessing. A more detailed discussion could be found in Section (SECREF36). Notice that different BERTlarge models were used, so the comparison is less direct. Overall, our large models exhibits similar results to those of MTMSNlarge. For multi-span questions we achieve a significantly better performance. While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still",
        "type": "Document"
      },
      {
        "id": "8836cac5-0770-4b24-b87a-3d1fa58536ca",
        "metadata": {
          "vector_store_key": "1703.09684-5",
          "chunk_id": 65,
          "document_id": "1703.09684",
          "start_idx": 24411,
          "end_idx": 24828
        },
        "page_content": " simple baseline methods routinely exceed more complex methods using simple accuracy BIBREF7 , BIBREF8 , BIBREF19 . On TDIUC, MLP surpasses MCB and NMN in terms of simple accuracy, but a closer inspection reveals that MLP's score is highly determined by performance on categories with a large number of examples, such as `absurd' and `object presence.' Using MPT, we find that both NMN and MCB outperform MLP. Inspect",
        "type": "Document"
      },
      {
        "id": "439e15bb-4608-47c1-84bf-d30ecc52c819",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 62,
          "document_id": "1909.13375",
          "start_idx": 21089,
          "end_idx": 21507
        },
        "page_content": " available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1. All that, while keeping in mind we compare a base model to a large model (for reference, note the 8 point improvement between MTMSNbase and MTMSNlarge in both EM and F1). Our best model, large-squad, exhibits a huge improvement of 29.7 in EM and",
        "type": "Document"
      },
      {
        "id": "ad7c5810-296f-46af-a1e6-4b1ede1b1854",
        "metadata": {
          "vector_store_key": "1910.00458-1",
          "chunk_id": 42,
          "document_id": "1910.00458",
          "start_idx": 13934,
          "end_idx": 14439
        },
        "page_content": " huge amount of trainable parameters. However, by leveraging the transfer learning techniques we proposed, we can significantly improve the generalization capability of BERT and RoBERTa models on the small datasets so that the best performance of MC160 can even surpass that of MC500. This demonstrates the effectiveness of our method. To better understand why MMM can be successful, we conducted an ablation study be removing one feature at a time on the BERT-Base model. The results are shown in Table T",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the problems related to ambiguity in PICO sentence prediction tasks?",
    "llm_answer": "The assignment of a single label limits the quality of predictions for use in real-world reviewing tasks.\n",
    "context": [
      {
        "id": "d3a4eb9b-b3fa-41cd-a0d2-3039ff5c49c3",
        "metadata": {
          "vector_store_key": "2001.11268-2",
          "chunk_id": 26,
          "document_id": "2001.11268",
          "start_idx": 10281,
          "end_idx": 10811
        },
        "page_content": " noise in the evaluation because as the network learns, it can assign correct tags to wrongly labelled data. We also show that sentence labels are often ambiguous, and that the assignment of a single label limits the quality of the predictions for their use in real-world reviewing tasks. We acknowledge that the assignment of classes such as `Results' or `Conclusions' to sentences is potentially valuable for many use-cases. However, those sentences can contain additional information related to the PICO classes of interest. In",
        "type": "Document"
      },
      {
        "id": "77ad03a8-09f0-4b40-881e-a827ccaffe86",
        "metadata": {
          "vector_store_key": "2001.11268-2",
          "chunk_id": 66,
          "document_id": "2001.11268",
          "start_idx": 26100,
          "end_idx": 26630
        },
        "page_content": " included an entity, whereas its sub-classes age, gender, condition and size averaged 10% each. In the remaining classes, these percentages were higher. F1 scores for correctly detecting that a sentence includes no PICO element exceeded 0.92 in all classes. This indicates that the addition of impossible answer elements was successful, and that the model learned a representation of how to discriminate PICO contexts. The scores for correctly predicting PICOs in positive scenarios are lower. These results are presented in Table",
        "type": "Document"
      },
      {
        "id": "fcdfa33b-571f-4df3-ab05-d1ff5db94410",
        "metadata": {
          "vector_store_key": "2001.11268-2",
          "chunk_id": 28,
          "document_id": "2001.11268",
          "start_idx": 11105,
          "end_idx": 11632
        },
        "page_content": " weakness of predicting labels for whole sentences is the practical usability of the predictions. We will show sentence highlighting as a potential use-case for focusing reader's attention to passages of interest. However, the data obtained through this method are not fine-grained enough for usage in data extraction, or for the use in pipelines for automated evidence synthesis. Therefore, we expand our experiments to include QA-BERT, a question-answering model that predicts the locations of PICO entities within sentences.",
        "type": "Document"
      },
      {
        "id": "30e32272-b56b-4177-aec0-5abaab1805b7",
        "metadata": {
          "vector_store_key": "2001.11268-2",
          "chunk_id": 32,
          "document_id": "2001.11268",
          "start_idx": 12852,
          "end_idx": 13416
        },
        "page_content": "-crafted rules. Our fine-tuning approach shows that an abstract as context, together with a combination of annotated PICO entities and SQuAD data can result in a system that outperforms contemporary entity recognition systems, while retaining general reading comprehension capabilities. A language processing model's performance is limited by its capability of representing linguistic concepts numerically. In this preliminary experiment, we used the PubMed corpus for sentence classification to show the quality of PICO sentence embeddings retrieved from BERT. We",
        "type": "Document"
      },
      {
        "id": "4b101766-45d9-494c-933f-e72f8f63daa9",
        "metadata": {
          "vector_store_key": "2001.11268-2",
          "chunk_id": 29,
          "document_id": "2001.11268",
          "start_idx": 11632,
          "end_idx": 12145
        },
        "page_content": "T, a question-answering model that predicts the locations of PICO entities within sentences. In this work we investigate state-of-the-art methods for language modelling and sentence classification. Our contributions are centred around developing transformer-based fine-tuning approaches tailored to SR tasks. We compare our sentence classification with the LSTM baseline and evaluate the biggest set of PICO sentence data available at this point BIBREF13. We demonstrate that models based on the BERT architecture",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is knowledge stored in the memory?",
    "llm_answer": "The context describes storing information in memory in a hierarchical structure, with recent memories prioritized, and using memory slots to model entities and relations in text.  It also mentions using LSTM networks to encode contextual word sequences and entity descriptions as fixed-size vectors.\n",
    "context": [
      {
        "id": "77d55b59-72b6-4116-8abc-ece0110b73ac",
        "metadata": {
          "vector_store_key": "1909.00754-1",
          "chunk_id": 46,
          "document_id": "1909.00754",
          "start_idx": 13218,
          "end_idx": 13687
        },
        "page_content": " the belief, is decided empirically. We can interpret this hierarchical structure as the internal order for the memory processing, since from the daily life experience, people tend to attend to the most contemporary memories (system/user utterance) first and then attend to the older history (belief states). All of the parameters are shared between the attention modules. The concatenated working memory, $\\mathbf {r}_0$ , is then fed into a Multi-Layer Perceptron (ML",
        "type": "Document"
      },
      {
        "id": "10603d84-ec71-4909-bba5-f965ddbfb493",
        "metadata": {
          "vector_store_key": "1706.07179-4",
          "chunk_id": 3,
          "document_id": "1706.07179",
          "start_idx": 1224,
          "end_idx": 1706
        },
        "page_content": " question-answering, we instead make an attempt at an end-to-end approach which directly models the entities and relations in the text as memory slots. While incorporating existing knowledge (from curated knowledge bases) for the purpose of question-answering BIBREF11 , BIBREF8 , BIBREF15 is an important area of research, we consider the simpler setting where all the information is contained within the text itself \u2013 which is the approach taken by many recent memory based neural",
        "type": "Document"
      },
      {
        "id": "6572b77a-afc4-4ebd-92fa-d31183ad68eb",
        "metadata": {
          "vector_store_key": "2001.06888-6",
          "chunk_id": 24,
          "document_id": "2001.06888",
          "start_idx": 9500,
          "end_idx": 9914
        },
        "page_content": " on the most recent tokens. Instead of using RNN, authors used LSTM. The long and short term memory capability of these networks helps them to keep in memory what is important and forget what is not necessary to remember. Equation DISPLAY_FORM6 formulates forget-gate of an LSTM neural network, eq. shows input-gate, eq. notes output-gate and eq. presents memory-cell. Finally, eq. shows the hidden part of an LSTM",
        "type": "Document"
      },
      {
        "id": "318360a8-e1ec-4db8-8265-87747a8f5795",
        "metadata": {
          "vector_store_key": "1902.00330-7",
          "chunk_id": 21,
          "document_id": "1902.00330",
          "start_idx": 8140,
          "end_idx": 8466
        },
        "page_content": "IBREF11 . Then, we use Long Short-Term Memory (LSTM) networks to encode the contextual word sequence $\\lbrace w_c^1, w_c^2,..., w_c^n\\rbrace $ as a fixed-size vector $V_{m_t}$ . The description of entity is encoded as $D_{e_t^i}$ in the same way. Apart from the description of entity, there are many other valuable information",
        "type": "Document"
      },
      {
        "id": "180d3da8-03d8-4ec4-81d5-52d57f859601",
        "metadata": {
          "vector_store_key": "1703.04617-8",
          "chunk_id": 41,
          "document_id": "1703.04617",
          "start_idx": 11824,
          "end_idx": 12097
        },
        "page_content": "L$ for the left child input and $\\mathbf {f}_t^R$ for the right. The memory cell $\\mathbf {c}_t$ considers each child's cell vector, $\\mathbf {c}_{t-1}^L$ and $\\mathbf {c}_{t-1}^R$ , which are gated by the left forget gate $\\mathbf {f}_t^L$ and right forget gate $\\mathbf {",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they measure the diversity of inferences?",
    "llm_answer": "The context describes how NE and token/type diversity are measured for corpora, but does not describe how diversity of inferences is measured.  The answer is not found in the context.\n",
    "context": [
      {
        "id": "ffee1133-fcf9-4d2d-a845-b43568ac2068",
        "metadata": {
          "vector_store_key": "1701.02877-3",
          "chunk_id": 31,
          "document_id": "1701.02877",
          "start_idx": 11428,
          "end_idx": 11888
        },
        "page_content": " training data size for subsequent experiments. In order to compare corpus diversity across genres, we measure NE and token/type diversity (following e.g. BIBREF2 ). Note that types are the unique tokens, so the ratio can be understood as ratio of total tokens to unique ones. Table 4 shows the ratios between the number of NEs and the number of unique NEs per corpus, while Table 5 reports the token/type ratios. The lower those ratios are, the more diverse a",
        "type": "Document"
      },
      {
        "id": "f9db1bf0-c1ad-443d-892f-3f705c167159",
        "metadata": {
          "vector_store_key": "1911.03243-1",
          "chunk_id": 19,
          "document_id": "1911.03243",
          "start_idx": 7494,
          "end_idx": 8068
        },
        "page_content": " and modality extracted from the question. Final reported numbers on labelled argument detection rates are based on bipartite aligned arguments passing Strict-Match. We later manually estimate the rate of correct equivalences missed by this conservative method. As we will see, our evaluation heuristics, adapted from those in BIBREF5, significantly underestimate agreement between annotations, hence reflecting performance lower bounds. Devising more tight evaluation measures remains a challenge for future research. We extend our metric for evaluating manual or automatic",
        "type": "Document"
      },
      {
        "id": "52feea02-590d-4a68-94e6-58ff772fe940",
        "metadata": {
          "vector_store_key": "1701.02877-3",
          "chunk_id": 32,
          "document_id": "1701.02877",
          "start_idx": 11679,
          "end_idx": 12145
        },
        "page_content": " while Table 5 reports the token/type ratios. The lower those ratios are, the more diverse a corpus is. While token/type ratios also include tokens which are NEs, they are a good measure of broader linguistic diversity. Aside from these metrics, there are other factors which contribute to corpus diversity, including how big a corpus is and how well sampled it is, e.g. if a corpus is only about one story, it should not be surprising to see a high token/type ratio",
        "type": "Document"
      },
      {
        "id": "ab8904f6-9f7c-4e4d-85be-fc677f14aae4",
        "metadata": {
          "vector_store_key": "1911.03243-1",
          "chunk_id": 20,
          "document_id": "1911.03243",
          "start_idx": 7821,
          "end_idx": 8395
        },
        "page_content": " more tight evaluation measures remains a challenge for future research. We extend our metric for evaluating manual or automatic redundant annotations, like the Dense dataset or the parser in BIBREF5, which predicts argument spans independently of each other. To that end, we ignore predicted arguments that match ground-truth but are not selected by the bipartite matching due to redundancy. After connecting unmatched predicted arguments that overlap, we count one false positive for every connected component to avoid penalizing precision too harshly when predictions are",
        "type": "Document"
      },
      {
        "id": "4cb07a25-7588-4684-b4ca-e2c9b06ca35f",
        "metadata": {
          "vector_store_key": "1708.05873-0",
          "chunk_id": 10,
          "document_id": "1708.05873",
          "start_idx": 4331,
          "end_idx": 4846
        },
        "page_content": " average proportion of a document discussing a topic. We assess the optimal number of topics that need to be specified for the STM analysis. We follow the recommendations of the original STM paper and focus on exclusivity and semantic coherence measures. BIBREF5 propose semantic coherence measure, which is closely related to point-wise mutual information measure posited by BIBREF6 to evaluate topic quality. BIBREF5 show that semantic coherence corresponds to expert judgments and more general human judgments in",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much do they improve the accuracy of inferences over state-of-the-art methods?",
    "llm_answer": "The context mentions accuracy improvements for specific methods (e.g., 95.4%, 95.0%), but does not quantify the improvement *over* state-of-the-art methods.  The answer is not found in the context.\n",
    "context": [
      {
        "id": "71057641-acb5-4637-8416-34d78c2bf17b",
        "metadata": {
          "vector_store_key": "1703.09684-5",
          "chunk_id": 64,
          "document_id": "1703.09684",
          "start_idx": 23901,
          "end_idx": 24411
        },
        "page_content": " lower amounts of training data, bias, or limitations of the models. We next investigate how much of the good performance is due to bias in the answer distribution, which N-MPT compensates for. One of our major aims was to compensate for the fact that algorithms can achieve high scores by simply learning to answer more populated and easier question-types. For existing datasets, earlier work has shown that simple baseline methods routinely exceed more complex methods using simple accuracy BIBREF7 , BIBREF8",
        "type": "Document"
      },
      {
        "id": "c270760a-3f97-41b4-84b6-68ddf24229ea",
        "metadata": {
          "vector_store_key": "1910.04269-4",
          "chunk_id": 54,
          "document_id": "1910.04269",
          "start_idx": 20617,
          "end_idx": 21192
        },
        "page_content": " robust to variations which might be present in real world scenarios. There can be further exploration of various feature extraction techniques like Constant-Q transform and Fast Fourier Transform and assessment of their impact on Language Identification. There can be further improvements in neural network architectures like concatenating the high level features obtained from 1D-ConvNet and 2D-ConvNet, before performing classification. There can be experiments using deeper networks with skip connections and Inception modules. These are known to have positively impacted",
        "type": "Document"
      },
      {
        "id": "4141e328-746b-46a4-bc49-9940bb13feb7",
        "metadata": {
          "vector_store_key": "1906.06448-8",
          "chunk_id": 63,
          "document_id": "1906.06448",
          "start_idx": 23574,
          "end_idx": 24095
        },
        "page_content": ", among the 394 problems for which all models provided wrong answers, 244 problems are non-lexical inference problems. This indicates that some non-lexical inference problems are more difficult than lexical inference problems, though accuracy on non-lexical inference problems was better than that on lexical inference problems. One of our findings is that there is a type of downward inferences to which every model fails to provide correct answers. One such example is concerned with the contrast between few and a few.",
        "type": "Document"
      },
      {
        "id": "fc85b849-8b62-410f-8f70-2a1182aa13bc",
        "metadata": {
          "vector_store_key": "1910.04269-4",
          "chunk_id": 57,
          "document_id": "1910.04269",
          "start_idx": 21961,
          "end_idx": 22224
        },
        "page_content": " which is a recently developed technique to prevent over\ufb01tting on test data.This approach achieved an accuracy of 95.4%. We also analysed how attention mechanism and recurrent layers impact the performance of networks. This approach achieved an accuracy of 95.0%.",
        "type": "Document"
      },
      {
        "id": "fdd7adf4-4054-4d31-8561-7a58a60f7e1a",
        "metadata": {
          "vector_store_key": "1911.01799-2",
          "chunk_id": 3,
          "document_id": "1911.01799",
          "start_idx": 1384,
          "end_idx": 1883
        },
        "page_content": " (DNNs) in representation learning, these methods can remove unwanted uncertainties by propagating speech signals through the DNN layer by layer and retain speaker-relevant features only BIBREF9. Significant improvement in robustness has been achieved by the DNN-based approach BIBREF10, which makes it more suitable for applications in unconstrained conditions. The success of DNN-based methods, however, largely relies on a large amount of data, in particular data that involve the true complexity",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does the context-aware variational autoencoder learn event background information?",
    "llm_answer": "In the pretrain stage, CWVAE is trained on an auxiliary dataset with rich event context information.  The context-acquiring latent variable $z_c$ is directly conditioned on the context, allowing it to acquire background knowledge.  Then, a context-aware latent variable $z_{c'}$ learns background knowledge from $z_c$.\n",
    "context": [
      {
        "id": "c9b7142b-798b-47ae-912e-666a47d3169a",
        "metadata": {
          "vector_store_key": "1909.08824-7",
          "chunk_id": 21,
          "document_id": "1909.08824",
          "start_idx": 6888,
          "end_idx": 7354
        },
        "page_content": " we model the If-Then reasoning as a [(background), event]-target process. It means that in addition to the observed event, we also want to involve the event background knowledge (which can be learned from event contexts) to generate the reasonable targets. To this end, we propose a context-aware variational autoencoder (CWVAE), with two additional latent variables: a context-acquiring latent variable $z_c$ to directly acquire context information, and a context-",
        "type": "Document"
      },
      {
        "id": "33acd1ec-8790-4c80-8b69-9c2ac58bdd34",
        "metadata": {
          "vector_store_key": "1909.08824-7",
          "chunk_id": 22,
          "document_id": "1909.08824",
          "start_idx": 7178,
          "end_idx": 7635
        },
        "page_content": "-acquiring latent variable $z_c$ to directly acquire context information, and a context-aware latent variable $z_{c^{\\prime }}$ to learn background knowledge from $z_c$, as shown in Figure FIGREF6 (a). However, the event context information is absent in the Event2Mind and Atomic dataset. To learn from the external event context information, we design the following two-stage training procedure for CWVAE. Pretrain: Learning Event Background Knowledge from",
        "type": "Document"
      },
      {
        "id": "a825b75a-1de6-4937-8748-aa5837926f56",
        "metadata": {
          "vector_store_key": "1909.08824-7",
          "chunk_id": 6,
          "document_id": "1909.08824",
          "start_idx": 2049,
          "end_idx": 2530
        },
        "page_content": " problem and generate diversified inferences BIBREF8, BIBREF9. In addition to the traditional VAE structure, we introduces an extra context-aware latent variable in CWVAE to learn the event background knowledge. In the pretrain stage, CWVAE is trained on an auxiliary dataset (consists of three narrative story corpora and contains rich event background knowledge), to learn the event background information by using the context-aware latent variable. Subsequently, in the finetune",
        "type": "Document"
      },
      {
        "id": "cc685ac4-453d-48d9-880a-028c97737a56",
        "metadata": {
          "vector_store_key": "1909.08824-7",
          "chunk_id": 23,
          "document_id": "1909.08824",
          "start_idx": 7635,
          "end_idx": 8089
        },
        "page_content": " the following two-stage training procedure for CWVAE. Pretrain: Learning Event Background Knowledge from Auxiliary Dataset In the pretrain stage, CWVAE is trained on three narrative story corpora with rich event context information. As shown in Figure FIGREF6 (a), context-acquiring latent variable $z_c$ is directly conditioned on the context $c$. Hence, $z_c$ could be employed for acquiring background knowledge from event contexts. Then, we minimize",
        "type": "Document"
      },
      {
        "id": "0992a612-741f-47f0-9098-c5a63e80f933",
        "metadata": {
          "vector_store_key": "1909.08824-7",
          "chunk_id": 7,
          "document_id": "1909.08824",
          "start_idx": 2530,
          "end_idx": 3016
        },
        "page_content": " event background information by using the context-aware latent variable. Subsequently, in the finetune stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target (e.g., intents, reactions, etc.). Experiments on the Event2Mind and Atomic dataset show that our proposed approach outperforms baseline methods in both the accuracy and diversity of inferences. The code is released at https://github.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much improvement does their method get over the fine tuning baseline?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "6b72903d-8c60-4ff1-bef1-d2308552dfc3",
        "metadata": {
          "vector_store_key": "1905.13413-1",
          "chunk_id": 33,
          "document_id": "1905.13413",
          "start_idx": 10297,
          "end_idx": 10787
        },
        "page_content": " of using the confidence (E.q. 7 ) of the fine-tuned model to rerank the extractions of the base model (Rerank Only), and the end-to-end performance of the fine-tuned model in assertion generation (Generate). We found both settings lead to improved performance compared to the base model, which demonstrates that calibrating confidence using binary classification loss can improve the performance of both reranking and assertion generation. Finally, our proposed iterative learning approach",
        "type": "Document"
      },
      {
        "id": "8dd869b3-9450-4771-a958-b10b51a3ead8",
        "metadata": {
          "vector_store_key": "1910.10408-7",
          "chunk_id": 24,
          "document_id": "1910.10408",
          "start_idx": 9045,
          "end_idx": 9446
        },
        "page_content": ", fine-tuning a pre-trained network shows to improve performance in several NMT scenarios BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF24. For our length control approaches, we further propose to use fine-tuning an NMT model with length information, instead of training it from scratch. By adopting a fine-tuning strategy, we specifically aim; i) to decouple the performance of the baseline NMT model",
        "type": "Document"
      },
      {
        "id": "380c4e94-ec4a-40a5-8fa7-ffac7d40efa4",
        "metadata": {
          "vector_store_key": "1701.03214-0",
          "chunk_id": 9,
          "document_id": "1701.03214",
          "start_idx": 3413,
          "end_idx": 3829
        },
        "page_content": " fine tuning method is a combination of the above methods (shown in Figure 2 ). The training procedure is as follows: Train an NMT model on out-of-domain data till convergence. Resume training the NMT model from step 1 on a mix of in-domain and out-of-domain data (by oversampling the in-domain data) till convergence. By default, we utilize domain tags, but we also consider settings where we do not use them (i.e.,",
        "type": "Document"
      },
      {
        "id": "535a4ad4-3aff-4297-862c-94166676565b",
        "metadata": {
          "vector_store_key": "1911.03894-0",
          "chunk_id": 51,
          "document_id": "1911.03894",
          "start_idx": 18085,
          "end_idx": 18526
        },
        "page_content": " learning rates and batch sizes. We select the best model on the validation set out of the 30 first epochs. Although this might push the performances even further, for all tasks except NLI, we don't apply any regularisation techniques such as weight decay, learning rate warm-up or discriminative fine-tuning. We show that fine-tuning CamemBERT in a straight-forward manner leads to state-of-the-art results on most tasks and outperforms the",
        "type": "Document"
      },
      {
        "id": "42150962-2d02-417b-b4fa-d948c244b2d5",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 58,
          "document_id": "1909.13375",
          "start_idx": 19611,
          "end_idx": 20052
        },
        "page_content": " to gain further insights on the effect of the components that we ourselves introduced. For ease of performance comparison, we followed NABERT+'s training settings: we used the BERT Adam optimizer from huggingface with default settings and a learning rate of $1e^{-5}$. The only difference was that we used a batch size of 12. We trained our base model for 20 epochs. For the large models we used a batch size of 3 with a learning rate of $5",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much do they outpeform previous results on the word discrimination task?",
    "llm_answer": "The provided text states that the approach \"establishes a new state of the art for classifying hate speech, outperforming previous results by as much as 12 F1 points\", but does not provide any information on the word discrimination task.  Therefore, the answer is not found in the context.\n",
    "context": [
      {
        "id": "173a0784-e08f-4515-935f-df7356f82e92",
        "metadata": {
          "vector_store_key": "1906.06448-9",
          "chunk_id": 59,
          "document_id": "1906.06448",
          "start_idx": 21919,
          "end_idx": 22444
        },
        "page_content": " also indicates the difficulty of problems from linguistics publications. Regarding non-monotone problems collected via crowdsourcing, there are very few non-monotone problems, so accuracy is 100%. Adding non-monotone problems to our test set is left for future work. Table 9 shows the evaluation results by type of linguistic phenomenon. While accuracy on problems involving NPIs and conditionals was improved on both upward and downward inferences, accuracy on problems involving conjunction and disjunction was improved on",
        "type": "Document"
      },
      {
        "id": "478b30bf-2ed5-4ef8-837c-3cfdfedce040",
        "metadata": {
          "vector_store_key": "1611.02550-6",
          "chunk_id": 33,
          "document_id": "1611.02550",
          "start_idx": 12001,
          "end_idx": 12521
        },
        "page_content": " respects our end objective, and can use more data since neither fully labeled data nor any minimum number of examples of each word should be needed.   Our end goal is to improve performance on downstream tasks requiring accurate word discrimination. In this paper we use an intermediate task that more directly tests whether same- and different-word pairs have the expected relationship. and that allows us to compare to a variety of prior work. Specifically, we use the word discrimination task of Carlin et al. BIBREF",
        "type": "Document"
      },
      {
        "id": "cd45ee02-9258-4e62-b22f-cb36e39ab775",
        "metadata": {
          "vector_store_key": "1611.02550-6",
          "chunk_id": 34,
          "document_id": "1611.02550",
          "start_idx": 12326,
          "end_idx": 12820
        },
        "page_content": " prior work. Specifically, we use the word discrimination task of Carlin et al. BIBREF20 , which is similar to a query-by-example task where the word segmentations are known. The evaluation consists of determining, for each pair of evaluation segments, whether they are examples of the same or different words, and measuring performance via the average precision (AP). We do this by measuring the cosine similarity between their acoustic word embeddings and declaring them to be the same if the",
        "type": "Document"
      },
      {
        "id": "5a12d393-42a0-49b6-9217-840e2467ae17",
        "metadata": {
          "vector_store_key": "1809.10644-4",
          "chunk_id": 20,
          "document_id": "1809.10644",
          "start_idx": 7678,
          "end_idx": 8121
        },
        "page_content": ", the authors reported their trained best logistic regression model's results on their respective datasets. SR: Sexist/Racist BIBREF3 , HATE: Hate BIBREF4 HAR: Harassment BIBREF9  The approach we have developed establishes a new state of the art for classifying hate speech, outperforming previous results by as much as 12 F1 points. Table TABREF10 illustrates the robustness of our method, which often outperform previous results, measured by",
        "type": "Document"
      },
      {
        "id": "f86302ba-bcc2-4023-a4fd-82387ffa9e5f",
        "metadata": {
          "vector_store_key": "1904.08386-4",
          "chunk_id": 15,
          "document_id": "1904.08386",
          "start_idx": 6277,
          "end_idx": 6716
        },
        "page_content": " is surprising as BERT is a bigger model trained on data from books (among other domains). Both ELMo and BERT outperform GloVe, which intuitively makes sense because the latter do not model the order or structure of the words in each description. While the purity of our methods is higher than that of a random clustering, it is still far below 1. To provide additional context to these results, we now switch to our \u201codd-one-out\u201d task and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many paraphrases are generated per question?",
    "llm_answer": "10\n",
    "context": [
      {
        "id": "f146425b-899c-427d-b8af-b2dec6916cff",
        "metadata": {
          "vector_store_key": "1601.06068-4",
          "chunk_id": 68,
          "document_id": "1601.06068",
          "start_idx": 22413,
          "end_idx": 22941
        },
        "page_content": " to generate paraphrases for questions, and applied it to a question answering system based on semantic parsing. We showed that using paraphrases for a question answering system is a useful way to improve its performance. Our method is rather generic and can be applied to any question answering system. The authors would like to thank Nitin Madnani for his help with the implementation of the paraphrase classifier. We would like to thank our anonymous reviewers for their insightful comments. This research was supported by an",
        "type": "Document"
      },
      {
        "id": "d47e490a-e794-487a-9de4-e9da4a7e6137",
        "metadata": {
          "vector_store_key": "1601.06068-0",
          "chunk_id": 15,
          "document_id": "1601.06068",
          "start_idx": 5378,
          "end_idx": 5797
        },
        "page_content": " are further filtered with a classifier to improve the precision of the generated paraphrases. We train the L-PCFG $G_{\\mathrm {syn}}$ on the Paralex corpus BIBREF9 . Paralex is a large monolingual parallel corpus, containing 18 million pairs of question paraphrases with 2.4M distinct questions in the corpus. It is suitable for our task of generating paraphrases since its large scale makes our model robust for open-",
        "type": "Document"
      },
      {
        "id": "b487d5ca-be4d-422a-8b26-b70730bf674a",
        "metadata": {
          "vector_store_key": "1601.06068-4",
          "chunk_id": 60,
          "document_id": "1601.06068",
          "start_idx": 19573,
          "end_idx": 19929
        },
        "page_content": "36 . In particular, we use Moses BIBREF37 to train a monolingual phrase-based MT system on the Paralex corpus. Finally, we use Moses decoder to generate 10-best distinct paraphrases for the test questions. For WebQuestions, we use 8 handcrafted part-of-speech patterns (e.g., the pattern (DT)?(JJ.? $\\mid $ NN.?){0,2}NN.? matches the noun phrase the big le",
        "type": "Document"
      },
      {
        "id": "2ed0fced-c73c-4096-b71b-938e3faf4872",
        "metadata": {
          "vector_store_key": "1601.06068-4",
          "chunk_id": 59,
          "document_id": "1601.06068",
          "start_idx": 19293,
          "end_idx": 19699
        },
        "page_content": "g R.) and average F $_1$ (avg F $_1$ ) proposed by berantsemantic2013 as evaluation metrics. We use GraphParser without paraphrases as our baseline. This gives an idea about the impact of using paraphrases. We compare our paraphrasing models with monolingual machine translation based model for paraphrase generation BIBREF24 , BIBREF36 . In particular, we use Moses BIBREF37 to train a monolingual phrase-",
        "type": "Document"
      },
      {
        "id": "a1076389-61ca-45ee-a148-3f28e0f40df3",
        "metadata": {
          "vector_store_key": "1601.06068-0",
          "chunk_id": 11,
          "document_id": "1601.06068",
          "start_idx": 4050,
          "end_idx": 4482
        },
        "page_content": " these constructions, we refer the reader to cohen-13 and narayan-15. The rest of this section describes our paraphrase generation algorithm. We define our paraphrase generation task as a sampling problem from an L-PCFG $G_{\\mathrm {syn}}$ , which is estimated from a large corpus of parsed questions. Once this grammar is estimated, our algorithm follows a pipeline with two major steps. We first build a word lattice $W_q$ for the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How strong was the correlation between exercise and diabetes?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "538274b0-a3ef-4a06-9ab1-59d04b5cc9b7",
        "metadata": {
          "vector_store_key": "1709.07916-1",
          "chunk_id": 31,
          "document_id": "1709.07916",
          "start_idx": 11632,
          "end_idx": 12133
        },
        "page_content": " users' interest about posting their opinions, sharing information, and conversing about exercise & diabetes, exercise & diet, diabetes & diet, diabetes & obesity, and diet & obesity (Figure FIGREF9 ). The strongest correlation among the topics was determined to be between exercise and obesity ( INLINEFORM0 ). Other notable correlations were: diabetes and obesity ( INLINEFORM1 ), and diet and obesity ( INLINEFORM2 ). Diabetes, diet, exercise, and obesity are common public health related opinions.",
        "type": "Document"
      },
      {
        "id": "27c5bd39-75c1-4cb0-9f62-6d9caf24c479",
        "metadata": {
          "vector_store_key": "1709.07916-3",
          "chunk_id": 35,
          "document_id": "1709.07916",
          "start_idx": 13327,
          "end_idx": 13877
        },
        "page_content": " diabetes and other diseases such as heart attack. The subtopic Alzheimer is also shown in the obesity subtopics. This overlap between categories prompts the discussion of research and linkages among obesity, diabetes, and Alzheimer's disease. Type 2 diabetes was another subtopic that was also expressed by users and scientifically documented in the literature. The inclusion of Yoga in posts about diabetes is interesting. While yoga would certainly be labeled as a form of fitness, when considering the post, it was insightful to see discussion on",
        "type": "Document"
      },
      {
        "id": "8016da1c-fe16-4eb1-a39e-77398f7c0469",
        "metadata": {
          "vector_store_key": "1709.07916-1",
          "chunk_id": 38,
          "document_id": "1709.07916",
          "start_idx": 14506,
          "end_idx": 14993
        },
        "page_content": " data. The diet plans of celebrities were also considered influential to explaining and informing diet opinions of Twitter users BIBREF64 . Exercise themes show the Twitter users' association of exercise with \u201cbrain\" benefits such as increased memory and cognitive performance (Tables TABREF7 and TABREF8 ) BIBREF65 . The topics also confirm that exercising is associated with controlling diabetes and assisting with meal planning BIBREF66 , BIBREF9 , and obesity BIBREF67 . Additionally",
        "type": "Document"
      },
      {
        "id": "cf516de3-1e12-484a-9d77-806ed08989ed",
        "metadata": {
          "vector_store_key": "1709.07916-1",
          "chunk_id": 2,
          "document_id": "1709.07916",
          "start_idx": 788,
          "end_idx": 1243
        },
        "page_content": "IBREF4 . There are several comorbidities associated with being overweight or obese, such as diabetes BIBREF5 . The prevalence of diabetes in adults has risen globally from 4.7% in 1980 to 8.5% in 2014. Current projections estimate that by 2050, 29 million Americans will be diagnosed with type 2 diabetes, which is a 165% increase from the 11 million diagnosed in 2002 BIBREF6 . Studies show that there are strong relations among diabetes, diet, exercise,",
        "type": "Document"
      },
      {
        "id": "1a5a6df5-4f2c-450d-a880-92e2de24cb4e",
        "metadata": {
          "vector_store_key": "1709.07916-1",
          "chunk_id": 41,
          "document_id": "1709.07916",
          "start_idx": 15848,
          "end_idx": 16405
        },
        "page_content": "7 and TABREF8 ). The scholarly literature has well documented the possible linkages between obesity and chronic diseases such as diabetes BIBREF1 as supported by the study results. The topic of children is another prominent subtopic associated with obesity. There has been an increasing number of opinions in regard to child obesity and national health campaigns that have been developed to encourage physical activity among children BIBREF70 . Alzheimer was also identified as a topic under obesity. Although considered a perplexing finding, recent studies",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How were topics of interest about DDEO identified?",
    "llm_answer": "Semantic and linguistics analyses of tweets containing DDEO words, using Latent Dirichlet Allocation (LDA) topic modeling.\n",
    "context": [
      {
        "id": "0f6b2ebd-1e6d-42e7-853c-e6306eeb652b",
        "metadata": {
          "vector_store_key": "1709.07916-2",
          "chunk_id": 10,
          "document_id": "1709.07916",
          "start_idx": 3695,
          "end_idx": 4288
        },
        "page_content": " population level. This study identifies the characteristics of the most common health opinions with respect to DDEO and discloses public perception of the relationship among diabetes, diet, exercise and obesity. These common public opinions/topics and perceptions can be used by providers and public health agencies to better understand the common opinions of their population denominators in regard to DDEO, and reflect upon those opinions accordingly. Our approach uses semantic and linguistics analyses for disclosing health characteristics of opinions in tweets containing DDEO words. The",
        "type": "Document"
      },
      {
        "id": "e498da5e-093f-42b9-a2a1-1aaeb51cce5e",
        "metadata": {
          "vector_store_key": "1604.05781-5",
          "chunk_id": 61,
          "document_id": "1604.05781",
          "start_idx": 21504,
          "end_idx": 21944
        },
        "page_content": " terms co-occur across a set of documents. We used the popular topic modeling method Latent Dirichlet Allocation (LDA) BIBREF39 . We ranked unigrams by how strongly associated they were with the topic. Inspecting these unigrams we found that a 10-topic model discovered meaningful topics. See Methods for full details. The top unigrams for each topic are shown in Tab. 1 . Topics in the causal corpus tend to fall into three main categories",
        "type": "Document"
      },
      {
        "id": "ced7b6f2-d1a5-4854-aa95-b88c45f12950",
        "metadata": {
          "vector_store_key": "1708.05873-1",
          "chunk_id": 9,
          "document_id": "1708.05873",
          "start_idx": 4026,
          "end_idx": 4560
        },
        "page_content": " the key international development topics discussed in the GD. We model topic prevalence in the context of the structural covariates. In addition, we control for region fixed effects and time trend. The aim is to allow the observed metadata to affect the frequency with which a topic is discussed in General Debate speeches. This allows us to test the degree of association between covariates (and region/time effects) and the average proportion of a document discussing a topic. We assess the optimal number of topics that need to be",
        "type": "Document"
      },
      {
        "id": "3025c1ed-cd42-4a9b-b034-69a0776ab280",
        "metadata": {
          "vector_store_key": "1709.07916-2",
          "chunk_id": 45,
          "document_id": "1709.07916",
          "start_idx": 17661,
          "end_idx": 18214
        },
        "page_content": " in one month; however, as our previous work has demonstrated BIBREF42 , public opinion can change during a year. Additionally, we did not track individuals across time to detect changes in common themes discussed. Our future research plans includes introducing a dynamic framework to collect and analyze DDEO related tweets during extended time periods (multiple months) and incorporating spatial analysis of DDEO-related tweets. This study represents the first step in developing routine processes to collect, analyze, and interpret DDEO-related posts",
        "type": "Document"
      },
      {
        "id": "1548d5d9-3a9a-4c32-8b74-25d56731f8b2",
        "metadata": {
          "vector_store_key": "1709.07916-2",
          "chunk_id": 3,
          "document_id": "1709.07916",
          "start_idx": 1243,
          "end_idx": 1680
        },
        "page_content": " 2002 BIBREF6 . Studies show that there are strong relations among diabetes, diet, exercise, and obesity (DDEO) BIBREF7 , BIBREF4 , BIBREF8 , BIBREF9 ; however, the general public's perception of DDEO remains limited to survey-based studies BIBREF10 . The growth of social media has provided a research opportunity to track public behaviors, information, and opinions about common health issues. It is estimated that the number of social",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do their train their embeddings?",
    "llm_answer": "They train their own word embeddings using CBOW (Continuous Bag-of-Words) and convert texts to sequences of integers, assigning each word a unique number from 1 to V (vocabulary size).  They also use character composition and pre-trained GloVe vectors.  Fine-tuning of pre-trained GloVe embeddings on the training data was also explored.\n",
    "context": [
      {
        "id": "59c0f487-efea-46fb-b5bf-44ee7a3f541c",
        "metadata": {
          "vector_store_key": "1910.05608-3",
          "chunk_id": 25,
          "document_id": "1910.05608",
          "start_idx": 9332,
          "end_idx": 9720
        },
        "page_content": "$ is the class weight, $y_i$ is the ground truth and $\\hat{y}_i$ is the output of the model. If the class weight is not set, we find that model cannot adjust parameters. The model tends to output all clean classes. We experiment 8 types of embedding in total: comment: CBOW embedding training in all dataset comment, each word is splited by space. Embedding size is 200. comment_bpe: CBOW",
        "type": "Document"
      },
      {
        "id": "326617a7-bff5-4ef2-a244-395be56e2b57",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 40,
          "document_id": "1910.12618",
          "start_idx": 16003,
          "end_idx": 16518
        },
        "page_content": " concatenate all the vector representations of a sentence together beforehand and result in a network with too many parameters to be trained correctly with our number of available documents. Recall that we decided to train our own vector representation of words instead of using an already available one. In order to obtain the embedding, the texts are first converted into a sequence of integers: each word is given a number ranging from 1 to $V$, where $V$ is the vocabulary size (0 is used for padding or unknown",
        "type": "Document"
      },
      {
        "id": "13292678-e061-4360-8cb8-210a1d42c4d8",
        "metadata": {
          "vector_store_key": "1911.12579-9",
          "chunk_id": 84,
          "document_id": "1911.12579",
          "start_idx": 30470,
          "end_idx": 30862
        },
        "page_content": " often produce better results but more epochs take long training time. Therefore, we evaluate 10, 20, 30 and 40 epochs for each word embedding model, and 40 epochs constantly produce good results. Learning rate (lr): We tried lr of $0.05$, $0.1$, and $0.25$, the optimal lr $(0.25)$ gives the better results for training all the embedding models. Dimensions ($D$): We evaluate and compare the",
        "type": "Document"
      },
      {
        "id": "31b32436-1140-4de8-9831-458f982987e1",
        "metadata": {
          "vector_store_key": "1703.04617-3",
          "chunk_id": 11,
          "document_id": "1703.04617",
          "start_idx": 4374,
          "end_idx": 4829
        },
        "page_content": ", and prediction. Below we discuss these components in more details. We concatenate embedding at two levels to represent a word: the character composition and word-level embedding. The character composition feeds all characters of a word into a convolutional neural network (CNN) BIBREF19 to obtain a representation for the word. And we use the pre-trained 300-D GloVe vectors BIBREF20 (see the experiment section for details) to initialize our word-level",
        "type": "Document"
      },
      {
        "id": "509cac58-0f37-4a38-ab66-2d0970a4716f",
        "metadata": {
          "vector_store_key": "1912.13109-4",
          "chunk_id": 27,
          "document_id": "1912.13109",
          "start_idx": 11011,
          "end_idx": 11520
        },
        "page_content": " representation from Glove word embedding were considered and the best results were obtained with 100d representation, consistent with choices made in the previous work. Transfer learning on Embedding; Another bit flag for training the embedding on the train data or freezing the embedding from Glove was used. It was determined that set of pre-trained weights from Glove was best when it was fine tuned with Hinglish data. It provides evidence that a separate word or sentence level embedding when learnt for",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they model travel behavior?",
    "llm_answer": "They use random utility maximization (RUM) models, which are predominantly numeric, linear, and often flat.  They also introduce Travel Behavior embeddings, borrowing from NLP concepts, to encode categorical variables.\n",
    "context": [
      {
        "id": "9fd589f1-d6dd-4e75-9bc4-4d1b2eb27447",
        "metadata": {
          "vector_store_key": "1807.03367-8",
          "chunk_id": 28,
          "document_id": "1807.03367",
          "start_idx": 11074,
          "end_idx": 11628
        },
        "page_content": "task is localization: without such capabilities the guide can not tell whether the tourist reached the target location. In this work, we establish a minimal baseline for Talk The Walk by utilizing agents trained for localization. Specifically, we let trained tourist models undertake random walks, using the following protocol: at each step, the tourist communicates its observations and actions to the guide, who predicts the tourist's location. If the guide predicts that the tourist is at target, we evaluate its location. If successful, the task ends",
        "type": "Document"
      },
      {
        "id": "fdc8735b-d286-4e96-9379-e84ec82e19a6",
        "metadata": {
          "vector_store_key": "1909.00154-5",
          "chunk_id": 51,
          "document_id": "1909.00154",
          "start_idx": 19320,
          "end_idx": 19766
        },
        "page_content": " one considers the complicated challenges for opening or sharing travel survey datasets in our field. Of course, a major question arises: are behaviors that consistent across the world? There are certainly nuances across the world, but we believe that general patterns would emerge (e.g. a \u201cbusiness\" trip purpose will be closer to \u201cwork\" than \u201cleisure\", in a departure time choice model; \u201cstudent\" will be closer to \u201cunemployed\" than to \u201cretired",
        "type": "Document"
      },
      {
        "id": "3eeb420a-9ba5-4f61-bea2-37c164636bc2",
        "metadata": {
          "vector_store_key": "1807.03367-8",
          "chunk_id": 29,
          "document_id": "1807.03367",
          "start_idx": 11628,
          "end_idx": 12165
        },
        "page_content": " guide predicts that the tourist is at target, we evaluate its location. If successful, the task ends, otherwise we continue until there have been three wrong evaluations. The protocol is given as pseudo-code in Appendix SECREF12 . The designed navigation protocol relies on a trained localization model that predicts the tourist's location from a communicated message. Before we formalize this localization sub-task in Section UID21 , we further introduce two simplifying assumptions\u2014perfect perception and orientation-agnosticism\u2014so as",
        "type": "Document"
      },
      {
        "id": "a1c480e8-1842-4b2f-9783-53c747b1e723",
        "metadata": {
          "vector_store_key": "1909.00154-5",
          "chunk_id": 10,
          "document_id": "1909.00154",
          "start_idx": 3999,
          "end_idx": 4489
        },
        "page_content": " ownership versus departure time choice models for example. The key idea in this paper is to introduce a method, called Travel Behavior embeddings, that borrows much from the NLP concept. This method serves to encode categorical variables, and is dependent on the problem at hand. We will focus on mode choice, and test on a well-known dataset, by comparing with both dummy and PCA encoding. All the dataset and code are made openly available, and the reader can follow and generate results",
        "type": "Document"
      },
      {
        "id": "704e5cb3-faf5-4577-9e1b-8124c6038675",
        "metadata": {
          "vector_store_key": "1909.00154-5",
          "chunk_id": 14,
          "document_id": "1909.00154",
          "start_idx": 5640,
          "end_idx": 6156
        },
        "page_content": ", that complexity can grow quick enough to justify an embeddings based method even if without the shockingly better performance observed in NLP applications. We are generally concerned with random utility maximization (RUM) models, for they have a dominant role in travel behavior modeling. The nature of such models is predominantly numeric, linear, and quite often strictly flat (notwithstanding hierarchical variations, such as nested models BIBREF1, hierarchical Bayes BIBREF2, or non-linear transformations). As",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do their interpret the coefficients?",
    "llm_answer": "The context states that concreteness is a significant negative predictor of change toward moral relevance, meaning abstract concepts are more strongly associated with increasing moral relevance over time.  For dummy variables, coefficients have a direct interpretation, but for encoded variables (with K>1), individual coefficients lack meaning.\n",
    "context": [
      {
        "id": "0c792bd2-06c4-4891-bd4f-7553b54a0509",
        "metadata": {
          "vector_store_key": "2001.07209-3",
          "chunk_id": 47,
          "document_id": "2001.07209",
          "start_idx": 18369,
          "end_idx": 18689
        },
        "page_content": " of moral relevance change for word $w$; $f(w$) is its average frequency; $l(w)$ is its character length; $c(w)$ is its concreteness rating; $\\beta _f$, $\\beta _l$, $\\beta _c$, and $\\beta _0$ are the corresponding factor weights and intercept, respectively; and $\\epsilon \\sim \\mathcal {N}(0, \\sigma )$ is the regression",
        "type": "Document"
      },
      {
        "id": "bfe9311a-1f40-4b01-868c-b811bc755707",
        "metadata": {
          "vector_store_key": "2001.07209-3",
          "chunk_id": 48,
          "document_id": "2001.07209",
          "start_idx": 18566,
          "end_idx": 19074
        },
        "page_content": "on \\sim \\mathcal {N}(0, \\sigma )$ is the regression error term. Table TABREF27 shows the results of multiple linear regression. We observe that concreteness is a significant negative predictor of change toward moral relevance, suggesting that abstract concepts are more strongly associated with increasing moral relevance over time than concrete concepts. This significance persists under partial correlation test against the control factors ($p < 0.01$). We further verified the diachronic component of this",
        "type": "Document"
      },
      {
        "id": "1525d236-43b7-4a33-995a-174b03c3220a",
        "metadata": {
          "vector_store_key": "1910.14497-7",
          "chunk_id": 10,
          "document_id": "1910.14497",
          "start_idx": 3655,
          "end_idx": 4014
        },
        "page_content": "in B} cos(w,a)$, and $X$,$Y$,$A$, and $B$ are groups of words for which the association is measured. Possible values range from $-2$ to 2 depending on the association of the words groups, and a value of zero indicates $X$ and $Y$ are equally associated with $A$ and $B$. See BIBREF4 for further details on WEAT. The RIPA (relational inner product association)",
        "type": "Document"
      },
      {
        "id": "2e41c750-0958-4323-bb38-b26d0f2bf5c1",
        "metadata": {
          "vector_store_key": "1909.00154-0",
          "chunk_id": 42,
          "document_id": "1909.00154",
          "start_idx": 15697,
          "end_idx": 16145
        },
        "page_content": " of dummy variables we have a direct interpretation (e.g. a beta coefficient value in a logit model) for the effect of a given category, while the same doesn't happen for an encoded variable (i.e. there is no meaning for the value of a single beta coefficient in an embeddings encoding when K>1). In order to preserve statistical significance information (e.g. p-values) we only need to follow the well known rules of normal random variables. There",
        "type": "Document"
      },
      {
        "id": "76d6e267-c660-4afe-a59f-598d2908f809",
        "metadata": {
          "vector_store_key": "1905.08949-5",
          "chunk_id": 24,
          "document_id": "1905.08949",
          "start_idx": 9103,
          "end_idx": 9529
        },
        "page_content": " on deep question datasets, such as LearningQ BIBREF8 , shown in Section \"Generation of Deep Questions\" . The second factor is the answer type, i.e., the expected form of the answer, typically having four settings: (1) the answer is a text span in the passage, which is usually the case for factoid questions, (2) human-generated, abstractive answer that may not appear in the passage, usually the case for deep questions, (3)",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much do they outperform previous state-of-the-art models?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "555a266e-0a08-43e5-b719-8cd2fd10ebb8",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "439e15bb-4608-47c1-84bf-d30ecc52c819",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 62,
          "document_id": "1909.13375",
          "start_idx": 21089,
          "end_idx": 21507
        },
        "page_content": " available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still achieve a 12.2 absolute improvement in EM, and a 2.3 improvement in F1. All that, while keeping in mind we compare a base model to a large model (for reference, note the 8 point improvement between MTMSNbase and MTMSNlarge in both EM and F1). Our best model, large-squad, exhibits a huge improvement of 29.7 in EM and",
        "type": "Document"
      },
      {
        "id": "b8463b23-0372-4422-ba8d-e82788fecd2b",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 61,
          "document_id": "1909.13375",
          "start_idx": 20788,
          "end_idx": 21299
        },
        "page_content": " For the other types, most of the improvement came from better preprocessing. A more detailed discussion could be found in Section (SECREF36). Notice that different BERTlarge models were used, so the comparison is less direct. Overall, our large models exhibits similar results to those of MTMSNlarge. For multi-span questions we achieve a significantly better performance. While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still",
        "type": "Document"
      },
      {
        "id": "1e44ac68-a426-47a3-aad3-5a441fcefe6f",
        "metadata": {
          "vector_store_key": "1603.04553-1",
          "chunk_id": 38,
          "document_id": "1603.04553",
          "start_idx": 11106,
          "end_idx": 11513
        },
        "page_content": " the MIR model, we obtain significant improvements of 2.62% and 3.02% on CoNLL F1 score. To make a thorough empirical comparison with previous studies, Table 3 (below the dashed line) also shows the results of some state-of-the-art supervised coreference resolution systems \u2014 IMS: the second best system in the CoNLL 2012 shared task BIBREF28 ; Latent-Tree: the latent tree model BIBREF29 obtaining the best",
        "type": "Document"
      },
      {
        "id": "42150962-2d02-417b-b4fa-d948c244b2d5",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 58,
          "document_id": "1909.13375",
          "start_idx": 19611,
          "end_idx": 20052
        },
        "page_content": " to gain further insights on the effect of the components that we ourselves introduced. For ease of performance comparison, we followed NABERT+'s training settings: we used the BERT Adam optimizer from huggingface with default settings and a learning rate of $1e^{-5}$. The only difference was that we used a batch size of 12. We trained our base model for 20 epochs. For the large models we used a batch size of 3 with a learning rate of $5",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance difference between proposed method and state-of-the-arts on these datasets?",
    "llm_answer": "TRADE, the state-of-the-art model, performs poorly in this setting.  The proposed method (\"Ours with Mask and Ordered Triplets\") outperforms the baseline and ablation models on all metrics in previously seen environments.\n",
    "context": [
      {
        "id": "8f0169f9-d118-414b-bb28-c12c53b30466",
        "metadata": {
          "vector_store_key": "2002.11893-2",
          "chunk_id": 72,
          "document_id": "2002.11893",
          "start_idx": 26889,
          "end_idx": 27376
        },
        "page_content": " system often records full user constraints, while the last one records relaxed constraints for recommendation. Thus the last one involves system policy, which is out of the scope of state tracking. We used the first query for these models and left state tracking with recommendation for future work. Result Analysis: We evaluated the joint state accuracy (percentage of exact matching) of these two models (Table TABREF31). TRADE, the state-of-the-art model on MultiWOZ, performs poorly",
        "type": "Document"
      },
      {
        "id": "04fe1abe-a675-484e-963d-9d80c75d505e",
        "metadata": {
          "vector_store_key": "1908.10449-2",
          "chunk_id": 66,
          "document_id": "1908.10449",
          "start_idx": 22880,
          "end_idx": 23381
        },
        "page_content": " hard time learning in this setting. As shown in Table TABREF16, both datasets have a vocabulary size of more than 100k. This is much larger than in the other two settings, where on average the length of questions is around 10. This suggests that the methods with better sample efficiency are needed to act in more realistic problem settings with huge action spaces. Experiments also show that a larger memory slot size always helps. Intuitively, with a memory mechanism (either implicit or explicit),",
        "type": "Document"
      },
      {
        "id": "dc50a5cf-b755-41a0-a173-1b561db0ed05",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "97d0589b-102d-402c-bcb2-27484aa3c4fb",
        "metadata": {
          "vector_store_key": "1701.00185-8",
          "chunk_id": 76,
          "document_id": "1701.00185",
          "start_idx": 27376,
          "end_idx": 27736
        },
        "page_content": " and NMI performance of our proposed approaches and four baseline methods, K-means, SkipVec, RecNN and Para2vec based clustering methods. Intuitively, we get a general observation that (1) BoW based approaches, including K-means (TF) and K-means (TF-IDF), and SkipVec based approaches perform not well; (2) RecNN based approaches, both RecNN (Ave.) and RecNN (",
        "type": "Document"
      },
      {
        "id": "c03315b1-7d9a-4a9a-a552-f7259ef1288c",
        "metadata": {
          "vector_store_key": "1909.02764-7",
          "chunk_id": 53,
          "document_id": "1909.02764",
          "start_idx": 20698,
          "end_idx": 21118
        },
        "page_content": " to our data set. The results are shown in column \u201cSimple\u201d in Table TABREF19. We observe a clear drop in performance, with an average of F$_1$=48 %. The best performing model is again the one trained on TEC, en par with the one trained on the Figure8 data. The model trained on ISEAR performs second best in Experiment 1, it performs worst in Experiment 2. To adapt models trained on previously existing data sets to our",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What benchmarks are created?",
    "llm_answer": "SQuAD\n",
    "context": [
      {
        "id": "9d9ab555-416a-4bc9-a24d-f0230333ee94",
        "metadata": {
          "vector_store_key": "1910.12618-4",
          "chunk_id": 49,
          "document_id": "1910.12618",
          "start_idx": 19565,
          "end_idx": 20023
        },
        "page_content": " the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014 and 2015 serve as validation set (730 days). It is used to tune the hyperparameters of the different approaches. All the data from January the 1st 2016 (974 days for France and 1,096 for the UK) is used as test set, on which the final results are presented. Grid search is applied to find the best combination of values: for each hyperparameter,",
        "type": "Document"
      },
      {
        "id": "bb75c3fa-e2c5-412d-9db8-2f31cd178291",
        "metadata": {
          "vector_store_key": "1908.06606-4",
          "chunk_id": 45,
          "document_id": "1908.06606",
          "start_idx": 15988,
          "end_idx": 16383
        },
        "page_content": ") and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage",
        "type": "Document"
      },
      {
        "id": "14bde222-3b50-4347-8cbc-18a9a301554b",
        "metadata": {
          "vector_store_key": "1905.08949-9",
          "chunk_id": 26,
          "document_id": "1905.08949",
          "start_idx": 9823,
          "end_idx": 10318
        },
        "page_content": " cognitive level and answer type, along with their statistics. Among them, SQuAD was used by most groups as the benchmark to evaluate their NQG models. This provides a fair comparison between different techniques. However, it raises the issue that most NQG models work on factoid questions with answer as text span, leaving other types of QG problems less investigated, such as generating deep multi-choice questions. To overcome this, a wider variety of corpora should be benchmarked against in",
        "type": "Document"
      },
      {
        "id": "175d0a0b-77ff-450c-8f37-24f131b845e7",
        "metadata": {
          "vector_store_key": "1905.08949-9",
          "chunk_id": 28,
          "document_id": "1905.08949",
          "start_idx": 10602,
          "end_idx": 11043
        },
        "page_content": " they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks. As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE B",
        "type": "Document"
      },
      {
        "id": "8cf9b7c9-3579-41ac-9e68-ebbe040f34c6",
        "metadata": {
          "vector_store_key": "1703.09684-3",
          "chunk_id": 16,
          "document_id": "1703.09684",
          "start_idx": 5866,
          "end_idx": 6368
        },
        "page_content": " address the fact that some categories were much easier to classify than others (e.g., faces and planes were easy and also had the largest number of test images). Mean per-class accuracy compensates for this by requiring a system to do well on each category, even when the amount of test instances in categories vary considerably. Existing benchmarks do not require reporting accuracies across different question-types. Even when they are reported, the question-types can be too coarse to be useful, e.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What percentage fewer errors did professional translations make?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "e2fb0ec0-f5d9-4acb-851f-0def2fe8f257",
        "metadata": {
          "vector_store_key": "2004.01694-2",
          "chunk_id": 88,
          "document_id": "2004.01694",
          "start_idx": 34017,
          "end_idx": 34587
        },
        "page_content": " in the field, and found that the choice of raters, the availability of linguistic context, and the creation of reference translations have a strong impact on perceived translation quality. As for the choice of raters, professional translators showed a significant preference for human translation, while non-expert raters did not. In terms of linguistic context, raters found human translation significantly more accurate than machine translation when evaluating full documents, but not when evaluating single sentences out of context. They also found human translation",
        "type": "Document"
      },
      {
        "id": "bb1ec514-5d89-48f5-9756-12f568bf72fc",
        "metadata": {
          "vector_store_key": "2004.01694-8",
          "chunk_id": 89,
          "document_id": "2004.01694",
          "start_idx": 34369,
          "end_idx": 35012
        },
        "page_content": " when evaluating full documents, but not when evaluating single sentences out of context. They also found human translation significantly more fluent than machine translation, both when evaluating full documents and single sentences. Moreover, we showed that aggressive editing of human reference translations for target language fluency can decrease adequacy to the point that they become indistinguishable from machine translation, and that raters found human translations significantly better than machine translations of original source texts, but not of source texts that were translations themselves. Our results strongly suggest that in",
        "type": "Document"
      },
      {
        "id": "5fb4fb6b-471d-47bf-ab31-d7719ec08308",
        "metadata": {
          "vector_store_key": "2004.01694-8",
          "chunk_id": 81,
          "document_id": "2004.01694",
          "start_idx": 31011,
          "end_idx": 31558
        },
        "page_content": " fluency, specifically in grammaticality. In professional translation workflows, texts are typically revised with a focus on target language fluency after an initial translation step. As shown in our experiment in Section SECREF24, aggressive revision can make translations more fluent but less accurate, to the degree that they become indistinguishable from MT in terms of accuracy (Table TABREF30). Raters show a significant preference for human over machine translations of texts that were originally written in the source language, but not for",
        "type": "Document"
      },
      {
        "id": "2d0ab993-2184-4b9d-8c85-8172b9f7757f",
        "metadata": {
          "vector_store_key": "2004.01694-2",
          "chunk_id": 56,
          "document_id": "2004.01694",
          "start_idx": 21596,
          "end_idx": 22082
        },
        "page_content": " stronger preference for human translation in terms of fluency (compare Tables TABREF30 and TABREF21). In a direct comparison of the two human translations (Table TABREF30), we also find that H$_A$ is considered significantly more adequate than H$_B$, while there is no significant difference in fluency. To achieve a finer-grained understanding of what errors the evaluated translations exhibit, we perform a categorisation of 150 randomly sampled sentences based on the classification",
        "type": "Document"
      },
      {
        "id": "0190f123-dd89-4593-93f8-edf520e2fc9e",
        "metadata": {
          "vector_store_key": "2004.01694-2",
          "chunk_id": 55,
          "document_id": "2004.01694",
          "start_idx": 21325,
          "end_idx": 21767
        },
        "page_content": " The results are shown in Table TABREF30. From this, we can see that the human translation H$_B$, which was aggressively edited to ensure target fluency, resulted in lower adequacy (Table TABREF30). With more fluent and less accurate translations, raters do not prefer human over machine translation in terms of adequacy (Table TABREF30), but have a stronger preference for human translation in terms of fluency (compare Tables TABREF30 and T",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was the weakness in Hassan et al's evaluation design?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "13e47b9a-dcd7-4e18-be3d-956d52d8a575",
        "metadata": {
          "vector_store_key": "1904.08386-1",
          "chunk_id": 24,
          "document_id": "1904.08386",
          "start_idx": 9817,
          "end_idx": 10140
        },
        "page_content": " anonymous reviewers for their insightful comments. Additionally, we thank Nader Akoury, Garrett Bernstein, Chenghao Lv, Ari Kobren, Kalpesh Krishna, Saumya Lal, Tu Vu, Zhichao Yang, Mengxue Zhang and the UMass NLP group for suggestions that improved the paper's clarity, coverage of related work, and analysis experiments.",
        "type": "Document"
      },
      {
        "id": "853401c7-057e-4ceb-99d5-8e83b9f7c1fa",
        "metadata": {
          "vector_store_key": "1908.07816-4",
          "chunk_id": 63,
          "document_id": "1908.07816",
          "start_idx": 22248,
          "end_idx": 22663
        },
        "page_content": "31 on the human evaluation results, showing the improvements of MEED are significant (with $p$-value $<0.01$). We present four sample dialogs in Table TABREF36, along with the responses generated by the three models. Dialog 1 and 2 are emotionally positive and dialog 3 and 4 are negative. For the first two examples, we can see that MEED is able to generate more emotional content (like \u201cfun\u201d and \u201ccongratulations\ufffd",
        "type": "Document"
      },
      {
        "id": "be8b40f0-56ba-4775-9ef5-8844908beaec",
        "metadata": {
          "vector_store_key": "1905.00563-4",
          "chunk_id": 47,
          "document_id": "1905.00563",
          "start_idx": 13447,
          "end_idx": 13944
        },
        "page_content": " evaluate by ( \"Influence Function vs \" ) comparing estimate with the actual effect of the attacks, ( \"Robustness of Link Prediction Models\" ) studying the effect of adversarial attacks on evaluation metrics, ( \"Interpretability of Models\" ) exploring its application to the interpretability of KG representations, and ( \"Finding Errors in Knowledge Graphs\" ) detecting incorrect triples. To evaluate the quality of our approximations and compare with influence function (IF), we conduct leave one",
        "type": "Document"
      },
      {
        "id": "dc50a5cf-b755-41a0-a173-1b561db0ed05",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "2e4eae6f-7f63-4e93-a68b-320cc2c58f2a",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 25,
          "document_id": "1909.00542",
          "start_idx": 9690,
          "end_idx": 10149
        },
        "page_content": "2 receive good human evaluation results, and as a matter of fact they are the best of our runs in two of the batches. In contrast, the regression systems (NNR) fare relatively poorly. Section SECREF6 expands on the comparison between the ROUGE and human evaluation scores. Encouraged by the results of Table TABREF29, we decided to continue with our experiments with reinforcement learning. We use the same features as in BIBREF2, namely the length (in number",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What evidence do they present that the model attends to shallow context clues?",
    "llm_answer": "Figure FIGREF37 shows that the model learns to extract shallow clues of identifying actions exerted upon only the entity being tracked, regardless of other entities, by leveraging verb semantics.\n",
    "context": [
      {
        "id": "50239ac8-1548-4095-8c3e-1de0b88f4ada",
        "metadata": {
          "vector_store_key": "1603.07044-5",
          "chunk_id": 46,
          "document_id": "1603.07044",
          "start_idx": 16137,
          "end_idx": 16605
        },
        "page_content": " are shown in Figure 5 , and categorized into short, long, and noisy sentences for discussion. A darker blue patch refers to a larger weight relative to other words in the same sentence. Figure 5 illustrates two cQA examples whose questions are relatively short. The comments corresponding to these questions are \u201c...snorkeling two days ago off the coast of dukhan...\u201d and \u201cthe doha international airport...\u201d. We can observe that our model successfully learns to focus",
        "type": "Document"
      },
      {
        "id": "5247181e-01f8-4a17-856e-3a80e3bce437",
        "metadata": {
          "vector_store_key": "2004.02393-1",
          "chunk_id": 3,
          "document_id": "2004.02393",
          "start_idx": 1273,
          "end_idx": 1817
        },
        "page_content": " may require different forms of evidence annotations. As a result, the supervision from these datasets can still be insufficient for training accurate models. Taking question answering with multi-hop reasoning as an example, annotating only supporting passages is not sufficient to show the reasoning processes due to the lack of necessary structural information (Figure FIGREF1). One example is the order of annotated evidence, which is crucial in logic reasoning and the importance of which has also been demonstrated in text-based QA BIBREF8",
        "type": "Document"
      },
      {
        "id": "af26070d-a5b4-48c1-997f-baccacee08d1",
        "metadata": {
          "vector_store_key": "2004.03034-8",
          "chunk_id": 63,
          "document_id": "2004.03034",
          "start_idx": 24164,
          "end_idx": 24784
        },
        "page_content": " this paper, we present a dataset of claims with their corresponding impact votes, and investigate the role of argumentative discourse context in argument impact classification. We experiment with various models to represent the claims and their context and find that incorporating the context information gives significant improvement in predicting argument impact. In our study, we find that flat representation of the context gives the best improvement in the performance and our analysis indicates that the contextual models perform better even for the claims with limited context. This work was supported in part by",
        "type": "Document"
      },
      {
        "id": "e1501e6b-b5d9-4ffe-8d77-bdcf1e218cad",
        "metadata": {
          "vector_store_key": "2002.00652-2",
          "chunk_id": 73,
          "document_id": "2002.00652",
          "start_idx": 24143,
          "end_idx": 24692
        },
        "page_content": " methods. This work conducts an exploratory study on semantic parsing in context, to realize how far we are from effective context modeling. Through a thorough comparison, we find that existing context modeling methods are not as effective as expected. A simple concatenation method can be much competitive. Furthermore, by performing a fine-grained analysis, we summarize two potential directions as our future work: incorporating common sense for better pronouns inference, and modeling contextual clues in a more explicit manner. By open-sourcing",
        "type": "Document"
      },
      {
        "id": "1020ac05-9465-4250-91c1-ab61d6f03e1a",
        "metadata": {
          "vector_store_key": "1909.02635-2",
          "chunk_id": 66,
          "document_id": "1909.02635",
          "start_idx": 25351,
          "end_idx": 25874
        },
        "page_content": " way to analyze the model is to compute model gradients with respect to input features BIBREF26, BIBREF25. Figure FIGREF37 shows that in this particular example, the most important model inputs are verbs possibly associated with the entity butter, in addition to the entity's mentions themselves. It further shows that the model learns to extract shallow clues of identifying actions exerted upon only the entity being tracked, regardless of other entities, by leveraging verb semantics. In an ideal scenario, we would want",
        "type": "Document"
      }
    ]
  },
  {
    "query": "In what way is the input restructured?",
    "llm_answer": "The input is restructured by an embedding layer that combines token embeddings, segmentation embeddings, and position embeddings.\n",
    "context": [
      {
        "id": "dbf24675-827e-4c27-9d8a-87caba4ecf75",
        "metadata": {
          "vector_store_key": "1706.07179-2",
          "chunk_id": 7,
          "document_id": "1706.07179",
          "start_idx": 2809,
          "end_idx": 3302
        },
        "page_content": " in this section. Figure 1 provides a high-level view of the model. The model is sequential in nature, consisting of the following steps: read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer. We model the dynamic memory in a fashion similar to Recurrent Entity Networks BIBREF17 and then equip it with an additional relational memory. There are three main components to the model: 1) input encoder 2) dynamic memory, and 3)",
        "type": "Document"
      },
      {
        "id": "b47eb24e-554c-49db-afc1-a49279bccaa0",
        "metadata": {
          "vector_store_key": "2001.00137-7",
          "chunk_id": 17,
          "document_id": "2001.00137",
          "start_idx": 7115,
          "end_idx": 7610
        },
        "page_content": " by an embedding layer used for input representation, with the final input embedding being a sum of token embedddings, segmentation embeddings and position embeddings. The first one, token embedding layer, uses a vocabulary dictionary to convert each token into a more representative embedding. The segmentation embedding layer indicates which tokens constitute a sentence by signaling either 1 or 0. In our case, since our data are formed of single sentences, the segment is 1 until the first `",
        "type": "Document"
      },
      {
        "id": "e8873e54-d9ad-44a9-8119-64a636b31002",
        "metadata": {
          "vector_store_key": "1812.07023-8",
          "chunk_id": 39,
          "document_id": "1812.07023",
          "start_idx": 15144,
          "end_idx": 15594
        },
        "page_content": " INLINEFORM0 which is the input of the decoder. We first combine past utterances using a dialogue-level encoder which is a single-layer lstm recurrent neural network. The input to this encoder are the final hidden states of the utterance-level lstm. To combine the hidden states of these diverse modalities, we found concatenation to perform better on the validation set than averaging or the Hadamard product. The answer decoder consists of a single",
        "type": "Document"
      },
      {
        "id": "dc4c3d00-20e6-4ce0-b63e-2b9c488e53f0",
        "metadata": {
          "vector_store_key": "1908.08419-6",
          "chunk_id": 38,
          "document_id": "1908.08419",
          "start_idx": 13807,
          "end_idx": 14293
        },
        "page_content": " the interaction between the characters. There are two types of gates in this network structure: reset gates and update gates. These two gated vectors determine the final output of the gated recurrent neural network, where the update gate helps the model determine what to be passed, and the reset gate primarily helps the model decide what to be cleared. In particular, the word embedding of a word with INLINEFORM0 characters can be computed as: DISPLAYFORM0  where INLINEFORM0 and IN",
        "type": "Document"
      },
      {
        "id": "9dbf3db3-49d1-44fb-94bf-0fe5ed32db74",
        "metadata": {
          "vector_store_key": "1905.08949-5",
          "chunk_id": 46,
          "document_id": "1905.08949",
          "start_idx": 16668,
          "end_idx": 17216
        },
        "page_content": " When entering this mode, the decoder produces a question word distribution based on a restricted set of vocabulary using the answer embedding, the decoder state, and the context vector. The switch between different modes is controlled by a discrete variable produced by a learnable module of the model in each decoding step. Determining the appropriate question word harks back to question type identification, which is correlated with the question intention, as different intents may yield different questions, even when presented with the same (",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What language is the Twitter content in?",
    "llm_answer": "English\n",
    "context": [
      {
        "id": "d322d4dc-4bf2-4c3e-9106-92b243c71e31",
        "metadata": {
          "vector_store_key": "1604.05781-6",
          "chunk_id": 11,
          "document_id": "1604.05781",
          "start_idx": 4432,
          "end_idx": 4838
        },
        "page_content": " BIBREF29 . A tweet will be referred to as a `document' for the rest of this work. All document text was processed the same way. Punctuation, XML characters, and hyperlinks were removed, as were Twitter-specific \u201cat-mentions\u201d and \u201chashtags\u201d (see also the Appendix). There is useful information here, but it is either not natural language text, or it is Twitter-specific, or both. Documents were broken into",
        "type": "Document"
      },
      {
        "id": "8f0d4ac1-b4fa-4bd1-b1f7-192709613c48",
        "metadata": {
          "vector_store_key": "1912.13109-0",
          "chunk_id": 13,
          "document_id": "1912.13109",
          "start_idx": 5229,
          "end_idx": 5731
        },
        "page_content": "uations, username mentions, hastags, emoticons, numbers and lots of special characters. These were all cleaned up in a preprocessing cycle to clean the data. Stop words: Stop words corpus obtained from NLTK was used to eliminate most unproductive words which provide little information about individual tweets. Transliteration: Followed by above two processes, we translated Hinglish tweets into English words using a two phase process Transliteration: In phase I, we used translation API's provided by",
        "type": "Document"
      },
      {
        "id": "c49cc2aa-6709-4450-80b1-67c6e0781647",
        "metadata": {
          "vector_store_key": "1604.05781-6",
          "chunk_id": 10,
          "document_id": "1604.05781",
          "start_idx": 3952,
          "end_idx": 4432
        },
        "page_content": " statements will be considered in future work.) We considered only English-language tweets for this study. To avoid cross-language effects, we kept only tweets with a user-reported language of `English' and, as a second constraint, individual tweets needed to match more English stopwords than any other language's set of stopwords. Stopwords considered for each language were determined using NLTK's database BIBREF29 . A tweet will be referred to as a `document' for the rest of",
        "type": "Document"
      },
      {
        "id": "63649166-1f19-4784-b360-2253aece32e7",
        "metadata": {
          "vector_store_key": "1904.00648-0",
          "chunk_id": 16,
          "document_id": "1904.00648",
          "start_idx": 6273,
          "end_idx": 6789
        },
        "page_content": " two methods: on one side, we use the entities extracted from the radio schedule for generating candidates entities in the user-generated tweets, thanks to a matching algorithm based on time proximity and string similarity. On the other side, we create a statistical model capable of detecting entities directly from the UGC, aimed to model the informal language of the raw texts. In Figure 1, an overview of the system proposed is presented. In May 2018, we crawled Twitter using the Python library Tweepy, creating",
        "type": "Document"
      },
      {
        "id": "e7fd5798-209e-4db5-a807-32bba60047f4",
        "metadata": {
          "vector_store_key": "1602.08741-7",
          "chunk_id": 17,
          "document_id": "1602.08741",
          "start_idx": 6528,
          "end_idx": 6973
        },
        "page_content": " criteria: matching to one or more search keywords, produced by subset of users, or coming from certain geo location. The last one provides the full set of Tweets, although it is not available by default. In order to get Twitter \u201cfirehose\u201d one can contact Twitter, or buy this stream from third-parties. In our case the simplest approach would be to use \u201csample\u201d endpoint, but it provides Tweets in all possible languages from all over the World",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What evaluations did the authors use on their system?",
    "llm_answer": "Human evaluation of responses' grammatical correctness, contextual coherence, and emotional appropriateness; relative ranking; direct assessment; head-to-head evaluation.\n",
    "context": [
      {
        "id": "086a3cca-ae2a-4f5d-a0cd-6c70e8cc30a2",
        "metadata": {
          "vector_store_key": "1709.10217-2",
          "chunk_id": 25,
          "document_id": "1709.10217",
          "start_idx": 8906,
          "end_idx": 9404
        },
        "page_content": " 2, there are 7 submitted systems. However, only 4 systems can provide correct results or be connected in a right way at the test phase. Therefore, Table TABREF16 shows the complete results of the task 2. In this paper, we introduce the first evaluation of Chinese human-computer dialogue technology. In detail, we first present the two tasks of the evaluation as well as the evaluation metrics. We then describe the released data for evaluation. Finally, we also show the evaluation results of the",
        "type": "Document"
      },
      {
        "id": "1fdf0687-2c96-44b8-9001-12e83353c279",
        "metadata": {
          "vector_store_key": "1709.10217-2",
          "chunk_id": 7,
          "document_id": "1709.10217",
          "start_idx": 2447,
          "end_idx": 3021
        },
        "page_content": " dialogue systems, especially considering the language characteristics of Chinese, we organize the first evaluation of Chinese human-computer dialogue technology. In this paper, we will present the evaluation scheme and the released corpus in detail. The rest of this paper is as follows. In Section 2, we will briefly introduce the first evaluation of Chinese human-computer dialogue technology, which includes the descriptions and the evaluation metrics of the two tasks. We then present the evaluation data and final results in Section 3 and 4 respectively, following the",
        "type": "Document"
      },
      {
        "id": "ba767a70-d04e-4667-bc5c-183b82dd1e4f",
        "metadata": {
          "vector_store_key": "1908.07816-2",
          "chunk_id": 59,
          "document_id": "1908.07816",
          "start_idx": 20712,
          "end_idx": 21186
        },
        "page_content": " TABREF35 summarize the human evaluation results on the responses' grammatical correctness, contextual coherence, and emotional appropriateness, respectively. In the tables, we give the percentage of votes each model received for the three scores, the average score obtained with improvements over S2S, and the agreement score among the raters. Note that we report Fleiss' $\\kappa $ score BIBREF27 for contextual coherence and emotional appropriateness, and Finn's $r$ score",
        "type": "Document"
      },
      {
        "id": "53d93768-f0ef-41e4-ad57-97072af55d9b",
        "metadata": {
          "vector_store_key": "1910.10408-0",
          "chunk_id": 59,
          "document_id": "1910.10408",
          "start_idx": 21469,
          "end_idx": 21983
        },
        "page_content": " goal is to be able to generate shorter translations and as close as possible to the length of the source sentences, we focused the manual evaluation on the Short output class and aimed to verify possible losses in quality with respect to the baseline system. We ran a head-to-head evaluation on the first 10 sentences of each test talk, for a total of 270 sentences, by asking annotators to blindly rank the two system outputs (ties were also permitted) in terms of quality with respect to a reference translation",
        "type": "Document"
      },
      {
        "id": "ce129788-9548-44f3-bc58-be1d4d73027e",
        "metadata": {
          "vector_store_key": "2004.01694-0",
          "chunk_id": 7,
          "document_id": "2004.01694",
          "start_idx": 3098,
          "end_idx": 3549
        },
        "page_content": " to the translation hint by human post-editors or raters, respectively. As part of the large-scale evaluation campaign at WMT, two primary evaluation methods have been used in recent years: relative ranking and direct assessment BIBREF9. In the case of relative ranking, raters are presented with outputs from two or more systems, which they are asked to evaluate relative to each other (e.g., to determine system A is better than system B). Ties (e.g",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What accuracy does CNN model achieve?",
    "llm_answer": "The CNN accuracy drops from around 0.8 to 0.5 when noise rate increases from 0 to 0.5.\n",
    "context": [
      {
        "id": "584f3479-56e6-45ca-8b9d-4b9160359370",
        "metadata": {
          "vector_store_key": "1909.00124-9",
          "chunk_id": 42,
          "document_id": "1909.00124",
          "start_idx": 13391,
          "end_idx": 13735
        },
        "page_content": "0, $0.1$, $0.2$, $0.3$, $0.4$, $0.5$] are shown in Figure FIGREF13. From the figure, we can see that the test accuracy drops from around 0.8 to 0.5 when the noise rate increases from 0 to 0.5, but our NetAb outperforms CNN. The results clearly show that the performance of the CNN drops quite a lot with the noise rate increasing. Experiment 2:",
        "type": "Document"
      },
      {
        "id": "777580e2-ab60-45cb-9dc7-41d1cfaf5a9d",
        "metadata": {
          "vector_store_key": "1911.07228-6",
          "chunk_id": 31,
          "document_id": "1911.07228",
          "start_idx": 8371,
          "end_idx": 8784
        },
        "page_content": " 3 shows perfomance of the BLSTM-CNN-CRF model. In our experiments, we use three evaluation parameters (precision, recall, and F1 score) to access our experimental result. They will be described as follow in Table 3. The \"correctNE\", the number of correct label for entity that the model can found. The \"goldNE\", number of the real label annotated by annotator in the gold data. The \"foundNE\", number of the label",
        "type": "Document"
      },
      {
        "id": "fc85b849-8b62-410f-8f70-2a1182aa13bc",
        "metadata": {
          "vector_store_key": "1910.04269-4",
          "chunk_id": 57,
          "document_id": "1910.04269",
          "start_idx": 21961,
          "end_idx": 22224
        },
        "page_content": " which is a recently developed technique to prevent over\ufb01tting on test data.This approach achieved an accuracy of 95.4%. We also analysed how attention mechanism and recurrent layers impact the performance of networks. This approach achieved an accuracy of 95.0%.",
        "type": "Document"
      },
      {
        "id": "a038f163-78d3-4778-bc4c-06835bfdfc19",
        "metadata": {
          "vector_store_key": "1810.09774-7",
          "chunk_id": 38,
          "document_id": "1810.09774",
          "start_idx": 14308,
          "end_idx": 14688
        },
        "page_content": " accuracy of 22.5 points. Our baselines for SNLI (90.4%) and SNLI + MultiNLI (90.6%) outperform the previous state-of-the-art accuracy for SNLI (90.1%) by BIBREF24 . To understand better the types of errors made by neural network models in NLI we looked at some example failure-pairs for selected models. Tables 5 and 6 contain some randomly selected failure-pairs for two models:",
        "type": "Document"
      },
      {
        "id": "f5d6daa9-1243-4073-b29b-46016020acab",
        "metadata": {
          "vector_store_key": "1911.07228-6",
          "chunk_id": 28,
          "document_id": "1911.07228",
          "start_idx": 7484,
          "end_idx": 7865
        },
        "page_content": " about 200k words Based on state-of-the-art methods for NER, BLSTM-CNN-CRF is the end-to-end deep neural network model that achieves the best result on F-score BIBREF9. Therefore, we decide to conduct the experiment on this model and analyze the errors. We run experiment with the Ma and Hovy (2016) model BIBREF8, source code provided by (Motoki Sato) and analysis the errors from",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many documents are in the Indiscapes dataset?",
    "llm_answer": "508\n",
    "context": [
      {
        "id": "c145936a-f4ee-4752-b979-fcc3608ff1ee",
        "metadata": {
          "vector_store_key": "1912.07025-8",
          "chunk_id": 10,
          "document_id": "1912.07025",
          "start_idx": 3958,
          "end_idx": 4356
        },
        "page_content": "14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20. No publicly available large-scale, annotated dataset of historical Indic manuscripts exists to the best of our knowledge. In contrast with existing collections, our proposed dataset contains a much larger diversity in terms of document type (palm-leaf and early paper), scripts and annotated layout elements (see Tables TABREF5,TABREF",
        "type": "Document"
      },
      {
        "id": "4e80b6b2-12dc-4597-87f4-6a0172900fa9",
        "metadata": {
          "vector_store_key": "1912.07025-0",
          "chunk_id": 22,
          "document_id": "1912.07025",
          "start_idx": 8670,
          "end_idx": 9168
        },
        "page_content": " it truly an end-to-end system. Overall, our dataset contains 508 annotated Indic manuscripts. Some salient aspects of the dataset can be viewed in Table TABREF5 and a pictorial illustration of layout regions can be viewed in Figure FIGREF13. Note that multiple regions can overlap, unlike existing historical document datasets which typically contain disjoint region annotations. For the rest of the section, we discuss the challenges associated with annotating Indic manuscripts (Section SECREF9)",
        "type": "Document"
      },
      {
        "id": "fbd88e94-8fe3-489c-9c27-9edafbb3ba1b",
        "metadata": {
          "vector_store_key": "1912.07025-0",
          "chunk_id": 2,
          "document_id": "1912.07025",
          "start_idx": 705,
          "end_idx": 1221
        },
        "page_content": " within these documents before it is lost forever. Surprisingly, no large-scale annotated Indic manuscript image datasets exist for the benefit of researchers in the community. In this paper, we take a significant step to address this gap by creating such a dataset. Given the large diversity in language, script and non-textual regional elements in these manuscripts, spatial layout parsing is crucial in enabling downstream applications such as OCR, word-spotting, style-and-content based retrieval and clustering.",
        "type": "Document"
      },
      {
        "id": "9270562a-8e53-4e1d-b760-74d8fd0bc872",
        "metadata": {
          "vector_store_key": "1905.06566-7",
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "c56528c2-3d45-4cb1-b6df-066baae2fbcb",
        "metadata": {
          "vector_store_key": "1912.07025-8",
          "chunk_id": 18,
          "document_id": "1912.07025",
          "start_idx": 6986,
          "end_idx": 7473
        },
        "page_content": "38. The Indic manuscript document images in our dataset are obtained from two sources. The first source is the publicly available Indic manuscript collection from University of Pennsylvania's Rare Book and Manuscript Library BIBREF39, also referred to as Penn-in-Hand (PIH). From the $2{,}880$ Indic manuscript book-sets, we carefully curated 193 manuscript images for annotation. Our curated selection aims to maximize the diversity of the dataset in terms of various attributes such as",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are simulated datasets collected?",
    "llm_answer": "A dataset of 8066 pairs of free-form natural language instructions and navigation plans for training behavioral robot navigation, and a prototype dataset based on videos from the Charades dataset.\n",
    "context": [
      {
        "id": "4568bae0-5aed-4d2d-b5e2-9bf92f272c5c",
        "metadata": {
          "vector_store_key": "1809.08731-7",
          "chunk_id": 57,
          "document_id": "1809.08731",
          "start_idx": 19179,
          "end_idx": 19597
        },
        "page_content": " scenarios; the reason we use them is that they allow for a proof of concept. In this setting, we train scikit's BIBREF18 support vector regression model (SVR) with the default parameters on predicting fluency, given WPSLOR and ROUGE-L-mult. We use 500 of our total 2955 examples for each of training and development, and the remaining 1955 for testing. Second, we simulate a setting in which we have only access to (i",
        "type": "Document"
      },
      {
        "id": "f8061e5e-74f5-49d0-8dbe-90e4db84d4eb",
        "metadata": {
          "vector_store_key": "1810.00663-7",
          "chunk_id": 46,
          "document_id": "1810.00663",
          "start_idx": 17810,
          "end_idx": 18361
        },
        "page_content": " . This dataset was created using Amazon Mechanical Turk and 100 maps of simulated indoor environments, each with 6 to 65 rooms. To the best of our knowledge, this is the first benchmark for comparing translation models in the context of behavioral robot navigation. As shown in Table TABREF16 , the dataset consists of 8066 pairs of free-form natural language instructions and navigation plans for training. This training data was collected from 88 unique simulated environments, totaling 6064 distinct navigation plans (2002 plans have two different",
        "type": "Document"
      },
      {
        "id": "af1a972a-bca3-4b96-a175-5e7434f86258",
        "metadata": {
          "vector_store_key": "1709.01256-1",
          "chunk_id": 52,
          "document_id": "1709.01256",
          "start_idx": 19441,
          "end_idx": 19942
        },
        "page_content": " We removed empty lines in the documents and trained word2vec embeddings on the entire corpus. We used the documents occurring in the first INLINEFORM0 of the revision period for INLINEFORM1 calibration, and the remaining documents for test. The generation process of the simulated data sets is designed to mimic the real world. Users open some existing documents in a file system, make some changes (e.g. addition, deletion or replacement), and save them as separate documents. These documents become",
        "type": "Document"
      },
      {
        "id": "0e97c7e7-33a8-40dd-9b79-3c5ebe85d772",
        "metadata": {
          "vector_store_key": "1812.07023-1",
          "chunk_id": 27,
          "document_id": "1812.07023",
          "start_idx": 10725,
          "end_idx": 11271
        },
        "page_content": " prototype dataset, the training set of the avsd dataset corresponds to videos taken from the training set of the Charades dataset while the validation and test sets of the avsd dataset correspond to videos taken from the validation set of the Charades dataset. For the official dataset, training, validation and test sets are drawn from the corresponding Charades sets. The Charades dataset also provides additional annotations for the videos such as action, scene, and object annotations, which are considered to be external data sources by the",
        "type": "Document"
      },
      {
        "id": "1c380a0c-1037-4628-8ada-168d9c8247bc",
        "metadata": {
          "vector_store_key": "1707.03904-9",
          "chunk_id": 13,
          "document_id": "1707.03904",
          "start_idx": 5304,
          "end_idx": 5850
        },
        "page_content": " subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short. We plan to release these annotations along with the datasets, and our retrieved documents for each question. Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What human evaluation metrics were used in the paper?",
    "llm_answer": "Relevance, conciseness, and readability on a 1 to 5 scale.\n",
    "context": [
      {
        "id": "bf420fd0-2a99-48c9-9d52-0d17e3074796",
        "metadata": {
          "vector_store_key": "1910.00825-4",
          "chunk_id": 47,
          "document_id": "1910.00825",
          "start_idx": 16888,
          "end_idx": 17413
        },
        "page_content": " our method's increased performance on automatic evaluation metrics entails better human perceived quality. We randomly select 100 test samples from MultiWOZ test set for evaluation. We recruit 150 crowd workers from Amazon Mechanical Turk. For each sample, we show the conversation, reference summary, as well as summaries generated by Pointer-Generator and SPNet to three different participants. The participants are asked to score each summary on three indicators: relevance, conciseness and readability on a 1 to 5 scale,",
        "type": "Document"
      },
      {
        "id": "c38f5922-1475-4d6e-9f43-869b634dcd9b",
        "metadata": {
          "vector_store_key": "1809.08731-2",
          "chunk_id": 33,
          "document_id": "1809.08731",
          "start_idx": 11198,
          "end_idx": 11606
        },
        "page_content": "\\text{NCE}(S))$$   (Eq. 24)  Due to its popularity, we also performed initial experiments with BLEU BIBREF17 . Its correlation with human scores was so low that we do not consider it in our final experiments. Following earlier work BIBREF2 , we evaluate our metrics using Pearson correlation with human judgments. It is defined as the covariance divided by the product of the standard deviations:  $$\\rho _{X",
        "type": "Document"
      },
      {
        "id": "f0422910-e68e-4c7c-b7dc-7014114bab05",
        "metadata": {
          "vector_store_key": "1809.08731-2",
          "chunk_id": 67,
          "document_id": "1809.08731",
          "start_idx": 22769,
          "end_idx": 23227
        },
        "page_content": " single reference. This was supported by isabelle-cherry-foster:2017:EMNLP2017, who proposed a so-called challenge set approach as an alternative. graham-EtAl:2016:COLING performed a large-scale evaluation of human-targeted metrics for machine translation, which can be seen as a compromise between human evaluation and fully automatic metrics. They also found fully automatic metrics to correlate only weakly or moderately with human judgments. bojar2016ten",
        "type": "Document"
      },
      {
        "id": "80b9670e-b107-4fd4-b43a-2b7c7c92c002",
        "metadata": {
          "vector_store_key": "2004.01694-2",
          "chunk_id": 85,
          "document_id": "2004.01694",
          "start_idx": 32633,
          "end_idx": 33097
        },
        "page_content": " a small number of human raters: five in the experiments of Sections SECREF3 and SECREF35, four per condition (adequacy and fluency) in Section SECREF4, and one in the fine-grained error analysis presented in Section SECREF24. Moreover, the results presented in this article are based on one text domain (news) and one language direction (Chinese to English), and while a large-scale evaluation with another language pair supports our findings (see above), further",
        "type": "Document"
      },
      {
        "id": "4786191e-fed1-4406-89b6-7b8835719e5f",
        "metadata": {
          "vector_store_key": "1709.10217-2",
          "chunk_id": 18,
          "document_id": "1709.10217",
          "start_idx": 6394,
          "end_idx": 6850
        },
        "page_content": " evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the number of total tasks. User satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied, neutral, satisfied and very satisfied, respectively. Response fluency: There are three scores -1, 0, 1, which indicate nonfluency, neutral, fluency. Number of dialogue turns: The number of utterances in a task",
        "type": "Document"
      }
    ]
  },
  {
    "query": "For the purposes of this paper, how is something determined to be domain specific knowledge?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "bea618fb-90d6-4dc7-8e60-c8e7bcf0b45f",
        "metadata": {
          "vector_store_key": "1701.03214-2",
          "chunk_id": 15,
          "document_id": "1701.03214",
          "start_idx": 5368,
          "end_idx": 5787
        },
        "page_content": " (essentially open) in-domain data. The scientific domain MT was conducted on the Chinese-Japanese paper excerpt corpus (ASPEC-CJ) BIBREF11 , which is one subtask of the workshop on Asian translation (WAT) BIBREF15 . The ASPEC-CJ task uses 672315, 2090, and 2107 sentences for training, development, and testing, respectively. The Wikipedia domain task was conducted on a Chinese-Japanese corpus automatically extracted",
        "type": "Document"
      },
      {
        "id": "93868b0b-4b9f-4465-9f7e-c688fa1a5347",
        "metadata": {
          "vector_store_key": "1701.03214-0",
          "chunk_id": 11,
          "document_id": "1701.03214",
          "start_idx": 4004,
          "end_idx": 4439
        },
        "page_content": "-of-domain data is used for the in-domain data; while for the \u201cmulti domain\u201d and \u201cmixed fine tuning\u201d methods, we use a vocabulary obtained from the mixed in-domain and out-of-domain data for all the training stages. We conducted NMT domain adaptation experiments in two different settings as follows: Chinese-to-English translation was the focus of the high quality in-domain corpus setting. We utilized the resource rich patent out-of",
        "type": "Document"
      },
      {
        "id": "de6203b2-b3a6-4156-90ff-25e5ea6dd48f",
        "metadata": {
          "vector_store_key": "1905.06906-2",
          "chunk_id": 16,
          "document_id": "1905.06906",
          "start_idx": 5943,
          "end_idx": 6219
        },
        "page_content": " task in target domain. Domain Adaptation (DA) is defined by the target predictive function $D_{S}$8 calculated using the knowledge of $D_{S}$9 and $(x_{s_{1}},y_{s_{1}})$0 where $(x_{s_{1}},y_{s_{1}})$1 but $(x_{s_{1}},y_{s_{1}})$2 . It is imperative to note that the domains",
        "type": "Document"
      },
      {
        "id": "54cf9f95-81f4-4b30-9e85-bab5dd16c8da",
        "metadata": {
          "vector_store_key": "1902.00330-2",
          "chunk_id": 1,
          "document_id": "1902.00330",
          "start_idx": 505,
          "end_idx": 1046
        },
        "page_content": " information retrieval (IE) and has many applications, such as knowledge base population (KBP), question answering (QA), etc. Existing EL methods can be divided into two categories: local model and global model. Local models concern mainly on contextual words surrounding the mentions, where mentions are disambiguated independently. These methods are not work well when the context information is not rich enough. Global models take into account the topical coherence among the referred entities within the same document, where mentions are",
        "type": "Document"
      },
      {
        "id": "9d665fdf-7be9-4b15-ae83-e0f94156b3c9",
        "metadata": {
          "vector_store_key": "2002.11893-8",
          "chunk_id": 28,
          "document_id": "2002.11893",
          "start_idx": 10717,
          "end_idx": 11221
        },
        "page_content": " before the conversation, we specially design cross-domain informable slots (bold values) whose values refer to other sub-goals. Cross-domain informable slots utilize sub-goal id to connect different sub-goals. Thus the actual constraints vary according to the different contexts instead of being pre-specified. The values of common informable slots are sampled randomly from the database. Based on the informable slots, users are required to gather the values of requestable slots (blank values in Table",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What type of model are the ELMo representations used in?",
    "llm_answer": "CNNs and bi-directional LSTMs.\n",
    "context": [
      {
        "id": "d5e0b0c8-9760-4640-8119-192fcdaf5b71",
        "metadata": {
          "vector_store_key": "1809.09795-2",
          "chunk_id": 14,
          "document_id": "1809.09795",
          "start_idx": 5437,
          "end_idx": 5886
        },
        "page_content": ", our architecture is based on Embeddings from Language Model or ELMo BIBREF10 . The ELMo layer allows to recover a rich 1,024-dimensional dense vector for each word. Using CNNs, each vector is built upon the characters that compose the underlying words. As ELMo also contains a deep bi-directional LSTM on top of this character-derived vectors, each word-level embedding contains contextual information from their surroundings. Concretely, we use a",
        "type": "Document"
      },
      {
        "id": "6cd0e150-048c-4d4c-b286-8490e631314b",
        "metadata": {
          "vector_store_key": "1810.09774-1",
          "chunk_id": 29,
          "document_id": "1810.09774",
          "start_idx": 11025,
          "end_idx": 11459
        },
        "page_content": " , BIBREF2 and BIBREF0 respectively. For BERT we used the uncased 768-dimensional model (BERT-base). For ESIM + ELMo we used the AllenNLP BIBREF23 PyTorch implementation with the default settings and hyperparameter values. Table 4 contains all the experimental results. Our experiments show that, while all of the six models perform well when the test set is drawn from the same corpus as the training and development set, accuracy is",
        "type": "Document"
      },
      {
        "id": "94b7dfa9-8286-4dfc-a97f-25dec6bd36cd",
        "metadata": {
          "vector_store_key": "1911.03894-6",
          "chunk_id": 12,
          "document_id": "1911.03894",
          "start_idx": 4523,
          "end_idx": 4978
        },
        "page_content": " have been released: ELMo models for Japanese, Portuguese, German and Basque and BERT for Simplified and Traditional Chinese and German. However, to the best of our knowledge, no particular effort has been made toward training models for languages other than English, at a scale similar to the latest English models (e.g. RoBERTa trained on more than 100GB of data). Our approach is based on RoBERTa BIBREF9, which replicates and improves the initial BERT",
        "type": "Document"
      },
      {
        "id": "7f461d05-dd98-4a6a-9325-c9aa5cae2449",
        "metadata": {
          "vector_store_key": "1809.09795-1",
          "chunk_id": 6,
          "document_id": "1809.09795",
          "start_idx": 2393,
          "end_idx": 2864
        },
        "page_content": " word representations, which have recently been used to achieve the state of the art on six NLP tasks, including sentiment analysis BIBREF10 . We test our proposed architecture on 7 different irony/sarcasm datasets derived from 3 different data sources, providing state-of-the-art performance in 6 of them and otherwise offering competitive results, showing the effectiveness of our proposal. We make our code available at https://github.com/epochx/elmo4irony. Apart from",
        "type": "Document"
      },
      {
        "id": "81dc58c3-0234-41f5-b84f-c62bd8623ce3",
        "metadata": {
          "vector_store_key": "1810.09774-1",
          "chunk_id": 26,
          "document_id": "1810.09774",
          "start_idx": 9942,
          "end_idx": 10400
        },
        "page_content": " is particularly interesting in this context as it performed significantly better than other models in the Breaking NLI experiment conducted by BIBREF1 . The success of pre-trained language models in multiple NLP tasks make ESIM + ELMo and BERT interesting additions to this experiment. Table 3 lists the different models used in the experiments. For BiLSTM-max we used the Adam optimizer BIBREF21 , a learning rate of 5e-4 and batch size of 64. The learning",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does using phonetic feedback improve state-of-the-art systems?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "eaec219a-bdcd-4b2a-bb65-07c0a20a8f5d",
        "metadata": {
          "vector_store_key": "2003.01769-0",
          "chunk_id": 31,
          "document_id": "2003.01769",
          "start_idx": 12435,
          "end_idx": 12978
        },
        "page_content": " well as the mimic setup. Compared to the previous experiment without parallel data, the presence of the spectral magnitude and time-domain losses likely keep the enhancement output more stable when joint training, at the cost of requiring parallel training data. We have shown that phonetic feedback is valuable for speech enhancement systems. In addition, we show that our approach to this feedback, the mimic loss framework, is useful in many scenarios: with and without the presence of parallel data, in both the enhancement and robust ASR",
        "type": "Document"
      },
      {
        "id": "3baea8bf-3b4d-45eb-a100-52a4409d7455",
        "metadata": {
          "vector_store_key": "2003.01769-4",
          "chunk_id": 1,
          "document_id": "2003.01769",
          "start_idx": 594,
          "end_idx": 1096
        },
        "page_content": " learn the phonetic patterns of the speech signal in the coordinated output of time-domain or time-frequency units. However, our hypothesis is that directly providing phonetic feedback can be a powerful additional signal for speech enhancement. For example, many local metrics will be more attuned to high-energy regions of speech, but not all phones of a language carry equal energy in production (compare /v/ to /ae/). Our proxy for phonetic intelligibility is a frozen automatic speech recognition (",
        "type": "Document"
      },
      {
        "id": "c3510645-f7ac-4972-a1fb-237ca8bd56af",
        "metadata": {
          "vector_store_key": "2003.01769-0",
          "chunk_id": 3,
          "document_id": "2003.01769",
          "start_idx": 1439,
          "end_idx": 1983
        },
        "page_content": " provides key linguistic insights to the enhancement model about what a recognizable phoneme looks like. When no parallel data is available, but transcripts are available, a loss is easily computed against hard senone labels and backpropagated to the enhancement model trained from scratch. Since the clean acoustic model is frozen, the only way for the enhancement model to improve the loss is to make a signal that is more recognizable to the acoustic model. The improvement by this model demonstrates the power of phonetic feedback; very few",
        "type": "Document"
      },
      {
        "id": "7ddbeceb-93d9-45f3-875f-c9f53efca5ae",
        "metadata": {
          "vector_store_key": "1909.02764-5",
          "chunk_id": 14,
          "document_id": "1909.02764",
          "start_idx": 5772,
          "end_idx": 6281
        },
        "page_content": "2004automatic compare local and global features in support vector machines. Next to such discriminative approaches, hidden Markov models are well-studied, however, there is no agreement on which feature-based classifier is most suitable BIBREF13. Similar to the facial expression modality, recent efforts on applying deep learning have been increased for acoustic speech processing. For instance, lee2015high use a recurrent neural network and palaz2015analysis apply a convolutional neural network to the raw",
        "type": "Document"
      },
      {
        "id": "1db9fc0e-8ca3-499e-a5d1-208eddd237af",
        "metadata": {
          "vector_store_key": "2001.00137-0",
          "chunk_id": 3,
          "document_id": "2001.00137",
          "start_idx": 1405,
          "end_idx": 1944
        },
        "page_content": " Speech-to-Text (STT) accuracy diminishes error propagation to modules using the recognized text. With that in mind, in order for current systems to improve the quality of their services, there is a need for development of robust intelligent systems that are able to understand a user even when faced with incomplete representation in language. The advancement of deep neural networks have immensely aided in the development of the Natural Language Processing (NLP) domain. Tasks such as text generation, sentence correction, image caption",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what are the baselines?",
    "llm_answer": "content, content-network, image-based models; BiRNN language model; GA model; LSTM baseline;  TF-IDF, S2S, IR, and Proposed+IR\n",
    "context": [
      {
        "id": "25bb2afe-7835-48e9-a6f8-dbdbbde2116d",
        "metadata": {
          "vector_store_key": "1902.06843-8",
          "chunk_id": 91,
          "document_id": "1902.06843",
          "start_idx": 35139,
          "end_idx": 35416
        },
        "page_content": "s by -0.27. Baselines: To test the efficacy of our multi-modal framework for detecting depressed users, we compare it against existing content, content-network, and image-based models (based on the aforementioned general image feature, facial presence, and facial expressions.)",
        "type": "Document"
      },
      {
        "id": "95d982c1-df48-40f1-aea4-aa545ace30dc",
        "metadata": {
          "vector_store_key": "1909.02635-8",
          "chunk_id": 23,
          "document_id": "1909.02635",
          "start_idx": 9050,
          "end_idx": 9548
        },
        "page_content": " (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo",
        "type": "Document"
      },
      {
        "id": "f189eac9-0435-44a6-90b3-1a428fecf8da",
        "metadata": {
          "vector_store_key": "1707.03904-4",
          "chunk_id": 53,
          "document_id": "1707.03904",
          "start_idx": 20086,
          "end_idx": 20524
        },
        "page_content": " \u2013 finding the few most relevant ones will allow the reader to work best. In Tables 2 and 3 we compare all baselines when the context size is tuned to maximize the overall accuracy on the validation set. For Quasar-S the best performing baseline is the BiRNN language model, which achieves $33.6\\%$ accuracy. The GA model achieves $48.3\\%$ accuracy on the set of instances for which the answer is in context, however, a search accuracy of",
        "type": "Document"
      },
      {
        "id": "824971d7-63ea-4037-9e29-269aae646ddf",
        "metadata": {
          "vector_store_key": "1810.09774-5",
          "chunk_id": 18,
          "document_id": "1810.09774",
          "start_idx": 6998,
          "end_idx": 7473
        },
        "page_content": " SNLI + MultiNLI we report the baseline using the SNLI test data. All the experimental combinations are listed in Table 1 . Examples from the selected datasets are provided in Table 2 . To be more precise, we vary three things: training dataset, model and testing dataset. We should qualify this though, since the three datasets we look at, can also be grouped by text domain/genre and type of data collection, with MultiNLI and SNLI using the same data collection style, and",
        "type": "Document"
      },
      {
        "id": "ac08e6c0-ed7c-44a4-8569-c15f1a5e35d1",
        "metadata": {
          "vector_store_key": "1809.04960-5",
          "chunk_id": 58,
          "document_id": "1809.04960",
          "start_idx": 22497,
          "end_idx": 22960
        },
        "page_content": " the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types of comments as described in the above retrieval evaluation setting: Correct, Plausible, Popular, and Random. We rank the candidate comment set with four models (TF-IDF, S2S, IR, and Proposed+IR), and record the types of top-1 comments. Figure FIGREF40 shows the percentage of different types of top-1 comments generated by each model. It shows",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the results of the experiment?",
    "llm_answer": "The provided text describes experiments, but does not present results in a summary form.  The text describes *what* experiments were conducted and *how* they were performed, but not the *results* of those experiments.  The answer is not found in the context.\n",
    "context": [
      {
        "id": "17f3676f-8f5c-4da9-b0c7-cede94c8171d",
        "metadata": {
          "vector_store_key": "1904.05862-2",
          "chunk_id": 42,
          "document_id": "1904.05862",
          "start_idx": 15343,
          "end_idx": 15454
        },
        "page_content": "letter++ experiments, and Tatiana Likhomanenko for providing convolutional language models for our experiments.",
        "type": "Document"
      },
      {
        "id": "c5eaf502-a11a-40bd-84a8-78f62e8d8d02",
        "metadata": {
          "vector_store_key": "1701.00185-9",
          "chunk_id": 10,
          "document_id": "1701.00185",
          "start_idx": 3790,
          "end_idx": 4304
        },
        "page_content": " this paper we use a new short text dataset, Biomedical, in the experiment to verify the effectiveness of our approach. Third, we put much effort on studying the influence of various different semantic features integrated in our self-taught CNN framework, which is not involved in the conference paper. For the purpose of reproducibility, we make the datasets and software used in our experiments publicly available at the website. The remainder of this paper is organized as follows: In Section SECREF2 , we first",
        "type": "Document"
      },
      {
        "id": "11be1f49-2639-45f5-b788-25afa92f1e92",
        "metadata": {
          "vector_store_key": "1910.11949-9",
          "chunk_id": 41,
          "document_id": "1910.11949",
          "start_idx": 15508,
          "end_idx": 15935
        },
        "page_content": " : i like to eat healthy $>$ have you seen the last chapter? Elisabot : no i have not . $>$ where do you live? Elisabot : i live in the mountains As most of the metrics correlate very weakly with human judgements in the non-technical domain BIBREF31 we decide to evaluate our system with a simple user study with two patients. We present the user interface built and the feedback obtained from the patients. We developed a user",
        "type": "Document"
      },
      {
        "id": "c94d339e-36f5-4336-a21a-05be0b487cef",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      },
      {
        "id": "08676cf7-5cf4-4a2c-b9d7-ddbe513c2062",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 66,
          "document_id": "1810.00663",
          "start_idx": 25415,
          "end_idx": 25983
        },
        "page_content": " In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models. The previous section evaluated model performance on new instructions (and corresponding navigation plans) for environments that were previously seen at training time. Here, we examine whether the trained models succeed on environments that are completely new. The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How was the dataset collected?",
    "llm_answer": "The datasets were collected from Kaggle.com (StackOverflow), BioASQ's official website (Biomedical), and crowd sourcing or from examinations designed by educational experts (multi-choice QA).  For some datasets, 20,000 question titles or paper titles from 20 different tags were randomly selected.\n",
    "context": [
      {
        "id": "1c380a0c-1037-4628-8ada-168d9c8247bc",
        "metadata": {
          "vector_store_key": "1707.03904-9",
          "chunk_id": 13,
          "document_id": "1707.03904",
          "start_idx": 5304,
          "end_idx": 5850
        },
        "page_content": " subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short. We plan to release these annotations along with the datasets, and our retrieved documents for each question. Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated",
        "type": "Document"
      },
      {
        "id": "1eb879ee-df56-4263-afd5-19bb5ae493ff",
        "metadata": {
          "vector_store_key": "1701.00185-9",
          "chunk_id": 51,
          "document_id": "1701.00185",
          "start_idx": 18784,
          "end_idx": 19212
        },
        "page_content": " BIBREF41 . StackOverflow. We use the challenge data published in Kaggle.com. The raw dataset consists 3,370,528 samples through July 31st, 2012 to August 14, 2012. In our experiments, we randomly select 20,000 question titles from 20 different tags as in Table TABREF25 . Biomedical. We use the challenge data published in BioASQ's official website. In our experiments, we randomly select 20, 000 paper titles from 20 different",
        "type": "Document"
      },
      {
        "id": "e7b0d9fd-8534-4312-9d21-60512ad3cfae",
        "metadata": {
          "vector_store_key": "1910.00458-8",
          "chunk_id": 75,
          "document_id": "1910.00458",
          "start_idx": 26170,
          "end_idx": 26674
        },
        "page_content": "26 are human-generated and based on source documents or summaries in free text format. However, since annotators tend to copy spans as answers BIBREF27, the majority of answers are still extractive in these datasets. The multi-choice QA datasets are collected either via crowd sourcing, or collected from examinations designed by educational experts BIBREF7. In this type of QA datasets, besides token matching, a significant portion of questions require multi-sentence reasoning and external knowledge B",
        "type": "Document"
      },
      {
        "id": "e2f6e78c-bf7d-46ad-bd51-435c8758190a",
        "metadata": {
          "vector_store_key": "1707.03904-2",
          "chunk_id": 39,
          "document_id": "1707.03904",
          "start_idx": 14370,
          "end_idx": 14849
        },
        "page_content": "\ufffdopen-book\u201d setting, where they had access to a search engine over the short pseudo-documents extracted for each dataset (as described in Section \"Context Retrieval\" ). We decided to use short pseudo-documents for this exercise to reduce the burden of reading on the volunteers, though we note that the long pseudo-documents have greater coverage of answers. We also asked the volunteers to provide annotations to categorize the type of each question they were asked, and a label",
        "type": "Document"
      },
      {
        "id": "4ce440cb-dc47-4b8a-b61c-dee4751879c3",
        "metadata": {
          "vector_store_key": "1610.00879-1",
          "chunk_id": 11,
          "document_id": "1610.00879",
          "start_idx": 4217,
          "end_idx": 4654
        },
        "page_content": "damn', along with profane words. Heuristics other than these hashtags could have been used for dataset creation. For example, timestamps were a good option to account for time at which a tweet was posted. However, this could not be used because user's local times was not available, since very few users had geolocation enabled. The complete set of features is shown in Table TABREF7 . There are two sets of features: (a) N-gram features",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many annotators participated?",
    "llm_answer": "Four annotators.\n",
    "context": [
      {
        "id": "ae777c6e-5f84-42f7-8843-5ba4994ac482",
        "metadata": {
          "vector_store_key": "1703.05260-8",
          "chunk_id": 49,
          "document_id": "1703.05260",
          "start_idx": 17551,
          "end_idx": 18087
        },
        "page_content": " results were discussed with the authors. Throughout the whole annotation phase, annotators could discuss any emerging issues with the authors. All annotations were done by undergraduate students of computational linguistics. The annotation was rather time-consuming due to the complexity of the task, and thus we decided for single annotation mode. To assess annotation quality, a small sample of texts was annotated by all four annotators and their inter-annotator agreement was measured (see Section \"Inter-Annotator Agreement\" ). It",
        "type": "Document"
      },
      {
        "id": "aa16c8df-eb22-44b0-9a4f-494e45c4cc17",
        "metadata": {
          "vector_store_key": "1912.01673-1",
          "chunk_id": 33,
          "document_id": "1912.01673",
          "start_idx": 12785,
          "end_idx": 13244
        },
        "page_content": " how many annotators saw the same input sentence: 30 annotations were annotated by one person only, 30 annotations by two different persons etc. The last column shows the number of unique outputs obtained in that group. Across all cases, 96.8% of produced strings were unique. In line with instructions, the annotators were using the IMPOSSIBLE option scarcely (95 times, i.e. only 2%). It was also a case of 7 annotators only; the remaining 5 annotators were",
        "type": "Document"
      },
      {
        "id": "521b024e-758a-4249-af24-eed867d46c15",
        "metadata": {
          "vector_store_key": "1703.05260-8",
          "chunk_id": 50,
          "document_id": "1703.05260",
          "start_idx": 17876,
          "end_idx": 18350
        },
        "page_content": " inter-annotator agreement was measured (see Section \"Inter-Annotator Agreement\" ). It was found to be sufficiently high. Annotation of the corpus together with some pre- and post-processing of the data required about 500 hours of work. All stories were annotated with event and participant types (a total of 12,188 and 43,946 instances, respectively). On average there were 7 coreference chains per story with an average length of 6 tokens. After the first annotation round",
        "type": "Document"
      },
      {
        "id": "413c231b-c771-4c94-97a8-b20b290a953c",
        "metadata": {
          "vector_store_key": "1906.06448-6",
          "chunk_id": 32,
          "document_id": "1906.06448",
          "start_idx": 11534,
          "end_idx": 12014
        },
        "page_content": " the premise and/or the hypothesis is ungrammatical or does not make sense. Workers were paid US$0.04 for each question, and each question was assigned to three workers. To collect high-quality annotation results, we imposed ten test questions on each worker, and removed workers who gave more than three wrong answers. We also set the minimum time it should take to complete each question to 200 seconds. 1,237 workers contributed to this task, and we annotated gold labels of 15",
        "type": "Document"
      },
      {
        "id": "5e5dfe45-f801-413b-ac8a-29f23d004928",
        "metadata": {
          "vector_store_key": "1703.05260-8",
          "chunk_id": 67,
          "document_id": "1703.05260",
          "start_idx": 23974,
          "end_idx": 24460
        },
        "page_content": " 3 annotators (qualified majority vote) compared to the set of those pairs annotated by at least one person (see Figure 4 ). We take the result of 90.5% between annotators to be a good agreement. Figure 5 gives an overview of the number of event and participant types provided in the templates. Taking a flight and getting a haircut stand out with a large number of both event and participant types, which is due to the inherent complexity of the scenarios. In contrast, planting a tree",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they show their model discovers underlying syntactic structure?",
    "llm_answer": "The model implicitly models tree structure through attention.\n",
    "context": [
      {
        "id": "bff1dd16-d077-4093-89cd-b7848256cab0",
        "metadata": {
          "vector_store_key": "1906.00180-0",
          "chunk_id": 10,
          "document_id": "1906.00180",
          "start_idx": 3986,
          "end_idx": 4571
        },
        "page_content": " consistently higher testing accuracy provides evidence that the recurrent networks are not only capable of recognizing FOL entailment relations between unseen sentences. They can also outperform the tree-shaped models on this task, although they do not use any of the symbolic structure that seemed to explain the success of their recursive predecessors. The recurrent classifiers have learned to apply their own strategies, which we will investigate in the remainder of this paper. Compositionality is the ability to interpret and generate a possibly infinite number of constructions",
        "type": "Document"
      },
      {
        "id": "677f9b27-662d-4707-a904-2d9fd7048978",
        "metadata": {
          "vector_store_key": "1906.00180-0",
          "chunk_id": 45,
          "document_id": "1906.00180",
          "start_idx": 16996,
          "end_idx": 17562
        },
        "page_content": " nontrivial hierarchical structure. The formal language we defined did not exploit the full expressive power of first-order predicate logic; nevertheless by using standard first-order predicate logic, a standard theorem prover, and a set-up where the training set only covers a tiny fraction of the space of possible logical expressions, our experiments avoid the problems observed in earlier attempts to demonstrate logical reasoning in recurrent networks. The experiments performed in the last few sections moreover show that the GRU and LSTM architectures exhibit",
        "type": "Document"
      },
      {
        "id": "b7f97ae4-10c4-4e1c-b7b5-1451a463ec18",
        "metadata": {
          "vector_store_key": "1909.12140-2",
          "chunk_id": 5,
          "document_id": "1909.12140",
          "start_idx": 2277,
          "end_idx": 2783
        },
        "page_content": " These patterns were heuristically determined in a comprehensive linguistic analysis and encode syntactic and lexical features that can be derived from a sentence's parse tree. Each rule specifies (1) how to split up and rephrase the input into structurally simplified sentences and (2) how to set up a semantic hierarchy between them. They are recursively applied on a given source sentence in a top-down fashion. When no more rule matches, the algorithm stops and returns the generated discourse tree. In",
        "type": "Document"
      },
      {
        "id": "4b945524-169f-4c04-af25-068ff16e94bf",
        "metadata": {
          "vector_store_key": "1711.02013-3",
          "chunk_id": 10,
          "document_id": "1711.02013",
          "start_idx": 3876,
          "end_idx": 4391
        },
        "page_content": " analysis of recursive models that are able to learn tree structure without being exposed to any grammar trees. Our model is also able to infer tree structure in an unsupervised setting, but different from theirs, it is a recurrent network that implicitly models tree structure through attention. Apart from the approach of using recursive networks to capture structures, there is another line of research which try to learn recurrent features at multiple scales, which can be dated back to 1990s (e.g. BIBREF23 , B",
        "type": "Document"
      },
      {
        "id": "2a69d7dc-76ba-4d3c-b7e7-0bdbb8a6870f",
        "metadata": {
          "vector_store_key": "1711.02013-1",
          "chunk_id": 7,
          "document_id": "1711.02013",
          "start_idx": 2866,
          "end_idx": 3349
        },
        "page_content": " language model. With our model, we assume that language can be naturally represented as a tree-structured graph. The model is composed of three parts: We evaluate our model on three tasks: word-level language modeling, character-level language modeling, and unsupervised constituency parsing. The proposed model achieves (or is close to) the state-of-the-art on both word-level and character-level language modeling. The model's unsupervised parsing outperforms some strong baseline",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they measure performance of language model tasks?",
    "llm_answer": "Accuracy, EM-score, and F$_1$-score.\n",
    "context": [
      {
        "id": "4199c59c-1927-47cb-857b-9c1bed5dae20",
        "metadata": {
          "vector_store_key": "1911.03894-7",
          "chunk_id": 44,
          "document_id": "1911.03894",
          "start_idx": 15623,
          "end_idx": 16121
        },
        "page_content": " is also machine translated for all languages. The dataset is composed of 122k train, 2490 valid and 5010 test examples. As usual, NLI performance is evaluated using accuracy. To evaluate a model on a language other than English (such as French), we consider the two following settings: TRANSLATE-TEST: The French test set is machine translated into English, and then used with an English classification model. This setting provides a reasonable, although imperfect, way to circumvent the fact that",
        "type": "Document"
      },
      {
        "id": "07344119-d726-4d67-a8fe-3e22de9ad73d",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 46,
          "document_id": "1910.04269",
          "start_idx": 17760,
          "end_idx": 18217
        },
        "page_content": " of 1,500 samples for each of six languages. Out of 1,500 samples for each language, 1,200 were randomly selected as training dataset for that language and rest 300 as validation dataset using k-fold cross-validation. To sum up, we trained our model on 7,200 samples and validated it on 1800 samples comprising six languages. The results are discussed in next section. This paper discusses two end-to-end approaches which achieve state-of-the-art results in",
        "type": "Document"
      },
      {
        "id": "fa4c023c-4652-4382-a1cb-a09a525932f2",
        "metadata": {
          "vector_store_key": "2001.00137-6",
          "chunk_id": 34,
          "document_id": "2001.00137",
          "start_idx": 13325,
          "end_idx": 13822
        },
        "page_content": " of our model's robustness to different rates of data incompleteness. The dataset used to evaluate the models' performance is the Chatbot Natural Language Unerstanding (NLU) Evaluation Corpus, introduced by Braun et al. BIBREF20 to test NLU services. It is a publicly available benchmark and is composed of sentences obtained from a German Telegram chatbot used to answer questions about public transport connections. The dataset has two intents, namely Departure Time and Find Connection with 100",
        "type": "Document"
      },
      {
        "id": "ec1e0fc2-2147-4667-a800-7d4da5f437fa",
        "metadata": {
          "vector_store_key": "1910.05456-8",
          "chunk_id": 35,
          "document_id": "1910.05456",
          "start_idx": 12667,
          "end_idx": 13138
        },
        "page_content": " 1000, and 10000 examples, respectively. For all L1 languages, we train our models on the high-resource datasets with 10000 examples. For fine-tuning, we use the low-resource datasets. In Table TABREF18, we show the final test accuracy for all models and languages. Pretraining on EUS and NAV results in the weakest target language inflection models for ENG, which might be explained by those two languages being unrelated to ENG and making at least partial use of prefix",
        "type": "Document"
      },
      {
        "id": "bb75c3fa-e2c5-412d-9db8-2f31cd178291",
        "metadata": {
          "vector_store_key": "1908.06606-4",
          "chunk_id": 45,
          "document_id": "1908.06606",
          "start_idx": 15988,
          "end_idx": 16383
        },
        "page_content": ") and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are content clusters used to improve the prediction of incident severity?",
    "llm_answer": "The method uses machine learning to predict the degree of harm of an incident based on features of the record (location, external category, medical specialty) and the content clusters.\n",
    "context": [
      {
        "id": "ea295a17-6ca9-4d78-8bce-e57f8c1d5fde",
        "metadata": {
          "vector_store_key": "1909.00183-1",
          "chunk_id": 140,
          "document_id": "1909.00183",
          "start_idx": 51827,
          "end_idx": 52413
        },
        "page_content": " higher agreement in the severe and deadly incidents. These results confirm that our method performs as well as the original annotators but is better at identifying risky events. We have applied a multiscale graph partitioning algorithm (Markov Stability) to extract content-based clusters of documents from a textual dataset of incident reports in an unsupervised manner at different levels of resolution. The method uses paragraph vectors to represent the records and analyses the ensuing similarity graph of documents through multi-resolution capabilities to capture clusters without",
        "type": "Document"
      },
      {
        "id": "40873c85-cc88-4a8a-bdf1-d0409ae695b8",
        "metadata": {
          "vector_store_key": "1909.00183-1",
          "chunk_id": 117,
          "document_id": "1909.00183",
          "start_idx": 43409,
          "end_idx": 43881
        },
        "page_content": " clusters) does Infomap, which partitions graphs into small clique-like subgraphs BIBREF40, BIBREF56, provide a slightly improved $NMI$. Therefore, MS finds both relevant and high quality clusterings across all scales by sweeping the Markov time parameter. Here we approach the task of training a supervised classifier that predicts the degree of harm of an incident based on other features of the record (such as location, external category, and medical specialty) and on",
        "type": "Document"
      },
      {
        "id": "1fa9a335-2c22-494c-bf32-1beaf45176b8",
        "metadata": {
          "vector_store_key": "1909.00183-9",
          "chunk_id": 5,
          "document_id": "1909.00183",
          "start_idx": 2360,
          "end_idx": 2871
        },
        "page_content": " system. Such tools can also offer unbiased insight into the root cause analysis of incidents that could improve the safety and quality of care and efficiency of healthcare services. In this work, we showcase an algorithmic methodology that detects content-based groups of records in an unsupervised manner, based only on the free (unstructured) textual descriptions of the incidents. To do so, we combine deep neural-network high-dimensional text-embedding algorithms with graph-theoretical methods for multisc",
        "type": "Document"
      },
      {
        "id": "db955ab4-f4cb-4c17-b08a-e620eb872749",
        "metadata": {
          "vector_store_key": "1909.00183-9",
          "chunk_id": 9,
          "document_id": "1909.00183",
          "start_idx": 3949,
          "end_idx": 4515
        },
        "page_content": " levels of resolution; hence the content categories emerge from the textual data, rather than from pre-designed classifications. The obtained results can help mitigate human error or effort in finding the right category in complex classification trees. We illustrate in our analysis the insight gained from this unsupervised, multi-resolution approach in this specialised corpus of medical records. As an additional application, we use machine learning methods for the prediction of the degree of harm of incidents directly from the text in the NRLS incident reports",
        "type": "Document"
      },
      {
        "id": "99f3bd20-c3c7-4eff-bd45-6a798ee382c4",
        "metadata": {
          "vector_store_key": "1909.00183-9",
          "chunk_id": 150,
          "document_id": "1909.00183",
          "start_idx": 56239,
          "end_idx": 56836
        },
        "page_content": " itself. Additionally, the method allows for the discovery of emerging topics or classes of incidents directly from the data when such events do not fit existing categories by using methods for anomaly detection to decide whether new topic clusters should be created. This is a direction of future work. Further work also includes the use of our method to enable comparisons across healthcare organisations and also to monitor changes in their incident reports over time. Another interesting direction is to provide online classification suggestions to users based on the text they input as an aid",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What cluster identification method is used in this paper?",
    "llm_answer": "A simple clustering algorithm that initializes with random cluster assignments and iteratively proposes random exchanges of memberships is used.\n",
    "context": [
      {
        "id": "c34ecec7-76d3-4588-9e6b-2d3455a780f3",
        "metadata": {
          "vector_store_key": "1703.04617-4",
          "chunk_id": 62,
          "document_id": "1703.04617",
          "start_idx": 16880,
          "end_idx": 17340
        },
        "page_content": " vector, in stead of just choosing the closest cluster. Then the discriminative vector $\\mathbf {\\delta _x}$ with regard to these most relevant clusters are computed. All this information is combined to obtain the discriminative information. In order to keep the full information of input, we also copy the input question $\\mathbf {x}$ , together with the acquired discriminative information, to a feed-forward layer to obtain a new representation $\\mathbf {x^",
        "type": "Document"
      },
      {
        "id": "89e57c25-d0a7-4a44-b460-0d34d0c8d634",
        "metadata": {
          "vector_store_key": "1909.00183-2",
          "chunk_id": 64,
          "document_id": "1909.00183",
          "start_idx": 23950,
          "end_idx": 24472
        },
        "page_content": " represent graphs on the plane. This layout assigns a harmonic spring to each edge and finds through iterative rearrangements finds an arrangement on the plane that balances attractive and repulsive forces between nodes. Hence similar nodes tend to appear close together on this layout. We colour the nodes by either hand-coded categories (Figure FIGREF7) or multiscale MS communities (Figure FIGREF21). Spatially coherent colourings on this layout imply good clusters in terms of the similarity graph. Tracking membership",
        "type": "Document"
      },
      {
        "id": "4fcb552f-5f6f-4030-92e8-de5672a1d69c",
        "metadata": {
          "vector_store_key": "2003.06279-4",
          "chunk_id": 39,
          "document_id": "2003.06279",
          "start_idx": 15274,
          "end_idx": 15751
        },
        "page_content": " (DT), nearest neighbors (kNN), Naive Bayes (NB) and Support Vector Machines (SVM). We used some heuristics to optimize classifier parameters. Such techniques are described in the literature BIBREF49. The accuracy of the pattern recognition methods were evaluated using cross-validation BIBREF50. In summary, the methodology used in this paper encompasses the following steps: Network construction: here texts are mapped into a co-occurrence networks. Some variations exists in",
        "type": "Document"
      },
      {
        "id": "3e5f0bcf-46ad-4638-a961-7b9f8e1ab3fc",
        "metadata": {
          "vector_store_key": "1904.08386-4",
          "chunk_id": 11,
          "document_id": "1904.08386",
          "start_idx": 4630,
          "end_idx": 5112
        },
        "page_content": "ating all possible cluster assignments is intractable given the large search space ( INLINEFORM0 possible assignments). We devise a simple clustering algorithm to approximate this process. First, we initialize with random cluster assignments and define \u201ccluster strength\u201d to be the relative difference between \u201cintra-group\u201d Euclidean distance and \u201cinter-group\u201d Euclidean distance. Then, we iteratively propose random exchanges of memberships, only accepting these proposals when the",
        "type": "Document"
      },
      {
        "id": "97453121-681a-438b-8d91-33653ede0ea6",
        "metadata": {
          "vector_store_key": "1904.00648-2",
          "chunk_id": 3,
          "document_id": "1904.00648",
          "start_idx": 1369,
          "end_idx": 1822
        },
        "page_content": " (SVM), and a recurrent neural network architecture, a bidirectional LSTM with a CRF layer (biLSTM-CRF). The contributions in this work are summarized as follows: The paper is structured as follows. In Section 2, we present a review of the previous works related to Named Entity Recognition, focusing on its application on UGC and MIR. Afterwards, in Section 3 it is presented the methodology of this work, describing the dataset and the method proposed",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Why did they think this was a good idea?",
    "llm_answer": "They believed the superior visual grounding of their method would generate natural language explanations justifying answers to visual questions, thus increasing the capabilities of AI-based technologies to explain their decisions.\n",
    "context": [
      {
        "id": "97e95238-2c1e-497f-b311-c5c845047620",
        "metadata": {
          "vector_store_key": "2004.03744-4",
          "chunk_id": 42,
          "document_id": "2004.03744",
          "start_idx": 15146,
          "end_idx": 15649
        },
        "page_content": ", and use at least half of the highlighted words to write an explanation for their decision. The last two steps thus follow the quality control of crowd-sourced explanations introduced by Camburu BIBREF6. We also ensured that workers do not simply use a copy of the given hypothesis as explanation. We ensured all the above via in-browser checks before workers' submission. An example of collected explanations is given in Figure FIGREF21. To check the success of our crowdsourcing, we manually assessed",
        "type": "Document"
      },
      {
        "id": "1c99687c-43d8-447d-a342-25769f3987ff",
        "metadata": {
          "vector_store_key": "1912.01673-1",
          "chunk_id": 26,
          "document_id": "1912.01673",
          "start_idx": 10133,
          "end_idx": 10419
        },
        "page_content": ". This step was necessary to remove sentences that are: too unreal, out of this world, such as: Jedno fotonov\u00fd torp\u00e9do a je z tebe vesm\u00edrn\u00e1 topinka. \u201cOne photon torpedo and you're a space toast.\u201d photo captions (i.e. incomplete sentences), e.g.: Zvl\u00e1\u0161tn\u00ed ekv\u00e1dorsk\u00fd p\u0159\u00edpad Correa vs. Cr",
        "type": "Document"
      },
      {
        "id": "b4c866dc-cb22-40e9-a38b-e1aa1f416fae",
        "metadata": {
          "vector_store_key": "1808.00265-3",
          "chunk_id": 66,
          "document_id": "1808.00265",
          "start_idx": 25112,
          "end_idx": 25599
        },
        "page_content": " believe that the superior visual grounding provided by the proposed method can play a relevant role to generate natural language explanations to justify the answer to a given visual question. This scenario will help to demonstrate the relevance of our technique as a tool to increase the capabilities of AI based technologies to explain their decisions.  Acknowledgements: This work was partially funded by Oppo, Panasonic and the Millennium Institute for Foundational Research on Data.",
        "type": "Document"
      },
      {
        "id": "413c231b-c771-4c94-97a8-b20b290a953c",
        "metadata": {
          "vector_store_key": "1906.06448-6",
          "chunk_id": 32,
          "document_id": "1906.06448",
          "start_idx": 11534,
          "end_idx": 12014
        },
        "page_content": " the premise and/or the hypothesis is ungrammatical or does not make sense. Workers were paid US$0.04 for each question, and each question was assigned to three workers. To collect high-quality annotation results, we imposed ten test questions on each worker, and removed workers who gave more than three wrong answers. We also set the minimum time it should take to complete each question to 200 seconds. 1,237 workers contributed to this task, and we annotated gold labels of 15",
        "type": "Document"
      },
      {
        "id": "08695378-9162-4cd2-94ff-c494a86e2cd9",
        "metadata": {
          "vector_store_key": "1711.02013-4",
          "chunk_id": 5,
          "document_id": "1711.02013",
          "start_idx": 2121,
          "end_idx": 2586
        },
        "page_content": "supervised syntactic structure induction has been among the longstanding challenges of computational linguistic BIBREF11 , BIBREF12 , BIBREF13 . Researchers are interested in this problem for a variety of reasons: to be able to parse languages for which no annotated treebanks exist BIBREF14 ; to create a dependency structure to better suit a particular NLP application BIBREF10 ; to empirically argue for or against the poverty of the stimulus BIBREF15 , BIBREF16",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What QA models were used?",
    "llm_answer": "QA-BERT, XLM, XLNet, DistilBERT, and ALBERT.\n",
    "context": [
      {
        "id": "43fb2167-3ab4-4f1f-acd6-3a1896195fc6",
        "metadata": {
          "vector_store_key": "1605.08675-8",
          "chunk_id": 22,
          "document_id": "1605.08675",
          "start_idx": 8762,
          "end_idx": 9243
        },
        "page_content": " QA system called Hipisek were presented by BIBREF12 . It bases on a fairly common scheme of transforming a question into a search query and finding the most appropriate sentence, satisfying question constrains. Unfortunately, a very small evaluation set (65 question) and an unspecified knowledge base (gathered by a web crawler) make it difficult to compare the results. In their later works BIBREF13 , BIBREF14 , the team concentrated on spatial reasoning using a knowledge base",
        "type": "Document"
      },
      {
        "id": "a87ec9ec-2594-4938-98a5-87cb0432cd66",
        "metadata": {
          "vector_store_key": "2001.11268-6",
          "chunk_id": 72,
          "document_id": "2001.11268",
          "start_idx": 28640,
          "end_idx": 29140
        },
        "page_content": " systematic review automation. With our provided code and data, the QA-BERT model can be switched with more advanced transformer architectures, including XLM, XLNet, DistilBERT and ALBERT pre-trained models. More detailed investigations into multilingual predictions BIBREF26 pre-processing and predicting more than one PICO per sentence are reserved for future work. Limitations in the automatically annotated PubMed training data mostly consist of incomplete detection or noise P, I, and O entities",
        "type": "Document"
      },
      {
        "id": "acfc03f4-7aab-4728-9e7c-cc09ecc37517",
        "metadata": {
          "vector_store_key": "1804.03396-9",
          "chunk_id": 26,
          "document_id": "1804.03396",
          "start_idx": 9142,
          "end_idx": 9591
        },
        "page_content": " named QA4IE to overcome the weaknesses of existing IE systems. As we discussed above, the problem of step 1, 2 and 4 can be solved by existing work and we propose to solve the problem of step 3 with QA models. To train a high quality neural network QA model, we build a large IE benchmark in QA style named QA4IE benchmark which consists of 293K Wikipedia articles and 2 million golden relation triples with 636 different relation types. To adapt Q",
        "type": "Document"
      },
      {
        "id": "c8d4c64f-d3db-4ebf-8e7d-9f03adf63770",
        "metadata": {
          "vector_store_key": "1905.08949-7",
          "chunk_id": 75,
          "document_id": "1905.08949",
          "start_idx": 27701,
          "end_idx": 28047
        },
        "page_content": "ers (VAEs) and GANs, such models are also used in open-ended VQG to bring \u201ccreativity\u201d into generated questions BIBREF77 , BIBREF78 , showing promising results. This also brings hope to address deep QG from text, as applied in NLG: e.g., SeqGAN BIBREF79 and LeakGAN BIBREF80 . Endowing a QG system with the ability to ask deep questions will help",
        "type": "Document"
      },
      {
        "id": "c7bf943a-55c5-43c9-86d5-f5bb2d95df7e",
        "metadata": {
          "vector_store_key": "1908.10449-5",
          "chunk_id": 35,
          "document_id": "1908.10449",
          "start_idx": 12341,
          "end_idx": 12774
        },
        "page_content": " the information gathering procedure continues. In this section, we describe the high-level model structure and training strategies of QA-DQN. We refer readers to BIBREF18 for detailed information. We will release datasets and code in the near future. In this section, we use game step $t$ to denote one round of interaction between an agent with the iMRC environment. We use $o_t$ to denote text observation at game step $t$ and $q$",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do their results compare against other competitors in the PAN 2017 shared task on Author Profiling?",
    "llm_answer": "N-GrAM was the best-performing system out of 22 submissions.\n",
    "context": [
      {
        "id": "ed2aa19f-c003-44f3-b340-267bbae0bca4",
        "metadata": {
          "vector_store_key": "1707.03764-4",
          "chunk_id": 40,
          "document_id": "1707.03764",
          "start_idx": 15462,
          "end_idx": 15940
        },
        "page_content": " margin. Our system significantly out-performed the baseline on the joint task, as the baseline scored significantly lower for the gender task than for the variety task. We conclude that, for the current author profiling task, a seemingly simple system using word and character n-grams and an SVM classifier proves very hard to beat. Indeed, N-GrAM turned out to be the best-performing out of the 22 systems submitted in this shared task. Using additional training data, `smart'",
        "type": "Document"
      },
      {
        "id": "58ad8dfc-bc19-468e-90b7-bd117380344b",
        "metadata": {
          "vector_store_key": "1707.03764-0",
          "chunk_id": 17,
          "document_id": "1707.03764",
          "start_idx": 6499,
          "end_idx": 7022
        },
        "page_content": " PAN 2017 training data using five-fold cross-validation, unless otherwise specified. We extended the training dataset by adding data and gender labels from the PAN 16 Author Profiling shared task BIBREF5 . However, the additional data consistently resulted in lower cross-validation scores than when using only the training data provided with the PAN 17 task. One possible explanation for this is that our unigram model captures aspects that are tied specifically to the PAN 17 dataset, because it contains topics that may",
        "type": "Document"
      },
      {
        "id": "a70efe22-ba14-4242-be72-dc515fbf0537",
        "metadata": {
          "vector_store_key": "1904.08386-4",
          "chunk_id": 16,
          "document_id": "1904.08386",
          "start_idx": 6559,
          "end_idx": 7060
        },
        "page_content": " to these results, we now switch to our \u201codd-one-out\u201d task and compare directly to human performance. For each triplet of cities, we identify the intruder as the city with the maximum Euclidean distance from the other two. Interestingly, crowd workers achieve only slightly higher accuracy than ELMo city representations; their interannotator agreement is also low, which indicates that close reading to analyze literary coherence between multiple texts is a difficult task, even for human annotators.",
        "type": "Document"
      },
      {
        "id": "1c9f0f82-e385-4712-9d8d-eab5e266b0f9",
        "metadata": {
          "vector_store_key": "1911.03243-0",
          "chunk_id": 3,
          "document_id": "1911.03243",
          "start_idx": 1181,
          "end_idx": 1742
        },
        "page_content": ", allowing for substantially more reliable performance evaluation of QA-SRL parsers. To address worker quality, we systematically screen workers, provide concise yet effective guidelines, and perform a short training procedure, all within a crowd-sourcing platform. To address coverage, we employ two independent workers plus an additional one for consolidation \u2014 similar to conventional expert-annotation practices. In addition to yielding 25% more roles, our coverage gain is demonstrated by evaluating against expertly annotated data and comparison with Prop",
        "type": "Document"
      },
      {
        "id": "fb6c90b3-a2eb-4e22-b196-f4d978e9ae69",
        "metadata": {
          "vector_store_key": "1709.10217-4",
          "chunk_id": 28,
          "document_id": "1709.10217",
          "start_idx": 10224,
          "end_idx": 10506
        },
        "page_content": "qi Zhu and Xiaoming Shi from the research center for social computing and information retrieval for their support on the data annotation, establishing the system testing environment and the communication to the participants and help connect their systems to the testing environment.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "On which task does do model do worst?",
    "llm_answer": "Arithmetic problems.\n",
    "context": [
      {
        "id": "c36a2874-9c68-48be-b7f1-06ccd0d191dc",
        "metadata": {
          "vector_store_key": "1909.00183-6",
          "chunk_id": 82,
          "document_id": "1909.00183",
          "start_idx": 30425,
          "end_idx": 30882
        },
        "page_content": " is carried out with five-fold cross validation, using 80% of the data to train the model and the remaining 20% to test it. As a measure of performance of the classifiers and models, we use the weighted average of the F1 score for all levels of DoH, which takes into account both precision and recall, i.e., both the exactness and completeness of the model. We showcase our methodology through the analysis of the text from NRLS patient incident reports. In",
        "type": "Document"
      },
      {
        "id": "4596c59c-c395-4f7a-880d-81a3eb81efcf",
        "metadata": {
          "vector_store_key": "1910.00458-0",
          "chunk_id": 73,
          "document_id": "1910.00458",
          "start_idx": 25409,
          "end_idx": 25967
        },
        "page_content": " do math? To investigate this question, we sampled some arithmetic questions that are correctly predicted by our model, made small alterations to the passage or question, and then checked whether our model can still make correct choices. We found our model is very fragile to these minor alterations, implicating that the model is actually not that good at arithmetic problems. We provided one interesting example in the Section 3 of the Supplementary Material. There are increasing interests in machine reading comprehension (MRC) for question answering (QA",
        "type": "Document"
      },
      {
        "id": "a1818deb-c19f-40c4-849b-fba1a0612301",
        "metadata": {
          "vector_store_key": "1909.08859-9",
          "chunk_id": 57,
          "document_id": "1909.08859",
          "start_idx": 19980,
          "end_idx": 20459
        },
        "page_content": " a single model is trained to solve all the tasks at once, PRN and BIDAF w/ static memory perform comparably and give much better results than BIDAF. Note that the model performances in the multi-task training setting are worse than single-task performances. We believe that this is due to the nature of the tasks that some are more difficult than the others. We think that the performance could be improved by employing a carefully selected curriculum strategy BIBREF20. In Fig.",
        "type": "Document"
      },
      {
        "id": "e2bdbc4d-fed4-4cda-a705-5c573003e16d",
        "metadata": {
          "vector_store_key": "1911.11744-0",
          "chunk_id": 26,
          "document_id": "1911.11744",
          "start_idx": 9878,
          "end_idx": 10434
        },
        "page_content": " on the generated scenario, combinations of these three features are necessary to distinguish the targets from each other, allowing for tasks of varying complexity. To train our model, we generated a dataset of 20,000 demonstrated 7 DOF trajectories (6 robot joints and 1 gripper dimension) in our simulated environment together with a sentence generator capable of creating natural task descriptions for each scenario. In order to create the language generator, we conducted an human-subject study to collect sentence templates of a placement task as well",
        "type": "Document"
      },
      {
        "id": "8731ed08-bbaa-4311-b8ad-1e40984dd136",
        "metadata": {
          "vector_store_key": "1902.09087-0",
          "chunk_id": 72,
          "document_id": "1902.09087",
          "start_idx": 26639,
          "end_idx": 27095
        },
        "page_content": " NLP tasks. Figure FIGREF37 shows a case study comparing models in different input levels. The word level model is relatively coarse in utilizing informations, and finds a sentence with the longest overlap (5 words, 12 characters). However, it does not realize that the question is about numbers of people, and the \u201cDaoHang\u201d(navigate) in question is a verb, but noun in the sentence. The character level model finds a long sentence which covers most of the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "On which task does do model do best?",
    "llm_answer": "Cooperative game.\n",
    "context": [
      {
        "id": "e2bdbc4d-fed4-4cda-a705-5c573003e16d",
        "metadata": {
          "vector_store_key": "1911.11744-0",
          "chunk_id": 26,
          "document_id": "1911.11744",
          "start_idx": 9878,
          "end_idx": 10434
        },
        "page_content": " on the generated scenario, combinations of these three features are necessary to distinguish the targets from each other, allowing for tasks of varying complexity. To train our model, we generated a dataset of 20,000 demonstrated 7 DOF trajectories (6 robot joints and 1 gripper dimension) in our simulated environment together with a sentence generator capable of creating natural task descriptions for each scenario. In order to create the language generator, we conducted an human-subject study to collect sentence templates of a placement task as well",
        "type": "Document"
      },
      {
        "id": "3403420d-7275-4475-96c4-892e57999916",
        "metadata": {
          "vector_store_key": "1910.09399-9",
          "chunk_id": 31,
          "document_id": "1910.09399",
          "start_idx": 12524,
          "end_idx": 12961
        },
        "page_content": " to become increasingly adept at its specific task until a threshold is reached. This is analogous to a min-max game between the two models, according to the following equation: In Eq. (DISPLAY_FORM10), $x$ denotes a multi-dimensional sample, e.g., an image, and $z$ denotes a multi-dimensional latent space vector, e.g., a multidimensional data point following a predefined distribution function such as that of normal distributions. $D",
        "type": "Document"
      },
      {
        "id": "4929fae6-93c6-4b92-89e4-044833a06c92",
        "metadata": {
          "vector_store_key": "2004.02393-5",
          "chunk_id": 39,
          "document_id": "2004.02393",
          "start_idx": 13193,
          "end_idx": 13770
        },
        "page_content": " ability to distinguish the more informative signals from distant supervision labels. By introducing additional inductive bias of orders, the conditional selection model further improves with a large margin. Finally, our cooperative game gives the best performance, showing that a trained Reasoner has the ability of ignoring entity links that are irrelevant to the reasoning chain. Table TABREF22 demonstrates the effect of selecting directions, together with the methods' recall on head passages and tail passages. The latter is evaluated on a subset of bridge-type questions",
        "type": "Document"
      },
      {
        "id": "4596c59c-c395-4f7a-880d-81a3eb81efcf",
        "metadata": {
          "vector_store_key": "1910.00458-0",
          "chunk_id": 73,
          "document_id": "1910.00458",
          "start_idx": 25409,
          "end_idx": 25967
        },
        "page_content": " do math? To investigate this question, we sampled some arithmetic questions that are correctly predicted by our model, made small alterations to the passage or question, and then checked whether our model can still make correct choices. We found our model is very fragile to these minor alterations, implicating that the model is actually not that good at arithmetic problems. We provided one interesting example in the Section 3 of the Supplementary Material. There are increasing interests in machine reading comprehension (MRC) for question answering (QA",
        "type": "Document"
      },
      {
        "id": "b3a51a4f-253b-4283-9a0a-9a10e9a110da",
        "metadata": {
          "vector_store_key": "1901.02262-2",
          "chunk_id": 84,
          "document_id": "1901.02262",
          "start_idx": 23149,
          "end_idx": 23457
        },
        "page_content": " in the answer sentence; our model will be promising for controlling other styles such as length and speaking styles. Future work will involve exploring the potential of hybrid models combining extractive and abstractive approaches and improving the passage re-ranking and answerable question identification.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does counterfactual data augmentation aim to tackle bias?",
    "llm_answer": "The provided context does not describe how counterfactual data augmentation aims to tackle bias.  It discusses counterfactuals in the context of evaluating model truthfulness, but not in relation to bias mitigation.\n",
    "context": [
      {
        "id": "80857b4c-fa13-4ec8-9362-99918398ddf7",
        "metadata": {
          "vector_store_key": "1907.05664-3",
          "chunk_id": 38,
          "document_id": "1907.05664",
          "start_idx": 14088,
          "end_idx": 14575
        },
        "page_content": " in our experiment: \u201cWould the important words in the input be deleted, we would have a different summary\". Such counterfactuals are however more difficult to define for image classification for example, where it could be applying a mask over an image, or just filtering a colour or a pattern. We believe that defining a counterfactual and testing it allows us to measure and evaluate the truthfulness of the attributions and thus weight how much we can trust them. In this work, we have",
        "type": "Document"
      },
      {
        "id": "16ee5a25-9f0a-4412-a242-7a702c57ef17",
        "metadata": {
          "vector_store_key": "1812.06705-7",
          "chunk_id": 0,
          "document_id": "1812.06705",
          "start_idx": 0,
          "end_idx": 467
        },
        "page_content": "Deep neural network-based models are easy to overfit and result in losing their generalization due to limited size of training data. In order to address the issue, data augmentation methods are often applied to generate more training samples. Recent years have witnessed great success in applying data augmentation in the field of speech area BIBREF0 , BIBREF1 and computer vision BIBREF2 , BIBREF3 , BIBREF4 . Data augmentation in these areas can be easily performed",
        "type": "Document"
      },
      {
        "id": "c7c5bc35-3e4b-4593-ac2b-aaee1f76d9cc",
        "metadata": {
          "vector_store_key": "1910.14497-0",
          "chunk_id": 7,
          "document_id": "1910.14497",
          "start_idx": 2950,
          "end_idx": 3356
        },
        "page_content": "ometric bias mitigation uses the cosine distances between words to both measure and remove gender bias BIBREF0. This method implicitly defines bias as a geometric asymmetry between words when projected onto a subspace, such as the gender subspace constructed from a set of gender pairs such as $\\mathcal {P} = \\lbrace (he,she),(man,woman),(king,queen)...\\rbrace $. The projection of a vector $v$ onto $B$ (",
        "type": "Document"
      },
      {
        "id": "84e41b0f-ce3c-4ccc-8ae4-9a9b1415decd",
        "metadata": {
          "vector_store_key": "1906.06448-0",
          "chunk_id": 70,
          "document_id": "1906.06448",
          "start_idx": 26153,
          "end_idx": 26670
        },
        "page_content": " the data augmentation technique showed that accuracy on upward and downward inferences depends on the proportion of upward and downward inferences in the training set. This indicates that current neural models might have limitations on their generalization ability in monotonicity reasoning. We hope that the MED will be valuable for future research on more advanced models that are capable of monotonicity reasoning in a proper way. This work was partially supported by JST AIP- PRISM Grant Number JPMJCR18Y1, Japan",
        "type": "Document"
      },
      {
        "id": "84aafcf1-665c-4a34-8fef-2109d42e2ffc",
        "metadata": {
          "vector_store_key": "1910.14497-0",
          "chunk_id": 1,
          "document_id": "1910.14497",
          "start_idx": 500,
          "end_idx": 988
        },
        "page_content": " BIBREF1. The impact of this bias has manifested in a range of downstream tasks, ranging from autocomplete suggestions BIBREF2 to advertisement delivery BIBREF3, increasing the likelihood of amplifying harmful biases through the use of these models. The most well-established method thus far for mitigating bias relies on projecting target words onto a bias subspace (such as a gender subspace) and subtracting out the difference between the resulting distances BIBREF0. On the other hand",
        "type": "Document"
      }
    ]
  },
  {
    "query": "In the targeted data collection approach, what type of data is targetted?",
    "llm_answer": "Entity or entity aspect.\n",
    "context": [
      {
        "id": "e115f3ad-cfb7-4243-b123-51a669484bbc",
        "metadata": {
          "vector_store_key": "1703.04617-0",
          "chunk_id": 55,
          "document_id": "1703.04617",
          "start_idx": 14584,
          "end_idx": 15140
        },
        "page_content": "ity. The previous models are often trained for all questions without explicitly discriminating different question types; however, for a target question, both the common features shared by all questions and the specific features for a specific type of question are further considered in this paper, as they could potentially obey different distributions. In this paper we further explicitly model different types of questions in the end-to-end training. We start from a simple way to first analyze the word frequency of all questions, and obtain top-10 most",
        "type": "Document"
      },
      {
        "id": "01441853-36b1-4322-a8b7-0bf4ed1aeec9",
        "metadata": {
          "vector_store_key": "1909.00542-2",
          "chunk_id": 6,
          "document_id": "1909.00542",
          "start_idx": 2427,
          "end_idx": 2993
        },
        "page_content": " relevant concepts from designated terminologies and ontologies, relevant articles from PubMed, relevant snippets from the relevant articles, and relevant RDF triples from designated ontologies. Phase B provides a biomedical question and a list of relevant articles and snippets, and participant systems are expected to return the exact answers and the ideal answers. The training data is composed of the test data from all previous years, and amounts to 2,747 samples. There has been considerable research on the use of machine learning approaches for tasks related",
        "type": "Document"
      },
      {
        "id": "81d38da7-4a8e-48ed-b1df-f9b3a4c8e0c4",
        "metadata": {
          "vector_store_key": "1902.09314-0",
          "chunk_id": 1,
          "document_id": "1902.09314",
          "start_idx": 437,
          "end_idx": 950
        },
        "page_content": " \u201cservice\u201d and \u201cfood\u201d are negative and positive respectively. A target is usually an entity or an entity aspect. In recent years, neural network models are designed to automatically learn useful low-dimensional representations from targets and contexts and obtain promising results BIBREF0 , BIBREF1 . However, these neural network models are still in infancy to deal with the fine-grained targeted sentiment classification task. Attention mechanism, which has been successfully used in machine translation BIBREF",
        "type": "Document"
      },
      {
        "id": "99f3bd20-c3c7-4eff-bd45-6a798ee382c4",
        "metadata": {
          "vector_store_key": "1909.00183-9",
          "chunk_id": 150,
          "document_id": "1909.00183",
          "start_idx": 56239,
          "end_idx": 56836
        },
        "page_content": " itself. Additionally, the method allows for the discovery of emerging topics or classes of incidents directly from the data when such events do not fit existing categories by using methods for anomaly detection to decide whether new topic clusters should be created. This is a direction of future work. Further work also includes the use of our method to enable comparisons across healthcare organisations and also to monitor changes in their incident reports over time. Another interesting direction is to provide online classification suggestions to users based on the text they input as an aid",
        "type": "Document"
      },
      {
        "id": "cbd54b68-b7e3-4729-a2a2-2a4a9e27a0cc",
        "metadata": {
          "vector_store_key": "1904.00648-4",
          "chunk_id": 34,
          "document_id": "1904.00648",
          "start_idx": 13075,
          "end_idx": 13636
        },
        "page_content": ". In Figure 2, an example of Musical Work entity recognized in an user-generated tweet using the schedule information is presented. The entities recognized from the schedule matching are joined with the ones obtained directly from the statistical models. In the joined results, the criteria is to give priority to the entities recognized from the machine learning techniques. If they do not return any entities, the entities predicted by the schedule matching are considered. Our strategy is justified by the poorer results obtained by the NER based only on the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they determine which words are informative?",
    "llm_answer": "Informative words are nouns and verbs.  Two are matched if their raw text, lemmatizations, WordNet synsets, or aliases are the same.\n",
    "context": [
      {
        "id": "bdc2d0c0-9371-43d9-b8dc-b374f8b0429b",
        "metadata": {
          "vector_store_key": "1808.00265-5",
          "chunk_id": 39,
          "document_id": "1808.00265",
          "start_idx": 15648,
          "end_idx": 16012
        },
        "page_content": " Informative words are all nouns and verbs, where two informative words are matched if at least one of the following conditions is met: (1) Their raw text as they appear in INLINEFORM5 or INLINEFORM6 are the same; (2) Their lemmatizations (using NLTK BIBREF22 ) are the same; (3) Their synsets in WordNet BIBREF23 are the same; (4) Their aliases (provided from VG)",
        "type": "Document"
      },
      {
        "id": "ddce2653-c2d1-4d03-89a1-230565758c2e",
        "metadata": {
          "vector_store_key": "2002.06644-4",
          "chunk_id": 1,
          "document_id": "2002.06644",
          "start_idx": 523,
          "end_idx": 1035
        },
        "page_content": " the news objectively , thus implying the prevalence of the bias. Therefore, when presenting factual information, it becomes necessary to differentiate subjective language from objective language. There has been considerable work on capturing subjectivity using text-classification models ranging from linguistic-feature-based modelsBIBREF1 to finetuned pre-trained word embeddings like BERTBIBREF2. The detection of bias-inducing words in a Wikipedia statement was explored in BIBREF1. The authors propose the \"",
        "type": "Document"
      },
      {
        "id": "cd45ee02-9258-4e62-b22f-cb36e39ab775",
        "metadata": {
          "vector_store_key": "1611.02550-6",
          "chunk_id": 34,
          "document_id": "1611.02550",
          "start_idx": 12326,
          "end_idx": 12820
        },
        "page_content": " prior work. Specifically, we use the word discrimination task of Carlin et al. BIBREF20 , which is similar to a query-by-example task where the word segmentations are known. The evaluation consists of determining, for each pair of evaluation segments, whether they are examples of the same or different words, and measuring performance via the average precision (AP). We do this by measuring the cosine similarity between their acoustic word embeddings and declaring them to be the same if the",
        "type": "Document"
      },
      {
        "id": "45976e7f-fb64-4f87-8901-bc81f17467f2",
        "metadata": {
          "vector_store_key": "1908.08419-6",
          "chunk_id": 8,
          "document_id": "1908.08419",
          "start_idx": 2923,
          "end_idx": 3393
        },
        "page_content": " score and sequence score, which effectively evaluates the segmentation decisions. Specifically, we combine information branch and gated neural network to determine if the segment is a legal word, i.e., word score. Meanwhile, we use the hidden layer output of the long short-term memory (LSTM) BIBREF11 to find out how the word is linked to its surroundings, i.e., link score. The final decision on the selection of labeling samples is made by calculating the average of",
        "type": "Document"
      },
      {
        "id": "2dda2c84-3bb9-4c81-a68d-5d0d8ed067e4",
        "metadata": {
          "vector_store_key": "1911.12569-3",
          "chunk_id": 32,
          "document_id": "1911.12569",
          "start_idx": 12784,
          "end_idx": 13301
        },
        "page_content": " each word. For example, the candidate terms for the word good are: great, nice awesome and superb. DT offers the primary attention mechanism external knowledge in the form of candidate terms. It assists the system to perform better when presented with unseen words during testing as the unseen words could have been a part of the DT expansion list. For example, the system may not come across the word superb during training but it can appear in the test set. Since the system has already seen the word superb in the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What improvement does the MOE model make over the SOTA on language modelling?",
    "llm_answer": "The context states that the larger MoE model has a similar computational budget to the best published model from the literature, and a lower test perplexity after 10 epochs.\n",
    "context": [
      {
        "id": "38e44378-0b3c-4844-b4f4-fd83a446b352",
        "metadata": {
          "vector_store_key": "1701.06538-2",
          "chunk_id": 60,
          "document_id": "1701.06538",
          "start_idx": 22654,
          "end_idx": 23101
        },
        "page_content": " training examples (about 3 billion sentence pairs). Our training time was shorter due to the lower computational budget of our model. Results for the single-pair GNMT models, the multilingual GNMT model and the multilingual MoE model are given in Table TABREF50 . The MoE model achieves 19% lower perplexity on the dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats the multilingual GNMT model on 11 of the",
        "type": "Document"
      },
      {
        "id": "9736bfc5-ea5d-40e1-bb84-e23a683cfe38",
        "metadata": {
          "vector_store_key": "1701.06538-2",
          "chunk_id": 61,
          "document_id": "1701.06538",
          "start_idx": 23101,
          "end_idx": 23538
        },
        "page_content": "LEU score, the MoE model significantly beats the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English INLINEFORM0 Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number of real examples were highly oversampled in the training corpus.  This work is the first to",
        "type": "Document"
      },
      {
        "id": "af0c21ae-5181-40dc-8194-6fb4ea072a91",
        "metadata": {
          "vector_store_key": "1701.06538-2",
          "chunk_id": 87,
          "document_id": "1701.06538",
          "start_idx": 32078,
          "end_idx": 32501
        },
        "page_content": " with a hidden layer of size 2048. MoE-143M uses a hierarchical MoE layer with 256 experts, each with a hidden layer of size 8192. Both models have 4B parameters in the MoE layers. We searched for the best INLINEFORM0 for each model, and trained each model for 10 epochs. The two models achieved test perplexity of INLINEFORM0 and INLINEFORM1 respectively, showing that even in the presence of a large MoE, more computation",
        "type": "Document"
      },
      {
        "id": "4f972117-a3cf-49dc-95a0-7737e0300ccd",
        "metadata": {
          "vector_store_key": "1701.06538-2",
          "chunk_id": 44,
          "document_id": "1701.06538",
          "start_idx": 16844,
          "end_idx": 17346
        },
        "page_content": " similarly to the computationally-matched baseline models, while the largest of the models (4096 experts) achieved an impressive 24% lower perplexity on the test set. In addition to the largest model from the previous section, we trained two more MoE models with similarly high capacity (4 billion parameters), but higher computation budgets. These models had larger LSTMs, and fewer but larger and experts. Details can be found in Appendix UID77 . Results of these three models form the bottom line of",
        "type": "Document"
      },
      {
        "id": "3297fd18-2489-4a4d-a7b2-b3f620fcda80",
        "metadata": {
          "vector_store_key": "1701.06538-2",
          "chunk_id": 88,
          "document_id": "1701.06538",
          "start_idx": 32330,
          "end_idx": 32790
        },
        "page_content": " INLINEFORM1 respectively, showing that even in the presence of a large MoE, more computation is still useful. Results are reported at the bottom of Table TABREF76 . The larger of the two models has a similar computational budget to the best published model from the literature, and training times are similar. Comparing after 10 epochs, our model has a lower test perplexity by INLINEFORM2 . The models are similar in structure to the 8-million-operations-per",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the difference in performance between the interpretable system (e.g. vectors and cosine distance) and LSTM with ELMo system?",
    "llm_answer": "The context states that the ELMo-based model under-performed the first-occurrence baseline, implying the LSTM with ELMo system performed worse than a simpler baseline.  No direct comparison to an interpretable system (vectors and cosine distance) is provided.\n",
    "context": [
      {
        "id": "9d7f927a-0072-41ca-93b3-e545cc316153",
        "metadata": {
          "vector_store_key": "1909.02635-8",
          "chunk_id": 41,
          "document_id": "1909.02635",
          "start_idx": 15788,
          "end_idx": 16293
        },
        "page_content": " and precision are substantially increaesd compared to these baseline models. Interestingly, the ELMo-based model under-performs the first-occurrence baseline, indicating that the LSTM model is not learning much in terms of recognizing complex entity semantics grounded in long term contexts. Comparing the four variants of structuring input in proposed architectures as discussed in Section SECREF4, we observe that the document-level, entity-first model is the best performing variant. Given the left-to",
        "type": "Document"
      },
      {
        "id": "44ecacba-8fd2-43b1-9986-57175f30c5f1",
        "metadata": {
          "vector_store_key": "1909.02635-8",
          "chunk_id": 24,
          "document_id": "1909.02635",
          "start_idx": 9367,
          "end_idx": 9776
        },
        "page_content": " also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo$_{token}$ and ELMo$_{sent}$) BIBREF22 compared to GPT. Table TABREF10 compares the performance of the discussed models against the baselines, evaluating per-step entity prediction performance. Using the ground truth about ingredient's state, we also report the uncombined (UR) and combined (CR) recalls, which are per-timest",
        "type": "Document"
      },
      {
        "id": "81871031-8aba-4262-bfcd-30627e3dcb1f",
        "metadata": {
          "vector_store_key": "1810.09774-3",
          "chunk_id": 33,
          "document_id": "1810.09774",
          "start_idx": 12517,
          "end_idx": 13001
        },
        "page_content": " The sentences included in SNLI are also much simpler compared to those in MultiNLI, as they are taken from the Flickr image captions. This might also explain why the difference in accuracy for all of the six models is lowest when the models are trained on MultiNLI and tested on SNLI. It is also very surprising that the model with the biggest difference in accuracy was ESIM + ELMo which includes a pre-trained ELMo language model. BERT performed significantly better than the other",
        "type": "Document"
      },
      {
        "id": "d5e0b0c8-9760-4640-8119-192fcdaf5b71",
        "metadata": {
          "vector_store_key": "1809.09795-2",
          "chunk_id": 14,
          "document_id": "1809.09795",
          "start_idx": 5437,
          "end_idx": 5886
        },
        "page_content": ", our architecture is based on Embeddings from Language Model or ELMo BIBREF10 . The ELMo layer allows to recover a rich 1,024-dimensional dense vector for each word. Using CNNs, each vector is built upon the characters that compose the underlying words. As ELMo also contains a deep bi-directional LSTM on top of this character-derived vectors, each word-level embedding contains contextual information from their surroundings. Concretely, we use a",
        "type": "Document"
      },
      {
        "id": "86a9373f-5cc0-4173-b521-5aa96205677a",
        "metadata": {
          "vector_store_key": "1707.02377-6",
          "chunk_id": 69,
          "document_id": "1707.02377",
          "start_idx": 25491,
          "end_idx": 25897
        },
        "page_content": "ecC is better at handling longer paragraphs, LSTM-based methods are superior for relatively short sentences (of length in the order of 10s). We would like to point out that Doc2VecC is much faster to train and test comparing to skip-thought vectors. It takes less than 2 hours to learn the embeddings on the large book corpus for Doc2VecC on a desktop with Intel i7 2.2Ghz cpu, in comparison to the 2 weeks",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which language-pair had the better performance?",
    "llm_answer": "English-French\n",
    "context": [
      {
        "id": "567bbb0a-0092-4d21-9d90-c38602b59835",
        "metadata": {
          "vector_store_key": "1910.07481-4",
          "chunk_id": 31,
          "document_id": "1910.07481",
          "start_idx": 12134,
          "end_idx": 12501
        },
        "page_content": " language pair: English-French. En$\\rightarrow $Fr: The Document system obtained the best results considering all metrics on all test sets with strong statistical evidence. It surpassed the Baseline by 1.09 BLEU points and 0.85 TER points on tst2015, 0.75 BLEU points and 0.76 TER points on tst2014, and 0.48 BLEU points and 0.68 TER points on tst2013. Fr$\\rightarrow",
        "type": "Document"
      },
      {
        "id": "61dc26d0-f941-46d5-8b04-126be06594fc",
        "metadata": {
          "vector_store_key": "2004.01694-5",
          "chunk_id": 66,
          "document_id": "2004.01694",
          "start_idx": 25377,
          "end_idx": 25845
        },
        "page_content": " MT$_1$ in the pairwise ranking study, and exhibits fewer translation errors in our error classification. H$_B$ has a substantially higher number of missing words than the other two translations, which agrees with the lower perceived adequacy in the pairwise ranking. However, the analysis not only supports the findings of the pairwise ranking study, but also adds nuance to it. Even though H$_B$ has the highest number of deletions, and does worse than the other two",
        "type": "Document"
      },
      {
        "id": "570a8b4f-0dd5-473f-b4ba-5d2ad4cff5fe",
        "metadata": {
          "vector_store_key": "1911.00069-3",
          "chunk_id": 73,
          "document_id": "1911.00069",
          "start_idx": 23531,
          "end_idx": 23962
        },
        "page_content": " model transfer performance. Among the 2 neural network models, the Bi-LSTM model achieves a better cross-lingual RE performance than the CNN model for 6 out of the 7 target languages. In terms of absolute performance, the Bi-LSTM model achieves over $40.0$ $F_1$ scores for German, Spanish, Portuguese and Chinese. In terms of relative performance, it reaches over $75\\%$ of the accuracy of the supervised target-language RE model",
        "type": "Document"
      },
      {
        "id": "55242db0-1b45-4b4b-924a-5d4e91ea4920",
        "metadata": {
          "vector_store_key": "1911.00069-5",
          "chunk_id": 67,
          "document_id": "1911.00069",
          "start_idx": 21384,
          "end_idx": 21798
        },
        "page_content": " be 1K. We compare the performance of cross-lingual RE model transfer under the following bilingual word embedding mappings: Regular-1K: the regular mapping learned in (DISPLAY_FORM14) using 1K word pairs; Orthogonal-1K: the orthogonal mapping with length normalization learned in (DISPLAY_FORM16) using 1K word pairs (in this case we train the English RE models with the normalized English word embeddings); Semi-",
        "type": "Document"
      },
      {
        "id": "a07d90a3-3575-4b29-ad2c-2ba2ad87153b",
        "metadata": {
          "vector_store_key": "1808.09409-2",
          "chunk_id": 37,
          "document_id": "1808.09409",
          "start_idx": 14113,
          "end_idx": 14619
        },
        "page_content": " and Russian L2 sentences, the syntax-based system has better performances though it sometimes works worse on the corresponding L1 sentences, indicating the syntax-based systems are more robust when handling learner texts. Furthermore, the neural-parser-based system achieves the best overall performance on the L2 data. Though performing slightly worse than the neural syntax-agnostic one on the L1 data, it has much smaller INLINEFORM0 F, showing that as the syntactic analysis improves, the performances",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which psycholinguistic and basic linguistic features are used?",
    "llm_answer": "Ratio of quotation marks, exclamation marks, modal verbs, stop words, type-token ratio, hedging, named entity types, POS n-grams, sentiment and subjectivity scores, spell-checking, readability features (Coleman-Liau, Flesch), argument lexicon features, word lengths, sentence lengths, word/usage/linguistic-features, emoticons, and punctuation features.\n",
    "context": [
      {
        "id": "9d89b6fb-b36c-4270-98b0-ec080100b44e",
        "metadata": {
          "vector_store_key": "2001.05493-3",
          "chunk_id": 11,
          "document_id": "2001.05493",
          "start_idx": 4208,
          "end_idx": 4680
        },
        "page_content": " also handle different writing styles. Our approach is based on three main ideas: Deep-Text Learning. The goal is to learn long range associations, dependencies between regions of text, N-grams, key-patterns, topical information, and sequential dependencies. Exploiting psycho-linguistic features with basic linguistic features as meta-data. The main aim is to minimize the direct dependencies on in-depth grammatical structure of the language (i.e., to support code-mixed",
        "type": "Document"
      },
      {
        "id": "e2033bb5-9bd7-4158-9157-d4c296ef00e8",
        "metadata": {
          "vector_store_key": "2004.03034-7",
          "chunk_id": 39,
          "document_id": "2004.03034",
          "start_idx": 15301,
          "end_idx": 15680
        },
        "page_content": ", ratio of quotation marks, exclamation marks, modal verbs, stop words, type-token ratio, hedging BIBREF29, named entity types, POS n-grams, sentiment BIBREF30 and subjectivity scores BIBREF31, spell-checking, readibility features such as Coleman-Liau BIBREF32, Flesch BIBREF33, argument lexicon features BIBREF34 and surface features such as word lengths, sentence lengths, word",
        "type": "Document"
      },
      {
        "id": "6361765d-5341-4200-aca3-22ca69f418e3",
        "metadata": {
          "vector_store_key": "2001.05493-3",
          "chunk_id": 34,
          "document_id": "2001.05493",
          "start_idx": 12425,
          "end_idx": 12975
        },
        "page_content": "/usage/linguistic-features#pos-tagging We have identified a novel combination of features which are highly effective in aggression classification when applied in addition to the features obtained from the deep learning classifier at the classification layer. We have introduced two new features in addition to the previously available features. The first one is the Emotion Sensor Feature which use a statistical model to classify the words into 7 different classes based on the sentences obtained from twitter and blogs which contain total 1,185,540",
        "type": "Document"
      },
      {
        "id": "0d4746e8-e39c-4c61-8085-032a25f76b59",
        "metadata": {
          "vector_store_key": "2001.05493-3",
          "chunk_id": 12,
          "document_id": "2001.05493",
          "start_idx": 4503,
          "end_idx": 4930
        },
        "page_content": " in-depth grammatical structure of the language (i.e., to support code-mixed data). We have also included emoticons, and punctuation features with it. We use the term \"NLP Features\" to represent it in the entire paper. Dual embedding based on FastText and Glove. This dual embedding helps in high vocabulary coverage and to capture the rare and partially incorrect words in the text (specially by FastText BIBREF20). Our \"Deep-",
        "type": "Document"
      },
      {
        "id": "3849fed7-d4c5-43f8-af30-4bd49f874d97",
        "metadata": {
          "vector_store_key": "1810.06743-0",
          "chunk_id": 9,
          "document_id": "1810.06743",
          "start_idx": 3582,
          "end_idx": 4043
        },
        "page_content": " Thus, to prepare for later discussion, we divide the morphological features of a word into two categories: the modifiable inflectional features and the fixed lexical features. A part of speech (POS) is a coarse syntactic category (like verb) that begets a word's particular menu of lexical and inflectional features. In English, verbs express no gender, and adjectives do not reflect person or number. The part of speech dictates a set of inflectional slots to",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How have the differences in communication styles between Twitter and Facebook increase the complexity of the problem?",
    "llm_answer": "The context does not contain information about Facebook communication styles.  Therefore, the answer is not found in the context.\n",
    "context": [
      {
        "id": "979acf99-70ef-4b02-9d36-4de0eccd706f",
        "metadata": {
          "vector_store_key": "1710.01492-4",
          "chunk_id": 6,
          "document_id": "1710.01492",
          "start_idx": 2556,
          "end_idx": 3007
        },
        "page_content": " challenging with its use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and #hashtags. In addition to the genre difference, there is also a difference in length: social media messages are generally short, often length-limited by design as in Twitter, i.e., a sentence or a headline rather than a full document. How to handle such challenges has only",
        "type": "Document"
      },
      {
        "id": "aacd5386-8168-48b6-be79-379c22b0e7d0",
        "metadata": {
          "vector_store_key": "1911.12569-8",
          "chunk_id": 0,
          "document_id": "1911.12569",
          "start_idx": 0,
          "end_idx": 587
        },
        "page_content": "The emergence of social media sites with limited character constraint has ushered in a new style of communication. Twitter users within 280 characters per tweet share meaningful and informative messages. These short messages have a powerful impact on how we perceive and interact with other human beings. Their compact nature allows them to be transmitted efficiently and assimilated easily. These short messages can shape people's thought and opinion. This makes them an interesting and important area of study. Tweets are not only important for an individual but also for the companies",
        "type": "Document"
      },
      {
        "id": "1492e5fc-efba-47b9-8a98-b02e892005ab",
        "metadata": {
          "vector_store_key": "1809.09795-3",
          "chunk_id": 32,
          "document_id": "1809.09795",
          "start_idx": 11967,
          "end_idx": 12483
        },
        "page_content": "attention. This shows the effectiveness of using pre-trained character-based word representations, that allow us to recover many of the morpho-syntactic cues that tend to denote irony and sarcasm. Finally, our experiments showed that enlarging existing Twitter datasets by adding external soft-labeled data from the same media source does not yield improvements in the overall performance. This complies with the observations made by BIBREF18 . Since we have designed our augmentation tactics to maximize the overlap",
        "type": "Document"
      },
      {
        "id": "3026ec62-07cb-4c58-8d26-428e31d4c9e9",
        "metadata": {
          "vector_store_key": "2001.06888-5",
          "chunk_id": 1,
          "document_id": "2001.06888",
          "start_idx": 525,
          "end_idx": 1067
        },
        "page_content": " in Twitter micro-blogging social media is oriented towards their daily life, first witness news-reporting and engaging in various events (sports, political stands etc.). According to studies, news in twitter is propagated and reported faster than conventional news media BIBREF1. Thus, extracting first hand news and entities occurring in this fast and versatile online media gives valuable information. However, abridged and noisy content of Tweets makes it even more difficult and challenging for tasks such as named entity recognition and",
        "type": "Document"
      },
      {
        "id": "a332fb04-019a-4a03-89a6-068ea15a2a0c",
        "metadata": {
          "vector_store_key": "1809.10644-0",
          "chunk_id": 7,
          "document_id": "1809.10644",
          "start_idx": 2893,
          "end_idx": 3457
        },
        "page_content": " for metadata of the Twitter user, that were trained jointly in interleaved fashion. Both networks used in combination - and especially when trained using transfer learning - achieved higher F1 scores than either neural network classifier alone. In contrast to the methods described above, our approach relies on a simple word embedding (SWEM)-based architecture BIBREF5 , reducing the number of required parameters and length of training required, while still yielding improved performance and resilience across related classification tasks. Moreover, our network",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What data/studies do the authors provide to support the assertion that the majority of aggressive conversations contain code-mixed languages?",
    "llm_answer": "The authors cite the TRAC-2018 dataset as providing code-mixed data.  They specifically mention a training dataset provided by TRAC.\n",
    "context": [
      {
        "id": "c40193a6-dff2-4024-a745-c02fca6a9c39",
        "metadata": {
          "vector_store_key": "2001.05493-6",
          "chunk_id": 5,
          "document_id": "2001.05493",
          "start_idx": 2052,
          "end_idx": 2570
        },
        "page_content": " different languages in a single utterance or sentence, whereas code-switching refers to the co-occurrence of speech extracts belonging to two different grammatical systemsBIBREF3. This language interchange makes the grammar more complex and thus it becomes tough to handle it by traditional algorithms. Thus the presence of high percentage of code-mixed content in social media text has increased the complexity of the aggression detection task. For example, the dataset provided by the organizers of TRAC-2018 BIBREF",
        "type": "Document"
      },
      {
        "id": "1989e799-b6d2-4195-a1e9-d3ed84afa17c",
        "metadata": {
          "vector_store_key": "2001.05493-6",
          "chunk_id": 23,
          "document_id": "2001.05493",
          "start_idx": 8516,
          "end_idx": 9051
        },
        "page_content": " training dataset provided by TRAC. This also proves the effectiveness of our approach. Our system outperforms all the previous state of the art approaches used for aggression identification on English code-mixed TRAC data, while being trained only from Facebook comments the system outperforms other approaches on the additional Twitter test set. The remaining part of this paper is organized as follows: Section SECREF2 is an overview of related work. Section SECREF3 presents the methodology and algorithmic details. Section SECREF4",
        "type": "Document"
      },
      {
        "id": "b9ad45d4-2fd9-4ee8-beb2-f2362e15a56f",
        "metadata": {
          "vector_store_key": "2001.05493-6",
          "chunk_id": 80,
          "document_id": "2001.05493",
          "start_idx": 26969,
          "end_idx": 27231
        },
        "page_content": " future, we are planning to pay attention on a generalized language model for code-mixed texts which can also handle Hindi-code-mixed and other multi-lingual code-mixed datasets (i.e., trying to reduce the dependencies on language-specific code-mixed resources).",
        "type": "Document"
      },
      {
        "id": "2133040c-792e-4071-83ee-09aa72f2f25e",
        "metadata": {
          "vector_store_key": "2001.05493-6",
          "chunk_id": 10,
          "document_id": "2001.05493",
          "start_idx": 3764,
          "end_idx": 4208
        },
        "page_content": " systems either shows high performance on (a) Twitter dataset or (b) Facebook dataset (given in the TRAC-2018), but not on both English code-mixed datasets. This may be due to the text style or level of complexities of both datasets. So, we concentrated to develop a robust system for English code-mixed texts, and uni-lingual texts, which can also handle different writing styles. Our approach is based on three main ideas: Deep-Text Learning.",
        "type": "Document"
      },
      {
        "id": "ef229e00-b34d-452e-9bd2-c1258b9e4518",
        "metadata": {
          "vector_store_key": "2001.05493-6",
          "chunk_id": 79,
          "document_id": "2001.05493",
          "start_idx": 26641,
          "end_idx": 27169
        },
        "page_content": " is noisy and code-mixed. We presented an ensemble of deep learning models which outperform previous approaches by sufficient margin while having the ability to generalize across domains. In future, we will explore other methods to increase the understanding of deep learning models on group targeted text, although the categories are well defined we will look after if we further fine-tune the categories with more data. In the future, we are planning to pay attention on a generalized language model for code-mixed texts which",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What datasets are used to evaluate the model?",
    "llm_answer": "Swissmetro, development, and test sets.  Also, news (bn and nw), and a portion of bc.\n",
    "context": [
      {
        "id": "3488785c-d053-43b7-8293-c3c50f3c5473",
        "metadata": {
          "vector_store_key": "1909.00154-2",
          "chunk_id": 65,
          "document_id": "1909.00154",
          "start_idx": 24630,
          "end_idx": 25057
        },
        "page_content": " vectors) Development set: 20% of the dataset (2003 vectors) Test set: 20% of the dataset (2003 vectors) The PyLogit package BIBREF11 also uses Swissmetro as an example. Therefore, our model specifications will extend the default one from this package. We re-estimated this model with the train set and validated with testset. The results are shown in tables TABREF31 and TABREF32. Since we are comparing the models at the test",
        "type": "Document"
      },
      {
        "id": "c94d339e-36f5-4336-a21a-05be0b487cef",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      },
      {
        "id": "9cdcf3d4-51f3-43d9-9cea-7a3d8cdb29a9",
        "metadata": {
          "vector_store_key": "1909.00154-2",
          "chunk_id": 61,
          "document_id": "1909.00154",
          "start_idx": 23166,
          "end_idx": 23592
        },
        "page_content": "CCM), for example. We will apply the methodology to the well-known \u201cSwissmetro\" dataset. We will compare it with a dummy variables and PCA baselines. We will follow the 3-way experimental design mentioned before: split the dataset into train, development and test sets, so that the embeddings, PCA eigenvectors and the choice model are estimated from the same train and development sets, and validate it out-of-sample. For the",
        "type": "Document"
      },
      {
        "id": "9294c12b-a98f-42d1-8f4c-b38676b1c154",
        "metadata": {
          "vector_store_key": "1909.00183-7",
          "chunk_id": 124,
          "document_id": "1909.00183",
          "start_idx": 45834,
          "end_idx": 46293
        },
        "page_content": " We first train models using three categories from the reports: location (L), external hand-coded category (C), and medical specialty (S). We also compute the performance of models trained on text features, both TF-iDF and Doc2Vec. We also study models trained on a mixture of text and categories. Finally, we run Markov Stability as described above to obtain cluster labels for each dataset (O and PRC) at different resolutions (70, 45, 30 and 13 communities",
        "type": "Document"
      },
      {
        "id": "726a5191-bdde-433d-a6f5-4ea54e338988",
        "metadata": {
          "vector_store_key": "1911.00069-9",
          "chunk_id": 61,
          "document_id": "1911.00069",
          "start_idx": 19175,
          "end_idx": 19594
        },
        "page_content": "IBREF6, which uses news (the union of bn and nw) as the training set, a half of bc as the development set and the remaining data as the test set. We learn the model parameters using Adam BIBREF32. We apply dropout BIBREF33 to the hidden layers to reduce overfitting. The development set is used for tuning the model hyperparameters and for early stopping. In Table TABREF40 we compare our models with the best models in",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What baseline models do they compare against?",
    "llm_answer": "NABERT+ baseline, Stanford system, Multigraph system, MIR system.\n",
    "context": [
      {
        "id": "555a266e-0a08-43e5-b719-8cd2fd10ebb8",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "dc50a5cf-b755-41a0-a173-1b561db0ed05",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "c357514b-19f8-468b-b68e-b8bc90741f38",
        "metadata": {
          "vector_store_key": "1701.06538-1",
          "chunk_id": 45,
          "document_id": "1701.06538",
          "start_idx": 17346,
          "end_idx": 17826
        },
        "page_content": ". Details can be found in Appendix UID77 . Results of these three models form the bottom line of Figure FIGREF32 -right. Table TABREF33 compares the results of these models to the best previously-published result on this dataset . Even the fastest of these models beats the best published result (when controlling for the number of training epochs), despite requiring only 6% of the computation. We trained our models using TensorFlow BIBREF30 on clusters containing 16-32 Tesla K",
        "type": "Document"
      },
      {
        "id": "cbe1ae0c-a3e6-408d-b599-2f1e42c2205e",
        "metadata": {
          "vector_store_key": "1603.04553-1",
          "chunk_id": 36,
          "document_id": "1603.04553",
          "start_idx": 10414,
          "end_idx": 10861
        },
        "page_content": " baseline with two deterministic systems, namely Stanford: the Stanford system BIBREF10 and Multigraph: the unsupervised multigraph system BIBREF26 , and one unsupervised system, namely MIR: the unsupervised system using most informative relations BIBREF27 . Our model outperforms the three baseline systems on all the evaluation metrics. Specifically, our model achieves improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford system,",
        "type": "Document"
      },
      {
        "id": "11d9b2e6-e20d-4ab9-8d7c-c35a7e2d27e6",
        "metadata": {
          "vector_store_key": "1603.07044-6",
          "chunk_id": 43,
          "document_id": "1603.07044",
          "start_idx": 14997,
          "end_idx": 15515
        },
        "page_content": " task A already had a substantial amount of training data. Table 4 gives the final comparison between different models (we only list the MAP score because it is the official score for the challenge). Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the differences with previous applications of neural networks for this task?",
    "llm_answer": "The context mentions further exploration of feature extraction techniques (e.g., Constant-Q transform, FFT), improvements in neural network architectures (e.g., concatenating 1D/2D ConvNets, deeper networks with skip connections/Inception modules), and experiments using different CNN variants.  It also highlights the use of LSTMs, GRNNs, and deep layers of neural networks to learn character representations, contrasting with earlier methods not explicitly detailed.\n",
    "context": [
      {
        "id": "c270760a-3f97-41b4-84b6-68ddf24229ea",
        "metadata": {
          "vector_store_key": "1910.04269-4",
          "chunk_id": 54,
          "document_id": "1910.04269",
          "start_idx": 20617,
          "end_idx": 21192
        },
        "page_content": " robust to variations which might be present in real world scenarios. There can be further exploration of various feature extraction techniques like Constant-Q transform and Fast Fourier Transform and assessment of their impact on Language Identification. There can be further improvements in neural network architectures like concatenating the high level features obtained from 1D-ConvNet and 2D-ConvNet, before performing classification. There can be experiments using deeper networks with skip connections and Inception modules. These are known to have positively impacted",
        "type": "Document"
      },
      {
        "id": "32bc6f7b-91b5-4df8-ba7b-f914ff37a3af",
        "metadata": {
          "vector_store_key": "1908.08419-5",
          "chunk_id": 13,
          "document_id": "1908.08419",
          "start_idx": 4671,
          "end_idx": 5143
        },
        "page_content": " Deep Learning Methods. Recently, neural networks have been applied in CWS tasks. To name a few, Zheng et al. BIBREF14 used deep layers of neural networks to learn feature representations of characters. Chen et al. BIBREF6 adopted LSTM to capture the previous important information. Chen et al. BIBREF18 proposed a gated recursive neural network (GRNN), which contains reset and update gates to incorporate the complicated combinations of characters. Jiang and Tang BIBREF",
        "type": "Document"
      },
      {
        "id": "6bdc0729-aca4-426b-b4cf-d817a9699633",
        "metadata": {
          "vector_store_key": "1812.07023-4",
          "chunk_id": 0,
          "document_id": "1812.07023",
          "start_idx": 0,
          "end_idx": 519
        },
        "page_content": "Deep neural networks have been successfully applied to several computer vision tasks such as image classification BIBREF0 , object detection BIBREF1 , video action classification BIBREF2 , etc. They have also been successfully applied to natural language processing tasks such as machine translation BIBREF3 , machine reading comprehension BIBREF4 , etc. There has also been an explosion of interest in tasks which combine multiple modalities such as audio, vision, and language together. Some popular multi-modal tasks",
        "type": "Document"
      },
      {
        "id": "dfbf8f51-6e5c-4751-822e-ab48ade54a77",
        "metadata": {
          "vector_store_key": "1701.00185-7",
          "chunk_id": 21,
          "document_id": "1701.00185",
          "start_idx": 8101,
          "end_idx": 8526
        },
        "page_content": ", CNN is better to learn non-biased implicit features which has been successfully exploited for many supervised NLP learning tasks as described in Section SECREF1 , and various CNN based variants are proposed in the recent works, such as Dynamic Convolutional Neural Network (DCNN) BIBREF10 , Gated Recursive Convolutional Neural Network (grConv) BIBREF31 and Self-Adaptive Hierarchical Sentence model (AdaSent) BIBREF32 . In",
        "type": "Document"
      },
      {
        "id": "74b0c32c-7c50-4022-9d5a-72ef320da768",
        "metadata": {
          "vector_store_key": "1810.09774-3",
          "chunk_id": 0,
          "document_id": "1810.09774",
          "start_idx": 0,
          "end_idx": 467
        },
        "page_content": "Natural Language Inference (NLI) has attracted considerable interest in the NLP community and, recently, a large number of neural network-based systems have been proposed to deal with the task. One can attempt a rough categorization of these systems into: a) sentence encoding systems, and b) other neural network systems. Both of them have been very successful, with the state of the art on the SNLI and MultiNLI datasets being 90.4%, which is our baseline with BERT",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much improvement is gained from the proposed approaches?",
    "llm_answer": "The context does not state a specific improvement value for the proposed approaches.  It mentions outperformance in some comparisons but does not give numerical gains.\n",
    "context": [
      {
        "id": "644dd4da-698b-443e-aeae-137d54b264d9",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 23,
          "document_id": "1909.00542",
          "start_idx": 8896,
          "end_idx": 9426
        },
        "page_content": " the results are compatible with the top part of the table and prior work suggesting that classification-based approaches improve over regression-based approaches. We also experiment with the use of reinforcement learning techniques. Again these experiments are based on BIBREF2, who uses REINFORCE to train a global policy. The policy predictor uses a simple feedforward network with a hidden layer. The results reported by BIBREF2 used ROUGE Recall and indicated no improvement with respect to deep learning architectures. Human",
        "type": "Document"
      },
      {
        "id": "89c4b13e-2aa6-4de2-9819-446385c9ad66",
        "metadata": {
          "vector_store_key": "2002.00652-4",
          "chunk_id": 58,
          "document_id": "2002.00652",
          "start_idx": 18202,
          "end_idx": 18702
        },
        "page_content": " modeling methods, they do not significantly improve the performance as we expected. As mentioned in Section SECREF1, intuitively, methods which only use the precedent SQL enjoys better generalizability. To validate it, we further conduct an out-of-distribution experiment to assess the generalizability of different context modeling methods. Concretely, we select three representative methods and train them on questions at turn 1 and 2, whereas test them at turn 3, 4 and beyond. As shown in Figure",
        "type": "Document"
      },
      {
        "id": "97d0589b-102d-402c-bcb2-27484aa3c4fb",
        "metadata": {
          "vector_store_key": "1701.00185-8",
          "chunk_id": 76,
          "document_id": "1701.00185",
          "start_idx": 27376,
          "end_idx": 27736
        },
        "page_content": " and NMI performance of our proposed approaches and four baseline methods, K-means, SkipVec, RecNN and Para2vec based clustering methods. Intuitively, we get a general observation that (1) BoW based approaches, including K-means (TF) and K-means (TF-IDF), and SkipVec based approaches perform not well; (2) RecNN based approaches, both RecNN (Ave.) and RecNN (",
        "type": "Document"
      },
      {
        "id": "7a3763d4-793b-483d-9bf2-cdefe870cd27",
        "metadata": {
          "vector_store_key": "2003.07433-0",
          "chunk_id": 66,
          "document_id": "2003.07433",
          "start_idx": 25731,
          "end_idx": 26161
        },
        "page_content": "REF28 illustrates the comparisons between LAXARY and Coppersmith et. al. proposed method. Here we can see, the outperformance of our proposed method as well as the importance of $s-score$ estimation. We also illustrates the importance of $\\alpha -score$ and $S-score$ in Fig FIGREF30. Fig FIGREF30 illustrates that if we change the number of training samples (%), LAXARY models outperforms Coppersmith et. al. proposed model under",
        "type": "Document"
      },
      {
        "id": "e47b2d6a-c071-4a61-b692-68b484884e3d",
        "metadata": {
          "vector_store_key": "1908.07816-7",
          "chunk_id": 10,
          "document_id": "1908.07816",
          "start_idx": 4017,
          "end_idx": 4509
        },
        "page_content": " relate. It is the first time such an approach is designed with consideration for the human judges. Our main goal is to increase the objectivity of the results and reduce judges' mistakes due to out-of-context dialogs they have to evaluate. The rest of the paper unfolds as follows. Section SECREF2 discusses some related work. In Section SECREF3, we give detailed description of the methodology. We present experimental results and some analysis in Section SECREF4. The paper is concluded in",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is infinite-length sequence generation a result of training with maximum likelihood?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "72739a92-0756-4e6c-a1a6-535f04374989",
        "metadata": {
          "vector_store_key": "2002.02492-2",
          "chunk_id": 98,
          "document_id": "2002.02492",
          "start_idx": 30976,
          "end_idx": 31467
        },
        "page_content": " Maximum likelihood learning fits the model $p_{\\theta }$ using the data distribution, whereas a decoded sequence from the trained model follows the distribution $q_{\\mathcal {F}}$ induced by a decoding algorithm. Based on this discrepancy, we make a strong conjecture: we cannot be guaranteed to obtain a good consistent sequence generator using maximum likelihood learning and greedy decoding. Sequence-level learning, however, uses a decoding algorithm during training BIBREF25, BIBREF26.",
        "type": "Document"
      },
      {
        "id": "7099f9d4-7019-4618-adc9-28bf4fdfe9ca",
        "metadata": {
          "vector_store_key": "2002.02492-9",
          "chunk_id": 77,
          "document_id": "2002.02492",
          "start_idx": 24026,
          "end_idx": 24555
        },
        "page_content": " is inconsistent. We use $L=1500$, which is more than 10 times the maximum training sequence length. In each experiment, we report the mean and standard deviation of metrics across 10 independent initializations. Unless specified otherwise, we report metrics using the test context distribution, since the train, valid, and randomly generated context distributions had similar results. We train recurrent language models for sequence completion with maximum likelihood, using the following loss on each sequence $Y=(c_1,\\ldots ,c",
        "type": "Document"
      },
      {
        "id": "affed995-f1ed-4d29-a32f-54f1a0bf3e6a",
        "metadata": {
          "vector_store_key": "1905.13413-1",
          "chunk_id": 23,
          "document_id": "1905.13413",
          "start_idx": 7437,
          "end_idx": 7950
        },
        "page_content": " also at assertion generation, suggesting that extractions of higher quality can be added as training samples to continue this training process iteratively. The resulting iterative learning procedure (alg:iter) incrementally includes extractions generated by the current model as training samples to optimize the binary classification loss to obtain a better model, and this procedure is continued until convergence. [t] training data $\\mathcal {D}$ , initial model $\\theta ^{(0)}$ model after convergence $\\theta",
        "type": "Document"
      },
      {
        "id": "f886dd7a-7e02-47f5-9111-f92a0767c558",
        "metadata": {
          "vector_store_key": "2002.02492-2",
          "chunk_id": 18,
          "document_id": "2002.02492",
          "start_idx": 6877,
          "end_idx": 7247
        },
        "page_content": " and can be used for finding the most probable sequence, although it requires a substantial number of samples to achieve a low-variance estimate. Definition 2.5 (Ancestral sampling) Ancestral sampling $\\mathcal {F}_{\\text{anc}}$ generates a sequence from a recurrent language model $p_{\\theta }$ given context $C$ by recursively sampling from $p_{\\theta }(y_t\\,|\\,\\tilde",
        "type": "Document"
      },
      {
        "id": "a3e8740a-e277-4626-8df0-ba8410637e7e",
        "metadata": {
          "vector_store_key": "2002.11402-3",
          "chunk_id": 18,
          "document_id": "2002.11402",
          "start_idx": 6558,
          "end_idx": 7010
        },
        "page_content": " it was evident that, sequence length plays a vital role in deciding context and locally/globally important n-grams. Final output is a concatenation of both the model outputs. Trained the topic model on single 32gb NVidia-V100 and it took around 50 hours to train the model with sequence length 512. We had to take 256gb ram machine to accommodate all data in memory for faster read/write. Also, trained model with 64 sequence length in around 17 hours",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is dataset for this challenge?",
    "llm_answer": "The provided text describes various datasets with sizes ranging from 20,000 to 3,370,528 samples,  287,226 documents, and more.  No single \"challenge\" dataset size is explicitly stated.\n",
    "context": [
      {
        "id": "4c6c7b55-7160-4f18-8f21-89da87c2f8a0",
        "metadata": {
          "vector_store_key": "1603.07044-4",
          "chunk_id": 29,
          "document_id": "1603.07044",
          "start_idx": 9966,
          "end_idx": 10403
        },
        "page_content": " for task A, there are a total number of 26,700 question-comment pairs. For task B, there are 2,670 question-question pairs. For task C, there are 26,700 question-comment pairs. The test dataset includes 50 questions, 500 related questions and 5,000 comments which do not overlap with the training set. To evaluate the performance, we use mean average precision (MAP) and F1 score. Table 2 shows the initial results using the RNN encoder",
        "type": "Document"
      },
      {
        "id": "9270562a-8e53-4e1d-b760-74d8fd0bc872",
        "metadata": {
          "vector_store_key": "1905.06566-7",
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "1eb879ee-df56-4263-afd5-19bb5ae493ff",
        "metadata": {
          "vector_store_key": "1701.00185-9",
          "chunk_id": 51,
          "document_id": "1701.00185",
          "start_idx": 18784,
          "end_idx": 19212
        },
        "page_content": " BIBREF41 . StackOverflow. We use the challenge data published in Kaggle.com. The raw dataset consists 3,370,528 samples through July 31st, 2012 to August 14, 2012. In our experiments, we randomly select 20,000 question titles from 20 different tags as in Table TABREF25 . Biomedical. We use the challenge data published in BioASQ's official website. In our experiments, we randomly select 20, 000 paper titles from 20 different",
        "type": "Document"
      },
      {
        "id": "d4f8006a-0fdf-4c79-8105-14b5833206b1",
        "metadata": {
          "vector_store_key": "1901.02262-2",
          "chunk_id": 66,
          "document_id": "1901.02262",
          "start_idx": 16859,
          "end_idx": 17269
        },
        "page_content": " NLG task will be \u201cThere are 16 tablespoons in a cup.\u201d In addition to the ALL dataset, we prepared two subsets (Table 1 ). The ANS set consists of answerable questions, and the WFA set consists of the answerable questions and well-formed answers, where WFA $\\subset $ ANS $\\subset $ ALL. We trained our model on a machine with eight NVIDIA P100 GPUs. Our model was jointly trained with the two answer styles in",
        "type": "Document"
      },
      {
        "id": "f4274fc8-8af1-4f7e-a32d-82ff390e6bcc",
        "metadata": {
          "vector_store_key": "1907.00758-4",
          "chunk_id": 35,
          "document_id": "1907.00758",
          "start_idx": 13403,
          "end_idx": 13866
        },
        "page_content": " or a total 1,149,368 samples (63.9hrs) which we divide into training, validation and test sets. We aim to test whether our model generalises to data from new speakers, and to data from new sessions recorded with known speakers. To simulate this, we select a group of speakers from each dataset, and hold out all of their data either for validation or for testing. Additionally, we hold out one entire session from each of the remaining speakers, and use the rest",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How better is performance of proposed model compared to baselines?",
    "llm_answer": "The proposed model outperforms baselines by a clear margin in new environments, with improvements of INLINEFORM0 EM and GM compared to the second-best model.  In previously seen environments, the model increases accuracy by 35% and 25% for Exact Match and Goal Match, respectively, compared to the baseline.  On task A, with sufficient training data, the single system performs better than a strong feature-rich system.  The model also substantially improves over base models on DROP's development set across all metrics, surpassing the NABERT+ baseline.\n",
    "context": [
      {
        "id": "a1974dac-f964-4aeb-a415-a2006317d90a",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 68,
          "document_id": "1810.00663",
          "start_idx": 26280,
          "end_idx": 26763
        },
        "page_content": " room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM. Note that the average number of actions in the ground truth output sequences is 7.07 for the Test-New set. Our model's predictions are just INLINEFORM1 edits off on average from the correct navigation plans. This",
        "type": "Document"
      },
      {
        "id": "11d9b2e6-e20d-4ab9-8d7c-c35a7e2d27e6",
        "metadata": {
          "vector_store_key": "1603.07044-6",
          "chunk_id": 43,
          "document_id": "1603.07044",
          "start_idx": 14997,
          "end_idx": 15515
        },
        "page_content": " task A already had a substantial amount of training data. Table 4 gives the final comparison between different models (we only list the MAP score because it is the official score for the challenge). Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited",
        "type": "Document"
      },
      {
        "id": "dc50a5cf-b755-41a0-a173-1b561db0ed05",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "555a266e-0a08-43e5-b719-8cd2fd10ebb8",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "08676cf7-5cf4-4a2c-b9d7-ddbe513c2062",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 66,
          "document_id": "1810.00663",
          "start_idx": 25415,
          "end_idx": 25983
        },
        "page_content": " In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models. The previous section evaluated model performance on new instructions (and corresponding navigation plans) for environments that were previously seen at training time. Here, we examine whether the trained models succeed on environments that are completely new. The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What DCGs are used?",
    "llm_answer": "DGC-max, DGC-ave, and DGC-gated.\n",
    "context": [
      {
        "id": "d4373584-52b4-4882-b16b-14c4e6292bda",
        "metadata": {
          "vector_store_key": "1902.09087-1",
          "chunk_id": 30,
          "document_id": "1902.09087",
          "start_idx": 11366,
          "end_idx": 11812
        },
        "page_content": "12 , which use poolings on neighboring vertexes that ignore the semantic structure of n-grams. But to some situations, their formulations can be very similar to ours (See Appendix for derivation). For example, if we set the kernel size in LCNs to 3, use linear activations and suppose the pooling mode is average in both LCNs and DGCs, at each word in each layer, the DGCs compute the average of the first order neighbors together with the center",
        "type": "Document"
      },
      {
        "id": "cfcc879e-da3b-45a5-b521-7cdf7264a9c2",
        "metadata": {
          "vector_store_key": "2001.06354-7",
          "chunk_id": 45,
          "document_id": "2001.06354",
          "start_idx": 14994,
          "end_idx": 15452
        },
        "page_content": ". For evaluation, the Visual Dialog task employs four metrics. NDCG is the primary metric of the Visual Dialog Challenge which considers multiple similar answers as correct ones. The other three are MRR, recall@k, and mean rank where they only consider the rank of a single answer. Our experiments show the scores of NDCG and non-NDCG metrics from our image-only and joint models have a trade-off relationship due to their different ability (as shown in Sec.",
        "type": "Document"
      },
      {
        "id": "dcb8b3a7-697b-4907-b64c-eba4385bd51b",
        "metadata": {
          "vector_store_key": "1905.00563-3",
          "chunk_id": 36,
          "document_id": "1905.00563",
          "start_idx": 10486,
          "end_idx": 10702
        },
        "page_content": "{argmin} (G)$4 Ho $\\operatornamewithlimits{argmin} (G)$5 dd $\\operatornamewithlimits{argmin} (G)$6 d $\\operatornamewithlimits{argmin} (G)$7 s,r,s',r'd $\\operatornamewithlimits{argmin} (G)$8 s, r, o $\\operatornamewith",
        "type": "Document"
      },
      {
        "id": "774c58c7-64cb-49f1-8295-b5098b668ae6",
        "metadata": {
          "vector_store_key": "1902.09087-1",
          "chunk_id": 46,
          "document_id": "1902.09087",
          "start_idx": 16940,
          "end_idx": 17407
        },
        "page_content": "REF18 are strong baselines that perform CNNs over directed graphs to produce high level representation for each vertex in the graph, which can be used to build a sentence representation via certain pooling operation. We therefore choose to compare with DGC-max (with maximum pooling), DGC-ave (with average pooling), and DGC-gated (with gated pooling), where the gate value is computed using the concatenation of the vertex vector and the center vertex vector through",
        "type": "Document"
      },
      {
        "id": "c6ca403d-51c0-4b78-8dcb-887f8ffaa7e8",
        "metadata": {
          "vector_store_key": "1902.09087-1",
          "chunk_id": 80,
          "document_id": "1902.09087",
          "start_idx": 29304,
          "end_idx": 29717
        },
        "page_content": "RNNs BIBREF27 , BIBREF28 have extended CNNs and RNNs to model graph information, and DGCs generalize GCNs on directed graphs in the fields of semantic-role labeling BIBREF12 , document dating BIBREF18 , and SQL query embedding BIBREF29 . However, DGCs control information flowing from neighbor vertexes via edge types, while we focus on capturing different contexts for each word in word lattice via convolutional",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance difference of using a generated summary vs. a user-written one?",
    "llm_answer": "The models work consistently better when a user-written gold summary is used compared to a system-generated summary.\n",
    "context": [
      {
        "id": "5fdf13ac-3b0e-4fc2-aec6-9efefb47a24d",
        "metadata": {
          "vector_store_key": "1911.02711-9",
          "chunk_id": 49,
          "document_id": "1911.02711",
          "start_idx": 17553,
          "end_idx": 18109
        },
        "page_content": " use summary information shows that the review summary is useful for sentiment classification. In addition, the same models work consistently better when the user written gold summary is used compared to a system generated summary, which is intuitively reasonable since the current state-of-the-art abstractive summarization models are far from perfect. Interestingly, as shown in the second section of the table, the gold summary itself does not lead to better sentiment accuracy compared with the review itself, which shows that summaries better serve as",
        "type": "Document"
      },
      {
        "id": "a16913cf-a66b-46d8-872a-b22c7c3dd8ba",
        "metadata": {
          "vector_store_key": "1909.00578-1",
          "chunk_id": 33,
          "document_id": "1909.00578",
          "start_idx": 11017,
          "end_idx": 11604
        },
        "page_content": " stochastic learning algorithms are involved. An in-depth exploration of this point would involve further investigation, which will be part of future work. We propose a novel Quality Estimation model for summarization which does not require human references to estimate the quality of automatically produced summaries. Sum-QE successfully predicts qualitative aspects of summaries that recall-oriented evaluation metrics fail to approximate. Leveraging powerful BERT representations, it achieves high correlations with human scores for most linguistic qualities rated, on three different",
        "type": "Document"
      },
      {
        "id": "d5bf1383-667f-4600-bae5-6ea570d42092",
        "metadata": {
          "vector_store_key": "1909.00578-1",
          "chunk_id": 37,
          "document_id": "1909.00578",
          "start_idx": 12681,
          "end_idx": 13094
        },
        "page_content": ", not only summaries. It could thus be applied to other text generation tasks, such as natural language generation and sentence compression. We would like to thank the anonymous reviewers for their helpful feedback on this work. The work has been partly supported by the Research Center of the Athens University of Economics and Business, and by the French National Research Agency under project ANR-16-CE33-0013.",
        "type": "Document"
      },
      {
        "id": "f65a2feb-ad2e-42ff-bfff-9c6307e7d531",
        "metadata": {
          "vector_store_key": "1909.00542-4",
          "chunk_id": 12,
          "document_id": "1909.00542",
          "start_idx": 4863,
          "end_idx": 5365
        },
        "page_content": " the target summaries are human-generated and they do not have a perfect mapping to the input sentences. In addition, some samples have multiple reference summaries. BIBREF11 showed that different data labelling approaches influence the quality of the final summary, and some labelling approaches may lead to better results than using regression. In this paper we experiment with the following labelling approaches: : Label as \u201csummary\u201d all sentences from the input text that have a ROUGE score above a",
        "type": "Document"
      },
      {
        "id": "2f6bacce-bebd-41c2-b12b-afb3df3001f9",
        "metadata": {
          "vector_store_key": "1910.00825-4",
          "chunk_id": 50,
          "document_id": "1910.00825",
          "start_idx": 18207,
          "end_idx": 18733
        },
        "page_content": " This motivates us to collect a dialog summarization dataset with high-quality human-written summaries in the future. Results in the ranking evaluation show more differences between different summaries. SPNet outperforms Pointer-Generator with a large margin. Its performance is relatively close to the ground truth summary. Table TABREF25 shows an example summary from all models along with ground truth summary. We observe that Pointer-Generator ignores some essential fragments, such as the restaurant booking information (",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What evaluation metrics did they look at?",
    "llm_answer": "Automatic evaluation metrics, relevance, conciseness, readability, task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, task ease, user expertise, F1-score, Recall rate, F1 score, Overall accuracy.\n",
    "context": [
      {
        "id": "bf420fd0-2a99-48c9-9d52-0d17e3074796",
        "metadata": {
          "vector_store_key": "1910.00825-4",
          "chunk_id": 47,
          "document_id": "1910.00825",
          "start_idx": 16888,
          "end_idx": 17413
        },
        "page_content": " our method's increased performance on automatic evaluation metrics entails better human perceived quality. We randomly select 100 test samples from MultiWOZ test set for evaluation. We recruit 150 crowd workers from Amazon Mechanical Turk. For each sample, we show the conversation, reference summary, as well as summaries generated by Pointer-Generator and SPNet to three different participants. The participants are asked to score each summary on three indicators: relevance, conciseness and readability on a 1 to 5 scale,",
        "type": "Document"
      },
      {
        "id": "4786191e-fed1-4406-89b6-7b8835719e5f",
        "metadata": {
          "vector_store_key": "1709.10217-2",
          "chunk_id": 18,
          "document_id": "1709.10217",
          "start_idx": 6394,
          "end_idx": 6850
        },
        "page_content": " evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the number of total tasks. User satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied, neutral, satisfied and very satisfied, respectively. Response fluency: There are three scores -1, 0, 1, which indicate nonfluency, neutral, fluency. Number of dialogue turns: The number of utterances in a task",
        "type": "Document"
      },
      {
        "id": "4c2eddaa-f6bf-49ef-ba86-0a2738423408",
        "metadata": {
          "vector_store_key": "2003.05995-7",
          "chunk_id": 55,
          "document_id": "2003.05995",
          "start_idx": 22449,
          "end_idx": 22776
        },
        "page_content": " that I needed?\u201d on a scale of 1 (no, not at all) to 7 (yes, completely). Task ease: \u201cHow easy was the task?\u201d on a scale of 1 (very easy) to 7 (very difficult). User expertise: \u201cIn this conversation, did you know what you could say or do at each point of the dialog?\u201d on a scale of 1 (no, not at all) to 7 (yes, completely). At",
        "type": "Document"
      },
      {
        "id": "dc58289f-90f2-4f31-9a31-296d00bb01a9",
        "metadata": {
          "vector_store_key": "1709.10217-2",
          "chunk_id": 11,
          "document_id": "1709.10217",
          "start_idx": 4141,
          "end_idx": 4657
        },
        "page_content": ", in which only the released data can be used for training and developing. The other is an open evaluation that allow to collect external data for training and developing. For task 1, we use F1-score as evaluation metric. For the task-oriented dialogue systems, the best way for evaluation is to use the online human-computer dialogue. After finishing an online human-computer dialogue with a dialogue system, the human then manually evaluate the system by using the metrics of user satisfaction degree, dialogue flu",
        "type": "Document"
      },
      {
        "id": "638d3f9b-929b-4fa6-b0d4-efe44cd66255",
        "metadata": {
          "vector_store_key": "1912.13109-2",
          "chunk_id": 25,
          "document_id": "1912.13109",
          "start_idx": 10203,
          "end_idx": 10681
        },
        "page_content": " metrics - Recall rate, F1 score and Overall accuracy. Choice of model parameters were in the above models were inspired from previous work done but then were tuned to the best performance of the Test dataset. Following parameters were considered for tuning. Learning rate: Based on grid search the best performance was achieved when learning rate was set to 0.01. This value was arrived by a grid search on lr parameter. Number of Bidirectional LSTM units: A set of 32, 64, 128",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the datasets used for the task?",
    "llm_answer": "Stanford Sentiment Treebank (SST-2), Yelp dataset, Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), Quora Question Pairs, MCTest, CNN/Daily Mail, Children's Book Test, and a dataset of simulated indoor environments.\n",
    "context": [
      {
        "id": "19ad6a1f-7f24-400e-9235-38467b0ba391",
        "metadata": {
          "vector_store_key": "1910.00458-7",
          "chunk_id": 51,
          "document_id": "1910.00458",
          "start_idx": 17496,
          "end_idx": 17916
        },
        "page_content": "500). For the sentiment analysis category, we used the Stanford Sentiment Treebank (SST-2) dataset from the GLUE benchmark BIBREF22 (around 60k train examples) and the Yelp dataset (around 430k train examples). For the paraphrase category, three paraphrasing datasets are used from the GLUE benchmark: Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), and Quora Question Pairs (",
        "type": "Document"
      },
      {
        "id": "1c380a0c-1037-4628-8ada-168d9c8247bc",
        "metadata": {
          "vector_store_key": "1707.03904-9",
          "chunk_id": 13,
          "document_id": "1707.03904",
          "start_idx": 5304,
          "end_idx": 5850
        },
        "page_content": " subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short. We plan to release these annotations along with the datasets, and our retrieved documents for each question. Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated",
        "type": "Document"
      },
      {
        "id": "048a3d0c-944a-4696-9c1d-8564b637bab2",
        "metadata": {
          "vector_store_key": "1910.12618-0",
          "chunk_id": 17,
          "document_id": "1910.12618",
          "start_idx": 7204,
          "end_idx": 7727
        },
        "page_content": " section 5 concludes this paper and gives insight on future possible work. In order to prove the consistency of our work, experiments have been conducted on two data sets, one for France and the other for the UK. In this section details about the text and time series data are given, as well as the major preprocessing steps. Three types of time series are considered in our work: national net electricity consumption (also referred as load or demand), national temperature and wind speed. The load data sets were retrieved",
        "type": "Document"
      },
      {
        "id": "f8061e5e-74f5-49d0-8dbe-90e4db84d4eb",
        "metadata": {
          "vector_store_key": "1810.00663-7",
          "chunk_id": 46,
          "document_id": "1810.00663",
          "start_idx": 17810,
          "end_idx": 18361
        },
        "page_content": " . This dataset was created using Amazon Mechanical Turk and 100 maps of simulated indoor environments, each with 6 to 65 rooms. To the best of our knowledge, this is the first benchmark for comparing translation models in the context of behavioral robot navigation. As shown in Table TABREF16 , the dataset consists of 8066 pairs of free-form natural language instructions and navigation plans for training. This training data was collected from 88 unique simulated environments, totaling 6064 distinct navigation plans (2002 plans have two different",
        "type": "Document"
      },
      {
        "id": "4cac7588-28e9-4b6e-ac0d-d763db64bc58",
        "metadata": {
          "vector_store_key": "1703.04617-6",
          "chunk_id": 3,
          "document_id": "1703.04617",
          "start_idx": 1343,
          "end_idx": 1868
        },
        "page_content": " can help attain better results on our competitive baselines. Recent advance on reading comprehension and question answering has been closely associated with the availability of various datasets. BIBREF0 released the MCTest data consisting of 500 short, fictional open-domain stories and 2000 questions. The CNN/Daily Mail dataset BIBREF1 contains news articles for close style machine comprehension, in which only entities are removed and tested for comprehension. Children's Book Test (CBT) BIBREF2 leverages named entities",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the accuracy of the model for the six languages tested?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "07344119-d726-4d67-a8fe-3e22de9ad73d",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 46,
          "document_id": "1910.04269",
          "start_idx": 17760,
          "end_idx": 18217
        },
        "page_content": " of 1,500 samples for each of six languages. Out of 1,500 samples for each language, 1,200 were randomly selected as training dataset for that language and rest 300 as validation dataset using k-fold cross-validation. To sum up, we trained our model on 7,200 samples and validated it on 1800 samples comprising six languages. The results are discussed in next section. This paper discusses two end-to-end approaches which achieve state-of-the-art results in",
        "type": "Document"
      },
      {
        "id": "db8cc05f-baec-423c-86bb-785b69fe9e54",
        "metadata": {
          "vector_store_key": "2001.11268-9",
          "chunk_id": 59,
          "document_id": "2001.11268",
          "start_idx": 23352,
          "end_idx": 23963
        },
        "page_content": " this project's GitHub repository , including true class labels and a detailed breakdown of true and false predictions for each class. The highest proportion of false classification appears between the results and conclusion classes. The fine-tuned multilingual model showed marginally inferior classification scores on the exclusively English testing data. However, this model's contribution is not limited to the English language because its interior weights embed a shared vocabulary of 100 languages, including German and Chinese. Our evaluation of the multilingual model's capacity for language transfer is",
        "type": "Document"
      },
      {
        "id": "4199c59c-1927-47cb-857b-9c1bed5dae20",
        "metadata": {
          "vector_store_key": "1911.03894-7",
          "chunk_id": 44,
          "document_id": "1911.03894",
          "start_idx": 15623,
          "end_idx": 16121
        },
        "page_content": " is also machine translated for all languages. The dataset is composed of 122k train, 2490 valid and 5010 test examples. As usual, NLI performance is evaluated using accuracy. To evaluate a model on a language other than English (such as French), we consider the two following settings: TRANSLATE-TEST: The French test set is machine translated into English, and then used with an English classification model. This setting provides a reasonable, although imperfect, way to circumvent the fact that",
        "type": "Document"
      },
      {
        "id": "173a0784-e08f-4515-935f-df7356f82e92",
        "metadata": {
          "vector_store_key": "1906.06448-9",
          "chunk_id": 59,
          "document_id": "1906.06448",
          "start_idx": 21919,
          "end_idx": 22444
        },
        "page_content": " also indicates the difficulty of problems from linguistics publications. Regarding non-monotone problems collected via crowdsourcing, there are very few non-monotone problems, so accuracy is 100%. Adding non-monotone problems to our test set is left for future work. Table 9 shows the evaluation results by type of linguistic phenomenon. While accuracy on problems involving NPIs and conditionals was improved on both upward and downward inferences, accuracy on problems involving conjunction and disjunction was improved on",
        "type": "Document"
      },
      {
        "id": "635e5e6c-9cc3-4b1a-9a80-ed3f4fc00b6f",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 17,
          "document_id": "1910.04269",
          "start_idx": 6552,
          "end_idx": 7071
        },
        "page_content": " for feature extraction. An elaborate analysis of how deep architectures outperform their shallow counterparts is presented in this reseacrch. The difficulties in classifying perceptually similar languages like German and English were also put forward in this work. It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which models achieve state-of-the-art performances?",
    "llm_answer": "BERT-based ensemble models, and specifically RoBERTa<sub>large</sub>, achieve state-of-the-art performance.\n",
    "context": [
      {
        "id": "023a760d-81e9-48e4-8692-88260707e423",
        "metadata": {
          "vector_store_key": "2002.11402-1",
          "chunk_id": 24,
          "document_id": "2002.11402",
          "start_idx": 8819,
          "end_idx": 9288
        },
        "page_content": "es other models in all scores. Spacy results are good despite not being trained for case-less data. In terms of F1 and overall stability Spacy did better than Stanford NER, on our News Validation set. Similarly, Stanford did well in Precision but could not catch up with Spacy and our model in terms of Recall. Flair overall performed poorly, but as said before these open-source models are not trained for our particular use-case. Lets check some examples for detailed",
        "type": "Document"
      },
      {
        "id": "fb01b160-a168-4070-9f5a-fd61cc41f56f",
        "metadata": {
          "vector_store_key": "2002.06644-2",
          "chunk_id": 17,
          "document_id": "2002.06644",
          "start_idx": 6467,
          "end_idx": 6878
        },
        "page_content": "ERT based models, and BERT-based ensemble models outperform the baselines for all the metrics. Among the optimized BERT based models, $RoBERTa_{large}$ outperforms all other non-ensemble models and the baselines for all metrics. It further achieves a maximum recall of $0.681$ for all the proposed models. We note that DistillRoBERTa, a distilled model, performs competitively, achieving $69.69\\%$ accuracy, and",
        "type": "Document"
      },
      {
        "id": "3766798b-578c-4bd4-9bb8-d1fa398b63e3",
        "metadata": {
          "vector_store_key": "1606.08140-6",
          "chunk_id": 42,
          "document_id": "1606.08140",
          "start_idx": 12268,
          "end_idx": 12747
        },
        "page_content": " that the models employing path information generally achieve better results than models that do not use such information. In terms of models not exploiting path information or external information, the STransE model produces the highest filtered mean rank on WN18 and the highest filtered Hits@10 and mean reciprocal rank on FB15k. Compared to the closely related models SE, TransE, TransR, CTransR, TransD and TranSparse, our STransE model does better than these models on both",
        "type": "Document"
      },
      {
        "id": "3c3e7bd6-8955-4353-bdf3-c8d8c69b5e4d",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 59,
          "document_id": "1909.13375",
          "start_idx": 20052,
          "end_idx": 20444
        },
        "page_content": "s. For the large models we used a batch size of 3 with a learning rate of $5e^{-6}$ and trained for 5 epochs, except for the model without the single-span heads that was trained with a batch size of 2 for 7 epochs. F1 was used as our validation metric. All models were trained on a single GPU with 12-16GB of memory. Table TABREF24 shows the results on DROP's development set. Compared to our",
        "type": "Document"
      },
      {
        "id": "577474d8-b0fd-4808-8b7d-5f80c7a29369",
        "metadata": {
          "vector_store_key": "2001.06354-5",
          "chunk_id": 62,
          "document_id": "2001.06354",
          "start_idx": 21248,
          "end_idx": 21648
        },
        "page_content": " Specifically, our image-only model shows much higher NDCG score (60.16). On the other hand, our consensus dropout fusion model shows more balanced results over all metrics while still outperforming on most evaluation metrics (NDCG, MRR, R@1, and R@5). Compared to results of the Visual Dialog challenge 2019, our models also show strong results. Although ReDAN+ BIBREF26 and MReaL\u2013BDAI show higher N",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which competitive relational classification models do they test?",
    "llm_answer": "Spacy, Stanford NER, and Flair.\n",
    "context": [
      {
        "id": "25e545cc-762a-46b5-8f2c-49d07cfd655e",
        "metadata": {
          "vector_store_key": "1906.03338-2",
          "chunk_id": 8,
          "document_id": "1906.03338",
          "start_idx": 2993,
          "end_idx": 3586
        },
        "page_content": " arguments, our work investigates how (de-)contextualization of argumentative units affects automatic argumentative relation classification models. In this section, we describe different formulations of the argumentative relation classification task and describe features used by our replicated model. In order to test our hypotheses, we propose to group all features into three distinct types. Now, we introduce a classification of three different prediction models used in the argumentative relation prediction literature. We will inspect all of them and show that all can suffer from severe",
        "type": "Document"
      },
      {
        "id": "0e5b6c51-9d68-4a95-bda1-2831d8105483",
        "metadata": {
          "vector_store_key": "1908.05434-0",
          "chunk_id": 35,
          "document_id": "1908.05434",
          "start_idx": 13265,
          "end_idx": 13751
        },
        "page_content": " comparison of our proposed model with commonly used ordinal regression models as well as the previous state-of-the-art classification model by E. Tong et al. ( BIBREF9 ). To assess the effect of each component in our model, we perform an ablation test where the components are swapped by their more standard alternatives one at a time. Next, we perform a qualitative analysis on the model predictions on the raw data, which are scraped from a different escort website than the one that",
        "type": "Document"
      },
      {
        "id": "023a760d-81e9-48e4-8692-88260707e423",
        "metadata": {
          "vector_store_key": "2002.11402-1",
          "chunk_id": 24,
          "document_id": "2002.11402",
          "start_idx": 8819,
          "end_idx": 9288
        },
        "page_content": "es other models in all scores. Spacy results are good despite not being trained for case-less data. In terms of F1 and overall stability Spacy did better than Stanford NER, on our News Validation set. Similarly, Stanford did well in Precision but could not catch up with Spacy and our model in terms of Recall. Flair overall performed poorly, but as said before these open-source models are not trained for our particular use-case. Lets check some examples for detailed",
        "type": "Document"
      },
      {
        "id": "8bc8f6a6-27d5-4419-b96d-f225f61770c5",
        "metadata": {
          "vector_store_key": "1906.03338-1",
          "chunk_id": 48,
          "document_id": "1906.03338",
          "start_idx": 18109,
          "end_idx": 18674
        },
        "page_content": " ). This holds true for all three task formulations and provides further evidence that models \u2013 when given the option \u2013 put a strong focus on contextual clues while neglecting the information provided by the EAU span itself. While competitive systems for argumentative relation classification are considered to be robust, our experiments have shown that despite confidence-inspiring scores on unseen testing data, such systems can easily be fooled \u2013 they can deliver strong performance scores although the classifier does not have access to the content of the EAUs.",
        "type": "Document"
      },
      {
        "id": "14bde222-3b50-4347-8cbc-18a9a301554b",
        "metadata": {
          "vector_store_key": "1905.08949-9",
          "chunk_id": 26,
          "document_id": "1905.08949",
          "start_idx": 9823,
          "end_idx": 10318
        },
        "page_content": " cognitive level and answer type, along with their statistics. Among them, SQuAD was used by most groups as the benchmark to evaluate their NQG models. This provides a fair comparison between different techniques. However, it raises the issue that most NQG models work on factoid questions with answer as text span, leaving other types of QG problems less investigated, such as generating deep multi-choice questions. To overcome this, a wider variety of corpora should be benchmarked against in",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they gather human judgements for similarity between relations?",
    "llm_answer": "The provided text states experiments and analysis were aimed at answering how well the computed similarity score correlates with human judgment about the similarity between relations.  It does not describe *how* human judgments were gathered.\n",
    "context": [
      {
        "id": "904e16ff-21c9-4dd2-baa4-d93bb5a2ed94",
        "metadata": {
          "vector_store_key": "1907.08937-6",
          "chunk_id": 67,
          "document_id": "1907.08937",
          "start_idx": 26256,
          "end_idx": 26730
        },
        "page_content": " of applications. We note that there are a wide range of future directions: (1) human prior knowledge could be incorporated into the similarity quantification; (2) similarity between relations could also be considered in multi-modal settings, e.g., extracting relations from images, videos, or even from audios; (3) by analyzing the distributions corresponding to different relations, one can also find some \u201cmeta-relations\u201d between relations, such as hypernymy and hyponymy",
        "type": "Document"
      },
      {
        "id": "e6e037a3-04b5-49a3-aa46-6b6e0844a9ed",
        "metadata": {
          "vector_store_key": "1907.08937-0",
          "chunk_id": 7,
          "document_id": "1907.08937",
          "start_idx": 2997,
          "end_idx": 3549
        },
        "page_content": " a framework for assessing the similarity between relations, our second contribution is that we have done a survey of applications. We present experiments and analysis aimed at answering five questions: (1) How well does the computed similarity score correlate with human judgment about the similarity between relations? How does our approach compare to other possible approaches based on other kinds of relation embeddings to define a similarity? (sec:relationship and sec:human-judgment) (2) Open IE models inevitably extract many redundant relations",
        "type": "Document"
      },
      {
        "id": "a678752b-4cb2-4cc0-b75d-a6dc5d6f6bcd",
        "metadata": {
          "vector_store_key": "1907.08937-6",
          "chunk_id": 66,
          "document_id": "1907.08937",
          "start_idx": 25697,
          "end_idx": 26256
        },
        "page_content": " sparse data, which makes it difficult to achieve an effective and stable similarity metric. Motivated by this, we propose to measure relation similarity by leveraging their fact distribution so that we can identify nuances between similar relations, and merge those distant surface forms of the same relations, benefitting the tasks mentioned above. In this paper, we introduce an effective method to quantify the relation similarity and provide analysis and a survey of applications. We note that there are a wide range of future directions: (1) human prior",
        "type": "Document"
      },
      {
        "id": "aabace62-6c3e-4445-87a5-1264129735ba",
        "metadata": {
          "vector_store_key": "1907.08937-0",
          "chunk_id": 6,
          "document_id": "1907.08937",
          "start_idx": 2469,
          "end_idx": 2997
        },
        "page_content": " distributions over entity pairs. Specifically, we adopt Kullback\u2013Leibler (KL) divergence of both directions as the metric. However, computing exact KL requires iterating over the whole entity pair space INLINEFORM1 , which is quite intractable. Therefore, we further provide a sampling-based method to approximate the similarity score over the entity pair space for computational efficiency. Besides developing a framework for assessing the similarity between relations, our second contribution is that we have done a survey of",
        "type": "Document"
      },
      {
        "id": "860bee6a-0102-481a-b95f-00399582570a",
        "metadata": {
          "vector_store_key": "1907.08937-0",
          "chunk_id": 32,
          "document_id": "1907.08937",
          "start_idx": 12364,
          "end_idx": 12894
        },
        "page_content": " measure some degree of similarity. As shown in fig:correlation, the three baseline models could achieve moderate ( INLINEFORM0 ) positive correlation. On the other hand, our model shows a stronger correlation ( INLINEFORM1 ) with human judgment, indicating that considering the probability over whole entity pair space helps to gain a similarity closer to human judgments. These results provide evidence for our claim raised in sec:defining-similarity. Open IE extracts concise token patterns from plain text to represent various",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What text classification task is considered?",
    "llm_answer": "Question classification; predicting the degree of harm (DoH) among five possible values (1-5).\n",
    "context": [
      {
        "id": "be52086a-b93d-4190-a7cb-4e9dc1249dc6",
        "metadata": {
          "vector_store_key": "1605.08675-6",
          "chunk_id": 47,
          "document_id": "1605.08675",
          "start_idx": 18319,
          "end_idx": 18812
        },
        "page_content": " case of entity questions; a WordNet synset, to which a question focus belongs; necessary for DeepER. Search query \u2013 used to find possibly relevant documents, Question content \u2013 the words from question which are supposed to appear also in context of an answer. The task presented above, called question classification, is an example of text classification with very short texts. It could be tackled by a general-purpose classifier; for example, BIBREF11 used SVMs (Support Vector Machines) for",
        "type": "Document"
      },
      {
        "id": "899588b0-b0d8-42c9-a091-cdc04e773d20",
        "metadata": {
          "vector_store_key": "2003.06279-4",
          "chunk_id": 72,
          "document_id": "2003.06279",
          "start_idx": 27434,
          "end_idx": 28020
        },
        "page_content": "ding technique yielding the best result. All optimized results were obtained with the use of stopwords, global thresholding strategy and SVM as classification algorithm. A significant gain is more evident for intermediary text lengths. Textual classification remains one of the most important facets of the Natural Language Processing area. Here we studied a family of classification methods, the word co-occurrence networks. Despite this apparent simplicity, this model has been useful in several practical and theoretical scenarios. We proposed a modification of the traditional model",
        "type": "Document"
      },
      {
        "id": "a85acec6-de2b-4327-a116-5b49772f4489",
        "metadata": {
          "vector_store_key": "1909.00183-7",
          "chunk_id": 81,
          "document_id": "1909.00183",
          "start_idx": 30160,
          "end_idx": 30649
        },
        "page_content": " the performance of the supervised classification. The supervised classification was carried out by training on features and text three classifiers commonly applied to text classification tasks BIBREF22, BIBREF23: a Ridge classifier, Support Vector Machines with a linear kernel, and Random Forests. The goal is to predict the degree of harm (DoH) among five possible values (1-5). The classification is carried out with five-fold cross validation, using 80% of the data to train the model",
        "type": "Document"
      },
      {
        "id": "38e365e5-4b9a-429d-b356-dc4bcd6c1025",
        "metadata": {
          "vector_store_key": "2003.06279-4",
          "chunk_id": 3,
          "document_id": "2003.06279",
          "start_idx": 1262,
          "end_idx": 1718
        },
        "page_content": " text classification tasks BIBREF8, BIBREF9, BIBREF10. The main advantage of the model is that it does not rely on deep semantical information to obtain competitive results. Another advantage of graph-based approaches is that, when combined with other approaches, it yields competitive results BIBREF11. A simple, yet recurrent text model is the well-known word co-occurrence network. After optional textual pre-processing steps, in a co-occurrence network",
        "type": "Document"
      },
      {
        "id": "3ca43d71-34bc-4711-ba18-08848f0c6b4f",
        "metadata": {
          "vector_store_key": "2001.00137-0",
          "chunk_id": 4,
          "document_id": "2001.00137",
          "start_idx": 1748,
          "end_idx": 2222
        },
        "page_content": " Language Processing (NLP) domain. Tasks such as text generation, sentence correction, image captioning and text classification, have been possible via models such as Convolutional Neural Networks and Recurrent Neural Networks BIBREF5, BIBREF6, BIBREF7. More recently, state-of-the-art results have been achieved with attention models, more specifically Transformers BIBREF8. Surprisingly, however, there is currently no research on incomplete text classification in the NLP",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What novel class of recurrent-like networks is proposed?",
    "llm_answer": "A neural network architecture that leverages both recurrence and one-dimensional convolutions.\n",
    "context": [
      {
        "id": "11aab647-b798-4f89-aa3a-bdc7341ff198",
        "metadata": {
          "vector_store_key": "1911.00069-9",
          "chunk_id": 38,
          "document_id": "1911.00069",
          "start_idx": 12563,
          "end_idx": 12982
        },
        "page_content": ". Recurrent neural networks (RNNs) are a class of neural networks that operate on sequential data such as sequences of words. LSTM networks are a type of RNNs that have been invented to better capture long-range dependencies in sequential data. We pass the word embeddings $\\mathbf {x}_t$'s to a forward and a backward LSTM layer. A forward or backward LSTM layer consists of a set of recurrently connected blocks known",
        "type": "Document"
      },
      {
        "id": "f8424175-9a55-4b64-b909-1db04f3a5348",
        "metadata": {
          "vector_store_key": "1906.00180-0",
          "chunk_id": 3,
          "document_id": "1906.00180",
          "start_idx": 1317,
          "end_idx": 1867
        },
        "page_content": " rules that characterize it, and focus in particular on recurrent neural networks that are unambiguously `connectionist': trained recurrent nets do not rely on symbolic data and control structures such as trees and global variable binding, and can straightforwardly be implemented in biological networks BIBREF8 or neuromorphic hardware BIBREF9 . We report positive results on this challenge, and in the process develop a series of tests for compositional generalization that address the concerns of deep learning skeptics. The paper makes three main",
        "type": "Document"
      },
      {
        "id": "1cbb23f1-c4b5-4481-af4b-7d8b18c1b261",
        "metadata": {
          "vector_store_key": "1711.02013-4",
          "chunk_id": 11,
          "document_id": "1711.02013",
          "start_idx": 4391,
          "end_idx": 4818
        },
        "page_content": ", which can be dated back to 1990s (e.g. BIBREF23 , BIBREF24 , BIBREF25 ). The NARX RNN BIBREF25 is another example which used a feed forward net taking different inputs with predefined time delays to model long-term dependencies. More recently, BIBREF26 also used multiple layers of recurrent networks with different pre-defined updating frequencies. Instead, our model tries to learn the structure from data, rather than pred",
        "type": "Document"
      },
      {
        "id": "8d31cd1b-600a-4487-898c-4325e0156579",
        "metadata": {
          "vector_store_key": "1909.13362-8",
          "chunk_id": 18,
          "document_id": "1909.13362",
          "start_idx": 6723,
          "end_idx": 7196
        },
        "page_content": ", we present a neural network architecture that leverages both recurrence and one-dimensional convolutions. Recurrence enables our model to read a sequence much like a human would; a sequence with elements $abcd$ would be read one element at a time, updating a latent understanding after reading each $a$, $b$, $c$, and finally $d$. One-dimensional convolutions extract a spatial relationship between sequential elements. The $abcd$ example sequence may then be read as $ab",
        "type": "Document"
      },
      {
        "id": "ace214fb-a50f-49d8-9c81-7e17b1a34e86",
        "metadata": {
          "vector_store_key": "1908.05434-8",
          "chunk_id": 22,
          "document_id": "1908.05434",
          "start_idx": 8351,
          "end_idx": 8841
        },
        "page_content": " paragraphs after mapping the words to embeddings, we need a model to handle sequential data. Recurrent neural networks (RNNs) have recently seen great success at modeling sequential data, especially in natural language processing tasks BIBREF19 . On a high level, an RNN is a neural network that processes a sequence of inputs one at a time, taking the summary of the sequence seen so far from the previous time point as an additional input and producing a summary for the next time point.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much bigger is Switchboard-2000 than Switchboard-300 database?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "cc119115-77ce-4722-9013-f7e4e47fd7c6",
        "metadata": {
          "vector_store_key": "2001.07263-5",
          "chunk_id": 46,
          "document_id": "2001.07263",
          "start_idx": 17384,
          "end_idx": 17775
        },
        "page_content": " others, the 2000-hour system reaches 8.7% and 7.4% WER on rt02 and rt04. We observe that the regularization techniques, which are extremely important on the 300h setup, are still beneficial but have a significantly smaller effect.  For comparison with results in the literature we refer to the Switchboard-300 results in BIBREF3, BIBREF7, BIBREF51, BIBREF52 and the Switchboard-2000 results",
        "type": "Document"
      },
      {
        "id": "22a7eec8-39b3-4685-94d8-7c189df2b1fd",
        "metadata": {
          "vector_store_key": "1910.00458-1",
          "chunk_id": 41,
          "document_id": "1910.00458",
          "start_idx": 13703,
          "end_idx": 14141
        },
        "page_content": " almost the same way BIBREF9 with only one difference that MC160 is around three times smaller than MC500. We can see from Table TABREF17 that both the BERT and RoBERTa baselines perform much worse on MC160 than MC500. We think the reason is that the data size of MC160 is not enough to well fine-tune the large models with a huge amount of trainable parameters. However, by leveraging the transfer learning techniques we proposed, we can",
        "type": "Document"
      },
      {
        "id": "9903e8f1-289d-454b-9871-9c5eebbc73b2",
        "metadata": {
          "vector_store_key": "2001.07263-5",
          "chunk_id": 47,
          "document_id": "2001.07263",
          "start_idx": 17775,
          "end_idx": 18103
        },
        "page_content": "IBREF7, BIBREF51, BIBREF52 and the Switchboard-2000 results in BIBREF50, BIBREF51, BIBREF53, BIBREF54, BIBREF55, BIBREF56. Our 300-hour model not only outperforms the previous best attention based encoder-decoder model BIBREF3 by a large margin, it also surpasses the best hybrid systems with multiple LMs BIBREF7. Our result on",
        "type": "Document"
      },
      {
        "id": "04fe1abe-a675-484e-963d-9d80c75d505e",
        "metadata": {
          "vector_store_key": "1908.10449-2",
          "chunk_id": 66,
          "document_id": "1908.10449",
          "start_idx": 22880,
          "end_idx": 23381
        },
        "page_content": " hard time learning in this setting. As shown in Table TABREF16, both datasets have a vocabulary size of more than 100k. This is much larger than in the other two settings, where on average the length of questions is around 10. This suggests that the methods with better sample efficiency are needed to act in more realistic problem settings with huge action spaces. Experiments also show that a larger memory slot size always helps. Intuitively, with a memory mechanism (either implicit or explicit),",
        "type": "Document"
      },
      {
        "id": "f8cf2cb1-3e66-48cd-95c5-2c39cdf76b33",
        "metadata": {
          "vector_store_key": "1902.00330-8",
          "chunk_id": 72,
          "document_id": "1902.00330",
          "start_idx": 22937,
          "end_idx": 23326
        },
        "page_content": " We set the dimensions of word embedding and entity embedding to 300, where the word embedding and entity embedding are released by BIBREF20 and BIBREF0 respectively. For parameters of the local LSTM network, the number of LSTM cell units is set to 512, the batch size is 64, and the rank margin $\\gamma $ is 0.1. Similarly, in global LSTM network, the number of LSTM cell units is 700 and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What domains are detected in this paper?",
    "llm_answer": "Lifestyle, scientific.\n",
    "context": [
      {
        "id": "f28d38d4-083f-4ff9-9f2d-d32accf38e45",
        "metadata": {
          "vector_store_key": "1907.11499-2",
          "chunk_id": 8,
          "document_id": "1907.11499",
          "start_idx": 3251,
          "end_idx": 3868
        },
        "page_content": " involvement. Instead of learning from individually labeled segments, our model only requires document-level supervision and optionally prior domain knowledge and learns to introspectively judge the domain of constituent segments. Importantly, we do not require document-level domain annotations either since we obtain these via distant supervision by leveraging information drawn from Wikipedia. Our domain detection framework comprises two neural network modules; an encoder learns representations for words and sentences together with prior domain information if the latter is available (e.g., domain definitions),",
        "type": "Document"
      },
      {
        "id": "407faed0-32f9-4c97-83d1-f7d24f137788",
        "metadata": {
          "vector_store_key": "1907.11499-2",
          "chunk_id": 15,
          "document_id": "1907.11499",
          "start_idx": 6351,
          "end_idx": 6867
        },
        "page_content": " at hand. For example, the definition of the \u201cLifestyle\u201d domain is \u201cthe interests, opinions, behaviors, and behavioral orientations of an individual, group, or culture\u201d. Figure FIGREF5 provides an overview of our Domain Detection Network, which we call DetNet. The model comprises two modules; an encoder learns representations for words and sentences whilst incorporating prior domain information; a detector generates domain scores for words, sentences, and documents by selectively attending to previously encoded",
        "type": "Document"
      },
      {
        "id": "bea618fb-90d6-4dc7-8e60-c8e7bcf0b45f",
        "metadata": {
          "vector_store_key": "1701.03214-2",
          "chunk_id": 15,
          "document_id": "1701.03214",
          "start_idx": 5368,
          "end_idx": 5787
        },
        "page_content": " (essentially open) in-domain data. The scientific domain MT was conducted on the Chinese-Japanese paper excerpt corpus (ASPEC-CJ) BIBREF11 , which is one subtask of the workshop on Asian translation (WAT) BIBREF15 . The ASPEC-CJ task uses 672315, 2090, and 2107 sentences for training, development, and testing, respectively. The Wikipedia domain task was conducted on a Chinese-Japanese corpus automatically extracted",
        "type": "Document"
      },
      {
        "id": "0352d77e-3697-4ce5-ba05-c51cdfa16df7",
        "metadata": {
          "vector_store_key": "1907.11499-2",
          "chunk_id": 9,
          "document_id": "1907.11499",
          "start_idx": 3868,
          "end_idx": 4419
        },
        "page_content": " and sentences together with prior domain information if the latter is available (e.g., domain definitions), while a detector generates domain-specific scores for words, sentences, and documents. We obtain a segment-level domain predictor which is trained end-to-end on document-level labels using a hierarchical, attention-based neural architecture BIBREF15 . We conduct domain detection experiments on English and Chinese and measure system performance using both automatic and human-based evaluation. Experimental results show that our model outper",
        "type": "Document"
      },
      {
        "id": "bcaf3d16-6416-45d6-936d-6737e52559e7",
        "metadata": {
          "vector_store_key": "1907.11499-0",
          "chunk_id": 12,
          "document_id": "1907.11499",
          "start_idx": 5095,
          "end_idx": 5596
        },
        "page_content": " We formulate domain detection as a multilabel learning problem. Our model is trained on samples of document-label pairs. Each document consists of INLINEFORM0 sentences INLINEFORM1 and is associated with discrete labels INLINEFORM2 . In this work, domain labels are not annotated manually but extrapolated from Wikipedia (see Section SECREF6 for details). In a non-MIL framework, a model typically learns to predict document labels by directly conditioning on its sentence representations INLINEFORM3",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Why do they think this task is hard?  What is the baseline performance?",
    "llm_answer": "The task is hard because baselines lag behind human performance by 16.4% and 32.1% for Quasar-S and Quasar-T respectively.\n",
    "context": [
      {
        "id": "24bf2271-5c8e-4680-b893-d10c22b99f5e",
        "metadata": {
          "vector_store_key": "1707.03904-4",
          "chunk_id": 55,
          "document_id": "1707.03904",
          "start_idx": 20754,
          "end_idx": 21226
        },
        "page_content": " baselines, however, lag behind human performance by $16.4\\%$ and $32.1\\%$ for Quasar-S and Quasar-T respectively, indicating the strong potential for improvement. Interestingly, for human performance we observe that non-experts are able to match or beat the performance of experts when given access to the background corpus for searching the answers. We also emphasize that the human performance is limited by either the knowledge of the experts, or the usefulness of the",
        "type": "Document"
      },
      {
        "id": "794fbce4-0a70-4baa-aed6-111dd06f921c",
        "metadata": {
          "vector_store_key": "1910.08210-1",
          "chunk_id": 48,
          "document_id": "1910.08210",
          "start_idx": 18169,
          "end_idx": 18684
        },
        "page_content": "-to-one group assignments and natural language templated descriptions. The performance across the curriculum is shown in Table TABREF32 (see Figure FIGREF58 in appendix SECREF12 for training curves of each stage). We see that curriculum learning is crucial to making progress on , and that initial policy training (first row of Table TABREF32) with additional complexities in any of the dimensions result in significantly worse performance. We take each of the 5 runs after training through the whole curriculum and",
        "type": "Document"
      },
      {
        "id": "154800a9-8cc0-479d-bdb3-8f3aa6520648",
        "metadata": {
          "vector_store_key": "1906.06448-6",
          "chunk_id": 28,
          "document_id": "1906.06448",
          "start_idx": 9932,
          "end_idx": 10429
        },
        "page_content": " each set of substitutions, and each set was assigned to three workers. To remove low-quality examples, we set the minimum time it should take to complete each set to 200 seconds. The entry in our task was restricted to workers from native speaking English countries. 128 workers contributed to the task, and we created 15,339 hypotheses (7,179 upward examples and 8,160 downward examples). The gold label of each premise-hypothesis pair created in the previous task is automatically determined by",
        "type": "Document"
      },
      {
        "id": "a9183e0c-e932-463e-99f3-3dde14406578",
        "metadata": {
          "vector_store_key": "1706.07179-1",
          "chunk_id": 15,
          "document_id": "1706.07179",
          "start_idx": 5878,
          "end_idx": 6329
        },
        "page_content": " the graph. We evaluate the model's performance on the bAbI tasks BIBREF18 , a collection of 20 question answering tasks which have become a benchmark for evaluating memory-augmented neural networks. We compare the performance with the Recurrent Entity Networks model (EntNet) BIBREF17 . Performance is measured in terms of mean percentage error on the tasks. Training Details: We used Adam and did a grid search for the learning rate in {0.01, 0.005,",
        "type": "Document"
      },
      {
        "id": "413c231b-c771-4c94-97a8-b20b290a953c",
        "metadata": {
          "vector_store_key": "1906.06448-6",
          "chunk_id": 32,
          "document_id": "1906.06448",
          "start_idx": 11534,
          "end_idx": 12014
        },
        "page_content": " the premise and/or the hypothesis is ungrammatical or does not make sense. Workers were paid US$0.04 for each question, and each question was assigned to three workers. To collect high-quality annotation results, we imposed ten test questions on each worker, and removed workers who gave more than three wrong answers. We also set the minimum time it should take to complete each question to 200 seconds. 1,237 workers contributed to this task, and we annotated gold labels of 15",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they generate the synthetic dataset?",
    "llm_answer": "Users open existing documents, make changes, and save them as separate documents.  Another method uses an artificial language defined by a grammar, with sentences generated according to the grammar and entailment relations established by a fixed background logic (using FOL instead of natural logic).\n",
    "context": [
      {
        "id": "af1a972a-bca3-4b96-a175-5e7434f86258",
        "metadata": {
          "vector_store_key": "1709.01256-1",
          "chunk_id": 52,
          "document_id": "1709.01256",
          "start_idx": 19441,
          "end_idx": 19942
        },
        "page_content": " We removed empty lines in the documents and trained word2vec embeddings on the entire corpus. We used the documents occurring in the first INLINEFORM0 of the revision period for INLINEFORM1 calibration, and the remaining documents for test. The generation process of the simulated data sets is designed to mimic the real world. Users open some existing documents in a file system, make some changes (e.g. addition, deletion or replacement), and save them as separate documents. These documents become",
        "type": "Document"
      },
      {
        "id": "ba8654c6-30c3-4a22-a555-6a227e8c58da",
        "metadata": {
          "vector_store_key": "1906.00180-0",
          "chunk_id": 5,
          "document_id": "1906.00180",
          "start_idx": 2202,
          "end_idx": 2706
        },
        "page_content": " data generation process is inspired by BIBREF13 : an artificial language is defined, sentences are generated according to its grammar and the entailment relation between pairs of such sentences is established according to a fixed background logic. However, our language is significantly more complex, and instead of natural logic we use FOL. Our main model is a recurrent network, sketched in Figure 4 . It is a so-called `Siamese' network because it uses the same parameters to process the left and the",
        "type": "Document"
      },
      {
        "id": "1c380a0c-1037-4628-8ada-168d9c8247bc",
        "metadata": {
          "vector_store_key": "1707.03904-9",
          "chunk_id": 13,
          "document_id": "1707.03904",
          "start_idx": 5304,
          "end_idx": 5850
        },
        "page_content": " subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short. We plan to release these annotations along with the datasets, and our retrieved documents for each question. Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated",
        "type": "Document"
      },
      {
        "id": "c0775cc4-03e9-40bb-b090-b64251bcdd86",
        "metadata": {
          "vector_store_key": "1906.00180-0",
          "chunk_id": 4,
          "document_id": "1906.00180",
          "start_idx": 1632,
          "end_idx": 2202
        },
        "page_content": " tests for compositional generalization that address the concerns of deep learning skeptics. The paper makes three main contributions. First, we develop a protocol for automatically generating data that can be used in entailment recognition tasks. Second, we demonstrate that several deep learning architectures succeed at one such task. Third, we present and apply a number of experiments to test whether models are capable of compositional generalization. The data generation process is inspired by BIBREF13 : an artificial language is defined, sentences are generated",
        "type": "Document"
      },
      {
        "id": "b7255a02-bb40-444b-9711-392ac1920171",
        "metadata": {
          "vector_store_key": "1910.09399-5",
          "chunk_id": 21,
          "document_id": "1910.09399",
          "start_idx": 8599,
          "end_idx": 9117
        },
        "page_content": " direct methods (single generator and discriminator), hierarchical methods (two or more generator-discriminator pairs, each with a different goal), and iterative methods (each generator-discriminator pair generates a gradually higher-resolution image). Following the introduction, BIBREF6 discusses methods for text-to-image and image-to-image synthesis, respectively, and also describes several evaluation metrics for synthetic images, including inception scores and Frechet Inception Distance (FID), and explains the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the average length of the claims?",
    "llm_answer": "Answer not found in the context.\n",
    "context": [
      {
        "id": "23abb5f1-1c93-4336-adcb-5e7c2b02da29",
        "metadata": {
          "vector_store_key": "2004.03034-9",
          "chunk_id": 32,
          "document_id": "2004.03034",
          "start_idx": 12548,
          "end_idx": 13000
        },
        "page_content": ") for a particular claim C is defined by number of claims included in the argument path starting from the thesis until the claim C. For example, in Figure FIGREF1, the context length for O1 and S3 are 1 and 2 respectively. Table TABREF8 shows number of claims with the given range of context length for the claims with more than 5 votes and $60\\%$ agreement score. We observe that more than half of these claims have 3 or higher context length. Similar",
        "type": "Document"
      },
      {
        "id": "51da70c4-0a00-4fbb-8232-e268baa48195",
        "metadata": {
          "vector_store_key": "2004.03034-9",
          "chunk_id": 31,
          "document_id": "2004.03034",
          "start_idx": 12247,
          "end_idx": 12760
        },
        "page_content": " claim is timely and appropriate given its context. There are cases in the dataset where the same claim has different impact labels, when presented within a different context. Therefore, we claim that it is not sufficient to only study the linguistic characteristic of a claim to determine its impact, but it is also necessary to consider its context in determining the impact. Context length ($\\text{C}_{l}$) for a particular claim C is defined by number of claims included in the argument path starting from the",
        "type": "Document"
      },
      {
        "id": "7147a5b8-5358-4b48-9d8e-2dbf89cbc501",
        "metadata": {
          "vector_store_key": "2004.03034-9",
          "chunk_id": 33,
          "document_id": "2004.03034",
          "start_idx": 13000,
          "end_idx": 13546
        },
        "page_content": " agreement score. We observe that more than half of these claims have 3 or higher context length. Similar to prior work, our aim is to understand the characteristics of impactful claims in argumentation. However, we hypothesize that the qualitative characteristics of arguments is not independent of the context in which they are presented. To understand the relationship between argument context and the impact of a claim, we aim to incorporate the context along with the claim itself in our predictive models. Prediction task. Given a claim, we",
        "type": "Document"
      },
      {
        "id": "521b024e-758a-4249-af24-eed867d46c15",
        "metadata": {
          "vector_store_key": "1703.05260-8",
          "chunk_id": 50,
          "document_id": "1703.05260",
          "start_idx": 17876,
          "end_idx": 18350
        },
        "page_content": " inter-annotator agreement was measured (see Section \"Inter-Annotator Agreement\" ). It was found to be sufficiently high. Annotation of the corpus together with some pre- and post-processing of the data required about 500 hours of work. All stories were annotated with event and participant types (a total of 12,188 and 43,946 instances, respectively). On average there were 7 coreference chains per story with an average length of 6 tokens. After the first annotation round",
        "type": "Document"
      },
      {
        "id": "f2a0fa1b-9a4b-4ffb-b087-6000074a903f",
        "metadata": {
          "vector_store_key": "2004.03034-3",
          "chunk_id": 23,
          "document_id": "2004.03034",
          "start_idx": 9268,
          "end_idx": 9778
        },
        "page_content": " our study to the claims with at least 5 votes to have a more reliable assignment for the accumulated impact label for each claim. Impact label statistics. Table TABREF7 shows the distribution of the number of votes for each of the impact categories. The claims have $241,884$ total votes. The majority of the impact votes belong to medium impact category. We observe that users assign more high impact and very high impact votes than low impact and no impact votes respectively. When we restrict the claims to",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What debate topics are included in the dataset?",
    "llm_answer": "Politics, World, Society, Law, Ethics, War, Conflict in the Middle East, Latin America, Commonwealth, and 10+ other topics.\n",
    "context": [
      {
        "id": "f65849a0-dbae-4d4c-9691-5376c2bab712",
        "metadata": {
          "vector_store_key": "1906.03538-1",
          "chunk_id": 33,
          "document_id": "1906.03538",
          "start_idx": 13163,
          "end_idx": 13614
        },
        "page_content": " Table TABREF19 . To better understand the topical breakdown of claims in the dataset, we crowdsource the set of \u201ctopics\u201d associated with each claim (e.g., Law, Ethics, etc.) We observe that, as expected, the three topics of Politics, World, and Society have the biggest portions (Figure FIGREF21 ). Additionally, the included claims touch upon 10+ different topics. Figure FIGREF22 depicts a few popular categories and sampled questions from each. We",
        "type": "Document"
      },
      {
        "id": "b7e3d104-0319-4c0d-92a3-13a44ab7e9ae",
        "metadata": {
          "vector_store_key": "2004.03034-8",
          "chunk_id": 8,
          "document_id": "2004.03034",
          "start_idx": 3138,
          "end_idx": 3668
        },
        "page_content": ",000 claims that are part of a diverse collection of arguments on 741 controversial topics. The structure of the website dictates that each argument must present a supporting or opposing claim for its parent claim, and stay within the topic of the main thesis. Rather than being posts on a social media platform, these are community-curated claims. Furthermore, for each presented claim, the audience votes on its impact within the given line of reasoning. Critically then, the dataset includes the argument context for each claim",
        "type": "Document"
      },
      {
        "id": "b81d53aa-d5ff-416d-8421-552407342414",
        "metadata": {
          "vector_store_key": "1708.05873-3",
          "chunk_id": 20,
          "document_id": "1708.05873",
          "start_idx": 8352,
          "end_idx": 8830
        },
        "page_content": "solution', `effort', `general', etc. Topic 9 - War. This topic directly relates to issues of war. The key words appear to be linked to discussions around ongoing wars. Topic 10 - Conflict in the Middle East. This topic clearly picks up issues related to the Middle East \u2013 particularly around peace and conflict in the Middle East. Topic 11 - Latin America. This is another topic with a regional focus, picking up on issues related to Latin America. Topic 12 - Commonwealth. This",
        "type": "Document"
      },
      {
        "id": "202ebcf3-04ed-4ceb-bd4f-977aa722c3c5",
        "metadata": {
          "vector_store_key": "1710.01492-1",
          "chunk_id": 32,
          "document_id": "1710.01492",
          "start_idx": 12206,
          "end_idx": 12623
        },
        "page_content": " them. A topic can be anything that people express opinions about, e.g., a product (e.g., iPhone6), a political candidate (e.g., Donald Trump), a policy (e.g., Obamacare), an event (e.g., Brexit), etc. For example, in (6) the author is positive about Donald Trump but negative about Hillary Clinton. A political analyzer would not be interested so much in the overall sentiment expressed in the tweet (even though one",
        "type": "Document"
      },
      {
        "id": "51f5890a-f27d-4c1e-bf09-052e8c9f8570",
        "metadata": {
          "vector_store_key": "1708.05873-0",
          "chunk_id": 13,
          "document_id": "1708.05873",
          "start_idx": 5623,
          "end_idx": 6152
        },
        "page_content": " positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence. The topic quality is usually evaluated by highest probability words, which is presented in Figure FIGREF4 . Figure FIGREF4 provides a list of the main topics (and the highest probability words associated these topics) that emerge from the STM of UN General Debate statements. In addition to the highest probability words, we use several other measures of key words (not presented here) to interpret the dimensions.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much, the proposed method improves BiDAF and DCN on SQuAD dataset?",
    "llm_answer": "The context states that the hybrid and DCA models perform better than BiDAF and Co-Attention, but does not provide specific improvement numbers.\n",
    "context": [
      {
        "id": "dc199ef9-84f9-4aae-9f86-07e6e302124e",
        "metadata": {
          "vector_store_key": "2001.11268-7",
          "chunk_id": 74,
          "document_id": "2001.11268",
          "start_idx": 29425,
          "end_idx": 29920
        },
        "page_content": " use of original SQuAD domains to enrich our data. This was done in order to save computing resources, as an addition of 100 SQuAD domains resulted in training time increases of two hours, depending on various other parameter settings. Adjusted parameters include increased batch size, and decreased maximal context length in order to reduce training time. With this paper we aimed to explore state-of-the-art NLP methods to advance systematic review (semi)automation. Both of the presented fine",
        "type": "Document"
      },
      {
        "id": "3270ecc7-c2fd-4dbe-9e50-c8d2ad7428b8",
        "metadata": {
          "vector_store_key": "1803.09230-2",
          "chunk_id": 38,
          "document_id": "1803.09230",
          "start_idx": 12850,
          "end_idx": 13315
        },
        "page_content": " the results. Co-Attention with char embedding was giving us worse results so we put the best numbers we got for Co-Attention there. We would like to point out that the BiDAF model here doesn't include BiLSTM layer as present in original paper because the BiLSTM didn't give any advantage except for slowing down the training. Selected tensorboard visualizations are also shown in Figure 3 . Visualizations demonstrate that both hybrid and DCA models perform better",
        "type": "Document"
      },
      {
        "id": "1ba7e843-b9ea-40a7-b06a-a04cb106e448",
        "metadata": {
          "vector_store_key": "1804.03396-0",
          "chunk_id": 65,
          "document_id": "1804.03396",
          "start_idx": 19961,
          "end_idx": 20345
        },
        "page_content": " SQuAD dataset are all performed with the open source code from BIBREF27 . We use 100 1D filters with width 5 to construct the CharCNN in our char embedding layer. We set the hidden size $d=100$ for all the hidden states in our model. The optimizer we use is the AdaDelta optimizer BIBREF45 with an initial learning rate of 2. A dropout BIBREF46 rate of 0.2 is applied in all the CNN,",
        "type": "Document"
      },
      {
        "id": "cbe9f208-3730-4128-8f81-ac3a378ea889",
        "metadata": {
          "vector_store_key": "1707.03764-0",
          "chunk_id": 16,
          "document_id": "1707.03764",
          "start_idx": 6168,
          "end_idx": 6712
        },
        "page_content": " out to be detrimental to performance, when compared to the simpler system that we have described in Section SECREF2 . We divide our attempts according to the different ways we attempted to enhance performance: manipulating the data itself (adding more, and changing preprocessing), using a large variety of features, and changing strategies in modelling the problem by using different algorithms and paradigms. All reported results are on the PAN 2017 training data using five-fold cross-validation, unless otherwise specified. We extended the",
        "type": "Document"
      },
      {
        "id": "2a042588-be0c-4291-94e3-c2bf45292f7f",
        "metadata": {
          "vector_store_key": "1803.09230-2",
          "chunk_id": 39,
          "document_id": "1803.09230",
          "start_idx": 13315,
          "end_idx": 13814
        },
        "page_content": "izations are also shown in Figure 3 . Visualizations demonstrate that both hybrid and DCA models perform better than vanilla Co-Attention and BiDAF attention mechanisms and reduce the losses faster and increase the dev F1/EM scores faster as well. We made a brief attempt to do a bit of hyperparameter tuning on our proposed DCA model and we report the results in Table 3 . Ideally, hyperparameter tuning for neural network architectures should be done using bayesian hyperparameter optimization but",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the linguistic differences between each class?",
    "llm_answer": "The languages are divided into Indo-European and Sino-Tibetan families, with Basque as an isolate.  Within Indo-European, there are West Germanic and Romance subfamilies.  Languages within the same phyla (e.g., Romance, Germanic) share similar pronounced words.  Language varieties also exist within some languages (e.g., Portuguese, Spanish).  The languages differ in their syllable structures, verbal morphology, plural morphemes, and past tense acquisition.\n",
    "context": [
      {
        "id": "7b1c09b6-d8db-4d35-8a92-0acba47cc1bc",
        "metadata": {
          "vector_store_key": "1909.13362-4",
          "chunk_id": 44,
          "document_id": "1909.13362",
          "start_idx": 15624,
          "end_idx": 16095
        },
        "page_content": " language groupings within families. We selected six evaluation languages: English, Dutch, Italian, French, Basque, and Manipuri. These represent two language families (Indo-European, Sino-Tibetan), a language isolate thought to be unrelated to any existing language (Basque), and two different subfamilies within the Indo-European family (West Germanic, Romance). The primary constraint was the availability of syllabified datasets for training and testing. Table TABREF",
        "type": "Document"
      },
      {
        "id": "e90f75a5-f326-4202-82c5-878736af8308",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 51,
          "document_id": "1910.04269",
          "start_idx": 19553,
          "end_idx": 20023
        },
        "page_content": ", the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Kat",
        "type": "Document"
      },
      {
        "id": "c392de3c-59c3-4999-9da2-64d4f7ea8caf",
        "metadata": {
          "vector_store_key": "1909.13362-0",
          "chunk_id": 15,
          "document_id": "1909.13362",
          "start_idx": 5671,
          "end_idx": 6191
        },
        "page_content": " each are closely related. Evaluating a model solely on these three languages, the approach taken in BIBREF13 and others, does not adequately test a model's generalized ability to learn diverse syllable structures. In this paper, we present a neural network that can syllabify phone sequences without introducing any fixed principles or rules of syllabification. We show that this novel approach to syllabification is language-agnostic by evaluating it on datasets of six languages, five from two major language families",
        "type": "Document"
      },
      {
        "id": "76742302-3175-4030-92bb-a96733cbea73",
        "metadata": {
          "vector_store_key": "1910.05456-5",
          "chunk_id": 71,
          "document_id": "1910.05456",
          "start_idx": 25338,
          "end_idx": 25885
        },
        "page_content": "ques2011study investigated the effect of a stay abroad on Spanish L2 acquisition, including learning of its verbal morphology in English speakers. jia2003acquisition studied how Mandarin Chinese-speaking children learned the English plural morpheme. nicoladis2012young studied the English past tense acquisition in Chinese\u2013English and French\u2013English bilingual children. They found that, while both groups showed similar production accuracy, they differed slightly in the type of errors they made. Also considering the effect of the native language",
        "type": "Document"
      },
      {
        "id": "846d88d4-17a3-45cb-9069-ca8ebef0653d",
        "metadata": {
          "vector_store_key": "1707.03764-2",
          "chunk_id": 2,
          "document_id": "1707.03764",
          "start_idx": 794,
          "end_idx": 1328
        },
        "page_content": " and language variety from their tweets in four different languages: English, Spanish, Arabic, and Portuguese. Gender consists of a binary classification (male/female), whereas language variety differs per language, from 2 varieties for Portuguese (Brazilian and Portugal) to 7 varieties for Spanish (Argentina, Chile, Colombia, Mexico, Peru, Spain, Venezuela). The challenge is thus to classify users along two very different axes, and in four highly different languages \u2013 forcing participants to either build models that can capture",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what genres do they songs fall under?",
    "llm_answer": "Blues, Rap, Metal, Folk, R&B, Reggae, Country, and Religious.\n",
    "context": [
      {
        "id": "87d1c82d-5146-4f0a-a4a5-cde65590d82c",
        "metadata": {
          "vector_store_key": "2003.05377-2",
          "chunk_id": 33,
          "document_id": "2003.05377",
          "start_idx": 11875,
          "end_idx": 12353
        },
        "page_content": ". In some of the songs, songwriters try to address religious education, which could explain the confusion between those genres. The MPB (Brazilian Popular Music) genre was the most confused of all, which may indicate that song lyrics of this genre cover a wide range of subjects that intersect with other genres. In this work we constructed a dataset of $138,368$ Brazilian song lyrics distributed in 14 genres. We applied SVM, Random Forest, and a Bidirectional Long Short-Term",
        "type": "Document"
      },
      {
        "id": "cbbd4415-b6bf-4a4d-9622-98da8a460834",
        "metadata": {
          "vector_store_key": "2003.05377-4",
          "chunk_id": 7,
          "document_id": "2003.05377",
          "start_idx": 2865,
          "end_idx": 3346
        },
        "page_content": " performance gains on three different classification tasks: detecting genre, distinguishing the best and the worst songs, and determining the approximate publication time of a song. The experiments in genre classification focused on eight genres: Blues, Rap, Metal, Folk, R&B, Reggae, Country, and Religious. Only lyrics in English were included and they used an SVM with the default settings for the classification. Ying et al. BIBREF0 used Part-of-Speech (POS) features extracted",
        "type": "Document"
      },
      {
        "id": "f725c40c-c23d-4441-af88-8b33912ba713",
        "metadata": {
          "vector_store_key": "2003.05377-4",
          "chunk_id": 1,
          "document_id": "2003.05377",
          "start_idx": 511,
          "end_idx": 1044
        },
        "page_content": " their music collections BIBREF1. Automatic music genre classification based only on the lyrics is considered a challenging task in the field of Natural Language Processing (NLP). Music genres remain a poorly defined concept, and boundaries between genres still remain fuzzy, which makes the automatic classification problem a nontrivial task BIBREF1. Traditional approaches in text classification have applied algorithms such as Support Vector Machine (SVM) and Na\u00efve Bayes, combined with handcraft features (POS and chunk tags) and",
        "type": "Document"
      },
      {
        "id": "9eac4ddf-03ae-460f-bbcc-94560cfd8b78",
        "metadata": {
          "vector_store_key": "2003.05377-4",
          "chunk_id": 0,
          "document_id": "2003.05377",
          "start_idx": 0,
          "end_idx": 511
        },
        "page_content": "Music is part of the day-to-day life of a huge number of people, and many works try to understand the best way to classify, recommend, and identify similarities between songs. Among the tasks that involve music classification, genre classification has been studied widely in recent years BIBREF0 since musical genres are the main top-level descriptors used by music dealers and librarians to organize their music collections BIBREF1. Automatic music genre classification based only on the lyrics is considered a",
        "type": "Document"
      },
      {
        "id": "d99dbbe2-d8ed-4feb-a6c5-9c88ac90d020",
        "metadata": {
          "vector_store_key": "2003.05377-4",
          "chunk_id": 8,
          "document_id": "2003.05377",
          "start_idx": 3156,
          "end_idx": 3624
        },
        "page_content": " Ying et al. BIBREF0 used Part-of-Speech (POS) features extracted from lyrics and combined them with three different machine learning techniques \u2013 k-Nearest-Neighbor, Na\u00efve Bayes, and Support Vector Machines \u2013 to classify a collection of 600 English songs by the genre and mood. Zaanen and Kanters BIBREF7 used the term frequency and inverse document frequency statistical metrics as features to solve music mood classification, obtaining an accuracy of more than 70%.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "To what other competitive baselines is this approach compared?",
    "llm_answer": "Baseline and Ablation models.\n",
    "context": [
      {
        "id": "dc50a5cf-b755-41a0-a173-1b561db0ed05",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "c5c3c735-6d4c-4070-9fa8-a5c0f544b37c",
        "metadata": {
          "vector_store_key": "1910.08210-9",
          "chunk_id": 44,
          "document_id": "1910.08210",
          "start_idx": 16635,
          "end_idx": 17105
        },
        "page_content": " We compare to baselines and ablated variants on a simplified variant of in which there are one-to-one group assignments (no group), stationary monsters (no dyna), and no natural language templated descriptions (no nl). Figure FIGREF29 shows that compared to baselines and ablated variants, is more sample efficient and converges to higher performance. Moreover, no ablated variant is able to solve the tasks\u2014it is the combination of ablated features that enables to win",
        "type": "Document"
      },
      {
        "id": "f0b7e3cd-e054-4500-ae09-a0ffa726b77b",
        "metadata": {
          "vector_store_key": "1812.07023-3",
          "chunk_id": 46,
          "document_id": "1812.07023",
          "start_idx": 17433,
          "end_idx": 17962
        },
        "page_content": " set have been provided by the challenge organizers. For the prototype test set and for the ablation study presented in Table TABREF24 , we use the same code for evaluation metrics as used by BIBREF11 for fairness and comparability. We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below: Our primary architectural differences over the baseline model are: not concatenating the question, answer pairs before encoding them, the auxiliary decoder module,",
        "type": "Document"
      },
      {
        "id": "11d9b2e6-e20d-4ab9-8d7c-c35a7e2d27e6",
        "metadata": {
          "vector_store_key": "1603.07044-6",
          "chunk_id": 43,
          "document_id": "1603.07044",
          "start_idx": 14997,
          "end_idx": 15515
        },
        "page_content": " task A already had a substantial amount of training data. Table 4 gives the final comparison between different models (we only list the MAP score because it is the official score for the challenge). Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited",
        "type": "Document"
      },
      {
        "id": "24bf2271-5c8e-4680-b893-d10c22b99f5e",
        "metadata": {
          "vector_store_key": "1707.03904-4",
          "chunk_id": 55,
          "document_id": "1707.03904",
          "start_idx": 20754,
          "end_idx": 21226
        },
        "page_content": " baselines, however, lag behind human performance by $16.4\\%$ and $32.1\\%$ for Quasar-S and Quasar-T respectively, indicating the strong potential for improvement. Interestingly, for human performance we observe that non-experts are able to match or beat the performance of experts when given access to the background corpus for searching the answers. We also emphasize that the human performance is limited by either the knowledge of the experts, or the usefulness of the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is human evaluation performed, what was the criteria?",
    "llm_answer": "Human evaluators graded generated poems from four perspectives: Fluency (grammatical and rhythmic well-formedness), Semantic coherence, adequacy, and appropriateness.  Evaluators were native Chinese speakers, divided into expert (15 with bachelor's degrees in Chinese literature) and amateur (15 with bachelor's degrees in other fields) groups.\n",
    "context": [
      {
        "id": "c1484a9f-d7af-4d07-9461-1d96d91be2bc",
        "metadata": {
          "vector_store_key": "2004.01694-7",
          "chunk_id": 4,
          "document_id": "2004.01694",
          "start_idx": 1628,
          "end_idx": 2197
        },
        "page_content": " changes in the evaluation design affect the outcome of the human evaluation. We find that for all three aspects, human translations are judged more favourably, and significantly better than MT, when we make changes that we believe strengthen the evaluation design. Based on our empirical findings, we formulate a set of recommendations for human MT evaluation in general, and assessing human\u2013machine parity in particular. All of our data are made publicly available for external validation and further analysis. We first review current methods to assess the quality of",
        "type": "Document"
      },
      {
        "id": "88a343cd-5733-4a91-9fbb-0158597a7c9b",
        "metadata": {
          "vector_store_key": "1909.00279-9",
          "chunk_id": 41,
          "document_id": "1909.00279",
          "start_idx": 15265,
          "end_idx": 15755
        },
        "page_content": "ators to join our human evaluation. The human evaluators were divided into two groups. The expert group contains 15 people who hold a bachelor degree in Chinese literature, and the amateur group contains 15 people who holds a bachelor degree in other fields. All 30 human evaluators are native Chinese speakers. We ask evaluators to grade each generated poem from four perspectives: 1) Fluency: Is the generated poem grammatically and rhythmically well formed, 2) Semantic coherence: Is the",
        "type": "Document"
      },
      {
        "id": "3b921cbd-e8cf-4b08-8097-48820c27b38c",
        "metadata": {
          "vector_store_key": "2004.01694-7",
          "chunk_id": 91,
          "document_id": "2004.01694",
          "start_idx": 35319,
          "end_idx": 35678
        },
        "page_content": " human evaluation of MT in general. Our recommendations have the aim of increasing the validity of MT evaluation, but we are aware of the high cost of having MT evaluation done by professional translators, and on the level of full documents. We welcome future research into alternative evaluation protocols that can demonstrate their validity at a lower cost.",
        "type": "Document"
      },
      {
        "id": "65420d2b-c729-4d92-a761-1ae85866f47a",
        "metadata": {
          "vector_store_key": "2004.01694-7",
          "chunk_id": 3,
          "document_id": "2004.01694",
          "start_idx": 1337,
          "end_idx": 1854
        },
        "page_content": " concur that these findings have demonstrated the need to critically re-evaluate the design of human MT evaluation. Our paper investigates three aspects of human MT evaluation, with a special focus on assessing human\u2013machine parity: the choice of raters, the use of linguistic context, and the creation of reference translations. We focus on the data shared by BIBREF3, and empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation. We find that for all three aspects",
        "type": "Document"
      },
      {
        "id": "80b9670e-b107-4fd4-b43a-2b7c7c92c002",
        "metadata": {
          "vector_store_key": "2004.01694-2",
          "chunk_id": 85,
          "document_id": "2004.01694",
          "start_idx": 32633,
          "end_idx": 33097
        },
        "page_content": " a small number of human raters: five in the experiments of Sections SECREF3 and SECREF35, four per condition (adequacy and fluency) in Section SECREF4, and one in the fine-grained error analysis presented in Section SECREF24. Moreover, the results presented in this article are based on one text domain (news) and one language direction (Chinese to English), and while a large-scale evaluation with another language pair supports our findings (see above), further",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much better were results of the proposed models than base LSTM-RNN model?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "e152b4ae-6c68-42bc-89fe-2987a9bd857d",
        "metadata": {
          "vector_store_key": "1906.00180-3",
          "chunk_id": 8,
          "document_id": "1906.00180",
          "start_idx": 3251,
          "end_idx": 3690
        },
        "page_content": "IBREF27 and LSTM BIBREF28 . Training and testing accuracies after 50 training epochs, averaged over five different model runs, are shown in Table UID18 . All recurrent models outperform the summing baseline. Even the simplest recurrent network, the SRN, achieves higher training and testing accuracy scores than the tree-shaped matrix model. The GRU and LSTM even beat the tensor model. The LSTM obtains slightly lower scores than the GRU,",
        "type": "Document"
      },
      {
        "id": "19ed571a-8dee-4450-b120-bb6d044a8169",
        "metadata": {
          "vector_store_key": "2002.02492-7",
          "chunk_id": 81,
          "document_id": "2002.02492",
          "start_idx": 25396,
          "end_idx": 25780
        },
        "page_content": " RNNs implies that all of the recurrent language models that we train are consistent according to Lemma UNKREF23. Our LSTM models achieve similar test perplexity ($91.86 \\pm 0.4$) to those reported in previous work BIBREF24; see Appendix for further details. Additionally, we train self-terminating $\\tanh $-RNN and LSTM-RNN variants (Definition UNKREF33) at various values of $\\epsil",
        "type": "Document"
      },
      {
        "id": "50539209-b504-4b2e-88ee-661900ad1269",
        "metadata": {
          "vector_store_key": "1910.12618-5",
          "chunk_id": 66,
          "document_id": "1910.12618",
          "start_idx": 26085,
          "end_idx": 26516
        },
        "page_content": " of the NNs, with the embedding RNN edging the MLP TF-IDF one. However for the UK data set the RFs yield significantly better results on the test set than the NNs. This inversion of performance of the algorithms is possibly due to a change in the way the reports were written by the Met Office after August 2017, since the results of the MLP and RNN on the validation set (not shown here) were satisfactory and better than both RFs",
        "type": "Document"
      },
      {
        "id": "1d080d26-4113-431d-9eb9-8c72c31e9b10",
        "metadata": {
          "vector_store_key": "1902.09314-4",
          "chunk_id": 46,
          "document_id": "1902.09314",
          "start_idx": 16362,
          "end_idx": 16777
        },
        "page_content": " is a lightweight alternative of recurrent models, we study the model size of each model on the Restaurant dataset. Statistical results are reported in Table TABREF37 . We implement all the compared models base on the same source code infrastructure, use the same hyperparameters, and run them on the same GPU . RNN-based and BERT-based models indeed have larger model size. ATAE-LSTM, IAN, RAM, and AEN-GloVe-BiLST",
        "type": "Document"
      },
      {
        "id": "570a8b4f-0dd5-473f-b4ba-5d2ad4cff5fe",
        "metadata": {
          "vector_store_key": "1911.00069-3",
          "chunk_id": 73,
          "document_id": "1911.00069",
          "start_idx": 23531,
          "end_idx": 23962
        },
        "page_content": " model transfer performance. Among the 2 neural network models, the Bi-LSTM model achieves a better cross-lingual RE performance than the CNN model for 6 out of the 7 target languages. In terms of absolute performance, the Bi-LSTM model achieves over $40.0$ $F_1$ scores for German, Spanish, Portuguese and Chinese. In terms of relative performance, it reaches over $75\\%$ of the accuracy of the supervised target-language RE model",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which one of the four proposed models performed best?",
    "llm_answer": "\"Ours with Mask and Ordered Triplets\"\n",
    "context": [
      {
        "id": "c357514b-19f8-468b-b68e-b8bc90741f38",
        "metadata": {
          "vector_store_key": "1701.06538-1",
          "chunk_id": 45,
          "document_id": "1701.06538",
          "start_idx": 17346,
          "end_idx": 17826
        },
        "page_content": ". Details can be found in Appendix UID77 . Results of these three models form the bottom line of Figure FIGREF32 -right. Table TABREF33 compares the results of these models to the best previously-published result on this dataset . Even the fastest of these models beats the best published result (when controlling for the number of training epochs), despite requiring only 6% of the computation. We trained our models using TensorFlow BIBREF30 on clusters containing 16-32 Tesla K",
        "type": "Document"
      },
      {
        "id": "dc50a5cf-b755-41a0-a173-1b561db0ed05",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "6e5dd4a7-27b3-4d21-8529-fac0bc4ba1f2",
        "metadata": {
          "vector_store_key": "1901.02257-9",
          "chunk_id": 60,
          "document_id": "1901.02257",
          "start_idx": 18849,
          "end_idx": 19405
        },
        "page_content": " Our ensemble model surpasses the current state-of-the-art model with an accuracy of 84.84%. We got the final ensemble result by voting on 4 single models. Every single model uses the same architecture but different parameters. To study the effectiveness of each perspective, we conduct several experiments on the three single perspectives and their combination perspective. Table 3 presents their comparison results. The first group of models are based on the three single perspectives, and we can observe that the union perspective performs best compared",
        "type": "Document"
      },
      {
        "id": "47b152dd-73e8-40eb-a927-52d5511639b3",
        "metadata": {
          "vector_store_key": "1905.06566-2",
          "chunk_id": 89,
          "document_id": "1905.06566",
          "start_idx": 24955,
          "end_idx": 25353
        },
        "page_content": " We also conducted human experiment with 20 randomly sampled documents from the CNNDM test set. We compared our model $\\text{\\sc Hibert}_M$ against Lead3, DCA, Latent, BERT and the human reference (Human). We asked the subjects to rank the outputs of these systems from best to worst. As shown in Table 4 , the output of $\\text{\\sc Hibert}_M$ is selected as the best in 30% of cases and we obtained",
        "type": "Document"
      },
      {
        "id": "c26476e4-a2d3-4d1e-9ac7-d7b03f81da16",
        "metadata": {
          "vector_store_key": "1909.13362-7",
          "chunk_id": 60,
          "document_id": "1909.13362",
          "start_idx": 21575,
          "end_idx": 22056
        },
        "page_content": " model, which replaces the CRF output of the Base model with a softmax. A comparison of the results of these three models can be seen in Table TABREF25. This comparison empirically motivates the CRF output because Base almost always outperforms Base-Softmax. Of these three models, the Base model performed the best with the exception of the French and Manipuri datasets. The differences in the French results can be considered negligible because the accuracies are all near $100\\%",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much is proposed model better than baselines in performed experiments?",
    "llm_answer": "The proposed model outperforms baselines by a clear margin in new environments, showing increases in EM and GM.  For example, it outperforms the second best model by several points in the Test-New set and in comparison to the next best two models by over INLINEFORM0 increases in EM and GM.  In previously seen environments, it increases accuracy by 35% and 25% compared to baselines for EM and GM metrics, respectively.\n",
    "context": [
      {
        "id": "a1974dac-f964-4aeb-a415-a2006317d90a",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 68,
          "document_id": "1810.00663",
          "start_idx": 26280,
          "end_idx": 26763
        },
        "page_content": " room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM. Note that the average number of actions in the ground truth output sequences is 7.07 for the Test-New set. Our model's predictions are just INLINEFORM1 edits off on average from the correct navigation plans. This",
        "type": "Document"
      },
      {
        "id": "08676cf7-5cf4-4a2c-b9d7-ddbe513c2062",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 66,
          "document_id": "1810.00663",
          "start_idx": 25415,
          "end_idx": 25983
        },
        "page_content": " In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models. The previous section evaluated model performance on new instructions (and corresponding navigation plans) for environments that were previously seen at training time. Here, we examine whether the trained models succeed on environments that are completely new. The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as",
        "type": "Document"
      },
      {
        "id": "11d9b2e6-e20d-4ab9-8d7c-c35a7e2d27e6",
        "metadata": {
          "vector_store_key": "1603.07044-6",
          "chunk_id": 43,
          "document_id": "1603.07044",
          "start_idx": 14997,
          "end_idx": 15515
        },
        "page_content": " task A already had a substantial amount of training data. Table 4 gives the final comparison between different models (we only list the MAP score because it is the official score for the challenge). Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited",
        "type": "Document"
      },
      {
        "id": "dc50a5cf-b755-41a0-a173-1b561db0ed05",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "97be5370-e788-41d2-9b17-3e7739266f06",
        "metadata": {
          "vector_store_key": "2001.11381-3",
          "chunk_id": 138,
          "document_id": "2001.11381",
          "start_idx": 31860,
          "end_idx": 32196
        },
        "page_content": "ados. Dicho lo anterior, el Modelo 1 podr\u00eda ser considerado como nuestro propio baseline. A continuaci\u00f3n presentamos un protocolo de evaluaci\u00f3n manual de los resultados obtenidos. El experimento consisti\u00f3 en la generaci\u00f3n de 15 frases por cada uno de los tres modelos propuestos. Para cada modelo, se consideraron tres queries: $Q=$ {AM",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much is precission increased?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "0ef3afdf-a68c-4d53-895f-0490c7b9672d",
        "metadata": {
          "vector_store_key": "1909.13375-1",
          "chunk_id": 66,
          "document_id": "1909.13375",
          "start_idx": 22494,
          "end_idx": 22947
        },
        "page_content": " writing, and not just on multi-span questions. In order to analyze the effect of each of our changes, we conduct ablation studies on the development set, depicted in Table TABREF26. Not using the simple preprocessing from Section SECREF17 resulted in a 2.5 point decrease in both EM and F1. The numeric questions were the most affected, with their performance dropping by 3.5 points. Given that number questions make up about 61% of the dataset, we can",
        "type": "Document"
      },
      {
        "id": "8e595a15-617e-47fa-8f20-8208066c69dc",
        "metadata": {
          "vector_store_key": "1905.06566-2",
          "chunk_id": 83,
          "document_id": "1905.06566",
          "start_idx": 23289,
          "end_idx": 23633
        },
        "page_content": " by 0.5 ROUGE. We also implemented two baselines. One is the hierarchical transformer summarization model (HeriTransfomer; described in \"Extractive Summarization\" ) without pre-training. Note the setting for HeriTransfomer is ( $L=4$ , $H=300$ and $A=4$ ) . We can see that the pre-training (details in Section \"Pre-training\" ) leads to a +1.25",
        "type": "Document"
      },
      {
        "id": "17be1d28-a556-4678-bddd-c8a1ea377d61",
        "metadata": {
          "vector_store_key": "1904.05862-0",
          "chunk_id": 38,
          "document_id": "1904.05862",
          "start_idx": 13711,
          "end_idx": 14238
        },
        "page_content": " increases. We suspect that this is because the training signal from the positive samples decreases as the number of negative samples increases. In this experiment, everything is kept equal except for the number of negative samples. Next, we analyze the effect of data augmentation through cropping audio sequences (\u00a7 SECREF11 ). When creating batches we crop sequences to a pre-defined maximum length. Table shows that a crop size of 150K frames results in the best performance. Not restricting the maximum length (None) gives",
        "type": "Document"
      },
      {
        "id": "b19155bf-7680-427f-a67b-14f3148caa1e",
        "metadata": {
          "vector_store_key": "1708.05873-1",
          "chunk_id": 35,
          "document_id": "1708.05873",
          "start_idx": 14464,
          "end_idx": 15020
        },
        "page_content": " largest populations. However, with Topic 7 there is an increase in expected topic proportion until countries have a population of around 300 million, after which there is a decline in discussion of Topic 7. For countries with populations larger than 500 million there is no effect of population on discussion of Topic 7. It is only with the very largest populations that we see a positive effect on discussion of Topic 7. We would also expect the extent to which states discuss international development in their GD statements to be impacted by the amount",
        "type": "Document"
      },
      {
        "id": "cb88a1c3-f404-4104-ae5b-f68cd7d3e059",
        "metadata": {
          "vector_store_key": "1909.13375-1",
          "chunk_id": 67,
          "document_id": "1909.13375",
          "start_idx": 22947,
          "end_idx": 23384
        },
        "page_content": " 3.5 points. Given that number questions make up about 61% of the dataset, we can deduce that our improved number handling is responsible for about a 2.1 point gain, while the rest could be be attributed to the improved Wikipedia parsing. Although NER span cleaning (Section SECREF23) affected only 3% of the multi-span questions, it provided a solid improvement of 5.4 EM in multi-span questions and 1.5 EM in single-span questions. The",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What labels are in the dataset?",
    "llm_answer": "LOC, PER, ORG, Miscellaneous, NAG, OAG, CAG, PIO,  (and possibly others depending on the source).\n",
    "context": [
      {
        "id": "45e75b36-ee14-4963-8966-836c2b4786de",
        "metadata": {
          "vector_store_key": "1911.07228-1",
          "chunk_id": 24,
          "document_id": "1911.07228",
          "start_idx": 6150,
          "end_idx": 6611
        },
        "page_content": " corpus which are provided by VLSP 2016 - Named Entity Recognition. The dataset contains four different types of label: Location (LOC), Person (PER), Organization (ORG) and Miscellaneous - Name of an entity that do not belong to 3 types above (Table TABREF15). Although the corpus has more information about the POS and chunks, but we do not use them as features in our model. There are two folders with 267 text files of training data and 45 text files of test",
        "type": "Document"
      },
      {
        "id": "8f5c91d1-d096-4ad6-a3c5-8813f67d0118",
        "metadata": {
          "vector_store_key": "1907.11499-7",
          "chunk_id": 56,
          "document_id": "1907.11499",
          "start_idx": 22325,
          "end_idx": 22841
        },
        "page_content": " from bag labels, small amounts of instance labels are also available BIBREF23 . It would also be interesting to examine how the label space influences model performance, especially since in our scenario the labels are extrapolated from Wikipedia and might be naturally noisy and/or ambiguous. The authors would like to thank the anonymous reviewers and the action editor, Yusuke Miyao, for their valuable feedback. We acknowledge the financial support of the European Research Council (Lapata; award number 681760).",
        "type": "Document"
      },
      {
        "id": "84d72307-bd9a-4537-95f3-1de4b037e375",
        "metadata": {
          "vector_store_key": "2001.05493-4",
          "chunk_id": 63,
          "document_id": "2001.05493",
          "start_idx": 21322,
          "end_idx": 21715
        },
        "page_content": " used the English code-mixed dataset provided by TRAC 2018. This dataset contains three labels, (a) Non-Aggressive(NAG), (b) Overtly-Aggressive (OAG) and (c) Covertly-Aggressive(CAG). The distribution of training, validation and test sets are described in Table TABREF56. Kaggle Dataset: This dataset contains 20001 tweets which are manually labeled. The labels are divided into two categories",
        "type": "Document"
      },
      {
        "id": "1deb0f9d-c0ed-4265-9fbe-86ac9b0749f6",
        "metadata": {
          "vector_store_key": "2001.11268-1",
          "chunk_id": 12,
          "document_id": "2001.11268",
          "start_idx": 4947,
          "end_idx": 5446
        },
        "page_content": " associated with labelling, its authors acquired sentence labels via automated annotation. Seven classes were assigned on the basis of structured headings within the text of each abstract. Table TABREF4 provides an overview of class abbreviations and their meaning.In the following we refer to it as the PubMed data. The LSTM itself yields impressive results with F1 scores for annotation of up to 0.85 for PIO elements, it generalizes across domains and assigns one label per sentence. We were able",
        "type": "Document"
      },
      {
        "id": "b2e60dc2-e07f-4d81-b110-45935449d7d7",
        "metadata": {
          "vector_store_key": "1911.00133-6",
          "chunk_id": 58,
          "document_id": "1911.00133",
          "start_idx": 22478,
          "end_idx": 23036
        },
        "page_content": " framing and intentions of the writer. We intend for our future work to use this dataset to contextualize stress and offer explanations using the content features of the text. Additional interesting problems applicable to this dataset include the development of effective distant labeling schemes, which is a significant first step to developing a quantitative model of stress. We would like to thank Fei-Tzin Lee, Christopher Hidey, Diana Abagyan, and our anonymous reviewers for their insightful comments during the writing of this paper. This research was",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are customer satisfaction, customer frustration and overall problem resolution data collected?",
    "llm_answer": "The analysis uses 216 satisfied and 500 unsatisfied customer conversations, 271 resolved and 425 unresolved problem conversations, and 534 frustrated and 229 not frustrated customer conversations.  The data is labeled based on consensus or majority vote.\n",
    "context": [
      {
        "id": "2c5eaaf2-349f-4a50-8341-447e0d6af9be",
        "metadata": {
          "vector_store_key": "1709.05413-2",
          "chunk_id": 98,
          "document_id": "1709.05413",
          "start_idx": 39019,
          "end_idx": 39532
        },
        "page_content": " that are satisfied/not frustrated and resolved problems) are shown at the top rows of the table, and undesirable outcomes (unsatisfied/frustrated customers and unresolved problems) are shown at the bottom rows. Our analysis helps zone in on how the use of certain dialogue acts may be likely to result in different outcomes. The weights we observe vary in the amount of insight provided: for example, offering extra help at the end of a conversation, or thanking the customer yields more satisfied customers, and",
        "type": "Document"
      },
      {
        "id": "7603abcf-37d1-43d4-a720-73ae7600fe8f",
        "metadata": {
          "vector_store_key": "1709.05413-2",
          "chunk_id": 88,
          "document_id": "1709.05413",
          "start_idx": 34849,
          "end_idx": 35438
        },
        "page_content": " receive majority consensus for a label, or received majority vote of \"can't tell\". Our final conversation sets consist of 216 satisfied and 500 unsatisfied customer conversations, 271 resolved and 425 unresolved problem conversations, and 534 frustrated and 229 not frustrated customer conversations. We retain the inherent imbalance in the data to match the natural distribution observed. The clear excess of consensus of responses that indicate negative outcomes further motivates us to understand what sorts of dialogic patterns results in such outcomes. We run the experiment for each",
        "type": "Document"
      },
      {
        "id": "ccfa112d-bec4-4373-915b-48763e576b7d",
        "metadata": {
          "vector_store_key": "1709.05413-2",
          "chunk_id": 100,
          "document_id": "1709.05413",
          "start_idx": 39812,
          "end_idx": 40354
        },
        "page_content": " strong association with unsatisfied customers. Giving elaborate answers that are not a simple affirmative, negative, or response acknowledgement (i.e. Answer (Other)) towards the middle of a conversation leads to satisfied customers that are not frustrated. Likewise, requesting information towards the end of a conversation (implying that more information is still necessary at the termination of the dialogue) leads to unsatisfied and unresolved customers, with ratios of at least 4:1. By using the feature weights we derive from using our",
        "type": "Document"
      },
      {
        "id": "5f616ed6-e936-4078-9526-fa7a006aeba5",
        "metadata": {
          "vector_store_key": "1709.05413-2",
          "chunk_id": 99,
          "document_id": "1709.05413",
          "start_idx": 39532,
          "end_idx": 40044
        },
        "page_content": " offering extra help at the end of a conversation, or thanking the customer yields more satisfied customers, and more resolved problems (with ratios of above 6:1). However, some outcomes are much more subtle: for example, asking yes-no questions early-on in a conversation is highly associated with problem resolution (ratio 3:1), but asking them at the end of a conversation has as similarly strong association with unsatisfied customers. Giving elaborate answers that are not a simple affirmative, negative, or",
        "type": "Document"
      },
      {
        "id": "86c71033-292f-48b3-9723-001fd49e109a",
        "metadata": {
          "vector_store_key": "1911.00133-6",
          "chunk_id": 24,
          "document_id": "1911.00133",
          "start_idx": 9726,
          "end_idx": 10257
        },
        "page_content": " perfect agreement on 39% of the data, and for another 32% the majority was 3/5 or less. This suggests that our data displays significant variation in how stress is expressed, which we explore in the next section. While all our data has the same genre and personal narrative style, we find distinctions among domains with which classification systems must contend in order to perform well, and distinctions between stressful and non-stressful data which may be useful when developing such systems. Posters in each subreddit express",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many improvements on the French-German translation benchmark?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "4199c59c-1927-47cb-857b-9c1bed5dae20",
        "metadata": {
          "vector_store_key": "1911.03894-7",
          "chunk_id": 44,
          "document_id": "1911.03894",
          "start_idx": 15623,
          "end_idx": 16121
        },
        "page_content": " is also machine translated for all languages. The dataset is composed of 122k train, 2490 valid and 5010 test examples. As usual, NLI performance is evaluated using accuracy. To evaluate a model on a language other than English (such as French), we consider the two following settings: TRANSLATE-TEST: The French test set is machine translated into English, and then used with an English classification model. This setting provides a reasonable, although imperfect, way to circumvent the fact that",
        "type": "Document"
      },
      {
        "id": "18a1dacf-15c7-46e2-bc4d-ae61cde4f9e1",
        "metadata": {
          "vector_store_key": "1910.07481-4",
          "chunk_id": 30,
          "document_id": "1910.07481",
          "start_idx": 11703,
          "end_idx": 12134
        },
        "page_content": " best results, up to 0.85 BLEU points over the Baseline on the newstest2019 corpus. It also surpassed the Baselinee by 0.18 points on the newstest2017 with strong statistical significance, and by 0.15 BLEU points on the newstest2018 but this time with no statistical evidence. These encouraging results prompted us to extend experiments to another language pair: English-French. En$\\rightarrow $Fr: The Document system obtained the",
        "type": "Document"
      },
      {
        "id": "402f6e4f-e275-4c3f-adc5-268894d3e312",
        "metadata": {
          "vector_store_key": "1809.04686-0",
          "chunk_id": 22,
          "document_id": "1809.04686",
          "start_idx": 7910,
          "end_idx": 8389
        },
        "page_content": " does not provide a multilingual test set, we used the public translation engine Google Translate to translate the SST test set to French. Previous work by BIBREF40 has shown that replacing the human translated test set with a synthetic set (obtained by using Google Translate) produces only a small difference of around 1% absolute accuracy on their human-translated French SNLI test set. Therefore, the performance measured on our `pseudo' French SST test set is expected to be",
        "type": "Document"
      },
      {
        "id": "567bbb0a-0092-4d21-9d90-c38602b59835",
        "metadata": {
          "vector_store_key": "1910.07481-4",
          "chunk_id": 31,
          "document_id": "1910.07481",
          "start_idx": 12134,
          "end_idx": 12501
        },
        "page_content": " language pair: English-French. En$\\rightarrow $Fr: The Document system obtained the best results considering all metrics on all test sets with strong statistical evidence. It surpassed the Baseline by 1.09 BLEU points and 0.85 TER points on tst2015, 0.75 BLEU points and 0.76 TER points on tst2014, and 0.48 BLEU points and 0.68 TER points on tst2013. Fr$\\rightarrow",
        "type": "Document"
      },
      {
        "id": "9dd30255-9d1e-4f26-9b87-e7a75597f1d2",
        "metadata": {
          "vector_store_key": "1910.07481-4",
          "chunk_id": 29,
          "document_id": "1910.07481",
          "start_idx": 11497,
          "end_idx": 11877
        },
        "page_content": " Table TABREF7 contains results for both English to French and French to English translation tasks, models are evaluated on the tst2013, tst2014 and tst2015 test sets. En$\\rightarrow $De: The Baseline model obtained State-of-The-Art BLEU and TER results according to BIBREF23, BIBREF24. The Document system shows best results, up to 0.85 BLEU points over the Baseline on the newst",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they prevent the model complexity increasing with the increased number of slots?",
    "llm_answer": "The provided text discusses models that remove the model's dependency on slot-specific mechanisms.\n",
    "context": [
      {
        "id": "9a2984d9-4ee4-40b3-9061-8103e620d647",
        "metadata": {
          "vector_store_key": "1909.00754-2",
          "chunk_id": 59,
          "document_id": "1909.00754",
          "start_idx": 16354,
          "end_idx": 16636
        },
        "page_content": "wise},\\\\\n\\end{array}\\right.\n\n$  where $O(x)$ means the Inference Time Complexity (ITC) of the variable $x$ . For a model having an ITC of $O(1)$ with respect to the number of slots $n$ , and values $m$ , the ITM will be a multiplier of 2.15x, while for an ITC of $O(n)$ , it will be",
        "type": "Document"
      },
      {
        "id": "bee6418f-917d-40ce-8e9d-b823ff0e284b",
        "metadata": {
          "vector_store_key": "1909.00754-2",
          "chunk_id": 56,
          "document_id": "1909.00754",
          "start_idx": 15551,
          "end_idx": 15976
        },
        "page_content": " dataset, MultiWoZ BIBREF9 . It has a more complex ontology with 7 domains and 25 predefined slots. Since the combined slot-value pairs representation of the belief states has to be applied for the model with $O(n)$ ITC, the total number of slots is 35. The statistics of these two datsets are shown in Table 2 . Based on the statistics from these two datasets, we can calculate the theoretical Inference Time Multiplier (ITM",
        "type": "Document"
      },
      {
        "id": "4aa2b549-5ffc-46d0-845f-498845c075dc",
        "metadata": {
          "vector_store_key": "1911.02711-8",
          "chunk_id": 45,
          "document_id": "1911.02711",
          "start_idx": 15887,
          "end_idx": 16402
        },
        "page_content": " results compared to a hidden size of 256 either. We see an evident improvement of our model when the hidden size increases from 128 to 256. However, the improvement becomes relatively small compared to a large increase in the number of parameters when the hidden size is further increased to 360. Therefore, we adopt 256 as the hidden size in our experiments. As Table TABREF29 shows, the accuracy increases when increasing layer numbers from 1 to 2. More layers do not increase the accuracy on development set. We",
        "type": "Document"
      },
      {
        "id": "84693dd3-e7ee-40cd-826a-354f2413a01f",
        "metadata": {
          "vector_store_key": "1806.00722-7",
          "chunk_id": 37,
          "document_id": "1806.00722",
          "start_idx": 13866,
          "end_idx": 14348
        },
        "page_content": ", we also construct a series of 8-layer models. We set the hidden number to be 192, such that both 4-layer models and 8-layer models have similar number of parameters. For dense structured models, we set the dimension of hidden states to be 96. Since NMT model usually allocates a large proportion of its parameters to the source/target sentence embedding and softmax matrix, we explore in our experiments to what extent decreasing the dimensions of the three parts would harm the B",
        "type": "Document"
      },
      {
        "id": "a978bd62-e4a7-40a7-b1d7-726fb56cf844",
        "metadata": {
          "vector_store_key": "1909.11833-4",
          "chunk_id": 4,
          "document_id": "1909.11833",
          "start_idx": 1558,
          "end_idx": 2046
        },
        "page_content": " structures for accurate prediction. For example, BIBREF5 defines a parametrized local attention matrix for each slot. Slot-specific mechanisms become unwieldy when the dialogue task involves many topics and slots, as is typical in a complex conversational setting like product troubleshooting. Furthermore, due to the sparsity of labels, there may not be enough data to thoroughly train each slot-specific network structure. BIBREF6, BIBREF7 both propose to remove the model's dependency",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they measure model size?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "e40643e0-eafa-4429-97d9-dc39b220513e",
        "metadata": {
          "vector_store_key": "1909.09484-9",
          "chunk_id": 54,
          "document_id": "1909.09484",
          "start_idx": 18909,
          "end_idx": 19263
        },
        "page_content": " size $d_{emb}$ is set to 300. For our proposed model, the hidden size of decoder in dialogue policy maker is 150. The vocabulary size $|V|$ is 540 for DSTC2 and 4712 for Maluuba. And the size of $k_t$ is set to 20. An Adam optimizer BIBREF22 is used for training our models and baselines, with a learning rate of 0.001 for supervised training and 0.0001",
        "type": "Document"
      },
      {
        "id": "6028c26f-c810-427d-87d7-b3a79f47aad3",
        "metadata": {
          "vector_store_key": "1701.06538-8",
          "chunk_id": 90,
          "document_id": "1701.06538",
          "start_idx": 33051,
          "end_idx": 33526
        },
        "page_content": " first level branching factors are 32, 32, 64, 128, 256 and 256, respectively. Models are trained on a cluster of 32 Tesla K40 GPUs, except for the last two models, which are trained on clusters of 64 and 128 GPUs so as to have enough memory for all the parameters. For all models, training batch sizes are approximately 2.5 million words. Models are trained once-through over about 100 billion words. We implement several memory optimizations in order to fit up to 1 billion",
        "type": "Document"
      },
      {
        "id": "3488785c-d053-43b7-8293-c3c50f3c5473",
        "metadata": {
          "vector_store_key": "1909.00154-2",
          "chunk_id": 65,
          "document_id": "1909.00154",
          "start_idx": 24630,
          "end_idx": 25057
        },
        "page_content": " vectors) Development set: 20% of the dataset (2003 vectors) Test set: 20% of the dataset (2003 vectors) The PyLogit package BIBREF11 also uses Swissmetro as an example. Therefore, our model specifications will extend the default one from this package. We re-estimated this model with the train set and validated with testset. The results are shown in tables TABREF31 and TABREF32. Since we are comparing the models at the test",
        "type": "Document"
      },
      {
        "id": "f4579321-9662-40a1-8a0f-48bd08c87d37",
        "metadata": {
          "vector_store_key": "1909.00154-1",
          "chunk_id": 56,
          "document_id": "1909.00154",
          "start_idx": 21118,
          "end_idx": 21619
        },
        "page_content": " different experiments in the paper (and report the respective mean and standard deviation statistics). Whenever we want to analyse a particular model (e.g. to check the coefficients of a choice model), we select the one with the highest likelihood at the development set (i.e. in practice, its out-of-sample generalization performance), and report its performance on the test set. Since a choice model will typically involve other variables than the categorical ones that we learn the embeddings for,",
        "type": "Document"
      },
      {
        "id": "6857383f-9133-403b-a905-92dd09a83851",
        "metadata": {
          "vector_store_key": "1911.12579-9",
          "chunk_id": 87,
          "document_id": "1911.12579",
          "start_idx": 31608,
          "end_idx": 32013
        },
        "page_content": " for learning character-level representations of words in CBoW and SG models. Therefore, the n-grams from $3-9$ were tested to analyse the impact on the accuracy of embedding. We optimized the length of character n-grams from $minn=2$ and $maxn=7$ by keeping in view the word frequencies depicted in Table TABREF57. Window size (ws): The large ws means considering more context words and similarly less ws",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which model architecture do they for sentence encoding?",
    "llm_answer": "BiLSTM-max with a hidden size of 600D per direction.\n",
    "context": [
      {
        "id": "ac38ee5f-ecdb-481c-b0fc-515c7ea11338",
        "metadata": {
          "vector_store_key": "1810.09774-1",
          "chunk_id": 24,
          "document_id": "1810.09774",
          "start_idx": 9339,
          "end_idx": 9703
        },
        "page_content": " encoding models, cross-sentence attention models as well as fine-tuned pre-trained language models. For sentence encoding models, we chose a simple one-layer bidirectional LSTM with max pooling (BiLSTM-max) with the hidden size of 600D per direction, used e.g. in InferSent BIBREF17 , and HBMP BIBREF18 . For the other models, we have chosen ESIM BIBREF19 , which",
        "type": "Document"
      },
      {
        "id": "d8f70f58-a4ad-4509-8736-3023095d3b43",
        "metadata": {
          "vector_store_key": "1912.01673-0",
          "chunk_id": 10,
          "document_id": "1912.01673",
          "start_idx": 4013,
          "end_idx": 4472
        },
        "page_content": " few: BiLSTM with the max-pooling trained for natural language inference BIBREF13, masked language modeling and next sentence prediction using bidirectional Transformer BIBREF14, max-pooling last states of neural machine translation among many languages BIBREF15 or the encoder final state in attentionless neural machine translation BIBREF16. The most common way of evaluating methods of sentence embeddings is extrinsic, using so called `transfer tasks', i.",
        "type": "Document"
      },
      {
        "id": "f8155616-0799-4c67-aa3b-9a8f46fbc840",
        "metadata": {
          "vector_store_key": "1907.11499-1",
          "chunk_id": 30,
          "document_id": "1907.11499",
          "start_idx": 12138,
          "end_idx": 12567
        },
        "page_content": " we generate sentence labels and further assume that these apply to sentence words too. Again, we report INLINEFORM3 2prp+r INLINEFORM4 p INLINEFORM5 r INLINEFORM6  We show model performance against AMT domain labels in Table TABREF42 . Consistent with the automatic evaluation results, DetNet variants are the best performing models on the sentence-level task. On the Wikipedia datasets, DetNet INLINEFORM0 or DetNet INLINEFORM1",
        "type": "Document"
      },
      {
        "id": "2aad297e-653a-407f-8112-cd84261bf66f",
        "metadata": {
          "vector_store_key": "1908.06606-6",
          "chunk_id": 32,
          "document_id": "1908.06606",
          "start_idx": 11284,
          "end_idx": 11795
        },
        "page_content": "i$. To tell the model $Q$ and $X$ is two different sentence, a sentence type input is generated which is a binary label sequence to denote what sentence each character in the input belongs to. Positional encoding and mask matrix is also constructed automatically to bring in absolute position information and eliminate the impact of zero padding respectively. Then a hidden vector $V_s$ which contains both query and text information is generated through BERT-base model. Since BERT is trained on general corpus",
        "type": "Document"
      },
      {
        "id": "18769693-c45d-4282-b37c-b3d3a8d4f246",
        "metadata": {
          "vector_store_key": "1910.05608-1",
          "chunk_id": 19,
          "document_id": "1910.05608",
          "start_idx": 7191,
          "end_idx": 7689
        },
        "page_content": " architectures to handle data in many viewpoints. In our system, we use five different model architectures combining many types of CNN, and RNN. Each model will use some types of word embedding or handle directly sentence embedding to achieve the best general result. Source code of five models is extended from the GitHub repository The first model is TextCNN (figure FIGREF2) proposed in BIBREF11. It only contains CNN blocks following by some Dense layers. The output of multiple CNN blocks with",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which data sources do they use?",
    "llm_answer": "Online debate websites, source documents or summaries (in free text format), crowd sourcing, examinations designed by educational experts, French meteorological office (M\u00e9t\u00e9o France), stations of the National Oceanic and Atmospheric Administration (NOAA), Stanford Sentiment Treebank (SST-2), Yelp dataset, Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), and Quora Question Pairs.\n",
    "context": [
      {
        "id": "eb1ce3aa-59c4-45e6-a3ea-77b0dadb95e7",
        "metadata": {
          "vector_store_key": "1906.03538-1",
          "chunk_id": 64,
          "document_id": "1906.03538",
          "start_idx": 25144,
          "end_idx": 25628
        },
        "page_content": " US Defense Advanced Research Projects Agency (DARPA). The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. We provide brief statistics on the sources of different content in our dataset in Table TABREF46 . In particular, this table shows: the size of the data collected from online debate websites (step 1). the size of the data filtered out (step 2a). the size of the perspectives added",
        "type": "Document"
      },
      {
        "id": "e7b0d9fd-8534-4312-9d21-60512ad3cfae",
        "metadata": {
          "vector_store_key": "1910.00458-8",
          "chunk_id": 75,
          "document_id": "1910.00458",
          "start_idx": 26170,
          "end_idx": 26674
        },
        "page_content": "26 are human-generated and based on source documents or summaries in free text format. However, since annotators tend to copy spans as answers BIBREF27, the majority of answers are still extractive in these datasets. The multi-choice QA datasets are collected either via crowd sourcing, or collected from examinations designed by educational experts BIBREF7. In this type of QA datasets, besides token matching, a significant portion of questions require multi-sentence reasoning and external knowledge B",
        "type": "Document"
      },
      {
        "id": "7b3b1c6b-e03e-47d0-95ad-cb2c9959eebd",
        "metadata": {
          "vector_store_key": "1910.12618-0",
          "chunk_id": 22,
          "document_id": "1910.12618",
          "start_idx": 9057,
          "end_idx": 9544
        },
        "page_content": " the French meteorological office, M\u00e9t\u00e9o France, while the British ones are scrapped from stations of the National Oceanic and Atmospheric Administration (NOAA). Available on the same time span as the consumption, they usually have a 3 hours temporal resolution but are averaged to a daily one as well. Finally the time series were scaled to the range $[0,1]$ before the training phase, and re-scaled during prediction time. Our work aims at predicting time series using exclusively text",
        "type": "Document"
      },
      {
        "id": "19ad6a1f-7f24-400e-9235-38467b0ba391",
        "metadata": {
          "vector_store_key": "1910.00458-7",
          "chunk_id": 51,
          "document_id": "1910.00458",
          "start_idx": 17496,
          "end_idx": 17916
        },
        "page_content": "500). For the sentiment analysis category, we used the Stanford Sentiment Treebank (SST-2) dataset from the GLUE benchmark BIBREF22 (around 60k train examples) and the Yelp dataset (around 430k train examples). For the paraphrase category, three paraphrasing datasets are used from the GLUE benchmark: Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), and Quora Question Pairs (",
        "type": "Document"
      },
      {
        "id": "f65849a0-dbae-4d4c-9691-5376c2bab712",
        "metadata": {
          "vector_store_key": "1906.03538-1",
          "chunk_id": 33,
          "document_id": "1906.03538",
          "start_idx": 13163,
          "end_idx": 13614
        },
        "page_content": " Table TABREF19 . To better understand the topical breakdown of claims in the dataset, we crowdsource the set of \u201ctopics\u201d associated with each claim (e.g., Law, Ethics, etc.) We observe that, as expected, the three topics of Politics, World, and Society have the biggest portions (Figure FIGREF21 ). Additionally, the included claims touch upon 10+ different topics. Figure FIGREF22 depicts a few popular categories and sampled questions from each. We",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How were breast cancer related posts compiled from the Twitter streaming API?",
    "llm_answer": "A total of 1,000 tweets were classified as relevant.  A secondary feed searched for the keyword \"cancer\" to collect additional tweets.  48,113 tweets from these accounts were compiled from both the \"cancer\" and \"breast cancer\" feeds.\n",
    "context": [
      {
        "id": "46cc02b2-b034-4769-97b5-dbec9938b228",
        "metadata": {
          "vector_store_key": "1805.09959-1",
          "chunk_id": 6,
          "document_id": "1805.09959",
          "start_idx": 2486,
          "end_idx": 3003
        },
        "page_content": "th, 2016 through December 9th, 2017. The primary feed for the analysis collected INLINEFORM0 million tweets containing the keywords `breast' AND `cancer'. See Figure FIGREF2 for detailed Twitter frequency statistics along with the user activity distribution. Our secondary feed searched just for the keyword `cancer' which served as a comparison ( INLINEFORM1 million tweets, see Appendix 1), and helped us collect additional tweets relevant to cancer from patients. The numeric account ID provided in tweets helps to",
        "type": "Document"
      },
      {
        "id": "f1d16e9c-03c8-49d3-af95-30abffc38290",
        "metadata": {
          "vector_store_key": "1805.09959-1",
          "chunk_id": 24,
          "document_id": "1805.09959",
          "start_idx": 9408,
          "end_idx": 9892
        },
        "page_content": " 1,000 tweets that were classified as `relevant'. We uncovered three categories of immediate interest including: tweets authored by patients regarding their condition (21.6%), tweets from friends/family with a direct connection to a patient (21.9%), and survivors in remission (8.8%). We also found users posting diagnostic related inquiries (7.6%) about possible symptoms that could be linked to breast cancer, or were interested in receiving preventative check-ups. The rest (40.2%)",
        "type": "Document"
      },
      {
        "id": "154d8188-3e70-4e2b-a841-88a56b9c95d6",
        "metadata": {
          "vector_store_key": "1805.09959-1",
          "chunk_id": 30,
          "document_id": "1805.09959",
          "start_idx": 11796,
          "end_idx": 12298
        },
        "page_content": " A total of 48,113 tweets from these accounts were compiled from both the `cancer' (69%) and `breast' `cancer' (31%) feeds. We provided tweet frequency statistics in Figure FIGREF7 . This is an indicator that this population of breast cancer patients and survivors are actively tweeting about topics related to `cancer' including their experiences and complications. Next, we applied hedonometrics to compare the patient posts with all collected breast cancer tweets. We found that the surveyed patient",
        "type": "Document"
      },
      {
        "id": "db14bd4e-f135-43a3-a374-fa816c9ce2b4",
        "metadata": {
          "vector_store_key": "1805.09959-1",
          "chunk_id": 12,
          "document_id": "1805.09959",
          "start_idx": 4913,
          "end_idx": 5432
        },
        "page_content": ",cancer\u201d and 65.2% of all tweets mentioning `cancer' while active. More information regarding the types of Twitter endpoints and calculating the sampling proportion of collected tweets is described in Appendix II. Our goal was to analyze content authored only by patients. To help ensure this outcome we removed posts containing a URL for classification, BIBREF19 . Twitter allows users to spread content from other users via `retweets'. We also removed these posts prior to classification to isolate tweets authored by",
        "type": "Document"
      },
      {
        "id": "eb6607d6-efed-46c7-8eca-d25b72574f29",
        "metadata": {
          "vector_store_key": "1805.09959-1",
          "chunk_id": 29,
          "document_id": "1805.09959",
          "start_idx": 11523,
          "end_idx": 11980
        },
        "page_content": " groups annotated the 1,331 tweets to identify patients and evaluate the classifier's results. The raters, showing high inter-rater reliability, individually evaluated each tweet as self-diagnostic of a breast cancer patient or survivor. The rater's independent annotations had a 96% agreement.  The classifier correctly identified 1,140 tweets (85.6%) from 845 profiles. A total of 48,113 tweets from these accounts were compiled from both the `cancer' (69",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What machine learning and NLP methods were used to sift tweets relevant to breast cancer experiences?",
    "llm_answer": "Convolutional Neural Network (CNN) with Google's Tensorflow interface.\n",
    "context": [
      {
        "id": "46cc02b2-b034-4769-97b5-dbec9938b228",
        "metadata": {
          "vector_store_key": "1805.09959-1",
          "chunk_id": 6,
          "document_id": "1805.09959",
          "start_idx": 2486,
          "end_idx": 3003
        },
        "page_content": "th, 2016 through December 9th, 2017. The primary feed for the analysis collected INLINEFORM0 million tweets containing the keywords `breast' AND `cancer'. See Figure FIGREF2 for detailed Twitter frequency statistics along with the user activity distribution. Our secondary feed searched just for the keyword `cancer' which served as a comparison ( INLINEFORM1 million tweets, see Appendix 1), and helped us collect additional tweets relevant to cancer from patients. The numeric account ID provided in tweets helps to",
        "type": "Document"
      },
      {
        "id": "6fc63440-110c-412d-a424-d4a979a2d66a",
        "metadata": {
          "vector_store_key": "1805.09959-0",
          "chunk_id": 3,
          "document_id": "1805.09959",
          "start_idx": 1277,
          "end_idx": 1849
        },
        "page_content": ". These self-reported diagnostic indicators allowed for a sentiment analysis of tweets authored by patients. However, this process was tedious, since the samples were hand verified and sifted through multiple keyword searches. Here, we aim to automate this process with machine learning context classifiers in order to build larger sets of patient self-reported outcomes in order to quantify the patent experience. Patients with breast cancer represent a majority of people affected by and living with cancer. As such, it becomes increasingly important to learn from their",
        "type": "Document"
      },
      {
        "id": "9c79dc87-47bd-41f7-87b0-e5165cae3fbe",
        "metadata": {
          "vector_store_key": "1805.09959-0",
          "chunk_id": 26,
          "document_id": "1805.09959",
          "start_idx": 10184,
          "end_idx": 10696
        },
        "page_content": "ated set of tweets to train a more sophisticated classifier to uncover self-diagnostic tweets from users describing their personal breast cancer experiences as current patients or survivors. We implemented the Convolutional Neural Network (CNN) with Google's Tensorflow interface, BIBREF26 . We adapted our framework from, BIBREF18 , but instead trained the CNN on these 1000 labeled cancer related tweets. The trained CNN was applied to predict patient self-diagnostic tweets from our breast cancer dataset. The",
        "type": "Document"
      },
      {
        "id": "f1d16e9c-03c8-49d3-af95-30abffc38290",
        "metadata": {
          "vector_store_key": "1805.09959-1",
          "chunk_id": 24,
          "document_id": "1805.09959",
          "start_idx": 9408,
          "end_idx": 9892
        },
        "page_content": " 1,000 tweets that were classified as `relevant'. We uncovered three categories of immediate interest including: tweets authored by patients regarding their condition (21.6%), tweets from friends/family with a direct connection to a patient (21.9%), and survivors in remission (8.8%). We also found users posting diagnostic related inquiries (7.6%) about possible symptoms that could be linked to breast cancer, or were interested in receiving preventative check-ups. The rest (40.2%)",
        "type": "Document"
      },
      {
        "id": "01515f05-e09e-4073-9da2-ba3bab47867d",
        "metadata": {
          "vector_store_key": "1805.09959-0",
          "chunk_id": 55,
          "document_id": "1805.09959",
          "start_idx": 22605,
          "end_idx": 23211
        },
        "page_content": " of our diagnostic training sample. Future studies could incorporate other languages using our proposed framework. It would be important to also expand the API queries with translations of `breast' and `cancer'. This could allow for a cross cultural comparison of how social media influences patients and what patients express on social media.  We have demonstrated the potential of using context classifiers for identifying diagnostic tweets related to the experience of breast cancer patients. Our framework provides a proof of concept for integrating machine learning with natural language processing as",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What approach performs better in experiments global latent or sequence of fine-grained latent variables?",
    "llm_answer": "GVT (global latent variable) achieves relatively lower reconstruction PPL, suggesting it contains rich latent information.  Sequential latent variables (SVT) further improve reconstruction PPL.\n",
    "context": [
      {
        "id": "2bbacee8-9d0b-4d50-8c27-219a6678e212",
        "metadata": {
          "vector_store_key": "2002.11402-3",
          "chunk_id": 17,
          "document_id": "2002.11402",
          "start_idx": 6246,
          "end_idx": 6761
        },
        "page_content": " hyper-parameter tuning which could have resulted in much more accurate results but at the same time could have taken very long time as well. So instead, chose and tweaked the parameters based on initial results. We trained two models, one with sequence length 512 to capture document level important n-grams and second with sequence length 64 to capture sentence/paragraph level important n-grams. Through experiments it was evident that, sequence length plays a vital role in deciding context and locally/globally",
        "type": "Document"
      },
      {
        "id": "d338bd1b-7e89-43fe-905a-46681ea3977e",
        "metadata": {
          "vector_store_key": "2003.12738-1",
          "chunk_id": 59,
          "document_id": "2003.12738",
          "start_idx": 20839,
          "end_idx": 21332
        },
        "page_content": " latent variable into both models (CVAE and GVT) promote more diverse generation results and boost the diversity scores such as Dist-1, Dist-2, and Dist-3. Compare to baseline models, the GVT achieves relatively lower reconstruction PPL, which suggests that the global latent variable contains rich latent information (e.g., topic) for response generation. Meanwhile, the sequential latent variables of the SVT encode fine-grained latent information and further improve the reconstruction PPL.",
        "type": "Document"
      },
      {
        "id": "a3e8740a-e277-4626-8df0-ba8410637e7e",
        "metadata": {
          "vector_store_key": "2002.11402-3",
          "chunk_id": 18,
          "document_id": "2002.11402",
          "start_idx": 6558,
          "end_idx": 7010
        },
        "page_content": " it was evident that, sequence length plays a vital role in deciding context and locally/globally important n-grams. Final output is a concatenation of both the model outputs. Trained the topic model on single 32gb NVidia-V100 and it took around 50 hours to train the model with sequence length 512. We had to take 256gb ram machine to accommodate all data in memory for faster read/write. Also, trained model with 64 sequence length in around 17 hours",
        "type": "Document"
      },
      {
        "id": "6c3a35ee-bc21-4d93-a38e-4683edeb04ce",
        "metadata": {
          "vector_store_key": "2003.12738-0",
          "chunk_id": 61,
          "document_id": "2003.12738",
          "start_idx": 21530,
          "end_idx": 22057
        },
        "page_content": " while in the combined dataset of Persona and ED, we observe performance drop of SVT compare to other models. This is because both Persona and ED are well designed and have lower entropy than MojiTalk which collected from Twitter. We hypothesize that the sequential latent variables have no advantage in term of similarity to single, fixed \"gold response\" when model low entropy response. Indeed, in open domain dialogue response generation, automatic metric is not always aligned with the human judgement BIBREF28. In contrast",
        "type": "Document"
      },
      {
        "id": "d25625a5-f39a-450f-ac88-16a969bd9d50",
        "metadata": {
          "vector_store_key": "1909.02480-5",
          "chunk_id": 53,
          "document_id": "1909.02480",
          "start_idx": 16806,
          "end_idx": 17283
        },
        "page_content": " conditional probability by marginalizing over all possible latent variables (see Eq. (DISPLAY_FORM5)), which is intractable in practice. We propose three approximating decoding algorithms to reduce the search space.  Following BIBREF6, one simple and effective method is to select the best sequence by choosing the highest-probability latent sequence $\\mathbf {z}$: where identifying $\\mathbf {y}^*$ only requires independently maximizing the local probability for each output",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which translation system do they use to translate to English?",
    "llm_answer": "Google Translate.\n",
    "context": [
      {
        "id": "b0d78a46-0cfa-4367-a963-2a9a8d82c9a4",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 18,
          "document_id": "1910.04269",
          "start_idx": 6853,
          "end_idx": 7342
        },
        "page_content": " comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (",
        "type": "Document"
      },
      {
        "id": "86a322ac-500f-49d5-bac4-e53ed84a1232",
        "metadata": {
          "vector_store_key": "2001.05493-3",
          "chunk_id": 32,
          "document_id": "2001.05493",
          "start_idx": 11700,
          "end_idx": 12148
        },
        "page_content": " architectures discussed above. (see Figure FIGREF22. for block diagram of system architecture). We consider the text to be well formatted before applying the text to the embedding layer. First, we detect non-English text(which are few) and translate all of them to English using Google Translate. Still, there is some code mixed words like \"mc\", \"bc\" and other English abbreviations and spelling errors like \"nd\" in place of \"and\", \"u\" in place of",
        "type": "Document"
      },
      {
        "id": "0d76377c-a28e-406d-8386-9ee984b874b1",
        "metadata": {
          "vector_store_key": "1912.13109-1",
          "chunk_id": 14,
          "document_id": "1912.13109",
          "start_idx": 5540,
          "end_idx": 6048
        },
        "page_content": " using a two phase process Transliteration: In phase I, we used translation API's provided by Google translation services and exposed via a SDK, to transliteration the Hinglish messages to English messages. Translation: After transliteration, words that were specific to Hinglish were translated to English using an Hinglish-English dictionary. By doing this we converted the Hinglish message to and assortment of isolated words being presented in the message in a sequence that can also be represented using",
        "type": "Document"
      },
      {
        "id": "a29e6e63-fec5-4e39-ad7b-cca89beadda5",
        "metadata": {
          "vector_store_key": "1911.03894-6",
          "chunk_id": 46,
          "document_id": "1911.03894",
          "start_idx": 16382,
          "end_idx": 16738
        },
        "page_content": "BERT. For the TRANSLATE-TEST setting, we report results of the English RoBERTa to act as a reference. In the TRANSLATE-TRAIN setting, we report the best scores from previous literature along with ours. BiLSTM-max is the best model in the original XNLI paper, mBERT which has been reported in French in BIBREF52 and XLM (MLM+TLM) is the best-presented model",
        "type": "Document"
      },
      {
        "id": "728ef13a-a59a-4db5-96c8-afb0d28ebc73",
        "metadata": {
          "vector_store_key": "2004.01694-3",
          "chunk_id": 76,
          "document_id": "2004.01694",
          "start_idx": 29040,
          "end_idx": 29491
        },
        "page_content": ".1710$, $SD=0.0025$, 95 % confidence interval $[0.1708,0.1711]$). Our results show that using translationese (Chinese translated from English) rather than original source texts results in higher scores for MT systems in human evaluation, and that the lexical variety of translationese is smaller than that of original text. Our experiments in Sections SECREF3\u2013SECREF5 show that machine translation quality has not yet reached the level of professional",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which pre-trained English NER model do they use?",
    "llm_answer": "Flair's default NER model.\n",
    "context": [
      {
        "id": "1db8cff1-922a-4abf-9369-e4a29203b6d3",
        "metadata": {
          "vector_store_key": "1906.01183-1",
          "chunk_id": 20,
          "document_id": "1906.01183",
          "start_idx": 7231,
          "end_idx": 7643
        },
        "page_content": " are trained on United Nation Parallel Corpus. For pre-trained English NER system, we use the default NER model of Flair. We train our NER model using vanilla SGD with no momentum for 150 epochs, with an initial learning rate of 0.1 and a learning rate annealing method in which the train loss does not fall in 3 consecutive epochs. The hidden size of BiLSTM model is set to 256 and mini-batch size is set to 16.",
        "type": "Document"
      },
      {
        "id": "6d93378f-b977-4fd4-b8a0-776b3a8cc869",
        "metadata": {
          "vector_store_key": "1906.01183-1",
          "chunk_id": 11,
          "document_id": "1906.01183",
          "start_idx": 4269,
          "end_idx": 4709
        },
        "page_content": "trained English NER model We construct the English NER system following BIBREF7 . This system uses a bidirectional LSTM as a character-level language model to take context information for word embedding generation. The hidden states of the character language model (CharLM) are used to create contextualized word embeddings. The final embedding INLINEFORM0 is concatenated by the CharLM embedding INLINEFORM1 and GLOVE embedding INLINEFORM2",
        "type": "Document"
      },
      {
        "id": "60a49a5d-141d-45cc-9b94-9b557dc853e1",
        "metadata": {
          "vector_store_key": "1911.03894-8",
          "chunk_id": 59,
          "document_id": "1911.03894",
          "start_idx": 20718,
          "end_idx": 21200
        },
        "page_content": "-specific model BIBREF48, but our results suggest that the multilingual setting in which mBERT was trained is simply not enough to use it alone and fine-tune it for French NER, as it shows worse performance than even simple CRF models, suggesting that monolingual models could be better at NER. CamemBERT displays improved performance compared to prior work for the 4 downstream tasks considered. This confirms the hypothesis that pretrained language models can be effectively fine-",
        "type": "Document"
      },
      {
        "id": "5ba1f534-e717-41a0-8385-a08f7a00da32",
        "metadata": {
          "vector_store_key": "2002.11402-1",
          "chunk_id": 34,
          "document_id": "2002.11402",
          "start_idx": 12596,
          "end_idx": 13151
        },
        "page_content": " this exercise, we were able to test out the best suitable model architecture and data preparation steps so that similar models could be trained for Indian languages. Building cased or caseless NERs for English was not the final goal and this has already been benchmarked and explored before in previous approaches explained in \"Related Work\" section. We didn't use traditional datasets for model performance comparisons & benchmarks. As mentioned before, all the comparisons are being done with open-source models and libraries from the productionization",
        "type": "Document"
      },
      {
        "id": "d16ec97e-6c1e-4412-b70a-7a9cbe04687f",
        "metadata": {
          "vector_store_key": "1701.02877-7",
          "chunk_id": 50,
          "document_id": "1701.02877",
          "start_idx": 18041,
          "end_idx": 18461
        },
        "page_content": " data, which might take a long time to gather and retrain. For the setup in BIBREF35 language models are trained for seven weeks on the English Wikipedia, Reuters RCV1 BIBREF46 and parts of the Wall Street Journal, and results are reported over the CoNLL 2003 NER dataset. Reuters RCV1 is chosen as unlabelled data because the English CoNLL 2003 corpus is created from the Reuters RCV1 corpus. For this paper, we use the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much of the ASR grapheme set is shared between languages?",
    "llm_answer": "The 7 languages share the common basic Latin alphabet and both Hindi and Marathi use Devanagari script.\n",
    "context": [
      {
        "id": "4f334b84-c6be-49ad-831a-9f612fd712f4",
        "metadata": {
          "vector_store_key": "1909.06522-2",
          "chunk_id": 4,
          "document_id": "1909.06522",
          "start_idx": 1526,
          "end_idx": 1963
        },
        "page_content": " known at decoding time, and the language specific lexicon and language model applied. In the absence of a phonetic lexicon, building graphemic systems has shown comparable performance to phonetic lexicon-based approaches in extensive monolingual evaluations BIBREF11, BIBREF12, BIBREF13. Recent advances in end-to-end ASR models have attempted to take the union of multiple language-specific grapheme (i.e. orthographic character) sets,",
        "type": "Document"
      },
      {
        "id": "0daaa3f6-d84f-4374-9463-4338837e8db2",
        "metadata": {
          "vector_store_key": "1909.06522-2",
          "chunk_id": 5,
          "document_id": "1909.06522",
          "start_idx": 1963,
          "end_idx": 2391
        },
        "page_content": " union of multiple language-specific grapheme (i.e. orthographic character) sets, and use such union as a universal grapheme set for a single sequence-to-sequence ASR model BIBREF14, BIBREF15, BIBREF16. It allows for learning a grapheme-based model jointly on data from multiple languages, and performing ASR on within training set languages. In various cases it can produce performance gains over monolingual modeling that uses",
        "type": "Document"
      },
      {
        "id": "044bdaec-f5f7-461d-9434-d6936dcede19",
        "metadata": {
          "vector_store_key": "1909.06522-0",
          "chunk_id": 33,
          "document_id": "1909.06522",
          "start_idx": 12081,
          "end_idx": 12561
        },
        "page_content": " these 7 languages have little overlap except that (i) they all include common basic Latin alphabet, and (ii) both Hindi and Marathi use Devanagari script. We took the union of 7 character sets therein as the multilingual grapheme set (Section SECREF2), which contained 432 characters. In addition, we deliberately split 7 languages into two groups, such that the languages within each group were more closely related in terms of language family, orthography or phonology. We thus",
        "type": "Document"
      },
      {
        "id": "080d770c-98ea-43f4-ac54-df17ba4ba49d",
        "metadata": {
          "vector_store_key": "1909.06522-0",
          "chunk_id": 49,
          "document_id": "1909.06522",
          "start_idx": 17876,
          "end_idx": 18409
        },
        "page_content": " of 11.0% on Kannada and 18.4% on Hindi. We have presented a multilingual grapheme-based ASR model can effectively perform language-independent recognition on any within training set languages, and substantially outperform each monolingual ASR alternative. Various data augmentation techniques can yield further complementary improvements. Such single multilingual model can not only provide better ASR performance, but also serves as an alternative to the standard production deployment that typically includes extensive monolingual",
        "type": "Document"
      },
      {
        "id": "ece43e5a-292a-476f-8457-c5b96273dfb9",
        "metadata": {
          "vector_store_key": "1909.06522-2",
          "chunk_id": 35,
          "document_id": "1909.06522",
          "start_idx": 12770,
          "end_idx": 13169
        },
        "page_content": " hours, which are referred to as 7lang, 4lang, and 3lang respectively. Note that Kannada, Malayalam and Tamil are Dravidian languages, which have rich agglutinative inflectional morphology BIBREF2 and resulted in around 10% OOV token rates on test sets (Hindi had the lowest OOV rate as 2-3%). Such experimental setup was designed to answer the questions: If a single graphemic ASR model could scale",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the languages used to test the model?",
    "llm_answer": "English, French, German, and Spanish.\n",
    "context": [
      {
        "id": "b0d78a46-0cfa-4367-a963-2a9a8d82c9a4",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 18,
          "document_id": "1910.04269",
          "start_idx": 6853,
          "end_idx": 7342
        },
        "page_content": " comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (",
        "type": "Document"
      },
      {
        "id": "4199c59c-1927-47cb-857b-9c1bed5dae20",
        "metadata": {
          "vector_store_key": "1911.03894-7",
          "chunk_id": 44,
          "document_id": "1911.03894",
          "start_idx": 15623,
          "end_idx": 16121
        },
        "page_content": " is also machine translated for all languages. The dataset is composed of 122k train, 2490 valid and 5010 test examples. As usual, NLI performance is evaluated using accuracy. To evaluate a model on a language other than English (such as French), we consider the two following settings: TRANSLATE-TEST: The French test set is machine translated into English, and then used with an English classification model. This setting provides a reasonable, although imperfect, way to circumvent the fact that",
        "type": "Document"
      },
      {
        "id": "4e2a3aa3-835b-4542-be84-73745f2002b8",
        "metadata": {
          "vector_store_key": "1910.07481-4",
          "chunk_id": 17,
          "document_id": "1910.07481",
          "start_idx": 7033,
          "end_idx": 7506
        },
        "page_content": ". We consider two different models for each language pair: the Baseline and the Document model. We evaluate them on 3 test sets and report BLEU and TER scores. All experiments are run 8 times with different seeds, we report averaged results and p-values for each experiment. Translation tasks are English to German, proposed in the first document-level translation task at WMT 2019 BIBREF17, English to French and French to English, following the IWSLT translation task BIB",
        "type": "Document"
      },
      {
        "id": "77ad9c8e-30eb-484f-9026-bfde7c947210",
        "metadata": {
          "vector_store_key": "1809.04686-0",
          "chunk_id": 25,
          "document_id": "1809.04686",
          "start_idx": 9144,
          "end_idx": 9613
        },
        "page_content": "40 chose a subset of the SNLI test set (1332 sentences) and professionally translated it into four major languages - Arabic, French, Russian, and Spanish. We use the French test set for evaluation in Section \"Zero-Shot Classification Results\" and \"Analyses\" . Here, we first describe the model and training details of the base multilingual NMT model whose encoder is reused in all other tasks. Then we provide details about the task-specific classifiers. For each task,",
        "type": "Document"
      },
      {
        "id": "e90f75a5-f326-4202-82c5-878736af8308",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 51,
          "document_id": "1910.04269",
          "start_idx": 19553,
          "end_idx": 20023
        },
        "page_content": ", the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Kat",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which language has the lowest error rate reduction?",
    "llm_answer": "Sinhala\n",
    "context": [
      {
        "id": "f8517f6c-1f4f-45bc-9a23-3aa73ff4850e",
        "metadata": {
          "vector_store_key": "1909.06522-8",
          "chunk_id": 42,
          "document_id": "1909.06522",
          "start_idx": 15207,
          "end_idx": 15689
        },
        "page_content": "annada and Hindi, via language-specific lexicon and language model at decoding time. We found that, the language-specific decoding provided only moderate gains, presumably as discussed above, the language-independent decoding had given the mismatched-language word token rates as sufficiently low as 1-3%. Additionally, the multilingual ASR of 4lang and 3lang (Section SECREF15) achieved the same, or even slightly better performance as compared to the ASR of 7lang, suggesting that",
        "type": "Document"
      },
      {
        "id": "66ea3be0-61b3-4f1b-a74f-235893bcee46",
        "metadata": {
          "vector_store_key": "1909.06522-8",
          "chunk_id": 39,
          "document_id": "1909.06522",
          "start_idx": 14189,
          "end_idx": 14631
        },
        "page_content": " validation set was used for evaluation on test set. ASR word error rate (WER%) results are shown in Table TABREF11. We found that, although not explicitly given any information on test language identities, multilingual ASR with language-independent decoding (Section SECREF3) - trained on 3, 4, or 7 languages - substantially outperformed each monolingual ASR in all cases, and on average led to relative WER reductions between 4.6% (Sinhala",
        "type": "Document"
      },
      {
        "id": "b223a506-d991-4ca3-97de-9a464181c190",
        "metadata": {
          "vector_store_key": "1909.13362-7",
          "chunk_id": 63,
          "document_id": "1909.13362",
          "start_idx": 22727,
          "end_idx": 23135
        },
        "page_content": "IBREF12. Best-known results are also obtained on the Italian, French, and Basque datasets. Our reported accuracy of $94.9 \\pm 0.3\\%$ on the Manipuri dataset is furthest from state of the art. We suspect this to be due to having limited amounts of training data; the $97.5\\%$ accurate system from BIBREF35 supplemented their data-driven approach with rules of syllabification. Examples from the outputs of the",
        "type": "Document"
      },
      {
        "id": "607157e9-d78f-43e8-9a9f-6c8532564f80",
        "metadata": {
          "vector_store_key": "1910.05456-6",
          "chunk_id": 37,
          "document_id": "1910.05456",
          "start_idx": 13392,
          "end_idx": 13836
        },
        "page_content": " best, followed closely by ITA. While the good performance of HUN as a source language is still unexpected, ITA is closely related to SPA, which could explain the high accuracy of the final model. As for ENG, pretraining on EUS and NAV yields the worst final models \u2013 importantly, accuracy is over $15\\%$ lower than for QVH, which is also an unrelated language. This again suggests that the prefixing morphology of EUS and NAV might play a role",
        "type": "Document"
      },
      {
        "id": "9f55ed1d-d637-4d95-bf62-5397aa8f3c64",
        "metadata": {
          "vector_store_key": "1809.04686-5",
          "chunk_id": 44,
          "document_id": "1809.04686",
          "start_idx": 15790,
          "end_idx": 16307
        },
        "page_content": ", we reuse the three proposed systems from Table 1 after being trained only on the English classification task and test the systems on data from an unseen language (e.g. French). A reasonable upper bound to which zero-shot performance should be compared to is bridging - translating a French test text to English and then applying the English classifier on the translated text. If we assume the translation to be perfect, we should expect this approach to perform as well as the English classifier. The Amazon Reviews",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is moral bias measured?",
    "llm_answer": "Moral bias is measured by averaging the biases from several question/answer prompts with the element of interest inserted, using a template for moral biases with ten prompts (Tab. TABREF15).\n",
    "context": [
      {
        "id": "6e3ae777-6fcd-4186-a9f4-af3b605ab802",
        "metadata": {
          "vector_store_key": "1912.05238-8",
          "chunk_id": 43,
          "document_id": "1912.05238",
          "start_idx": 16326,
          "end_idx": 16786
        },
        "page_content": " via cosine-similarity, clustering or semantic search\u2014 improves the scores of the extracted moral actions. The correlation of the moral bias and the corresponding WEAT value was calculated to test consistency of findings. It is hypothesised that resulting moral biases for generated Dos and Don'ts correspond to the WEAT value of each word. The correlation was tested by means of Pearson's Correlation Coefficient: where $m_x$ and $m_y$ are the the means of $X",
        "type": "Document"
      },
      {
        "id": "db990337-efb4-46aa-8e06-2c4d676bbb94",
        "metadata": {
          "vector_store_key": "1912.05238-8",
          "chunk_id": 45,
          "document_id": "1912.05238",
          "start_idx": 17001,
          "end_idx": 17399
        },
        "page_content": " two or three starlets. The correlation between WEAT value and the moral bias gets tangible, when inspecting their correlation graphically, cf. Fig. FIGREF4. The concrete bias scores can be found in the Appendix, Fig. TABREF28 and TABREF29. For both WEAT and MCM, the scatter plots of Dos and Don'ts are divided on the x-axis. The Pearson's Correlation Coefficient using USE as embedding (Top) $r =",
        "type": "Document"
      },
      {
        "id": "67e96d3a-2efe-4dc7-8592-4afcbff1c5b5",
        "metadata": {
          "vector_store_key": "1912.05238-8",
          "chunk_id": 73,
          "document_id": "1912.05238",
          "start_idx": 27185,
          "end_idx": 27600
        },
        "page_content": " Tab. TABREF28. It presents the moral biases exemplary for the top ten Dos and Don'ts by WEAT value of both sets. The threshold between the groups is not 0, but slightly shifted negatively (Using USE further shifted than Using BERT). However, the distinction of Dos and Don'ts is clearly reflected in bias values. Using USE the mean bias of all considered elements is $-0.018$ ($std=0.025$), whereat the mean of Dos",
        "type": "Document"
      },
      {
        "id": "6ca15c62-1479-475c-8ee2-dbd58071e99e",
        "metadata": {
          "vector_store_key": "2001.07209-0",
          "chunk_id": 38,
          "document_id": "2001.07209",
          "start_idx": 14939,
          "end_idx": 15306
        },
        "page_content": " \u201cnot a moral issue\". We compared human ratings with model predictions at two tiers: for moral relevance, we paired the proportion of \u201cnot a moral issue\u201d human responses with irrelevance predictions $p(c_0\\,|\\,\\mathbf {q})$ for each topic, and for moral acceptability, we paired the proportion of \u201cacceptable\u201d responses with positive predictions $p(c_+\\,|\\,\\mathbf {q",
        "type": "Document"
      },
      {
        "id": "63717b8a-9478-4a7d-a33c-1b39a1dfd112",
        "metadata": {
          "vector_store_key": "1912.05238-3",
          "chunk_id": 28,
          "document_id": "1912.05238",
          "start_idx": 10858,
          "end_idx": 11383
        },
        "page_content": " closer to the denying response and thus to yield a negative bias. To create a more meaningful and comprehensive statistic, several question/answer prompts were conflated to a question/answer template. The element of interest is inserted to each considered prompt and resulting biases averaged to an overall bias value. Specifically, we considered two different biases: gender and moral. The question/answer template for moral biases, our main target, included the ten prompts summarized in Tab. TABREF15(Appendix), where any",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What metrics are used for evaluation?",
    "llm_answer": "Task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, F1-score, BPRA, Belief Per-Response Accuracy, average rank, percentage of best-ranked questions, BLEU, ROUGE, METEOR.\n",
    "context": [
      {
        "id": "4786191e-fed1-4406-89b6-7b8835719e5f",
        "metadata": {
          "vector_store_key": "1709.10217-2",
          "chunk_id": 18,
          "document_id": "1709.10217",
          "start_idx": 6394,
          "end_idx": 6850
        },
        "page_content": " evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the number of total tasks. User satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied, neutral, satisfied and very satisfied, respectively. Response fluency: There are three scores -1, 0, 1, which indicate nonfluency, neutral, fluency. Number of dialogue turns: The number of utterances in a task",
        "type": "Document"
      },
      {
        "id": "dc58289f-90f2-4f31-9a31-296d00bb01a9",
        "metadata": {
          "vector_store_key": "1709.10217-2",
          "chunk_id": 11,
          "document_id": "1709.10217",
          "start_idx": 4141,
          "end_idx": 4657
        },
        "page_content": ", in which only the released data can be used for training and developing. The other is an open evaluation that allow to collect external data for training and developing. For task 1, we use F1-score as evaluation metric. For the task-oriented dialogue systems, the best way for evaluation is to use the online human-computer dialogue. After finishing an online human-computer dialogue with a dialogue system, the human then manually evaluate the system by using the metrics of user satisfaction degree, dialogue flu",
        "type": "Document"
      },
      {
        "id": "175d0a0b-77ff-450c-8f37-24f131b845e7",
        "metadata": {
          "vector_store_key": "1905.08949-9",
          "chunk_id": 28,
          "document_id": "1905.08949",
          "start_idx": 10602,
          "end_idx": 11043
        },
        "page_content": " they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks. As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE B",
        "type": "Document"
      },
      {
        "id": "44492013-ea6a-4d0d-9984-8b621a88cebb",
        "metadata": {
          "vector_store_key": "1906.03538-4",
          "chunk_id": 45,
          "document_id": "1906.03538",
          "start_idx": 17710,
          "end_idx": 18208
        },
        "page_content": "6 and INLINEFORM7 , respectively. The metrics are averaged across all the perspectives INLINEFORM8 participating in the test set. The goal is to get estimates of the overall performance of the systems. Instead of creating a complex measure that would take all the aspects into account, we approximate the overall performance by multiplying the disjoint measures in INLINEFORM0 , INLINEFORM1 and INLINEFORM2 . While this gives an estimate on the overall quality, it ignores the pipeline structure of",
        "type": "Document"
      },
      {
        "id": "140ad6e1-f0ef-4e66-a00e-12f677b6f352",
        "metadata": {
          "vector_store_key": "1909.09484-3",
          "chunk_id": 46,
          "document_id": "1909.09484",
          "start_idx": 15874,
          "end_idx": 16378
        },
        "page_content": " of the dialogue state tracker, it aims to show the impact of the dialogue state tracker on the dialogue policy maker; (2) the accuracy of dialogue policy maker, it aims to explain the performance of different methods of constructing dialogue policy; (3) the quality of the final response, it aims to explain the impact of the dialogue policy on the final dialogue response. The evaluation metrics are listed as follows: BPRA: Belief Per-Response Accuracy (BPRA) tests the ability to generate the correct",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much training data is used?",
    "llm_answer": "2489 messages, then augmented to 7934 messages.  Also, 17M training tokens.\n",
    "context": [
      {
        "id": "a872be77-b483-4ddf-9cee-412fb23e61cf",
        "metadata": {
          "vector_store_key": "1912.13109-4",
          "chunk_id": 19,
          "document_id": "1912.13109",
          "start_idx": 7657,
          "end_idx": 8136
        },
        "page_content": " above few data augmentation techniques were applied to boost the learning of the network. Before applying augmentation, a train-test split of 78%-22% was done from the original, cleansed data set. Thus, 700 tweets/messages were held out for testing. All model evaluation were done in on the test set that got generated by this process. The results presented in this report are based on the performance of the model on the test set. The training set of 2489 messages were however",
        "type": "Document"
      },
      {
        "id": "3fd0d403-b663-4f73-b8f1-cfbe94ff3a61",
        "metadata": {
          "vector_store_key": "1910.12618-4",
          "chunk_id": 48,
          "document_id": "1910.12618",
          "start_idx": 19089,
          "end_idx": 19565
        },
        "page_content": " maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014",
        "type": "Document"
      },
      {
        "id": "2e1622b2-1517-4239-8581-693f3aece329",
        "metadata": {
          "vector_store_key": "1711.02013-3",
          "chunk_id": 55,
          "document_id": "1711.02013",
          "start_idx": 20658,
          "end_idx": 21116
        },
        "page_content": "ings, between recurrent layers, and on recurrent states were (0.7, 0.5, 0.5) respectively. dataset contains 17M training tokens and has a vocabulary size of 44k words. The dataset is partitioned into a training set (first 99M characters) and a development set (last 1M characters) that is used to report performance. As this dataset contains various articles from Wikipedia, the longer term information (such as current topic) plays a bigger role than in the",
        "type": "Document"
      },
      {
        "id": "148ea921-3ae7-4687-bc70-85a9690fe58b",
        "metadata": {
          "vector_store_key": "1912.13109-4",
          "chunk_id": 20,
          "document_id": "1912.13109",
          "start_idx": 7921,
          "end_idx": 8471
        },
        "page_content": " on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we",
        "type": "Document"
      },
      {
        "id": "3e56db6e-ce32-4bdd-b12d-d8c8ecbf7707",
        "metadata": {
          "vector_store_key": "1902.10525-4",
          "chunk_id": 82,
          "document_id": "1902.10525",
          "start_idx": 31058,
          "end_idx": 31608
        },
        "page_content": " commercially available data, artificially inflated data, and labeled/self-labeled anonymized recognition requests (see BIBREF14 for a more detailed description). The number of training samples varies from tens of thousands to several million per script, depending on the complexity and usage. The best configuration for our production systems were identified by running multiple experiments over a range of layer depths and widths on our Latin script datasets. For the Latin script experiments shown in Figure FIGREF63 , the training set we used was",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does their model differ from BERT?",
    "llm_answer": "The input embeddings are the sum of token, segmentation, and position embeddings.  The segmentation embeddings use learned sentence A and B embeddings for the first and second sentences, respectively.  Also, different parameterizations and sharing are used for parameter reduction.\n",
    "context": [
      {
        "id": "b547a998-461d-4987-b16a-b7bc943d03a8",
        "metadata": {
          "vector_store_key": "1909.00578-2",
          "chunk_id": 32,
          "document_id": "1909.00578",
          "start_idx": 10519,
          "end_idx": 11017
        },
        "page_content": " have a larger receptive field as well. The BERT multi-task versions perform better with highly correlated qualities like $\\mathcal {Q}4$ and $\\mathcal {Q}5$ (as illustrated in Figures 2 to 4 in the supplementary material). However, there is not a clear winner among them. Mathematical equivalence does not lead to deterministic results, especially when random initialization and stochastic learning algorithms are involved. An in-depth exploration of this point would involve further investigation",
        "type": "Document"
      },
      {
        "id": "e349c699-79b6-477b-b0bb-a583c1a9034d",
        "metadata": {
          "vector_store_key": "1812.06705-1",
          "chunk_id": 28,
          "document_id": "1812.06705",
          "start_idx": 10253,
          "end_idx": 10716
        },
        "page_content": " BERT shares the same model architecture with the original BERT. The differences are the input representation and training procedure. The input embeddings of BERT are the sum of the token embeddings, the segmentation embeddings and the position embeddings. For the segmentation embeddings in BERT, a learned sentence A embedding is added to every token of the first sentence, and if a second sentence exists, a sentence B embedding will be added to every token of",
        "type": "Document"
      },
      {
        "id": "03338a3c-2ae1-4e58-85fb-ceb67bae2f01",
        "metadata": {
          "vector_store_key": "2002.06644-2",
          "chunk_id": 10,
          "document_id": "2002.06644",
          "start_idx": 3849,
          "end_idx": 4295
        },
        "page_content": " as twelve times as compared to $BERT_{large}$, with bigger batches, and longer sequences. ALBERT, introduced in BIBREF7, uses factorized embedding parameterization and cross-layer parameter sharing for parameter reduction. These optimizations have led both the models to outperform $BERT_{large}$ in various benchmarking tests, like GLUE for text classification and SQuAD for Question Answering. Distilled BERT-based models: Secondly, we propose",
        "type": "Document"
      },
      {
        "id": "08ece5ac-66a8-40ce-98e7-f2c4f56b84ac",
        "metadata": {
          "vector_store_key": "1905.06566-2",
          "chunk_id": 86,
          "document_id": "1905.06566",
          "start_idx": 24100,
          "end_idx": 24427
        },
        "page_content": "SEP] tokens) into BERT and use the representation at [CLS] token to classify each sentence. Our model $\\text{\\sc Hibert}_S$1 outperforms BERT by 0.4 to 0.5 ROUGE despite with only half the number of model parameters ( $\\text{\\sc Hibert}_S$2 54.6M v.s. BERT 110M). Results on the NYT50 dataset show the similar trends (see Table",
        "type": "Document"
      },
      {
        "id": "f7743946-8df1-4eb8-a909-19c326a67246",
        "metadata": {
          "vector_store_key": "1905.13497-2",
          "chunk_id": 10,
          "document_id": "1905.13497",
          "start_idx": 3655,
          "end_idx": 4034
        },
        "page_content": "iguation dataset, PDP-60, our method achieves 68.3% accuracy, which is better than the state-of-art accuracy of 66.7%. On a WSC dataset, WSC-273, our method achieves 60.3%. As of today, state-of-the-art accuracy on the WSC-273 for single model performance is around 57%, BIBREF14 and BIBREF13 . These results suggest that BERT implicitly learns to establish complex relationships",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does explicit constraint on the KL divergence term that authors propose looks like?",
    "llm_answer": "$|D_{KL}\\big (q_\\phi ({z}|{x}) || p({z})\\big )-C|$\n",
    "context": [
      {
        "id": "2b3cd80d-2ea0-40af-9d4e-5b8a5317f6cb",
        "metadata": {
          "vector_store_key": "1909.13668-1",
          "chunk_id": 76,
          "document_id": "1909.13668",
          "start_idx": 24751,
          "end_idx": 25130
        },
        "page_content": " term ($|D_{KL}\\big (q_\\phi ({z}|{x}) || p({z})\\big )-C|$). We experimented with a range of constraints ($C$) on the KL term and various powerful and weak decoder architectures (LSTM, GRU, and CNN), and empirically confirmed that in all cases the constraint was satisfied. We showed that the higher value of KL encourages not only divergence from the prior distribution, but also",
        "type": "Document"
      },
      {
        "id": "0428b4b4-b07f-4949-8b5d-83d99dc11e29",
        "metadata": {
          "vector_store_key": "1909.13668-7",
          "chunk_id": 30,
          "document_id": "1909.13668",
          "start_idx": 9356,
          "end_idx": 9661
        },
        "page_content": " the results for $\\beta _C$-VAEGRU, $\\beta _C$-VAELSTM, and $\\beta _C$-VAECNN models on Yahoo and Yelp corpora. In all our experiments we found that $C\\!-\\!1\\!\\le KL\\!\\le \\! C\\!+\\!1$, demonstrating that the objective function effectively imposed the desired constraint on KL term. Hence, setting any $C>0$",
        "type": "Document"
      },
      {
        "id": "d1cdd55c-2963-44c2-b76e-4ec54369d00c",
        "metadata": {
          "vector_store_key": "1909.00430-1",
          "chunk_id": 19,
          "document_id": "1909.00430",
          "start_idx": 6876,
          "end_idx": 7257
        },
        "page_content": " would like to set INLINEFORM3 such that INLINEFORM4 and INLINEFORM5 are close. BIBREF0 suggest to use KL-divergence for this. KL-divergence is composed of two parts: INLINEFORM6 INLINEFORM7  Since INLINEFORM0 is constant, we only need to minimize INLINEFORM1 , therefore the loss function becomes: DISPLAYFORM0  Notice that computing INLINEFORM0 requires summation over INLINEFORM",
        "type": "Document"
      },
      {
        "id": "06f303de-cd74-41c6-afa9-1f9dfc90709c",
        "metadata": {
          "vector_store_key": "1909.13668-1",
          "chunk_id": 75,
          "document_id": "1909.13668",
          "start_idx": 24489,
          "end_idx": 24872
        },
        "page_content": " variant of $\\beta $-VAE objective, which permits explicit control of the KL term, and treats KL as a mechanism to control the amount of information transmitted between the encoder and decoder. The immediate impact of the explicit constraint is avoiding the collapse issue ($D_{KL}=0$) by setting a non-zero positive constraint ($C\\ge 0$) on the KL term ($|D_{KL}\\big (q_\\phi ({z}|{x",
        "type": "Document"
      },
      {
        "id": "c34959bb-0305-4c00-8e2a-9c33288abcbd",
        "metadata": {
          "vector_store_key": "1907.08937-2",
          "chunk_id": 20,
          "document_id": "1907.08937",
          "start_idx": 7767,
          "end_idx": 8248
        },
        "page_content": " that by taking both sides of KL divergence into account, our definition incorporates both the entity pairs with high probability in INLINEFORM3 and INLINEFORM4 . Intuitively, if INLINEFORM5 mainly distributes on a proportion of entities pairs that INLINEFORM6 emphasizes, INLINEFORM7 is only hyponymy of INLINEFORM8 . Considering both sides of KL divergence could help model yield more comprehensive consideration. We will talk about the advantage of this method in detail in sec:",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what was the baseline?",
    "llm_answer": "There are multiple baselines described.  Content, content-network, and image-based models; a LSTM baseline; a hierarchical transformer summarization model (HeriTransfomer); and a standard parallel corpus-trained baseline.\n",
    "context": [
      {
        "id": "25bb2afe-7835-48e9-a6f8-dbdbbde2116d",
        "metadata": {
          "vector_store_key": "1902.06843-8",
          "chunk_id": 91,
          "document_id": "1902.06843",
          "start_idx": 35139,
          "end_idx": 35416
        },
        "page_content": "s by -0.27. Baselines: To test the efficacy of our multi-modal framework for detecting depressed users, we compare it against existing content, content-network, and image-based models (based on the aforementioned general image feature, facial presence, and facial expressions.)",
        "type": "Document"
      },
      {
        "id": "95d982c1-df48-40f1-aea4-aa545ace30dc",
        "metadata": {
          "vector_store_key": "1909.02635-8",
          "chunk_id": 23,
          "document_id": "1909.02635",
          "start_idx": 9050,
          "end_idx": 9548
        },
        "page_content": " (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo",
        "type": "Document"
      },
      {
        "id": "8e595a15-617e-47fa-8f20-8208066c69dc",
        "metadata": {
          "vector_store_key": "1905.06566-2",
          "chunk_id": 83,
          "document_id": "1905.06566",
          "start_idx": 23289,
          "end_idx": 23633
        },
        "page_content": " by 0.5 ROUGE. We also implemented two baselines. One is the hierarchical transformer summarization model (HeriTransfomer; described in \"Extractive Summarization\" ) without pre-training. Note the setting for HeriTransfomer is ( $L=4$ , $H=300$ and $A=4$ ) . We can see that the pre-training (details in Section \"Pre-training\" ) leads to a +1.25",
        "type": "Document"
      },
      {
        "id": "947ca67f-f47a-4cb3-91ff-40d57888bc24",
        "metadata": {
          "vector_store_key": "1909.09484-5",
          "chunk_id": 55,
          "document_id": "1909.09484",
          "start_idx": 19263,
          "end_idx": 19734
        },
        "page_content": " models and baselines, with a learning rate of 0.001 for supervised training and 0.0001 for reinforcement learning. In reinforcement learning, the decay parameter $\\lambda $ is set to 0.8. The weight decay is set to 0.001. And early stopping is performed on developing set. The experimental results of the proposed model and baselines will be analyzed from the following aspects. BPRA Results: As shown in Table TABREF35, most of the models have similar performance on BP",
        "type": "Document"
      },
      {
        "id": "58a67b02-83ce-4b8f-9d11-44067be92302",
        "metadata": {
          "vector_store_key": "1910.07481-1",
          "chunk_id": 24,
          "document_id": "1910.07481",
          "start_idx": 9488,
          "end_idx": 9899
        },
        "page_content": " layers, $d_{ff} = 2048$ as the inner-layer dimensionality, $h = 8$ attention heads, $d_k = 64$ as queries and keys dimension and $Pdrop = 0.1$ as dropout probability. All experiments, including baselines, are run over 600k training steps with a batch size of approximately 3000 tokens. For all language pairs we trained a Baseline and a Document model. The Baseline is trained on a standard parallel corpus and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is dataset used?",
    "llm_answer": "The provided text describes multiple datasets with varying sizes.  No single \"the\" dataset size is explicitly stated.\n",
    "context": [
      {
        "id": "9270562a-8e53-4e1d-b760-74d8fd0bc872",
        "metadata": {
          "vector_store_key": "1905.06566-7",
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "0723454a-4062-463e-972a-81a58c6a504e",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "2e1622b2-1517-4239-8581-693f3aece329",
        "metadata": {
          "vector_store_key": "1711.02013-3",
          "chunk_id": 55,
          "document_id": "1711.02013",
          "start_idx": 20658,
          "end_idx": 21116
        },
        "page_content": "ings, between recurrent layers, and on recurrent states were (0.7, 0.5, 0.5) respectively. dataset contains 17M training tokens and has a vocabulary size of 44k words. The dataset is partitioned into a training set (first 99M characters) and a development set (last 1M characters) that is used to report performance. As this dataset contains various articles from Wikipedia, the longer term information (such as current topic) plays a bigger role than in the",
        "type": "Document"
      },
      {
        "id": "49dd09c0-f752-4a00-bc37-38c26e5e9acc",
        "metadata": {
          "vector_store_key": "1709.05404-5",
          "chunk_id": 48,
          "document_id": "1709.05404",
          "start_idx": 17579,
          "end_idx": 18022
        },
        "page_content": "-validation setup) for each dataset while holding the number of posts per class constant. Figure FIGREF26 shows the performance of each of the Gen, RQ, and Hyp datasets at intervals of 100 posts per class (up to the maximum size of 582 posts per class for Hyp, and 851 posts per class for RQ). From the graph, we can see that as a general trend, the datasets benefit from larger dataset sizes. Interestingly, the results for the RQ dataset are",
        "type": "Document"
      },
      {
        "id": "3e56db6e-ce32-4bdd-b12d-d8c8ecbf7707",
        "metadata": {
          "vector_store_key": "1902.10525-4",
          "chunk_id": 82,
          "document_id": "1902.10525",
          "start_idx": 31058,
          "end_idx": 31608
        },
        "page_content": " commercially available data, artificially inflated data, and labeled/self-labeled anonymized recognition requests (see BIBREF14 for a more detailed description). The number of training samples varies from tens of thousands to several million per script, depending on the complexity and usage. The best configuration for our production systems were identified by running multiple experiments over a range of layer depths and widths on our Latin script datasets. For the Latin script experiments shown in Figure FIGREF63 , the training set we used was",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much better does this baseline neural model do?",
    "llm_answer": "The baseline neural model outperforms the EntNet model in terms of mean error and achieves 0% test error on more tasks.\n",
    "context": [
      {
        "id": "a1974dac-f964-4aeb-a415-a2006317d90a",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 68,
          "document_id": "1810.00663",
          "start_idx": 26280,
          "end_idx": 26763
        },
        "page_content": " room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM. Note that the average number of actions in the ground truth output sequences is 7.07 for the Test-New set. Our model's predictions are just INLINEFORM1 edits off on average from the correct navigation plans. This",
        "type": "Document"
      },
      {
        "id": "bc8ac242-0b5e-4c71-be0b-48d167015fad",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 43,
          "document_id": "1909.02764",
          "start_idx": 17421,
          "end_idx": 17831
        },
        "page_content": " layer. This setup is inspired by BIBREF38. We use a dropout rate of 0.3 in all layers and optimize with Adam BIBREF39 with a learning rate of $10^{-5}$ (These parameters are the same for all further experiments). We build on top of the Keras library with the TensorFlow backend. We consider this setup our baseline model. We train models on a variety of corpora, namely the common format published by BIBREF27",
        "type": "Document"
      },
      {
        "id": "ed8aaf94-c5e8-4cf0-9b3c-4a4e591733f5",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 39,
          "document_id": "1909.00542",
          "start_idx": 15059,
          "end_idx": 15516
        },
        "page_content": " the results and details of the runs submitted to BioASQ. The table uses ROUGE-SU4 Recall since this is the metric available at the time of writing this paper. However, note that, as explained in Section SECREF6, these results might differ from the final human evaluation results. Therefore we do not comment on the results, other than observing that the \u201cfirst $n$\u201d baseline produces the same results as the neural regressor. As mentioned in Section SECREF",
        "type": "Document"
      },
      {
        "id": "dc50a5cf-b755-41a0-a173-1b561db0ed05",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "96a4dc3a-bb09-48ad-97d1-78cae6bc4848",
        "metadata": {
          "vector_store_key": "1706.07179-1",
          "chunk_id": 18,
          "document_id": "1706.07179",
          "start_idx": 6896,
          "end_idx": 7300
        },
        "page_content": ". The baseline EntNet model was run for 10 times for each task BIBREF17 . The results are shown in Table 1 . The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the EntNet model BIBREF17 . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the EntNet model achieves 0% error on 7 of the tasks. We demonstrated an end-to-end trained",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the baseline models?",
    "llm_answer": "Model 1, a setup inspired by BIBREF38, using a dropout rate of 0.3, optimized with Adam, and a learning rate of 10<sup>-5</sup>.  A baseline model composed of word embedding, input encoder, alignment, aggregation, and prediction.\n",
    "context": [
      {
        "id": "97be5370-e788-41d2-9b17-3e7739266f06",
        "metadata": {
          "vector_store_key": "2001.11381-3",
          "chunk_id": 138,
          "document_id": "2001.11381",
          "start_idx": 31860,
          "end_idx": 32196
        },
        "page_content": "ados. Dicho lo anterior, el Modelo 1 podr\u00eda ser considerado como nuestro propio baseline. A continuaci\u00f3n presentamos un protocolo de evaluaci\u00f3n manual de los resultados obtenidos. El experimento consisti\u00f3 en la generaci\u00f3n de 15 frases por cada uno de los tres modelos propuestos. Para cada modelo, se consideraron tres queries: $Q=$ {AM",
        "type": "Document"
      },
      {
        "id": "344cdcd9-2892-4ed9-8902-26bffb76e164",
        "metadata": {
          "vector_store_key": "1905.11037-2",
          "chunk_id": 14,
          "document_id": "1905.11037",
          "start_idx": 5306,
          "end_idx": 5824
        },
        "page_content": "ered perceptron, convolutional neural networks and long short-term memory networks. We outline the essentials of each of these models, but will treat them as black boxes. In a related line, kaushik2018much discuss the need of providing rigorous baselines that help better understand the improvement coming from future and complex models, and also the need of not demanding architectural novelty when introducing new datasets. Although not done in this work, an alternative (but also natural) way to address the task is",
        "type": "Document"
      },
      {
        "id": "bc8ac242-0b5e-4c71-be0b-48d167015fad",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 43,
          "document_id": "1909.02764",
          "start_idx": 17421,
          "end_idx": 17831
        },
        "page_content": " layer. This setup is inspired by BIBREF38. We use a dropout rate of 0.3 in all layers and optimize with Adam BIBREF39 with a learning rate of $10^{-5}$ (These parameters are the same for all further experiments). We build on top of the Keras library with the TensorFlow backend. We consider this setup our baseline model. We train models on a variety of corpora, namely the common format published by BIBREF27",
        "type": "Document"
      },
      {
        "id": "34076701-f317-4195-aae3-c9349b7c88e8",
        "metadata": {
          "vector_store_key": "1709.05413-5",
          "chunk_id": 104,
          "document_id": "1709.05413",
          "start_idx": 41585,
          "end_idx": 41999
        },
        "page_content": " outperforms all non-sequential baselines, and plan to continue our exploration of other sequential models including Conditional Random Fields (CRF) BIBREF32 and Long Short-Term Memory (LSTM) BIBREF33 , as well as of dialogue modeling using different Markov Decision Process (MDP) BIBREF34 models such as the Partially-Observed MDP (POMDP) BIBREF35 . We establish that agents are more predictable than customers in",
        "type": "Document"
      },
      {
        "id": "05f513f6-29b2-46d8-8ba4-af84d6b0b34d",
        "metadata": {
          "vector_store_key": "1703.04617-0",
          "chunk_id": 10,
          "document_id": "1703.04617",
          "start_idx": 3849,
          "end_idx": 4374
        },
        "page_content": " questions share a basic model but still discriminate them when needed. Specifically, we are motivated by the ideas \"i-vector\" BIBREF18 in speech recognition, where neural network based adaptation is performed among different (groups) of speakers and we focused instead on different types of questions here. Our baseline model is composed of the following typical components: word embedding, input encoder, alignment, aggregation, and prediction. Below we discuss these components in more details. We concatenate embedding at",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much of MGNC-CNN out perform the baselines?",
    "llm_answer": "The provided text states that MGNC-CNN and MG-CNN outperform baseline methods, but does not give specific performance differences.\n",
    "context": [
      {
        "id": "199bdcbf-d0a7-4819-be40-2ad0384024c5",
        "metadata": {
          "vector_store_key": "1603.00968-0",
          "chunk_id": 35,
          "document_id": "1603.00968",
          "start_idx": 12591,
          "end_idx": 13006
        },
        "page_content": "IBREF11 for comparison; this represents the state-of-the-art on the three datasets other than TREC. We can see that MGNC-CNN and MG-CNN always outperform baseline methods (including C-CNN), and MGNC-CNN is usually better than MG-CNN. And on the Subj dataset, MG-CNN actually achieves slightly better results than BIBREF11 , with far less complexity and required training time (MGNC-CNN performs comparably, although",
        "type": "Document"
      },
      {
        "id": "88ea116d-239b-41c2-813f-464ab2c9c013",
        "metadata": {
          "vector_store_key": "1603.00968-0",
          "chunk_id": 36,
          "document_id": "1603.00968",
          "start_idx": 13006,
          "end_idx": 13420
        },
        "page_content": "11 , with far less complexity and required training time (MGNC-CNN performs comparably, although no better, here). On the TREC dataset, the best-ever accuracy we are aware of is 96.0% BIBREF21 , which falls within the range of the result of our MGNC-CNN model with three word embeddings. On the irony dataset, our model with three embeddings achieves 4% improvement (in terms of AUC) compared to the baseline model",
        "type": "Document"
      },
      {
        "id": "4a11e305-d945-481a-9fac-d1589d9805bf",
        "metadata": {
          "vector_store_key": "1603.00968-0",
          "chunk_id": 31,
          "document_id": "1603.00968",
          "start_idx": 11217,
          "end_idx": 11651
        },
        "page_content": " INLINEFORM1 on a validation set. For instantiations of MGNC-CNN in which we exploited two embeddings, we tuned both INLINEFORM2 , and INLINEFORM3 ; where we used three embedding sets, we tuned INLINEFORM4 and INLINEFORM5 . We used standard train/test splits for those datasets that had them. Otherwise, we performed 10-fold cross validation, creating nested development sets with which to tune hyperparameters. For all experiments we",
        "type": "Document"
      },
      {
        "id": "a24737a1-0cca-44dd-af66-1c4c45fe1397",
        "metadata": {
          "vector_store_key": "2002.06644-3",
          "chunk_id": 16,
          "document_id": "2002.06644",
          "start_idx": 6017,
          "end_idx": 6467
        },
        "page_content": " along with a recurrent dropout of $0.2$ in two 64 unit sized stacked BiLSTMs, using softmax activation layer as the final dense layer. tab:experimental-results shows the performance of different models on the WNC corpus evaluated on the following four metrics: Precision, Recall, F1, and Accuracy. Our proposed methodology, the use of finetuned optimized BERT based models, and BERT-based ensemble models outperform the baselines for all the metrics",
        "type": "Document"
      },
      {
        "id": "58a67b02-83ce-4b8f-9d11-44067be92302",
        "metadata": {
          "vector_store_key": "1910.07481-1",
          "chunk_id": 24,
          "document_id": "1910.07481",
          "start_idx": 9488,
          "end_idx": 9899
        },
        "page_content": " layers, $d_{ff} = 2048$ as the inner-layer dimensionality, $h = 8$ attention heads, $d_k = 64$ as queries and keys dimension and $Pdrop = 0.1$ as dropout probability. All experiments, including baselines, are run over 600k training steps with a batch size of approximately 3000 tokens. For all language pairs we trained a Baseline and a Document model. The Baseline is trained on a standard parallel corpus and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is increase in percentage of humor contained in headlines generated with TitleStylist method (w.r.t. baselines)?",
    "llm_answer": "Answer not found in the context.\n",
    "context": [
      {
        "id": "8bb67110-c2f1-42a2-87a5-5b8dab7b60d7",
        "metadata": {
          "vector_store_key": "2004.01980-3",
          "chunk_id": 8,
          "document_id": "2004.01980",
          "start_idx": 2990,
          "end_idx": 3543
        },
        "page_content": " without any supervised style-specific article-headline paired data. Through both automatic and human evaluation, we demonstrated that our proposed TitleStylist can generate relevant, fluent headlines with three styles (humor, romance, and clickbait), and they are even more attractive than human-written ones. Our model can flexibly incorporate multiple styles, thus efficiently and automatically providing humans with various creative headline options for references and inspiring them to think out of the box. Our work is related to summarization and",
        "type": "Document"
      },
      {
        "id": "c4f228c9-72ca-4241-aac8-3c1dcca4e3bd",
        "metadata": {
          "vector_store_key": "2004.01980-3",
          "chunk_id": 7,
          "document_id": "2004.01980",
          "start_idx": 2673,
          "end_idx": 3201
        },
        "page_content": " the model on three tasks: humorous, romantic, and click-baity headline generation. Both automatic and human evaluations show that TitleStylist can generate headlines with the desired styles that appeal more to human readers, as in Figure FIGREF2. The main contributions of our paper are listed below: To the best of our knowledge, it is the first research on the generation of attractive news headlines with styles without any supervised style-specific article-headline paired data. Through both automatic and human evaluation,",
        "type": "Document"
      },
      {
        "id": "d9d8c63b-9169-4812-bf23-89b34b2b41fc",
        "metadata": {
          "vector_store_key": "2004.01980-1",
          "chunk_id": 60,
          "document_id": "2004.01980",
          "start_idx": 20888,
          "end_idx": 21374
        },
        "page_content": "-weighting the original ones, which agrees with the fact that click-baity headlines are better at drawing readers' attention. To be noted, although we learned the \u201cClickbait\u201d style into our summarization system, we still made sure that we are generating relevant headlines instead of too exaggerated ones, which can be verified by our relevance scores. The human-annotated fluency scores in Table TABREF51 verified that our TitleStylist generated headlines are comparable or superior to",
        "type": "Document"
      },
      {
        "id": "721266d3-6145-4abe-879c-63671f2ad8c1",
        "metadata": {
          "vector_store_key": "2004.01980-3",
          "chunk_id": 59,
          "document_id": "2004.01980",
          "start_idx": 20574,
          "end_idx": 21076
        },
        "page_content": ") Our TitleStylist can generate more attractive headlines over the NHG and Multitask baselines for all three styles, demonstrating that adapting the model to these styles could improve the attraction and specialization of some parameters in the model for different styles can further enhance the attraction. (3) Adapting the model to the \u201cClickbait\u201d style could create the most attractive headlines, even out-weighting the original ones, which agrees with the fact that click-baity headlines are better",
        "type": "Document"
      },
      {
        "id": "4e211a14-b0b1-4505-83ea-e7a6475e2947",
        "metadata": {
          "vector_store_key": "2004.01980-3",
          "chunk_id": 5,
          "document_id": "2004.01980",
          "start_idx": 1921,
          "end_idx": 2391
        },
        "page_content": " of style in headline generation. We present a novel headline generation model, TitleStylist, to produce enticing titles with target styles including humorous, romantic, and click-baity. Our model leverages a multitasking framework to train both a summarization model on headline-article pairs, and a Denoising Autoencoder (DAE) on a style corpus. In particular, based on the transformer architecture BIBREF6, we use the style-dependent layer normalization and the style",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the languages they consider in this paper?",
    "llm_answer": "English, French, German, Spanish, Russian, Italian, Basque, Hungarian, Navajo, Turkish, Quechua, Arabic, Dutch,  Manipuri, Zulu, and potentially others.\n",
    "context": [
      {
        "id": "b0d78a46-0cfa-4367-a963-2a9a8d82c9a4",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 18,
          "document_id": "1910.04269",
          "start_idx": 6853,
          "end_idx": 7342
        },
        "page_content": " comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (",
        "type": "Document"
      },
      {
        "id": "e90f75a5-f326-4202-82c5-878736af8308",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 51,
          "document_id": "1910.04269",
          "start_idx": 19553,
          "end_idx": 20023
        },
        "page_content": ", the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Kat",
        "type": "Document"
      },
      {
        "id": "d16841ee-127f-473f-af88-1d373637d8c0",
        "metadata": {
          "vector_store_key": "1910.05456-8",
          "chunk_id": 5,
          "document_id": "1910.05456",
          "start_idx": 1974,
          "end_idx": 2423
        },
        "page_content": " on the L1, i.e., the \"native language\", in neural network models. To this goal, we select a diverse set of eight source languages from different language families \u2013 Basque, French, German, Hungarian, Italian, Navajo, Turkish, and Quechua \u2013 and three target languages \u2013 English, Spanish and Zulu. We pretrain a neural sequence-to-sequence architecture on each of the source languages and then fine-tune the resulting models on small datasets in each",
        "type": "Document"
      },
      {
        "id": "7b1c09b6-d8db-4d35-8a92-0acba47cc1bc",
        "metadata": {
          "vector_store_key": "1909.13362-4",
          "chunk_id": 44,
          "document_id": "1909.13362",
          "start_idx": 15624,
          "end_idx": 16095
        },
        "page_content": " language groupings within families. We selected six evaluation languages: English, Dutch, Italian, French, Basque, and Manipuri. These represent two language families (Indo-European, Sino-Tibetan), a language isolate thought to be unrelated to any existing language (Basque), and two different subfamilies within the Indo-European family (West Germanic, Romance). The primary constraint was the availability of syllabified datasets for training and testing. Table TABREF",
        "type": "Document"
      },
      {
        "id": "77ad9c8e-30eb-484f-9026-bfde7c947210",
        "metadata": {
          "vector_store_key": "1809.04686-0",
          "chunk_id": 25,
          "document_id": "1809.04686",
          "start_idx": 9144,
          "end_idx": 9613
        },
        "page_content": "40 chose a subset of the SNLI test set (1332 sentences) and professionally translated it into four major languages - Arabic, French, Russian, and Spanish. We use the French test set for evaluation in Section \"Zero-Shot Classification Results\" and \"Analyses\" . Here, we first describe the model and training details of the base multilingual NMT model whose encoder is reused in all other tasks. Then we provide details about the task-specific classifiers. For each task,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Did they experiment with tasks other than word problems in math?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "978205e9-1074-4c88-b60f-191c7bf2d939",
        "metadata": {
          "vector_store_key": "1905.13497-2",
          "chunk_id": 2,
          "document_id": "1905.13497",
          "start_idx": 776,
          "end_idx": 1212
        },
        "page_content": " reasoning. Particularly, it is not clear whether BERT shed light on solving tasks such as the Pronoun Disambiguation Problem (PDP) and Winograd Schema Challenge (WSC). These tasks have been proposed as potential alternatives to the Turing Test, because they are formulated to be robust to statistics of word co-occurrence BIBREF4 . Below is a popular example from the binary-choice pronoun coreference problem BIBREF5 of WSC: Sentence:",
        "type": "Document"
      },
      {
        "id": "4596c59c-c395-4f7a-880d-81a3eb81efcf",
        "metadata": {
          "vector_store_key": "1910.00458-0",
          "chunk_id": 73,
          "document_id": "1910.00458",
          "start_idx": 25409,
          "end_idx": 25967
        },
        "page_content": " do math? To investigate this question, we sampled some arithmetic questions that are correctly predicted by our model, made small alterations to the passage or question, and then checked whether our model can still make correct choices. We found our model is very fragile to these minor alterations, implicating that the model is actually not that good at arithmetic problems. We provided one interesting example in the Section 3 of the Supplementary Material. There are increasing interests in machine reading comprehension (MRC) for question answering (QA",
        "type": "Document"
      },
      {
        "id": "413c231b-c771-4c94-97a8-b20b290a953c",
        "metadata": {
          "vector_store_key": "1906.06448-6",
          "chunk_id": 32,
          "document_id": "1906.06448",
          "start_idx": 11534,
          "end_idx": 12014
        },
        "page_content": " the premise and/or the hypothesis is ungrammatical or does not make sense. Workers were paid US$0.04 for each question, and each question was assigned to three workers. To collect high-quality annotation results, we imposed ten test questions on each worker, and removed workers who gave more than three wrong answers. We also set the minimum time it should take to complete each question to 200 seconds. 1,237 workers contributed to this task, and we annotated gold labels of 15",
        "type": "Document"
      },
      {
        "id": "0e92704b-aa27-4faf-abf7-cfd23dbdf35a",
        "metadata": {
          "vector_store_key": "1807.03367-4",
          "chunk_id": 89,
          "document_id": "1807.03367",
          "start_idx": 33979,
          "end_idx": 34422
        },
        "page_content": "oriented dialogue research and situated navigation. Here, we describe related previous work in these areas. paragraph4 0.1ex plus0.1ex minus.1ex-1em Related tasks There has been a long line of work involving related tasks. Early work on task-oriented dialogue dates back to the early 90s with the introduction of the Map Task BIBREF11 and Maze Game BIBREF25 corpora. Recent efforts have led to larger-scale goal-oriented dialogue datasets, for",
        "type": "Document"
      },
      {
        "id": "777daf74-0a64-4f75-825f-0b5efa80f4aa",
        "metadata": {
          "vector_store_key": "1605.08675-8",
          "chunk_id": 1,
          "document_id": "1605.08675",
          "start_idx": 494,
          "end_idx": 1010
        },
        "page_content": " in Natural Language Processing (NLP) community as it provides a way to employ numerous NLP tools in an exploitable end-user system. It has resulted in valuable contributions within TREC competitions BIBREF1 and, quite recently, in a system called IBM Watson BIBREF2 , successfully competing with humans in the task. However, the problem remains far from solved. Firstly, solutions designed for English are not always easily transferable to other languages with more complex syntax rules and less resources available",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What evaluation metrics are used?",
    "llm_answer": "Task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, F1-score, BPRA, Belief Per-Response Accuracy,  average rank or percentage of best-ranked questions.\n",
    "context": [
      {
        "id": "4786191e-fed1-4406-89b6-7b8835719e5f",
        "metadata": {
          "vector_store_key": "1709.10217-2",
          "chunk_id": 18,
          "document_id": "1709.10217",
          "start_idx": 6394,
          "end_idx": 6850
        },
        "page_content": " evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the number of total tasks. User satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied, neutral, satisfied and very satisfied, respectively. Response fluency: There are three scores -1, 0, 1, which indicate nonfluency, neutral, fluency. Number of dialogue turns: The number of utterances in a task",
        "type": "Document"
      },
      {
        "id": "175d0a0b-77ff-450c-8f37-24f131b845e7",
        "metadata": {
          "vector_store_key": "1905.08949-9",
          "chunk_id": 28,
          "document_id": "1905.08949",
          "start_idx": 10602,
          "end_idx": 11043
        },
        "page_content": " they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks. As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE B",
        "type": "Document"
      },
      {
        "id": "dc58289f-90f2-4f31-9a31-296d00bb01a9",
        "metadata": {
          "vector_store_key": "1709.10217-2",
          "chunk_id": 11,
          "document_id": "1709.10217",
          "start_idx": 4141,
          "end_idx": 4657
        },
        "page_content": ", in which only the released data can be used for training and developing. The other is an open evaluation that allow to collect external data for training and developing. For task 1, we use F1-score as evaluation metric. For the task-oriented dialogue systems, the best way for evaluation is to use the online human-computer dialogue. After finishing an online human-computer dialogue with a dialogue system, the human then manually evaluate the system by using the metrics of user satisfaction degree, dialogue flu",
        "type": "Document"
      },
      {
        "id": "44492013-ea6a-4d0d-9984-8b621a88cebb",
        "metadata": {
          "vector_store_key": "1906.03538-4",
          "chunk_id": 45,
          "document_id": "1906.03538",
          "start_idx": 17710,
          "end_idx": 18208
        },
        "page_content": "6 and INLINEFORM7 , respectively. The metrics are averaged across all the perspectives INLINEFORM8 participating in the test set. The goal is to get estimates of the overall performance of the systems. Instead of creating a complex measure that would take all the aspects into account, we approximate the overall performance by multiplying the disjoint measures in INLINEFORM0 , INLINEFORM1 and INLINEFORM2 . While this gives an estimate on the overall quality, it ignores the pipeline structure of",
        "type": "Document"
      },
      {
        "id": "140ad6e1-f0ef-4e66-a00e-12f677b6f352",
        "metadata": {
          "vector_store_key": "1909.09484-3",
          "chunk_id": 46,
          "document_id": "1909.09484",
          "start_idx": 15874,
          "end_idx": 16378
        },
        "page_content": " of the dialogue state tracker, it aims to show the impact of the dialogue state tracker on the dialogue policy maker; (2) the accuracy of dialogue policy maker, it aims to explain the performance of different methods of constructing dialogue policy; (3) the quality of the final response, it aims to explain the impact of the dialogue policy on the final dialogue response. The evaluation metrics are listed as follows: BPRA: Belief Per-Response Accuracy (BPRA) tests the ability to generate the correct",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What kind of Youtube video transcripts did they use?",
    "llm_answer": "Anonymized, transcribed Voice Search data.\n",
    "context": [
      {
        "id": "e982b9fb-76fb-431e-b201-39f0e044934d",
        "metadata": {
          "vector_store_key": "1911.13087-2",
          "chunk_id": 6,
          "document_id": "1911.13087",
          "start_idx": 2266,
          "end_idx": 2709
        },
        "page_content": " includes three lines to indicate the possible pauses at the beginning and end of the sentences and also after each word. This file includes the list of files in which the narrated sentences have been recorded. The recorded files are in wav formats. However, in the file IDs, the extension is omitted. A sample of the file content is given below. The test directory is the directory in which the files are located. test/T1-1-50-01 test/T1-1-50",
        "type": "Document"
      },
      {
        "id": "c53ef79d-56fd-4cce-b700-5570dc3958a0",
        "metadata": {
          "vector_store_key": "1911.13087-0",
          "chunk_id": 12,
          "document_id": "1911.13087",
          "start_idx": 3610,
          "end_idx": 4109
        },
        "page_content": " school in the Kurdistan Region of Iraq. The reason that we have taken only 200 sentences is to have a smaller dictionary and also to increase the repetition of each word in the narrated speech. We transformed the corpus sentences, which are in Persian-Arabic script, into the format which complies with the suggested phones for the related Sorani letters (see Section SECREF6). Two thousand narration files were created. We used Audacity to record the narrations. We used a normal laptop in a quiet",
        "type": "Document"
      },
      {
        "id": "6bb58866-d827-4810-9706-e5dea8e32f98",
        "metadata": {
          "vector_store_key": "1909.06522-1",
          "chunk_id": 24,
          "document_id": "1909.06522",
          "start_idx": 8980,
          "end_idx": 9471
        },
        "page_content": "ated 10-second sound clips from YouTube videos BIBREF28. Note that in our video datasets, video lengths vary between 10 seconds and 5 minutes, with an average duration of about 2 minutes. Rather than constantly repeating the 10-second sound clip to match the original minute-long audio, we superpose each sound clip on the short utterances via audio segmentation. Specifically, we first use an initial bootstrap model to align each original long audio, and segment each audio into around 10-",
        "type": "Document"
      },
      {
        "id": "1523b4ed-4fb2-4b9e-8647-9a7bc686f31b",
        "metadata": {
          "vector_store_key": "2002.11268-4",
          "chunk_id": 37,
          "document_id": "2002.11268",
          "start_idx": 10875,
          "end_idx": 11324
        },
        "page_content": "K, 1M and 21M utterance-level {audio, transcript} pairs taken from anonymized, transcribed Voice Search data. These fine-tuning sets roughly correspond to 10 hours, 100 hours, 1000 hours and 21,000 hours of audio, respectively. The following data sources were used to choose scaling factors and/or evaluate the final model performance. Source-domain Eval Set (YouTube). The in-domain performance of the YouTube-trained RNN-T baseline was measured on",
        "type": "Document"
      },
      {
        "id": "d322d4dc-4bf2-4c3e-9106-92b243c71e31",
        "metadata": {
          "vector_store_key": "1604.05781-6",
          "chunk_id": 11,
          "document_id": "1604.05781",
          "start_idx": 4432,
          "end_idx": 4838
        },
        "page_content": " BIBREF29 . A tweet will be referred to as a `document' for the rest of this work. All document text was processed the same way. Punctuation, XML characters, and hyperlinks were removed, as were Twitter-specific \u201cat-mentions\u201d and \u201chashtags\u201d (see also the Appendix). There is useful information here, but it is either not natural language text, or it is Twitter-specific, or both. Documents were broken into",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What makes it a more reliable metric?",
    "llm_answer": "It has an interpretable meaning: the expected error made by a given metric as compared to the human rating.\n",
    "context": [
      {
        "id": "a3945c5a-0d01-4716-a840-3c14bd750a67",
        "metadata": {
          "vector_store_key": "1809.08731-9",
          "chunk_id": 37,
          "document_id": "1809.08731",
          "start_idx": 12336,
          "end_idx": 12724
        },
        "page_content": "$ , it is a non-symmetric quasi-metric. We apply it with $Y$ being the human ratings. An additional advantage as compared to Pearson is that it has an interpretable meaning: the expected error made by a given metric as compared to the human rating. As shown in Table 3 , WordSLOR and WPSLOR correlate best with human judgments: WordSLOR (respectively WPSLOR) has a $0.025$ (respectively $",
        "type": "Document"
      },
      {
        "id": "e3c452f8-81d9-4830-b756-b0d433c97917",
        "metadata": {
          "vector_store_key": "2001.06354-4",
          "chunk_id": 9,
          "document_id": "2001.06354",
          "start_idx": 3605,
          "end_idx": 4142
        },
        "page_content": " model's generalization ability (because it allows multiple similar answers), while the other metrics measure a model's preciseness, we interpret the results of these above experiments to mean that a model with more history information tends to predict correct answers by memorizing keywords or patterns in the history while a model with less history information (i.e., the image-only model) is better at generalization by avoiding relying on such exact-match extracted information. We think that an ideal model should have more balanced",
        "type": "Document"
      },
      {
        "id": "e95fc921-173e-4001-ae90-f8debc610f09",
        "metadata": {
          "vector_store_key": "2001.09332-3",
          "chunk_id": 35,
          "document_id": "2001.09332",
          "start_idx": 12665,
          "end_idx": 13168
        },
        "page_content": " related to one or more words absent from the vocabulary). As it can be seen, regardless of the metric used, our model has significantly better results than the other two models, both overall and within the two macro-areas. Furthermore, the other two models seem to be more subject to the metric used, perhaps due to a stabilization not yet reached for the few training epochs. For a complete comparison, both models were also tested considering only the subset of the analogies in common with our model",
        "type": "Document"
      },
      {
        "id": "cd800d9b-a4e2-472b-a718-4d5563ba413d",
        "metadata": {
          "vector_store_key": "2002.02492-9",
          "chunk_id": 93,
          "document_id": "2002.02492",
          "start_idx": 29289,
          "end_idx": 29596
        },
        "page_content": " perplexity. When $\\epsilon $ is too large ($\\epsilon =10^{-2}$), perplexity degrades. When $\\epsilon $ is too small ($\\epsilon =10^{-4}$), the lower-bound grows slowly, so $\\left<\\text{eos}\\right>$ is not guaranteed to be top-ranked within $L$ steps, and the metrics resemble the baseline's. An $\\epsilon $",
        "type": "Document"
      },
      {
        "id": "41b247c9-4efd-4c72-a969-d19320c333f1",
        "metadata": {
          "vector_store_key": "1909.00542-3",
          "chunk_id": 37,
          "document_id": "1909.00542",
          "start_idx": 14353,
          "end_idx": 14835
        },
        "page_content": ". Recall shows poor correlation, and virtually no correlation when using the revised Kendall measure. For reporting the evaluation of results, it will be therefore more useful to use precision or F1. However, given the small difference between precision and F1, and given that precision may favour short summaries when used as a function to optimise in a machine learning setting (e.g. using reinforcement learning), it may be best to use F1 as the metric to optimise. Fig. FIGREF40",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much in experiments is performance improved for models trained with generated adversarial examples?",
    "llm_answer": "The performance of all target models raises significantly after adversarial training.\n",
    "context": [
      {
        "id": "83f03968-2b86-4d6b-8604-15b441459a3d",
        "metadata": {
          "vector_store_key": "1909.02560-3",
          "chunk_id": 65,
          "document_id": "1909.02560",
          "start_idx": 24644,
          "end_idx": 25212
        },
        "page_content": "ially trained models, as shown in Table TABREF18. After adversarial training, the performance of all the target models raises significantly, while that on the original examples remain comparable. Note that since the focus of this paper is on model robustness which can hardly be reflected in original data, we do not expect performance improvement on original data. The results demonstrate that adversarial training with our adversarial examples can significantly improve the robustness we focus on without remarkably hurting the performance on original data. Moreover",
        "type": "Document"
      },
      {
        "id": "aa046e6b-2bd2-47d4-86b9-d653f7db5f1f",
        "metadata": {
          "vector_store_key": "1909.02560-3",
          "chunk_id": 75,
          "document_id": "1909.02560",
          "start_idx": 29007,
          "end_idx": 29299
        },
        "page_content": " are much less affected and the modified sentences retain a good grammarticality. We also show that model robustness can be improved using adversarial training with our adversarial examples. Moreover, our adversarial examples can foster future research for further improving model robustness.",
        "type": "Document"
      },
      {
        "id": "ad765179-d00f-4758-96ba-2f3c23acc937",
        "metadata": {
          "vector_store_key": "1909.02560-3",
          "chunk_id": 58,
          "document_id": "1909.02560",
          "start_idx": 21491,
          "end_idx": 22071
        },
        "page_content": " robustness issues. Moreover, we present some generated adversarial examples in the appendix. We notice that the original models are more vulnerable to unmatched adversarial examples, because there are generally more replaceable position choices during the generation. Nevertheless, the results of the matched case are also sufficiently strong to reveal the robustness issues. We do not quantitatively compare the performance drop of the target models on the adversarial examples with previous work, because we generate a new type of adversarial examples that previous methods are",
        "type": "Document"
      },
      {
        "id": "8eeb9bb9-3e16-4744-a875-11a0a2ca61d0",
        "metadata": {
          "vector_store_key": "1909.02560-3",
          "chunk_id": 59,
          "document_id": "1909.02560",
          "start_idx": 22071,
          "end_idx": 22669
        },
        "page_content": "arial examples with previous work, because we generate a new type of adversarial examples that previous methods are not capable of. We have different experiment settings, including original example sampling and constraints on adversarial modifications, which are tailored to the robustness issues we study. Performance drop on different kinds of adversarial examples with little overlap is not comparable, and thus surpassing other adversarial examples on model performance drop is unnecessary and irrelevant to support our contributions. Therefore, such comparisons are not included in this paper.",
        "type": "Document"
      },
      {
        "id": "e85639f1-da25-4102-89f5-b789d2a5d015",
        "metadata": {
          "vector_store_key": "1909.02560-3",
          "chunk_id": 63,
          "document_id": "1909.02560",
          "start_idx": 23850,
          "end_idx": 24336
        },
        "page_content": "arial examples is also small, suggesting that most adversarial examples retain a good grammaticality. This verifies the validity of our adversarial examples. Adversarial training can often improve model robustness BIBREF25, BIBREF27. We also fine-tune the target models using adversarial training. At each training step, we train the model with a batch of original examples along with adversarial examples with balanced labels. The adversarial examples account for around 10% in a batch",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the delta-softmax calculated?",
    "llm_answer": "The provided text does not describe how a delta-softmax is calculated.\n",
    "context": [
      {
        "id": "9eb2656f-b1c0-47ff-a6c4-448bae790f55",
        "metadata": {
          "vector_store_key": "1908.06606-3",
          "chunk_id": 40,
          "document_id": "1908.06606",
          "start_idx": 14117,
          "end_idx": 14461
        },
        "page_content": "_s$ denotes the length of sequence. Then we permute the two dimensions for softmax calculation. The calculation process of loss function can be defined as followed. where $O_s = softmax(permute(H_f)_0)$ denotes the probability score of each word to be the start word and similarly $O_e = softmax(permute(H_f)_1)$ denotes the end. $y_s$ and $y_e",
        "type": "Document"
      },
      {
        "id": "beaa3992-d3e6-4cce-b8b7-5b64dc9ba44c",
        "metadata": {
          "vector_store_key": "1909.00154-4",
          "chunk_id": 34,
          "document_id": "1909.00154",
          "start_idx": 12833,
          "end_idx": 13177
        },
        "page_content": ") for the softmax layer, so $B_c$ is simply the coefficients (row) vector for output class (alternative) $c$, and $\\alpha _c$ is the corresponding intercept. The typical loss function used in such models is called the categorical cross entropy: Where $\\delta _{i}$ is the kronecker delta ($\\delta _{true}=1; \\delta _{false}=0$), and $\\mathcal {",
        "type": "Document"
      },
      {
        "id": "713230a4-17f6-4798-b153-a056a1c518cb",
        "metadata": {
          "vector_store_key": "2001.05493-5",
          "chunk_id": 47,
          "document_id": "2001.05493",
          "start_idx": 16703,
          "end_idx": 16976
        },
        "page_content": "y_{i1}^*$ be the softmax probabilities, specifically for class label $k$ is given as: where $K$ is the number of classes, $W_{dpcnn}$ and $b_{dpcnn}$ are the weight matrix and bias respectively. Given a sequence $s_i = [x_{1}, x_{2}, x_{3},....x_{n}]$ where $x_{j} \\in R^d$",
        "type": "Document"
      },
      {
        "id": "b81cd515-f8cc-4a97-af68-1b322a5d37a5",
        "metadata": {
          "vector_store_key": "1909.13362-1",
          "chunk_id": 37,
          "document_id": "1909.13362",
          "start_idx": 13130,
          "end_idx": 13483
        },
        "page_content": " for each $o_i$: The softmax normalizes each $o_i$ to a probability distribution over the two discrete class labels. We can then model $p(y|o)$ by multiplying the maximum of each $s_i$ together: When using the softmax, $p(y|o)$ is calculated under the limiting assumption that each $o_i$ is independent. To more accurately model $p(y|o)$, we replace the",
        "type": "Document"
      },
      {
        "id": "2505bd5c-29e6-4317-bf46-c1cebfd81910",
        "metadata": {
          "vector_store_key": "2002.00876-1",
          "chunk_id": 20,
          "document_id": "2002.00876",
          "start_idx": 7430,
          "end_idx": 7804
        },
        "page_content": " now describe the technical approach underlying the library. To establish notation first consider the implementation of a categorical distribution, Cat($\\ell $), with one-hot categories $z$ with $z_i = 1$ from a set $\\cal Z$ and probabilities given by the softmax, Define the log-partition as $A(\\ell ) = \\mathrm {LSE}(\\ell )$, i.e. log of the denominator, where $\\mathrm {L",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which two datasets does the resource come from?",
    "llm_answer": "REF0 and ASQ\n",
    "context": [
      {
        "id": "e2c0d574-ff23-4b3a-95cc-9f3f61f41afc",
        "metadata": {
          "vector_store_key": "1901.02257-3",
          "chunk_id": 15,
          "document_id": "1901.02257",
          "start_idx": 6056,
          "end_idx": 6539
        },
        "page_content": "REF0 dataset concerns answering the question which requires using commonsense knowledge. including Wikipedia articles, examinations, narrative stories, news articles. Answering questions in these datasets. Meanwhile, the question types and answer types vary differently. The answer type multiple choice, span-answer, exact match Many architectures on MRC follow the process of representation, attention, fusion, and aggregation BIBREF12 , BIBREF2 , BIBREF13 , BIBREF14 , BIBREF15 , B",
        "type": "Document"
      },
      {
        "id": "401df3ea-241b-476d-97f4-1276dc131769",
        "metadata": {
          "vector_store_key": "1701.00185-9",
          "chunk_id": 99,
          "document_id": "1701.00185",
          "start_idx": 35418,
          "end_idx": 35678
        },
        "page_content": "ASQ for making the datasets available. This work is supported by the National Natural Science Foundation of China (No. 61602479, No. 61303172, No. 61403385) and the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDB02070005).",
        "type": "Document"
      },
      {
        "id": "1c380a0c-1037-4628-8ada-168d9c8247bc",
        "metadata": {
          "vector_store_key": "1707.03904-9",
          "chunk_id": 13,
          "document_id": "1707.03904",
          "start_idx": 5304,
          "end_idx": 5850
        },
        "page_content": " subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short. We plan to release these annotations along with the datasets, and our retrieved documents for each question. Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated",
        "type": "Document"
      },
      {
        "id": "e3405ea7-1861-4bec-ac73-1551f818ad88",
        "metadata": {
          "vector_store_key": "1711.11221-1",
          "chunk_id": 20,
          "document_id": "1711.11221",
          "start_idx": 6905,
          "end_idx": 7300
        },
        "page_content": " in the target-side vocabulary of RNNSearch*. In order to build the topic cache, we first use an off-the-shelf LDA topic tool to learn topic distributions of source- and target-side documents separately. Then we estimate a topic projection distribution over all target-side topics $p(z_t|z_s)$ for each source topic $z_s$ by collecting events and accumulating counts of $(z_s, z_t)$ from aligned",
        "type": "Document"
      },
      {
        "id": "e2f6e78c-bf7d-46ad-bd51-435c8758190a",
        "metadata": {
          "vector_store_key": "1707.03904-2",
          "chunk_id": 39,
          "document_id": "1707.03904",
          "start_idx": 14370,
          "end_idx": 14849
        },
        "page_content": "\ufffdopen-book\u201d setting, where they had access to a search engine over the short pseudo-documents extracted for each dataset (as described in Section \"Context Retrieval\" ). We decided to use short pseudo-documents for this exercise to reduce the burden of reading on the volunteers, though we note that the long pseudo-documents have greater coverage of answers. We also asked the volunteers to provide annotations to categorize the type of each question they were asked, and a label",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the size of the second dataset?",
    "llm_answer": "141 symptom-prescription pairs\n",
    "context": [
      {
        "id": "a2224590-41a6-4a47-8caa-583d1ca40539",
        "metadata": {
          "vector_store_key": "1801.09030-3",
          "chunk_id": 49,
          "document_id": "1801.09030",
          "start_idx": 17043,
          "end_idx": 17552
        },
        "page_content": " lower quality, as this dataset was parsed with simple rules, which may not cover all exceptions. The second test dataset has been proofread and all of the prescriptions are the most classical and influential ones in the history. So the quality is much better than the first one. However, the number of the cases is limited. There are 141 symptom-prescription pairs in the second dataset. Thus we use two test sets to do evaluation to take the advantages of both data magnitude and quality. In our experiments",
        "type": "Document"
      },
      {
        "id": "04fe1abe-a675-484e-963d-9d80c75d505e",
        "metadata": {
          "vector_store_key": "1908.10449-2",
          "chunk_id": 66,
          "document_id": "1908.10449",
          "start_idx": 22880,
          "end_idx": 23381
        },
        "page_content": " hard time learning in this setting. As shown in Table TABREF16, both datasets have a vocabulary size of more than 100k. This is much larger than in the other two settings, where on average the length of questions is around 10. This suggests that the methods with better sample efficiency are needed to act in more realistic problem settings with huge action spaces. Experiments also show that a larger memory slot size always helps. Intuitively, with a memory mechanism (either implicit or explicit),",
        "type": "Document"
      },
      {
        "id": "844aa3d6-e60c-46ad-bd37-977b71d9e959",
        "metadata": {
          "vector_store_key": "1707.03904-4",
          "chunk_id": 34,
          "document_id": "1707.03904",
          "start_idx": 12758,
          "end_idx": 13269
        },
        "page_content": " to evaluate the performance of the reading system independently from the search system, while the full set allows us to evaluate the performance of Quasar as a whole. We also split the full set into training, validation and test sets. The final size of each data subset after all discards is listed in Table 1 . Evaluation is straightforward on Quasar-S since each answer comes from a fixed output vocabulary of entities, and we report the average accuracy of predictions as the evaluation metric. For Quasar-T",
        "type": "Document"
      },
      {
        "id": "84720de1-c988-45c6-bcbe-7a0435cdf6fb",
        "metadata": {
          "vector_store_key": "1706.08032-0",
          "chunk_id": 38,
          "document_id": "1706.08032",
          "start_idx": 14264,
          "end_idx": 14661
        },
        "page_content": " datasets in Table III. for all datasets, the filter window size ( INLINEFORM0 ) is 7 with 6 feature maps each for the first wide convolution layer, the second wide convolution layer has a filter window size of 5 with 14 feature maps each. Dropout rate ( INLINEFORM1 ) is 0.5, INLINEFORM2 constraint, learning rate is 0.1 and momentum of 0.9. Mini-batch size for STS Corpus is 100 and others are 4",
        "type": "Document"
      },
      {
        "id": "49dd09c0-f752-4a00-bc37-38c26e5e9acc",
        "metadata": {
          "vector_store_key": "1709.05404-5",
          "chunk_id": 48,
          "document_id": "1709.05404",
          "start_idx": 17579,
          "end_idx": 18022
        },
        "page_content": "-validation setup) for each dataset while holding the number of posts per class constant. Figure FIGREF26 shows the performance of each of the Gen, RQ, and Hyp datasets at intervals of 100 posts per class (up to the maximum size of 582 posts per class for Hyp, and 851 posts per class for RQ). From the graph, we can see that as a general trend, the datasets benefit from larger dataset sizes. Interestingly, the results for the RQ dataset are",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How large is the first dataset?",
    "llm_answer": "The first dataset has 287,226 documents.\n",
    "context": [
      {
        "id": "04fe1abe-a675-484e-963d-9d80c75d505e",
        "metadata": {
          "vector_store_key": "1908.10449-2",
          "chunk_id": 66,
          "document_id": "1908.10449",
          "start_idx": 22880,
          "end_idx": 23381
        },
        "page_content": " hard time learning in this setting. As shown in Table TABREF16, both datasets have a vocabulary size of more than 100k. This is much larger than in the other two settings, where on average the length of questions is around 10. This suggests that the methods with better sample efficiency are needed to act in more realistic problem settings with huge action spaces. Experiments also show that a larger memory slot size always helps. Intuitively, with a memory mechanism (either implicit or explicit),",
        "type": "Document"
      },
      {
        "id": "0723454a-4062-463e-972a-81a58c6a504e",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "49dd09c0-f752-4a00-bc37-38c26e5e9acc",
        "metadata": {
          "vector_store_key": "1709.05404-5",
          "chunk_id": 48,
          "document_id": "1709.05404",
          "start_idx": 17579,
          "end_idx": 18022
        },
        "page_content": "-validation setup) for each dataset while holding the number of posts per class constant. Figure FIGREF26 shows the performance of each of the Gen, RQ, and Hyp datasets at intervals of 100 posts per class (up to the maximum size of 582 posts per class for Hyp, and 851 posts per class for RQ). From the graph, we can see that as a general trend, the datasets benefit from larger dataset sizes. Interestingly, the results for the RQ dataset are",
        "type": "Document"
      },
      {
        "id": "9270562a-8e53-4e1d-b760-74d8fd0bc872",
        "metadata": {
          "vector_store_key": "1905.06566-7",
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "844aa3d6-e60c-46ad-bd37-977b71d9e959",
        "metadata": {
          "vector_store_key": "1707.03904-4",
          "chunk_id": 34,
          "document_id": "1707.03904",
          "start_idx": 12758,
          "end_idx": 13269
        },
        "page_content": " to evaluate the performance of the reading system independently from the search system, while the full set allows us to evaluate the performance of Quasar as a whole. We also split the full set into training, validation and test sets. The final size of each data subset after all discards is listed in Table 1 . Evaluation is straightforward on Quasar-S since each answer comes from a fixed output vocabulary of entities, and we report the average accuracy of predictions as the evaluation metric. For Quasar-T",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Who was the top-scoring team?",
    "llm_answer": "AM\n",
    "context": [
      {
        "id": "c1853e1d-fb24-41d5-8aa1-8b885d145b0d",
        "metadata": {
          "vector_store_key": "2001.11381-5",
          "chunk_id": 134,
          "document_id": "2001.11381",
          "start_idx": 30919,
          "end_idx": 31185
        },
        "page_content": "imizaci\u00f3n-maximizaci\u00f3n) para calcular un score $S_i$, seg\u00fan la ecuaci\u00f3n (DISPLAY_FORM36): Mientras m\u00e1s elevado sea el valor $S_i$, mejor obedece a nuestros objetivos: acercarse al $query$ y alejarse de la sem\u00e1ntica original. Finalmente ordenamos en forma decreciente",
        "type": "Document"
      },
      {
        "id": "82a7cb52-516a-44f9-90c9-37df4a0add6e",
        "metadata": {
          "vector_store_key": "1707.03764-1",
          "chunk_id": 37,
          "document_id": "1707.03764",
          "start_idx": 14249,
          "end_idx": 14661
        },
        "page_content": "AM came first in the shared task, with a score of 0.8253 for gender 0.9184 for variety, a joint score of 0.8361 and an average score of 0.8599 (final rankings were taken from this average score BIBREF0 ). For the global scores, all languages are combined. We present finer-grained scores showing the breakdown per language in Table TABREF24 . We compare our gender and variety accuracies against the LDR-baseline",
        "type": "Document"
      },
      {
        "id": "db0aabc6-5151-4ae0-aa1c-8cb8ee824378",
        "metadata": {
          "vector_store_key": "1704.05572-3",
          "chunk_id": 14,
          "document_id": "1704.05572",
          "start_idx": 4739,
          "end_idx": 5005
        },
        "page_content": "it {tf-idf}(t, q)=\\sum _{x \\in t\\cap q} idf(x)\n$   where $N$ is the number of tuples in the KB and $n_x$ are the number of tuples containing $x$ . We normalize the tf-idf score by the number of tokens in $t$ and $q$ . We finally take the 50 top-scoring tuples $T_{qa",
        "type": "Document"
      },
      {
        "id": "de125043-7319-4e71-bfa7-85b046350ef9",
        "metadata": {
          "vector_store_key": "1905.00563-1",
          "chunk_id": 94,
          "document_id": "1905.00563",
          "start_idx": 29074,
          "end_idx": 29259
        },
        "page_content": "1-) (Hs + (1-) r',or',o)-1 r',o) r,o and then calculate the change in the score as: (s,r,o)-(s, r, o)=  s,r.(-) +(-).r,o =  s,r ((1-) (Ho + (1-) s,r's,r')-1 s,r')+  ((1-) (Hs + (1-) r',",
        "type": "Document"
      },
      {
        "id": "ea9c95b3-7b29-4410-9c6d-addc1061e51a",
        "metadata": {
          "vector_store_key": "1601.06068-5",
          "chunk_id": 49,
          "document_id": "1601.06068",
          "start_idx": 16772,
          "end_idx": 17063
        },
        "page_content": " the knowledge base). Following reddylargescale2014, we use beam search to find the highest scoring tuple of paraphrase, ungrounded and grounded graphs $(\\hat{p}, \\hat{u}, \\hat{g})$ under the model $\\theta \\in \\mathbb {R}^n$ : $\n({\\hat{p},\\hat{u},\\hat{g}}) = \\operatornamewithlimits{arg\\,max",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Why is big data not appropriate for this task?",
    "llm_answer": "Big data is not always available, particularly in computational social science NLP applications where data of interest is often not from large-scale sources.  Even very large corpora have idiosyncrasies that can make their embeddings irrelevant for a particular analysis.\n",
    "context": [
      {
        "id": "31dfb04c-2cac-4649-b621-18f81bc14ddc",
        "metadata": {
          "vector_store_key": "1705.07368-0",
          "chunk_id": 4,
          "document_id": "1705.07368",
          "start_idx": 1597,
          "end_idx": 2093
        },
        "page_content": " I offer a somewhat contrarian perspective to the currently prevailing trend of big data optimism, as exemplified by the work of BIBREF0 , BIBREF1 , BIBREF3 , and others, who argue that massive datasets are sufficient to allow language models to automatically resolve many challenging NLP tasks. Note that \u201cbig\u201d datasets are not always available, particularly in computational social science NLP applications, where the data of interest are often not obtained from large scale sources such as the",
        "type": "Document"
      },
      {
        "id": "be492148-365e-4e9e-a520-dcc9cb951038",
        "metadata": {
          "vector_store_key": "1705.07368-0",
          "chunk_id": 3,
          "document_id": "1705.07368",
          "start_idx": 1280,
          "end_idx": 1811
        },
        "page_content": ". Word embeddings have risen in popularity for NLP applications due to the success of models designed specifically for the big data setting. In particular, BIBREF0 , BIBREF1 showed that very simple word embedding models with high-dimensional representations can scale up to massive datasets, allowing them to outperform more sophisticated neural network language models which can process fewer documents. In this work, I offer a somewhat contrarian perspective to the currently prevailing trend of big data optimism, as exemplified",
        "type": "Document"
      },
      {
        "id": "c34c68fb-0506-429b-ad54-76b302326b52",
        "metadata": {
          "vector_store_key": "1812.07023-2",
          "chunk_id": 51,
          "document_id": "1812.07023",
          "start_idx": 19554,
          "end_idx": 19699
        },
        "page_content": " intersection between language and video are currently constrained by the quality of video features, and emphasizes the need for larger datasets.",
        "type": "Document"
      },
      {
        "id": "20aee739-e3da-480f-9d59-75c902234d8a",
        "metadata": {
          "vector_store_key": "1705.07368-0",
          "chunk_id": 6,
          "document_id": "1705.07368",
          "start_idx": 2340,
          "end_idx": 2763
        },
        "page_content": " embedding models on a generic large corpus such as Wikipedia, and use the embeddings for NLP tasks on the target dataset, cf. BIBREF3 , BIBREF0 , BIBREF16 , BIBREF17 . However, as we shall see here, this standard practice might not always be effective, as the size of a dataset does not correspond to its degree of relevance for a particular analysis. Even very large corpora have idiosyncrasies that can make their embedd",
        "type": "Document"
      },
      {
        "id": "2480ab2a-41f2-4d26-94e2-bdc4e0823665",
        "metadata": {
          "vector_store_key": "1705.07368-0",
          "chunk_id": 13,
          "document_id": "1705.07368",
          "start_idx": 5206,
          "end_idx": 5758
        },
        "page_content": " better modeling performance without big data, as measured by predictive performance (when the context is leveraged for prediction), as well as to interpretable latent representations that are highly valuable for computational social science applications. The interpretability of the representations arises from defining embeddings for words (and hence, documents) in terms of embeddings for topics. My experiments also shed light on the relative merits of training embeddings on generic big data corpora versus domain-specific data. In this section, I",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is an example of a computational social science NLP task?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "8c44a69d-975d-4318-a4e4-939649f9515a",
        "metadata": {
          "vector_store_key": "1901.02257-3",
          "chunk_id": 3,
          "document_id": "1901.02257",
          "start_idx": 1344,
          "end_idx": 1854
        },
        "page_content": "erences from multiple sentences from the passage and requireswith the commonsense knowledge that does not appear in the passage explicitly. Table 1 shows an example of CRC. Content: Previous Research 1. Category the methods in SemEval2018 task 11 2. Describe the first method 3. Describe the second method 4. State that your work is belong to which method Most studies on CRC task are neural network based (NN-based) models, which typically have the following characteristics. Firstly, word representations are",
        "type": "Document"
      },
      {
        "id": "52b17fb8-9cc0-4d8b-b640-08fd776c5642",
        "metadata": {
          "vector_store_key": "1803.09230-4",
          "chunk_id": 1,
          "document_id": "1803.09230",
          "start_idx": 578,
          "end_idx": 1094
        },
        "page_content": ", research in the field of Natural Language Processing (NLP) has massively benefited from neural architectures. Those approaches have outperformed former state-of-the-art non-neural machine learning model families while needing far less human intervention since they don't require any manual feature engineering. A subset of NLP research focuses on building systems that are able to answer questions about a given document. To jointly expand the current best practice, the Stanford Question Answering Dataset (SQuAD)",
        "type": "Document"
      },
      {
        "id": "3ca43d71-34bc-4711-ba18-08848f0c6b4f",
        "metadata": {
          "vector_store_key": "2001.00137-0",
          "chunk_id": 4,
          "document_id": "2001.00137",
          "start_idx": 1748,
          "end_idx": 2222
        },
        "page_content": " Language Processing (NLP) domain. Tasks such as text generation, sentence correction, image captioning and text classification, have been possible via models such as Convolutional Neural Networks and Recurrent Neural Networks BIBREF5, BIBREF6, BIBREF7. More recently, state-of-the-art results have been achieved with attention models, more specifically Transformers BIBREF8. Surprisingly, however, there is currently no research on incomplete text classification in the NLP",
        "type": "Document"
      },
      {
        "id": "33dc7dc7-655d-4d19-811c-6f54d603b97c",
        "metadata": {
          "vector_store_key": "1906.04571-9",
          "chunk_id": 0,
          "document_id": "1906.04571",
          "start_idx": 0,
          "end_idx": 481
        },
        "page_content": "One of the biggest challenges faced by modern natural language processing (NLP) systems is the inadvertent replication or amplification of societal biases. This is because NLP systems depend on language corpora, which are inherently \u201cnot objective; they are creations of human design\u201d BIBREF0 . One type of societal bias that has received considerable attention from the NLP community is gender stereotyping BIBREF1 , BIBREF2 , BIBREF3 . Gender stereotypes can manifest in language",
        "type": "Document"
      },
      {
        "id": "3a4bf1ee-5767-4294-b57f-e6bc21d66439",
        "metadata": {
          "vector_store_key": "2001.02380-4",
          "chunk_id": 11,
          "document_id": "2001.02380",
          "start_idx": 4283,
          "end_idx": 4881
        },
        "page_content": " work is conducted within Rhetorical Structural Theory, we hope that it can shed light on signal identification of discourse relations across genres and provide empirical evidence to motivate research on theory-neutral and genre-diverse discourse processing, which would be beneficial for pushing forward theories of discourse across frameworks or formalisms. Furthermore, employing a computational approach to studying discourse relations has a promising impact on various NLP downstream tasks such as question answering and document summarization etc. For example, BIBREF20 incorporated discourse",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which major geographical regions are studied?",
    "llm_answer": "Latin America and the Caribbean (LCN), South Asia (SAS), Sub-Saharan Africa (SSA), Europe and Central Asia (ECS), Middle East and North Africa (MEA), East Asia and the Pacific (EAS), North America (NAC)\n",
    "context": [
      {
        "id": "73283bf7-a4cf-46ab-9252-5a003d2fa964",
        "metadata": {
          "vector_store_key": "1708.05873-1",
          "chunk_id": 43,
          "document_id": "1708.05873",
          "start_idx": 17963,
          "end_idx": 18387
        },
        "page_content": " of regions: Latin America and the Caribbean (LCN), South Asia (SAS), Sub-Saharan Africa (SSA), Europe and Central Asia (ECS), Middle East and North Africa (MEA), East Asia and the Pacific (EAS), North America (NAC). The figure shows that states in South Asia, and Latin America and the Caribbean are likely to discuss Topic 2 the most. States in South Asia and East Asia and the Pacific discuss Topic 7 the most. The figure",
        "type": "Document"
      },
      {
        "id": "ea72e437-ec4e-47e8-b6bd-d9e6d434a1df",
        "metadata": {
          "vector_store_key": "1809.02494-1",
          "chunk_id": 21,
          "document_id": "1809.02494",
          "start_idx": 8157,
          "end_idx": 8743
        },
        "page_content": " highlight the creation of models of geographical descriptors, comparing models between both subject groups, studying combinations of models of cardinal directions, and researching on geographical referring expression generation. Furthermore, insights about the semantics of geographical concepts could be inferred under a more thorough analysis. One of the inconveniences that our data sets present is the appearance of the issues described in Sec. SECREF10 . It could be necessary to filter some of the answers according to different criteria (e.g., deviation of the centroid location",
        "type": "Document"
      },
      {
        "id": "46a7e06d-e5e1-4ecd-b704-98b4f397ba57",
        "metadata": {
          "vector_store_key": "1809.02494-1",
          "chunk_id": 4,
          "document_id": "1809.02494",
          "start_idx": 1540,
          "end_idx": 2054
        },
        "page_content": " However, we believe it can be useful for other related research purposes as well. The resource is composed of data from two different surveys. In both surveys subjects were asked to draw on a map (displayed under a Mercator projection) a polygon representing a given geographical descriptor, in the context of the geography of Galicia in Northwestern Spain (see Fig. FIGREF1 ). However, the surveys were run with different purposes, and the subject groups that participated in each survey and the list of descript",
        "type": "Document"
      },
      {
        "id": "8258dfaf-4219-4e17-8c74-6fe656baa554",
        "metadata": {
          "vector_store_key": "1809.02494-3",
          "chunk_id": 11,
          "document_id": "1809.02494",
          "start_idx": 4387,
          "end_idx": 4877
        },
        "page_content": " the survey. Comparing the visualizations of both data sets we have observed that this is clearly the case; the polygons drawn by the experts are more concentrated and therefore there is a higher agreement among them. On top of these differences, some students provided unexpected drawings in terms of shape, size, or location of the polygon for several descriptors. If we focus on single descriptors, one interesting outcome is that some of the answers for \u201cNorthern Galicia\u201d and \u201cSouthern",
        "type": "Document"
      },
      {
        "id": "edee593a-b3c1-4dab-9cd3-5ca0be13e076",
        "metadata": {
          "vector_store_key": "1809.02494-3",
          "chunk_id": 13,
          "document_id": "1809.02494",
          "start_idx": 5140,
          "end_idx": 5577
        },
        "page_content": " time (see Fig. FIGREF9 ). In the case of \u201cwest\u201d and \u201ceast\u201d, the drawings made by the experts were almost divergent and showed no overlapping between those two descriptors. Regarding \u201cInland Galicia\u201d, the unions of the answers for each group occupy approximately the same area with a similar shape, but there is a very high overlapping among the answers of the meteorologists. A similar situation is found for the remaining descriptor \u201cR",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How strong is the correlation between the prevalence of the #MeToo movement and official reports [of sexual harassment]?",
    "llm_answer": "The change in the number of reported cases constitutes a considerable change in the number of #MeToo users on Twitter (p-value extremely small at 5.7e-13).\n",
    "context": [
      {
        "id": "eb864d88-4e1c-414c-9572-4a4d0a1a439d",
        "metadata": {
          "vector_store_key": "2001.05970-2",
          "chunk_id": 2,
          "document_id": "2001.05970",
          "start_idx": 848,
          "end_idx": 1382
        },
        "page_content": " sexual harassment risk of the entire student body. Alternatively, social media opens up new opportunities to gather a larger and more comprehensive amount of data and mitigate the risk of false or inaccurate narratives from the studied subjects. On October 15 of 2017, prominent Hollywood actress Alyssa Milano, by accusing Oscar-winning film producer, Harvey Weinstein, for multiple sexual impropriety attempts on herself and many other women in the film industry, ignited the \"MeToo\" trend on social media that called for women and",
        "type": "Document"
      },
      {
        "id": "1bcc6174-6d2f-4fab-9dec-8622b64510fa",
        "metadata": {
          "vector_store_key": "2001.05970-2",
          "chunk_id": 4,
          "document_id": "2001.05970",
          "start_idx": 1672,
          "end_idx": 2274
        },
        "page_content": " propose a new approach to characterizing sexual harassment by mining the tweets from college users with the hash-tag #metoo on Twitter. Our main contributions are several folds. We investigate campus sexual harassment using a big-data approach by collecting data from Twitter. We employ traditional topic modeling and linear regression methods on a new dataset to highlight patterns of the ongoing troubling social behaviors at both institutional and individual levels. We propose a novel approach to combining domain-general deep semantic parsing and sentiment analysis to dissect personal narratives",
        "type": "Document"
      },
      {
        "id": "b3270102-423a-49a0-8ec8-369cb8357594",
        "metadata": {
          "vector_store_key": "2001.05970-0",
          "chunk_id": 34,
          "document_id": "2001.05970",
          "start_idx": 13447,
          "end_idx": 13945
        },
        "page_content": " will further examine the emotion features in the latter results. Observing the results of the linear regression in Table 2, we find the normalized governmental reported cases count and regional feature to be statistically significant on the sexual harassment rate in the Twitter data ($p-value<0.05$). Specifically, the change in the number of reported cases constitutes a considerable change in the number of #MeToo users on Twitter as p-value is extremely small at $5.7e-13$. This corresponds to",
        "type": "Document"
      },
      {
        "id": "39a3216f-e4bd-4a13-b7dd-709b52ae9888",
        "metadata": {
          "vector_store_key": "2001.05970-2",
          "chunk_id": 32,
          "document_id": "2001.05970",
          "start_idx": 12717,
          "end_idx": 13188
        },
        "page_content": " of LDA on #MeToo tweets of college users (Table 1) fall into the same pattern as the research of Modrek and Chakalov (2019), which suggests that a large portion of #MeToo tweets on Twitter focuses on sharing personal traumatic stories about sexual harassment BIBREF10. In fact, in our top 5 topics, Topics 1 and 5 mainly depict gruesome stories and childhood or college time experience. This finding seems to support the validity of the Twitter sample of Modrek and Chak",
        "type": "Document"
      },
      {
        "id": "514aaf21-7b65-4eb4-9062-5f8fcac8fce9",
        "metadata": {
          "vector_store_key": "2001.05970-2",
          "chunk_id": 3,
          "document_id": "2001.05970",
          "start_idx": 1382,
          "end_idx": 1876
        },
        "page_content": " in the film industry, ignited the \"MeToo\" trend on social media that called for women and men to share their own sexual harassment experience. According to CNN, over 1.7 million users had used the hash-tag in 85 countries. Benefiting from the tremendous amount of data supplied by this trend and the existing state-of-the-art semantic parser and generative statistical models, we propose a new approach to characterizing sexual harassment by mining the tweets from college users with the hash-",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are the topics embedded in the #MeToo tweets extracted?",
    "llm_answer": "Latent Dirichlet Allocation (LDA) is used to label universal topics.  The optimal topic number is selected based on the highest coherence score.\n",
    "context": [
      {
        "id": "4c728a25-9721-4a1d-b817-d071dfaa7e30",
        "metadata": {
          "vector_store_key": "2001.05970-2",
          "chunk_id": 20,
          "document_id": "2001.05970",
          "start_idx": 8131,
          "end_idx": 8655
        },
        "page_content": ". We determine the optimal topic number by selecting the one with the highest coherence score. Since certain words frequently appear in those #MeToo tweets (e.g., sexual harassment, men, women, story, etc.), we transform our corpus using TF-IDF, a term-weighting scheme that discounts the influence of common terms. Learning deep meaning representations, which enables the preservation of rich semantic content of entities, meaning ambiguity resolution and partial relational understanding of texts, is one of the challenges",
        "type": "Document"
      },
      {
        "id": "d322d4dc-4bf2-4c3e-9106-92b243c71e31",
        "metadata": {
          "vector_store_key": "1604.05781-6",
          "chunk_id": 11,
          "document_id": "1604.05781",
          "start_idx": 4432,
          "end_idx": 4838
        },
        "page_content": " BIBREF29 . A tweet will be referred to as a `document' for the rest of this work. All document text was processed the same way. Punctuation, XML characters, and hyperlinks were removed, as were Twitter-specific \u201cat-mentions\u201d and \u201chashtags\u201d (see also the Appendix). There is useful information here, but it is either not natural language text, or it is Twitter-specific, or both. Documents were broken into",
        "type": "Document"
      },
      {
        "id": "39a3216f-e4bd-4a13-b7dd-709b52ae9888",
        "metadata": {
          "vector_store_key": "2001.05970-2",
          "chunk_id": 32,
          "document_id": "2001.05970",
          "start_idx": 12717,
          "end_idx": 13188
        },
        "page_content": " of LDA on #MeToo tweets of college users (Table 1) fall into the same pattern as the research of Modrek and Chakalov (2019), which suggests that a large portion of #MeToo tweets on Twitter focuses on sharing personal traumatic stories about sexual harassment BIBREF10. In fact, in our top 5 topics, Topics 1 and 5 mainly depict gruesome stories and childhood or college time experience. This finding seems to support the validity of the Twitter sample of Modrek and Chak",
        "type": "Document"
      },
      {
        "id": "b9e35a49-27a9-4b03-b4b7-b654f885db6c",
        "metadata": {
          "vector_store_key": "2001.05970-2",
          "chunk_id": 19,
          "document_id": "2001.05970",
          "start_idx": 7862,
          "end_idx": 8316
        },
        "page_content": " context (peer-to-peer, school employee or work employer, and third-parties). These labels are based on definitions by the U.S. Dept. of Education BIBREF14. In order to understand the latent topics of those #MeToo tweets for college followers, we first utilize Latent Dirichlet Allocation (LDA) to label universal topics demonstrated by the users. We determine the optimal topic number by selecting the one with the highest coherence score. Since certain",
        "type": "Document"
      },
      {
        "id": "1edded34-18f5-483f-b5af-971bf45ef720",
        "metadata": {
          "vector_store_key": "2001.05970-2",
          "chunk_id": 18,
          "document_id": "2001.05970",
          "start_idx": 7394,
          "end_idx": 7862
        },
        "page_content": " users who posted #MeToo tweets per student enrolled) for each individual college. Per our topic modeling results, we decide to look deeper into the narratives of #MeToo users who reveal their personal stories. We examine 6,760 tweets from the most relevant topic of our LDA model, and categorize them based on the following metrics: harassment types (verbal, physical, and visual abuse) and context (peer-to-peer, school employee or work employer, and third-parties).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which geographical regions correlate to the trend?",
    "llm_answer": "South Asia, Latin America and the Caribbean, East Asia and the Pacific, the Southeast and Midwest of the US, and coastal cities.\n",
    "context": [
      {
        "id": "73283bf7-a4cf-46ab-9252-5a003d2fa964",
        "metadata": {
          "vector_store_key": "1708.05873-1",
          "chunk_id": 43,
          "document_id": "1708.05873",
          "start_idx": 17963,
          "end_idx": 18387
        },
        "page_content": " of regions: Latin America and the Caribbean (LCN), South Asia (SAS), Sub-Saharan Africa (SSA), Europe and Central Asia (ECS), Middle East and North Africa (MEA), East Asia and the Pacific (EAS), North America (NAC). The figure shows that states in South Asia, and Latin America and the Caribbean are likely to discuss Topic 2 the most. States in South Asia and East Asia and the Pacific discuss Topic 7 the most. The figure",
        "type": "Document"
      },
      {
        "id": "f0239b08-9806-448a-adf5-6a6b0dd796bb",
        "metadata": {
          "vector_store_key": "1904.07342-4",
          "chunk_id": 20,
          "document_id": "1904.07342",
          "start_idx": 7626,
          "end_idx": 8166
        },
        "page_content": " of positive tweets emanate from traditionally liberal hubs (e.g. San Francisco, Los Angeles, Austin), while most negative tweets come from the Philadelphia metropolitan area. These regions aside, rural areas tended to see more negative sentiment tweeters post-event, whereas urban regions saw more positive sentiment tweeters; however, overall average climate change sentiment pre- and post-event was relatively stable geographically. This map further confirms findings that coastal cities tend to be more aware of climate change BIBREF8 .",
        "type": "Document"
      },
      {
        "id": "51f48907-afa6-4f5b-b629-d7308838d58f",
        "metadata": {
          "vector_store_key": "1904.07342-4",
          "chunk_id": 18,
          "document_id": "1904.07342",
          "start_idx": 6890,
          "end_idx": 7375
        },
        "page_content": " on a subset with geographic user information and compare results to the prior literature. In Figure FIGREF3 , we map 4-clustering results on three dimensions: predicted sentiments, latitude, and longitude. The clusters correspond to four major regions of the U.S.: the Northeast (green), Southeast (yellow), Midwest (blue), and West Coast (purple); centroids are designated by crosses. Average sentiments within each cluster confirm prior knowledge BIBREF1 : the Southeast and Midwest",
        "type": "Document"
      },
      {
        "id": "ea72e437-ec4e-47e8-b6bd-d9e6d434a1df",
        "metadata": {
          "vector_store_key": "1809.02494-1",
          "chunk_id": 21,
          "document_id": "1809.02494",
          "start_idx": 8157,
          "end_idx": 8743
        },
        "page_content": " highlight the creation of models of geographical descriptors, comparing models between both subject groups, studying combinations of models of cardinal directions, and researching on geographical referring expression generation. Furthermore, insights about the semantics of geographical concepts could be inferred under a more thorough analysis. One of the inconveniences that our data sets present is the appearance of the issues described in Sec. SECREF10 . It could be necessary to filter some of the answers according to different criteria (e.g., deviation of the centroid location",
        "type": "Document"
      },
      {
        "id": "9c849e2b-e753-4b19-aed5-a75dc40db488",
        "metadata": {
          "vector_store_key": "1904.07342-4",
          "chunk_id": 19,
          "document_id": "1904.07342",
          "start_idx": 7375,
          "end_idx": 7830
        },
        "page_content": " by crosses. Average sentiments within each cluster confirm prior knowledge BIBREF1 : the Southeast and Midwest have lower average sentiments ( INLINEFORM0 and INLINEFORM1 , respectively) than the West Coast and Northeast (0.22 and 0.09, respectively). In Figure FIGREF5 , we plot predicted sentiment averaged by U.S. city of event-related tweeters. The majority of positive tweets emanate from traditionally liberal hubs (e.g. San Francisco, Los Angeles,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What two components are included in their proposed framework?",
    "llm_answer": "A deep learning classifier and a tweet processor.\n",
    "context": [
      {
        "id": "692e0ed1-d4ba-47e7-a034-873c65948722",
        "metadata": {
          "vector_store_key": "1603.04553-1",
          "chunk_id": 39,
          "document_id": "1603.04553",
          "start_idx": 11513,
          "end_idx": 11902
        },
        "page_content": "IBREF28 ; Latent-Tree: the latent tree model BIBREF29 obtaining the best results in the shared task; Berkeley: the Berkeley system with the final feature set BIBREF12 ; LaSO: the structured perceptron system with non-local features BIBREF30 ; Latent-Strc: the latent structure system BIBREF31 ; Model-Stack: the entity-centric system with model stacking BIBREF32 ; and Non-Linear: the non-",
        "type": "Document"
      },
      {
        "id": "78db7738-7d94-4b34-9355-58f104a0c62f",
        "metadata": {
          "vector_store_key": "1703.05260-2",
          "chunk_id": 84,
          "document_id": "1703.05260",
          "start_idx": 30689,
          "end_idx": 30773
        },
        "page_content": " Foundation (DFG) as part of SFB 1102 'Information Density and Linguistic Encoding'.",
        "type": "Document"
      },
      {
        "id": "8ba3f8b2-e6c4-46ce-8610-5b23955cb2fd",
        "metadata": {
          "vector_store_key": "2002.00876-2",
          "chunk_id": 47,
          "document_id": "2002.00876",
          "start_idx": 16529,
          "end_idx": 17069
        },
        "page_content": " we hope to support research and production applications employing structured models. We also believe the library provides a strong foundation for building generic tools for interpretablity, control, and visualization through its probabilistic API. Finally, we hope to explore further optimizations to make core algorithms competitive with highly-optimized neural network components. We thank Yoon Kim, Xiang Lisa Li, Sebastian Gehrmann, Yuntian Deng, and Justin Chiu for discussion and feedback on the project. The project was supported by",
        "type": "Document"
      },
      {
        "id": "2266d94d-963a-4c7a-a957-8102c9cf55c4",
        "metadata": {
          "vector_store_key": "2001.05493-9",
          "chunk_id": 31,
          "document_id": "2001.05493",
          "start_idx": 11386,
          "end_idx": 11895
        },
        "page_content": ". Section SECREF26 describes the computation of NLP features. In Sections SECREF30, SECREF34 and SECREF45 we have described the architecture of different deep learning models like Deep Pyramid CNN, Disconnected RNN and Pooled BiLSTM respectively. Finally, in Section SECREF49, we describe model averaging based classification model which combines the prediction probabilities from three deep learninig architectures discussed above. (see Figure FIGREF22. for block diagram of system architecture). We consider",
        "type": "Document"
      },
      {
        "id": "d9c2384b-f1e2-494d-9cf0-f282c0e71d6c",
        "metadata": {
          "vector_store_key": "1706.08032-3",
          "chunk_id": 6,
          "document_id": "1706.08032",
          "start_idx": 2368,
          "end_idx": 2886
        },
        "page_content": " unnecessary parts of a tweet. The paper makes the following contributions: The organization of the present paper is as follows: In section 2, we describe the model architecture which introduces the structure of the model. We explain the basic idea of model and the way of constructing the model. Section 3 show results and analysis and section 4 summarize this paper. Our proposed model consists of a deep learning classifier and a tweet processor. The deep learning classifier is a combination of DeepCNN and Bi-LSTM",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which modifications do they make to well-established Seq2seq architectures?",
    "llm_answer": "Consistent application of regularization techniques, and consideration of the monotonic nature of attention in TTS tasks.  They also use variational-based methods and a flow-based sequence-to-sequence model.\n",
    "context": [
      {
        "id": "3e498595-d783-4c31-bf54-4d97b69e7fa6",
        "metadata": {
          "vector_store_key": "2001.07263-2",
          "chunk_id": 3,
          "document_id": "2001.07263",
          "start_idx": 1190,
          "end_idx": 1675
        },
        "page_content": " problems; a detailed list can be found in Sec. SECREF2. Recently, increasingly complex attention mechanisms have been proposed to improve seq2seq model performance, including stacking self and regular attention layers and using multiple attention heads in the encoder and decoder BIBREF4, BIBREF8. We show that consistent application of various regularization techniques brings a simple, single-head LSTM attention based encoder-decoder model to state-of-the-art performance on Switch",
        "type": "Document"
      },
      {
        "id": "b69be169-e813-4614-b3af-d8f2e1b4aa3f",
        "metadata": {
          "vector_store_key": "1903.07398-1",
          "chunk_id": 22,
          "document_id": "1903.07398",
          "start_idx": 7684,
          "end_idx": 8197
        },
        "page_content": " proposed improvements come from the observation that employing generic Seq2seq models for TTS application misses out on further optimization that can be achieved when we consider the specific problem of TTS. Specifically, we notice that in TTS, unlike in applications like machine translation, the Seq2Seq attention mechanism should be mostly monotonic. In other words, when one reads a sequence of text, it is natural to assume that the text position progress nearly linearly in time with the sequence of output",
        "type": "Document"
      },
      {
        "id": "db9cab81-4d11-4a7a-8c54-b887fb66eb51",
        "metadata": {
          "vector_store_key": "1909.08824-3",
          "chunk_id": 59,
          "document_id": "1909.08824",
          "start_idx": 18096,
          "end_idx": 18537
        },
        "page_content": "q2Seq and variational-based methods, including Variational Seq2Seq, VRNMT, CWVAE-unpretrained and CWVAE shows that, variational-based methods could increase the diversity of generations. This confirms one of our motivations that variational-based methods could capture the latent semantic distribution within targets and increase the diversity of If-Then reasoning. (2) Comparing CWVAE-unpretrained with other baseline methods shows that, in",
        "type": "Document"
      },
      {
        "id": "ba16ebfb-5fbd-4938-82f8-929d97ed0da5",
        "metadata": {
          "vector_store_key": "1909.02480-7",
          "chunk_id": 1,
          "document_id": "1909.02480",
          "start_idx": 279,
          "end_idx": 619
        },
        "page_content": "ldots , x_{T^{\\prime }}\\rbrace $ using conditional probabilities $P_\\theta (\\mathbf {y}|\\mathbf {x})$ predicted by neural networks (parameterized by $\\theta $). Most seq2seq models are autoregressive, meaning that they factorize the joint probability of the output sequence given the input sequence $P_\\theta (\\mathbf {y}|\\mathbf {x})$ into",
        "type": "Document"
      },
      {
        "id": "d35fa69c-4bb2-4e57-8809-2de43b7db247",
        "metadata": {
          "vector_store_key": "1909.02480-7",
          "chunk_id": 11,
          "document_id": "1909.02480",
          "start_idx": 3609,
          "end_idx": 4051
        },
        "page_content": "Seq is a flow-based sequence-to-sequence model, which is (to our knowledge) the first non-autoregressive seq2seq model utilizing generative flows. It allows for efficient parallel decoding while modeling the joint distribution of the output sequence. Experimentally, on three benchmark datasets for machine translation \u2013 WMT2014, WMT2016 and IWSLT-2014, FlowSeq achieves comparable performance with state-of-the-art non-autoregressive models,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How was speed measured?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "e4170908-93bd-4ce5-aa3b-995165fba5aa",
        "metadata": {
          "vector_store_key": "1909.02764-3",
          "chunk_id": 29,
          "document_id": "1909.02764",
          "start_idx": 11795,
          "end_idx": 12335
        },
        "page_content": " instructed to reach the destination of the route as fast as possible while following traffic rules and speed limits. They are informed that the time needed for the task would be compared to other participants. The route comprises highways, rural roads, and city streets. A navigation system with voice commands and information on the screen keeps the participants on the predefined track. To trigger emotion changes in the participant, we use the following events: (i) a car on the right lane cutting off to the left lane when participants",
        "type": "Document"
      },
      {
        "id": "be18fd72-60be-4a02-9d39-f82a92a6b509",
        "metadata": {
          "vector_store_key": "1910.12618-2",
          "chunk_id": 85,
          "document_id": "1910.12618",
          "start_idx": 33548,
          "end_idx": 34010
        },
        "page_content": " are given in the appendix tables TABREF57 and TABREF58. The first row corresponds to the reference word vector $\\overrightarrow{w_1}$ used to calculate the distance from (thus the distance is always zero), while the following ones are the 9 closest to it. The two last rows correspond to words we deemed important to check the distance with (an antagonistic one or relevant one not in the top 9 for instance). The results of the experiments are very similar for",
        "type": "Document"
      },
      {
        "id": "9d72851e-4878-487a-9fd4-8f5d8149c61b",
        "metadata": {
          "vector_store_key": "1709.01256-6",
          "chunk_id": 46,
          "document_id": "1709.01256",
          "start_idx": 17240,
          "end_idx": 17745
        },
        "page_content": " running, new documents become available and INLINEFORM0 can be periodically updated by using the same method. This section reports the results of the experiments conducted on two data sets for evaluating the performances of wDTW and wTED against other baseline methods. We denote the following distance/similarity measures. wDTW: Our semantic distance measure explained in Section SECREF21 . wTED: Our semantic distance measure explained in Section SECREF23 . WMD: The Word Mover's Distance introduced in",
        "type": "Document"
      },
      {
        "id": "4ce440cb-dc47-4b8a-b61c-dee4751879c3",
        "metadata": {
          "vector_store_key": "1610.00879-1",
          "chunk_id": 11,
          "document_id": "1610.00879",
          "start_idx": 4217,
          "end_idx": 4654
        },
        "page_content": "damn', along with profane words. Heuristics other than these hashtags could have been used for dataset creation. For example, timestamps were a good option to account for time at which a tweet was posted. However, this could not be used because user's local times was not available, since very few users had geolocation enabled. The complete set of features is shown in Table TABREF7 . There are two sets of features: (a) N-gram features",
        "type": "Document"
      },
      {
        "id": "44bc9a3c-10b9-43b6-b19a-c95e60a6929b",
        "metadata": {
          "vector_store_key": "1709.01256-6",
          "chunk_id": 47,
          "document_id": "1709.01256",
          "start_idx": 17745,
          "end_idx": 18165
        },
        "page_content": " distance measure explained in Section SECREF23 . WMD: The Word Mover's Distance introduced in Section SECREF1 . WMD adapts the earth mover's distance to the space of documents. VSM: The similarity measure introduced in Section UID12 . PV-DTW: PV-DTW is the same as Algorithm SECREF21 except that the distance between two paragraphs is not based on Algorithm SECREF20 but rather computed as INLINEFORM0 where INLINEFORM1",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What were their accuracy results on the task?",
    "llm_answer": "The provided text mentions accuracy results for SNLI (90.4% and 90.6%) and an F1 score of 57% on TEC, but no accuracy results for tasks A, B, or C.  The answer is not found in the context.\n",
    "context": [
      {
        "id": "4c6c7b55-7160-4f18-8f21-89da87c2f8a0",
        "metadata": {
          "vector_store_key": "1603.07044-4",
          "chunk_id": 29,
          "document_id": "1603.07044",
          "start_idx": 9966,
          "end_idx": 10403
        },
        "page_content": " for task A, there are a total number of 26,700 question-comment pairs. For task B, there are 2,670 question-question pairs. For task C, there are 26,700 question-comment pairs. The test dataset includes 50 questions, 500 related questions and 5,000 comments which do not overlap with the training set. To evaluate the performance, we use mean average precision (MAP) and F1 score. Table 2 shows the initial results using the RNN encoder",
        "type": "Document"
      },
      {
        "id": "743843d3-029b-4695-8a9f-002c181ab2a6",
        "metadata": {
          "vector_store_key": "2004.03744-4",
          "chunk_id": 73,
          "document_id": "2004.03744",
          "start_idx": 26592,
          "end_idx": 27055
        },
        "page_content": " a standard deviation of 5.54. We required the workers to have a previous approval rate above 90%. No restriction was put on the workers' location. Each assignment consisted of a set of 10 image-sentence pairs. For each pair, the participant was asked to (a) choose a label, (b) highlight words in the sentence that led to their decision, and (c) explain their decision in a comprehensive and concise manner, using a subset of the words that they highlighted. The",
        "type": "Document"
      },
      {
        "id": "e824c00b-fbd1-48fb-b927-d1bdd2c60b93",
        "metadata": {
          "vector_store_key": "1803.09230-3",
          "chunk_id": 47,
          "document_id": "1803.09230",
          "start_idx": 16584,
          "end_idx": 17109
        },
        "page_content": " the task challenging but we were extremely satisfied with our own personal learning curve. We are sure that with even more time, we could significantly improve our model from the baseline enhancement we achieved so far. All in all, we believe that the experience of this project, will be of utmost value for our future professional work. First of all, we would like to thank the course instructor Richard Socher for making the class highly informative and a great learning experience. We would also like to thank the TAs for",
        "type": "Document"
      },
      {
        "id": "a038f163-78d3-4778-bc4c-06835bfdfc19",
        "metadata": {
          "vector_store_key": "1810.09774-7",
          "chunk_id": 38,
          "document_id": "1810.09774",
          "start_idx": 14308,
          "end_idx": 14688
        },
        "page_content": " accuracy of 22.5 points. Our baselines for SNLI (90.4%) and SNLI + MultiNLI (90.6%) outperform the previous state-of-the-art accuracy for SNLI (90.1%) by BIBREF24 . To understand better the types of errors made by neural network models in NLI we looked at some example failure-pairs for selected models. Tables 5 and 6 contain some randomly selected failure-pairs for two models:",
        "type": "Document"
      },
      {
        "id": "31e3f6a2-9e9b-4d6d-824a-22195518284a",
        "metadata": {
          "vector_store_key": "1909.02764-7",
          "chunk_id": 52,
          "document_id": "1909.02764",
          "start_idx": 20420,
          "end_idx": 20841
        },
        "page_content": "=73 % on TEC. The model trained on our AMMER corpus achieves an F1 score of 57%. This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set. These results are mostly in line with Bostan2018. Now we analyze how well the models trained in Experiment 1 perform when applied to our data set. The results are shown in column \u201cSimple\u201d in Table TAB",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What two types the Chinese reading comprehension dataset consists of?",
    "llm_answer": "People Daily news datasets and Children's Fairy Tale datasets.\n",
    "context": [
      {
        "id": "db97f4b8-ca10-4375-afb2-73bb24c7eac3",
        "metadata": {
          "vector_store_key": "1709.08299-1",
          "chunk_id": 10,
          "document_id": "1709.08299",
          "start_idx": 4080,
          "end_idx": 4586
        },
        "page_content": " The previously mentioned datasets are all in English. To add diversities to the reading comprehension datasets, Cui et al. cui-etal-2016 proposed the first Chinese cloze-style reading comprehension dataset: People Daily & Children's Fairy Tale, including People Daily news datasets and Children's Fairy Tale datasets. They also generate the data in an automatic manner, which is similar to the previous datasets. They choose short articles (several hundreds of words) as Document and remove a word from it",
        "type": "Document"
      },
      {
        "id": "b68ef1ce-1178-4fea-8685-23c7c8a0eae5",
        "metadata": {
          "vector_store_key": "1806.09103-4",
          "chunk_id": 9,
          "document_id": "1806.09103",
          "start_idx": 3703,
          "end_idx": 4224
        },
        "page_content": " and low-frequency words, we employ a short list mechanism. Our evaluation will be performed on three public Chinese reading comprehension datasets and one English benchmark dataset for showing our method is also effective in multi-lingual case. The concerned reading comprehension task can be roughly categorized as user-query type and cloze-style according to the answer form. Answers in the former are usually a span of texts while in the cloze-style task, the answers are words or phrases which lets the latter be the",
        "type": "Document"
      },
      {
        "id": "ff941968-1e11-4ceb-8f1b-f59ee19159f3",
        "metadata": {
          "vector_store_key": "1709.08299-4",
          "chunk_id": 31,
          "document_id": "1709.08299",
          "start_idx": 12705,
          "end_idx": 13259
        },
        "page_content": " comprehension dataset for the 1st Evaluation on Chinese Machine Reading Comprehension (CMRC-2017), consisting large-scale automatically generated training set and human-annotated validation and test set. Many participants have verified their algorithms on this dataset and tested on the hidden test set for final evaluation. The experimental results show that the neural baselines are tough to beat and there is still much room for using complicated transfer learning method to better solve the User Query Task. We hope the release of the full dataset (",
        "type": "Document"
      },
      {
        "id": "97611658-9a4f-499e-90c2-644693094e7d",
        "metadata": {
          "vector_store_key": "1709.08299-4",
          "chunk_id": 1,
          "document_id": "1709.08299",
          "start_idx": 556,
          "end_idx": 1058
        },
        "page_content": " sentence according to the document material. Several cloze-style reading comprehension datasets are publicly available, such as CNN/Daily Mail BIBREF0 , Children's Book Test BIBREF1 , People Daily and Children's Fairy Tale BIBREF2 . In this paper, we provide a new Chinese reading comprehension dataset, which has the following features We also host the 1st Evaluation on Chinese Machine Reading Comprehension (CMRC2017), which has attracted over 30 participants and finally there were 17 participants",
        "type": "Document"
      },
      {
        "id": "54924d97-cc79-4e62-8296-655918b8d3b4",
        "metadata": {
          "vector_store_key": "1806.09103-5",
          "chunk_id": 34,
          "document_id": "1806.09103",
          "start_idx": 12961,
          "end_idx": 13401
        },
        "page_content": " current cloze-style datasets for English reading comprehension, such as CBT, Daily Mail and CNN BIBREF0 , the three Chinese datasets do not provide candidate answers. Thus, the model has to find the correct answer from the entire document. Besides, we also use the Children's Book Test (CBT) dataset BIBREF1 to test the generalization ability in multi-lingual case. We only focus on subsets where the answer is either a common noun (CN) or",
        "type": "Document"
      }
    ]
  },
  {
    "query": "For which languages most of the existing MRC datasets are created?",
    "llm_answer": "English, German, Spanish, Italian, Japanese, and Portuguese.\n",
    "context": [
      {
        "id": "ef088dd9-ab26-43d4-a236-5cdf234c79f7",
        "metadata": {
          "vector_store_key": "1910.11493-6",
          "chunk_id": 13,
          "document_id": "1910.11493",
          "start_idx": 5173,
          "end_idx": 5630
        },
        "page_content": " languages. Data for all but four languages (Basque, Kurmanji, Murrinhpatha, and Sorani) are extracted from English Wiktionary, a large multi-lingual crowd-sourced dictionary with morphological paradigms for many lemmata. 20 of the 100 language pairs are either distantly related or unrelated; this allows speculation into the relative importance of data quantity and linguistic relatedness. For each language, the basic data consists of triples of the form",
        "type": "Document"
      },
      {
        "id": "bf5d3708-e83c-48ce-beee-ee05ecc04bfd",
        "metadata": {
          "vector_store_key": "1911.00069-7",
          "chunk_id": 56,
          "document_id": "1911.00069",
          "start_idx": 17396,
          "end_idx": 17813
        },
        "page_content": "11. Our in-house dataset includes manually annotated RE data for 6 languages: English, German, Spanish, Italian, Japanese and Portuguese. It defines 56 entity types (e.g., Person, Organization, Geo-Political Entity, Location, Facility, Time, Event_Violence, etc.) and 53 relation types between the entities (e.g., AgentOf, LocatedAt, PartOf, TimeOf, AffectedBy, etc.). The ACE05 dataset includes manually annotated RE",
        "type": "Document"
      },
      {
        "id": "ec4f5a1a-ee42-43ff-81a9-ab41c4ff8914",
        "metadata": {
          "vector_store_key": "1909.06522-0",
          "chunk_id": 32,
          "document_id": "1909.06522",
          "start_idx": 11616,
          "end_idx": 12081
        },
        "page_content": ": live streams from users. UserVOD (video on demand): was-live videos. For each language, the train and test set size are described in Table TABREF10, and most training data were Pages. On each language we also had a small validation set for model parameter tuning. Each monolingual ASR baseline was trained on language-specific data only. The character sets of these 7 languages have little overlap except that (i) they all include common basic Latin alphabet, and",
        "type": "Document"
      },
      {
        "id": "bea618fb-90d6-4dc7-8e60-c8e7bcf0b45f",
        "metadata": {
          "vector_store_key": "1701.03214-2",
          "chunk_id": 15,
          "document_id": "1701.03214",
          "start_idx": 5368,
          "end_idx": 5787
        },
        "page_content": " (essentially open) in-domain data. The scientific domain MT was conducted on the Chinese-Japanese paper excerpt corpus (ASPEC-CJ) BIBREF11 , which is one subtask of the workshop on Asian translation (WAT) BIBREF15 . The ASPEC-CJ task uses 672315, 2090, and 2107 sentences for training, development, and testing, respectively. The Wikipedia domain task was conducted on a Chinese-Japanese corpus automatically extracted",
        "type": "Document"
      },
      {
        "id": "98ec5fad-9146-4cee-9f5a-42fbd39babf8",
        "metadata": {
          "vector_store_key": "1911.03894-7",
          "chunk_id": 4,
          "document_id": "1911.03894",
          "start_idx": 1502,
          "end_idx": 1983
        },
        "page_content": " as the investigation of their language modeling capacity, something that remains to be investigated in the case of, for instance, morphologically rich languages. We take advantage of the newly available multilingual corpus OSCAR BIBREF13 and train a monolingual language model for French using the RoBERTa architecture. We pretrain the model - which we dub CamemBERT- and evaluate it in four different downstream tasks for French: part-of-speech (POS) tagging, dependency parsing,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which sentiment analysis tasks are addressed?",
    "llm_answer": "Tweet-level sentiment, deeper analysis of clinical narratives in EHRs, and emotion detection.\n",
    "context": [
      {
        "id": "bf781e24-ae76-47e2-96a6-cd675bbf8bab",
        "metadata": {
          "vector_store_key": "1710.01492-0",
          "chunk_id": 12,
          "document_id": "1710.01492",
          "start_idx": 4787,
          "end_idx": 5328
        },
        "page_content": ", which is needed to train the high number of parameters of such complex models. Finally, as methods for sentiment analysis mature, more attention is also being paid to linguistic structure and to multi-linguality and cross-linguality. Sentiment analysis emerged as a popular research direction in the early 2000s. Initially, it was regarded as standard document classification into topics such as business, sport, and politics BIBREF10 . However, researchers soon realized that it was quite different from standard document classification B",
        "type": "Document"
      },
      {
        "id": "e8c488cd-359b-4a40-9599-f5c283e7ea5e",
        "metadata": {
          "vector_store_key": "1710.01492-1",
          "chunk_id": 23,
          "document_id": "1710.01492",
          "start_idx": 8983,
          "end_idx": 9412
        },
        "page_content": " implicit event polarity BIBREF35 , stance in tweets BIBREF36 , out-of-context sentiment intensity of phrases BIBREF37 , and emotion detection BIBREF38 . Some of these tasks featured languages other than English. Tweet-level sentiment. The simplest and also the most popular task of sentiment analysis on Twitter is to determine the overall sentiment expressed by the author of a tweet BIBREF30 , BIBREF28 , BIBREF26 , BIBREF29 ,",
        "type": "Document"
      },
      {
        "id": "694dd23c-9ccf-432a-8d86-67d75883c114",
        "metadata": {
          "vector_store_key": "1809.05752-4",
          "chunk_id": 58,
          "document_id": "1809.05752",
          "start_idx": 23383,
          "end_idx": 23945
        },
        "page_content": " These include a deeper analysis of clinical narratives in EHRs: our next task will be to extend our EHR data pipeline by distinguishing between clinically positive and negative phenomena within each risk factor domain. This will involve a series of annotation tasks that will allow us to generate lexicon-based and corpus-based sentiment analysis tools. We can then use these clinical sentiment scores to generate a gradient of patient improvement or deterioration over time. We will also take into account structured data that have been collected on the target",
        "type": "Document"
      },
      {
        "id": "8448f360-547f-4701-9e82-26ef1b354393",
        "metadata": {
          "vector_store_key": "1710.01492-0",
          "chunk_id": 78,
          "document_id": "1710.01492",
          "start_idx": 28162,
          "end_idx": 28625
        },
        "page_content": " etc. We expect the quest for more interesting formulations of the general sentiment analysis task to continue. We see competitions such as those at SemEval as the engine of this innovation, as they not only perform head-to-head comparisons, but also create databases and tools that enable follow-up research for many years afterward. In terms of methods, we believe that deep learning BIBREF55 , BIBREF54 , BIBREF56 , together with semi-supervised and distantly-",
        "type": "Document"
      },
      {
        "id": "3130764c-8d0a-48ee-a4b1-b441d693f285",
        "metadata": {
          "vector_store_key": "1911.12569-0",
          "chunk_id": 16,
          "document_id": "1911.12569",
          "start_idx": 6633,
          "end_idx": 7132
        },
        "page_content": " analysis. This may be because of the fine-grained characteristics of emotion analysis that provides useful evidences for sentiment analysis. We propose a novel two-layered multi-task attention based neural network for sentiment analysis where emotion analysis is utilized to improve its efficiency. Figure FIGREF1 illustrates the overall architecture of the proposed multi-task system. The proposed system consists of a Bi-directional Long Short-Term Memory (BiLSTM) BIBREF16, a two-level attention",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which 3 NLP areas are cited the most?",
    "llm_answer": "Answer not found in the context.\n",
    "context": [
      {
        "id": "485f9e44-0bdc-4e12-a7b7-b4d77dfe4dc6",
        "metadata": {
          "vector_store_key": "1911.03562-9",
          "chunk_id": 140,
          "document_id": "1911.03562",
          "start_idx": 54519,
          "end_idx": 55001
        },
        "page_content": " is common across a majority of the high-citations areas within NLP. This work examined the ACL Anthology to identify broad trends in productivity, focus, and impact. We examined several questions such as: who and how many of us are publishing? what are we publishing on? where and in what form are we publishing? and what is the impact of our publications? Particular attention was paid to the demographics and inclusiveness of the NLP community. Notably, we showed that only about",
        "type": "Document"
      },
      {
        "id": "6e565ceb-3dcd-4ed6-b85b-2a2334c2309f",
        "metadata": {
          "vector_store_key": "1911.03562-9",
          "chunk_id": 2,
          "document_id": "1911.03562",
          "start_idx": 729,
          "end_idx": 1219
        },
        "page_content": " what are we publishing on? where and in what form are we publishing? and what is the impact of our publications? The answers are usually in the form of numbers, graphs, and inter-connected visualizations. We focus on the following aspects of NLP research: size, demographics, areas of research, impact, and correlation of citations with demographic attributes (age and gender).  Target Audience: The analyses presented here are likely to be of interest to any NLP researcher. This might be",
        "type": "Document"
      },
      {
        "id": "84cd4dc4-c3c6-4b42-b2d9-f4a9f0340f7b",
        "metadata": {
          "vector_store_key": "1911.03562-9",
          "chunk_id": 3,
          "document_id": "1911.03562",
          "start_idx": 1219,
          "end_idx": 1727
        },
        "page_content": ": The analyses presented here are likely to be of interest to any NLP researcher. This might be particularly the case for those that are new to the field and wish to get a broad overview of the NLP publishing landscape. On the other hand, even seasoned NLP'ers have likely wondered about the questions raised here and might be interested in the empirical evidence.  Data: The analyses presented below are based on information about the papers taken directly from AA (as of June 2019) and citation information",
        "type": "Document"
      },
      {
        "id": "1dd4de98-7fe3-46e4-92b4-6bf23069dc12",
        "metadata": {
          "vector_store_key": "2004.03034-7",
          "chunk_id": 12,
          "document_id": "2004.03034",
          "start_idx": 4715,
          "end_idx": 5200
        },
        "page_content": "REF28. Consistent with findings in the social sciences and psychology, some of the work in NLP has shown that the impact and persuasive power of the arguments are not simply related to the linguistic characteristics of the language, but also on characteristics the source (ethos) BIBREF16 and the audience BIBREF12, BIBREF13. These studies suggest that perception of the arguments can be influenced by the credibility of the source, and the background of the audience. It has also been",
        "type": "Document"
      },
      {
        "id": "f952de20-5613-43d3-8ed8-8633df71ec7a",
        "metadata": {
          "vector_store_key": "1911.03562-9",
          "chunk_id": 16,
          "document_id": "1911.03562",
          "start_idx": 5387,
          "end_idx": 5831
        },
        "page_content": "\ufffdTop-tier Conference\u201d is somewhat arbitrary, but it helps make certain comparisons more meaningful (for example, when comparing the average number of citations, etc.). For this work, we consider ACL, EMNLP, NAACL, COLING, and EACL as top-tier conferences, but certainly other groupings are also reasonable. Q. How many papers have been published at ACL (main conference papers)? What are the other NLP venues and what is the distribution of the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which journal and conference are cited the most in recent years?",
    "llm_answer": "The context states that CL Journal, EMNLP, and ACL have the best track record for accepting papers that have gone on to receive 1000 or more citations, but does not definitively say which is cited the most in recent years.\n",
    "context": [
      {
        "id": "05d107a4-d31d-4e0c-9b7b-fea14c940ca3",
        "metadata": {
          "vector_store_key": "1911.03562-1",
          "chunk_id": 79,
          "document_id": "1911.03562",
          "start_idx": 30434,
          "end_idx": 30877
        },
        "page_content": " the paper title, authors, year of publication, publication venue, and #citations. Discussion: With time, not only have the number of papers grown, but also the number of high-citation papers. We see a marked jump in the 1990s over the previous decades, but the 2000s are the most notable in terms of the high number of citations. The 2010s papers will likely surpass the 2000s papers in the years to come. Q. What are the most cited papers in",
        "type": "Document"
      },
      {
        "id": "2bcc2d57-a73c-4d3a-8cb1-0e70ddab1a67",
        "metadata": {
          "vector_store_key": "1911.03562-1",
          "chunk_id": 105,
          "document_id": "1911.03562",
          "start_idx": 40431,
          "end_idx": 40920
        },
        "page_content": " marked in CL journal, and thus they were not removed from analysis. Also, letters to editors, which are more common in CL journal, tend to often obtain 0 citations. CL, EMNLP, and ACL have the best track record for accepting papers that have gone on to receive 1000 or more citations. *Sem, the semantics conference, seems to have notably lower percentage of high-citation papers, even though it has fairly competitive acceptance rates. Instead of percentage, if one considers raw numbers",
        "type": "Document"
      },
      {
        "id": "fe9d76b6-51ce-4a34-b925-2a4de85fffd7",
        "metadata": {
          "vector_store_key": "1911.03562-1",
          "chunk_id": 104,
          "document_id": "1911.03562",
          "start_idx": 40114,
          "end_idx": 40632
        },
        "page_content": " or more citations. The percentages are markedly lower (between 30 and 70%) for the other conferences shown above, and even lower for some other conferences (not shown above). CL Journal is particularly notable for the largest percentage of papers with 100 or more citations. The somewhat high percentage of papers that are never cited (4.3%) are likely because some of the book reviews from earlier years are not explicitly marked in CL journal, and thus they were not removed from analysis. Also, letters to editors,",
        "type": "Document"
      },
      {
        "id": "b736cbc4-361a-4ef7-862a-0bf34309e1ec",
        "metadata": {
          "vector_store_key": "1911.03562-2",
          "chunk_id": 99,
          "document_id": "1911.03562",
          "start_idx": 38236,
          "end_idx": 38701
        },
        "page_content": " become competitive with some top-tier conferences such as EACL and COLING. Q. What percent of the AA\u2019 papers that were published in 2016 or earlier are cited more than 1000 times? How many more than 10 times? How many papers are cited 0 times? A. Google Scholar invented the i-10 index as another measure of author research impact. It stands for the number of papers by an author that received ten or more citations. (Ten here is somewhat arbitrary, but reasonable",
        "type": "Document"
      },
      {
        "id": "842e3546-5c8f-4ff1-a944-a58f3ba001bc",
        "metadata": {
          "vector_store_key": "1911.03562-1",
          "chunk_id": 90,
          "document_id": "1911.03562",
          "start_idx": 34616,
          "end_idx": 35134
        },
        "page_content": " 26,949 such papers. Figures FIGREF42 and FIGREF43 show the average citations by paper type when considering papers published 1965\u20132016 and 2010\u20132016, respectively. Figures FIGREF45 and FIGREF46 show the medians. Discussion: Journal papers have much higher average and median citations than other papers, but the gap between them and top-tier conferences is markedly reduced when considering papers published since 2010. System demo papers have the third highest average citations; however, shared task papers have the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which 5 languages appear most frequently in AA paper titles?",
    "llm_answer": "Chinese, Arabic, Korean, Japanese, and Hindi\n",
    "context": [
      {
        "id": "098ffcac-f007-4521-8b31-cf8fc66bbb28",
        "metadata": {
          "vector_store_key": "1911.03562-6",
          "chunk_id": 43,
          "document_id": "1911.03562",
          "start_idx": 15948,
          "end_idx": 16450
        },
        "page_content": " language, then the language is mentioned in the title. We collected a list of 122 languages indexed by Wiktionary and looked for the presence of these words in the titles of AA papers. (Of course there are hundreds of other lesser known languages as well, but here we wanted to see the representation of these more prominent languages in NLP literature.) Figure FIGREF27 is a treemap of the 122 languages arranged alphabetically and shaded such that languages that appear more often in AA paper titles",
        "type": "Document"
      },
      {
        "id": "057f82cb-a806-499c-8a8c-9ecc92bf4ff5",
        "metadata": {
          "vector_store_key": "1911.03562-6",
          "chunk_id": 44,
          "document_id": "1911.03562",
          "start_idx": 16450,
          "end_idx": 16950
        },
        "page_content": " the 122 languages arranged alphabetically and shaded such that languages that appear more often in AA paper titles have a darker shade of green. Discussion: Even though the amount of work done on English is much larger than that on any other language, often the word English does not appear in the title, and this explains why English is not the first (but the second-most) common language name to appear in the titles. This is likely due to the fact that many papers fail to mention the language of",
        "type": "Document"
      },
      {
        "id": "50c097f6-5aea-4a2b-ba0f-f1d0595653ea",
        "metadata": {
          "vector_store_key": "1911.03562-6",
          "chunk_id": 46,
          "document_id": "1911.03562",
          "start_idx": 17227,
          "end_idx": 17704
        },
        "page_content": " on how central it is to the paper. We can see from the treemap that the most widely spoken Asian and Western European languages enjoy good representation in AA. These include: Chinese, Arabic, Korean, Japanese, and Hindi (Asian) as well as French, German, Swedish, Spanish, Portuguese, and Italian (European). This is followed by the relatively less widely spoken European languages (such as Russian, Polish, Norwegian, Romanian, Dutch, and Czech) and Asian languages (such as",
        "type": "Document"
      },
      {
        "id": "97f46859-3e07-4928-a15f-1cba6c3fba5c",
        "metadata": {
          "vector_store_key": "1911.03562-6",
          "chunk_id": 45,
          "document_id": "1911.03562",
          "start_idx": 16740,
          "end_idx": 17227
        },
        "page_content": " in the titles. This is likely due to the fact that many papers fail to mention the language of study or the language of the datasets used if it is English. There is growing realization in the community that this is not quite right. However, the language of study can be named in other less prominent places than the title, for example the abstract, introduction, or when the datasets are introduced, depending on how central it is to the paper. We can see from the treemap that the most",
        "type": "Document"
      },
      {
        "id": "074f78a1-c118-4286-a09f-64b8080aeeb9",
        "metadata": {
          "vector_store_key": "1911.03562-3",
          "chunk_id": 58,
          "document_id": "1911.03562",
          "start_idx": 21972,
          "end_idx": 22500
        },
        "page_content": " the unigram in that year and the two years before it is at least ten. The period before 1980 is not included because of the small number of papers. Discussion: Appropriately enough, the most common term in the titles of NLP papers is language. Presence of high-ranking terms pertaining to machine translation suggest that it is the area of research that has received considerable attention. Other areas associated with the high-frequency title terms include lexical semantics, named entity recognition, question answering, word",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much F1 was improved after adding skip connections?",
    "llm_answer": "The provided text states that skip connections improved scores further, but does not give a specific F1 score improvement number.\n",
    "context": [
      {
        "id": "be88be3b-889d-4260-9a8d-613dbf046e7d",
        "metadata": {
          "vector_store_key": "1912.10435-1",
          "chunk_id": 43,
          "document_id": "1912.10435",
          "start_idx": 16280,
          "end_idx": 16800
        },
        "page_content": " than the original BERT attention. The skip connections improved the scores further by adding the BERT embeddings back in to the coattention vectors and providing information that may have been lost by the C2Q/Q2C network in addition to providing a convenient path for backpropagation to the BERT embedding layers. The skip connection containing the transformer provides minimal gains while adding a significant overhead to runtime. Therefore, we built the final convolutional experiments on the Simple Skip architecture",
        "type": "Document"
      },
      {
        "id": "a658169c-2202-4f8b-badf-18f481de632a",
        "metadata": {
          "vector_store_key": "1912.10435-1",
          "chunk_id": 42,
          "document_id": "1912.10435",
          "start_idx": 15804,
          "end_idx": 16280
        },
        "page_content": " C2Q/Q2C network produced a significant improvement in the No Answer F1 score while causing a symmetric drop in the Has Answer F1 score. The C2Q/Q2C network attends the context relative to the query and vice versa instead of as a concatenated whole. This method of attention provides more information regarding whether there is an answer to the question in the context than the original BERT attention. The skip connections improved the scores further by adding the BERT embed",
        "type": "Document"
      },
      {
        "id": "9dac7b7a-da24-4242-bf4e-c64a053dd14f",
        "metadata": {
          "vector_store_key": "2001.11268-9",
          "chunk_id": 64,
          "document_id": "2001.11268",
          "start_idx": 25325,
          "end_idx": 25807
        },
        "page_content": " observed increased performance when adding additional data. For classifying I entities, an increase from 20 to 200 additional SQuAD domains resulted in an increase of 8% for the F1 score, whereas the increase for the O domain was less than 1%. After training a model with 200 additional SQuAD domains, we also evaluated it on the original SQuAD development set and obtained a F1 score of 0.72 for this general reading comprehension task. In this evaluation, the F1 scores represent",
        "type": "Document"
      },
      {
        "id": "34b0992f-2981-42b6-9f23-b9cd7ad335f0",
        "metadata": {
          "vector_store_key": "1912.10435-7",
          "chunk_id": 45,
          "document_id": "1912.10435",
          "start_idx": 17060,
          "end_idx": 17540
        },
        "page_content": " and 3 are described in detail in Section 3.6. Each of the models built on BERT large used our augmented dataset in addition to the coattention architecture, simple skip connection, and separate start and end logit logic. The Model 1 results show that a moderately augmented (35%) data set helps the training since both unaugmented and highly augmented (50%) models did not perform as well. It seems that adding too much augmented data reduces the F1 because the augmented data is",
        "type": "Document"
      },
      {
        "id": "e63d443f-91b8-4a21-84d3-fc8e1f4c72cc",
        "metadata": {
          "vector_store_key": "1810.00663-5",
          "chunk_id": 64,
          "document_id": "1810.00663",
          "start_idx": 24612,
          "end_idx": 25150
        },
        "page_content": "FORM1 . Note that the impact of the masking function is less evident in terms of the F1 score because this metric considers if a predicted behavior exists in the ground truth navigation plan, irrespective of its specific position in the output sequence. The results in the last four rows of Table TABREF28 suggest that ordering the graph triplets can facilitate predicting correct navigation plans in previously seen environments. Providing the triplets that surround the starting location of the robot first to the model leads to a boost",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much gain does the model achieve with pretraining MVCNN?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "e7ad2065-0857-4083-8209-9dcbcc08f84b",
        "metadata": {
          "vector_store_key": "1603.04513-0",
          "chunk_id": 10,
          "document_id": "1603.04513",
          "start_idx": 3855,
          "end_idx": 4308
        },
        "page_content": " sets are available. MVCNN has a large number of parameters, so that overfitting is a danger when they are trained on small training sets. We address this problem by pretraining MVCNN on unlabeled data. These pretrained weights can then be fine-tuned for the specific classification task. In sum, we attribute the success of MVCNN to: (i) designing variable-size convolution filters to extract variable-range features of sentences and (ii) exploring the",
        "type": "Document"
      },
      {
        "id": "8d0c4e5d-631f-4048-baf4-182ba9dfc9eb",
        "metadata": {
          "vector_store_key": "2003.01769-1",
          "chunk_id": 28,
          "document_id": "2003.01769",
          "start_idx": 11190,
          "end_idx": 11648
        },
        "page_content": " is critical since it requires the enhancement model to produce speech the acoustic model sees as \u201cclean.\u201d In addition to the setting without any parallel data, we show results given parallel data. In Table TABREF10 we demonstrate that training the AECNN framework with mimic loss improves intelligibility over both the model trained with only time-domain loss (AECNN-T), as well as the model trained with both time-domain and spectral-domain losses (AECNN-T",
        "type": "Document"
      },
      {
        "id": "56ca0e33-ca80-4d23-b972-16cd2c643947",
        "metadata": {
          "vector_store_key": "1905.06566-2",
          "chunk_id": 84,
          "document_id": "1905.06566",
          "start_idx": 23481,
          "end_idx": 23825
        },
        "page_content": " pre-training (details in Section \"Pre-training\" ) leads to a +1.25 ROUGE improvement. Another baseline is based on a pre-trained BERT BIBREF0 and finetuned on the CNNDM dataset. We used the $\\text{BERT}_{\\text{base}}$ model because our 16G RAM V100 GPU cannot fit $\\text{BERT}_{\\text{large}}$ for the summarization task even with batch size of",
        "type": "Document"
      },
      {
        "id": "148ea921-3ae7-4687-bc70-85a9690fe58b",
        "metadata": {
          "vector_store_key": "1912.13109-4",
          "chunk_id": 20,
          "document_id": "1912.13109",
          "start_idx": 7921,
          "end_idx": 8471
        },
        "page_content": " on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we",
        "type": "Document"
      },
      {
        "id": "42150962-2d02-417b-b4fa-d948c244b2d5",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 58,
          "document_id": "1909.13375",
          "start_idx": 19611,
          "end_idx": 20052
        },
        "page_content": " to gain further insights on the effect of the components that we ourselves introduced. For ease of performance comparison, we followed NABERT+'s training settings: we used the BERT Adam optimizer from huggingface with default settings and a learning rate of $1e^{-5}$. The only difference was that we used a batch size of 12. We trained our base model for 20 epochs. For the large models we used a batch size of 3 with a learning rate of $5",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the highest accuracy score achieved?",
    "llm_answer": "93.06% (unnormalized) for scene recognition.\n",
    "context": [
      {
        "id": "975b0340-cb8a-4c46-87f7-07ef48e97379",
        "metadata": {
          "vector_store_key": "2001.11268-9",
          "chunk_id": 58,
          "document_id": "2001.11268",
          "start_idx": 22809,
          "end_idx": 23352
        },
        "page_content": " precision scores in ranges between 0.92 and 0.97 are possible with F1 scores not dropping below 0.84 for the main classes of interest. In practice, the detachment between model predictions and assignment of labels means that a reviewer who wishes to switch between high recall and high precision results can do so very quickly, without obtaining new predictions from the model itself. More visualizations can be found in this project's GitHub repository , including true class labels and a detailed breakdown of true and false predictions for",
        "type": "Document"
      },
      {
        "id": "ec651bfc-61bb-46ab-a708-6779938b6cdb",
        "metadata": {
          "vector_store_key": "1701.02877-6",
          "chunk_id": 62,
          "document_id": "1701.02877",
          "start_idx": 22362,
          "end_idx": 22802
        },
        "page_content": " data gives little to no improvement, whereas adding positive examples drastically improves performance. Table UID9 shows results with size normalised precision (P), recall (R), and F1-Score (F1). The five lowest P, R and F1 values per method (CRFSuite, Stanford NER, SENNA) are in bold to highlight underperformers. Results for all corpora are summed with macro average. Comparing the different methods, the highest F1 results are achieved",
        "type": "Document"
      },
      {
        "id": "c0a36e83-6a5f-4829-b712-316da2b2e885",
        "metadata": {
          "vector_store_key": "1910.03814-8",
          "chunk_id": 51,
          "document_id": "1910.03814",
          "start_idx": 19558,
          "end_idx": 20033
        },
        "page_content": " results for the LSTM, for the Davison method proposed in BIBREF7 trained with MMHS150K, and for random scores. Fig. FIGREF32 shows the Precision vs Recall plot and the ROC curve (which plots the True Positive Rate vs the False Positive Rate) of the different models. First, notice that given the subjectivity of the task and the discrepancies between annotators, getting optimal scores in the evaluation metrics is virtually impossible. However, a system with relatively low",
        "type": "Document"
      },
      {
        "id": "29b3764c-9471-4220-b6e2-e2140032472a",
        "metadata": {
          "vector_store_key": "1703.09684-7",
          "chunk_id": 68,
          "document_id": "1703.09684",
          "start_idx": 25413,
          "end_idx": 25866
        },
        "page_content": " distribution to obtain high scores. We found that for MCB-A, the accuracy on subordinate object recognition drops from 85.54% with unnormalized to 23.22% with normalized, and for scene recognition it drops from 93.06% (unnormalized) to 38.53% (normalized). Both these categories have a heavily skewed answer distribution; the top-25 answers in subordinate object recognition and the top-5 answers in scene recognition cover over 80% of all questions in",
        "type": "Document"
      },
      {
        "id": "ce376bfe-d8c2-496b-b75c-ec218519a887",
        "metadata": {
          "vector_store_key": "1703.09684-3",
          "chunk_id": 62,
          "document_id": "1703.09684",
          "start_idx": 23131,
          "end_idx": 23590
        },
        "page_content": " Accuracy scores are given for each of the 12 question-types in Table 3 , and scores that are normalized by using mean-per-unique-answer are given in appendix Table 5 . By inspecting Table 3 , we can see that some question-types are comparatively easy ( $>90$ %) under MPT: scene recognition, sport recognition, and object presence. High accuracy is also achieved on absurd, which we discuss in greater detail in Sec. \"Effects of Including Absurd Questions\" .",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the three datasets used in the paper?",
    "llm_answer": "Biomedical, Stanford Sentiment Treebank (SST-2) / Yelp, and Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), and Quora Question Pairs (ASQ).\n",
    "context": [
      {
        "id": "c5eaf502-a11a-40bd-84a8-78f62e8d8d02",
        "metadata": {
          "vector_store_key": "1701.00185-9",
          "chunk_id": 10,
          "document_id": "1701.00185",
          "start_idx": 3790,
          "end_idx": 4304
        },
        "page_content": " this paper we use a new short text dataset, Biomedical, in the experiment to verify the effectiveness of our approach. Third, we put much effort on studying the influence of various different semantic features integrated in our self-taught CNN framework, which is not involved in the conference paper. For the purpose of reproducibility, we make the datasets and software used in our experiments publicly available at the website. The remainder of this paper is organized as follows: In Section SECREF2 , we first",
        "type": "Document"
      },
      {
        "id": "425bc893-08e9-4be5-a0d9-da3a54b2078f",
        "metadata": {
          "vector_store_key": "1604.05781-8",
          "chunk_id": 8,
          "document_id": "1604.05781",
          "start_idx": 3160,
          "end_idx": 3653
        },
        "page_content": " organized as follows: In Sec. \"Materials and Methods\" we discuss our materials and methods, including the dataset we studied, how we preprocessed that data and extracted a `causal' corpus and a corresponding `control' corpus, and the details of the statistical and language analysis tools we studied these corpora with. In Sec. \"Results\" we present results using these tools to compare the causal statements to control statements. We conclude with a discussion in Sec. \"Discussion\" . Data was",
        "type": "Document"
      },
      {
        "id": "c3435ddc-e5cd-4c15-88a6-79bf7cb93888",
        "metadata": {
          "vector_store_key": "1911.03562-5",
          "chunk_id": 4,
          "document_id": "1911.03562",
          "start_idx": 1524,
          "end_idx": 2018
        },
        "page_content": " are based on information about the papers taken directly from AA (as of June 2019) and citation information extracted from Google Scholar (as of June 2019). Thus, all subsequent papers and citations are not included in the analysis. A fresh data collection is planned for January 2020.  Interactive Visualizations: The visualizations we are developing for this work (using Tableau) are interactive\u2014so one can hover, click to select and filter, move sliders, etc. Since this work is high in the",
        "type": "Document"
      },
      {
        "id": "19ad6a1f-7f24-400e-9235-38467b0ba391",
        "metadata": {
          "vector_store_key": "1910.00458-7",
          "chunk_id": 51,
          "document_id": "1910.00458",
          "start_idx": 17496,
          "end_idx": 17916
        },
        "page_content": "500). For the sentiment analysis category, we used the Stanford Sentiment Treebank (SST-2) dataset from the GLUE benchmark BIBREF22 (around 60k train examples) and the Yelp dataset (around 430k train examples). For the paraphrase category, three paraphrasing datasets are used from the GLUE benchmark: Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), and Quora Question Pairs (",
        "type": "Document"
      },
      {
        "id": "401df3ea-241b-476d-97f4-1276dc131769",
        "metadata": {
          "vector_store_key": "1701.00185-9",
          "chunk_id": 99,
          "document_id": "1701.00185",
          "start_idx": 35418,
          "end_idx": 35678
        },
        "page_content": "ASQ for making the datasets available. This work is supported by the National Natural Science Foundation of China (No. 61602479, No. 61303172, No. 61403385) and the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDB02070005).",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is improvement in accuracy for short Jokes in relation other types of jokes?",
    "llm_answer": "The Transformer model achieved 98.6% accuracy on the Short Jokes dataset, an 8% improvement over the previous best CNN model.\n",
    "context": [
      {
        "id": "cba75883-f676-4aa4-a1ed-526540abbf5e",
        "metadata": {
          "vector_store_key": "1909.00252-2",
          "chunk_id": 23,
          "document_id": "1909.00252",
          "start_idx": 8852,
          "end_idx": 9334
        },
        "page_content": " joke was most important for humor. We found that all of the models, including humans, relied more on the punchline of the joke in their predictions (Table 2). Thus, it seems that although both parts of the joke are needed for it to be humorous, the punchline carries higher weight than the body. We hypothesize that this is due to the variations found in the different joke bodies: some take paragraphs to set up the joke, while others are less than a sentence. Our experiment with",
        "type": "Document"
      },
      {
        "id": "0f3fe868-8ec9-42d0-abe9-4e793c6d3a4e",
        "metadata": {
          "vector_store_key": "1909.00252-1",
          "chunk_id": 16,
          "document_id": "1909.00252",
          "start_idx": 6079,
          "end_idx": 6554
        },
        "page_content": " learn in a more balanced way despite the uneven amount of non-humorous jokes. Our validation and test sets were composed of the remaining 25%, downsampling the data into a 50/50 class split so that the accuracy metric could be balanced and easily understood. To show how our model compares to the previous work done, we also test on the Short Joke and Pun datasets mentioned in the Data section. For these datasets we will use the metrics (Accuracy, Precision, Recall, and F",
        "type": "Document"
      },
      {
        "id": "30cc8447-25a5-4b88-baaa-b8dc4a984302",
        "metadata": {
          "vector_store_key": "1909.00252-1",
          "chunk_id": 24,
          "document_id": "1909.00252",
          "start_idx": 9136,
          "end_idx": 9591
        },
        "page_content": " some take paragraphs to set up the joke, while others are less than a sentence. Our experiment with the Short Jokes dataset found the Transformer model's accuracy and F1 score to be 0.986. This was a jump of 8 percent from the most recent work done with CNNs (Table 4). The results on the Pun of the Day dataset are shown in Table 3 above. It shows an accuracy of 93 percent, close to 4 percent greater accuracy than the best CNN model proposed. Although",
        "type": "Document"
      },
      {
        "id": "79ba2350-56bc-4f14-8eb6-7af18b58900d",
        "metadata": {
          "vector_store_key": "1909.00252-2",
          "chunk_id": 29,
          "document_id": "1909.00252",
          "start_idx": 11191,
          "end_idx": 11743
        },
        "page_content": " the idea of using machine learning tools, specifically a Transformer neural network architecture, to discern what jokes are funny and what jokes are not. This proposed model does not require any human interaction to determine, aside from the text of the joke itself, which jokes are humorous. This architecture can predict the level of humor for a specific audience to a higher degree than a general audience consensus. We also showed that this model has increased capability in joke identification as a result, with higher accuracy and F1 scores than",
        "type": "Document"
      },
      {
        "id": "b231f0ae-5345-4c74-a9df-4f1aff2884b3",
        "metadata": {
          "vector_store_key": "1909.00252-2",
          "chunk_id": 27,
          "document_id": "1909.00252",
          "start_idx": 10399,
          "end_idx": 10893
        },
        "page_content": " forum. This would suggest that humor can be learned for a specific subset of the population. The model's high accuracy and F1 scores on the Short Jokes and Pun of the Day dataset show the effectiveness of the model for transfer learning. This result is not terribly surprising. If the model can figure out which jokes are funny, it seems to be an easier task to tell when something isn't a joke at all. Although these results have high potential, defining the absolute truth value for a joke's",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What baseline did they compare Entity-GCN to?",
    "llm_answer": "Many techniques that use expensive question-aware recurrent document encoders.  BiDAF BIBREF3, FastQA BIBREF4.\n",
    "context": [
      {
        "id": "97b652bb-5ef9-4cf6-b96d-411ab979074b",
        "metadata": {
          "vector_store_key": "1808.09920-3",
          "chunk_id": 11,
          "document_id": "1808.09920",
          "start_idx": 4290,
          "end_idx": 4787
        },
        "page_content": " Entity-GCN still performs on par with many techniques that use expensive question-aware recurrent document encoders. Despite not using recurrent document encoders, the full Entity-GCN model achieves over 2% improvement over the best previously-published results. As our model is efficient, we also reported results of an ensemble which brings further 3.6% of improvement and only 3% below the human performance reported by BIBREF0 . Our contributions can be summarized as follows: In this section",
        "type": "Document"
      },
      {
        "id": "bbdb55bb-c035-41de-9000-8a259b5601a6",
        "metadata": {
          "vector_store_key": "1808.09920-3",
          "chunk_id": 66,
          "document_id": "1808.09920",
          "start_idx": 22186,
          "end_idx": 22698
        },
        "page_content": " advantage. This is our best system evaluating on the development. Since Entity-GCN seems to gain little advantage using the coreference system, we report test results both with and without using it. Surprisingly, with coreference, we observe performance degradation on the test set. It is likely that the test documents are harder for the coreference system. We do perform one last ablation, namely, we replace our heuristic for assigning edges and their labels by a model component that predicts them. The last",
        "type": "Document"
      },
      {
        "id": "0444f872-24e7-44b0-a895-6c012d434d5e",
        "metadata": {
          "vector_store_key": "1808.09920-3",
          "chunk_id": 76,
          "document_id": "1808.09920",
          "start_idx": 25894,
          "end_idx": 26375
        },
        "page_content": " Pearson's correlation). This is important as document sets can be large in practical applications. See Figure 3 in the supplemental material for plots. In Table 6 , we report three samples from WikiHop development set where out Entity-GCN fails. In particular, we show two instances where our model presents high confidence on the answer, and one where is not. We commented these samples explaining why our model might fail in these cases. In previous work, BiDAF BIBREF3 , FastQA",
        "type": "Document"
      },
      {
        "id": "fe24566f-400f-451e-87cf-21be9681d88f",
        "metadata": {
          "vector_store_key": "1808.09920-3",
          "chunk_id": 10,
          "document_id": "1808.09920",
          "start_idx": 3800,
          "end_idx": 4290
        },
        "page_content": ". This implies that only a lightweight computation has to be performed online, both at train and test time, whereas the rest is preprocessed. Even in the somewhat contrived WikiHop setting, where fairly small sets of candidates are provided, the model is at least 5 times faster to train than BiDAF. Interestingly, when we substitute ELMo with simple pre-trained word embeddings, Entity-GCN still performs on par with many techniques that use expensive question-aware recurrent document enc",
        "type": "Document"
      },
      {
        "id": "36b15ed0-c8af-4637-a752-7e3e4b5c1846",
        "metadata": {
          "vector_store_key": "1808.09920-3",
          "chunk_id": 65,
          "document_id": "1808.09920",
          "start_idx": 21894,
          "end_idx": 22391
        },
        "page_content": " document. Secondly, we notice that coreference links and complement edges seem to play a more marginal role. Though it may be surprising for coreference edges, recall that the MATCH heuristic already captures the easiest coreference cases, and for the rest the out-of-domain coreference system may not be reliable. Still, modelling all these different relations together gives our Entity-GCN a clear advantage. This is our best system evaluating on the development. Since Entity-GCN seems to gain",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How did they get relations between mentions?",
    "llm_answer": "The context describes associating mentions by encoding selected entity information, but does not specify the method for obtaining relations.\n",
    "context": [
      {
        "id": "bcda11da-68e2-42e7-abce-85740ed52515",
        "metadata": {
          "vector_store_key": "1902.00330-1",
          "chunk_id": 99,
          "document_id": "1902.00330",
          "start_idx": 33470,
          "end_idx": 33958
        },
        "page_content": " the pairs of mentions at most once, they assume each mention only needs to be consistent with one another mention in the document. The limitation of their method is that the consistency information is too sparse, resulting in low confidence. Similar to us, Guo et al. BIBREF18 also sort mentions according to the difficulty of disambiguation, but they did not make full use of the information of previously referred entities for the subsequent entity disambiguation. Nguyen et al. BIBREF",
        "type": "Document"
      },
      {
        "id": "ef2edcd1-66cb-49d2-8627-d3326eabc6a6",
        "metadata": {
          "vector_store_key": "1902.00330-1",
          "chunk_id": 87,
          "document_id": "1902.00330",
          "start_idx": 28375,
          "end_idx": 28918
        },
        "page_content": " most of datasets, indicating that it is effective to place the mention that with a higher local similarity in front of the sequence. It is worth noting that the effect of ranking mentions is not obvious on the MSNBC dataset, the reason is that most of mentions in MSNBC have similar local similarities, the order of disambiguation has little effect on the final result. Most of previous methods mainly use the similarities between entities to correlate each other, but our model associates them by encoding the selected entity information. To",
        "type": "Document"
      },
      {
        "id": "607450a7-b435-43c5-a17e-e17ce93f1e43",
        "metadata": {
          "vector_store_key": "1808.09920-1",
          "chunk_id": 20,
          "document_id": "1808.09920",
          "start_idx": 7321,
          "end_idx": 7717
        },
        "page_content": "cup \\lbrace s\\rbrace $ and create one node per mention. This process is based on the following heuristic: we consider mentions spans in $S_q$ exactly matching an element of $C_q \\cup \\lbrace s\\rbrace $ . Admittedly, this is a rather simple strategy which may suffer from low recall. we use predictions from a coreference resolution system to add mentions of elements in $C_q \\cup \\lbrace s\\rbrace",
        "type": "Document"
      },
      {
        "id": "cdfb1b4d-a4c8-4127-a98b-0b86b7df107d",
        "metadata": {
          "vector_store_key": "1603.04553-4",
          "chunk_id": 13,
          "document_id": "1603.04553",
          "start_idx": 4332,
          "end_idx": 4638
        },
        "page_content": " separately for each mention: $\nc^*_j = \\operatornamewithlimits{argmax}\\limits _{c_j} P(m_j|m_{c_j}) P(c_j|j)\n$  The model is a so-called ranking model because it is able to identify the most probable candidate antecedent given a mention to be resolved. According to previous work BIBREF17 , BIBREF18 , BIB",
        "type": "Document"
      },
      {
        "id": "f434de0c-800a-42f0-b3e3-f358bed443ef",
        "metadata": {
          "vector_store_key": "1605.08675-3",
          "chunk_id": 39,
          "document_id": "1605.08675",
          "start_idx": 15409,
          "end_idx": 15878
        },
        "page_content": " extracted (ENTITY RECOGNITION). For each of them, a context is generated (CONTEXT GENERATION). Finally, a distance between a question content and the entity context is computed to asses its relevance (DISTANCE MEASURE). All the mentions and their distance scores are stored and, after no more documents are left, used to select the best match (BEST ENTITY SELECTION). The system returns the entity, supplied with information about a supporting sentence and a document,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How did they detect entity mentions?",
    "llm_answer": "An entity recognition step is performed within the question answering process and aims at selecting all entity mentions in a given annotated document.\n",
    "context": [
      {
        "id": "f434de0c-800a-42f0-b3e3-f358bed443ef",
        "metadata": {
          "vector_store_key": "1605.08675-3",
          "chunk_id": 39,
          "document_id": "1605.08675",
          "start_idx": 15409,
          "end_idx": 15878
        },
        "page_content": " extracted (ENTITY RECOGNITION). For each of them, a context is generated (CONTEXT GENERATION). Finally, a distance between a question content and the entity context is computed to asses its relevance (DISTANCE MEASURE). All the mentions and their distance scores are stored and, after no more documents are left, used to select the best match (BEST ENTITY SELECTION). The system returns the entity, supplied with information about a supporting sentence and a document,",
        "type": "Document"
      },
      {
        "id": "a6f90624-36c3-4702-83ce-6eb77a791655",
        "metadata": {
          "vector_store_key": "1902.00330-1",
          "chunk_id": 105,
          "document_id": "1902.00330",
          "start_idx": 35978,
          "end_idx": 36422
        },
        "page_content": " In experiments, we evaluate our method on AIDA-B and other well-known datasets, the results show that our system outperforms state-of-the-art solutions. In the future, we would like to use reinforcement learning to detect mentions and determine which mention should be firstly disambiguated in the document. This research is supported by the GS501100001809National Key Research and Development Program of China (No. GS5011000018092018YFB100470",
        "type": "Document"
      },
      {
        "id": "dd32829c-a0df-46b6-914d-e8ba980decfb",
        "metadata": {
          "vector_store_key": "1605.08675-1",
          "chunk_id": 65,
          "document_id": "1605.08675",
          "start_idx": 25068,
          "end_idx": 25585
        },
        "page_content": "ERF, Liner2 and Quant) or novel DeepER; this choice affects its overall performance. Experiments showing precision and recall of the whole system with respect to applied entity recognition technique are demonstrated in section SECREF88 . An entity recognition step is performed within the question answering process and aims at selecting all entity mentions in a given annotated document. Before it begins, the entity library is read into a PATRICIA trie, a very efficient prefix tree. In this structure, every entity",
        "type": "Document"
      },
      {
        "id": "273e8824-6985-4a56-8046-4c697fee9ffa",
        "metadata": {
          "vector_store_key": "1902.00330-2",
          "chunk_id": 20,
          "document_id": "1902.00330",
          "start_idx": 7750,
          "end_idx": 8140
        },
        "page_content": " and the corresponding candidate set $\\lbrace e_t^1, e_t^2,..., \\\\ e_t^k\\rbrace $ , we aim to get their local representation based on the mention context and the candidate entity description. For each mention, we firstly select its $n$ surrounding words, and represent them as word embedding using a pre-trained lookup table BIBREF11 . Then, we use Long Short-Term Memory (LSTM) networks to",
        "type": "Document"
      },
      {
        "id": "bcf70b1c-5a8d-4898-822b-b3d11f775acd",
        "metadata": {
          "vector_store_key": "2001.06888-0",
          "chunk_id": 6,
          "document_id": "2001.06888",
          "start_idx": 2502,
          "end_idx": 3003
        },
        "page_content": " named entity recognition in social media is a most challenging task because users tend to invent, mistype and epitomize words. Sometimes these words correspond to named entities which makes the recognition task even more difficult BIBREF7. In some cases, the context that carries the entity (surrounding words and related image) is more descriptive than the entity word presentation BIBREF8. To find a solution to the issues at hand, and keeping multimodal data in mind, recognition of named entities",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What performance does the Entity-GCN get on WIKIHOP?",
    "llm_answer": "Entity-GCN achieves over 2% improvement over the best previously-published results, and an ensemble brings a further 3.6% improvement, getting only 3% below human performance.\n",
    "context": [
      {
        "id": "fe24566f-400f-451e-87cf-21be9681d88f",
        "metadata": {
          "vector_store_key": "1808.09920-3",
          "chunk_id": 10,
          "document_id": "1808.09920",
          "start_idx": 3800,
          "end_idx": 4290
        },
        "page_content": ". This implies that only a lightweight computation has to be performed online, both at train and test time, whereas the rest is preprocessed. Even in the somewhat contrived WikiHop setting, where fairly small sets of candidates are provided, the model is at least 5 times faster to train than BiDAF. Interestingly, when we substitute ELMo with simple pre-trained word embeddings, Entity-GCN still performs on par with many techniques that use expensive question-aware recurrent document enc",
        "type": "Document"
      },
      {
        "id": "3ddba576-588a-4d35-a9c6-8faee39e35c3",
        "metadata": {
          "vector_store_key": "1808.09920-3",
          "chunk_id": 60,
          "document_id": "1808.09920",
          "start_idx": 19847,
          "end_idx": 20268
        },
        "page_content": " this experiment we investigate the effect of the different relations available in the entity graph and processed by the R-GCN module. We start off by testing our stronger encoder (i.e., ELMo) in absence of edges connecting mentions in the supporting documents (i.e., using only self-loops \u2013 No R-GCN in Table 3 ). The results suggest that WikipHop genuinely requires multihop inference, as our best model is 6.1% and 8.4",
        "type": "Document"
      },
      {
        "id": "97b652bb-5ef9-4cf6-b96d-411ab979074b",
        "metadata": {
          "vector_store_key": "1808.09920-3",
          "chunk_id": 11,
          "document_id": "1808.09920",
          "start_idx": 4290,
          "end_idx": 4787
        },
        "page_content": " Entity-GCN still performs on par with many techniques that use expensive question-aware recurrent document encoders. Despite not using recurrent document encoders, the full Entity-GCN model achieves over 2% improvement over the best previously-published results. As our model is efficient, we also reported results of an ensemble which brings further 3.6% of improvement and only 3% below the human performance reported by BIBREF0 . Our contributions can be summarized as follows: In this section",
        "type": "Document"
      },
      {
        "id": "0444f872-24e7-44b0-a895-6c012d434d5e",
        "metadata": {
          "vector_store_key": "1808.09920-3",
          "chunk_id": 76,
          "document_id": "1808.09920",
          "start_idx": 25894,
          "end_idx": 26375
        },
        "page_content": " Pearson's correlation). This is important as document sets can be large in practical applications. See Figure 3 in the supplemental material for plots. In Table 6 , we report three samples from WikiHop development set where out Entity-GCN fails. In particular, we show two instances where our model presents high confidence on the answer, and one where is not. We commented these samples explaining why our model might fail in these cases. In previous work, BiDAF BIBREF3 , FastQA",
        "type": "Document"
      },
      {
        "id": "114b18e8-a855-4d9a-a9d2-0cd9e0fa7543",
        "metadata": {
          "vector_store_key": "1808.09920-3",
          "chunk_id": 12,
          "document_id": "1808.09920",
          "start_idx": 4572,
          "end_idx": 5066
        },
        "page_content": " human performance reported by BIBREF0 . Our contributions can be summarized as follows: In this section we explain our method. We first introduce the dataset we focus on, WikiHop by BIBREF0 , as well as the task abstraction. We then present the building blocks that make up our Entity-GCN model, namely, an entity graph used to relate mentions to entities within and across documents, a document encoder used to obtain representations of mentions in context, and a relational graph convolution",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What document context was added?",
    "llm_answer": "Newspaper articles.\n",
    "context": [
      {
        "id": "5e531e13-2052-48e7-ae5d-fda774e22e05",
        "metadata": {
          "vector_store_key": "1910.07481-1",
          "chunk_id": 20,
          "document_id": "1910.07481",
          "start_idx": 7995,
          "end_idx": 8544
        },
        "page_content": " means that only a fraction of the source training data contains document context. The enhanced model learns to use document information only when it is available. All test sets contain well delimited documents, Baseline models are evaluated on standard corpora while Document models are evaluated on the same standard corpora that have been augmented with document context. We evaluate the English-German systems on newstest2017, newstest2018 and newstest2019 where documents consist of newspaper articles to keep consistency with the training data",
        "type": "Document"
      },
      {
        "id": "e91c9449-842e-4dc8-a9bf-287b88763bf0",
        "metadata": {
          "vector_store_key": "1910.07481-1",
          "chunk_id": 34,
          "document_id": "1910.07481",
          "start_idx": 13011,
          "end_idx": 13548
        },
        "page_content": " document embeddings described by Eq. DISPLAY_FORM2 must be preserved for the model to fully benefit from document context. In this analysis we present some of the many cases that suggest the Document model can handle ambiguous situations. These examples are often isolated sentences where even a human translator could not predict the good translation without looking at the document, making it almost impossible for the Baseline model which is blind to the context. Table TABREF10 contains an extract of these interesting cases for the",
        "type": "Document"
      },
      {
        "id": "19ae6dc2-a25d-4c8e-987c-b41b83761df0",
        "metadata": {
          "vector_store_key": "1906.03338-7",
          "chunk_id": 43,
          "document_id": "1906.03338",
          "start_idx": 16306,
          "end_idx": 16766
        },
        "page_content": " discourse markers indicating an opposite class. In other words, in this setting we want to examine effects when porting a context-sensitive system to a multi-document setting. For example, as seen in Figure FIGREF42 , the context of an argumentative unit may change from \u201cHowever\u201d to \u201cMoreover\u201d \u2013 which can happen naturally in open debates. The results are displayed in Figure FIGREF43 . In the standard setting (Figure FIGREF43 ), the models that have access",
        "type": "Document"
      },
      {
        "id": "064c91e3-4705-402a-ad1a-d60baea47204",
        "metadata": {
          "vector_store_key": "1707.03904-2",
          "chunk_id": 21,
          "document_id": "1707.03904",
          "start_idx": 8059,
          "end_idx": 8503
        },
        "page_content": "Scale $485 day + Picasso Painting\u201d) to debatable (\u201cAccording to Earth Medicine what's the birth totem for march\u201d \u201cThe Falcon\u201d) The context document for each record consists of a list of ranked and scored pseudodocuments relevant to the question. Context documents for each query were generated in a two-phase fashion, first collecting a large pool of semirelevant text, then filling a temporary index with short or long pseudodocuments from the",
        "type": "Document"
      },
      {
        "id": "a272483f-3054-4556-9359-e728bfa96316",
        "metadata": {
          "vector_store_key": "2004.01694-1",
          "chunk_id": 44,
          "document_id": "2004.01694",
          "start_idx": 17239,
          "end_idx": 17622
        },
        "page_content": "x=86, n=189, p=.244$). This is in accordance with BIBREF3, who found the same in a source-based direct assessment experiment with crowd workers. With the availability of document-level context, however, preference for MT$_1$ drops from 49.5 to 37.0 % and is significantly lower than preference for human translation ($x=104, n=178, p<.05$). This evidences that document-level context",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What were the results of the first experiment?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "d1c89bac-6a19-4b4e-8af7-215ed859ba92",
        "metadata": {
          "vector_store_key": "1803.02839-4",
          "chunk_id": 118,
          "document_id": "1803.02839",
          "start_idx": 45558,
          "end_idx": 45772
        },
        "page_content": " mathematical consistency of the experiments, and provided useful insights into the results. Mark prompted the investigation into potential emergent monoid structures since they appear frequently in state machines.",
        "type": "Document"
      },
      {
        "id": "17f3676f-8f5c-4da9-b0c7-cede94c8171d",
        "metadata": {
          "vector_store_key": "1904.05862-2",
          "chunk_id": 42,
          "document_id": "1904.05862",
          "start_idx": 15343,
          "end_idx": 15454
        },
        "page_content": "letter++ experiments, and Tatiana Likhomanenko for providing convolutional language models for our experiments.",
        "type": "Document"
      },
      {
        "id": "9a66176f-fe2e-4da0-a2df-70a3d739924f",
        "metadata": {
          "vector_store_key": "1802.05322-2",
          "chunk_id": 38,
          "document_id": "1802.05322",
          "start_idx": 13905,
          "end_idx": 14341
        },
        "page_content": " explanation of the whole process needed to conduct an experiment like this. The process includes downloading a data set, web scraping for extra information, data preprocessing, model tuning and evaluation of the results. Action Adult Adventure Animation Biography Comedy Crime Documentary Drama Family Fantasy Film-Noir Game-Show History Horror Music Musical Mystery Reality-TV Romance Sci-Fi Short Sport Talk-Show Thriller War Western",
        "type": "Document"
      },
      {
        "id": "425bc893-08e9-4be5-a0d9-da3a54b2078f",
        "metadata": {
          "vector_store_key": "1604.05781-8",
          "chunk_id": 8,
          "document_id": "1604.05781",
          "start_idx": 3160,
          "end_idx": 3653
        },
        "page_content": " organized as follows: In Sec. \"Materials and Methods\" we discuss our materials and methods, including the dataset we studied, how we preprocessed that data and extracted a `causal' corpus and a corresponding `control' corpus, and the details of the statistical and language analysis tools we studied these corpora with. In Sec. \"Results\" we present results using these tools to compare the causal statements to control statements. We conclude with a discussion in Sec. \"Discussion\" . Data was",
        "type": "Document"
      },
      {
        "id": "c5eaf502-a11a-40bd-84a8-78f62e8d8d02",
        "metadata": {
          "vector_store_key": "1701.00185-9",
          "chunk_id": 10,
          "document_id": "1701.00185",
          "start_idx": 3790,
          "end_idx": 4304
        },
        "page_content": " this paper we use a new short text dataset, Biomedical, in the experiment to verify the effectiveness of our approach. Third, we put much effort on studying the influence of various different semantic features integrated in our self-taught CNN framework, which is not involved in the conference paper. For the purpose of reproducibility, we make the datasets and software used in our experiments publicly available at the website. The remainder of this paper is organized as follows: In Section SECREF2 , we first",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does their model outperform existing methods?",
    "llm_answer": "Our model outperforms the previous feature engineering benchmark and other work in terms of F1 score and accuracy of detection.  No specific percentage is given.\n",
    "context": [
      {
        "id": "b8463b23-0372-4422-ba8d-e82788fecd2b",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 61,
          "document_id": "1909.13375",
          "start_idx": 20788,
          "end_idx": 21299
        },
        "page_content": " For the other types, most of the improvement came from better preprocessing. A more detailed discussion could be found in Section (SECREF36). Notice that different BERTlarge models were used, so the comparison is less direct. Overall, our large models exhibits similar results to those of MTMSNlarge. For multi-span questions we achieve a significantly better performance. While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still",
        "type": "Document"
      },
      {
        "id": "555a266e-0a08-43e5-b719-8cd2fd10ebb8",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "fb01b160-a168-4070-9f5a-fd61cc41f56f",
        "metadata": {
          "vector_store_key": "2002.06644-2",
          "chunk_id": 17,
          "document_id": "2002.06644",
          "start_idx": 6467,
          "end_idx": 6878
        },
        "page_content": "ERT based models, and BERT-based ensemble models outperform the baselines for all the metrics. Among the optimized BERT based models, $RoBERTa_{large}$ outperforms all other non-ensemble models and the baselines for all metrics. It further achieves a maximum recall of $0.681$ for all the proposed models. We note that DistillRoBERTa, a distilled model, performs competitively, achieving $69.69\\%$ accuracy, and",
        "type": "Document"
      },
      {
        "id": "6e5dd4a7-27b3-4d21-8529-fac0bc4ba1f2",
        "metadata": {
          "vector_store_key": "1901.02257-9",
          "chunk_id": 60,
          "document_id": "1901.02257",
          "start_idx": 18849,
          "end_idx": 19405
        },
        "page_content": " Our ensemble model surpasses the current state-of-the-art model with an accuracy of 84.84%. We got the final ensemble result by voting on 4 single models. Every single model uses the same architecture but different parameters. To study the effectiveness of each perspective, we conduct several experiments on the three single perspectives and their combination perspective. Table 3 presents their comparison results. The first group of models are based on the three single perspectives, and we can observe that the union perspective performs best compared",
        "type": "Document"
      },
      {
        "id": "777a828f-4895-4597-8f11-f23765090ec8",
        "metadata": {
          "vector_store_key": "1710.01507-3",
          "chunk_id": 41,
          "document_id": "1710.01507",
          "start_idx": 14782,
          "end_idx": 15280
        },
        "page_content": " Table 1, we compare our model with the existing state-of-the-art for the dataset used and other models which have employed similar techniques to accomplish the task. Calculation and comparison across these metrics was conducted on TIRA BIBREF2 , a platform that offers evaluation as a service. It is clear that our proposed model outperforms the previous feature engineering benchmark and other work done in the field both in terms of F1 score and accuracy of detection. In this work, we have come",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance of their model?",
    "llm_answer": "Their large models exhibit a substantial improvement across all metrics compared to base models, surpassing the NABERT+ baseline.  They achieve significantly better performance on multi-span questions.  Results are similar to MTMSNlarge, but better on multi-span questions.  The larger model has a similar computational budget to the best published model and similar training times.\n",
    "context": [
      {
        "id": "555a266e-0a08-43e5-b719-8cd2fd10ebb8",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "31e3f6a2-9e9b-4d6d-824a-22195518284a",
        "metadata": {
          "vector_store_key": "1909.02764-7",
          "chunk_id": 52,
          "document_id": "1909.02764",
          "start_idx": 20420,
          "end_idx": 20841
        },
        "page_content": "=73 % on TEC. The model trained on our AMMER corpus achieves an F1 score of 57%. This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set. These results are mostly in line with Bostan2018. Now we analyze how well the models trained in Experiment 1 perform when applied to our data set. The results are shown in column \u201cSimple\u201d in Table TAB",
        "type": "Document"
      },
      {
        "id": "b8463b23-0372-4422-ba8d-e82788fecd2b",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 61,
          "document_id": "1909.13375",
          "start_idx": 20788,
          "end_idx": 21299
        },
        "page_content": " For the other types, most of the improvement came from better preprocessing. A more detailed discussion could be found in Section (SECREF36). Notice that different BERTlarge models were used, so the comparison is less direct. Overall, our large models exhibits similar results to those of MTMSNlarge. For multi-span questions we achieve a significantly better performance. While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still",
        "type": "Document"
      },
      {
        "id": "3297fd18-2489-4a4d-a7b2-b3f620fcda80",
        "metadata": {
          "vector_store_key": "1701.06538-2",
          "chunk_id": 88,
          "document_id": "1701.06538",
          "start_idx": 32330,
          "end_idx": 32790
        },
        "page_content": " INLINEFORM1 respectively, showing that even in the presence of a large MoE, more computation is still useful. Results are reported at the bottom of Table TABREF76 . The larger of the two models has a similar computational budget to the best published model from the literature, and training times are similar. Comparing after 10 epochs, our model has a lower test perplexity by INLINEFORM2 . The models are similar in structure to the 8-million-operations-per",
        "type": "Document"
      },
      {
        "id": "638d3f9b-929b-4fa6-b0d4-efe44cd66255",
        "metadata": {
          "vector_store_key": "1912.13109-2",
          "chunk_id": 25,
          "document_id": "1912.13109",
          "start_idx": 10203,
          "end_idx": 10681
        },
        "page_content": " metrics - Recall rate, F1 score and Overall accuracy. Choice of model parameters were in the above models were inspired from previous work done but then were tuned to the best performance of the Test dataset. Following parameters were considered for tuning. Learning rate: Based on grid search the best performance was achieved when learning rate was set to 0.01. This value was arrived by a grid search on lr parameter. Number of Bidirectional LSTM units: A set of 32, 64, 128",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they damage different neural modules?",
    "llm_answer": "The provided text describes damaging the analog of Broca's area (the LSTMs).  No other methods of damaging neural modules are mentioned.\n",
    "context": [
      {
        "id": "81c4eacc-3caf-4a64-af42-2baa404d611a",
        "metadata": {
          "vector_store_key": "2001.07263-4",
          "chunk_id": 7,
          "document_id": "2001.07263",
          "start_idx": 2754,
          "end_idx": 3239
        },
        "page_content": "ization by suppressing some of the effects of static noise on the targets. Dropout randomly deactivates neurons with a predefined probability in every training step BIBREF13 to reduce co-adaptation of neurons. DropConnect, which is similar in spirit to dropout, randomly deactivates connections between neurons by temporarily zeroing out weights BIBREF14. Zoneout, which is also inspired by dropout and was especially developed for recurrent models BIBREF15, stochastically forces some",
        "type": "Document"
      },
      {
        "id": "cceb89ac-2c08-4891-a3a3-1909d324b02c",
        "metadata": {
          "vector_store_key": "1911.11744-3",
          "chunk_id": 24,
          "document_id": "1911.11744",
          "start_idx": 9158,
          "end_idx": 9592
        },
        "page_content": " $$ after being processed in a single perceptron with weight $_G$ and bias $_G$. These parameters are then used in the third part of the network, which is a DMP BIBREF0, allowing us leverage a large body of research regarding their behavior and stability, while also allowing other extensions of DMPs BIBREF5, BIBREF24, BIBREF25 to be incorporated to our framework. We evaluate our model in a simulated binning task in which the robot",
        "type": "Document"
      },
      {
        "id": "df54b3ec-decd-4df4-9deb-cf9ec5ba74e1",
        "metadata": {
          "vector_store_key": "2002.08899-2",
          "chunk_id": 29,
          "document_id": "2002.08899",
          "start_idx": 10466,
          "end_idx": 10912
        },
        "page_content": " damaged to resemble the damage that causes aphasia. To avoid cherry-picking the results, we arbitrarily chose an input for each domain, subject to the following constraint. The input is not in the train set and the undamaged LLA-LSTM model produces a translation that we judge to be correct. For all inputs that we chose, damage to the analog of Broca's area (the LSTMs) results in an output that describes content only if it is described by the",
        "type": "Document"
      },
      {
        "id": "83f03968-2b86-4d6b-8604-15b441459a3d",
        "metadata": {
          "vector_store_key": "1909.02560-3",
          "chunk_id": 65,
          "document_id": "1909.02560",
          "start_idx": 24644,
          "end_idx": 25212
        },
        "page_content": "ially trained models, as shown in Table TABREF18. After adversarial training, the performance of all the target models raises significantly, while that on the original examples remain comparable. Note that since the focus of this paper is on model robustness which can hardly be reflected in original data, we do not expect performance improvement on original data. The results demonstrate that adversarial training with our adversarial examples can significantly improve the robustness we focus on without remarkably hurting the performance on original data. Moreover",
        "type": "Document"
      },
      {
        "id": "90f74b6b-2514-4589-b152-cbd418027917",
        "metadata": {
          "vector_store_key": "2001.06888-3",
          "chunk_id": 28,
          "document_id": "2001.06888",
          "start_idx": 10924,
          "end_idx": 11447
        },
        "page_content": " and reduces spelling noise. It also helps the model to have an embedding when pretrained word embedding is empty or initialized as random for new words. These words are encountered when they were not present in the training set, thus, in the test phase, model fails to provide a useful embedding. Multimodal learning has become an emerging research interest and with the rise of deep learning techniques, it has become more visible in different research areas ranging from medical imaging to image segmentation and natural",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How long are the datasets?",
    "llm_answer": "The context provides various dataset sizes, but no single answer to \"how long\" the datasets are.  Different datasets have different numbers of documents, tokens, and characters.\n",
    "context": [
      {
        "id": "3eaf84ae-14a5-4380-9cf2-5affad82b419",
        "metadata": {
          "vector_store_key": "2004.01980-5",
          "chunk_id": 39,
          "document_id": "2004.01980",
          "start_idx": 13028,
          "end_idx": 13433
        },
        "page_content": " romantic, and click-baity text. The average sentence length in the NYT, CNN, Humor, Romance, and Clickbait datasets are 8.8, 9.2, 12.6, 11.6 and 8.7 words, respectively. The source dataset contains news articles paired with corresponding headlines. To enrich the training corpus, we combine two datasets: the New York Times (56K) and CNN (90K). After combining these two datasets, we randomly selected 3,",
        "type": "Document"
      },
      {
        "id": "2e1622b2-1517-4239-8581-693f3aece329",
        "metadata": {
          "vector_store_key": "1711.02013-3",
          "chunk_id": 55,
          "document_id": "1711.02013",
          "start_idx": 20658,
          "end_idx": 21116
        },
        "page_content": "ings, between recurrent layers, and on recurrent states were (0.7, 0.5, 0.5) respectively. dataset contains 17M training tokens and has a vocabulary size of 44k words. The dataset is partitioned into a training set (first 99M characters) and a development set (last 1M characters) that is used to report performance. As this dataset contains various articles from Wikipedia, the longer term information (such as current topic) plays a bigger role than in the",
        "type": "Document"
      },
      {
        "id": "9270562a-8e53-4e1d-b760-74d8fd0bc872",
        "metadata": {
          "vector_store_key": "1905.06566-7",
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "a2c17c40-d003-46d0-ba08-8966b42e8e23",
        "metadata": {
          "vector_store_key": "1906.03538-7",
          "chunk_id": 31,
          "document_id": "1906.03538",
          "start_idx": 12505,
          "end_idx": 12830
        },
        "page_content": ". We now provide a brief summary of [wave]390P[wave]415e[wave]440r[wave]465s[wave]485p[wave]525e[wave]535c[wave]595t[wave]610r[wave]635u[wave]660m. The dataset contains about INLINEFORM0 claims with a significant length diversity (Table TABREF19 ). Additionally, the dataset comes with INLINEFORM1 perspectives, most of which",
        "type": "Document"
      },
      {
        "id": "de28f623-e56b-46af-88a3-78e233bbcff1",
        "metadata": {
          "vector_store_key": "1607.06025-5",
          "chunk_id": 66,
          "document_id": "1607.06025",
          "start_idx": 24769,
          "end_idx": 25338
        },
        "page_content": " were constructed by filtering the generated datasets according to various thresholds. Thus, the generated datasets were constructed to contain enough examples, so that the filtered datasets had at least the number of examples as the original dataset. In the end, all the datasets were trimmed down to the size of the original dataset by selecting the samples sequentially from the beginning until the dataset had the right size. Also, the datasets were filtered so that each of the labels was represented equally. All the models, including classification and discrimin",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the sources of the data?",
    "llm_answer": "Online debate websites, Twitter, and Arabic WikiNews.\n",
    "context": [
      {
        "id": "eb1ce3aa-59c4-45e6-a3ea-77b0dadb95e7",
        "metadata": {
          "vector_store_key": "1906.03538-1",
          "chunk_id": 64,
          "document_id": "1906.03538",
          "start_idx": 25144,
          "end_idx": 25628
        },
        "page_content": " US Defense Advanced Research Projects Agency (DARPA). The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. We provide brief statistics on the sources of different content in our dataset in Table TABREF46 . In particular, this table shows: the size of the data collected from online debate websites (step 1). the size of the data filtered out (step 2a). the size of the perspectives added",
        "type": "Document"
      },
      {
        "id": "9b1b8448-a7df-43dc-a261-60a3dbf8974f",
        "metadata": {
          "vector_store_key": "2001.05970-1",
          "chunk_id": 6,
          "document_id": "2001.05970",
          "start_idx": 2564,
          "end_idx": 3064
        },
        "page_content": " methods of gathering data are limited in scale and might be influenced by the psychological and cognitive tendencies of respondents not to provide faithful answers BIBREF5. The ubiquity of social media has motivated various research on widely-debated social topics such as gang violence, hate code, or presidential election using Twitter data BIBREF6, BIBREF7, BIBREF8, BIBREF9. Recently, researchers have taken the earliest steps to understand sexual harassment using textual data on Twitter. Using",
        "type": "Document"
      },
      {
        "id": "f65849a0-dbae-4d4c-9691-5376c2bab712",
        "metadata": {
          "vector_store_key": "1906.03538-1",
          "chunk_id": 33,
          "document_id": "1906.03538",
          "start_idx": 13163,
          "end_idx": 13614
        },
        "page_content": " Table TABREF19 . To better understand the topical breakdown of claims in the dataset, we crowdsource the set of \u201ctopics\u201d associated with each claim (e.g., Law, Ethics, etc.) We observe that, as expected, the three topics of Politics, World, and Society have the biggest portions (Figure FIGREF21 ). Additionally, the included claims touch upon 10+ different topics. Figure FIGREF22 depicts a few popular categories and sampled questions from each. We",
        "type": "Document"
      },
      {
        "id": "048a3d0c-944a-4696-9c1d-8564b637bab2",
        "metadata": {
          "vector_store_key": "1910.12618-0",
          "chunk_id": 17,
          "document_id": "1910.12618",
          "start_idx": 7204,
          "end_idx": 7727
        },
        "page_content": " section 5 concludes this paper and gives insight on future possible work. In order to prove the consistency of our work, experiments have been conducted on two data sets, one for France and the other for the UK. In this section details about the text and time series data are given, as well as the major preprocessing steps. Three types of time series are considered in our work: national net electricity consumption (also referred as load or demand), national temperature and wind speed. The load data sets were retrieved",
        "type": "Document"
      },
      {
        "id": "81a9f7e7-9fa9-44a0-86e8-388530083a75",
        "metadata": {
          "vector_store_key": "1710.06700-3",
          "chunk_id": 17,
          "document_id": "1710.06700",
          "start_idx": 5155,
          "end_idx": 5651
        },
        "page_content": " allowing researches to evaluate using the same dataset that we used, and reproduce same experiments. To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site https://ar.wikinews.org/wiki. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the new labeling strategy?",
    "llm_answer": "A two-stage labeling strategy.  In the first stage, annotators label short texts with pure sentiment. In the second stage, a smaller number of texts with mixed sentiment are labeled by multiple annotators.\n",
    "context": [
      {
        "id": "a1ce2d6e-a00a-4755-a7ef-087c2ada2591",
        "metadata": {
          "vector_store_key": "1803.07771-5",
          "chunk_id": 4,
          "document_id": "1803.07771",
          "start_idx": 1808,
          "end_idx": 2303
        },
        "page_content": " issues with a new methodology. First, we introduce a two-stage labeling strategy for sentiment texts. In the first stage, annotators are invited to label a large number of short texts with relatively pure sentiment orientations. Each sample is labeled by only one annotator. In the second stage, a relatively small number of text samples with mixed sentiment orientations are annotated, and each sample is labeled by multiple annotators. Second, we propose a two-level long short-term memory (L",
        "type": "Document"
      },
      {
        "id": "d7e42451-1e06-43a9-b0d6-b5da094cdfa4",
        "metadata": {
          "vector_store_key": "1803.07771-4",
          "chunk_id": 92,
          "document_id": "1803.07771",
          "start_idx": 32711,
          "end_idx": 33231
        },
        "page_content": " are difficult for humans to label in text sentiment classification. In this study, a new labeling strategy is introduced to partition texts into those with pure and mixed sentiment orientations. These two categories of texts are labeled using different processes. A two-level network is accordingly proposed to utilize the two labeled data in our two-stage labeling strategy. Lexical cues (e.g., polar words, POS, conjunction words) are particularly useful in sentiment analysis. These lexical cues are used in our two-",
        "type": "Document"
      },
      {
        "id": "41a52ab5-0a22-481c-8202-3b6c933291e9",
        "metadata": {
          "vector_store_key": "1803.07771-5",
          "chunk_id": 64,
          "document_id": "1803.07771",
          "start_idx": 22848,
          "end_idx": 23303
        },
        "page_content": "stage labeling strategy. In the first stage, only one user is invited to label each clause sample as the sentiment orientations for clauses (or sub-sentences) are easy to label. In the second stage, five users are invited to label each text sample in the three raw data sets. The average score of the five users on each sample is calculated. Samples with average scores located in [0.6, 1] are labeled as \u201cpositive\". Samples with average scores located in",
        "type": "Document"
      },
      {
        "id": "1dbef2df-5fd5-4d81-b06c-1a34083db3d1",
        "metadata": {
          "vector_store_key": "1909.13362-8",
          "chunk_id": 17,
          "document_id": "1909.13362",
          "start_idx": 6449,
          "end_idx": 6905
        },
        "page_content": "-of-speech tagging, named-entity recognition, and chunking BIBREF17. Neural networks have recently outpaced more traditional methods in sequence labeling tasks. These neural-based approaches are taking the place of HMMs, maximum entropy Markov models (MEMM), and conditional random fields (CRF) BIBREF18. In the following section and in Fig. FIGREF1, we present a neural network architecture that leverages both recurrence and one-dimensional convolutions.",
        "type": "Document"
      },
      {
        "id": "4167b839-adb9-45a3-84b6-f5ccb12973c3",
        "metadata": {
          "vector_store_key": "2004.03744-4",
          "chunk_id": 74,
          "document_id": "2004.03744",
          "start_idx": 26850,
          "end_idx": 27381
        },
        "page_content": " their decision in a comprehensive and concise manner, using a subset of the words that they highlighted. The instructions are shown in Figure FIGREF42. Workers were also guided with three annotated examples, one for each label. For each assignment of 10 questions, one trusted annotation with gold standard label was inserted at a random position, as a measure to control the quality of label annotation. Each assignment was completed by three different workers. An example of question is shown in Figure FIGREF8 in the core paper",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are their changes evaluated?",
    "llm_answer": "Changes are evaluated by human judgment, with human translations judged more favorably and significantly better than machine translations when evaluation design is improved.\n",
    "context": [
      {
        "id": "c1484a9f-d7af-4d07-9461-1d96d91be2bc",
        "metadata": {
          "vector_store_key": "2004.01694-7",
          "chunk_id": 4,
          "document_id": "2004.01694",
          "start_idx": 1628,
          "end_idx": 2197
        },
        "page_content": " changes in the evaluation design affect the outcome of the human evaluation. We find that for all three aspects, human translations are judged more favourably, and significantly better than MT, when we make changes that we believe strengthen the evaluation design. Based on our empirical findings, we formulate a set of recommendations for human MT evaluation in general, and assessing human\u2013machine parity in particular. All of our data are made publicly available for external validation and further analysis. We first review current methods to assess the quality of",
        "type": "Document"
      },
      {
        "id": "0fcf38db-8036-49e9-9319-5de14182cf56",
        "metadata": {
          "vector_store_key": "1709.01256-7",
          "chunk_id": 53,
          "document_id": "1709.01256",
          "start_idx": 19942,
          "end_idx": 20473
        },
        "page_content": "e.g. addition, deletion or replacement), and save them as separate documents. These documents become revisions of the original documents. We started from an initial corpus that did not have revisions, and kept adding new documents and revising existing documents. Similar to a file system, at any moment new documents could be added and/or some of the current documents could be revised. The revision operations we used were deletion, addition and replacement of words, sentences, paragraphs, section names and document titles. The",
        "type": "Document"
      },
      {
        "id": "a2203714-1e91-4b9e-8237-3e2afa02553d",
        "metadata": {
          "vector_store_key": "1912.01673-1",
          "chunk_id": 21,
          "document_id": "1912.01673",
          "start_idx": 8152,
          "end_idx": 8670
        },
        "page_content": " several of its possible alternations, see tab:firstroundexamples. Unfortunately, these examples turned out to be highly influential on the annotators' decisions and they correspond to almost two thirds of all of modifications gathered in the first round. Other very common transformations include change of a word order or transformation into a interrogative/imperative sentence. Other interesting modification were also proposed such as change into a fairy-tale style, excessive use of diminutives/vulgarisms or dada",
        "type": "Document"
      },
      {
        "id": "c968364f-f16d-497b-9d76-5c0060e67220",
        "metadata": {
          "vector_store_key": "1709.01256-7",
          "chunk_id": 1,
          "document_id": "1709.01256",
          "start_idx": 578,
          "end_idx": 1123
        },
        "page_content": " It is worthwhile to develop a system that is able to intelligently identify, manage and represent revisions. Given a collection of text documents, our study identifies revision relationships in a completely unsupervised way. For each document in a corpus we only use its content and the last modified timestamp. We assume that a document can be revised by many users, but that the documents are not merged together. We consider collaborative editing as revising documents one by one. The two research problems that are most relevant to document",
        "type": "Document"
      },
      {
        "id": "44f30eb2-8556-495a-bc01-81ca1e7b2160",
        "metadata": {
          "vector_store_key": "1701.02877-0",
          "chunk_id": 83,
          "document_id": "1701.02877",
          "start_idx": 29760,
          "end_idx": 30227
        },
        "page_content": "; that is, the changes are both rapid and correspond to a large amount of surface form occurrences (e.g. BIBREF12 , BIBREF52 ). We examined the impact of drift in newswire and Twitter corpora, taking datasets based in different timeframes. The goal is to gauge how much diversity is due to new entities appearing over time. To do this, we used just the surface lexicalisations of entities as the entity representation. The overlap of surface forms was measured across",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the six target languages?",
    "llm_answer": "English, Spanish, and Zulu.\n",
    "context": [
      {
        "id": "d16841ee-127f-473f-af88-1d373637d8c0",
        "metadata": {
          "vector_store_key": "1910.05456-8",
          "chunk_id": 5,
          "document_id": "1910.05456",
          "start_idx": 1974,
          "end_idx": 2423
        },
        "page_content": " on the L1, i.e., the \"native language\", in neural network models. To this goal, we select a diverse set of eight source languages from different language families \u2013 Basque, French, German, Hungarian, Italian, Navajo, Turkish, and Quechua \u2013 and three target languages \u2013 English, Spanish and Zulu. We pretrain a neural sequence-to-sequence architecture on each of the source languages and then fine-tune the resulting models on small datasets in each",
        "type": "Document"
      },
      {
        "id": "1920df44-401c-4a6d-a53c-a400f8cc87f0",
        "metadata": {
          "vector_store_key": "1910.05456-2",
          "chunk_id": 27,
          "document_id": "1910.05456",
          "start_idx": 9884,
          "end_idx": 10340
        },
        "page_content": " additionally makes use of internal stem changes (e.g., o $\\rightarrow $ ue). Since English and Spanish are both Indo-European languages, and, thus, relatively similar, we further add a third, unrelated target language. We choose Zulu (ZUL), a Bantoid language. In contrast to the first two, it is strongly prefixing. For pretraining, we choose languages with different degrees of relatedness and varying morphological similarity to English, Spanish, and Z",
        "type": "Document"
      },
      {
        "id": "e90f75a5-f326-4202-82c5-878736af8308",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 51,
          "document_id": "1910.04269",
          "start_idx": 19553,
          "end_idx": 20023
        },
        "page_content": ", the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Kat",
        "type": "Document"
      },
      {
        "id": "7b1c09b6-d8db-4d35-8a92-0acba47cc1bc",
        "metadata": {
          "vector_store_key": "1909.13362-4",
          "chunk_id": 44,
          "document_id": "1909.13362",
          "start_idx": 15624,
          "end_idx": 16095
        },
        "page_content": " language groupings within families. We selected six evaluation languages: English, Dutch, Italian, French, Basque, and Manipuri. These represent two language families (Indo-European, Sino-Tibetan), a language isolate thought to be unrelated to any existing language (Basque), and two different subfamilies within the Indo-European family (West Germanic, Romance). The primary constraint was the availability of syllabified datasets for training and testing. Table TABREF",
        "type": "Document"
      },
      {
        "id": "b0d78a46-0cfa-4367-a963-2a9a8d82c9a4",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 18,
          "document_id": "1910.04269",
          "start_idx": 6853,
          "end_idx": 7342
        },
        "page_content": " comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the size of the released dataset?",
    "llm_answer": "287,226 documents for training.\n",
    "context": [
      {
        "id": "9270562a-8e53-4e1d-b760-74d8fd0bc872",
        "metadata": {
          "vector_store_key": "1905.06566-7",
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "0723454a-4062-463e-972a-81a58c6a504e",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "0d593f74-7474-41cc-9928-ce68ac96f714",
        "metadata": {
          "vector_store_key": "1802.05574-2",
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "2e1622b2-1517-4239-8581-693f3aece329",
        "metadata": {
          "vector_store_key": "1711.02013-3",
          "chunk_id": 55,
          "document_id": "1711.02013",
          "start_idx": 20658,
          "end_idx": 21116
        },
        "page_content": "ings, between recurrent layers, and on recurrent states were (0.7, 0.5, 0.5) respectively. dataset contains 17M training tokens and has a vocabulary size of 44k words. The dataset is partitioned into a training set (first 99M characters) and a development set (last 1M characters) that is used to report performance. As this dataset contains various articles from Wikipedia, the longer term information (such as current topic) plays a bigger role than in the",
        "type": "Document"
      },
      {
        "id": "84720de1-c988-45c6-bcbe-7a0435cdf6fb",
        "metadata": {
          "vector_store_key": "1706.08032-0",
          "chunk_id": 38,
          "document_id": "1706.08032",
          "start_idx": 14264,
          "end_idx": 14661
        },
        "page_content": " datasets in Table III. for all datasets, the filter window size ( INLINEFORM0 ) is 7 with 6 feature maps each for the first wide convolution layer, the second wide convolution layer has a filter window size of 5 with 14 feature maps each. Dropout rate ( INLINEFORM1 ) is 0.5, INLINEFORM2 constraint, learning rate is 0.1 and momentum of 0.9. Mini-batch size for STS Corpus is 100 and others are 4",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which OpenIE systems were used?",
    "llm_answer": "OLLIE, Reverb, MinIE, ClauseIE, Stanford OIE, ReVerb, Stanford OpenIE, and PropS.\n",
    "context": [
      {
        "id": "5bf615f1-9b75-4f86-ae8b-69bc7cdaaa8a",
        "metadata": {
          "vector_store_key": "1905.07471-4",
          "chunk_id": 0,
          "document_id": "1905.07471",
          "start_idx": 0,
          "end_idx": 447
        },
        "page_content": "Open Information Extraction (OpenIE) is the NLP task of generating (subject, relation, object) tuples from unstructured text e.g. \u201cFed chair Powell indicates rate hike\u201d outputs (Powell, indicates, rate hike). The modifier open is used to contrast IE research in which the relation belongs to a fixed set. OpenIE has been shown to be useful for several downstream applications such as knowledge base construction BIBREF0 , textual entailment BIBREF",
        "type": "Document"
      },
      {
        "id": "9f788163-0829-4b20-bf98-7b86aec0c6f7",
        "metadata": {
          "vector_store_key": "1802.05574-9",
          "chunk_id": 13,
          "document_id": "1802.05574",
          "start_idx": 5024,
          "end_idx": 5440
        },
        "page_content": " two popular OIE systems OLLIE BIBREF10 and Reverb BIBREF10 . We view this as a baseline system. The second was MinIE BIBREF7 , which is reported as performing better than OLLIE, ClauseIE BIBREF9 and Stanford OIE BIBREF9 . MinIE focuses on the notion of minimization - producing compact extractions from sentences. In our experience using OIE on scientific text, we have found that these systems often produce overly",
        "type": "Document"
      },
      {
        "id": "1963f1fa-4609-4e28-bb80-653a6f2b6370",
        "metadata": {
          "vector_store_key": "1905.07471-0",
          "chunk_id": 9,
          "document_id": "1905.07471",
          "start_idx": 3391,
          "end_idx": 3748
        },
        "page_content": "IBREF8 , ReVerb BIBREF9 , OLLIE BIBREF10 , Stanford OpenIE BIBREF11 , and PropS BIBREF12 . Stanovsky et al. SupervisedOIE frame OpenIE as a BIO-tagging problem and train an LSTM to tag an input sentence. Tuples can be derived from the tagger, input, and BIO CFG parser. This method outperforms traditional systems, though the tagging scheme inherently const",
        "type": "Document"
      },
      {
        "id": "8769eaf2-16b0-4ccf-82be-5f941d3733c0",
        "metadata": {
          "vector_store_key": "1905.07471-0",
          "chunk_id": 11,
          "document_id": "1905.07471",
          "start_idx": 4053,
          "end_idx": 4538
        },
        "page_content": " The system is inhibited by its synthetic training data which is bootstrapped from a rule-based system. Due to the lack of large datasets for OpenIE, previous works have focused on generating datasets from other tasks. These have included QA-SRL datasets BIBREF14 and QAMR datasets BIBREF6 . These methods are limited by the size of the source training data which are an order of magnitude smaller than existing reading comprehension datasets. Span-based Question-Answer datasets are a",
        "type": "Document"
      },
      {
        "id": "c6698b91-ad09-4b8e-874e-490ec46a1dfc",
        "metadata": {
          "vector_store_key": "1802.05574-6",
          "chunk_id": 14,
          "document_id": "1802.05574",
          "start_idx": 5276,
          "end_idx": 5763
        },
        "page_content": ". In our experience using OIE on scientific text, we have found that these systems often produce overly specific extractions that do not provide the redundancy useful for downstream tasks. Hence, we thought this was a useful package to explore. We note that both OpenIE 4 and MiniIE support relation extractions that go beyond binary tuples, supporting the extraction of n-ary relations. We note that the most recent version of Open IE (version 5) is focused on n-ary relations. For ease",
        "type": "Document"
      }
    ]
  },
  {
    "query": "how are the bidirectional lms obtained?",
    "llm_answer": "A bidirectional LSTM (BiLSTM) is formed by concatenating an LSTM moving forward through time with an LSTM moving backward through time.\n",
    "context": [
      {
        "id": "19685c5f-022b-4914-91a1-20b1bdea7890",
        "metadata": {
          "vector_store_key": "2003.05377-3",
          "chunk_id": 20,
          "document_id": "2003.05377",
          "start_idx": 7538,
          "end_idx": 7792
        },
        "page_content": " for $h_{t-1}$ input; $U_f$, $U_i$, $U_C$, $U_o$ are the weight matrices for $x_t$ input; and $b_f$, $b_i$, $b_C$, $b_o$ are the bias vectors. Basically, a Bidirectional LSTM network consists of using two LSTM networks: a forward LSTM and a backward LSTM",
        "type": "Document"
      },
      {
        "id": "d72c69f0-c0a2-4a1f-94da-1f58c43c258e",
        "metadata": {
          "vector_store_key": "1902.10525-9",
          "chunk_id": 43,
          "document_id": "1902.10525",
          "start_idx": 15990,
          "end_idx": 16460
        },
        "page_content": " bidirectional LSTMs, i.e. we process the input sequence forward and backward and merge the output states of each layer before feeding them to the next layer. The exact number of layers and nodes is determined empirically for each script. We give an overview of the impact of the number of nodes and layers in section SECREF4 . We also list the configurations for several scripts in our production system, as of this writing. The output of the LSTM layers at each timest",
        "type": "Document"
      },
      {
        "id": "4a02c031-330d-44de-9cd2-eee7e8ddc082",
        "metadata": {
          "vector_store_key": "1909.13362-3",
          "chunk_id": 26,
          "document_id": "1909.13362",
          "start_idx": 9631,
          "end_idx": 10052
        },
        "page_content": " Thus, processing a phonetic sequence in both the forward and backwards directions provides an improved context for assigning syllable boundaries. A bidirectional LSTM (BiLSTM) is formed when an LSTM moving forward through time is concatenated with an LSTM moving backward through time BIBREF23. We use the LSTM network as follows. The $x$ vector is fed through the LSTM network which outputs a vector $\\overrightarrow{h_",
        "type": "Document"
      },
      {
        "id": "3dddfd9b-1110-489b-9344-50b4068005e3",
        "metadata": {
          "vector_store_key": "1812.06705-6",
          "chunk_id": 24,
          "document_id": "1812.06705",
          "start_idx": 9029,
          "end_idx": 9352
        },
        "page_content": "i=1}^{N}p(t_i|t_{i+1}, t_{i+2},..., t_N).$$   (Eq. 9)  Traditionally, a bidirectional language model a shallow concatenation of independently trained forward and backward LMs. In order to train a deep bidirectional language model, BERT proposed Masked Language Model (MLM) task, which was also referred to Cloze Task BIBREF",
        "type": "Document"
      },
      {
        "id": "471e7611-bb70-4901-88bd-de3d90d36dfe",
        "metadata": {
          "vector_store_key": "2001.02380-3",
          "chunk_id": 86,
          "document_id": "2001.02380",
          "start_idx": 33875,
          "end_idx": 34249
        },
        "page_content": " from the composed output of the function $h$ across time steps $0 \\ldots t$, $\\delta \\in \\lbrace b,f\\rbrace $ is the direction of the respective LSTMs, $c_t^\\delta $ is the recurrent context in each direction and $\\theta = {W,b}$ gives the model weights and bias parameters (see BIBREF46 for details). Note that although the output of the system is ostensibly a probability",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what metrics are used in evaluation?",
    "llm_answer": "Manual evaluation, ROUGE, BLEU, METEOR, F1-score, task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, BPRA.\n",
    "context": [
      {
        "id": "cad945f1-4338-4c8c-988a-66e53227489d",
        "metadata": {
          "vector_store_key": "1607.06025-6",
          "chunk_id": 26,
          "document_id": "1607.06025",
          "start_idx": 10088,
          "end_idx": 10607
        },
        "page_content": "REF37 , the evaluation metrics of text-generating models fall into three categories: manual evaluation, automatic evaluation metrics, task-based evaluation. In evaluation based on human judgment each generated textual example is inspected manually. The automatic evaluation metrics, like ROUGE, BLEU and METEOR, compare human texts and generated texts. BIBREF38 shows METEOR has the strongest correlation with human judgments in image description evaluation. The last category is task-based evaluation, where the impact",
        "type": "Document"
      },
      {
        "id": "4786191e-fed1-4406-89b6-7b8835719e5f",
        "metadata": {
          "vector_store_key": "1709.10217-2",
          "chunk_id": 18,
          "document_id": "1709.10217",
          "start_idx": 6394,
          "end_idx": 6850
        },
        "page_content": " evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the number of total tasks. User satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied, neutral, satisfied and very satisfied, respectively. Response fluency: There are three scores -1, 0, 1, which indicate nonfluency, neutral, fluency. Number of dialogue turns: The number of utterances in a task",
        "type": "Document"
      },
      {
        "id": "dc58289f-90f2-4f31-9a31-296d00bb01a9",
        "metadata": {
          "vector_store_key": "1709.10217-2",
          "chunk_id": 11,
          "document_id": "1709.10217",
          "start_idx": 4141,
          "end_idx": 4657
        },
        "page_content": ", in which only the released data can be used for training and developing. The other is an open evaluation that allow to collect external data for training and developing. For task 1, we use F1-score as evaluation metric. For the task-oriented dialogue systems, the best way for evaluation is to use the online human-computer dialogue. After finishing an online human-computer dialogue with a dialogue system, the human then manually evaluate the system by using the metrics of user satisfaction degree, dialogue flu",
        "type": "Document"
      },
      {
        "id": "44492013-ea6a-4d0d-9984-8b621a88cebb",
        "metadata": {
          "vector_store_key": "1906.03538-4",
          "chunk_id": 45,
          "document_id": "1906.03538",
          "start_idx": 17710,
          "end_idx": 18208
        },
        "page_content": "6 and INLINEFORM7 , respectively. The metrics are averaged across all the perspectives INLINEFORM8 participating in the test set. The goal is to get estimates of the overall performance of the systems. Instead of creating a complex measure that would take all the aspects into account, we approximate the overall performance by multiplying the disjoint measures in INLINEFORM0 , INLINEFORM1 and INLINEFORM2 . While this gives an estimate on the overall quality, it ignores the pipeline structure of",
        "type": "Document"
      },
      {
        "id": "140ad6e1-f0ef-4e66-a00e-12f677b6f352",
        "metadata": {
          "vector_store_key": "1909.09484-3",
          "chunk_id": 46,
          "document_id": "1909.09484",
          "start_idx": 15874,
          "end_idx": 16378
        },
        "page_content": " of the dialogue state tracker, it aims to show the impact of the dialogue state tracker on the dialogue policy maker; (2) the accuracy of dialogue policy maker, it aims to explain the performance of different methods of constructing dialogue policy; (3) the quality of the final response, it aims to explain the impact of the dialogue policy on the final dialogue response. The evaluation metrics are listed as follows: BPRA: Belief Per-Response Accuracy (BPRA) tests the ability to generate the correct",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what results do they achieve?",
    "llm_answer": "Classification produces better results than regression, but result quality depends on data annotation approach.  Precision and F1 correlate better with human evaluations than Recall. Reinforcement learning shows promising results, especially in human evaluations.\n",
    "context": [
      {
        "id": "b3eb1308-3e01-4903-be04-528f4258d508",
        "metadata": {
          "vector_store_key": "2002.11268-7",
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "30f83adf-18b4-4e21-94e2-8d9a7d7fd19b",
        "metadata": {
          "vector_store_key": "1803.02839-6",
          "chunk_id": 4,
          "document_id": "1803.02839",
          "start_idx": 1481,
          "end_idx": 1849
        },
        "page_content": " , BIBREF7 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF10 , BIBREF11 , BIBREF8 , BIBREF9 . Often, the improvement simply mitigates the exploding/vanishing gradient problem BIBREF18 , BIBREF19 , but, for many tasks, the improvement is more capable of generalizing the network's training for that task. Understanding better how and why certain networks excel at certain NLP",
        "type": "Document"
      },
      {
        "id": "d1c89bac-6a19-4b4e-8af7-215ed859ba92",
        "metadata": {
          "vector_store_key": "1803.02839-4",
          "chunk_id": 118,
          "document_id": "1803.02839",
          "start_idx": 45558,
          "end_idx": 45772
        },
        "page_content": " mathematical consistency of the experiments, and provided useful insights into the results. Mark prompted the investigation into potential emergent monoid structures since they appear frequently in state machines.",
        "type": "Document"
      },
      {
        "id": "894f4ffd-fbcb-43a7-ba0f-9cca665b9304",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 2,
          "document_id": "1909.00542",
          "start_idx": 734,
          "end_idx": 1323
        },
        "page_content": " the training procedure was based on a regression set up, this year we experiment with various classification set ups. The main contributions of this paper are: We compare classification and regression approaches and show that classification produces better results than regression but the quality of the results depends on the approach followed to annotate the data labels. We conduct correlation analysis between various ROUGE evaluation metrics and the human evaluations conducted at BioASQ and show that Precision and F1 correlate better than Recall. Section SECREF2 briefly introduces",
        "type": "Document"
      },
      {
        "id": "20b8ec83-cb62-4e9d-bd50-91b5234703f2",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 43,
          "document_id": "1909.00542",
          "start_idx": 16697,
          "end_idx": 17106
        },
        "page_content": " for the reward function used by a reinforcement learning system. Reinforcement learning gives promising results, especially in human evaluations made on the runs submitted to BioASQ 6b. This year we introduced very small changes to the runs using reinforcement learning, and will aim to explore more complex reinforcement learning strategies and more complex neural models in the policy and value estimators.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what previous systems were compared to?",
    "llm_answer": "Baseline systems, top-ranked systems in a competition, and deep convolutional networks (SemEval-2015 and SemEval-2016).  Also, kernel machines.\n",
    "context": [
      {
        "id": "477c8abd-8f3e-4dca-bfeb-b9fb45a8a798",
        "metadata": {
          "vector_store_key": "1707.03764-1",
          "chunk_id": 15,
          "document_id": "1707.03764",
          "start_idx": 5731,
          "end_idx": 6168
        },
        "page_content": " that distinguishing words include both time-specific ones, like \u201cgilmore\u201d and \u201cimacelebrityau\u201d, and general words from everyday life, which are less likely to be subject to time-specific trends, like \u201cplayer\u201d, and \u201cchocolate\u201d. This section is meant to highlight all of the potential contributions to the systems which turned out to be detrimental to performance, when compared to the simpler system that we have described in Section SEC",
        "type": "Document"
      },
      {
        "id": "17d3256d-56f6-4b9d-983d-669c916f7fae",
        "metadata": {
          "vector_store_key": "1709.08299-2",
          "chunk_id": 23,
          "document_id": "1709.08299",
          "start_idx": 9435,
          "end_idx": 10003
        },
        "page_content": " questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models. The",
        "type": "Document"
      },
      {
        "id": "d9c817c3-a211-4b55-9b45-d9c4f63cece2",
        "metadata": {
          "vector_store_key": "1701.02877-4",
          "chunk_id": 141,
          "document_id": "1701.02877",
          "start_idx": 50827,
          "end_idx": 51373
        },
        "page_content": " The other genre that was particularly difficult for the systems was ACE Usenet. This is a form of user-generated content, not intended for publication but rather discussion among communities. In this sense, it is social media, and so it is not surprising that system performance on ACE UN resembles performance on social media more than other genres. Crucially, the computationally-cheap memorisation method actually acts as a reasonable predictor of the performance of other methods. This suggests that high entity diversity predicts difficulty",
        "type": "Document"
      },
      {
        "id": "5354e0ee-63d1-46d4-8d15-7710a849a586",
        "metadata": {
          "vector_store_key": "1601.06068-7",
          "chunk_id": 66,
          "document_id": "1601.06068",
          "start_idx": 21698,
          "end_idx": 22114
        },
        "page_content": " mt, naive, ppdb and bilayered. First two are baseline systems. Other three systems use paraphrases generated from an L-PCFG grammar. naive uses a word lattice with a single start-to-end path representing the input question itself, ppdb uses a word lattice constructed using the PPDB rules, and bilayered uses bi-layered L-PCFG to build word lattices. Note that naive does not require any parallel resource to train,",
        "type": "Document"
      },
      {
        "id": "817a9fbd-0ad5-4350-9d90-c039e237a296",
        "metadata": {
          "vector_store_key": "1710.01492-5",
          "chunk_id": 53,
          "document_id": "1710.01492",
          "start_idx": 19472,
          "end_idx": 19941
        },
        "page_content": " particular, the best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks BIBREF53 , BIBREF54 . Conversely, kernel machines seem to be less frequently used than in the past, and the use of learning methods other than the ones mentioned above is at this point scarce. All these models are examples of supervised learning as they need labeled training data. Semi-supervised learning. We should note two things about the use of deep neural",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Are this models usually semi/supervised or unsupervised?",
    "llm_answer": "Supervised.\n",
    "context": [
      {
        "id": "817a9fbd-0ad5-4350-9d90-c039e237a296",
        "metadata": {
          "vector_store_key": "1710.01492-5",
          "chunk_id": 53,
          "document_id": "1710.01492",
          "start_idx": 19472,
          "end_idx": 19941
        },
        "page_content": " particular, the best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks BIBREF53 , BIBREF54 . Conversely, kernel machines seem to be less frequently used than in the past, and the use of learning methods other than the ones mentioned above is at this point scarce. All these models are examples of supervised learning as they need labeled training data. Semi-supervised learning. We should note two things about the use of deep neural",
        "type": "Document"
      },
      {
        "id": "33b7cc42-71c8-4ea3-b2b4-3686e0a41be4",
        "metadata": {
          "vector_store_key": "1905.11901-3",
          "chunk_id": 32,
          "document_id": "1905.11901",
          "start_idx": 11183,
          "end_idx": 11687
        },
        "page_content": " data, our results are also relevant for work on using auxiliary data to improve low-resource MT. Supervised systems serve as an important baseline to judge the effectiveness of semisupervised or unsupervised approaches, and the quality of supervised systems trained on little data can directly impact semi-supervised workflows, for instance for the back-translation of monolingual data. Rico Sennrich has received funding from the Swiss National Science Foundation in the project CoNTra (grant number 10",
        "type": "Document"
      },
      {
        "id": "ff16a428-e2e6-43a0-90ad-d5f81b281cc2",
        "metadata": {
          "vector_store_key": "1808.00265-7",
          "chunk_id": 5,
          "document_id": "1808.00265",
          "start_idx": 2296,
          "end_idx": 2873
        },
        "page_content": " have been treated as output variables that receive direct supervision from human annotations. Unfortunately, both of these approaches have disadvantages. First, unsupervised training of attention tends to lead to models that cannot ground their decision in the image in a human interpretable manner. Second, supervised training of attention is difficult and expensive: human annotators may consider different regions to be relevant for the question at hand, which entails ambiguity and increased annotation cost. Our goal is to leverage the best of both worlds by providing VQ",
        "type": "Document"
      },
      {
        "id": "05357860-6b6e-4fcd-80d6-703425aae06c",
        "metadata": {
          "vector_store_key": "1904.05862-2",
          "chunk_id": 2,
          "document_id": "1904.05862",
          "start_idx": 841,
          "end_idx": 1272
        },
        "page_content": " . Unsupervised pre-training for computer vision has also shown promise BIBREF6 . In natural language processing (NLP), unsupervised pre-training of language models BIBREF7 , BIBREF8 , BIBREF9 improved many tasks such as text classification, phrase structure parsing and machine translation BIBREF10 , BIBREF11 . In speech processing, pre-training has focused on emotion recogniton BIBREF12 , speaker identification BIBREF13 , phon",
        "type": "Document"
      },
      {
        "id": "21bdd991-2357-48d5-8eeb-a90f815eb8a0",
        "metadata": {
          "vector_store_key": "2004.02393-5",
          "chunk_id": 38,
          "document_id": "2004.02393",
          "start_idx": 12674,
          "end_idx": 13193
        },
        "page_content": " best performed model first selects the tail passage $p_t$ and then the head passage $p_h$, because the number of candidates of tail is smaller ($\\sim $2 per question). Table TABREF21 shows the results. First, training a ranker with distant supervision performs significantly better than the random baseline, showing that the training process itself has a certain degree of denoising ability to distinguish the more informative signals from distant supervision labels. By introducing additional inductive bias of orders",
        "type": "Document"
      }
    ]
  },
  {
    "query": "When they say \"comparable performance\", how much of a performance drop do these new embeddings result in?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "a03ce29c-bd68-4b18-ba12-b8770176468e",
        "metadata": {
          "vector_store_key": "1603.00968-3",
          "chunk_id": 37,
          "document_id": "1603.00968",
          "start_idx": 13234,
          "end_idx": 13676
        },
        "page_content": " three embeddings achieves 4% improvement (in terms of AUC) compared to the baseline model. On SST-1 and SST-2, our model performs slightly worse than BIBREF11 . However, we again note that their performance is achieved using a much more complex model which involves pre-training and mutual-learning steps. This model takes days to train, whereas our model requires on the order of an hour. We note that the method proposed by Astudillo et al",
        "type": "Document"
      },
      {
        "id": "1a06330d-2802-4e60-a568-7c676507c966",
        "metadata": {
          "vector_store_key": "1603.04513-5",
          "chunk_id": 18,
          "document_id": "1603.04513",
          "start_idx": 7007,
          "end_idx": 7518
        },
        "page_content": "through data and user query data, showing that combining them gets stronger results than using individual word embeddings in web search ranking and word similarity task. However, these two papers either learned word representations on the same corpus BIBREF13 or enhanced the embedding quality by extending training corpora, not learning algorithms BIBREF17 . In our work, there is no limit to the type of embedding versions we can use and they leverage not only the diversity of corpora, but also the different",
        "type": "Document"
      },
      {
        "id": "680d868c-8fa1-4f85-855e-76ec782605b9",
        "metadata": {
          "vector_store_key": "1911.12579-9",
          "chunk_id": 85,
          "document_id": "1911.12579",
          "start_idx": 30862,
          "end_idx": 31340
        },
        "page_content": " better results for training all the embedding models. Dimensions ($D$): We evaluate and compare the quality of $100-D$, $200-D$, and $300-D$ using WordSim353 on different $ws$, and the optimal $300-D$ are evaluated with cosine similarity matrix for querying nearest neighboring words and calculating the similarity between word pairs. The embedding dimensions have little affect on the quality of the intrinsic evaluation process. However, the selection of embedding dimensions",
        "type": "Document"
      },
      {
        "id": "f739e2e9-2119-459e-aa84-fb3311097b5a",
        "metadata": {
          "vector_store_key": "1806.00722-9",
          "chunk_id": 50,
          "document_id": "1806.00722",
          "start_idx": 18197,
          "end_idx": 18606
        },
        "page_content": "1.5 BLEU on Tr-En-morph. We observe significant gain using other embedding sizes as well. Furthermore, in Table TABREF36 , we investigate DenseNMT models through ablation study. In order to make the comparison fair, six models listed have roughly the same number of parameters. On De-En, Tr-En and Tr-En-morph, we see improvement by making the encoder dense, making the decoder dense, and making the attention",
        "type": "Document"
      },
      {
        "id": "7fd0510b-0afc-4cd1-bd67-ccf219261707",
        "metadata": {
          "vector_store_key": "2003.06279-1",
          "chunk_id": 67,
          "document_id": "2003.06279",
          "start_idx": 25674,
          "end_idx": 26174
        },
        "page_content": "% was observed for $w=2,500$ with Word2vec. For larger pieces of texts, the gain is less expressive or absent. All in all, the results show that the use of virtual edges can also benefit the network approach based on stopwords. However, no significant improvement could be observed with very short and very large documents. The comparison of all three embedding methods showed that no method performed better than the others in all cases. We also investigated if more informed thresholding strategies",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What types of word representations are they evaluating?",
    "llm_answer": "GloVe word embedding, character-CNN embedding, Part-Of-Speech (POS) embedding, Named-Entity-Recognition (NER) embedding, and exact match feature.\n",
    "context": [
      {
        "id": "09ae2448-d905-49bb-8ddd-6fe9db263675",
        "metadata": {
          "vector_store_key": "1804.00079-3",
          "chunk_id": 62,
          "document_id": "1804.00079",
          "start_idx": 23747,
          "end_idx": 24271
        },
        "page_content": " tasks that are typically used to evaluate sentence representations. We provide a condensed summary and refer readers to their work for a more thorough description. We evaluate on text classification benchmarks - sentiment classification on movie reviews (MR), product reviews (CR) and Stanford sentiment (SST), question type classification (TREC), subjectivity/objectivity classification (SUBJ) and opinion polarity (MPQA). Representations are used to train a logistic regression classifier with 10-fold cross validation to",
        "type": "Document"
      },
      {
        "id": "ecd6e2be-9c40-4a1d-92ac-b0d253d54e40",
        "metadata": {
          "vector_store_key": "1604.05781-6",
          "chunk_id": 39,
          "document_id": "1604.05781",
          "start_idx": 13542,
          "end_idx": 13981
        },
        "page_content": " for time of year (see Methods). We applied Parts-of-Speech (POS) and Named Entity (NE) taggers to all these texts. Some post-processed and tagged example documents, both causal and control, are shown in Fig. 1 A. We also applied sentiment analysis methods to these documents (Methods) and we have highlighted very positive and very negative words throughout Fig. 1 . In Fig. 1 B we present odds ratios for how frequently unigrams (words),",
        "type": "Document"
      },
      {
        "id": "b6d1d3b6-83a6-49b8-a3d8-9fc57ab195c8",
        "metadata": {
          "vector_store_key": "1804.00079-5",
          "chunk_id": 34,
          "document_id": "1804.00079",
          "start_idx": 13363,
          "end_idx": 13834
        },
        "page_content": " addition, we also evaluate the quality of our learned individual word representations using standard benchmarks BIBREF36 , BIBREF37 . The choice of transfer tasks and evaluation framework are borrowed largely from BIBREF9 . We provide a condensed summary of the tasks in section SECREF10 in the Appendix but refer readers to their paper for a more detailed description. https://github.com/kudkudak/word-embeddings-benchmarks/wiki Table 2 presents the results of training",
        "type": "Document"
      },
      {
        "id": "dd08770a-09bc-4887-9b96-eb1c763a05d6",
        "metadata": {
          "vector_store_key": "1910.12618-3",
          "chunk_id": 39,
          "document_id": "1910.12618",
          "start_idx": 15726,
          "end_idx": 16229
        },
        "page_content": "F has a natural interpretation. Furthermore the RF and LASSO methods give the possibility to interpret marginal effects and analyze the importance of features, and thus to find the words which affect the time series the most. As for the word embedding, recurrent or convolutional neural networks (respectively RNN and CNN) were used with them. MLPs are not used, for they would require to concatenate all the vector representations of a sentence together beforehand and result in a network with too many",
        "type": "Document"
      },
      {
        "id": "00a4d7ec-54db-4dc5-af7f-35ed62b46bd1",
        "metadata": {
          "vector_store_key": "1909.11833-0",
          "chunk_id": 18,
          "document_id": "1909.11833",
          "start_idx": 6253,
          "end_idx": 6534
        },
        "page_content": ")$. For each word $w_i$, we use GloVe word embedding $e_i$, character-CNN embedding $c_i$, Part-Of-Speech (POS) embedding $\\operatorname{POS}_i$, Named-Entity-Recognition (NER) embedding $\\operatorname{NER}_i$ and exact match feature $\\operatorname{EM}_i$. The POS and NER tags are",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What type of recurrent layers does the model use?",
    "llm_answer": "Deep RNN, LSTM, and Quasi-Recurrent Neural Networks (QRNNs).\n",
    "context": [
      {
        "id": "23b2b8a6-1caf-45c0-85bf-b55b7758683a",
        "metadata": {
          "vector_store_key": "1710.01507-1",
          "chunk_id": 20,
          "document_id": "1710.01507",
          "start_idx": 7820,
          "end_idx": 8342
        },
        "page_content": ") and extract CNN features. We use the output of the fully-connected layer (FC7), which has 4096 dimensions, as feature representations for our architecture. We now go into detail about the components of the model, individual and combined, and how the parameters are learned. Recurrent Neural Network (RNN) is a class of artificial neural networks which utilizes sequential information and maintains history through its intermediate layers. A standard RNN has an internal state whose output at every time-step which can be",
        "type": "Document"
      },
      {
        "id": "dbf24675-827e-4c27-9d8a-87caba4ecf75",
        "metadata": {
          "vector_store_key": "1706.07179-2",
          "chunk_id": 7,
          "document_id": "1706.07179",
          "start_idx": 2809,
          "end_idx": 3302
        },
        "page_content": " in this section. Figure 1 provides a high-level view of the model. The model is sequential in nature, consisting of the following steps: read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer. We model the dynamic memory in a fashion similar to Recurrent Entity Networks BIBREF17 and then equip it with an additional relational memory. There are three main components to the model: 1) input encoder 2) dynamic memory, and 3)",
        "type": "Document"
      },
      {
        "id": "3fbce5a9-bb1d-4cb3-87df-5c0588ed6df4",
        "metadata": {
          "vector_store_key": "1611.02550-1",
          "chunk_id": 16,
          "document_id": "1611.02550",
          "start_idx": 5793,
          "end_idx": 6236
        },
        "page_content": " FIGREF1 . The model consists of a deep RNN with some number INLINEFORM3 of stacked layers, whose final hidden state vector is passed as input to a set of INLINEFORM4 of fully connected layers; the output of the final fully connected layer is the embedding INLINEFORM5 . The RNN hidden state at each time frame can be viewed as a representation of the input seen thus far, and its value in the last time frame INLINEFORM0 could itself serve as",
        "type": "Document"
      },
      {
        "id": "17bf7079-daa9-402e-b3cf-e25adbef1e3f",
        "metadata": {
          "vector_store_key": "1811.00942-1",
          "chunk_id": 11,
          "document_id": "1811.00942",
          "start_idx": 4545,
          "end_idx": 4922
        },
        "page_content": " introduced. BIBREF4 name their three-layer LSTM model trained with such tricks, \u201cAWD-LSTM.\u201d Quasi-Recurrent Neural Networks. Quasi-recurrent neural networks (QRNNs; BIBREF10 ) achieve current state of the art in word-level language modeling BIBREF11 . A quasi-recurrent layer comprises two separate parts: a convolution layer with three weights, and a recurrent pooling layer.",
        "type": "Document"
      },
      {
        "id": "b0acb005-01ea-424f-b249-be4013d0e3eb",
        "metadata": {
          "vector_store_key": "1910.04269-6",
          "chunk_id": 34,
          "document_id": "1910.04269",
          "start_idx": 12971,
          "end_idx": 13505
        },
        "page_content": " its hyperparameter. We took some specific design choices for this model, which are as follows: We added residual connections with each convolutional layer. Residual connections in a way makes the model selective of the contributing layers, determines the optimal number of layers required for training and solves the problem of vanishing gradients. Residual connections or skip connections skip training of those layers that do not contribute much in the overall outcome of model. We added spatial attention BIBREF27 networks to help",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is a word confusion network?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "8f7b33f4-12a8-4292-8e7d-bc961ff3030c",
        "metadata": {
          "vector_store_key": "1905.10810-0",
          "chunk_id": 1,
          "document_id": "1905.10810",
          "start_idx": 570,
          "end_idx": 1103
        },
        "page_content": " of isolated correction of non-word errors. They are traditionally separated in error correction literature BIBREF0 . Non-word errors are here incorrect word forms that not only differ from what was intended, but also do not constitute another, existing word themselves. Much of the initial research on error correction focused on this simple task, tackled without means of taking the context of the nearest words into account. It is true that, especially in the case of neural networks, it is often possible and desirable to combine",
        "type": "Document"
      },
      {
        "id": "77c5f0c6-578b-4c4f-a35c-077945e42a95",
        "metadata": {
          "vector_store_key": "2003.06279-4",
          "chunk_id": 4,
          "document_id": "2003.06279",
          "start_idx": 1563,
          "end_idx": 2079
        },
        "page_content": "-occurrence network. After optional textual pre-processing steps, in a co-occurrence network each different word becomes a node and edges are established via co-occurrence in a desired window. A common strategy connects only adjacent words in the so called word adjacency networks. While the co-occurrence representation yields good results in classification scenarios, some important features are not considered in the model. For example, long-range syntactical links, though less frequent than adjacent syntactical",
        "type": "Document"
      },
      {
        "id": "f2d17ccd-10d3-4558-8940-cac72e38484d",
        "metadata": {
          "vector_store_key": "1907.11499-2",
          "chunk_id": 19,
          "document_id": "1907.11499",
          "start_idx": 8020,
          "end_idx": 8555
        },
        "page_content": " recurrent neural networks. It eliminates recurrence in favor of applying a self-attention mechanism which directly models relationships between all words in a sentence, regardless of their position. DetNet adopts three detectors corresponding to words, sentences, and documents: INLINEFORM0   WordDet first produces word domain scores using both lexical semantic information INLINEFORM0 and prior (domain) knowledge INLINEFORM1 ; SentDet yields domain scores for sentences while integrating downstream instance signals INLINEFORM2 and",
        "type": "Document"
      },
      {
        "id": "f7369e1d-c142-48d2-84ce-bc093ed80655",
        "metadata": {
          "vector_store_key": "1705.07368-2",
          "chunk_id": 71,
          "document_id": "1705.07368",
          "start_idx": 26965,
          "end_idx": 27424
        },
        "page_content": " of underlying clusters is learned from data via Bayesian inference, have also been proposed BIBREF44 . In this work, dictionary words are assigned a mixed membership distribution over a set of shared latent vector space embeddings. Each instantiation of a dictionary word (an \u201cinput\u201d word) is assigned to one of the shared embeddings based on its dictionary word's membership vector. The words in its context (\u201coutput\u201d words) are assumed to be drawn based on",
        "type": "Document"
      },
      {
        "id": "856d44b5-09b5-4481-bd40-2bb7ca338486",
        "metadata": {
          "vector_store_key": "1911.12569-3",
          "chunk_id": 24,
          "document_id": "1911.12569",
          "start_idx": 9725,
          "end_idx": 10201
        },
        "page_content": " whether they tend to occur in a similar context. It provides a word expansion list for words based on their contextual similarity. We use the top-4 words for each word as their candidate terms. We only use the top-4 words for each word as we observed that the expansion list with more words started to contain the antonyms of the current word which empirically reduced the system performance. Word embeddings of these four candidate terms and the hidden state vector $h_t$ of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What evaluation metrics were used in the experiment?",
    "llm_answer": "micro-F1, EM-score, F$_1$-score, relevance, attraction, fluency, style strength.\n",
    "context": [
      {
        "id": "952c3ead-6461-49aa-a21f-f08fccb02046",
        "metadata": {
          "vector_store_key": "1909.07734-0",
          "chunk_id": 17,
          "document_id": "1909.07734",
          "start_idx": 6722,
          "end_idx": 7207
        },
        "page_content": ", sadness, anger, and neutral. Evaluation of submissions was done using only utterances with these four labels. Utterances with labels other than the above four (i.e., surprise, disgust, fear or non-neutral) were discarded and not used in the evaluation. Scripts for verifying and evaluating the submissions were made available online. We used micro-F1 as the comparison metric. A total of eleven teams submitted their evaluations, and are presented in the online leaderboard. Seven of",
        "type": "Document"
      },
      {
        "id": "5b953cb7-9114-48c2-b577-fa0a59e4afed",
        "metadata": {
          "vector_store_key": "1804.03396-1",
          "chunk_id": 93,
          "document_id": "1804.03396",
          "start_idx": 29645,
          "end_idx": 30157
        },
        "page_content": " $= 93.53$ and the IE measurements can be found in Figure 3 . Most of the error cases are the fourth case which is acceptable by human annotators. Note that our framework takes the whole document as the input while the baseline systems take the individual sentence as the input, which means the experiment setting is much more difficult for our framework. Finally, we perform a human evaluation on our QA4IE benchmark to verify the reliability of former experiments. The evaluation metrics are as follows: Triple",
        "type": "Document"
      },
      {
        "id": "bb75c3fa-e2c5-412d-9db8-2f31cd178291",
        "metadata": {
          "vector_store_key": "1908.06606-4",
          "chunk_id": 45,
          "document_id": "1908.06606",
          "start_idx": 15988,
          "end_idx": 16383
        },
        "page_content": ") and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage",
        "type": "Document"
      },
      {
        "id": "60e45509-683c-4639-be3b-33b2eab42183",
        "metadata": {
          "vector_store_key": "2004.01980-7",
          "chunk_id": 55,
          "document_id": "2004.01980",
          "start_idx": 18999,
          "end_idx": 19503
        },
        "page_content": " style corpus, and the sampling strategy follows the uniform distribution with the probability being equal to $\\lambda $. The human evaluation is to have a comprehensive measurement of the performances. We conduct experiments on four criteria, relevance, attraction, fluency, and style strength. We summarize the human evaluation results on the first three criteria in Table TABREF51, and the last criteria in Table TABREF57. Note that through automatic evaluation, the baselines NST, Fine-tuned, and Gig",
        "type": "Document"
      },
      {
        "id": "44492013-ea6a-4d0d-9984-8b621a88cebb",
        "metadata": {
          "vector_store_key": "1906.03538-4",
          "chunk_id": 45,
          "document_id": "1906.03538",
          "start_idx": 17710,
          "end_idx": 18208
        },
        "page_content": "6 and INLINEFORM7 , respectively. The metrics are averaged across all the perspectives INLINEFORM8 participating in the test set. The goal is to get estimates of the overall performance of the systems. Instead of creating a complex measure that would take all the aspects into account, we approximate the overall performance by multiplying the disjoint measures in INLINEFORM0 , INLINEFORM1 and INLINEFORM2 . While this gives an estimate on the overall quality, it ignores the pipeline structure of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What kind of instructional videos are in the dataset?",
    "llm_answer": "Screencast tutorial videos with spoken narratives for a photo-editing software.\n",
    "context": [
      {
        "id": "d2838737-eef6-42dc-8207-5d816b6e05ee",
        "metadata": {
          "vector_store_key": "1912.01046-5",
          "chunk_id": 53,
          "document_id": "1912.01046",
          "start_idx": 20739,
          "end_idx": 21318
        },
        "page_content": ", a new type of dataset used to find answer spans in tutorial videos. Our data collection method for question-answer pairs on instructional video can be further adopted to other domains where the answers involve multiple steps and are part of an overall goal, such as cooking or educational videos. We have shown that current baseline models for finding the answer spans are not sufficient for achieving high accuracy and hope that by releasing this new dataset and task, more appropriate question answering models can be developed for question answering on instructional videos.",
        "type": "Document"
      },
      {
        "id": "3334a481-3186-4846-a835-bb893c25ae59",
        "metadata": {
          "vector_store_key": "1912.01046-5",
          "chunk_id": 14,
          "document_id": "1912.01046",
          "start_idx": 5574,
          "end_idx": 6121
        },
        "page_content": " collection process . Our dataset consists of 76 tutorial videos pertaining to an image editing software. All of the videos include spoken instructions which are transcribed and manually segmented into multiple segments. Specifically, we asked the annotators to manually divide each video into multiple segments such that each of the segments can serve as an answer to any question. For example, Fig. FIGREF1 shows example segments marked in red (each which are a complete unit as an answer span). Each sentence is associated with the starting and",
        "type": "Document"
      },
      {
        "id": "9f582961-55f6-4a75-a5f9-94ebb80fe99a",
        "metadata": {
          "vector_store_key": "1912.01046-5",
          "chunk_id": 13,
          "document_id": "1912.01046",
          "start_idx": 5276,
          "end_idx": 5797
        },
        "page_content": " augment with a visual cue detector on food entities. Their task focuses on procedure-based cooking videos, and contrary to our task is primarily a text alignment task. In our task we aim to answer questions-using the transcripts-on instructional-style videos, in which the answer can involve steps not mentioned in the question. In this section, we introduce the TutorialVQA dataset and describe the data collection process . Our dataset consists of 76 tutorial videos pertaining to an image editing software. All of the",
        "type": "Document"
      },
      {
        "id": "485899fd-1e59-4d5c-8f3d-662f9218d2f7",
        "metadata": {
          "vector_store_key": "1912.01046-5",
          "chunk_id": 6,
          "document_id": "1912.01046",
          "start_idx": 2345,
          "end_idx": 2865
        },
        "page_content": ", as illustrated in Figure FIGREF1. The remainder of this paper is structured as follows. Section SECREF3 introduces TutorialVQA dataset as a case study of our proposed problem. The dataset includes about 6,000 triples, comprised of videos, questions, and answer spans manually collected from screencast tutorial videos with spoken narratives for a photo-editing software. Section SECREF4 presents the baseline models and their experiment details on the sentence-level prediction and video segment retrieval tasks on our",
        "type": "Document"
      },
      {
        "id": "0e97c7e7-33a8-40dd-9b79-3c5ebe85d772",
        "metadata": {
          "vector_store_key": "1812.07023-1",
          "chunk_id": 27,
          "document_id": "1812.07023",
          "start_idx": 10725,
          "end_idx": 11271
        },
        "page_content": " prototype dataset, the training set of the avsd dataset corresponds to videos taken from the training set of the Charades dataset while the validation and test sets of the avsd dataset correspond to videos taken from the validation set of the Charades dataset. For the official dataset, training, validation and test sets are drawn from the corresponding Charades sets. The Charades dataset also provides additional annotations for the videos such as action, scene, and object annotations, which are considered to be external data sources by the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What baseline algorithms were presented?",
    "llm_answer": "Four baseline systems, including two simple ones and two neural network models.\n",
    "context": [
      {
        "id": "17d3256d-56f6-4b9d-983d-669c916f7fae",
        "metadata": {
          "vector_store_key": "1709.08299-2",
          "chunk_id": 23,
          "document_id": "1709.08299",
          "start_idx": 9435,
          "end_idx": 10003
        },
        "page_content": " questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models. The",
        "type": "Document"
      },
      {
        "id": "577fb06e-9340-4422-93f3-91bc89574aee",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 43,
          "document_id": "2001.00137",
          "start_idx": 16781,
          "end_idx": 17197
        },
        "page_content": " the online platforms. Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector",
        "type": "Document"
      },
      {
        "id": "344cdcd9-2892-4ed9-8902-26bffb76e164",
        "metadata": {
          "vector_store_key": "1905.11037-2",
          "chunk_id": 14,
          "document_id": "1905.11037",
          "start_idx": 5306,
          "end_idx": 5824
        },
        "page_content": "ered perceptron, convolutional neural networks and long short-term memory networks. We outline the essentials of each of these models, but will treat them as black boxes. In a related line, kaushik2018much discuss the need of providing rigorous baselines that help better understand the improvement coming from future and complex models, and also the need of not demanding architectural novelty when introducing new datasets. Although not done in this work, an alternative (but also natural) way to address the task is",
        "type": "Document"
      },
      {
        "id": "947ca67f-f47a-4cb3-91ff-40d57888bc24",
        "metadata": {
          "vector_store_key": "1909.09484-5",
          "chunk_id": 55,
          "document_id": "1909.09484",
          "start_idx": 19263,
          "end_idx": 19734
        },
        "page_content": " models and baselines, with a learning rate of 0.001 for supervised training and 0.0001 for reinforcement learning. In reinforcement learning, the decay parameter $\\lambda $ is set to 0.8. The weight decay is set to 0.001. And early stopping is performed on developing set. The experimental results of the proposed model and baselines will be analyzed from the following aspects. BPRA Results: As shown in Table TABREF35, most of the models have similar performance on BP",
        "type": "Document"
      },
      {
        "id": "18bc4047-159c-4932-9d6c-755753773398",
        "metadata": {
          "vector_store_key": "1603.04513-7",
          "chunk_id": 82,
          "document_id": "1603.04513",
          "start_idx": 26830,
          "end_idx": 27230
        },
        "page_content": "ational aspect of the system. All results in the five blocks are with respect to row 34, \u201cMVCNN (overall)\u201d; e.g., row 19 shows what happens when HLBL is removed from row 34, row 28 shows what happens when mutual learning is removed from row 34 etc. The block \u201cbaselines\u201d (1\u201318) lists some systems representative of previous work on the corresponding datasets, including the state-of-the-art systems (",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does TP-N2F compare to LSTM-based Seq2Seq in terms of training and inference speed?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "f5cb7737-14be-492e-a869-b36c22d56c3b",
        "metadata": {
          "vector_store_key": "1910.02339-5",
          "chunk_id": 73,
          "document_id": "1910.02339",
          "start_idx": 21875,
          "end_idx": 22293
        },
        "page_content": " engine) and operation sequence accuracy (where the generated operation sequence must match the ground truth sequence exactly). TP-N2F is compared to a baseline provided by the seq2prog model in BIBREF16, an LSTM-based seq2seq model with attention. Our model outperforms both the original seq2prog, designated SEQ2PROG-orig, and the best reimplemented seq2prog after an extensive hyperparameter search, designated SEQ2",
        "type": "Document"
      },
      {
        "id": "bb58855a-3ba0-4646-bfdc-9840cc1c15ec",
        "metadata": {
          "vector_store_key": "1910.02339-5",
          "chunk_id": 78,
          "document_id": "1910.02339",
          "start_idx": 23532,
          "end_idx": 23901
        },
        "page_content": " (M-Acc). AlgoLisp has about 10% noisy data (details in the Appendix), so we report results both on the full test set and the cleaned test set (in which all noisy testing samples are removed). TP-N2F is compared with an LSTM seq2seq with attention model, the Seq2Tree model in BIBREF17, and a seq2seq model with a pre-trained tree decoder from the Tree2Tree autoencoder",
        "type": "Document"
      },
      {
        "id": "bb9a058c-933a-4a7b-9edf-9d42c3262335",
        "metadata": {
          "vector_store_key": "1910.02339-6",
          "chunk_id": 79,
          "document_id": "1910.02339",
          "start_idx": 23901,
          "end_idx": 24241
        },
        "page_content": "2seq model with a pre-trained tree decoder from the Tree2Tree autoencoder (SAPS) reported in BIBREF18. As shown in Table TABREF18, TP-N2F outperforms all existing models on both the full test set and the cleaned test set. Ablation experiments with TP2LSTM and LSTM2TP show that, for this task, the TP-N2F Decoder is more helpful than TP-N2F",
        "type": "Document"
      },
      {
        "id": "5202df74-1f70-4df3-b07c-7bb092b64cea",
        "metadata": {
          "vector_store_key": "1803.07771-6",
          "chunk_id": 77,
          "document_id": "1803.07771",
          "start_idx": 27491,
          "end_idx": 27849
        },
        "page_content": " two-level network (without lexicon embedding) with character embedding, Tl-LSTM-C, outperforms all the other involved algorithms. On the travel and the mobile corpora, TI-LSTM-W outperforms Bi-LSTM-W. The results in Table 4 indicate that the performances of Tl-LSTM on the mixed training and test data (R+C) are better than those of Bi-LSTM. This comparison",
        "type": "Document"
      },
      {
        "id": "05abc159-4473-4d2a-aae8-3c48d92d8838",
        "metadata": {
          "vector_store_key": "1910.02339-6",
          "chunk_id": 74,
          "document_id": "1910.02339",
          "start_idx": 22080,
          "end_idx": 22455
        },
        "page_content": " best reimplemented seq2prog after an extensive hyperparameter search, designated SEQ2PROG-best. Table TABREF16 presents the results. To verify the importance of the TP-N2F encoder and decoder, we conducted experiments to replace either the encoder with a standard LSTM (denoted LSTM2TP) or the decoder with a standard attentional LSTM (denoted TP2LSTM). We observe that both",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance proposed model achieved on AlgoList benchmark?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "dc50a5cf-b755-41a0-a173-1b561db0ed05",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "b5725682-ba0e-4f3d-a11e-6a0a04be2345",
        "metadata": {
          "vector_store_key": "1910.02339-5",
          "chunk_id": 93,
          "document_id": "1910.02339",
          "start_idx": 29094,
          "end_idx": 29547
        },
        "page_content": " Appendix A.4. AlgoLisp provides an execution script to run the generated program and has three evaluation metrics: accuracy of passing all test cases (Acc), accuracy of passing 50% of test cases (50p-Acc), and accuracy of generating an exactly matched program (M-Acc). AlgoLisp has about 10% noise data (where the execution script fails to pass all test cases on the ground truth program), so we report results both on the full test set and the cleaned",
        "type": "Document"
      },
      {
        "id": "148ea921-3ae7-4687-bc70-85a9690fe58b",
        "metadata": {
          "vector_store_key": "1912.13109-4",
          "chunk_id": 20,
          "document_id": "1912.13109",
          "start_idx": 7921,
          "end_idx": 8471
        },
        "page_content": " on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we",
        "type": "Document"
      },
      {
        "id": "379fe70d-a3de-4063-9841-5b784c0181cb",
        "metadata": {
          "vector_store_key": "1701.06538-2",
          "chunk_id": 94,
          "document_id": "1701.06538",
          "start_idx": 34529,
          "end_idx": 34995
        },
        "page_content": "ity after 100 billion training words is 39% lower for the 68-billion-parameter MoE model than for the baseline model. It is notable that the measured computational efficiency of the largest model (0.30 TFLOPS/GPU) is very low compared to the other models. This is likely a result of the fact that, for purposes of comparison to the other models, we did not increase the training batch size proportionally to the number of GPUs. For comparison, we include results for",
        "type": "Document"
      },
      {
        "id": "3297fd18-2489-4a4d-a7b2-b3f620fcda80",
        "metadata": {
          "vector_store_key": "1701.06538-2",
          "chunk_id": 88,
          "document_id": "1701.06538",
          "start_idx": 32330,
          "end_idx": 32790
        },
        "page_content": " INLINEFORM1 respectively, showing that even in the presence of a large MoE, more computation is still useful. Results are reported at the bottom of Table TABREF76 . The larger of the two models has a similar computational budget to the best published model from the literature, and training times are similar. Comparing after 10 epochs, our model has a lower test perplexity by INLINEFORM2 . The models are similar in structure to the 8-million-operations-per",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance proposed model achieved on MathQA?",
    "llm_answer": "The context states the model has a lower test perplexity after 10 epochs compared to a baseline model.  It also achieves similar computational efficiency to the best published model in the literature.  However, it does not provide a specific performance metric (e.g., accuracy).\n",
    "context": [
      {
        "id": "02ab4efc-be86-4312-a7d3-062f01e9377c",
        "metadata": {
          "vector_store_key": "1910.02339-5",
          "chunk_id": 96,
          "document_id": "1910.02339",
          "start_idx": 29959,
          "end_idx": 30212
        },
        "page_content": " MathQA dataset, we use $n_{\\mathrm {F}}= 150$, $n_{\\mathrm {R}}= 50$, $d_{\\mathrm {F}}= 30$, $d_{\\mathrm {R}}= 20$, $d_{Rel} = 20$, $d_{Arg} = 10$, $d_{Pos} = 5$ and we train the model for 60 epochs with learning rate 0.00115. The reasoning module only",
        "type": "Document"
      },
      {
        "id": "f1c7d405-3ea7-4310-b51f-2f65976219f3",
        "metadata": {
          "vector_store_key": "1910.02339-5",
          "chunk_id": 89,
          "document_id": "1910.02339",
          "start_idx": 27544,
          "end_idx": 28014
        },
        "page_content": ". The MathQA dataset consists of about 37k math word problems ((80/12/8)% training/dev/testing problems), each with a corresponding list of multi-choice options and an straight-line operation sequence program to solve the problem. An example from the dataset is presented in the Appendix A.4. In this task, TP-N2F is deployed to generate the operation sequence given the question. The generated operations are executed to generate the solution for the given math problem",
        "type": "Document"
      },
      {
        "id": "3297fd18-2489-4a4d-a7b2-b3f620fcda80",
        "metadata": {
          "vector_store_key": "1701.06538-2",
          "chunk_id": 88,
          "document_id": "1701.06538",
          "start_idx": 32330,
          "end_idx": 32790
        },
        "page_content": " INLINEFORM1 respectively, showing that even in the presence of a large MoE, more computation is still useful. Results are reported at the bottom of Table TABREF76 . The larger of the two models has a similar computational budget to the best published model from the literature, and training times are similar. Comparing after 10 epochs, our model has a lower test perplexity by INLINEFORM2 . The models are similar in structure to the 8-million-operations-per",
        "type": "Document"
      },
      {
        "id": "379fe70d-a3de-4063-9841-5b784c0181cb",
        "metadata": {
          "vector_store_key": "1701.06538-2",
          "chunk_id": 94,
          "document_id": "1701.06538",
          "start_idx": 34529,
          "end_idx": 34995
        },
        "page_content": "ity after 100 billion training words is 39% lower for the 68-billion-parameter MoE model than for the baseline model. It is notable that the measured computational efficiency of the largest model (0.30 TFLOPS/GPU) is very low compared to the other models. This is likely a result of the fact that, for purposes of comparison to the other models, we did not increase the training batch size proportionally to the number of GPUs. For comparison, we include results for",
        "type": "Document"
      },
      {
        "id": "16a27ee5-b7b6-45ce-b49d-b926f1918472",
        "metadata": {
          "vector_store_key": "1703.09684-0",
          "chunk_id": 49,
          "document_id": "1703.09684",
          "start_idx": 19045,
          "end_idx": 19508
        },
        "page_content": " average them together for the question-type. To compute overall performance, we compute the arithmetic normalized mean per-type (N-MPT) and harmonic N-MPT scores. A large discrepancy between unnormalized and normalized scores suggests an algorithm is not generalizing to rarer answers. While there are alternative formulations (e.g., BIBREF4 , BIBREF19 ), the majority of VQA systems formulate it as a classification problem in which the system is given an image",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What previous methods is the proposed method compared against?",
    "llm_answer": "Coppersmith et al.'s method, K-means, SkipVec, RecNN, and Para2vec based clustering methods.\n",
    "context": [
      {
        "id": "7a3763d4-793b-483d-9bf2-cdefe870cd27",
        "metadata": {
          "vector_store_key": "2003.07433-0",
          "chunk_id": 66,
          "document_id": "2003.07433",
          "start_idx": 25731,
          "end_idx": 26161
        },
        "page_content": "REF28 illustrates the comparisons between LAXARY and Coppersmith et. al. proposed method. Here we can see, the outperformance of our proposed method as well as the importance of $s-score$ estimation. We also illustrates the importance of $\\alpha -score$ and $S-score$ in Fig FIGREF30. Fig FIGREF30 illustrates that if we change the number of training samples (%), LAXARY models outperforms Coppersmith et. al. proposed model under",
        "type": "Document"
      },
      {
        "id": "97d0589b-102d-402c-bcb2-27484aa3c4fb",
        "metadata": {
          "vector_store_key": "1701.00185-8",
          "chunk_id": 76,
          "document_id": "1701.00185",
          "start_idx": 27376,
          "end_idx": 27736
        },
        "page_content": " and NMI performance of our proposed approaches and four baseline methods, K-means, SkipVec, RecNN and Para2vec based clustering methods. Intuitively, we get a general observation that (1) BoW based approaches, including K-means (TF) and K-means (TF-IDF), and SkipVec based approaches perform not well; (2) RecNN based approaches, both RecNN (Ave.) and RecNN (",
        "type": "Document"
      },
      {
        "id": "02fa457c-7ba5-426e-a156-5762af90abe9",
        "metadata": {
          "vector_store_key": "1908.07816-1",
          "chunk_id": 11,
          "document_id": "1908.07816",
          "start_idx": 4509,
          "end_idx": 5017
        },
        "page_content": " methodology. We present experimental results and some analysis in Section SECREF4. The paper is concluded in Section SECREF5, followed by some future work we plan to do. Many early open-domain dialog systems are rule-based and often require expert knowledge to develop. More recent work in response generation seeks data-driven solutions, leveraging on machine learning techniques and the availability of data. Ritter et al. BIBREF14 first applied statistical machine translation (SMT) methods to this area.",
        "type": "Document"
      },
      {
        "id": "8270f85e-7747-4bf2-b632-cf277c50cb48",
        "metadata": {
          "vector_store_key": "1910.11949-4",
          "chunk_id": 7,
          "document_id": "1910.11949",
          "start_idx": 2984,
          "end_idx": 3467
        },
        "page_content": "REF8 by Joseph Weizenbaum at MIT. Its implementation consisted in pattern matching and substitution methodology. Recently, data driven approaches have drawn significant attention. Existing work along this line includes retrieval-based methods BIBREF9BIBREF10 and generation-based methodsBIBREF11BIBREF12. In this work we focus on generative models, where sequence-to-sequence algorithm that uses RNNs to encode and decode inputs into responses is a current best practice. Our convers",
        "type": "Document"
      },
      {
        "id": "0c6f06cc-2953-4285-84a9-5100ee127f17",
        "metadata": {
          "vector_store_key": "1905.13497-3",
          "chunk_id": 24,
          "document_id": "1905.13497",
          "start_idx": 8250,
          "end_idx": 8720
        },
        "page_content": ", and the matrices involved. We evaluate our method on two commonsense reasoning tasks, PDP and WSC. On the former task, we use the original set of 60 questions (PDP-60) as the main benchmark. The second task (WSC-273) is qualitatively much more difficult. The recent best reported result are not much above random guess. This task consists of 273 questions and is designed to work against traditional linguistic techniques, common heuristics or simple statistical tests",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the baseline model used?",
    "llm_answer": "Modelo 1\n",
    "context": [
      {
        "id": "97be5370-e788-41d2-9b17-3e7739266f06",
        "metadata": {
          "vector_store_key": "2001.11381-3",
          "chunk_id": 138,
          "document_id": "2001.11381",
          "start_idx": 31860,
          "end_idx": 32196
        },
        "page_content": "ados. Dicho lo anterior, el Modelo 1 podr\u00eda ser considerado como nuestro propio baseline. A continuaci\u00f3n presentamos un protocolo de evaluaci\u00f3n manual de los resultados obtenidos. El experimento consisti\u00f3 en la generaci\u00f3n de 15 frases por cada uno de los tres modelos propuestos. Para cada modelo, se consideraron tres queries: $Q=$ {AM",
        "type": "Document"
      },
      {
        "id": "947ca67f-f47a-4cb3-91ff-40d57888bc24",
        "metadata": {
          "vector_store_key": "1909.09484-5",
          "chunk_id": 55,
          "document_id": "1909.09484",
          "start_idx": 19263,
          "end_idx": 19734
        },
        "page_content": " models and baselines, with a learning rate of 0.001 for supervised training and 0.0001 for reinforcement learning. In reinforcement learning, the decay parameter $\\lambda $ is set to 0.8. The weight decay is set to 0.001. And early stopping is performed on developing set. The experimental results of the proposed model and baselines will be analyzed from the following aspects. BPRA Results: As shown in Table TABREF35, most of the models have similar performance on BP",
        "type": "Document"
      },
      {
        "id": "95d982c1-df48-40f1-aea4-aa545ace30dc",
        "metadata": {
          "vector_store_key": "1909.02635-8",
          "chunk_id": 23,
          "document_id": "1909.02635",
          "start_idx": 9050,
          "end_idx": 9548
        },
        "page_content": " (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo",
        "type": "Document"
      },
      {
        "id": "ac08e6c0-ed7c-44a4-8569-c15f1a5e35d1",
        "metadata": {
          "vector_store_key": "1809.04960-5",
          "chunk_id": 58,
          "document_id": "1809.04960",
          "start_idx": 22497,
          "end_idx": 22960
        },
        "page_content": " the representative baseline models. We provide 200 unique comments as the candidate sets, which consists of four types of comments as described in the above retrieval evaluation setting: Correct, Plausible, Popular, and Random. We rank the candidate comment set with four models (TF-IDF, S2S, IR, and Proposed+IR), and record the types of top-1 comments. Figure FIGREF40 shows the percentage of different types of top-1 comments generated by each model. It shows",
        "type": "Document"
      },
      {
        "id": "0b77285e-fdf5-4273-8639-5afcd5734567",
        "metadata": {
          "vector_store_key": "2001.02380-3",
          "chunk_id": 140,
          "document_id": "2001.02380",
          "start_idx": 51470,
          "end_idx": 51956
        },
        "page_content": " baselines. The left, middle and right panels in Figure FIGREF40 correspond to measurements when all signals are included, only cases contained entirely in the head EDUs shown to the model, and only DMs, respectively. The scenario on the left is rather unreasonable and is included only for completeness: here the model is also penalized for not detecting signals such as lexical chains, part of which is outside the units that the model is being shown. An example of such a case can be",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What domains are present in the data?",
    "llm_answer": "20 domains,  HAR (Hotel, Attraction, Restaurant), and MT (scientific).  Wikipedia domain is also mentioned.\n",
    "context": [
      {
        "id": "896d120c-d921-4200-be03-d5842da38d1e",
        "metadata": {
          "vector_store_key": "2002.01359-3",
          "chunk_id": 29,
          "document_id": "2002.01359",
          "start_idx": 12314,
          "end_idx": 12843
        },
        "page_content": " common approach utilized in other datasets BIBREF0. It has been shown that such datasets suffer from substantial annotation errors BIBREF18. Thirdly, using a simulator greatly simplifies the data collection task and instructions as only paraphrasing is needed to achieve a natural dialogue. This is particularly important for creating a large dataset spanning multiple domains. The 20 domains present across the train, dev and test datasets are listed in Table TABREF10, as are the details regarding which domains are present in",
        "type": "Document"
      },
      {
        "id": "bea618fb-90d6-4dc7-8e60-c8e7bcf0b45f",
        "metadata": {
          "vector_store_key": "1701.03214-2",
          "chunk_id": 15,
          "document_id": "1701.03214",
          "start_idx": 5368,
          "end_idx": 5787
        },
        "page_content": " (essentially open) in-domain data. The scientific domain MT was conducted on the Chinese-Japanese paper excerpt corpus (ASPEC-CJ) BIBREF11 , which is one subtask of the workshop on Asian translation (WAT) BIBREF15 . The ASPEC-CJ task uses 672315, 2090, and 2107 sentences for training, development, and testing, respectively. The Wikipedia domain task was conducted on a Chinese-Japanese corpus automatically extracted",
        "type": "Document"
      },
      {
        "id": "93868b0b-4b9f-4465-9f7e-c688fa1a5347",
        "metadata": {
          "vector_store_key": "1701.03214-0",
          "chunk_id": 11,
          "document_id": "1701.03214",
          "start_idx": 4004,
          "end_idx": 4439
        },
        "page_content": "-of-domain data is used for the in-domain data; while for the \u201cmulti domain\u201d and \u201cmixed fine tuning\u201d methods, we use a vocabulary obtained from the mixed in-domain and out-of-domain data for all the training stages. We conducted NMT domain adaptation experiments in two different settings as follows: Chinese-to-English translation was the focus of the high quality in-domain corpus setting. We utilized the resource rich patent out-of",
        "type": "Document"
      },
      {
        "id": "604d5368-fff4-4b8a-9276-08e68a429705",
        "metadata": {
          "vector_store_key": "2002.11893-5",
          "chunk_id": 21,
          "document_id": "2002.11893",
          "start_idx": 8062,
          "end_idx": 8606
        },
        "page_content": "domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context. To make workers understand the task more easily, we crafted templates to generate natural language descriptions for each structured goal. Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and",
        "type": "Document"
      },
      {
        "id": "ff1d2011-690a-463e-a90d-639c5b8f00b7",
        "metadata": {
          "vector_store_key": "2002.11893-5",
          "chunk_id": 20,
          "document_id": "2002.11893",
          "start_idx": 7557,
          "end_idx": 8062
        },
        "page_content": " we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database. For the taxi domain, there is no need to store the information. Instead, we can call the API directly if necessary. Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many texts/datapoints are in the SemEval, TASS and SENTIPOLC datasets?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "0d593f74-7474-41cc-9928-ce68ac96f714",
        "metadata": {
          "vector_store_key": "1802.05574-2",
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "19ad6a1f-7f24-400e-9235-38467b0ba391",
        "metadata": {
          "vector_store_key": "1910.00458-7",
          "chunk_id": 51,
          "document_id": "1910.00458",
          "start_idx": 17496,
          "end_idx": 17916
        },
        "page_content": "500). For the sentiment analysis category, we used the Stanford Sentiment Treebank (SST-2) dataset from the GLUE benchmark BIBREF22 (around 60k train examples) and the Yelp dataset (around 430k train examples). For the paraphrase category, three paraphrasing datasets are used from the GLUE benchmark: Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), and Quora Question Pairs (",
        "type": "Document"
      },
      {
        "id": "9270562a-8e53-4e1d-b760-74d8fd0bc872",
        "metadata": {
          "vector_store_key": "1905.06566-7",
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "45e75b36-ee14-4963-8966-836c2b4786de",
        "metadata": {
          "vector_store_key": "1911.07228-1",
          "chunk_id": 24,
          "document_id": "1911.07228",
          "start_idx": 6150,
          "end_idx": 6611
        },
        "page_content": " corpus which are provided by VLSP 2016 - Named Entity Recognition. The dataset contains four different types of label: Location (LOC), Person (PER), Organization (ORG) and Miscellaneous - Name of an entity that do not belong to 3 types above (Table TABREF15). Although the corpus has more information about the POS and chunks, but we do not use them as features in our model. There are two folders with 267 text files of training data and 45 text files of test",
        "type": "Document"
      },
      {
        "id": "2e1622b2-1517-4239-8581-693f3aece329",
        "metadata": {
          "vector_store_key": "1711.02013-3",
          "chunk_id": 55,
          "document_id": "1711.02013",
          "start_idx": 20658,
          "end_idx": 21116
        },
        "page_content": "ings, between recurrent layers, and on recurrent states were (0.7, 0.5, 0.5) respectively. dataset contains 17M training tokens and has a vocabulary size of 44k words. The dataset is partitioned into a training set (first 99M characters) and a development set (last 1M characters) that is used to report performance. As this dataset contains various articles from Wikipedia, the longer term information (such as current topic) plays a bigger role than in the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "In which languages did the approach outperform the reported results?",
    "llm_answer": "Arabic, Portuguese, Russian, and Swedish.\n",
    "context": [
      {
        "id": "7fed6b92-48c7-48e6-8ad9-2412ea5449bc",
        "metadata": {
          "vector_store_key": "1612.05270-0",
          "chunk_id": 4,
          "document_id": "1612.05270",
          "start_idx": 1650,
          "end_idx": 2169
        },
        "page_content": " tested in eight different languages: Spanish, English, Italian, Arabic, German, Portuguese, Russian and Swedish. We compare our approach ranking in three international contests: TASS'15, SemEval'15-16 and SENTIPOLC'14, for Spanish, English and Italian respectively; the remaining languages are compared directly with the results reported in the literature. The experimental results locate our approach in good positions for all considered competitions; and excellent results in the other five languages tested. Finally",
        "type": "Document"
      },
      {
        "id": "635e5e6c-9cc3-4b1a-9a80-ed3f4fc00b6f",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 17,
          "document_id": "1910.04269",
          "start_idx": 6552,
          "end_idx": 7071
        },
        "page_content": " for feature extraction. An elaborate analysis of how deep architectures outperform their shallow counterparts is presented in this reseacrch. The difficulties in classifying perceptually similar languages like German and English were also put forward in this work. It is mentioned that the proposed approach is less robust to new speakers present in the test dataset. This method was able to achieve an accuracy of 91.2% on dataset comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize",
        "type": "Document"
      },
      {
        "id": "bb8a5a29-0724-48c9-af52-c94bd17f0f34",
        "metadata": {
          "vector_store_key": "1612.05270-1",
          "chunk_id": 3,
          "document_id": "1612.05270",
          "start_idx": 1304,
          "end_idx": 1873
        },
        "page_content": " given languages. Furthermore, our second contribution is to create a well-performing baseline to compare new sentiment classifiers in a broad range of languages or to bootstrap new sentiment analysis systems. Our approach is based on selecting the best text-transforming techniques that optimize some performance measures where the chosen techniques are robust to typical writing errors. In this context, we propose a robust multilingual sentiment analysis method, tested in eight different languages: Spanish, English, Italian, Arabic, German, Portuguese, Russian and",
        "type": "Document"
      },
      {
        "id": "173a0784-e08f-4515-935f-df7356f82e92",
        "metadata": {
          "vector_store_key": "1906.06448-9",
          "chunk_id": 59,
          "document_id": "1906.06448",
          "start_idx": 21919,
          "end_idx": 22444
        },
        "page_content": " also indicates the difficulty of problems from linguistics publications. Regarding non-monotone problems collected via crowdsourcing, there are very few non-monotone problems, so accuracy is 100%. Adding non-monotone problems to our test set is left for future work. Table 9 shows the evaluation results by type of linguistic phenomenon. While accuracy on problems involving NPIs and conditionals was improved on both upward and downward inferences, accuracy on problems involving conjunction and disjunction was improved on",
        "type": "Document"
      },
      {
        "id": "e285caa1-ce54-4575-8597-d8fbdec050fb",
        "metadata": {
          "vector_store_key": "1905.11901-1",
          "chunk_id": 6,
          "document_id": "1905.11901",
          "start_idx": 2085,
          "end_idx": 2580
        },
        "page_content": " , BIBREF19 , BIBREF20 , BIBREF21 . While semi-supervised and unsupervised approaches have been shown to be very effective for some language pairs, their effectiveness depends on the availability of large amounts of suitable auxiliary data, and other conditions being met. For example, the effectiveness of unsupervised methods is impaired when languages are morphologically different, or when training domains do not match BIBREF22  More broadly, this line of research still accepts the premise",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which is the baseline model?",
    "llm_answer": "The first example mentions a \"Baseline model\".  A different setup, using a specific layer configuration, is also called a \"baseline model\".  There is also a \"Model 1\" baseline.\n",
    "context": [
      {
        "id": "97be5370-e788-41d2-9b17-3e7739266f06",
        "metadata": {
          "vector_store_key": "2001.11381-3",
          "chunk_id": 138,
          "document_id": "2001.11381",
          "start_idx": 31860,
          "end_idx": 32196
        },
        "page_content": "ados. Dicho lo anterior, el Modelo 1 podr\u00eda ser considerado como nuestro propio baseline. A continuaci\u00f3n presentamos un protocolo de evaluaci\u00f3n manual de los resultados obtenidos. El experimento consisti\u00f3 en la generaci\u00f3n de 15 frases por cada uno de los tres modelos propuestos. Para cada modelo, se consideraron tres queries: $Q=$ {AM",
        "type": "Document"
      },
      {
        "id": "947ca67f-f47a-4cb3-91ff-40d57888bc24",
        "metadata": {
          "vector_store_key": "1909.09484-5",
          "chunk_id": 55,
          "document_id": "1909.09484",
          "start_idx": 19263,
          "end_idx": 19734
        },
        "page_content": " models and baselines, with a learning rate of 0.001 for supervised training and 0.0001 for reinforcement learning. In reinforcement learning, the decay parameter $\\lambda $ is set to 0.8. The weight decay is set to 0.001. And early stopping is performed on developing set. The experimental results of the proposed model and baselines will be analyzed from the following aspects. BPRA Results: As shown in Table TABREF35, most of the models have similar performance on BP",
        "type": "Document"
      },
      {
        "id": "95d982c1-df48-40f1-aea4-aa545ace30dc",
        "metadata": {
          "vector_store_key": "1909.02635-8",
          "chunk_id": 23,
          "document_id": "1909.02635",
          "start_idx": 9050,
          "end_idx": 9548
        },
        "page_content": " (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo",
        "type": "Document"
      },
      {
        "id": "27bce230-10fa-43c7-8e7d-0c72ae1b8e4e",
        "metadata": {
          "vector_store_key": "1910.07481-0",
          "chunk_id": 36,
          "document_id": "1910.07481",
          "start_idx": 13842,
          "end_idx": 14345
        },
        "page_content": "\". The first example in Table TABREF10 perfectly illustrate this ambiguity: the context clearly indicates that \"lui\" in the source sentence refers to \"ma fille\", which is located three sentences above, and should be translated into \"her\". In this case, the Baseline model predict the personal pronoun \"him\" while the Document model correctly predicts \"her\". It seems that the Baseline model does not benefit from any valuable information in the source sentence. Some might argue that the source sentence",
        "type": "Document"
      },
      {
        "id": "bc8ac242-0b5e-4c71-be0b-48d167015fad",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 43,
          "document_id": "1909.02764",
          "start_idx": 17421,
          "end_idx": 17831
        },
        "page_content": " layer. This setup is inspired by BIBREF38. We use a dropout rate of 0.3 in all layers and optimize with Adam BIBREF39 with a learning rate of $10^{-5}$ (These parameters are the same for all further experiments). We build on top of the Keras library with the TensorFlow backend. We consider this setup our baseline model. We train models on a variety of corpora, namely the common format published by BIBREF27",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the main contribution of the paper? ",
    "llm_answer": "The paper theoretically and empirically analyzes the problem of DIRL for domain adaptation when the marginal distribution P(Y) shifts across domains, and proposes a novel method to address the problem.\n",
    "context": [
      {
        "id": "13e47b9a-dcd7-4e18-be3d-956d52d8a575",
        "metadata": {
          "vector_store_key": "1904.08386-1",
          "chunk_id": 24,
          "document_id": "1904.08386",
          "start_idx": 9817,
          "end_idx": 10140
        },
        "page_content": " anonymous reviewers for their insightful comments. Additionally, we thank Nader Akoury, Garrett Bernstein, Chenghao Lv, Ari Kobren, Kalpesh Krishna, Saumya Lal, Tu Vu, Zhichao Yang, Mengxue Zhang and the UMass NLP group for suggestions that improved the paper's clarity, coverage of related work, and analysis experiments.",
        "type": "Document"
      },
      {
        "id": "72fd459a-a1d2-4e55-b521-a9788de0fdc0",
        "metadata": {
          "vector_store_key": "1910.09399-1",
          "chunk_id": 169,
          "document_id": "1910.09399",
          "start_idx": 65581,
          "end_idx": 66035
        },
        "page_content": " to contribute directly or indirectly to the expansion of the vast computer vision AI field. Looking into the future, an excellent extension from the works surveyed in this paper would be to give more independence to the several learning methods (e.g. less human intervention) involved in the studies as well as increasing the size of the output images. The authors declare that there is no conflict of interest regarding the publication of this article.",
        "type": "Document"
      },
      {
        "id": "e47b2d6a-c071-4a61-b692-68b484884e3d",
        "metadata": {
          "vector_store_key": "1908.07816-7",
          "chunk_id": 10,
          "document_id": "1908.07816",
          "start_idx": 4017,
          "end_idx": 4509
        },
        "page_content": " relate. It is the first time such an approach is designed with consideration for the human judges. Our main goal is to increase the objectivity of the results and reduce judges' mistakes due to out-of-context dialogs they have to evaluate. The rest of the paper unfolds as follows. Section SECREF2 discusses some related work. In Section SECREF3, we give detailed description of the methodology. We present experimental results and some analysis in Section SECREF4. The paper is concluded in",
        "type": "Document"
      },
      {
        "id": "53f36423-7b91-409d-b783-6727acc58060",
        "metadata": {
          "vector_store_key": "1909.08167-2",
          "chunk_id": 13,
          "document_id": "1909.08167",
          "start_idx": 4273,
          "end_idx": 4685
        },
        "page_content": "3 and the adversarial-learning-based DANN model BIBREF2 as an example, respectively. In summary, the contributions of this paper include: ($\\mathbf {i}$) We theoretically and empirically analyse the problem of DIRL for domain adaptation when the marginal distribution $\\rm {P}(\\rm {Y})$ shifts across domains. ($\\mathbf {ii}$) We proposed a novel method to address the problem and show how to incorporate it with",
        "type": "Document"
      },
      {
        "id": "04d575bd-e1b5-4737-b793-11679ffc109d",
        "metadata": {
          "vector_store_key": "1705.00108-9",
          "chunk_id": 49,
          "document_id": "1705.00108",
          "start_idx": 17614,
          "end_idx": 17740
        },
        "page_content": "urthy, Matt Gardner and Oren Etzioni for comments on earlier drafts that led to substantial improvements in the final version.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they get the formal languages?",
    "llm_answer": "The context describes training models on languages from various families, using multilingual corpora and treebanks.  It does not explain *how* the formal languages are obtained.\n",
    "context": [
      {
        "id": "d16841ee-127f-473f-af88-1d373637d8c0",
        "metadata": {
          "vector_store_key": "1910.05456-8",
          "chunk_id": 5,
          "document_id": "1910.05456",
          "start_idx": 1974,
          "end_idx": 2423
        },
        "page_content": " on the L1, i.e., the \"native language\", in neural network models. To this goal, we select a diverse set of eight source languages from different language families \u2013 Basque, French, German, Hungarian, Italian, Navajo, Turkish, and Quechua \u2013 and three target languages \u2013 English, Spanish and Zulu. We pretrain a neural sequence-to-sequence architecture on each of the source languages and then fine-tune the resulting models on small datasets in each",
        "type": "Document"
      },
      {
        "id": "51bd57fd-5cf8-4e72-a219-541eb5b26d2d",
        "metadata": {
          "vector_store_key": "1809.08510-5",
          "chunk_id": 47,
          "document_id": "1809.08510",
          "start_idx": 17158,
          "end_idx": 17644
        },
        "page_content": ", Russian, Arabic, Chinese, German, Spanish, French. We maintain the same procedure as described above. The hidden size of the language model was increased to 1024 with 16K BPE tokens being used. The first model was trained on English Russian, second was trained on English Russian Arabic and so on. For arabic we still trained from left to right even though naturally the language is read from right to left. We report the results in Figure 5 . As the number of languages increases the",
        "type": "Document"
      },
      {
        "id": "98ec5fad-9146-4cee-9f5a-42fbd39babf8",
        "metadata": {
          "vector_store_key": "1911.03894-7",
          "chunk_id": 4,
          "document_id": "1911.03894",
          "start_idx": 1502,
          "end_idx": 1983
        },
        "page_content": " as the investigation of their language modeling capacity, something that remains to be investigated in the case of, for instance, morphologically rich languages. We take advantage of the newly available multilingual corpus OSCAR BIBREF13 and train a monolingual language model for French using the RoBERTa architecture. We pretrain the model - which we dub CamemBERT- and evaluate it in four different downstream tasks for French: part-of-speech (POS) tagging, dependency parsing,",
        "type": "Document"
      },
      {
        "id": "75211848-4da6-43f4-96c3-50172aa7b29d",
        "metadata": {
          "vector_store_key": "1601.06068-8",
          "chunk_id": 1,
          "document_id": "1601.06068",
          "start_idx": 440,
          "end_idx": 887
        },
        "page_content": "-specific grammars BIBREF5 , strongly-typed CCG grammars BIBREF6 , BIBREF7 , or neural networks without requiring any grammar BIBREF8 . These methods are sensitive to the words used in a question and their word order, making them vulnerable to unseen words and phrases. Furthermore, mismatch between natural language and Freebase makes the problem even harder. For example, Freebase expresses the fact that \u201cCzech is the official language of Czech",
        "type": "Document"
      },
      {
        "id": "155700d7-8f16-4a86-94fe-c6fd91b1b068",
        "metadata": {
          "vector_store_key": "1906.04571-0",
          "chunk_id": 43,
          "document_id": "1906.04571",
          "start_idx": 15701,
          "end_idx": 16142
        },
        "page_content": " that are grammatical, we additionally consider the following log ratio (i.e., the grammatical phrase over the ungrammatical phrase): DISPLAYFORM0  We trained the linear parameterization using UD treebanks for Spanish, Hebrew, French, and Italian (see tab:data). For each of the four languages, we parsed one million sentences from Wikipedia (May 2018 dump) using BIBREF24 's parser and extracted taggings and lemmata using the method of BIB",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is a confusion network or lattice?",
    "llm_answer": "Graph-like structures where arcs connected by nodes represent hypothesized words.\n",
    "context": [
      {
        "id": "3b40fdcf-bddb-447e-8fb7-4f42dd408f4f",
        "metadata": {
          "vector_store_key": "1810.13024-1",
          "chunk_id": 27,
          "document_id": "1810.13024",
          "start_idx": 8547,
          "end_idx": 9018
        },
        "page_content": "ices and confusion networks, these metrics are computed across all arcs in the network.  A number of important downstream and upstream applications rely on accurate confidence scores in graph-like structures, such as confusion networks (CN) in Fig. 2 and lattices in Fig. 2 , where arcs connected by nodes represent hypothesised words. This section describes an extension of BiRNNs to CNs and lattices. Fig. 2 shows that compared to 1-best sequences in Fig. 2 , each node",
        "type": "Document"
      },
      {
        "id": "6f89073c-b4ac-4a3b-8fac-6e502a73cb57",
        "metadata": {
          "vector_store_key": "1810.13024-1",
          "chunk_id": 4,
          "document_id": "1810.13024",
          "start_idx": 1643,
          "end_idx": 2202
        },
        "page_content": "-best hypotheses were reported, the impact of these approaches on all hypotheses available within confusion networks and lattices has not been investigated. Extending confidence estimation to confusion network and lattice structures can be straightforward for some approaches, such as decision trees, and challenging for others, such as recurrent forms of neural networks. The previous work on encoding graph structures into neural networks BIBREF13 has mostly focused on embedding lattices into a fixed dimensional vector representation BIBREF14 , BIBREF15 .",
        "type": "Document"
      },
      {
        "id": "51b47327-abc0-4e96-8ad1-b76a19aa346b",
        "metadata": {
          "vector_store_key": "1902.09087-8",
          "chunk_id": 74,
          "document_id": "1902.09087",
          "start_idx": 27297,
          "end_idx": 27569
        },
        "page_content": "\u201d(founder), so it loses the most important information. While in lattice, although overlaps are limited, \u201cWangZhan\u201d(website, \u201cWang\u201d web, \u201cZhan\u201d station) can match \u201cWangZhi\u201d(Internet addresses, \u201cWang\u201d web, \u201cZhi\u201d addresses) and also relate to \u201cDaoHang\u201d(navigate), from which",
        "type": "Document"
      },
      {
        "id": "a64b0b3a-02ef-45d2-9c99-3698ba97232b",
        "metadata": {
          "vector_store_key": "1810.13024-1",
          "chunk_id": 26,
          "document_id": "1810.13024",
          "start_idx": 8202,
          "end_idx": 8547
        },
        "page_content": "all}(\\theta ) = \\dfrac{\\text{TP}(\\theta )}{\\text{TP}(\\theta ) + \\text{FN}(\\theta )}$$   (Eq. 10)  for a range of thresholds $\\theta $ , where TP are true positives, FP and FN are false positives and negatives. When evaluating performance on lattices and confusion networks, these metrics are computed across all arcs in the network.  A number of",
        "type": "Document"
      },
      {
        "id": "5ab42077-f97d-4a24-bddf-c531e348faa2",
        "metadata": {
          "vector_store_key": "1810.13024-1",
          "chunk_id": 8,
          "document_id": "1810.13024",
          "start_idx": 3334,
          "end_idx": 3790
        },
        "page_content": ". Section \"Confusion Network and Lattice Extensions\" describes the extension to confusion network and lattice structures. Experimental results are presented in Section \"Experiments\" . The conclusions drawn from this work are given in Section \"Conclusions\" .  Fig. 1 shows the simplest form of the BiRNN BIBREF16 . Unlike its uni-directional version, the BiRNN makes use of two recurrent states, one going in the forward direction in time $\\overrightarrow{\\",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How close do clusters match to ground truth tone categories?",
    "llm_answer": "In Mandarin, the 4 clusters correspond very well with the 4 phonemic tone categories.  In Cantonese, the 5 clusters correspond to low-falling, mid-level, high-level, mid-rising, and low-rising tones.\n",
    "context": [
      {
        "id": "0ca47f32-043b-4632-a2c8-5cf357e9cddb",
        "metadata": {
          "vector_store_key": "1910.08987-3",
          "chunk_id": 27,
          "document_id": "1910.08987",
          "start_idx": 10105,
          "end_idx": 10494
        },
        "page_content": " 5 clusters A-E correspond to low-falling, mid-level, high-level, mid-rising, and low-rising tones. Tone clustering in Cantonese is expected to be more difficult than in Mandarin because of 6 contrastive tones, rather than 4. The model is more effective at clustering the higher tones (1, 2, 3), and less effective at clustering the lower tones (4, 5, 6), particularly tone 4 (low-falling)",
        "type": "Document"
      },
      {
        "id": "c2696d2f-0961-46dc-b6e0-8d6b90884b9e",
        "metadata": {
          "vector_store_key": "1710.10609-1",
          "chunk_id": 49,
          "document_id": "1710.10609",
          "start_idx": 18530,
          "end_idx": 19040
        },
        "page_content": " and compared with the K-means approach. We used the partitioning formed by the 8 categories (from which the utterance pairs were selected) as the ground truth clustering. Table TABREF20 summarizes the results. We observe that for K-means algorithm, the vectors generated from sequence to sequence model perform better than the vectors generated using paragraph to vector for both the domains. This is expected as the vectors generated from sequence to sequence model encode some adjacency information as well.",
        "type": "Document"
      },
      {
        "id": "e59d617f-e4a2-4664-b145-c159584a70e1",
        "metadata": {
          "vector_store_key": "1910.08987-4",
          "chunk_id": 25,
          "document_id": "1910.08987",
          "start_idx": 9456,
          "end_idx": 9895
        },
        "page_content": " the latent space learned by the autoencoders and the clustering output. Our model found 4 tone clusters in Mandarin, matching the number of phonemic tones (Table TABREF12) and 5 in Cantonese, which is one fewer than the number of phonemic tones (Table TABREF13). In Mandarin, the 4 clusters correspond very well with the the 4 phonemic tone categories, and the generated contours closely match the ground truth in Figure FIGREF2. There is",
        "type": "Document"
      },
      {
        "id": "ec29846a-e9fc-4de5-97aa-ca355e2cb97d",
        "metadata": {
          "vector_store_key": "1910.08987-3",
          "chunk_id": 26,
          "document_id": "1910.08987",
          "start_idx": 9707,
          "end_idx": 10105
        },
        "page_content": " categories, and the generated contours closely match the ground truth in Figure FIGREF2. There is some overlap between tones 3 and 4; this is because tone 3 is sometimes realized a low-falling tone without the final rise, a process known as half T3 sandhi BIBREF19, thus, it may overlap with tone 4 (falling tone). In Cantonese, the 5 clusters A-E correspond to low-falling, mid-level, high-level,",
        "type": "Document"
      },
      {
        "id": "093f2f90-f670-432a-b31c-aa6ae276520d",
        "metadata": {
          "vector_store_key": "1910.08987-4",
          "chunk_id": 31,
          "document_id": "1910.08987",
          "start_idx": 11511,
          "end_idx": 11972
        },
        "page_content": " phrase. To test this hypothesis, we evaluate the model on only the first syllable of every word, which eliminates carry-over and declination effects (Table TABREF14). In both Mandarin and Cantonese, the clustering is more accurate when using only the first syllables, compared to using all of the syllables. We propose a model for unsupervised clustering and discovery of phonemic tones in tonal languages, using spoken words as input. Our model extracts the F",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what are the evaluation metrics?",
    "llm_answer": "Task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, ROUGE, BLEU, METEOR, micro-F1, BPRA, F1-score, user satisfaction degree, dialogue fluency.\n",
    "context": [
      {
        "id": "4786191e-fed1-4406-89b6-7b8835719e5f",
        "metadata": {
          "vector_store_key": "1709.10217-2",
          "chunk_id": 18,
          "document_id": "1709.10217",
          "start_idx": 6394,
          "end_idx": 6850
        },
        "page_content": " evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the number of total tasks. User satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied, neutral, satisfied and very satisfied, respectively. Response fluency: There are three scores -1, 0, 1, which indicate nonfluency, neutral, fluency. Number of dialogue turns: The number of utterances in a task",
        "type": "Document"
      },
      {
        "id": "cad945f1-4338-4c8c-988a-66e53227489d",
        "metadata": {
          "vector_store_key": "1607.06025-6",
          "chunk_id": 26,
          "document_id": "1607.06025",
          "start_idx": 10088,
          "end_idx": 10607
        },
        "page_content": "REF37 , the evaluation metrics of text-generating models fall into three categories: manual evaluation, automatic evaluation metrics, task-based evaluation. In evaluation based on human judgment each generated textual example is inspected manually. The automatic evaluation metrics, like ROUGE, BLEU and METEOR, compare human texts and generated texts. BIBREF38 shows METEOR has the strongest correlation with human judgments in image description evaluation. The last category is task-based evaluation, where the impact",
        "type": "Document"
      },
      {
        "id": "952c3ead-6461-49aa-a21f-f08fccb02046",
        "metadata": {
          "vector_store_key": "1909.07734-0",
          "chunk_id": 17,
          "document_id": "1909.07734",
          "start_idx": 6722,
          "end_idx": 7207
        },
        "page_content": ", sadness, anger, and neutral. Evaluation of submissions was done using only utterances with these four labels. Utterances with labels other than the above four (i.e., surprise, disgust, fear or non-neutral) were discarded and not used in the evaluation. Scripts for verifying and evaluating the submissions were made available online. We used micro-F1 as the comparison metric. A total of eleven teams submitted their evaluations, and are presented in the online leaderboard. Seven of",
        "type": "Document"
      },
      {
        "id": "140ad6e1-f0ef-4e66-a00e-12f677b6f352",
        "metadata": {
          "vector_store_key": "1909.09484-3",
          "chunk_id": 46,
          "document_id": "1909.09484",
          "start_idx": 15874,
          "end_idx": 16378
        },
        "page_content": " of the dialogue state tracker, it aims to show the impact of the dialogue state tracker on the dialogue policy maker; (2) the accuracy of dialogue policy maker, it aims to explain the performance of different methods of constructing dialogue policy; (3) the quality of the final response, it aims to explain the impact of the dialogue policy on the final dialogue response. The evaluation metrics are listed as follows: BPRA: Belief Per-Response Accuracy (BPRA) tests the ability to generate the correct",
        "type": "Document"
      },
      {
        "id": "dc58289f-90f2-4f31-9a31-296d00bb01a9",
        "metadata": {
          "vector_store_key": "1709.10217-2",
          "chunk_id": 11,
          "document_id": "1709.10217",
          "start_idx": 4141,
          "end_idx": 4657
        },
        "page_content": ", in which only the released data can be used for training and developing. The other is an open evaluation that allow to collect external data for training and developing. For task 1, we use F1-score as evaluation metric. For the task-oriented dialogue systems, the best way for evaluation is to use the online human-computer dialogue. After finishing an online human-computer dialogue with a dialogue system, the human then manually evaluate the system by using the metrics of user satisfaction degree, dialogue flu",
        "type": "Document"
      }
    ]
  },
  {
    "query": "which datasets were used in evaluation?",
    "llm_answer": "Clean-labeled data, noisy-labeled training datasets (movie, laptop, and restaurant reviews), and a dataset (SCI) of 220 sentences from scientific literature.\n",
    "context": [
      {
        "id": "87d9afb2-c546-418c-b5b1-18b5e1abe1e5",
        "metadata": {
          "vector_store_key": "1605.08675-6",
          "chunk_id": 134,
          "document_id": "1605.08675",
          "start_idx": 49919,
          "end_idx": 50491
        },
        "page_content": ", used at this stage, substantially differs from the one used during the development (see section SECREF80 ). A configuration for the final evaluation has been prepared based on results of the experiments. All of the tested versions share the following features: no question analysis corrections, question classification and query generation solutions which proved best in the previous experiments (see section SECREF18 ), a retrieved set of documents including 20 articles, no minimal confidence, singe sentence context with title. Tested solutions differ with respect to",
        "type": "Document"
      },
      {
        "id": "6e6ace1d-41d7-457e-a210-1e7d19864073",
        "metadata": {
          "vector_store_key": "1909.00124-3",
          "chunk_id": 39,
          "document_id": "1909.00124",
          "start_idx": 12394,
          "end_idx": 12855
        },
        "page_content": " during training. Summary statistics of the training, validation, and test data are shown in Table TABREF9. Noisy-labeled Training Datasets. For the above three domains (movie, laptop, and restaurant), we collected 2,000 reviews for each domain from the same review source. We extracted sentences from each review and assigned review's label to its sentences. Like previous work, we treat 4 or 5 stars as positive and 1 or 2 stars as negative. The data is noisy",
        "type": "Document"
      },
      {
        "id": "faa265f1-ef3e-4ef9-a60b-c6f40ff9ef6c",
        "metadata": {
          "vector_store_key": "1909.00124-3",
          "chunk_id": 40,
          "document_id": "1909.00124",
          "start_idx": 12671,
          "end_idx": 13117
        },
        "page_content": " we treat 4 or 5 stars as positive and 1 or 2 stars as negative. The data is noisy because a positive (negative) review can contain negative (positive) sentences, and there are also neutral sentences. This gives us three noisy-labeled training datasets. We still use the same test sets as those for the clean-labeled datasets. Summary statistics of all the datasets are shown in Table TABREF9. Experiment 1: Here we use the clean-labeled data (i.",
        "type": "Document"
      },
      {
        "id": "77a2c284-62f0-43e0-8bf9-52a2d068ed04",
        "metadata": {
          "vector_store_key": "2003.07433-1",
          "chunk_id": 49,
          "document_id": "2003.07433",
          "start_idx": 19311,
          "end_idx": 19770
        },
        "page_content": " well as dimensions' text analysis scores. We chose three categories based on the three different surveys: 1) DOSPERT scale; 2) BSSS scale; and 3) VIAS scale. Dimension selection: We have to define the word sets (also called dimension) for each category. We chose one dimension for each of the questions under each category to reflect real survey system evaluation. Our chosen categories are state in Fig FIGREF20. Score calculation $\\alpha $-score: $\\alpha $",
        "type": "Document"
      },
      {
        "id": "0d593f74-7474-41cc-9928-ce68ac96f714",
        "metadata": {
          "vector_store_key": "1802.05574-2",
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what are the baselines?",
    "llm_answer": "ANRES_Sing_R, ANRES_Sing_Z, ANRES_Seq_R, ANRES_Seq_Z;  content, content-network, and image-based models.\n",
    "context": [
      {
        "id": "fe0f18bc-f5e9-4a30-bd8f-1b27de376893",
        "metadata": {
          "vector_store_key": "2004.01878-5",
          "chunk_id": 40,
          "document_id": "2004.01878",
          "start_idx": 14526,
          "end_idx": 14915
        },
        "page_content": " normal distribution $\\mathcal {N}(0, 1)$. In summary, the following four baselines are designed: ANRES_Sing_R: randomly initializing the states for each single trading day. ANRES_Sing_Z: initializing the states as zeros for each single trading day. ANRES_Seq_R: randomly initializing the first states for each trading sequence only. ANRES_Seq_Z: initializing the first states as zeros for",
        "type": "Document"
      },
      {
        "id": "0b77285e-fdf5-4273-8639-5afcd5734567",
        "metadata": {
          "vector_store_key": "2001.02380-3",
          "chunk_id": 140,
          "document_id": "2001.02380",
          "start_idx": 51470,
          "end_idx": 51956
        },
        "page_content": " baselines. The left, middle and right panels in Figure FIGREF40 correspond to measurements when all signals are included, only cases contained entirely in the head EDUs shown to the model, and only DMs, respectively. The scenario on the left is rather unreasonable and is included only for completeness: here the model is also penalized for not detecting signals such as lexical chains, part of which is outside the units that the model is being shown. An example of such a case can be",
        "type": "Document"
      },
      {
        "id": "17d3256d-56f6-4b9d-983d-669c916f7fae",
        "metadata": {
          "vector_store_key": "1709.08299-2",
          "chunk_id": 23,
          "document_id": "1709.08299",
          "start_idx": 9435,
          "end_idx": 10003
        },
        "page_content": " questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models. The",
        "type": "Document"
      },
      {
        "id": "947ca67f-f47a-4cb3-91ff-40d57888bc24",
        "metadata": {
          "vector_store_key": "1909.09484-5",
          "chunk_id": 55,
          "document_id": "1909.09484",
          "start_idx": 19263,
          "end_idx": 19734
        },
        "page_content": " models and baselines, with a learning rate of 0.001 for supervised training and 0.0001 for reinforcement learning. In reinforcement learning, the decay parameter $\\lambda $ is set to 0.8. The weight decay is set to 0.001. And early stopping is performed on developing set. The experimental results of the proposed model and baselines will be analyzed from the following aspects. BPRA Results: As shown in Table TABREF35, most of the models have similar performance on BP",
        "type": "Document"
      },
      {
        "id": "25bb2afe-7835-48e9-a6f8-dbdbbde2116d",
        "metadata": {
          "vector_store_key": "1902.06843-8",
          "chunk_id": 91,
          "document_id": "1902.06843",
          "start_idx": 35139,
          "end_idx": 35416
        },
        "page_content": "s by -0.27. Baselines: To test the efficacy of our multi-modal framework for detecting depressed users, we compare it against existing content, content-network, and image-based models (based on the aforementioned general image feature, facial presence, and facial expressions.)",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What monolingual word representations are used?",
    "llm_answer": "Monolingual word embeddings.\n",
    "context": [
      {
        "id": "d35dae68-402e-4729-ad60-04dd71ca3a29",
        "metadata": {
          "vector_store_key": "1911.00069-4",
          "chunk_id": 8,
          "document_id": "1911.00069",
          "start_idx": 3095,
          "end_idx": 3624
        },
        "page_content": " across a variety of language families, without using aligned parallel corpora or machine translation systems. We organize the paper as follows. In Section 2 we provide an overview of our approach. In Section 3 we describe how to build monolingual word embeddings and learn a linear mapping between two languages. In Section 4 we present a neural network architecture for the source-language (English). In Section 5 we evaluate the performance of the proposed approach for a number of target languages. We discuss related work in",
        "type": "Document"
      },
      {
        "id": "bfe6fabb-3f4c-451d-a2f2-de42e6f1eeda",
        "metadata": {
          "vector_store_key": "2002.02427-2",
          "chunk_id": 20,
          "document_id": "2002.02427",
          "start_idx": 7113,
          "end_idx": 7613
        },
        "page_content": ".g., punctuation marks, positive and negative emoticons, quotations, personal pronouns, tweet's length, named entities) while others are language-dependent relying on dedicated lexicons (e.g., negation, opinion lexicons, opposition words). Several classical machine learning classifiers were tested with several feature combinations, among them Random Forest (RF) achieved the best result with all features. Neural model with monolingual embeddings. We used Convolutional Neural Network (CNN) network",
        "type": "Document"
      },
      {
        "id": "a817be83-ba5b-4889-9dd6-a92d6e0541c3",
        "metadata": {
          "vector_store_key": "1911.00069-8",
          "chunk_id": 13,
          "document_id": "1911.00069",
          "start_idx": 5108,
          "end_idx": 5566
        },
        "page_content": " vocabulary $V=|\\mathcal {V}|$ for efficient representation. It also aims to capture semantic similarities between the words based on their distributional properties in large samples of monolingual data. Cross-lingual word embedding models try to build word embeddings across multiple languages BIBREF15, BIBREF16. One approach builds monolingual word embeddings separately and then maps them to the same vector space using a bilingual dictionary BIBREF17, B",
        "type": "Document"
      },
      {
        "id": "f25202aa-5bfc-4704-b797-a9861607c5a0",
        "metadata": {
          "vector_store_key": "2002.02427-2",
          "chunk_id": 18,
          "document_id": "2002.02427",
          "start_idx": 6346,
          "end_idx": 6847
        },
        "page_content": "-art models in monolingual ID but to investigate which of the monolingual architectures (neural or feature-based) can achieve comparable results with existing systems. The result can show which kind of features works better in the monolingual settings and can be employed to detect irony in a multilingual setting. In addition, it can show us to what extend ID is language dependent by comparing their results to multilingual results. Two models have been built, as explained below. Prior to learning,",
        "type": "Document"
      },
      {
        "id": "210faf2a-e5eb-4e73-95f3-530397d53bac",
        "metadata": {
          "vector_store_key": "1911.00069-8",
          "chunk_id": 12,
          "document_id": "1911.00069",
          "start_idx": 4749,
          "end_idx": 5108
        },
        "page_content": " for many NLP applications BIBREF12, BIBREF13, BIBREF14. A monolingual word embedding model maps words in the vocabulary $\\mathcal {V}$ of a language to real-valued vectors in $\\mathbb {R}^{d\\times 1}$. The dimension of the vector space $d$ is normally much smaller than the size of the vocabulary $V=|\\mathcal {V}|$ for efficient representation. It also aims",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Do they build one model per topic or on all topics?",
    "llm_answer": "The context mentions explicitly modeling different types of questions, but doesn't state whether one model is built per topic or across all topics.\n",
    "context": [
      {
        "id": "e115f3ad-cfb7-4243-b123-51a669484bbc",
        "metadata": {
          "vector_store_key": "1703.04617-0",
          "chunk_id": 55,
          "document_id": "1703.04617",
          "start_idx": 14584,
          "end_idx": 15140
        },
        "page_content": "ity. The previous models are often trained for all questions without explicitly discriminating different question types; however, for a target question, both the common features shared by all questions and the specific features for a specific type of question are further considered in this paper, as they could potentially obey different distributions. In this paper we further explicitly model different types of questions in the end-to-end training. We start from a simple way to first analyze the word frequency of all questions, and obtain top-10 most",
        "type": "Document"
      },
      {
        "id": "3d5228b4-3b7a-46f1-98b1-420bad93ca51",
        "metadata": {
          "vector_store_key": "1804.08139-4",
          "chunk_id": 34,
          "document_id": "1804.08139",
          "start_idx": 12806,
          "end_idx": 13320
        },
        "page_content": " four models because each task-specific classifier should consider the output of shared layer which is quite unstable during the beginning of training phrase. Moreover, benefit from the attention mechanism which is useful in feature extraction, SA-TML and DA-MTL are converged much more quickly than the rest of models. Since all the tasks share the same sentence encoding layer, the query vector INLINEFORM0 of each task determines which part of the sentence to attend. Thus, similar tasks should have the similar",
        "type": "Document"
      },
      {
        "id": "c81be5ed-28b9-4d23-9114-e62232fb4ce9",
        "metadata": {
          "vector_store_key": "1709.07916-0",
          "chunk_id": 14,
          "document_id": "1709.07916",
          "start_idx": 5355,
          "end_idx": 5826
        },
        "page_content": "cancer\", and \u201cinfluenza\" into a topic that has an overall \u201cdisease\" theme BIBREF44 , BIBREF45 . Topic modeling has a wide range of applications in health and medical domains such as predicting protein-protein relationships based on the literature knowledge BIBREF46 , discovering relevant clinical concepts and structures in patients' health records BIBREF47 , and identifying patterns of clinical events in a cohort of brain cancer patients BIBREF48 . Among topic models",
        "type": "Document"
      },
      {
        "id": "4875f844-60e6-479f-937c-beed69ed9d3d",
        "metadata": {
          "vector_store_key": "1705.07368-6",
          "chunk_id": 19,
          "document_id": "1705.07368",
          "start_idx": 7527,
          "end_idx": 7989
        },
        "page_content": "0 belong to the same topic is relaxed, and replaced with a distribution over topics INLINEFORM1 . In the model's assumed generative process, for each word INLINEFORM2 in document INLINEFORM3 , a topic assignment INLINEFORM4 is drawn via INLINEFORM5 , then the word is drawn from the chosen topic INLINEFORM6 . The mixed membership formalism provides a useful compromise between model flexibility and statistical efficiency: the INLINEFORM7 topics INLINEFORM8 are",
        "type": "Document"
      },
      {
        "id": "60c0fd0d-43e7-4f3d-abc8-9dec54cb98fd",
        "metadata": {
          "vector_store_key": "1604.05781-5",
          "chunk_id": 37,
          "document_id": "1604.05781",
          "start_idx": 12804,
          "end_idx": 13245
        },
        "page_content": "T$ and documents are considered as mixtures of topics via $P(T|d)$ , the probability of topic $T$ given document $d$ . These distributions are then found via statistical inference given the observed distributions of unigrams across documents. The total number of topics is a parameter chosen by the practitioner. For this study we used the MALLET v2.0.8RC3 topic modeling toolkit BIBREF40 for model inference. By inspecting the most probable",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Do they quantitavely or qualitatively evalute the output of their low-rank approximation to verify the grouping of lexical items?",
    "llm_answer": "Quantitatively.  An intrinsic evaluation using human annotation is described.\n",
    "context": [
      {
        "id": "c57fab78-e81f-4a1c-927b-3cffe07d5dc4",
        "metadata": {
          "vector_store_key": "1807.09671-5",
          "chunk_id": 28,
          "document_id": "1807.09671",
          "start_idx": 10469,
          "end_idx": 10903
        },
        "page_content": ", hence the help of automatic summarization. Our extensions of this work are along three dimensions: (i) we crack the \u201cblack-box\u201d of the low-rank approximation algorithm to understand if it indeed allows lexically-diverse but semantically-similar items to share co-occurrence statistics; (ii) we compare the ILP-based summarization framework with state-of-the-art baselines, including a popular neural encoder-decoder model for summar",
        "type": "Document"
      },
      {
        "id": "add7abfa-3dee-4501-8e5f-2433a3f3dbaa",
        "metadata": {
          "vector_store_key": "1605.08675-1",
          "chunk_id": 136,
          "document_id": "1605.08675",
          "start_idx": 50790,
          "end_idx": 51265
        },
        "page_content": " results of the final evaluation, expressed by recall, precision, F1 measure and Mean Reciprocal Rank (MRR). Standard deviations of these values have been obtained by bootstrap resampling of the test set. Additionally, precision obtained by automatic evaluation has been added, where applicable. As we can see, only a small percentage of questions is handled by the quantitative entities recognition. NER-based solutions deal with slightly more (Nerf) or less (Liner2) than a",
        "type": "Document"
      },
      {
        "id": "50128ae6-a791-40f8-a9dd-cc18de5a12da",
        "metadata": {
          "vector_store_key": "1807.09671-5",
          "chunk_id": 11,
          "document_id": "1807.09671",
          "start_idx": 4233,
          "end_idx": 4762
        },
        "page_content": " allows different sentences to share co-occurrence statistics. For example, \u201cThe activity with the bicycle parts\" will be allowed to partially contain \u201cbike elements\" although the latter phrase does not appear in the sentence. The low-rank matrix approximation provides an effective way to implicitly group lexically-diverse but semantically-similar expressions. It can handle out-of-vocabulary expressions and domain-specific terminologies well, hence being a more principled approach than heuristically calculating similarities",
        "type": "Document"
      },
      {
        "id": "47ff6f02-f3cf-42d5-b8d0-a6d3f1386117",
        "metadata": {
          "vector_store_key": "1807.09671-5",
          "chunk_id": 54,
          "document_id": "1807.09671",
          "start_idx": 19986,
          "end_idx": 20421
        },
        "page_content": " an intrinsic evaluation to test whether the low-rank approximation does capture similar concepts or not. An example of the annotation is shown in Table TABREF15 , where phrases in the student responses that are semantically the same as the summary phrases are highlighted with the same color by human annotators. For example, \u201cerror bounding\" (S2), \u201cerror boundary\" (S4), \u201cfinding that error\" (S3), and \u201cdetermining the critical value",
        "type": "Document"
      },
      {
        "id": "b8e17f6e-8b58-4300-aa34-8a72380eb90f",
        "metadata": {
          "vector_store_key": "2004.01694-2",
          "chunk_id": 30,
          "document_id": "2004.01694",
          "start_idx": 12004,
          "end_idx": 12422
        },
        "page_content": " at WMT15, i. e., we run 1,000 iterations of the rankings recorded with Appraise followed by clustering (significance level $\\alpha =0.05$). Table TABREF17 shows the TrueSkill scores for each translation resulting from the evaluations by expert and non-expert translators. We find that translation expertise affects the judgement of MT$_1$ and H$_A$, where the rating gap is wider for the expert raters. This indicates",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How well does their system perform on the development set of SRE?",
    "llm_answer": "The provided text states that the system was evaluated on the SRE 2016 development set, but doesn't give the specific performance results.\n",
    "context": [
      {
        "id": "7d0ea89a-6851-44d9-8693-20c3c7a05240",
        "metadata": {
          "vector_store_key": "1707.03904-4",
          "chunk_id": 12,
          "document_id": "1707.03904",
          "start_idx": 4820,
          "end_idx": 5304
        },
        "page_content": "5\\%$ on Quasar-S and Quasar-T, while human performance is $50\\%$ and $60.6\\%$ respectively. For the automatic systems, we see an interesting tension between searching and reading accuracies \u2013 retrieving more documents in the search phase leads to a higher coverage of answers, but makes the comprehension task more difficult. We also collect annotations on a subset of the development set questions to allow researchers to analyze the categories in which their system performs well or",
        "type": "Document"
      },
      {
        "id": "2f2765aa-c10b-4eb8-a30f-f676ff2facee",
        "metadata": {
          "vector_store_key": "1701.09123-3",
          "chunk_id": 99,
          "document_id": "1701.09123",
          "start_idx": 37380,
          "end_idx": 37854
        },
        "page_content": ", 1.72 and 1.85 respectively over the best single clustering model (CW600). It is worth mentioning that our models do not score best in the development data. As the development data is closer in style and genre to the training data BIBREF31 , this may suggest that our system generalizes better on test data that is not close to the training data; indeed, the results reported in Section SECREF79 seem to confirm this hypothesis. We also compared our results with respect to",
        "type": "Document"
      },
      {
        "id": "fa1a7cbe-79c4-40d1-a6f7-d8eae14fafc3",
        "metadata": {
          "vector_store_key": "2001.02380-9",
          "chunk_id": 96,
          "document_id": "2001.02380",
          "start_idx": 37671,
          "end_idx": 38069
        },
        "page_content": ", we may note that the overall performance of the model, with an F-Score of 44.37, is not bad, though below the performance of state-of-the-art full discourse parsers (see BIBREF49) \u2013 this is to be expected, since the model is not aware of the entire RST tree, rather looking only at EDU pairs out of context, and given that standard scores on RST-DT come from a larger and more homogeneous corpus,",
        "type": "Document"
      },
      {
        "id": "e4138c30-0530-4660-8702-505ccbb2431c",
        "metadata": {
          "vector_store_key": "1611.00514-1",
          "chunk_id": 5,
          "document_id": "1611.00514",
          "start_idx": 2202,
          "end_idx": 2692
        },
        "page_content": " , we describe experimental evaluation of the system on the SRE 2016 development set. Finally, we present a timing analysis of the system in Section SECREF6 . The fixed training condition is used to build our speaker recognition system. Only conversational telephone speech data from datasets released through the linguistic data consortium (LDC) have been used, including NIST SRE 2004-2010 and the Switchboard corpora (Switchboard Cellular Parts I and II, Switchboard2 Phase I,II and III)",
        "type": "Document"
      },
      {
        "id": "eceb9ef2-a460-4a71-b84e-122a4fc7ddef",
        "metadata": {
          "vector_store_key": "1909.11833-3",
          "chunk_id": 34,
          "document_id": "1909.11833",
          "start_idx": 11056,
          "end_idx": 11491
        },
        "page_content": " and hidden size is 50. We apply a dropout rate of 0.1 for the input to each module. The hidden size of RNN is 125. During training, we pick the best model with highest joint goal score on development set and report the result on the test set. For DSTC2, we adhere to the standard procedure to use the N-best list from the noisy ASR results for testing. The ASR results are very noisy. We experimented with several strategies and ended",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which of the classifiers showed the best performance?",
    "llm_answer": "Support Vector Machine (SVM)\n",
    "context": [
      {
        "id": "6ca2ba29-58f2-4c59-84a8-838e87eda9c0",
        "metadata": {
          "vector_store_key": "2003.06279-2",
          "chunk_id": 55,
          "document_id": "2003.06279",
          "start_idx": 21474,
          "end_idx": 21889
        },
        "page_content": "ifier. Similarly to the results observed for Glove and Word2vec, the gain in performance obtained for kNN is low compared when compared to other methods. While Figures FIGREF14 \u2013 FIGREF16 show the relative behavior in the accuracy, it still interesting to observe the absolute accuracy rate obtained with the classifiers. In Table TABREF17, we show the best accuracy rate (i.e. $\\max \\Gamma _+ = \\max _p \\Gamma _+(p",
        "type": "Document"
      },
      {
        "id": "73fa6cd4-5005-4f0b-8af6-588187c6315b",
        "metadata": {
          "vector_store_key": "1707.03764-0",
          "chunk_id": 42,
          "document_id": "1707.03764",
          "start_idx": 16277,
          "end_idx": 16789
        },
        "page_content": " engineering. However, we believe that this is too strong a conclusion to draw from this limited study, since several factors specific to this setting need to be taken into account. For one, a support vector machine clearly outperforms other classifiers, but this does not mean that this is an inherently more powerful. Rather, we expect that an SVM is the best choice for the given amount of training data, but with more training data, a neural network-based approach would achieve better results. Regarding the",
        "type": "Document"
      },
      {
        "id": "1de66b24-e45c-4628-91b3-504ea78f74cd",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 45,
          "document_id": "2001.00137",
          "start_idx": 17426,
          "end_idx": 17822
        },
        "page_content": "Net penalty; Nearest Centroid with Euclidian metric, where classification is done by representing each class with a centroid; Bernoulli Naive Bayes with smoothing parameter $alpha=10^{-2}$; K-means clustering with 2 clusters and L2 penalty; and Logistic Regression classifier with L2 penalty, tolerance of $10^{-4}$ and regularization term of $1.0$. Most often, the best performing classifier was",
        "type": "Document"
      },
      {
        "id": "66de4f3c-3303-44fa-8833-a4b3fe9f2ea9",
        "metadata": {
          "vector_store_key": "1809.05752-5",
          "chunk_id": 37,
          "document_id": "1809.05752",
          "start_idx": 15241,
          "end_idx": 15722
        },
        "page_content": " zhang2011comparative found to improve classifier performance. Starting with the approach taken by McCoy et al. mccoy2015clinical, who used aggregate cosine similarity scores to compute domain similarity directly from their TF-IDF vector space model, we extend this method by training a suite of three-layer multilayer perceptron (MLP) and radial basis function (RBF) neural networks using a variety of parameters to compare performance. We employ the Keras deep learning library B",
        "type": "Document"
      },
      {
        "id": "04f50186-4f90-4339-8b82-8cfc36ca0af3",
        "metadata": {
          "vector_store_key": "1909.00183-7",
          "chunk_id": 133,
          "document_id": "1909.00183",
          "start_idx": 49280,
          "end_idx": 49763
        },
        "page_content": " with a 19% improvement with respect to TF-iDF text features and a 107% improvement with respect to categorical features. (For the PRC records, the corresponding improvements are 33% and 215%, respectively.) Note that similar improvements are also obtained for the other classifiers when using Doc2Vec and MS labels as features. It is also worth noting that the differences in the prediction of DoH between PRC and O-ranked records is reduced when using text tools and, specifically,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are the keywords associated with events such as protests selected?",
    "llm_answer": "Keywords are selected by searching for words that achieve the highest number of spikes matching the days of events, using Jaccard similarity to evaluate the match between word-pair spikes and event days.\n",
    "context": [
      {
        "id": "45aeba98-a809-431d-831b-0168427bc7b6",
        "metadata": {
          "vector_store_key": "1901.00570-4",
          "chunk_id": 11,
          "document_id": "1901.00570",
          "start_idx": 4463,
          "end_idx": 4890
        },
        "page_content": " associated word-pairs with protest days. This feature selection method is built upon the assumption that people discuss an event on the day of that event more than on any day before or after the event. This implies that word-pairs related to the event will form a spike on this specific day. Some of the spiking word-pairs are related to the nature of the event itself, such as \u201ctaxi protest\u201d or \u201cfair education\u201d. These word-p",
        "type": "Document"
      },
      {
        "id": "32d68c64-fae5-4490-b252-d8d07fd9d9ac",
        "metadata": {
          "vector_store_key": "1901.00570-4",
          "chunk_id": 7,
          "document_id": "1901.00570",
          "start_idx": 3002,
          "end_idx": 3504
        },
        "page_content": " the event. This will form a spike in the keyword count magnitude along the timeline as illustrated in Figure FIGREF6 . To find the words most associated with events, we search for the words that achieve the highest number of spikes matching the days of events. We use the Jaccard similarity metric as it values the spikes matching events and penalizes spikes with no event and penalizes events without spikes. Separate words can be noisy due to the misuse of the term by people, especially in big data",
        "type": "Document"
      },
      {
        "id": "9d1105b2-87c7-4e1d-a87f-3ffcdfff81a6",
        "metadata": {
          "vector_store_key": "1901.00570-4",
          "chunk_id": 74,
          "document_id": "1901.00570",
          "start_idx": 28155,
          "end_idx": 28667
        },
        "page_content": " paper, we proposed a framework to detect civil unrest events by tracking each word-pair volume in twitter. The main challenge with this model is to identify the word-pairs that are highly associated with the events with predictive power. We used temporal filtering to detect the spike within the time series vector and used Jaccard similarity to calculate the scores of each word-pair according to its similarity with the binary vector of event days. These scores are used to rank the word-pairs as features for",
        "type": "Document"
      },
      {
        "id": "318e5ef5-0a78-4b53-9be6-3c2d760e9e4e",
        "metadata": {
          "vector_store_key": "1901.00570-1",
          "chunk_id": 2,
          "document_id": "1901.00570",
          "start_idx": 894,
          "end_idx": 1381
        },
        "page_content": ", and retweets, as features. The text is used as features either by tracking the temporal patterns of keywords, clustering words into topics, or by evaluating sentiment scores and polarity. The main challenge in keyword-based models is to determine which words to use in the first place, especially as people use words in a non-standard way, particularly on Twitter. In this research, we aim for detecting large events as soon as they happen with near-live sensitivity. For example, When",
        "type": "Document"
      },
      {
        "id": "af28467c-ad72-4910-a5a8-ee89114b2f5e",
        "metadata": {
          "vector_store_key": "1901.00570-4",
          "chunk_id": 73,
          "document_id": "1901.00570",
          "start_idx": 27874,
          "end_idx": 28358
        },
        "page_content": " stemming and lemmatization. To verify the robustness of this feature selection method, we tested the selected features using multiple classifiers such as KNN, SVM, naive Bayes and decision trees. The results emphasized that the word-pairs selected using the spike-matching method achieve better AUC scores than the other correlation methods as listed in table TABREF19  In this paper, we proposed a framework to detect civil unrest events by tracking each word-pair volume in twitter",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many speeches are in the dataset?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "318f767d-561b-42bd-84b9-757129ef6caf",
        "metadata": {
          "vector_store_key": "2002.01359-4",
          "chunk_id": 34,
          "document_id": "2002.01359",
          "start_idx": 14455,
          "end_idx": 14981
        },
        "page_content": "ased into a natural conversation by crowd workers. We ensure that the annotations for the dialogue state and slots generated by the simulator are preserved and hence need no other annotation. We omit details for brevity: please refer to BIBREF19 for more details. The entire dataset consists of over 16K dialogues spanning multiple domains. Overall statistics of the dataset and comparison with other datasets can be seen in Table TABREF9. Figure FIGREF8 shows the details of the distribution of dialogue lengths across single",
        "type": "Document"
      },
      {
        "id": "f272c0a5-eec2-4e3b-ab41-defa8faa2506",
        "metadata": {
          "vector_store_key": "2003.06044-0",
          "chunk_id": 46,
          "document_id": "2003.06044",
          "start_idx": 16294,
          "end_idx": 16742
        },
        "page_content": " It covers various topics about our daily life. Each utterance in the conversation is manually labeled as one out of 4 dialogue act classes. Table TABREF18 presents the statistics for both datasets. In our preprocessing, the text was lowercased before tokenized, and then sentences were tokenized by WordPiece tokenizer BIBREF27 with a 30,000 token vocabulary to alleviate the Out-of-Vocabulary problem. [1]The author claimed that they achieved 78.",
        "type": "Document"
      },
      {
        "id": "f269af2e-e35a-41de-bd37-ffb992c794c9",
        "metadata": {
          "vector_store_key": "1808.06834-1",
          "chunk_id": 10,
          "document_id": "1808.06834",
          "start_idx": 4239,
          "end_idx": 4797
        },
        "page_content": " from a debate on NABARD bill in Lok Sabha. A speech can be labelled with multiple categories as members can appreciate and raise issues in the same speech. The following points are the contributions of this paper : Many linguists around the globe are concentrating on creation of parliamentary datasets. BIBREF1 gives an overview of the parliamentary records and corpora from countries with a focus on their availability through Clarin infrastructure. A dataset of Japanese Local Assembly minutes was created and analyzed for statistical data such as number",
        "type": "Document"
      },
      {
        "id": "fd033831-b148-43b3-a2cf-5018e2dda42c",
        "metadata": {
          "vector_store_key": "1911.01799-1",
          "chunk_id": 26,
          "document_id": "1911.01799",
          "start_idx": 10361,
          "end_idx": 10700
        },
        "page_content": " two parts: VoxCeleb1 and VoxCeleb2. We used SITW BIBREF21, a subset of VoxCeleb1 as the evaluation set. The rest of VoxCeleb1 was merged with VoxCeleb2 to form the training set (simply denoted by VoxCeleb). The training set involves $1,236,567$ utterances from $7,185$ speakers, and the evaluation set involves $6,445$ utterances from 299",
        "type": "Document"
      },
      {
        "id": "dc02a241-c576-4e18-8ef4-cb5e30d94d94",
        "metadata": {
          "vector_store_key": "1710.10609-9",
          "chunk_id": 45,
          "document_id": "1710.10609",
          "start_idx": 16984,
          "end_idx": 17477
        },
        "page_content": " information etc. A total of 1944 utterance pairs were selected. To create the vector representation we had used two distinct approaches:- Paragraph to vector approach (Doc2Vec) by Le and Mikolov LM14. Here we trained the vectors using distributed memory algorithm and trained for 40 iterations. A window size of 4 was used. We also trained the vectors using sequence to sequence approach BIBREF8 , on the Twitter dataset where we considered the task of predicting the reply of Amazon Help for",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is different in BERT-gen from standard BERT?",
    "llm_answer": "BERT-gen allows generating text from out-of-the-box data.\n",
    "context": [
      {
        "id": "e349c699-79b6-477b-b0bb-a583c1a9034d",
        "metadata": {
          "vector_store_key": "1812.06705-1",
          "chunk_id": 28,
          "document_id": "1812.06705",
          "start_idx": 10253,
          "end_idx": 10716
        },
        "page_content": " BERT shares the same model architecture with the original BERT. The differences are the input representation and training procedure. The input embeddings of BERT are the sum of the token embeddings, the segmentation embeddings and the position embeddings. For the segmentation embeddings in BERT, a learned sentence A embedding is added to every token of the first sentence, and if a second sentence exists, a sentence B embedding will be added to every token of",
        "type": "Document"
      },
      {
        "id": "51e1f4a6-b6c2-478b-9e1c-3445e96e6b48",
        "metadata": {
          "vector_store_key": "2002.10832-9",
          "chunk_id": 85,
          "document_id": "2002.10832",
          "start_idx": 29327,
          "end_idx": 29767
        },
        "page_content": "ed, the model learns a \u201ccontortion\" allowing it to align text and vision. Finally, Image + Caption $>$ Image only can be attributed to BERT fine-tuning, contributing to an increase in the observed gap, and its emergence in earlier layers. We investigated whether the abstractions encoded in a pre-trained BERT model can generalize beyond text. We proposed BERT-gen, a novel methodology that allows to directly generate text from out-of-the-",
        "type": "Document"
      },
      {
        "id": "b547a998-461d-4987-b16a-b7bc943d03a8",
        "metadata": {
          "vector_store_key": "1909.00578-2",
          "chunk_id": 32,
          "document_id": "1909.00578",
          "start_idx": 10519,
          "end_idx": 11017
        },
        "page_content": " have a larger receptive field as well. The BERT multi-task versions perform better with highly correlated qualities like $\\mathcal {Q}4$ and $\\mathcal {Q}5$ (as illustrated in Figures 2 to 4 in the supplementary material). However, there is not a clear winner among them. Mathematical equivalence does not lead to deterministic results, especially when random initialization and stochastic learning algorithms are involved. An in-depth exploration of this point would involve further investigation",
        "type": "Document"
      },
      {
        "id": "2ed25dd8-7718-495b-b13a-2d734ef2de24",
        "metadata": {
          "vector_store_key": "1902.00821-6",
          "chunk_id": 28,
          "document_id": "1902.00821",
          "start_idx": 9236,
          "end_idx": 9525
        },
        "page_content": "P]}, x_{j+1:n}, \\texttt {[SEP]})$ , where [CLS] and [SEP] are special tokens and $x_{1:n}$ is a document splited into two sides of sentences $x_{1:j}$ and $x_{j+1:n}$ . The key performance gain of BERT comes from two novel pre-training objectives: masked language model (MLM) and next text",
        "type": "Document"
      },
      {
        "id": "51164e20-5aaf-4c2b-88b6-ce5deab44dad",
        "metadata": {
          "vector_store_key": "1902.00821-6",
          "chunk_id": 27,
          "document_id": "1902.00821",
          "start_idx": 8866,
          "end_idx": 9236
        },
        "page_content": "ERT itself. As such, BERT requires pre-training on large-scale data (Wikipedia articles) to fill intensive parameters in exchange for human structured architecture designs for specific end-tasks that carry human's understanding of data of those tasks. One training example of BERT is formulated as $(\\texttt {[CLS]}, x_{1:j}, \\texttt {[SEP]}, x_{j+1:n}, \\texttt {[SEP]})",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are multimodal representations combined?",
    "llm_answer": "Methods include concatenation and element-wise multiplication,  bilinear pooling, and gated combinations.\n",
    "context": [
      {
        "id": "29ff2783-cf3f-4f9e-8313-77cadf4680ca",
        "metadata": {
          "vector_store_key": "1909.02764-4",
          "chunk_id": 2,
          "document_id": "1909.02764",
          "start_idx": 759,
          "end_idx": 1210
        },
        "page_content": "0. Note that the term \u201cmultimodal\u201d is also used beyond the combination of vision, audio, and text. For example, Soleymani2012 use it to refer to the combination of electroencephalogram, pupillary response and gaze distance. In this paper, we deal with the specific situation of car environments as a testbed for multimodal emotion recognition. This is an interesting environment since it is, to some degree, a controlled environment: Dialogue partners",
        "type": "Document"
      },
      {
        "id": "905bca0a-0d2f-435c-a45c-8486feacc601",
        "metadata": {
          "vector_store_key": "1909.08859-8",
          "chunk_id": 45,
          "document_id": "1909.08859",
          "start_idx": 15571,
          "end_idx": 15883
        },
        "page_content": " which results in another vector embedding $\\mathbf {f} \\in \\mathbb {R}^{2d_E \\times 1}$. Finally, these vector representations are concatenated and then projected to a fixed size representation using $\\mathbf {o}=\\varphi _o(\\left[\\mathbf {c}; \\mathbf {f}\\right]) \\in \\mathbb {R}^{2d \\times 1}$ where $\\varphi _o",
        "type": "Document"
      },
      {
        "id": "6f771572-80af-409a-91ee-fe082f8ec856",
        "metadata": {
          "vector_store_key": "1603.00968-2",
          "chunk_id": 19,
          "document_id": "1603.00968",
          "start_idx": 7125,
          "end_idx": 7645
        },
        "page_content": "LINEFORM11 to feed into the softmax function, where INLINEFORM12 . This representation contains feature vectors generated from all sets of embeddings under consideration. We call this method multiple group CNN (MG-CNN). Here groups refer to the features generated from different embeddings. Note that this differs from `multi-channel' models because at the convolution layer we use different filters on each word embedding matrix independently, whereas in a standard multi-channel approach each filter would consider all",
        "type": "Document"
      },
      {
        "id": "c4a78025-3991-42bb-aa72-7ef843f0504e",
        "metadata": {
          "vector_store_key": "1808.09920-8",
          "chunk_id": 46,
          "document_id": "1808.09920",
          "start_idx": 15254,
          "end_idx": 15527
        },
        "page_content": " representation is a gated combination of the previous representation and a non-linear transformation of the update message:  $$\\mathbf {h}^{(\\ell + 1)}_i = \\phi (\\mathbf {u}^{(\\ell )}_i) \\odot \\mathbf {a}^{(\\ell )}_i + \\mathbf {h}^{(\\ell )}_i \\odot (1 - \\mathbf {a}^{(\\ell",
        "type": "Document"
      },
      {
        "id": "266189a8-5a0e-41b3-b2fb-a9caf2704b5f",
        "metadata": {
          "vector_store_key": "1808.00265-1",
          "chunk_id": 14,
          "document_id": "1808.00265",
          "start_idx": 6006,
          "end_idx": 6468
        },
        "page_content": "REF2 propose an element-wise multiplication between image and question embeddings to generate spatial attention map. Fukui et al. BIBREF5 propose multimodal compact bilinear pooling (MCB) to efficiently implement an outer product operator that combines visual and textual representations. Yu et al. BIBREF6 extend this pooling scheme by introducing a multi-modal factorized bilinear pooling approach (MFB) that improves the representational capacity of the bilin",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the problem with existing metrics that they are trying to address?",
    "llm_answer": "Existing metrics penalize diversity and do not reward comprehensible hypotheses.  They also correlate weakly with human judgments, especially for single reference scenarios.  Joint goal accuracy is impractical for large numbers of services.\n",
    "context": [
      {
        "id": "b5e13913-f67b-4bcb-8787-10edc0c804f0",
        "metadata": {
          "vector_store_key": "1607.06025-0",
          "chunk_id": 8,
          "document_id": "1607.06025",
          "start_idx": 3043,
          "end_idx": 3551
        },
        "page_content": " issue of these metrics is that they penalize the diversity since they penalize the generated hypotheses that are dissimilar to the reference hypothesis. An alternative metric is to use a NLI classifier to test the generated hypothesis if the input label is correct in respect to the premise. A perfect classifier would not penalize diverse hypotheses and would reward accurate and (arguably to some degree) comprehensible hypotheses. However, it would not reward non-trivial hypotheses. Non-trivial examples",
        "type": "Document"
      },
      {
        "id": "f8ac03a1-3137-401c-bce9-531ab9054396",
        "metadata": {
          "vector_store_key": "2002.01359-7",
          "chunk_id": 50,
          "document_id": "2002.01359",
          "start_idx": 20828,
          "end_idx": 21375
        },
        "page_content": " setting, we introduce changes in the definitions of evaluation metrics from prior work. These are listed below: [leftmargin=*] Joint goal accuracy calculation: Traditionally, joint goal accuracy has been defined as the accuracy of predicting the dialogue state for all domains correctly. This is not practical in our setup, as the large number of services would result in near zero joint goal accuracy if the traditional definition is used. Furthermore, an incorrect dialogue state prediction for a service in the beginning of a dialogue degrades",
        "type": "Document"
      },
      {
        "id": "cd800d9b-a4e2-472b-a718-4d5563ba413d",
        "metadata": {
          "vector_store_key": "2002.02492-9",
          "chunk_id": 93,
          "document_id": "2002.02492",
          "start_idx": 29289,
          "end_idx": 29596
        },
        "page_content": " perplexity. When $\\epsilon $ is too large ($\\epsilon =10^{-2}$), perplexity degrades. When $\\epsilon $ is too small ($\\epsilon =10^{-4}$), the lower-bound grows slowly, so $\\left<\\text{eos}\\right>$ is not guaranteed to be top-ranked within $L$ steps, and the metrics resemble the baseline's. An $\\epsilon $",
        "type": "Document"
      },
      {
        "id": "41dafdc8-0a0b-4598-93e1-e7526e9e86af",
        "metadata": {
          "vector_store_key": "1912.01046-7",
          "chunk_id": 36,
          "document_id": "1912.01046",
          "start_idx": 14275,
          "end_idx": 14679
        },
        "page_content": " BIBREF16, which measures how far away the predicted span is from the gold standard span, as a metric. The rationale behind the metric is that, in practice, it suffices to recommend a rough span which contains the answer \u2013 a difference of a few seconds would not matter much to the user. Specifically, the predicted span is counted as correct if $|pred_{start} - gt_{start}| + |pred_{end} - gt_{end}| <=$",
        "type": "Document"
      },
      {
        "id": "3aef601e-2c02-401a-ac13-d8f27bde8e32",
        "metadata": {
          "vector_store_key": "1809.08731-2",
          "chunk_id": 68,
          "document_id": "1809.08731",
          "start_idx": 23047,
          "end_idx": 23501
        },
        "page_content": " also found fully automatic metrics to correlate only weakly or moderately with human judgments. bojar2016ten further confirmed that automatic MT evaluation methods do not perform well with a single reference. The need of better metrics for MT has been addressed since 2008 in the WMT metrics shared task BIBREF31 , BIBREF32 . For unsupervised dialogue generation, liu-EtAl:2016:EMNLP20163 obtained close to no correlation with human judgements for BLEU,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are discourse embeddings analyzed?",
    "llm_answer": "t-SNE clustering is performed on them, examining the closest neighbors of each embedding to observe if similar discourse relations tend to go together.\n",
    "context": [
      {
        "id": "3e9fb249-59db-4554-b9c2-bfaf5bbac5d0",
        "metadata": {
          "vector_store_key": "1709.02271-3",
          "chunk_id": 59,
          "document_id": "1709.02271",
          "start_idx": 21132,
          "end_idx": 21601
        },
        "page_content": " transitions/discourse relations through the INLINEFORM0 -sized filter sweeps. To further study the information encoded in the discourse embeddings, we perform t-SNE clustering BIBREF20 on them, using the best performing model CNN2-DE (global). We examine the closest neighbors of each embedding, and observe that similar discourse relations tend to go together (e.g., explanation and interpretation; consequence and result). Some examples are given in Table TABREF29 .",
        "type": "Document"
      },
      {
        "id": "143d48db-cdb6-4441-a1af-29a93035ad2e",
        "metadata": {
          "vector_store_key": "1912.01673-0",
          "chunk_id": 38,
          "document_id": "1912.01673",
          "start_idx": 14535,
          "end_idx": 15033
        },
        "page_content": " use this corpus to analyze a wide spectrum sentence embeddings methods to see to what extent the continuous space they induce reflects semantic relations between sentences in our corpus. The very first analysis using LASER embeddings indicates lack of \u201cmeaning relatability\u201d, i.e. the ability to move along a trajectory in the space in order to reach desired sentence transformations. Actually, not even paraphrases are found in close neighbourhoods of embedded sentences. More \u201csemantic\u201d sentence",
        "type": "Document"
      },
      {
        "id": "41ee45a7-4278-43ab-8f75-0c8e8398d23d",
        "metadata": {
          "vector_store_key": "1912.01673-0",
          "chunk_id": 3,
          "document_id": "1912.01673",
          "start_idx": 1159,
          "end_idx": 1704
        },
        "page_content": ", with novel representations appearing almost every other week. With an overwhelming number of methods to compute sentence vector representations, the study of their general properties becomes difficult. Furthermore, it is not so clear in which way the embeddings should be evaluated. In an attempt to bring together more traditional representations of sentence meanings and the emerging vector representations, bojar:etal:jnle:representations:2019 introduce a number of aspects or desirable properties of sentence embeddings. One of them is den",
        "type": "Document"
      },
      {
        "id": "b47eb24e-554c-49db-afc1-a49279bccaa0",
        "metadata": {
          "vector_store_key": "2001.00137-7",
          "chunk_id": 17,
          "document_id": "2001.00137",
          "start_idx": 7115,
          "end_idx": 7610
        },
        "page_content": " by an embedding layer used for input representation, with the final input embedding being a sum of token embedddings, segmentation embeddings and position embeddings. The first one, token embedding layer, uses a vocabulary dictionary to convert each token into a more representative embedding. The segmentation embedding layer indicates which tokens constitute a sentence by signaling either 1 or 0. In our case, since our data are formed of single sentences, the segment is 1 until the first `",
        "type": "Document"
      },
      {
        "id": "7d69050a-42d5-4cfb-bf09-698a0d7554b8",
        "metadata": {
          "vector_store_key": "2003.12738-4",
          "chunk_id": 53,
          "document_id": "2003.12738",
          "start_idx": 18607,
          "end_idx": 19018
        },
        "page_content": " that of a ground-truth response. In our experiments, we introduce two different ways to represent sentence embeddings. The first is $\\textbf {EMB}_\\textbf {FT}$ BIBREF28 that calculates the average of word embeddings in a sentence using FastText BIBREF29 which is trained with Common Crawl and Wikipedia data. We use FastText embeddings instead of other pre-trained word embeddings because it can handle out-of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are discourse features incorporated into the model?",
    "llm_answer": "Embedding the discourse features in a parallel CNN at the input end, or concatenating them to the output layer as a feature vector.\n",
    "context": [
      {
        "id": "ecf0d67a-d6ae-4fdd-94b2-9f208c91934f",
        "metadata": {
          "vector_store_key": "1709.02271-1",
          "chunk_id": 8,
          "document_id": "1709.02271",
          "start_idx": 2992,
          "end_idx": 3522
        },
        "page_content": " entities: grammatical relations, and RST discourse relations. We apply these models to datasets of varying sizes and genres, and find that adding any discourse information improves AA consistently on longer documents, but has mixed results on shorter documents. Further, embedding the discourse features in a parallel CNN at the input end yields better performance than concatenating them to the output layer as a feature vector (Section SECREF3 ). The global featurization is more effective than the local one. We also show that",
        "type": "Document"
      },
      {
        "id": "59c682c5-fdb3-4caf-bc09-d8f0b3e32788",
        "metadata": {
          "vector_store_key": "1709.02271-1",
          "chunk_id": 9,
          "document_id": "1709.02271",
          "start_idx": 3522,
          "end_idx": 4048
        },
        "page_content": "REF3 ). The global featurization is more effective than the local one. We also show that SVMs, which can only use discourse probability vectors, neither produce a competitive performance (even with fine-tuning), nor generalize in using the discourse information effectively. Entity-grid model. Typical lexical features for AA are relatively superficial and restricted to within the same sentence. F&H14 hypothesize that discourse features beyond the sentence level also help authorship attribution. In particular, they propose",
        "type": "Document"
      },
      {
        "id": "228b00b1-3418-407d-b707-fd4f7b9729df",
        "metadata": {
          "vector_store_key": "1709.02271-0",
          "chunk_id": 56,
          "document_id": "1709.02271",
          "start_idx": 19835,
          "end_idx": 20341
        },
        "page_content": " over CNN2-PV models. With an SVM, where the option is not available, we are only able to use relation probability vectors to obtain a very modest performance improvement. Further, we found an input-length threshold for the discourse features to help (Section SECREF26 ). Not surprisingly, discourse does not contribute on shorter texts. Many of the feature grids are empty for these shorter texts\u2013 either there are no coreference chains or they are not correctly resolved. Currently we only have empirical",
        "type": "Document"
      },
      {
        "id": "e9e26354-06d0-4ebd-9ded-97cd3981aa04",
        "metadata": {
          "vector_store_key": "1709.02271-0",
          "chunk_id": 52,
          "document_id": "1709.02271",
          "start_idx": 18334,
          "end_idx": 18777
        },
        "page_content": "ST features work better, with the best F1 score evidenced in the CNN2-DE (global) model (3.5 improvement in F1). On IMDB62, as expected with short text inputs (mean=349 words/review), the discourse features in general do not add further contribution. Even the best model CNN2-DE brings only marginal improvement, confirming our findings from varying the chunk size on novel-9, where discourse features did not help at this input size. Equipped",
        "type": "Document"
      },
      {
        "id": "d8d1caf5-42ef-486d-ad92-b281b5460b64",
        "metadata": {
          "vector_store_key": "1909.07734-3",
          "chunk_id": 29,
          "document_id": "1909.07734",
          "start_idx": 11211,
          "end_idx": 11723
        },
        "page_content": ". A few of the models incorporated the dialogue context into the model, such as the models proposed by teams IDEA and KU. Most of the submissions used deep learning models. Five of the models were based on the BERT architecture, with some using pre-trained BERT. Some of the submissions enhanced the model by adding context and speaker related encoding to improve performance. We also received submissions using more traditional networks such as CNN, as well as machine learning classics such as SVM. The results",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What discourse features are used?",
    "llm_answer": "Grammatical relations, and RST discourse relations.\n",
    "context": [
      {
        "id": "59c682c5-fdb3-4caf-bc09-d8f0b3e32788",
        "metadata": {
          "vector_store_key": "1709.02271-1",
          "chunk_id": 9,
          "document_id": "1709.02271",
          "start_idx": 3522,
          "end_idx": 4048
        },
        "page_content": "REF3 ). The global featurization is more effective than the local one. We also show that SVMs, which can only use discourse probability vectors, neither produce a competitive performance (even with fine-tuning), nor generalize in using the discourse information effectively. Entity-grid model. Typical lexical features for AA are relatively superficial and restricted to within the same sentence. F&H14 hypothesize that discourse features beyond the sentence level also help authorship attribution. In particular, they propose",
        "type": "Document"
      },
      {
        "id": "7b153ef2-141a-4049-93ce-baa94fbe38d5",
        "metadata": {
          "vector_store_key": "2001.02380-4",
          "chunk_id": 13,
          "document_id": "2001.02380",
          "start_idx": 5234,
          "end_idx": 5751
        },
        "page_content": " the next section, and point out some open questions which we would like to address. In Section SECREF3 we present the discourse annotated data that we will be using, which covers a number of English text types from the Web annotated for 20 discourse relations in the framework of Rhetorical Structure Theory, and is enriched with human annotations of discourse relation signaling devices for a subset of the data. Moreover, we also propose a taxonomy of anchored signals based on the discourse annotated data used in",
        "type": "Document"
      },
      {
        "id": "228b00b1-3418-407d-b707-fd4f7b9729df",
        "metadata": {
          "vector_store_key": "1709.02271-0",
          "chunk_id": 56,
          "document_id": "1709.02271",
          "start_idx": 19835,
          "end_idx": 20341
        },
        "page_content": " over CNN2-PV models. With an SVM, where the option is not available, we are only able to use relation probability vectors to obtain a very modest performance improvement. Further, we found an input-length threshold for the discourse features to help (Section SECREF26 ). Not surprisingly, discourse does not contribute on shorter texts. Many of the feature grids are empty for these shorter texts\u2013 either there are no coreference chains or they are not correctly resolved. Currently we only have empirical",
        "type": "Document"
      },
      {
        "id": "ecf0d67a-d6ae-4fdd-94b2-9f208c91934f",
        "metadata": {
          "vector_store_key": "1709.02271-1",
          "chunk_id": 8,
          "document_id": "1709.02271",
          "start_idx": 2992,
          "end_idx": 3522
        },
        "page_content": " entities: grammatical relations, and RST discourse relations. We apply these models to datasets of varying sizes and genres, and find that adding any discourse information improves AA consistently on longer documents, but has mixed results on shorter documents. Further, embedding the discourse features in a parallel CNN at the input end yields better performance than concatenating them to the output layer as a feature vector (Section SECREF3 ). The global featurization is more effective than the local one. We also show that",
        "type": "Document"
      },
      {
        "id": "d8d1caf5-42ef-486d-ad92-b281b5460b64",
        "metadata": {
          "vector_store_key": "1909.07734-3",
          "chunk_id": 29,
          "document_id": "1909.07734",
          "start_idx": 11211,
          "end_idx": 11723
        },
        "page_content": ". A few of the models incorporated the dialogue context into the model, such as the models proposed by teams IDEA and KU. Most of the submissions used deep learning models. Five of the models were based on the BERT architecture, with some using pre-trained BERT. Some of the submissions enhanced the model by adding context and speaker related encoding to improve performance. We also received submissions using more traditional networks such as CNN, as well as machine learning classics such as SVM. The results",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are proof paths?",
    "llm_answer": "Paths from the root to leaf nodes in an argument tree.\n",
    "context": [
      {
        "id": "dd409b65-898c-44ea-be04-fcefc45a1d23",
        "metadata": {
          "vector_store_key": "2004.03034-6",
          "chunk_id": 29,
          "document_id": "2004.03034",
          "start_idx": 11463,
          "end_idx": 11945
        },
        "page_content": " This argument path represents a particular line of reasoning for the given thesis. Similarly, for each claim, all the claims along the path from the thesis to the claim, represent the context for the claim. For example, in Figure FIGREF1, the context for O1 consists of only the thesis, whereas the context for S3 consists of both the thesis and O1 since S3 is provided to support the claim O1 which is an opposing claim for the thesis. The claims are not constructed independently",
        "type": "Document"
      },
      {
        "id": "83d0a92b-dea1-4e31-8047-8347197f35c1",
        "metadata": {
          "vector_store_key": "2004.02393-1",
          "chunk_id": 4,
          "document_id": "2004.02393",
          "start_idx": 1612,
          "end_idx": 2030
        },
        "page_content": " reasoning and the importance of which has also been demonstrated in text-based QA BIBREF8. The other example is how the annotated evidence pieces are connected, which requires at least the definition of arguments, such as a linking entity, concept, or event. Such information has proved useful by the recently popular entity-centric methods BIBREF9, BIBREF10, BIBREF11, BIBREF12, BIBREF0, BIBREF2 and intuitively will",
        "type": "Document"
      },
      {
        "id": "17033fc2-f76f-45fa-bef0-f3b1d2ea5ac0",
        "metadata": {
          "vector_store_key": "1810.00663-8",
          "chunk_id": 73,
          "document_id": "1810.00663",
          "start_idx": 28430,
          "end_idx": 28785
        },
        "page_content": " blue route (shortest path) and the red route (alternative path) are described by: [leftmargin=*, labelsep=0.2em, itemsep=0em] \u201cGo out the office and make a left. Turn right at the corner and go down the hall. Make a right at the next corner and enter the kitchen in front of table.\u201d \u201cExit the room 0 and turn right, go to the end of the corridor and turn",
        "type": "Document"
      },
      {
        "id": "aee88de0-a088-4ab4-b5bc-0e676aa2c7f1",
        "metadata": {
          "vector_store_key": "2004.02393-1",
          "chunk_id": 1,
          "document_id": "2004.02393",
          "start_idx": 445,
          "end_idx": 995
        },
        "page_content": "4, where the evidence sentences for the reasoning process were labeled by human annotators. Such evidence annotations are crucial for modern model training, since they provide finer-grained supervision for better guiding the model learning. Furthermore, they allow a pipeline fashion of model training, with each step, such as passage ranking and answer extraction, trained as a supervised learning sub-task. This is crucial from a practical perspective, in order to reduce the memory usage when handling a large amount of inputs with advanced, large",
        "type": "Document"
      },
      {
        "id": "6b9d1b3e-6ff0-49a1-98e1-a6f02f2d74b8",
        "metadata": {
          "vector_store_key": "2004.03034-6",
          "chunk_id": 19,
          "document_id": "2004.03034",
          "start_idx": 7706,
          "end_idx": 8236
        },
        "page_content": " impact labels depending on the context in which it is presented. Figure FIGREF1 shows a partial argument tree for the argument thesis \u201cPhysical torture of prisoners is an acceptable interrogation tool.\u201d. Each node in the argument tree corresponds to a claim, and these argument trees are constructed and edited collaboratively by the users of the platform. Except the thesis, every claim in the argument tree either opposes or supports its parent claim. Each path from the root to leaf nodes corresponds to an argument path which",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What external sources are used?",
    "llm_answer": "Fictional stories, general web documents, news articles, Wikipedia articles.\n",
    "context": [
      {
        "id": "d331ad1d-4d2a-4b67-8616-c6f6606b9b25",
        "metadata": {
          "vector_store_key": "1902.00821-3",
          "chunk_id": 18,
          "document_id": "1902.00821",
          "start_idx": 5993,
          "end_idx": 6277
        },
        "page_content": "REF15 ), fictional stories (MCTest BIBREF16 , CBT BIBREF17 , NarrativeQA BIBREF18 ), general Web documents (MS MARCO BIBREF19 , TriviaQA BIBREF20 , SearchQA BIBREF21 ) and news articles (NewsQA BIBREF22 , CNN/Daily Mail BIBREF23 , and RACE BIBREF24 ). Recently, CRC BIBREF6 , BIBREF25",
        "type": "Document"
      },
      {
        "id": "1b80e9e7-6a45-4bac-a002-64dd253aff20",
        "metadata": {
          "vector_store_key": "1710.01507-0",
          "chunk_id": 0,
          "document_id": "1710.01507",
          "start_idx": 0,
          "end_idx": 495
        },
        "page_content": "The Internet provides instant access to a wide variety of online content, news included. Formerly, users had static preferences, gravitating towards their trusted sources, incurring an unwavering sense of loyalty. The same cannot be said for current trends since users are likely to go with any source readily available to them. In order to stay in business, news agencies have switched, in part, to a digital front. Usually, they generate revenue by (1) advertisements on their websites, or (2)",
        "type": "Document"
      },
      {
        "id": "8b27f107-ec01-4522-b72b-f83888513a3a",
        "metadata": {
          "vector_store_key": "1704.08960-9",
          "chunk_id": 12,
          "document_id": "1704.08960",
          "start_idx": 4193,
          "end_idx": 4760
        },
        "page_content": " belongs to recent neural word segmentation. To our knowledge, there has been no work in the literature systematically investigating rich external resources for neural word segmentation training. Closest in spirit to our work, BIBREF11 empirically studied the use of various external resources for enhancing a statistical segmentor, including character mutual information, access variety information, punctuation and other statistical information. Their baseline is similar to ours in the sense that both character and word contexts are considered. On the other hand,",
        "type": "Document"
      },
      {
        "id": "e7b0d9fd-8534-4312-9d21-60512ad3cfae",
        "metadata": {
          "vector_store_key": "1910.00458-8",
          "chunk_id": 75,
          "document_id": "1910.00458",
          "start_idx": 26170,
          "end_idx": 26674
        },
        "page_content": "26 are human-generated and based on source documents or summaries in free text format. However, since annotators tend to copy spans as answers BIBREF27, the majority of answers are still extractive in these datasets. The multi-choice QA datasets are collected either via crowd sourcing, or collected from examinations designed by educational experts BIBREF7. In this type of QA datasets, besides token matching, a significant portion of questions require multi-sentence reasoning and external knowledge B",
        "type": "Document"
      },
      {
        "id": "5a263e4e-e650-4bb0-a353-0d907c3c13b2",
        "metadata": {
          "vector_store_key": "1803.09230-4",
          "chunk_id": 3,
          "document_id": "1803.09230",
          "start_idx": 1408,
          "end_idx": 1960
        },
        "page_content": " more then 500 Wikipedia articles and the answers were sourced with Amazon Mechanical Turk. Recently, researchers were able to make machines outperform humans (as of Jan 2018) BIBREF1 . Answers in this dataset are taken from the document itself and are not dynamically generated from scratch. Instead of generating text that provides a suitable answer, the objective is to find the boundaries in which the answer is contained in the document. The aim is to achieve close to human performance in generating correct answers from a context paragraph given",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much better peformance is achieved in human evaluation when model is trained considering proposed metric?",
    "llm_answer": "The provided text states the proposed metric correlates better with human judgment than other metrics, but does not quantify the improvement.\n",
    "context": [
      {
        "id": "182ba0c4-c36f-4213-95de-b4014f3146b7",
        "metadata": {
          "vector_store_key": "2002.05058-9",
          "chunk_id": 63,
          "document_id": "2002.05058",
          "start_idx": 24978,
          "end_idx": 25505
        },
        "page_content": " To ensure the computational budget to be roughly the same, we fix the number of plays in skill rating to 10,000. The experimental results are summarized in Table 1. We can see that the proposed comparative evaluator correlates far better with human judgment than BLEU and perplexity. When compared with recently proposed parameterized metrics including adversarial evaluator and ADEM, our model consistently outperforms them by a large margin, which demonstrates that our comparison-based evaluation metric is able to evaluate",
        "type": "Document"
      },
      {
        "id": "16b805d8-a054-424d-93dd-d3a27b90a8bf",
        "metadata": {
          "vector_store_key": "2002.05058-9",
          "chunk_id": 67,
          "document_id": "2002.05058",
          "start_idx": 26625,
          "end_idx": 27228
        },
        "page_content": " the proposed comparative evaluator with skill rating significantly outperforms all compared baselines, including comparative evaluator with averaged sample-level scores. This demonstrates the effectiveness of the skill rating system for performing model-level comparison with pairwise sample-level evaluation. In addition, the poor correlation between conventional evaluation metrics including BLEU and perplexity demonstrates the necessity of better automated evaluation metrics in open domain NLG evaluation. We further investigate the impact of imperfect metrics on training NLG models. As described",
        "type": "Document"
      },
      {
        "id": "b8aea242-4d2f-4c37-99e5-f667bc9f7c64",
        "metadata": {
          "vector_store_key": "2002.05058-9",
          "chunk_id": 79,
          "document_id": "2002.05058",
          "start_idx": 31816,
          "end_idx": 32364
        },
        "page_content": "level evaluation based on sample-level pairwise comparison. By transferring pretrained natural language understanding knowledge from BERT and fine-tuning with strong and weak supervision examples and human preference annotations, our model correlates better with human judgment than other compared metrics. In addition, we find that when used as evaluation metrics, conventional metrics such as BLEU and perplexity may affect the training stage of NLG models as they may lead to sub-optimal hyperparameter choice and checkpoint selection. Our model",
        "type": "Document"
      },
      {
        "id": "4786aca0-d17c-410b-a05b-652f20df7fc0",
        "metadata": {
          "vector_store_key": "2002.05058-9",
          "chunk_id": 10,
          "document_id": "2002.05058",
          "start_idx": 4096,
          "end_idx": 4674
        },
        "page_content": " evaluation information provided by our pairwise comparison model. The skill rating system is more efficient and accurate than several baseline approaches. We conduct experiments on both story generation task and open domain dialogue response generation task. Experimental results show that our approach correlates better with human evaluation on both datasets. Moreover, we show that using automated metrics such as BLEU to perform hyperparameter tuning and early-stopping results in sub-optimal model and our approach helps alleviate this problem. Evaluation of NLG models has",
        "type": "Document"
      },
      {
        "id": "34da022a-5bec-4beb-bd76-14370cf952e1",
        "metadata": {
          "vector_store_key": "2002.05058-1",
          "chunk_id": 69,
          "document_id": "2002.05058",
          "start_idx": 27517,
          "end_idx": 28041
        },
        "page_content": " evaluation is then employed to identify the best choice. We evaluate the performance of each metric by how many times (out of 10) they succeeded in selecting the best hyperparameter combination or early-stopping checkpoint (out of 4) and the average human-annotated score for their selected models. The results are shown in Table 3. We can see that conventional automated metrics perform poorly and result in sub-optimal result when performing hyperparameter search and selecting the best performing checkpoints. Converting",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much transcribed data is available for for Ainu language?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "72f864a7-0a9d-414e-a160-21434441a51b",
        "metadata": {
          "vector_store_key": "2002.06675-6",
          "chunk_id": 2,
          "document_id": "2002.06675",
          "start_idx": 844,
          "end_idx": 1319
        },
        "page_content": " recordings have been made. Nevertheless, a majority of the recordings have not been transcribed and utilized effectively. Since transcribing them requires expertise in the Ainu language, not so many people are able to work on this task. Hence, there is a strong demand for an ASR system for the Ainu language. We started a project of Ainu ASR and this article is the first report of this project. We have built an Ainu speech corpus based on data provided by the Ainu Museum",
        "type": "Document"
      },
      {
        "id": "b60c29b6-91ba-4c76-a2b5-4c455b9ea758",
        "metadata": {
          "vector_store_key": "2002.06675-6",
          "chunk_id": 14,
          "document_id": "2002.06675",
          "start_idx": 4927,
          "end_idx": 5394
        },
        "page_content": "of-speech (POS) tagging and word segmentation. Ainu speech recognition was tried by ainutrans with 2.5 hours of Ainu folklore data even though the Ainu language was not their main target. Their phone error rare was about 40% which is not an accuracy level for practical use yet. It appears that there has not been a substantial Ainu speech recognition study yet that utilizes corpora of a reasonable size. Therefore, our first step was to build a speech corpus for AS",
        "type": "Document"
      },
      {
        "id": "d51d09d5-e982-416a-9cc9-6bdfae9f5f04",
        "metadata": {
          "vector_store_key": "2002.06675-0",
          "chunk_id": 7,
          "document_id": "2002.06675",
          "start_idx": 2788,
          "end_idx": 3285
        },
        "page_content": " critically endangered in 2009 BIBREF8. In response to this situation, Ainu folklore and songs have been actively recorded since the late 20th century in efforts initiated by the Government of Japan. For example, the Ainu Museum started audio recording of Ainu folklore in 1976 with the cooperation of a few Ainu elders which resulted in the collection of speech data with the total duration of roughly 700 hours. This kind of data should be a key to the understanding of Ainu culture, but most of",
        "type": "Document"
      },
      {
        "id": "d52616e6-20ad-4c1a-b249-a83e975442bb",
        "metadata": {
          "vector_store_key": "2002.06675-0",
          "chunk_id": 8,
          "document_id": "2002.06675",
          "start_idx": 3106,
          "end_idx": 3579
        },
        "page_content": ". This kind of data should be a key to the understanding of Ainu culture, but most of it is not transcribed and fully studied yet. The Ainu language is an agglutinative language and has some similarities to Japanese. However, its genealogical relationship with other languages has not been clearly understood yet. Among its features such as closed syllables and personal verbal affixes, one important feature is that there are many compound words. For example, a word atuyk",
        "type": "Document"
      },
      {
        "id": "a21ed613-c541-4da7-90cd-37d003db1cf5",
        "metadata": {
          "vector_store_key": "2002.06675-0",
          "chunk_id": 5,
          "document_id": "2002.06675",
          "start_idx": 2018,
          "end_idx": 2467
        },
        "page_content": " BIBREF5, BIBREF6. In this work, we investigate the modeling unit and utilization of corpora of other languages. This section briefly overviews the background of the data collection, the Ainu language, and its writing system. After that, we describe how Ainu recordings are classified and review previous works dealing with the Ainu language. The Ainu people had total population of about 20,000 in the mid-19th century BIBREF7 and they used to live",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What baseline approaches do they compare against?",
    "llm_answer": "K-means, SkipVec, RecNN, Para2vec,  ILP, ILP+MC, content, content-network, and image-based models.\n",
    "context": [
      {
        "id": "97d0589b-102d-402c-bcb2-27484aa3c4fb",
        "metadata": {
          "vector_store_key": "1701.00185-8",
          "chunk_id": 76,
          "document_id": "1701.00185",
          "start_idx": 27376,
          "end_idx": 27736
        },
        "page_content": " and NMI performance of our proposed approaches and four baseline methods, K-means, SkipVec, RecNN and Para2vec based clustering methods. Intuitively, we get a general observation that (1) BoW based approaches, including K-means (TF) and K-means (TF-IDF), and SkipVec based approaches perform not well; (2) RecNN based approaches, both RecNN (Ave.) and RecNN (",
        "type": "Document"
      },
      {
        "id": "8836cac5-0770-4b24-b87a-3d1fa58536ca",
        "metadata": {
          "vector_store_key": "1703.09684-5",
          "chunk_id": 65,
          "document_id": "1703.09684",
          "start_idx": 24411,
          "end_idx": 24828
        },
        "page_content": " simple baseline methods routinely exceed more complex methods using simple accuracy BIBREF7 , BIBREF8 , BIBREF19 . On TDIUC, MLP surpasses MCB and NMN in terms of simple accuracy, but a closer inspection reveals that MLP's score is highly determined by performance on categories with a large number of examples, such as `absurd' and `object presence.' Using MPT, we find that both NMN and MCB outperform MLP. Inspect",
        "type": "Document"
      },
      {
        "id": "899129d5-0dac-4c1e-adc0-4aae82344e56",
        "metadata": {
          "vector_store_key": "1902.00821-9",
          "chunk_id": 83,
          "document_id": "1902.00821",
          "start_idx": 25453,
          "end_idx": 25621
        },
        "page_content": " that the pre-tuning approach is highly effective and outperforms existing baselines or highly competitive with supervised baselines trained from a large-scale dataset.",
        "type": "Document"
      },
      {
        "id": "00721464-3286-48f8-8803-01e1b4226dbe",
        "metadata": {
          "vector_store_key": "1807.09671-3",
          "chunk_id": 103,
          "document_id": "1807.09671",
          "start_idx": 37163,
          "end_idx": 37668
        },
        "page_content": " the direct comparison with the ILP and ILP+MC and leave the comparison to other baselines as future work. The proposed method achieved better summarization performance on Eng, CS2016, movie, and peer than the ILP baseline. Unfortunately, it does not work as expected on two courses for student responses (Stat2015 and Stat2016), review camera and news documents. This leaves the research question when and why the proposed method works better. In order to investigate what are key factors that impact the",
        "type": "Document"
      },
      {
        "id": "25bb2afe-7835-48e9-a6f8-dbdbbde2116d",
        "metadata": {
          "vector_store_key": "1902.06843-8",
          "chunk_id": 91,
          "document_id": "1902.06843",
          "start_idx": 35139,
          "end_idx": 35416
        },
        "page_content": "s by -0.27. Baselines: To test the efficacy of our multi-modal framework for detecting depressed users, we compare it against existing content, content-network, and image-based models (based on the aforementioned general image feature, facial presence, and facial expressions.)",
        "type": "Document"
      }
    ]
  },
  {
    "query": "how many domains did they experiment with?",
    "llm_answer": "20\n",
    "context": [
      {
        "id": "93868b0b-4b9f-4465-9f7e-c688fa1a5347",
        "metadata": {
          "vector_store_key": "1701.03214-0",
          "chunk_id": 11,
          "document_id": "1701.03214",
          "start_idx": 4004,
          "end_idx": 4439
        },
        "page_content": "-of-domain data is used for the in-domain data; while for the \u201cmulti domain\u201d and \u201cmixed fine tuning\u201d methods, we use a vocabulary obtained from the mixed in-domain and out-of-domain data for all the training stages. We conducted NMT domain adaptation experiments in two different settings as follows: Chinese-to-English translation was the focus of the high quality in-domain corpus setting. We utilized the resource rich patent out-of",
        "type": "Document"
      },
      {
        "id": "6944dbf0-7c48-4c06-a651-f3229742e955",
        "metadata": {
          "vector_store_key": "1907.11499-0",
          "chunk_id": 3,
          "document_id": "1907.11499",
          "start_idx": 1311,
          "end_idx": 1706
        },
        "page_content": " debates BIBREF0 , BIBREF3 , BIBREF4 , BIBREF5 . The question of how to best deal with multiple domains when training data is available for one or few of them has met with much interest in the literature. The field of domain adaptation BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 aims at improving the learning of a predictive function in a target domain where there is little or no labeled",
        "type": "Document"
      },
      {
        "id": "902249a7-4300-47ad-b86b-0f1934517c6b",
        "metadata": {
          "vector_store_key": "2002.06053-4",
          "chunk_id": 54,
          "document_id": "2002.06053",
          "start_idx": 20155,
          "end_idx": 20652
        },
        "page_content": " Later, domains were described as the words, and domain architectures as sentences of the language BIBREF58, BIBREF59. Protein domains were treated as the words of multi-domain proteins in order to evaluate the semantic meaning behind the domains BIBREF60. The study supported prior work by BIBREF59 suggesting that domains displayed syntactic and semantic features, but there are only a few multi-domain proteins with more than six domains limiting the use of domains as words to build sentences.",
        "type": "Document"
      },
      {
        "id": "f33f8688-14d7-44f3-95de-b816fffb302a",
        "metadata": {
          "vector_store_key": "1910.00825-8",
          "chunk_id": 32,
          "document_id": "1910.00825",
          "start_idx": 11594,
          "end_idx": 12020
        },
        "page_content": "406 of them are single-domain (8.93 turns on average) and 7,302 are multi-domain (15.39 turns on average). During MultiWOZ data collection, instruction is provided for crowd workers to perform the task. We use the instructions as the dialog summary, and an example data is shown in Table TABREF25. Dialog domain label is extracted from existing MultiWOZ annotation. In the experiment, we split the dataset into 8,438 training,",
        "type": "Document"
      },
      {
        "id": "896d120c-d921-4200-be03-d5842da38d1e",
        "metadata": {
          "vector_store_key": "2002.01359-3",
          "chunk_id": 29,
          "document_id": "2002.01359",
          "start_idx": 12314,
          "end_idx": 12843
        },
        "page_content": " common approach utilized in other datasets BIBREF0. It has been shown that such datasets suffer from substantial annotation errors BIBREF18. Thirdly, using a simulator greatly simplifies the data collection task and instructions as only paraphrasing is needed to achieve a natural dialogue. This is particularly important for creating a large dataset spanning multiple domains. The 20 domains present across the train, dev and test datasets are listed in Table TABREF10, as are the details regarding which domains are present in",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How long is the dataset?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "2e1622b2-1517-4239-8581-693f3aece329",
        "metadata": {
          "vector_store_key": "1711.02013-3",
          "chunk_id": 55,
          "document_id": "1711.02013",
          "start_idx": 20658,
          "end_idx": 21116
        },
        "page_content": "ings, between recurrent layers, and on recurrent states were (0.7, 0.5, 0.5) respectively. dataset contains 17M training tokens and has a vocabulary size of 44k words. The dataset is partitioned into a training set (first 99M characters) and a development set (last 1M characters) that is used to report performance. As this dataset contains various articles from Wikipedia, the longer term information (such as current topic) plays a bigger role than in the",
        "type": "Document"
      },
      {
        "id": "0723454a-4062-463e-972a-81a58c6a504e",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "9270562a-8e53-4e1d-b760-74d8fd0bc872",
        "metadata": {
          "vector_store_key": "1905.06566-7",
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "a2c17c40-d003-46d0-ba08-8966b42e8e23",
        "metadata": {
          "vector_store_key": "1906.03538-7",
          "chunk_id": 31,
          "document_id": "1906.03538",
          "start_idx": 12505,
          "end_idx": 12830
        },
        "page_content": ". We now provide a brief summary of [wave]390P[wave]415e[wave]440r[wave]465s[wave]485p[wave]525e[wave]535c[wave]595t[wave]610r[wave]635u[wave]660m. The dataset contains about INLINEFORM0 claims with a significant length diversity (Table TABREF19 ). Additionally, the dataset comes with INLINEFORM1 perspectives, most of which",
        "type": "Document"
      },
      {
        "id": "0d593f74-7474-41cc-9928-ce68ac96f714",
        "metadata": {
          "vector_store_key": "1802.05574-2",
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is a string kernel?",
    "llm_answer": "String kernels measure the similarity of strings through character n-grams.\n",
    "context": [
      {
        "id": "f9d4d441-6b20-4ca4-b809-4e641b332c40",
        "metadata": {
          "vector_store_key": "1811.01734-0",
          "chunk_id": 22,
          "document_id": "1811.01734",
          "start_idx": 8198,
          "end_idx": 8655
        },
        "page_content": " on string kernels have demonstrated remarkable performance in various text classification tasks BIBREF35 , BIBREF36 , BIBREF22 , BIBREF19 , BIBREF10 , BIBREF17 , BIBREF26 . String kernels represent a way of using information at the character level by measuring the similarity of strings through character n-grams. Lodhi et al. BIBREF35 used string kernels for document categorization, obtaining very good results. String kernels were also successfully used",
        "type": "Document"
      },
      {
        "id": "72ffba4d-4588-4de8-8072-991e5c034881",
        "metadata": {
          "vector_store_key": "1811.01734-0",
          "chunk_id": 24,
          "document_id": "1811.01734",
          "start_idx": 8977,
          "end_idx": 9385
        },
        "page_content": ", obtaining impressive results. For instance, Ionescu et al. BIBREF19 have employed string kernels in a cross-corpus (and implicitly cross-domain) native language identification experiment, improving the state-of-the-art accuracy by a remarkable INLINEFORM0 . Gim\u00e9nez-P\u00e9rez et al. BIBREF10 have used string kernels for single-source and multi-source polarity classification. Remarkably, they obtain state-of-",
        "type": "Document"
      },
      {
        "id": "c382291f-f819-4824-85cf-214fbf601216",
        "metadata": {
          "vector_store_key": "1811.01734-0",
          "chunk_id": 23,
          "document_id": "1811.01734",
          "start_idx": 8655,
          "end_idx": 9183
        },
        "page_content": "35 used string kernels for document categorization, obtaining very good results. String kernels were also successfully used in authorship identification BIBREF22 . More recently, various combinations of string kernels reached state-of-the-art accuracy rates in native language identification BIBREF19 and Arabic dialect identification BIBREF17 . Interestingly, string kernels have been used in cross-domain settings without any domain adaptation, obtaining impressive results. For instance, Ionescu et al. BIBREF19 have employed",
        "type": "Document"
      },
      {
        "id": "f6cb2900-87c7-47f5-9672-c563fcc52f8b",
        "metadata": {
          "vector_store_key": "1811.01734-8",
          "chunk_id": 28,
          "document_id": "1811.01734",
          "start_idx": 10505,
          "end_idx": 10956
        },
        "page_content": " between text samples, simply based on character n-grams. Various string kernel functions have been proposed to date BIBREF35 , BIBREF38 , BIBREF19 . Perhaps one of the most recently introduced string kernels is the histogram intersection string kernel BIBREF19 . For two strings over an alphabet INLINEFORM0 , INLINEFORM1 , the intersection string kernel is formally defined as follows: DISPLAYFORM0  where INLINEFORM0 is the number of occurrences of",
        "type": "Document"
      },
      {
        "id": "5915449c-03d2-4460-bae7-af41509272fe",
        "metadata": {
          "vector_store_key": "1811.01734-0",
          "chunk_id": 27,
          "document_id": "1811.01734",
          "start_idx": 10173,
          "end_idx": 10701
        },
        "page_content": " , BIBREF17 , we use unlabeled data from the target domain to significantly increase the performance of string kernels in cross-domain text classification, particularly in English polarity classification.   String kernels. Kernel functions BIBREF38 capture the intuitive notion of similarity between objects in a specific domain. For example, in text mining, string kernels can be used to measure the pairwise similarity between text samples, simply based on character n-grams. Various string kernel functions have been proposed",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they correlate NED with emotional bond levels?",
    "llm_answer": "They compute the correlation of the proposed NED measure (both patient-to-therapist and therapist-to-patient) with patient-perceived emotional bond ratings.  A t-SNE transformation of the difference of patient-to-therapist turn embedding pairs is also used to visualize the relationship between session emotional bond levels and the measure.\n",
    "context": [
      {
        "id": "02358a83-03b6-4ee4-b3aa-09c572ad68ff",
        "metadata": {
          "vector_store_key": "1804.08782-3",
          "chunk_id": 42,
          "document_id": "1804.08782",
          "start_idx": 15808,
          "end_idx": 16341
        },
        "page_content": " bond and the candidate measure. Results in Table TABREF26 show that the patient-to-therapist NED is negatively correlated with emotional bond with high statistical significance ( INLINEFORM0 ). This negative sign is consistent with previous studies as higher distance in acoustic features indicates lower entrainment. However, the therapist-to-patient NED does not have a significant correlation with emotional bond. A possible explanation for this finding is that the emotional bond is reported by the patient and influenced by the",
        "type": "Document"
      },
      {
        "id": "fbd9b487-7459-407a-b9c6-597b4c990554",
        "metadata": {
          "vector_store_key": "1804.08782-3",
          "chunk_id": 40,
          "document_id": "1804.08782",
          "start_idx": 15019,
          "end_idx": 15513
        },
        "page_content": " domain theory BIBREF16 and from experimental validation BIBREF6 , a high emotional bond in patient-therapist interactions in the suicide therapy domain is associated with more entrainment. In this experiment, we compute the correlation of the proposed NED measure with the patient-perceived emotional bond ratings. Since the proposed measure is asymmetric in nature, we compute the measures for both patient-to-therapist and therapist-to-patient entrainment. We also compute the correlation of",
        "type": "Document"
      },
      {
        "id": "f848f636-6203-475d-94a7-600c59c2fe24",
        "metadata": {
          "vector_store_key": "1804.08782-3",
          "chunk_id": 43,
          "document_id": "1804.08782",
          "start_idx": 16341,
          "end_idx": 16857
        },
        "page_content": " A possible explanation for this finding is that the emotional bond is reported by the patient and influenced by the degree of their perceived therapist-entrainment. Thus, equipped with an asymmetric measure, we are also able to identify the latent directionality of the emotional bond metric. The complexity measure (Baseline 2) also shows statistically significant correlation, but the value of INLINEFORM1 is lower than that of the proposed measure. To analyze the embeddings encoded by our model, we also compute",
        "type": "Document"
      },
      {
        "id": "313c4761-91c6-4865-80c6-a3f5fb1099f3",
        "metadata": {
          "vector_store_key": "1804.08782-3",
          "chunk_id": 44,
          "document_id": "1804.08782",
          "start_idx": 16647,
          "end_idx": 17078
        },
        "page_content": " that of the proposed measure. To analyze the embeddings encoded by our model, we also compute a t-SNE BIBREF20 transformation of the difference of all patient-to-therapist turn embedding pairs, denoted as INLINEFORM0 in Equation (3). Figure FIGREF27 shows the results of a session with high emotional bond and another one with low emotional bond (with values of 7 and 1 respectively) as a 2-dimensional scatter plot. Visibly there",
        "type": "Document"
      },
      {
        "id": "409eca04-60d4-4b75-b45f-c5e0f5eb4ad1",
        "metadata": {
          "vector_store_key": "1804.08782-0",
          "chunk_id": 45,
          "document_id": "1804.08782",
          "start_idx": 17078,
          "end_idx": 17571
        },
        "page_content": " (with values of 7 and 1 respectively) as a 2-dimensional scatter plot. Visibly there is some separation between the sessions with low and high emotional bond. In this work, a novel deep neural network-based Neural Entrainment Distance (NED) measure is proposed for capturing entrainment in conversational speech. The neural network architecture consisting of an encoder and a decoder is trained on the Fisher corpus in an unsupervised training framework and then the measure is defined on the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was their F1 score on the Bengali NER corpus?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "31556ba3-ef5a-4a26-8d28-53b603c7a4c7",
        "metadata": {
          "vector_store_key": "1701.09123-3",
          "chunk_id": 107,
          "document_id": "1701.09123",
          "start_idx": 40331,
          "end_idx": 40729
        },
        "page_content": " in Table TABREF71 show that our nl-cluster model outperforms the best result published on CoNLL 2002 BIBREF45 by 3.83 points in F1 score. Adding the English Illinois NER gazetteers BIBREF31 and trigram and character n-gram features increases the score to 85.04 F1, 5.41 points better than previous published work on this dataset. We also compared our system with the more recently developed SONAR-",
        "type": "Document"
      },
      {
        "id": "cf6d8aac-20ae-4f47-9500-27bcba2f456a",
        "metadata": {
          "vector_store_key": "1911.03894-2",
          "chunk_id": 38,
          "document_id": "1911.03894",
          "start_idx": 13533,
          "end_idx": 14005
        },
        "page_content": ", and F1 score. Here precision measures the percentage of entities found by the system that are correctly tagged, recall measures the percentage of named entities present in the corpus that are found and the F1 score combines both precision and recall measures giving a general idea of a model's performance. Most of the advances in NER haven been achieved on English, particularly focusing on the CoNLL 2003 BIBREF40 and the Ontonotes v5 BIBREF41, BIBREF42 English corpor",
        "type": "Document"
      },
      {
        "id": "88a6f2c9-fa7c-4999-bc75-792ef40761a8",
        "metadata": {
          "vector_store_key": "1701.02877-1",
          "chunk_id": 10,
          "document_id": "1701.02877",
          "start_idx": 3620,
          "end_idx": 4066
        },
        "page_content": " primarily by F1 score), and also to study the relationship between OOV NEs and features and F1. See Section \"Experiments\" for details. To ensure representativeness and comprehensiveness, our experimental findings are based on key benchmark NER corpora spanning multiple genres, time periods, and corpus annotation methodologies and guidelines. As detailed in Section \"Datasets\" , the corpora studied are OntoNotes BIBREF29 , ACE BIBREF30 , MUC 7",
        "type": "Document"
      },
      {
        "id": "e89483bc-1561-49de-904a-34c5a4f536a9",
        "metadata": {
          "vector_store_key": "1701.09123-3",
          "chunk_id": 108,
          "document_id": "1701.09123",
          "start_idx": 40552,
          "end_idx": 40973
        },
        "page_content": " published work on this dataset. We also compared our system with the more recently developed SONAR-1 corpus and the companion NERD system distributed inside its release BIBREF33 . They report 84.91 F1 for the six main named entity types via 10-fold cross validation. For this comparison we chose the local, nl-cluster and nl-cluster-dict configurations from Table TABREF71 and run them on SONAR-1 using the same settings",
        "type": "Document"
      },
      {
        "id": "2c1de9f3-1cd6-4cd9-8d12-f7fe403598f4",
        "metadata": {
          "vector_store_key": "1701.02877-7",
          "chunk_id": 21,
          "document_id": "1701.02877",
          "start_idx": 7791,
          "end_idx": 8297
        },
        "page_content": " studying NER performance on this genre gained momentum. So far, there have been no big evaluation efforts, such as ACE and OntoNotes, resulting in substantial amounts of gold standard data. Instead, benchmark corpora were created as part of smaller challenges or individual projects. The first such corpus is the UMBC corpus for Twitter NER BIBREF33 , where researchers used crowdsourcing to obtain annotations for persons, locations and organisations. A further Twitter NER corpus was created by BIBREF21",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the size of the dataset?",
    "llm_answer": "287,226 training documents, 13,368 validation documents, 11,490 test documents.  17M training tokens.\n",
    "context": [
      {
        "id": "9270562a-8e53-4e1d-b760-74d8fd0bc872",
        "metadata": {
          "vector_store_key": "1905.06566-7",
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "0723454a-4062-463e-972a-81a58c6a504e",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "0d593f74-7474-41cc-9928-ce68ac96f714",
        "metadata": {
          "vector_store_key": "1802.05574-2",
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "84720de1-c988-45c6-bcbe-7a0435cdf6fb",
        "metadata": {
          "vector_store_key": "1706.08032-0",
          "chunk_id": 38,
          "document_id": "1706.08032",
          "start_idx": 14264,
          "end_idx": 14661
        },
        "page_content": " datasets in Table III. for all datasets, the filter window size ( INLINEFORM0 ) is 7 with 6 feature maps each for the first wide convolution layer, the second wide convolution layer has a filter window size of 5 with 14 feature maps each. Dropout rate ( INLINEFORM1 ) is 0.5, INLINEFORM2 constraint, learning rate is 0.1 and momentum of 0.9. Mini-batch size for STS Corpus is 100 and others are 4",
        "type": "Document"
      },
      {
        "id": "2e1622b2-1517-4239-8581-693f3aece329",
        "metadata": {
          "vector_store_key": "1711.02013-3",
          "chunk_id": 55,
          "document_id": "1711.02013",
          "start_idx": 20658,
          "end_idx": 21116
        },
        "page_content": "ings, between recurrent layers, and on recurrent states were (0.7, 0.5, 0.5) respectively. dataset contains 17M training tokens and has a vocabulary size of 44k words. The dataset is partitioned into a training set (first 99M characters) and a development set (last 1M characters) that is used to report performance. As this dataset contains various articles from Wikipedia, the longer term information (such as current topic) plays a bigger role than in the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many examples do they have in the target domain?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "6defd9fe-bf69-4e6d-9e46-5d9b996baeff",
        "metadata": {
          "vector_store_key": "2002.11893-8",
          "chunk_id": 53,
          "document_id": "2002.11893",
          "start_idx": 19982,
          "end_idx": 20379
        },
        "page_content": "sim $5) in HAR domains with cross-domain informable slots. 572 dialogues have multiple sub-goals in HAR domains with cross-domain informable slots and at least one sub-goal in the metro or taxi domain (3$\\sim $5 sub-goals). The data statistics are shown in Table TABREF26. As mentioned in Section SECREF14, we generate independent multi-domain, cross multi-domain, and traffic domain sub-goals one",
        "type": "Document"
      },
      {
        "id": "950b6c62-3eea-4e8b-b084-c3d6f92a7e53",
        "metadata": {
          "vector_store_key": "2002.11893-8",
          "chunk_id": 51,
          "document_id": "2002.11893",
          "start_idx": 19391,
          "end_idx": 19796
        },
        "page_content": " (13.7). These statistics indicate that our dialogue data are more complex. According to the type of user goal, we group the dialogues in the training set into five categories: 417 dialogues have only one sub-goal in HAR domains. 1573 dialogues have multiple sub-goals (2$\\sim $3) in HAR domains. However, these sub-goals do not have cross-domain informable slots. 691 dialogues have multiple sub-goals in",
        "type": "Document"
      },
      {
        "id": "f33f8688-14d7-44f3-95de-b816fffb302a",
        "metadata": {
          "vector_store_key": "1910.00825-8",
          "chunk_id": 32,
          "document_id": "1910.00825",
          "start_idx": 11594,
          "end_idx": 12020
        },
        "page_content": "406 of them are single-domain (8.93 turns on average) and 7,302 are multi-domain (15.39 turns on average). During MultiWOZ data collection, instruction is provided for crowd workers to perform the task. We use the instructions as the dialog summary, and an example data is shown in Table TABREF25. Dialog domain label is extracted from existing MultiWOZ annotation. In the experiment, we split the dataset into 8,438 training,",
        "type": "Document"
      },
      {
        "id": "a0f5fba1-8965-4b9c-a9fe-ac0066b96ddc",
        "metadata": {
          "vector_store_key": "1709.05413-0",
          "chunk_id": 85,
          "document_id": "1709.05413",
          "start_idx": 33585,
          "end_idx": 34151
        },
        "page_content": " insurance domain. Our findings show that our model is robust to different domains as our test set size increases, and that our more generic, company-independent experiment gives us better performance than any domain-specific experiments. Given our observation that Agent turns are more predictable, and that we achieve best performance in a company-independent setting, we question whether the training that agents receive is actually reliable in terms of resulting in overall \"satisfied customers\", regardless of company domain. Ultimately, our goal is to discover",
        "type": "Document"
      },
      {
        "id": "e5c5da03-e4d4-44c0-af0a-76e6262d164b",
        "metadata": {
          "vector_store_key": "2002.11893-8",
          "chunk_id": 52,
          "document_id": "2002.11893",
          "start_idx": 19618,
          "end_idx": 19982
        },
        "page_content": " not have cross-domain informable slots. 691 dialogues have multiple sub-goals in HAR domains and at least one sub-goal in the metro or taxi domain (3$\\sim $5 sub-goals). The sub-goals in HAR domains do not have cross-domain informable slots. 1,759 dialogues have multiple sub-goals (2$\\sim $5) in HAR domains with cross-domain informable slots. 572 dialogues have",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the baseline model?",
    "llm_answer": "Baseline 1: smooth L1 distance; Baseline 2: PCA-based acoustic similarity; Baseline 3: Nonlinear dynamical systems-based complexity measure.  Model 1 is also mentioned as a baseline.\n",
    "context": [
      {
        "id": "97be5370-e788-41d2-9b17-3e7739266f06",
        "metadata": {
          "vector_store_key": "2001.11381-3",
          "chunk_id": 138,
          "document_id": "2001.11381",
          "start_idx": 31860,
          "end_idx": 32196
        },
        "page_content": "ados. Dicho lo anterior, el Modelo 1 podr\u00eda ser considerado como nuestro propio baseline. A continuaci\u00f3n presentamos un protocolo de evaluaci\u00f3n manual de los resultados obtenidos. El experimento consisti\u00f3 en la generaci\u00f3n de 15 frases por cada uno de los tres modelos propuestos. Para cada modelo, se consideraron tres queries: $Q=$ {AM",
        "type": "Document"
      },
      {
        "id": "593127ef-0325-4226-be3e-0ed371ce653f",
        "metadata": {
          "vector_store_key": "1804.08782-7",
          "chunk_id": 37,
          "document_id": "1804.08782",
          "start_idx": 13990,
          "end_idx": 14427
        },
        "page_content": " number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 .  For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose",
        "type": "Document"
      },
      {
        "id": "947ca67f-f47a-4cb3-91ff-40d57888bc24",
        "metadata": {
          "vector_store_key": "1909.09484-5",
          "chunk_id": 55,
          "document_id": "1909.09484",
          "start_idx": 19263,
          "end_idx": 19734
        },
        "page_content": " models and baselines, with a learning rate of 0.001 for supervised training and 0.0001 for reinforcement learning. In reinforcement learning, the decay parameter $\\lambda $ is set to 0.8. The weight decay is set to 0.001. And early stopping is performed on developing set. The experimental results of the proposed model and baselines will be analyzed from the following aspects. BPRA Results: As shown in Table TABREF35, most of the models have similar performance on BP",
        "type": "Document"
      },
      {
        "id": "95d982c1-df48-40f1-aea4-aa545ace30dc",
        "metadata": {
          "vector_store_key": "1909.02635-8",
          "chunk_id": 23,
          "document_id": "1909.02635",
          "start_idx": 9050,
          "end_idx": 9548
        },
        "page_content": " (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo",
        "type": "Document"
      },
      {
        "id": "0b77285e-fdf5-4273-8639-5afcd5734567",
        "metadata": {
          "vector_store_key": "2001.02380-3",
          "chunk_id": 140,
          "document_id": "2001.02380",
          "start_idx": 51470,
          "end_idx": 51956
        },
        "page_content": " baselines. The left, middle and right panels in Figure FIGREF40 correspond to measurements when all signals are included, only cases contained entirely in the head EDUs shown to the model, and only DMs, respectively. The scenario on the left is rather unreasonable and is included only for completeness: here the model is also penalized for not detecting signals such as lexical chains, part of which is outside the units that the model is being shown. An example of such a case can be",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Where did they get training data?",
    "llm_answer": "From a corpus of approximately 300,000 sentences, and 2,250 users contributing 225,000 tweets.\n",
    "context": [
      {
        "id": "a872be77-b483-4ddf-9cee-412fb23e61cf",
        "metadata": {
          "vector_store_key": "1912.13109-4",
          "chunk_id": 19,
          "document_id": "1912.13109",
          "start_idx": 7657,
          "end_idx": 8136
        },
        "page_content": " above few data augmentation techniques were applied to boost the learning of the network. Before applying augmentation, a train-test split of 78%-22% was done from the original, cleansed data set. Thus, 700 tweets/messages were held out for testing. All model evaluation were done in on the test set that got generated by this process. The results presented in this report are based on the performance of the model on the test set. The training set of 2489 messages were however",
        "type": "Document"
      },
      {
        "id": "200129dd-56ce-4e79-9e9b-d7d4d9cbf6c7",
        "metadata": {
          "vector_store_key": "1806.04524-3",
          "chunk_id": 25,
          "document_id": "1806.04524",
          "start_idx": 9624,
          "end_idx": 10136
        },
        "page_content": " the usage of other pre-trained vectors for future work. Using our platform, we extracted anonymized user interaction data in the manner of real quizzes generated for a collection of several input video sources. We obtained a corpus of approximately 300,000 sentences, from which roughly 1.5 million single-quiz question training examples were derived. We split this dataset using the regular 70/10/20 partition for training, validation and testing. As the system required the input sentences to be tokenized and",
        "type": "Document"
      },
      {
        "id": "894f4ffd-fbcb-43a7-ba0f-9cca665b9304",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 2,
          "document_id": "1909.00542",
          "start_idx": 734,
          "end_idx": 1323
        },
        "page_content": " the training procedure was based on a regression set up, this year we experiment with various classification set ups. The main contributions of this paper are: We compare classification and regression approaches and show that classification produces better results than regression but the quality of the results depends on the approach followed to annotate the data labels. We conduct correlation analysis between various ROUGE evaluation metrics and the human evaluations conducted at BioASQ and show that Precision and F1 correlate better than Recall. Section SECREF2 briefly introduces",
        "type": "Document"
      },
      {
        "id": "2e2f38b6-f339-4735-b9ba-8817249f68fd",
        "metadata": {
          "vector_store_key": "1909.04181-0",
          "chunk_id": 4,
          "document_id": "1909.04181",
          "start_idx": 1596,
          "end_idx": 2096
        },
        "page_content": " were expected to submit their predictions on test. The shared task predictions are expected by organizers at the level of users. The distribution has 100 tweets for each user, and so each tweet is distributed with a corresponding user id. As such, in total, the distributed training data has 2,250 users, contributing a total of 225,000 tweets. The official task test set contains 720,00 tweets posted by 720 users. For our experiments, we split the training data released by organizers into 90% TRA",
        "type": "Document"
      },
      {
        "id": "c94d339e-36f5-4336-a21a-05be0b487cef",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What extraction model did they use?",
    "llm_answer": "Sequence-to-sequence model.\n",
    "context": [
      {
        "id": "6870b31f-0951-46f9-aa38-84e0db2093cb",
        "metadata": {
          "vector_store_key": "1706.04815-1",
          "chunk_id": 90,
          "document_id": "1706.04815",
          "start_idx": 25770,
          "end_idx": 26320
        },
        "page_content": " The extraction model aims to match the question and passage and predict most important sub-spans in the passage related to the question as evidence. Then, the synthesis model synthesizes the question information and the evidence snippet to generate the final answer. We propose a multi-task learning framework to improve the evidence extraction model by passage ranking to extract the evidence snippet, and use the sequence-to-sequence model for answer synthesis. We conduct experiments on the MS-MARCO dataset. Results demonstrate that our approach",
        "type": "Document"
      },
      {
        "id": "f4e009ad-6dd6-4806-8c9a-809372a83a9e",
        "metadata": {
          "vector_store_key": "1706.04815-4",
          "chunk_id": 62,
          "document_id": "1706.04815",
          "start_idx": 15638,
          "end_idx": 16084
        },
        "page_content": "  (Eq. 32)  where $\\mathcal {D}$ is the set of data. $X$ represents the question and passage including evidence snippets, and $Y$ represents the answer. We conduct our experiments on the MS-MARCO dataset BIBREF1 . We compare our extraction-then-synthesis framework with pure extraction model and other baseline methods on the leaderboard of MS-MARCO. Experimental results show that our model achieves better results in official evaluation metrics",
        "type": "Document"
      },
      {
        "id": "546cb24a-634c-44cd-9dfe-e93c2878d856",
        "metadata": {
          "vector_store_key": "1908.06606-1",
          "chunk_id": 14,
          "document_id": "1908.06606",
          "start_idx": 5338,
          "end_idx": 5846
        },
        "page_content": "REF4 identified patients undergoing radical cystectomy for bladder cancer. Although they achieved good performance, none of their models could be used to another task due to output format difference. This makes building a new model for a new task a costly job. Pipeline methods BIBREF7, BIBREF8, BIBREF9 break down the entire task into several basic natural language processing tasks. Bill et al. BIBREF7 focused on attributes extraction which mainly relied on dependency parsing and named entity recognition",
        "type": "Document"
      },
      {
        "id": "05594f02-6d5b-4f0f-8af7-4e3dc0768458",
        "metadata": {
          "vector_store_key": "2001.06888-4",
          "chunk_id": 49,
          "document_id": "2001.06888",
          "start_idx": 18183,
          "end_idx": 18669
        },
        "page_content": "ding. The result of this phase is extracted features for each word. Image Feature Extraction is shown in the right part of figure FIGREF11. For this part, we have used InceptionV3 pretrained on ImageNet BIBREF60. Many models were available as first part of image feature extraction, however the main reason we used InceptionV3 as feature extractor backbone is better performance of it on ImageNet and the results obtained by this particular model were slightly better compared to others",
        "type": "Document"
      },
      {
        "id": "b7bb6c8c-3c57-465e-af2a-57d4d171b33c",
        "metadata": {
          "vector_store_key": "1905.13413-3",
          "chunk_id": 5,
          "document_id": "1905.13413",
          "start_idx": 2024,
          "end_idx": 2585
        },
        "page_content": " explicitly increase the confidences of correct extractions and decrease those of incorrect ones. Without adding additional model components, this training paradigm naturally leads to a better open IE model, whose extractions can be further included as training samples. We further propose an iterative learning procedure that gradually improves the model by incrementally adding extractions to the training data. Experiments on the OIE2016 dataset BIBREF8 indicate that our method significantly outperforms both neural and non-neural models. We briefly revisit",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which datasets did they experiment on?",
    "llm_answer": "Action Adult Adventure Animation Biography Comedy Crime Documentary Drama Family Fantasy Film-Noir Game-Show History Horror Music Musical Mystery Reality-TV Romance Sci-Fi Short Sport Talk-Show Thriller War Western, Biomedical, novel-9\n",
    "context": [
      {
        "id": "9a66176f-fe2e-4da0-a2df-70a3d739924f",
        "metadata": {
          "vector_store_key": "1802.05322-2",
          "chunk_id": 38,
          "document_id": "1802.05322",
          "start_idx": 13905,
          "end_idx": 14341
        },
        "page_content": " explanation of the whole process needed to conduct an experiment like this. The process includes downloading a data set, web scraping for extra information, data preprocessing, model tuning and evaluation of the results. Action Adult Adventure Animation Biography Comedy Crime Documentary Drama Family Fantasy Film-Noir Game-Show History Horror Music Musical Mystery Reality-TV Romance Sci-Fi Short Sport Talk-Show Thriller War Western",
        "type": "Document"
      },
      {
        "id": "de28f623-e56b-46af-88a3-78e233bbcff1",
        "metadata": {
          "vector_store_key": "1607.06025-5",
          "chunk_id": 66,
          "document_id": "1607.06025",
          "start_idx": 24769,
          "end_idx": 25338
        },
        "page_content": " were constructed by filtering the generated datasets according to various thresholds. Thus, the generated datasets were constructed to contain enough examples, so that the filtered datasets had at least the number of examples as the original dataset. In the end, all the datasets were trimmed down to the size of the original dataset by selecting the samples sequentially from the beginning until the dataset had the right size. Also, the datasets were filtered so that each of the labels was represented equally. All the models, including classification and discrimin",
        "type": "Document"
      },
      {
        "id": "c94d339e-36f5-4336-a21a-05be0b487cef",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      },
      {
        "id": "c5eaf502-a11a-40bd-84a8-78f62e8d8d02",
        "metadata": {
          "vector_store_key": "1701.00185-9",
          "chunk_id": 10,
          "document_id": "1701.00185",
          "start_idx": 3790,
          "end_idx": 4304
        },
        "page_content": " this paper we use a new short text dataset, Biomedical, in the experiment to verify the effectiveness of our approach. Third, we put much effort on studying the influence of various different semantic features integrated in our self-taught CNN framework, which is not involved in the conference paper. For the purpose of reproducibility, we make the datasets and software used in our experiments publicly available at the website. The remainder of this paper is organized as follows: In Section SECREF2 , we first",
        "type": "Document"
      },
      {
        "id": "189f2e62-5589-4ac4-a44d-70623d5e2128",
        "metadata": {
          "vector_store_key": "1709.02271-8",
          "chunk_id": 31,
          "document_id": "1709.02271",
          "start_idx": 11255,
          "end_idx": 11734
        },
        "page_content": " We begin by introducing the datasets (Section SECREF15 ), followed by detailing the featurization methods (Section SECREF17 ), the experiments (Section SECREF22 ), and finally reporting results (Section SECREF26 ). The statistics for the three datasets used in the experiments are summarized in Table TABREF16 . novel-9. This dataset was compiled by F&H14: a collection of 19 novels by 9 nineteenth century British and American authors in the Project Gutenberg. To compare to F&",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do slot binary classifiers improve performance?",
    "llm_answer": "By resolving issues of TSCP, eliminating undesired dependencies among slots, and performing binary classification for each slot.\n",
    "context": [
      {
        "id": "cc7fdeda-dc3b-439c-a60b-c9698db834b7",
        "metadata": {
          "vector_store_key": "1908.02402-2",
          "chunk_id": 44,
          "document_id": "1908.02402",
          "start_idx": 13555,
          "end_idx": 14034
        },
        "page_content": " to perform binary classification for each slot. This greatly resolves the issues of TSCP that uses a single decoder with each step having unconstrained vocabulary-size choices, which may potentially lead to generating non-slot words. Similar to the informable slots decoders, such a separate classifier also eliminates the undesired dependencies among slots. Let $\\lbrace k^R\\rbrace $ denote the set of requestable slots. A single GRU cell is used to perform the classification.",
        "type": "Document"
      },
      {
        "id": "d65e6f6b-601c-4ef4-98c4-822b7814e5c2",
        "metadata": {
          "vector_store_key": "2002.01359-0",
          "chunk_id": 56,
          "document_id": "2002.01359",
          "start_idx": 23358,
          "end_idx": 23874
        },
        "page_content": " slots is comparable to that on non-categorical slots. On the other hand, for seen services, the performance on categorical slots is better. This could be because there is less signal to differentiate between the different possible values for a categorical slot when they have not been observed in the training set. The winning team's performance on seen services is similar to that of the other top teams. However, the winning team has a considerable edge on unseen services, outperforming the second team by around",
        "type": "Document"
      },
      {
        "id": "731dea6c-8b47-4b32-880e-60dcf594f29a",
        "metadata": {
          "vector_store_key": "1908.02402-2",
          "chunk_id": 32,
          "document_id": "1908.02402",
          "start_idx": 11084,
          "end_idx": 11518
        },
        "page_content": " slot. The informable slot value decoder consists of GRU recurrent layers with a copy mechanism as shown in the yellow section of Figure 1 . It is composed of weight-tied GRU generators that take the same initial hidden state $h^{E}_{l}$ , but have different start-of-sentence symbols for each unique informable slot. This way, each informable slot value decoder is dependent on the encoder's output, but it is also independent of the",
        "type": "Document"
      },
      {
        "id": "7992434e-3dbd-4c7c-bb6e-121bf21a9efb",
        "metadata": {
          "vector_store_key": "1908.02402-2",
          "chunk_id": 23,
          "document_id": "1908.02402",
          "start_idx": 8509,
          "end_idx": 8900
        },
        "page_content": "t}$ is the user requested slots identified by the requestable slots multi-label classifier; (3) Given $I_t$ , the KB query component queries the KB and encodes the number of records returned in a one-hot vector $d_t$ ; (4) The response slot binary classifier predicts which slots should appear in the agent response $S_t$ ; (5) Finally, the agent response decoder takes in the KB output $d_t",
        "type": "Document"
      },
      {
        "id": "ab92069e-e8ef-4e9c-b039-fb114bfaa9a8",
        "metadata": {
          "vector_store_key": "2002.01359-2",
          "chunk_id": 47,
          "document_id": "2002.01359",
          "start_idx": 19646,
          "end_idx": 20166
        },
        "page_content": " C BIBREF30: They use a two-stage model with a candidate tracker for NLU and a candidate classifier to update the dialogue state. A slot tagger identifies slot values, which are used to update the candidate tracker. The candidate classifier uses the utterances and slot/intent descriptions to predict the final dialogue state. They also use an additional loss to penalize incorrect prediction on which slots appear in the current turn. We consider the following metrics for automatic evaluation of different submissions.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What baselines have been used in this work?",
    "llm_answer": "Baseline 1, Baseline 2, Baseline 3, ANRES_Sing_R, ANRES_Sing_Z, ANRES_Seq_R, ANRES_Seq_Z, LSTM baseline.\n",
    "context": [
      {
        "id": "593127ef-0325-4226-be3e-0ed371ce653f",
        "metadata": {
          "vector_store_key": "1804.08782-7",
          "chunk_id": 37,
          "document_id": "1804.08782",
          "start_idx": 13990,
          "end_idx": 14427
        },
        "page_content": " number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 .  For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose",
        "type": "Document"
      },
      {
        "id": "0b77285e-fdf5-4273-8639-5afcd5734567",
        "metadata": {
          "vector_store_key": "2001.02380-3",
          "chunk_id": 140,
          "document_id": "2001.02380",
          "start_idx": 51470,
          "end_idx": 51956
        },
        "page_content": " baselines. The left, middle and right panels in Figure FIGREF40 correspond to measurements when all signals are included, only cases contained entirely in the head EDUs shown to the model, and only DMs, respectively. The scenario on the left is rather unreasonable and is included only for completeness: here the model is also penalized for not detecting signals such as lexical chains, part of which is outside the units that the model is being shown. An example of such a case can be",
        "type": "Document"
      },
      {
        "id": "fe0f18bc-f5e9-4a30-bd8f-1b27de376893",
        "metadata": {
          "vector_store_key": "2004.01878-5",
          "chunk_id": 40,
          "document_id": "2004.01878",
          "start_idx": 14526,
          "end_idx": 14915
        },
        "page_content": " normal distribution $\\mathcal {N}(0, 1)$. In summary, the following four baselines are designed: ANRES_Sing_R: randomly initializing the states for each single trading day. ANRES_Sing_Z: initializing the states as zeros for each single trading day. ANRES_Seq_R: randomly initializing the first states for each trading sequence only. ANRES_Seq_Z: initializing the first states as zeros for",
        "type": "Document"
      },
      {
        "id": "95d982c1-df48-40f1-aea4-aa545ace30dc",
        "metadata": {
          "vector_store_key": "1909.02635-8",
          "chunk_id": 23,
          "document_id": "1909.02635",
          "start_idx": 9050,
          "end_idx": 9548
        },
        "page_content": " (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo",
        "type": "Document"
      },
      {
        "id": "97be5370-e788-41d2-9b17-3e7739266f06",
        "metadata": {
          "vector_store_key": "2001.11381-3",
          "chunk_id": 138,
          "document_id": "2001.11381",
          "start_idx": 31860,
          "end_idx": 32196
        },
        "page_content": "ados. Dicho lo anterior, el Modelo 1 podr\u00eda ser considerado como nuestro propio baseline. A continuaci\u00f3n presentamos un protocolo de evaluaci\u00f3n manual de los resultados obtenidos. El experimento consisti\u00f3 en la generaci\u00f3n de 15 frases por cada uno de los tres modelos propuestos. Para cada modelo, se consideraron tres queries: $Q=$ {AM",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what bottlenecks were identified?",
    "llm_answer": "Recognition bottlenecks in a menu-based speech solution.\n",
    "context": [
      {
        "id": "f26e9d66-643e-49c0-b961-d02f853035d7",
        "metadata": {
          "vector_store_key": "1601.02543-3",
          "chunk_id": 32,
          "document_id": "1601.02543",
          "start_idx": 12144,
          "end_idx": 12571
        },
        "page_content": " actually putting it to people test. There is a significant saving in terms of being able to identify recognition bottlenecks in a menu based speech solution through this analysis because it does not require actual people testing the system. This methodology was adopted to restructuring the set of active words at each node for better speech recognition in an actual menu based speech recognition system that caters to masses.",
        "type": "Document"
      },
      {
        "id": "cf998e7d-c3e7-44e3-9906-d37a80cfcee9",
        "metadata": {
          "vector_store_key": "1802.05574-4",
          "chunk_id": 28,
          "document_id": "1802.05574",
          "start_idx": 10995,
          "end_idx": 11490
        },
        "page_content": "12 . We used the time estimates from our internal annotation to price the task in order to be above US minimum wage. All workers were qualified before being issued tasks. Overall, we employed 10 crowd workers. On average it took 30 minutes for a worker to complete a HIT. In line with BIBREF13 , we monitored for potential non-performance or spam by looking for long response times and consecutive submitted results. We saw no indicators of low quality responses. In total, 11262 judgements were",
        "type": "Document"
      },
      {
        "id": "148ea921-3ae7-4687-bc70-85a9690fe58b",
        "metadata": {
          "vector_store_key": "1912.13109-4",
          "chunk_id": 20,
          "document_id": "1912.13109",
          "start_idx": 7921,
          "end_idx": 8471
        },
        "page_content": " on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we",
        "type": "Document"
      },
      {
        "id": "22b1ecd2-f790-46bb-ad4f-4e6f93ed32a0",
        "metadata": {
          "vector_store_key": "1611.00514-7",
          "chunk_id": 46,
          "document_id": "1611.00514",
          "start_idx": 17234,
          "end_idx": 17771
        },
        "page_content": " other portion to avoid any over-fitting, particularly important for the domain adaptation technique. This resulted in a relative improvement of 11% compared to the primary system in terms of the primary metric. However, the results can be misleading, since the recording condition may be the same for all speakers in the development set. This section reports on the CPU execution time (single threaded), and the amount of memory used to process a single trial, which includes the time for creating models from the enrolment data and the",
        "type": "Document"
      },
      {
        "id": "d9c817c3-a211-4b55-9b45-d9c4f63cece2",
        "metadata": {
          "vector_store_key": "1701.02877-4",
          "chunk_id": 141,
          "document_id": "1701.02877",
          "start_idx": 50827,
          "end_idx": 51373
        },
        "page_content": " The other genre that was particularly difficult for the systems was ACE Usenet. This is a form of user-generated content, not intended for publication but rather discussion among communities. In this sense, it is social media, and so it is not surprising that system performance on ACE UN resembles performance on social media more than other genres. Crucially, the computationally-cheap memorisation method actually acts as a reasonable predictor of the performance of other methods. This suggests that high entity diversity predicts difficulty",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much do they outperform BiLSTMs in Sentiment Analysis?",
    "llm_answer": "The context states that the model significantly outperforms BiLSTMs, but does not provide a specific numerical difference.\n",
    "context": [
      {
        "id": "e5126247-b9d4-4ede-88f7-a6ebb75e912b",
        "metadata": {
          "vector_store_key": "1911.02711-5",
          "chunk_id": 8,
          "document_id": "1911.02711",
          "start_idx": 3209,
          "end_idx": 3722
        },
        "page_content": " results show that our model significantly outperforms all strong baselines, including joint modeling, separate encoder and joint encoder methods. In addition, our model achieves new state-of-the-art performance, attaining 2.1% (with generated summary) and 4.8% (with golden summary) absolutely improvements compared to the previous best method on SNAP Amazon review benchmark. The majority of recent sentiment analysis models are based on either convolutional or recurrent neural networks to encode sequences BIB",
        "type": "Document"
      },
      {
        "id": "1d82cd1d-3db5-4116-97b4-195d19aadd65",
        "metadata": {
          "vector_store_key": "1604.05781-3",
          "chunk_id": 58,
          "document_id": "1604.05781",
          "start_idx": 20364,
          "end_idx": 20856
        },
        "page_content": " scored text in the causal corpus and 77.2% of the control corpus. The difference in sentiment between corpora was significant for all distributions (t-test: $p < 0.01$ ). Fourth, to further confirm that the causal documents tend toward negative sentiment, we applied a separate, independent sentiment analysis using the Stanford NLP sentiment toolkit BIBREF38 to classify the sentiment of individual documents not unigrams (see Methods). Instead of a numeric sentiment score, this classifier",
        "type": "Document"
      },
      {
        "id": "bf781e24-ae76-47e2-96a6-cd675bbf8bab",
        "metadata": {
          "vector_store_key": "1710.01492-0",
          "chunk_id": 12,
          "document_id": "1710.01492",
          "start_idx": 4787,
          "end_idx": 5328
        },
        "page_content": ", which is needed to train the high number of parameters of such complex models. Finally, as methods for sentiment analysis mature, more attention is also being paid to linguistic structure and to multi-linguality and cross-linguality. Sentiment analysis emerged as a popular research direction in the early 2000s. Initially, it was regarded as standard document classification into topics such as business, sport, and politics BIBREF10 . However, researchers soon realized that it was quite different from standard document classification B",
        "type": "Document"
      },
      {
        "id": "5597e546-6e83-43be-867a-4de35da596dc",
        "metadata": {
          "vector_store_key": "1809.10644-0",
          "chunk_id": 22,
          "document_id": "1809.10644",
          "start_idx": 8344,
          "end_idx": 8845
        },
        "page_content": "001 significance compared to both methods. We also include in-depth precision and recall results for all three datasets in the supplement. Our results indicate better performance than several more complex approaches, including BIBREF4 's best model (which used word and part-of-speech ngrams, sentiment, readability, text, and Twitter specific features), BIBREF6 (which used two fold classification and a hybrid of word and character CNNs, using approximately twice the parameters we use excluding the",
        "type": "Document"
      },
      {
        "id": "634ff76e-2c1c-4162-a752-be71dc636cd7",
        "metadata": {
          "vector_store_key": "1803.07771-0",
          "chunk_id": 12,
          "document_id": "1803.07771",
          "start_idx": 4985,
          "end_idx": 5392
        },
        "page_content": " lexicon-based algorithm shown in Fig. 1, the sentiment score of \u201cit is not bad\" is 0.25, and the sentiment score of \u201cit is good\" is 1. However, the score of \u201cit is not so bad\" is -0.75, and this score is definitely wrong. Therefore, machine learning (including feature learning) methodologies have become mainstream in sentiment analysis. Deep learning (including word embedding BIBREF12 ) has been applied",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what state of the accuracy did they obtain?",
    "llm_answer": "The provided text reports accuracies ranging from 58.3% to 99%.\n",
    "context": [
      {
        "id": "4967b5f1-8f00-469e-a23c-f10dd6b5fda1",
        "metadata": {
          "vector_store_key": "1804.03396-1",
          "chunk_id": 96,
          "document_id": "1804.03396",
          "start_idx": 30860,
          "end_idx": 31374
        },
        "page_content": " Consistency. We randomly sample 25 articles respectively from the 6 datasets (in total of 1002 ground truth triples with 2691 labeled answer locations) and let two human annotators label the Triple Accuracy for each ground truth triple and the Contextual Consistency for each answer location. The two human annotators reached an agreement on 131 of 132 randomly selected Triple Accuracy cases and on 229 of 234 randomly selected Contextual Consistency cases. The human evaluation results are shown in Table 8 . We",
        "type": "Document"
      },
      {
        "id": "ff224634-5e59-4333-8bf6-50641a781ae8",
        "metadata": {
          "vector_store_key": "1911.03243-4",
          "chunk_id": 27,
          "document_id": "1911.03243",
          "start_idx": 10852,
          "end_idx": 11313
        },
        "page_content": "Bank. To examine this, we analyzed 100 false positive arguments. Only 32 of those were due to wrong or incomplete QA annotations in our gold, while most others were outside of PropBank's scope, capturing either implied arguments or roles not covered in PropBank. Extrapolating from this manual analysis estimates our true precision (on all roles) to be about 91%, which is consistent with the 88% precision figure in Table TABREF19. Compared with 2015, our QA-S",
        "type": "Document"
      },
      {
        "id": "822f3247-f0a8-461d-ac3a-2fd0c48eb102",
        "metadata": {
          "vector_store_key": "1907.00758-9",
          "chunk_id": 45,
          "document_id": "1907.00758",
          "start_idx": 17058,
          "end_idx": 17541
        },
        "page_content": ".9% accuracy with a mean and standard deviation discrepancy of 32 INLINEFORM6 223ms. SyncNet reports INLINEFORM7 99% accuracy on lip video synchronisation using a manual evaluation where the lip error is not detectable to a human observer BIBREF4 . However, we argue that our data is more challenging (Section SECREF4 ). Analysis: We analyse the performance of our model across different conditions. Table shows the model accuracy broken down by utterance type. The model achieves 91",
        "type": "Document"
      },
      {
        "id": "b198b932-1761-4d0f-9718-5ab4e539a320",
        "metadata": {
          "vector_store_key": "1907.00758-9",
          "chunk_id": 44,
          "document_id": "1907.00758",
          "start_idx": 16635,
          "end_idx": 17058
        },
        "page_content": " for each utterance. If the discrepancy falls within the minimum detectability range ( INLINEFORM0 125 INLINEFORM1 INLINEFORM2 INLINEFORM3 INLINEFORM4 45) then the prediction is correct. Random prediction (averaged over 1000 runs) yields 14.6% accuracy with a mean and standard deviation discrepancy of 328 INLINEFORM5 518ms. We achieve 82.9% accuracy with a mean and standard deviation discrepancy of 32 INLINEFORM6 223ms.",
        "type": "Document"
      },
      {
        "id": "01045205-61e3-47b4-9e6f-acc85f88e67e",
        "metadata": {
          "vector_store_key": "1905.13497-3",
          "chunk_id": 28,
          "document_id": "1905.13497",
          "start_idx": 9658,
          "end_idx": 10040
        },
        "page_content": " 2016 competition (58.3%) by a large margin. Specifically, we achieve 68.3% accuracy, better than the more recently reported results from BIBREF24 (66.7%), who makes use of three KBs and a supervised deep network. On the harder task WSC-273, our method also outperforms the current state-of-the-art, as shown in Tab. 2. Namely, our method achieves an accuracy of 60.3%, nearly 3% of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what models did they compare to?",
    "llm_answer": "Stanford NER, Flair, and the best previously-published result,  NABERT+ baseline, and Ablation models.\n",
    "context": [
      {
        "id": "c357514b-19f8-468b-b68e-b8bc90741f38",
        "metadata": {
          "vector_store_key": "1701.06538-1",
          "chunk_id": 45,
          "document_id": "1701.06538",
          "start_idx": 17346,
          "end_idx": 17826
        },
        "page_content": ". Details can be found in Appendix UID77 . Results of these three models form the bottom line of Figure FIGREF32 -right. Table TABREF33 compares the results of these models to the best previously-published result on this dataset . Even the fastest of these models beats the best published result (when controlling for the number of training epochs), despite requiring only 6% of the computation. We trained our models using TensorFlow BIBREF30 on clusters containing 16-32 Tesla K",
        "type": "Document"
      },
      {
        "id": "023a760d-81e9-48e4-8692-88260707e423",
        "metadata": {
          "vector_store_key": "2002.11402-1",
          "chunk_id": 24,
          "document_id": "2002.11402",
          "start_idx": 8819,
          "end_idx": 9288
        },
        "page_content": "es other models in all scores. Spacy results are good despite not being trained for case-less data. In terms of F1 and overall stability Spacy did better than Stanford NER, on our News Validation set. Similarly, Stanford did well in Precision but could not catch up with Spacy and our model in terms of Recall. Flair overall performed poorly, but as said before these open-source models are not trained for our particular use-case. Lets check some examples for detailed",
        "type": "Document"
      },
      {
        "id": "555a266e-0a08-43e5-b719-8cd2fd10ebb8",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "6cd0e150-048c-4d4c-b286-8490e631314b",
        "metadata": {
          "vector_store_key": "1810.09774-1",
          "chunk_id": 29,
          "document_id": "1810.09774",
          "start_idx": 11025,
          "end_idx": 11459
        },
        "page_content": " , BIBREF2 and BIBREF0 respectively. For BERT we used the uncased 768-dimensional model (BERT-base). For ESIM + ELMo we used the AllenNLP BIBREF23 PyTorch implementation with the default settings and hyperparameter values. Table 4 contains all the experimental results. Our experiments show that, while all of the six models perform well when the test set is drawn from the same corpus as the training and development set, accuracy is",
        "type": "Document"
      },
      {
        "id": "dc50a5cf-b755-41a0-a173-1b561db0ed05",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "which benchmark tasks did they experiment on?",
    "llm_answer": "Molecule generation, processing a single trial, and a multi-task setting.\n",
    "context": [
      {
        "id": "304b5978-d253-43d4-92b8-7dc6acce1bd5",
        "metadata": {
          "vector_store_key": "2002.06053-5",
          "chunk_id": 123,
          "document_id": "2002.06053",
          "start_idx": 45532,
          "end_idx": 46097
        },
        "page_content": " datasets and different evaluation metrics makes the assessment of model performance challenging. Comprehensive benchmarking platforms that can assess the success of different tools are still lacking. A benchmarking environment rigorously brings together the suitable data sets and evaluation methodologies in order to provide a fair comparison between the available tools. Such environments are available for molecule generation task from MOSES BIBREF123 and GuacaMol BIBREF124. MoleculeNet is also a similar attempt to build a benchmarking platform for tasks such",
        "type": "Document"
      },
      {
        "id": "7445d9d5-f287-4162-8fc9-ee31261a72c6",
        "metadata": {
          "vector_store_key": "1611.00514-7",
          "chunk_id": 47,
          "document_id": "1611.00514",
          "start_idx": 17771,
          "end_idx": 18207
        },
        "page_content": " to process a single trial, which includes the time for creating models from the enrolment data and the time needed for processing the test segments. The analysis was performed on an Intel(R) Xeon(R) CPU E5-2670 2.60GHz. The results are shown in Table TABREF27 . We used the time command in Unix to report these results. The user time is the actual CPU time used in executing the process (single thread). The real time is the wall clock",
        "type": "Document"
      },
      {
        "id": "148ea921-3ae7-4687-bc70-85a9690fe58b",
        "metadata": {
          "vector_store_key": "1912.13109-4",
          "chunk_id": 20,
          "document_id": "1912.13109",
          "start_idx": 7921,
          "end_idx": 8471
        },
        "page_content": " on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we",
        "type": "Document"
      },
      {
        "id": "c6187822-5cbb-4bfd-abff-028a25c2a720",
        "metadata": {
          "vector_store_key": "1910.11769-3",
          "chunk_id": 19,
          "document_id": "1910.11769",
          "start_idx": 7264,
          "end_idx": 7771
        },
        "page_content": "-processed the data by using the SpaCy pipeline. We masked out named entities with entity-type specific placeholders to reduce the chance of benchmark models utilizing named entities as a basis for classification. Benchmark results are shown in Table TABREF17. The dataset is approximately balanced after discarding the Surprise and Disgust classes. We report the average micro-F1 scores, with 5-fold cross validation for each technique. We provide a brief overview of each benchmark experiment below. Among",
        "type": "Document"
      },
      {
        "id": "270630ad-16a4-4d78-9767-8ea17f811346",
        "metadata": {
          "vector_store_key": "1909.08859-5",
          "chunk_id": 51,
          "document_id": "1909.08859",
          "start_idx": 17614,
          "end_idx": 18113
        },
        "page_content": " We used Adam optimizer with a learning rate of 1e-4 with an early stopping criteria with the patience set to 10 indicating that the training procedure ends after 10 iterations if the performance would not improve. We considered a batch size of 32 due to our hardware constraints. In the multi-task setting, batches are sampled round-robin from all tasks, where each batch is solely composed of examples from one task. We performed our experiments on a system containing four NVIDIA GTX-1080Ti GPUs,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the proficiency score calculated?",
    "llm_answer": "Each indicator (of which there are 6) is assigned a value of 0, 1, or 2 (bad, medium, good, respectively).  The total score is computed by summing the scores of the indicators.\n",
    "context": [
      {
        "id": "2c9bd19d-ad10-4940-b011-6d2471744f49",
        "metadata": {
          "vector_store_key": "2001.08051-1",
          "chunk_id": 1,
          "document_id": "2001.08051",
          "start_idx": 486,
          "end_idx": 928
        },
        "page_content": " experts in terms of some predefined \u201cindicators\u201d which, in turn, were used to assign the proficiency level to each student undertaking the assigned test. This level is expressed according to the well-known Common European Framework of Reference for Languages (Council of Europe, 2001) scale. The CEFR defines 6 levels of proficiency: A1 (beginner), A2, B1, B2, C1 and C2. The levels considered in the evaluation campaigns where the data have",
        "type": "Document"
      },
      {
        "id": "30c65d97-4444-4774-9514-63d010dbe5de",
        "metadata": {
          "vector_store_key": "2001.08051-1",
          "chunk_id": 22,
          "document_id": "2001.08051",
          "start_idx": 8701,
          "end_idx": 9229
        },
        "page_content": " medium, good, respectively. The list of the indicators used by the experts to score written sentences and spoken utterances in the evaluations, grouped by similarity, is reported in Table . Since every utterance was scored by only one expert, it was not possible to evaluate any kind of agreement among experts. For future evaluations, more experts are expected to provide independent scoring on same data sets, so this kind of evaluation will be possible. The speaking part of the proficiency tests in 2017/2018 consists of 47",
        "type": "Document"
      },
      {
        "id": "8459ad0e-df87-440f-9b4d-cbcbd397eae2",
        "metadata": {
          "vector_store_key": "2002.05058-0",
          "chunk_id": 62,
          "document_id": "2002.05058",
          "start_idx": 24483,
          "end_idx": 24978
        },
        "page_content": " machine-generated samples for each task and compare each sample with all references by the comparative evaluator. A sample gets 3 points when beats a reference, 1 point when draws with the reference, and get 0 point when loses, 2) we adopt skill rating system by regarding each sample as an NLG model which always outputs the same sample and use the skill rating for each sample as its score. To ensure the computational budget to be roughly the same, we fix the number of plays in skill rating",
        "type": "Document"
      },
      {
        "id": "51bf1d53-4bc3-43f9-aac1-c9e376f8f602",
        "metadata": {
          "vector_store_key": "2001.08051-1",
          "chunk_id": 21,
          "document_id": "2001.08051",
          "start_idx": 8219,
          "end_idx": 8701
        },
        "page_content": " from both the written and spoken data collected so far in all the campaigns. Each written or spoken item received a total score by human experts, computed by summing up the scores related to 6 indicators in 2017/2018 (from 3 to 6 in the 2016 campaign, according to the proficiency levels and the type of test). Each indicator can assume a value 0, 1, 2, corresponding to bad, medium, good, respectively. The list of the indicators used by the experts to score written sentences and",
        "type": "Document"
      },
      {
        "id": "5a7402cd-29ef-42bc-8c01-be5fdbeb6ce3",
        "metadata": {
          "vector_store_key": "1705.03261-3",
          "chunk_id": 39,
          "document_id": "1705.03261",
          "start_idx": 14252,
          "end_idx": 14657
        },
        "page_content": " : the overall precision, recall, and F1 score on testing set. INLINEFORM0 denotes the set of {False, Mechanism, Effect, Advise, Int}. The precision and recall of each INLINEFORM1 are calculated by DISPLAYFORM0 DISPLAYFORM1  Then the overall precision, recall, and F1 score are calculated by DISPLAYFORM0  Besides, we evaluate the captured feature vectors with t-SNE BIBREF26 , a visualizing and intuitive",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What proficiency indicators are used to the score the utterances?",
    "llm_answer": "The context states that 6 indicators are used to score written and spoken utterances, but does not list them.\n",
    "context": [
      {
        "id": "0f9e7ea0-60f7-4435-97dd-c109e7ddefa7",
        "metadata": {
          "vector_store_key": "2001.08051-1",
          "chunk_id": 2,
          "document_id": "2001.08051",
          "start_idx": 768,
          "end_idx": 1203
        },
        "page_content": " B2, C1 and C2. The levels considered in the evaluation campaigns where the data have been collected are: A1, A2 and B1. The indicators measure the linguistic competence of test takers both in relation to the content (e.g. grammatical correctness, lexical richness, semantic coherence, etc.) and to the speaking capabilities (e.g. pronunciation, fluency, etc.). Refer to Section SECREF2 for a description of the adopted indicators. The",
        "type": "Document"
      },
      {
        "id": "30c65d97-4444-4774-9514-63d010dbe5de",
        "metadata": {
          "vector_store_key": "2001.08051-1",
          "chunk_id": 22,
          "document_id": "2001.08051",
          "start_idx": 8701,
          "end_idx": 9229
        },
        "page_content": " medium, good, respectively. The list of the indicators used by the experts to score written sentences and spoken utterances in the evaluations, grouped by similarity, is reported in Table . Since every utterance was scored by only one expert, it was not possible to evaluate any kind of agreement among experts. For future evaluations, more experts are expected to provide independent scoring on same data sets, so this kind of evaluation will be possible. The speaking part of the proficiency tests in 2017/2018 consists of 47",
        "type": "Document"
      },
      {
        "id": "51bf1d53-4bc3-43f9-aac1-c9e376f8f602",
        "metadata": {
          "vector_store_key": "2001.08051-1",
          "chunk_id": 21,
          "document_id": "2001.08051",
          "start_idx": 8219,
          "end_idx": 8701
        },
        "page_content": " from both the written and spoken data collected so far in all the campaigns. Each written or spoken item received a total score by human experts, computed by summing up the scores related to 6 indicators in 2017/2018 (from 3 to 6 in the 2016 campaign, according to the proficiency levels and the type of test). Each indicator can assume a value 0, 1, 2, corresponding to bad, medium, good, respectively. The list of the indicators used by the experts to score written sentences and",
        "type": "Document"
      },
      {
        "id": "a7b45e6e-e49e-49b5-a7e8-60688ecfa151",
        "metadata": {
          "vector_store_key": "2001.08051-1",
          "chunk_id": 58,
          "document_id": "2001.08051",
          "start_idx": 22646,
          "end_idx": 23174
        },
        "page_content": " to divide the second indicator (pronunciation and fluency) into two different indicators, since fluent students might not necessarily have good pronunciation skills and vice versa, drawing for example on the IELTS Speaking band descriptors. Also, next campaigns might consider an additional indicator specifically addressed to score prosody (in particular intonation and rhythm), especially for A2 and B1 level test-takers. Considering the scope of the evaluation campaign, it is important to be aware of the limitations of the",
        "type": "Document"
      },
      {
        "id": "829ebc07-bfa7-4ea0-96c7-eb5ad9a84647",
        "metadata": {
          "vector_store_key": "2001.08051-3",
          "chunk_id": 57,
          "document_id": "2001.08051",
          "start_idx": 22350,
          "end_idx": 22869
        },
        "page_content": " or encouraging code-mixing and negative transfer phenomena. The elicitation techniques in a broader sense will be object of revision (see BIBREF34 and specifically on children speech BIBREF35) in order to maximise the quality of the corpus. As for proficiency indicators, one first step that could be taken in order to increase accuracy in the evaluation phase both for human and automatic scoring would be to divide the second indicator (pronunciation and fluency) into two different indicators, since fluent students",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What accuracy is achieved by the speech recognition system?",
    "llm_answer": "91.2% accuracy on utterances containing words, sentences, and conversations.  82.9% accuracy for a specific prediction task.  80% accuracy in speaker-closed setting and 60% in speaker-open setting (average).\n",
    "context": [
      {
        "id": "51a7992d-48ab-413d-af66-94de3136c637",
        "metadata": {
          "vector_store_key": "1601.02543-1",
          "chunk_id": 1,
          "document_id": "1601.02543",
          "start_idx": 478,
          "end_idx": 1101
        },
        "page_content": " of words for enabling a transaction. Before commercial deployment of a speech solution it is imperative to have a quantitative measure of the performance of the speech solution which is primarily based on the speech recognition accuracy of the speech engine used. Generally, the recognition performance of any speech recognition based solution is quantitatively evaluated by putting it to actual use by the people who are the intended users and then analyzing the logs to identify successful and unsuccessful transactions. This evaluation is then used to identifying any further improvement in the speech recognition based",
        "type": "Document"
      },
      {
        "id": "bf6e594b-aef4-47b7-8211-04b14e0b13df",
        "metadata": {
          "vector_store_key": "1907.00758-5",
          "chunk_id": 46,
          "document_id": "1907.00758",
          "start_idx": 17344,
          "end_idx": 17855
        },
        "page_content": " across different conditions. Table shows the model accuracy broken down by utterance type. The model achieves 91.2% accuracy on utterances containing words, sentences, and conversations, all of which exhibit natural variation in speech. The model is less successful with Articulatory utterances, which contain isolated phones occurring once or repeated (e.g., \u201csh sh sh\"). Such utterances contain subtle tongue movement, making it more challenging to correlate the visual signal with the audio. And indeed, the",
        "type": "Document"
      },
      {
        "id": "ee8e1ca8-42a9-496d-abe9-ac838f7302ce",
        "metadata": {
          "vector_store_key": "1611.00514-6",
          "chunk_id": 12,
          "document_id": "1611.00514",
          "start_idx": 4729,
          "end_idx": 5148
        },
        "page_content": " on an NVIDIA TITAN X GPU, using Kaldi software BIBREF12 . Evaluated on 50 hours of telephone speech data from the same database, our DNN-HMM SAD indicated a frame-level miss-classification (speech/non-speech) rate of 5.9% whereas an energy-based SAD did not perform better than 20%. For acoustic features we have experimented with different configurations of cepstral features. We have used 39-dimensional PLP features",
        "type": "Document"
      },
      {
        "id": "b198b932-1761-4d0f-9718-5ab4e539a320",
        "metadata": {
          "vector_store_key": "1907.00758-9",
          "chunk_id": 44,
          "document_id": "1907.00758",
          "start_idx": 16635,
          "end_idx": 17058
        },
        "page_content": " for each utterance. If the discrepancy falls within the minimum detectability range ( INLINEFORM0 125 INLINEFORM1 INLINEFORM2 INLINEFORM3 INLINEFORM4 45) then the prediction is correct. Random prediction (averaged over 1000 runs) yields 14.6% accuracy with a mean and standard deviation discrepancy of 328 INLINEFORM5 518ms. We achieve 82.9% accuracy with a mean and standard deviation discrepancy of 32 INLINEFORM6 223ms.",
        "type": "Document"
      },
      {
        "id": "ec88ae40-8aa5-45e5-8dc6-d62a20c024e2",
        "metadata": {
          "vector_store_key": "2002.06675-2",
          "chunk_id": 51,
          "document_id": "2002.06675",
          "start_idx": 17326,
          "end_idx": 17787
        },
        "page_content": " WSJ throughout the experiments. Table 4 shows the phone error rates (PERs) and word error rates (WERs) for the speaker-closed and speaker-open settings. The `average' is weighted by the numbers of tokens in the ground truth transcriptions for speaker-wise evaluation sets. The word recognition accuracy reached about 80% in the speaker-closed setting. In the speaker-open setting it was 60% on average and varied greatly from speaker to speaker (from 50% to 70",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the speech recognition system evaluated?",
    "llm_answer": "Generally, recognition performance is evaluated by putting the solution to actual use by intended users and analyzing logs of successful and unsuccessful transactions.  A method for evaluation without actual user testing was also adopted.\n",
    "context": [
      {
        "id": "51a7992d-48ab-413d-af66-94de3136c637",
        "metadata": {
          "vector_store_key": "1601.02543-1",
          "chunk_id": 1,
          "document_id": "1601.02543",
          "start_idx": 478,
          "end_idx": 1101
        },
        "page_content": " of words for enabling a transaction. Before commercial deployment of a speech solution it is imperative to have a quantitative measure of the performance of the speech solution which is primarily based on the speech recognition accuracy of the speech engine used. Generally, the recognition performance of any speech recognition based solution is quantitatively evaluated by putting it to actual use by the people who are the intended users and then analyzing the logs to identify successful and unsuccessful transactions. This evaluation is then used to identifying any further improvement in the speech recognition based",
        "type": "Document"
      },
      {
        "id": "67c7bdd9-8926-4b32-b3b7-4b1914e74192",
        "metadata": {
          "vector_store_key": "1601.02543-1",
          "chunk_id": 2,
          "document_id": "1601.02543",
          "start_idx": 845,
          "end_idx": 1419
        },
        "page_content": " successful and unsuccessful transactions. This evaluation is then used to identifying any further improvement in the speech recognition based solution to better the overall transaction completion rates. This process of evaluation is both time consuming and expensive. For evaluation one needs to identify a set of users and also identify the set of actual usage situations and perform the test. It is also important that the set of users are able to use the system with ease meaning that even in the test conditions the performance of the system, should be good, while this",
        "type": "Document"
      },
      {
        "id": "47ac061f-4a27-400a-b066-399161755275",
        "metadata": {
          "vector_store_key": "1611.00514-1",
          "chunk_id": 50,
          "document_id": "1611.00514",
          "start_idx": 18797,
          "end_idx": 19312
        },
        "page_content": ", since they only include matrix operations. The time required for our SAD is also reported which increases linearly with the duration of segment. We have presented the Intelligent Voice speaker recognition system used for the NIST 2016 speaker recognition evaluation. Our system is based on a score fusion of MFCC- and PLP-based i-vector/PLDA systems. We have described the main components of the system including, acoustic feature extraction, speech activity detection, i-vector extraction as front-end processing",
        "type": "Document"
      },
      {
        "id": "54254872-be02-42ba-a453-7bdf328c0030",
        "metadata": {
          "vector_store_key": "1601.02543-3",
          "chunk_id": 5,
          "document_id": "1601.02543",
          "start_idx": 2270,
          "end_idx": 2754
        },
        "page_content": " method was adopted to evaluate a speech recognition based solution as a case study. This is the main contribution of the paper. The rest of the paper is organized as follows. The method for evaluation without testing is described in Section SECREF2 . In Section SECREF3 we present a case study and conclude in Section SECREF4 . Fig. FIGREF1 shows the schematic of a typical menu based speech solution having 3 nodes. At each node there are a set of words that the user is expected to",
        "type": "Document"
      },
      {
        "id": "f26e9d66-643e-49c0-b961-d02f853035d7",
        "metadata": {
          "vector_store_key": "1601.02543-3",
          "chunk_id": 32,
          "document_id": "1601.02543",
          "start_idx": 12144,
          "end_idx": 12571
        },
        "page_content": " actually putting it to people test. There is a significant saving in terms of being able to identify recognition bottlenecks in a menu based speech solution through this analysis because it does not require actual people testing the system. This methodology was adopted to restructuring the set of active words at each node for better speech recognition in an actual menu based speech recognition system that caters to masses.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many of the utterances are transcribed?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "fdcc0ced-821b-4f8a-a4e0-9cd29d09ee20",
        "metadata": {
          "vector_store_key": "2001.08051-4",
          "chunk_id": 36,
          "document_id": "2001.08051",
          "start_idx": 13834,
          "end_idx": 14305
        },
        "page_content": " speech in a different language from the target language has to be reported by means of an explicit marker \u201cI am 10 years old @it(io ho gi\u00e0 risposto)\u201d. Next, we concatenated utterances to be transcribed into blocks of about 5 minutes each. We noticed that knowing the question and hearing several answers could be of great help for transcribing some poorly pronounced words or phrases. Therefore, each block contains only answers to the same question, explicitly reported",
        "type": "Document"
      },
      {
        "id": "e0cc4ab9-eb02-4d7b-a1bc-754cf2f089ff",
        "metadata": {
          "vector_store_key": "2001.08051-4",
          "chunk_id": 34,
          "document_id": "2001.08051",
          "start_idx": 13177,
          "end_idx": 13680
        },
        "page_content": " the annotation, which were used by 5 researchers to manually transcribe about 20 minutes of audio data. This experience led to a discussion, from which a second set of guidelines originated, aiming at reaching a reasonable trade-off between transcription accuracy and speed. As a consequence, we decided to apply the following transcription rules: only the main speaker has to be transcribed; presence of other voices (schoolmates, teacher) should be reported only with the label \u201c@voices\u201d, presence of",
        "type": "Document"
      },
      {
        "id": "f272c0a5-eec2-4e3b-ab41-defa8faa2506",
        "metadata": {
          "vector_store_key": "2003.06044-0",
          "chunk_id": 46,
          "document_id": "2003.06044",
          "start_idx": 16294,
          "end_idx": 16742
        },
        "page_content": " It covers various topics about our daily life. Each utterance in the conversation is manually labeled as one out of 4 dialogue act classes. Table TABREF18 presents the statistics for both datasets. In our preprocessing, the text was lowercased before tokenized, and then sentences were tokenized by WordPiece tokenizer BIBREF27 with a 30,000 token vocabulary to alleviate the Out-of-Vocabulary problem. [1]The author claimed that they achieved 78.",
        "type": "Document"
      },
      {
        "id": "d6c25153-f92d-4d27-a4e4-f2bfceafb867",
        "metadata": {
          "vector_store_key": "1808.08850-3",
          "chunk_id": 35,
          "document_id": "1808.08850",
          "start_idx": 13014,
          "end_idx": 13545
        },
        "page_content": " cover different topics like technology, human rights, terrorism and politics with a length variation between 2 and 10 minutes. To encourage the diversity of content format we included newscasts, interviews, reports and round tables. During the transcription phase we opted for a manual transcription process because we observed that using transcripts from an ASR system will difficult in a large degree the manual segmentation process. The number of words per transcript oscilate between 271 and 1,602 with a total number of 8,080",
        "type": "Document"
      },
      {
        "id": "78f1feab-ff37-45ec-96e5-66f4bf567c3a",
        "metadata": {
          "vector_store_key": "2001.08051-4",
          "chunk_id": 37,
          "document_id": "2001.08051",
          "start_idx": 14305,
          "end_idx": 14798
        },
        "page_content": " poorly pronounced words or phrases. Therefore, each block contains only answers to the same question, explicitly reported at the beginning of the block. We engaged about 30 students from two Italian linguistic high schools (namely \u201cC\u201d and \u201cS\u201d) to perform manual transcriptions. After a joint training session, we paired students together. Each pair first transcribed, individually, the same block of 5 minutes. Then, they went through a comparison phase, where each pair of students discussed",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many utterances are in the corpus?",
    "llm_answer": "1,242,998\n",
    "context": [
      {
        "id": "d7cfe2bf-feec-4faa-bfe2-cad5cf0452e6",
        "metadata": {
          "vector_store_key": "2001.08051-2",
          "chunk_id": 40,
          "document_id": "2001.08051",
          "start_idx": 15428,
          "end_idx": 15968
        },
        "page_content": " assigned either to training or evaluation sets, with proportions of $\\frac{2}{3}$ and $\\frac{1}{3}$, respectively; then training and evaluation lists were built, accordingly. Table reports statistics from the spoken data set. The id All identifies the whole data set, while Clean defines the subset in which sentences containing background voices, incomprehensible speech and word fragments were excluded. From the above description it appears that the corpus can be effectively used in many research directions. The spoken corpus features",
        "type": "Document"
      },
      {
        "id": "53cda549-1215-4c35-a4f5-9a45363de587",
        "metadata": {
          "vector_store_key": "1909.00279-0",
          "chunk_id": 37,
          "document_id": "1909.00279",
          "start_idx": 13718,
          "end_idx": 14228
        },
        "page_content": " models, please refer to supplementary materials. Training and Validation Sets We collected a corpus of poems and a corpus of vernacular literature from online resources. The poem corpus contains 163K quatrain poems from Tang Poems and Song Poems, the vernacular literature corpus contains 337K short paragraphs from 281 famous books, the corpus covers various literary forms including prose, fiction and essay. Note that our poem corpus and a vernacular corpus are not aligned. We further split the two corpor",
        "type": "Document"
      },
      {
        "id": "8e92ccf8-5454-4527-9ca9-4993ca3ab76a",
        "metadata": {
          "vector_store_key": "1808.09409-5",
          "chunk_id": 14,
          "document_id": "1808.09409",
          "start_idx": 5300,
          "end_idx": 5801
        },
        "page_content": "1) ruling out redundant content, (2) excluding sentences containing foreign words or Chinese phonetic alphabet by checking the Unicode values, (3) dropping overly simple sentences which may not be informative, and (4) utilizing a rule-based classifier to determine whether to include the sentence into the corpus. The final corpus consists of 717,241 learner sentences from writers of 61 different native languages, in which English and Japanese constitute the majority. As for completeness, 82.78% of",
        "type": "Document"
      },
      {
        "id": "fd033831-b148-43b3-a2cf-5018e2dda42c",
        "metadata": {
          "vector_store_key": "1911.01799-1",
          "chunk_id": 26,
          "document_id": "1911.01799",
          "start_idx": 10361,
          "end_idx": 10700
        },
        "page_content": " two parts: VoxCeleb1 and VoxCeleb2. We used SITW BIBREF21, a subset of VoxCeleb1 as the evaluation set. The rest of VoxCeleb1 was merged with VoxCeleb2 to form the training set (simply denoted by VoxCeleb). The training set involves $1,236,567$ utterances from $7,185$ speakers, and the evaluation set involves $6,445$ utterances from 299",
        "type": "Document"
      },
      {
        "id": "18d20b47-cceb-4c51-bd69-08a417b53481",
        "metadata": {
          "vector_store_key": "2002.06675-6",
          "chunk_id": 15,
          "document_id": "2002.06675",
          "start_idx": 5394,
          "end_idx": 5852
        },
        "page_content": " corpora of a reasonable size. Therefore, our first step was to build a speech corpus for ASR based on the data sets provided by the Ainu Museum and the Nibutani Ainu Culture Museum. In this section we explain the content of the data sets and how we modified it for our ASR corpus. The corpus we have prepared for ASR in this study is composed of text and speech. Table 1 shows the number of episodes and the total speech duration for each speaker. Among the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does their model outperform both the state-of-the-art systems?",
    "llm_answer": "The model outperforms the three baseline systems (Stanford, Multigraph, and MIR) on all evaluation metrics, achieving improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford system.  Improvements of 2.62% and 3.02% were obtained over ReDAN+BIBREF26 and MReaL\u2013BDAI, respectively, on the CoNLL F1 score.\n",
    "context": [
      {
        "id": "577474d8-b0fd-4808-8b7d-5f80c7a29369",
        "metadata": {
          "vector_store_key": "2001.06354-5",
          "chunk_id": 62,
          "document_id": "2001.06354",
          "start_idx": 21248,
          "end_idx": 21648
        },
        "page_content": " Specifically, our image-only model shows much higher NDCG score (60.16). On the other hand, our consensus dropout fusion model shows more balanced results over all metrics while still outperforming on most evaluation metrics (NDCG, MRR, R@1, and R@5). Compared to results of the Visual Dialog challenge 2019, our models also show strong results. Although ReDAN+ BIBREF26 and MReaL\u2013BDAI show higher N",
        "type": "Document"
      },
      {
        "id": "1e44ac68-a426-47a3-aad3-5a441fcefe6f",
        "metadata": {
          "vector_store_key": "1603.04553-1",
          "chunk_id": 38,
          "document_id": "1603.04553",
          "start_idx": 11106,
          "end_idx": 11513
        },
        "page_content": " the MIR model, we obtain significant improvements of 2.62% and 3.02% on CoNLL F1 score. To make a thorough empirical comparison with previous studies, Table 3 (below the dashed line) also shows the results of some state-of-the-art supervised coreference resolution systems \u2014 IMS: the second best system in the CoNLL 2012 shared task BIBREF28 ; Latent-Tree: the latent tree model BIBREF29 obtaining the best",
        "type": "Document"
      },
      {
        "id": "cbe1ae0c-a3e6-408d-b599-2f1e42c2205e",
        "metadata": {
          "vector_store_key": "1603.04553-1",
          "chunk_id": 36,
          "document_id": "1603.04553",
          "start_idx": 10414,
          "end_idx": 10861
        },
        "page_content": " baseline with two deterministic systems, namely Stanford: the Stanford system BIBREF10 and Multigraph: the unsupervised multigraph system BIBREF26 , and one unsupervised system, namely MIR: the unsupervised system using most informative relations BIBREF27 . Our model outperforms the three baseline systems on all the evaluation metrics. Specifically, our model achieves improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford system,",
        "type": "Document"
      },
      {
        "id": "dc50a5cf-b755-41a0-a173-1b561db0ed05",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "8af7818b-bcb2-4c40-98ec-b2c855e5bf3d",
        "metadata": {
          "vector_store_key": "1902.10525-0",
          "chunk_id": 88,
          "document_id": "1902.10525",
          "start_idx": 33593,
          "end_idx": 34116
        },
        "page_content": " ). The new architecture performs between 20%\u201340% (relative) better over almost all languages. To understand how the different datasets relate to each other, we performed a set of experiments and evaluations with the goal of better characterizing the differences between the datasets. We trained a recognizer on each of the three training sets separately, then evaluated each system on all three test sets (Table TABREF65 ). The neural network architecture is the same as the one we determined earlier (5 layers bidirection",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the state-of-the art?",
    "llm_answer": "The provided text mentions several state-of-the-art models, including those from BIBREF24 (with 66.7% accuracy) and various other referenced works.  It also states that the described method outperforms some of these.\n",
    "context": [
      {
        "id": "577fb06e-9340-4422-93f3-91bc89574aee",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 43,
          "document_id": "2001.00137",
          "start_idx": 16781,
          "end_idx": 17197
        },
        "page_content": " the online platforms. Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector",
        "type": "Document"
      },
      {
        "id": "8f559cb6-96ff-446d-aaf3-7673d4a1d24e",
        "metadata": {
          "vector_store_key": "1908.02402-7",
          "chunk_id": 101,
          "document_id": "1908.02402",
          "start_idx": 29500,
          "end_idx": 29733
        },
        "page_content": "-the-art models, while at the same time providing a more practical solution for real-world applications. We would like to thank Alexandros Papangelis, Janice Lam, Stefan Douglas Webb and SIGDIAL reviewers for their valuable comments.",
        "type": "Document"
      },
      {
        "id": "7ddbeceb-93d9-45f3-875f-c9f53efca5ae",
        "metadata": {
          "vector_store_key": "1909.02764-5",
          "chunk_id": 14,
          "document_id": "1909.02764",
          "start_idx": 5772,
          "end_idx": 6281
        },
        "page_content": "2004automatic compare local and global features in support vector machines. Next to such discriminative approaches, hidden Markov models are well-studied, however, there is no agreement on which feature-based classifier is most suitable BIBREF13. Similar to the facial expression modality, recent efforts on applying deep learning have been increased for acoustic speech processing. For instance, lee2015high use a recurrent neural network and palaz2015analysis apply a convolutional neural network to the raw",
        "type": "Document"
      },
      {
        "id": "1e8417f9-5dc9-4c82-a29d-65a0c8783427",
        "metadata": {
          "vector_store_key": "1808.09920-9",
          "chunk_id": 50,
          "document_id": "1808.09920",
          "start_idx": 16484,
          "end_idx": 16858
        },
        "page_content": " BIBREF3 and FastQA BIBREF6 . We also compare against Coref-GRU BIBREF12 , MHPGM BIBREF11 , and Weaver BIBREF10 . Additionally, we include results of MHQA-GRN BIBREF23 , from a recent arXiv preprint describing concurrent work. They jointly train graph neural networks and recurrent encoders. We report single runs of our two best single models and an ensemble one on the unm",
        "type": "Document"
      },
      {
        "id": "01045205-61e3-47b4-9e6f-acc85f88e67e",
        "metadata": {
          "vector_store_key": "1905.13497-3",
          "chunk_id": 28,
          "document_id": "1905.13497",
          "start_idx": 9658,
          "end_idx": 10040
        },
        "page_content": " 2016 competition (58.3%) by a large margin. Specifically, we achieve 68.3% accuracy, better than the more recently reported results from BIBREF24 (66.7%), who makes use of three KBs and a supervised deep network. On the harder task WSC-273, our method also outperforms the current state-of-the-art, as shown in Tab. 2. Namely, our method achieves an accuracy of 60.3%, nearly 3% of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was the performance on the self-collected corpus?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "c88b3332-924c-45fb-bfa8-e46765ff4c2a",
        "metadata": {
          "vector_store_key": "2001.08051-7",
          "chunk_id": 4,
          "document_id": "2001.08051",
          "start_idx": 1506,
          "end_idx": 2043
        },
        "page_content": " recorded during some of the above mentioned evaluation campaigns. We will release the written answers in future. Details and critical issues found during the acquisition of the answers of the test takers will be discussed in Section SECREF2. The tasks that can be addressed by using the corpus are very challenging and pose many problems, which have only partially been solved by the interested scientific community. From the ASR perspective, major difficulties are represented by: a) recognition of both child and non-native speech, i.",
        "type": "Document"
      },
      {
        "id": "06e9a4b3-9db0-4e73-892a-9b9a4c094d48",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 27,
          "document_id": "1909.00694",
          "start_idx": 8898,
          "end_idx": 9330
        },
        "page_content": " CO such that it was five times larger than AL. The results are shown in Table TABREF16. We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively: . \u4f5c\u696d\u304c\u697d\u3060\u3002 The work",
        "type": "Document"
      },
      {
        "id": "b21234ce-7aac-4c6f-a7c4-a6222abebd12",
        "metadata": {
          "vector_store_key": "1603.04553-1",
          "chunk_id": 34,
          "document_id": "1603.04553",
          "start_idx": 9763,
          "end_idx": 10169
        },
        "page_content": "oNotes corpus BIBREF21 . The corpora statistics are shown in Table 2 . Our system is evaluated with automatically extracted mentions on the version of the data with automatic preprocessing information (e.g., predicted parse trees). Evaluation Metrics. We evaluate our model on three measures widely used in the literature: MUC BIBREF22 , B $^{3}$ BIBREF23 , and Entity-based CEAF (CEAF $_e$ ) BIBREF24 . In",
        "type": "Document"
      },
      {
        "id": "2c1de9f3-1cd6-4cd9-8d12-f7fe403598f4",
        "metadata": {
          "vector_store_key": "1701.02877-7",
          "chunk_id": 21,
          "document_id": "1701.02877",
          "start_idx": 7791,
          "end_idx": 8297
        },
        "page_content": " studying NER performance on this genre gained momentum. So far, there have been no big evaluation efforts, such as ACE and OntoNotes, resulting in substantial amounts of gold standard data. Instead, benchmark corpora were created as part of smaller challenges or individual projects. The first such corpus is the UMBC corpus for Twitter NER BIBREF33 , where researchers used crowdsourcing to obtain annotations for persons, locations and organisations. A further Twitter NER corpus was created by BIBREF21",
        "type": "Document"
      },
      {
        "id": "d7e88a3a-9f2c-444d-a064-4fc4e2448cb3",
        "metadata": {
          "vector_store_key": "1701.02877-7",
          "chunk_id": 121,
          "document_id": "1701.02877",
          "start_idx": 43692,
          "end_idx": 44145
        },
        "page_content": " the CoNLL 2003 corpus. The seed domain for the experiments here is newswire, where we use the classifier trained on the biggest NW corpus investigated in this study, i.e. OntoNotes NW. That classifier is then applied to all other corpora. The rationale is to test how suitable such a big corpus would be for improving Twitter NER, for which only small training corpora are available. Results for out-of-domain performance are reported in Table 11 . The",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the size of their dataset?",
    "llm_answer": "17M training tokens, 44k vocabulary size.\n",
    "context": [
      {
        "id": "9270562a-8e53-4e1d-b760-74d8fd0bc872",
        "metadata": {
          "vector_store_key": "1905.06566-7",
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "0723454a-4062-463e-972a-81a58c6a504e",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "2e1622b2-1517-4239-8581-693f3aece329",
        "metadata": {
          "vector_store_key": "1711.02013-3",
          "chunk_id": 55,
          "document_id": "1711.02013",
          "start_idx": 20658,
          "end_idx": 21116
        },
        "page_content": "ings, between recurrent layers, and on recurrent states were (0.7, 0.5, 0.5) respectively. dataset contains 17M training tokens and has a vocabulary size of 44k words. The dataset is partitioned into a training set (first 99M characters) and a development set (last 1M characters) that is used to report performance. As this dataset contains various articles from Wikipedia, the longer term information (such as current topic) plays a bigger role than in the",
        "type": "Document"
      },
      {
        "id": "0d593f74-7474-41cc-9928-ce68ac96f714",
        "metadata": {
          "vector_store_key": "1802.05574-2",
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "eb1ce3aa-59c4-45e6-a3ea-77b0dadb95e7",
        "metadata": {
          "vector_store_key": "1906.03538-1",
          "chunk_id": 64,
          "document_id": "1906.03538",
          "start_idx": 25144,
          "end_idx": 25628
        },
        "page_content": " US Defense Advanced Research Projects Agency (DARPA). The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. We provide brief statistics on the sources of different content in our dataset in Table TABREF46 . In particular, this table shows: the size of the data collected from online debate websites (step 1). the size of the data filtered out (step 2a). the size of the perspectives added",
        "type": "Document"
      }
    ]
  },
  {
    "query": "how was the dataset built?",
    "llm_answer": "The methods for constructing datasets vary.  Some were filtered, trimmed, and balanced.  Others involved web scraping, conversion of PDFs to text, and pattern matching.  Still others were sourced from existing corpora.\n",
    "context": [
      {
        "id": "de28f623-e56b-46af-88a3-78e233bbcff1",
        "metadata": {
          "vector_store_key": "1607.06025-5",
          "chunk_id": 66,
          "document_id": "1607.06025",
          "start_idx": 24769,
          "end_idx": 25338
        },
        "page_content": " were constructed by filtering the generated datasets according to various thresholds. Thus, the generated datasets were constructed to contain enough examples, so that the filtered datasets had at least the number of examples as the original dataset. In the end, all the datasets were trimmed down to the size of the original dataset by selecting the samples sequentially from the beginning until the dataset had the right size. Also, the datasets were filtered so that each of the labels was represented equally. All the models, including classification and discrimin",
        "type": "Document"
      },
      {
        "id": "1c380a0c-1037-4628-8ada-168d9c8247bc",
        "metadata": {
          "vector_store_key": "1707.03904-9",
          "chunk_id": 13,
          "document_id": "1707.03904",
          "start_idx": 5304,
          "end_idx": 5850
        },
        "page_content": " subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short. We plan to release these annotations along with the datasets, and our retrieved documents for each question. Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated",
        "type": "Document"
      },
      {
        "id": "9a66176f-fe2e-4da0-a2df-70a3d739924f",
        "metadata": {
          "vector_store_key": "1802.05322-2",
          "chunk_id": 38,
          "document_id": "1802.05322",
          "start_idx": 13905,
          "end_idx": 14341
        },
        "page_content": " explanation of the whole process needed to conduct an experiment like this. The process includes downloading a data set, web scraping for extra information, data preprocessing, model tuning and evaluation of the results. Action Adult Adventure Animation Biography Comedy Crime Documentary Drama Family Fantasy Film-Noir Game-Show History Horror Music Musical Mystery Reality-TV Romance Sci-Fi Short Sport Talk-Show Thriller War Western",
        "type": "Document"
      },
      {
        "id": "0d593f74-7474-41cc-9928-ce68ac96f714",
        "metadata": {
          "vector_store_key": "1802.05574-2",
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "b639e48a-5435-44b1-ac12-85270ef88749",
        "metadata": {
          "vector_store_key": "1808.06834-4",
          "chunk_id": 17,
          "document_id": "1808.06834",
          "start_idx": 7220,
          "end_idx": 7729
        },
        "page_content": " each speaker raises an issue of which he is concerned of but no discussion is done on the issues. The creation of the dataset involved 3 steps. The first step was to scrap the pdf files from the Lok Sabha website. Each pdf file is a session. The second step was to convert the pdf files into text files for easy parsing. The challenge here was to convert this unstructured information into a structured format. The third step is to extract relevant data using pattern matching. We developed a software parser",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what processing was done on the speeches before being parsed?",
    "llm_answer": "Punctuation, XML characters, hyperlinks, Twitter-specific \"at-mentions\" and \"hashtags\" were removed.  Speeches were broken into blocks of about 5 minutes each, and concatenated utterances were transcribed.\n",
    "context": [
      {
        "id": "e0d0a0e3-1f37-4e59-8b8d-e91f494ceb57",
        "metadata": {
          "vector_store_key": "1808.06834-4",
          "chunk_id": 46,
          "document_id": "1808.06834",
          "start_idx": 17785,
          "end_idx": 18358
        },
        "page_content": " parser for the conversion of unstructured pdfs into structured format i.e into a relational database using mongo database software. We analyzed the purpose of the speeches of the member of parliament and categorized them into 4 major categories and provided statistics of the categories. We also tried to identify them automatically using fastText algorithm and provided the results. The analysis is done for understanding the purpose of the speeches in the parliament. We also presented our results on binary stance classification of the speeches whether the member is in",
        "type": "Document"
      },
      {
        "id": "46f1239e-aea6-4dce-9ad0-bed5f336cceb",
        "metadata": {
          "vector_store_key": "1904.00648-0",
          "chunk_id": 21,
          "document_id": "1904.00648",
          "start_idx": 8377,
          "end_idx": 8850
        },
        "page_content": " and the models considered. All the resources used are publicy available. We define a set of features for characterizing the text at the token level. We mix standard linguistic features, such as Part-Of-Speech (POS) and chunk tag, together with several gazetteers specifically built for classical music, and a series of features representing tokens' left and right context. For extracting the POS and the chunk tag we use the Python library twitter_nlp, presented in BIBREF",
        "type": "Document"
      },
      {
        "id": "168eab78-7a2b-4e78-991b-fb4bd4e1b842",
        "metadata": {
          "vector_store_key": "1708.04557-0",
          "chunk_id": 59,
          "document_id": "1708.04557",
          "start_idx": 24883,
          "end_idx": 25397
        },
        "page_content": " to estimate the policy positions of individual actors from these speeches. In relation to Irish political actors such estimation has been hindered by the structure of the available data. While all speeches made in D\u00e1il \u00c9ireann are dutifully recorded, the architecture of the data set, where digitized versions of speeches are stored, makes it impossible to apply any of the existing text analysis software. Speeches are currently stored by D\u00e1il \u00c9ireann in more than half a million separate HTML files with entries",
        "type": "Document"
      },
      {
        "id": "d322d4dc-4bf2-4c3e-9106-92b243c71e31",
        "metadata": {
          "vector_store_key": "1604.05781-6",
          "chunk_id": 11,
          "document_id": "1604.05781",
          "start_idx": 4432,
          "end_idx": 4838
        },
        "page_content": " BIBREF29 . A tweet will be referred to as a `document' for the rest of this work. All document text was processed the same way. Punctuation, XML characters, and hyperlinks were removed, as were Twitter-specific \u201cat-mentions\u201d and \u201chashtags\u201d (see also the Appendix). There is useful information here, but it is either not natural language text, or it is Twitter-specific, or both. Documents were broken into",
        "type": "Document"
      },
      {
        "id": "fdcc0ced-821b-4f8a-a4e0-9cd29d09ee20",
        "metadata": {
          "vector_store_key": "2001.08051-4",
          "chunk_id": 36,
          "document_id": "2001.08051",
          "start_idx": 13834,
          "end_idx": 14305
        },
        "page_content": " speech in a different language from the target language has to be reported by means of an explicit marker \u201cI am 10 years old @it(io ho gi\u00e0 risposto)\u201d. Next, we concatenated utterances to be transcribed into blocks of about 5 minutes each. We noticed that knowing the question and hearing several answers could be of great help for transcribing some poorly pronounced words or phrases. Therefore, each block contains only answers to the same question, explicitly reported",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance change of the textual semantic similarity task when no error and maximum errors (noise) are present?",
    "llm_answer": "The context states that as the percentage of error increases, there is a significant drop in performance for the textual semantic similarity task.  However, it does not give specific performance values for no error or maximum error conditions.\n",
    "context": [
      {
        "id": "3a3d6289-8e18-42ed-83c6-af1a9deb75cd",
        "metadata": {
          "vector_store_key": "1808.09409-2",
          "chunk_id": 43,
          "document_id": "1808.09409",
          "start_idx": 16355,
          "end_idx": 16884
        },
        "page_content": " this part. From the figure we can observe that, all the semantic roles achieve significant improvements in performances. To better understand the overall results, we further look deep into the output by addressing the questions: What types of error negatively impact both systems over learner texts? What types of error are more problematic for the neural syntax-agnostic one over the L2 data but can be solved by the syntax-based one to some extent? We first carry out a suite of empirical investigations by breaking down error",
        "type": "Document"
      },
      {
        "id": "358a285f-5599-429b-a86b-b048cbeac708",
        "metadata": {
          "vector_store_key": "1705.03151-4",
          "chunk_id": 120,
          "document_id": "1705.03151",
          "start_idx": 42556,
          "end_idx": 43057
        },
        "page_content": " (SNR=10). This can be observed more clearly in Fig. 6 , where the performance degradation rates compared to the noise-free condition are shown. The figure shows that when the noise increases, the performance degradation with the PTN system is less significant compared to the degradation with the i-vector system. As the Babel speech data is much more noisy than the AP16-OLR speech, this noise robustness with the PTN approach partly explains why the relative performance is inconsistent between the",
        "type": "Document"
      },
      {
        "id": "669decdf-6395-468a-9baf-2ab1b26d3f64",
        "metadata": {
          "vector_store_key": "1909.09270-3",
          "chunk_id": 69,
          "document_id": "1909.09270",
          "start_idx": 24769,
          "end_idx": 25253
        },
        "page_content": "43. The Noise Adaptation method outperforms the Raw annotations Cogcomp baseline in most cases, but does not reach the performance of the Self-training method, despite using some fully labeled data. It is instructive to compare the neural and non-neural versions of each setup. The neural method is better overall, but is less able to learn from the knowledge-based initialization weights. In the non-neural method, the difference between Raw and Combined is nearly 20 points, but the",
        "type": "Document"
      },
      {
        "id": "98d54adf-57ef-4ea7-9725-ee8bc8ed94b5",
        "metadata": {
          "vector_store_key": "2003.12932-2",
          "chunk_id": 21,
          "document_id": "2003.12932",
          "start_idx": 7589,
          "end_idx": 8028
        },
        "page_content": " score vs % of error for Sentiment analysis on SST-2 dataset after fine tuning for 10 and 50 epochs respectively. Figure FIGREF12stsa and FIGREF12stsb shows Pearson-Spearman correlation vs % of error for textual semantic similarity on STS-B dataset after fine tuning for 10 and 50 epochs respectively. It is clear from the above plots that as we increase the percentage of error, for each of the three tasks, we see a significant drop in B",
        "type": "Document"
      },
      {
        "id": "9d1f63a5-d02d-4218-8048-b2e7e133f2c2",
        "metadata": {
          "vector_store_key": "2003.12932-0",
          "chunk_id": 6,
          "document_id": "2003.12932",
          "start_idx": 2146,
          "end_idx": 2647
        },
        "page_content": " evaluate the same. This evaluation is motivated from the business use case we are solving where we are building a dialogue system to screen candidates for blue collar jobs. Our candidate user base, coming from underprivileged backgrounds, are often high school graduates. This coupled with \u2018fat finger\u2019 problem over a mobile keypad leads to a lot of typos and spelling mistakes in the responses sent to the dialogue system. Hence, for this work we focus on spelling mistakes as the noise in the data.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which sentiment analysis data set has a larger performance drop when a 10% error is introduced?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "98d54adf-57ef-4ea7-9725-ee8bc8ed94b5",
        "metadata": {
          "vector_store_key": "2003.12932-2",
          "chunk_id": 21,
          "document_id": "2003.12932",
          "start_idx": 7589,
          "end_idx": 8028
        },
        "page_content": " score vs % of error for Sentiment analysis on SST-2 dataset after fine tuning for 10 and 50 epochs respectively. Figure FIGREF12stsa and FIGREF12stsb shows Pearson-Spearman correlation vs % of error for textual semantic similarity on STS-B dataset after fine tuning for 10 and 50 epochs respectively. It is clear from the above plots that as we increase the percentage of error, for each of the three tasks, we see a significant drop in B",
        "type": "Document"
      },
      {
        "id": "995430fe-8c15-4a42-8065-3518945ada8d",
        "metadata": {
          "vector_store_key": "2003.12932-2",
          "chunk_id": 20,
          "document_id": "2003.12932",
          "start_idx": 7196,
          "end_idx": 7589
        },
        "page_content": " the sentiment analysis task. We plot F1 score vs % of error, as shown in Figure FIGREF6. Figure FIGREF6imdba shows the performance after fine tuning for 10 epochs, while Figure FIGREF6imdbb shows the performance after fine tuning for 50 epochs. Similarly, Figure FIGREF9ssta and Figure FIGREF9sstb) shows F1 score vs % of error for Sentiment analysis on SST-2 dataset after fine tuning for 10",
        "type": "Document"
      },
      {
        "id": "9b80eacc-ab22-4d6f-94df-68ed1be685d7",
        "metadata": {
          "vector_store_key": "1904.07342-3",
          "chunk_id": 23,
          "document_id": "1904.07342",
          "start_idx": 8927,
          "end_idx": 9438
        },
        "page_content": "-related data, since the low number of geo-tagged samples would produce an underpowered study. In Figure FIGREF8 , we see that overall sentiment averages rarely show movement post-event: that is, only Hurricane Florence shows a significant difference in average tweet sentiment pre- and post-event at the 1% level, corresponding to a 0.12 point decrease in positive climate change sentiment. However, controlling for the same group of users tells a different story: both Hurricane Florence and Hurricane Michael",
        "type": "Document"
      },
      {
        "id": "071cf9d4-4f10-4550-a4a4-cb8bbdc9f3de",
        "metadata": {
          "vector_store_key": "2001.00137-3",
          "chunk_id": 49,
          "document_id": "2001.00137",
          "start_idx": 18686,
          "end_idx": 19161
        },
        "page_content": " Chatbot Intent Classification Corpus). Experimental results for the Twitter Sentiment Classification task on Kaggle's Sentiment140 Corpus dataset, displayed in Table TABREF37, show that our model has better F1-micros scores, outperforming the baseline models by 6$\\%$ to 8$\\%$. We evaluate our model and baseline models on three versions of the dataset. The first one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80",
        "type": "Document"
      },
      {
        "id": "021aa65a-3d0f-4917-acd9-fb87a95610fb",
        "metadata": {
          "vector_store_key": "1803.07771-4",
          "chunk_id": 2,
          "document_id": "1803.07771",
          "start_idx": 875,
          "end_idx": 1471
        },
        "page_content": " improved. The construction of a high-accuracy sentiment classification model usually entails the challenging compilation of training sets with numerous samples and sufficiently accurate labels. The reason behind this difficulty is two-fold. First, sentiment is somewhat subjective, and a sample may receive different labels from different users. Second, some texts contain complex sentiment representations, and a single label is difficult to provide. We conduct a statistical analysis of public Chinese sentiment text sets in GitHub. The results show that the average label error is larger than",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much is pre-training loss increased in Low/Medium/Hard level of pruning?",
    "llm_answer": "Low levels (30-40%): no increase.\nMedium levels: increases.\nHard levels (past 40%): increases.\n",
    "context": [
      {
        "id": "d82d9326-8b7b-40cf-941b-9c6b9128ec95",
        "metadata": {
          "vector_store_key": "2002.08307-2",
          "chunk_id": 34,
          "document_id": "2002.08307",
          "start_idx": 12593,
          "end_idx": 13063
        },
        "page_content": "training and pushes the rest of the weights to 0. We can then prune those weights without affecting the output of the network. Past 40% pruning, performance starts to degrade. Pre-training loss increases as we prune weights necessary for fitting the pre-training data (Table TABREF27). Feature activations of the hidden layers start to diverge from models with low levels of pruning (Figure FIGREF18). Downstream accuracy also begins to degrade at this point. We believe",
        "type": "Document"
      },
      {
        "id": "60f1f23e-6117-4229-96f1-a239a8ab5b69",
        "metadata": {
          "vector_store_key": "2002.08307-2",
          "chunk_id": 32,
          "document_id": "2002.08307",
          "start_idx": 11904,
          "end_idx": 12310
        },
        "page_content": "2, 3, 4, 5] \\times 10^{-5}$ and show the results with the best development accuracy in Figure FIGREF15 / Table TABREF27. We also measure the difference in which weights are selected for pruning during pre-training vs. downstream fine-tuning and plot the results in Figure FIGREF25. Figure FIGREF15 shows that the first 30-40% of weights pruned by magnitude weight pruning do not impact pre-training loss or",
        "type": "Document"
      },
      {
        "id": "50e53cc6-2f8a-4327-9531-35ddaa55cd29",
        "metadata": {
          "vector_store_key": "2002.08307-9",
          "chunk_id": 36,
          "document_id": "2002.08307",
          "start_idx": 13363,
          "end_idx": 13881
        },
        "page_content": " network without trying many at once. Why does pruning at these levels hurt downstream performance? On one hand, pruning deletes pre-training information by setting weights to 0, preventing the transfer of the useful inductive biases learned during pre-training. On the other hand, pruning regularizes the model by keeping certain weights at zero, which might prevent fitting downstream datasets. Figure FIGREF15 and Table TABREF27 show information deletion is the main cause of performance degradation between 40 - 60",
        "type": "Document"
      },
      {
        "id": "8cf6cca5-7527-4918-9b3e-3f1d847bbc72",
        "metadata": {
          "vector_store_key": "2002.08307-2",
          "chunk_id": 6,
          "document_id": "2002.08307",
          "start_idx": 2226,
          "end_idx": 2714
        },
        "page_content": " evaluation. We chose magnitude weight pruning, which compresses models by removing weights close to 0, because it is one of the most fine-grained and effective compression methods and because there are many interesting ways to view pruning, which we explore in the next section. Our findings are as follows: Low levels of pruning (30-40%) do not increase pre-training loss or affect transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent",
        "type": "Document"
      },
      {
        "id": "25bf0803-25f9-4a00-a7a1-40fe3bc981c1",
        "metadata": {
          "vector_store_key": "2002.08307-9",
          "chunk_id": 35,
          "document_id": "2002.08307",
          "start_idx": 13063,
          "end_idx": 13562
        },
        "page_content": "uning (Figure FIGREF18). Downstream accuracy also begins to degrade at this point. We believe this observation may point towards a more principled stopping criterion for pruning. Currently, the only way to know how much to prune is by trial and (dev-set) error. Predictors of performance degradation while pruning might help us decide which level of sparsity is appropriate for a given trained network without trying many at once. Why does pruning at these levels hurt downstream performance? On one",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the average length of the recordings?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "76b27449-6e8c-48aa-95ec-c484155aad5a",
        "metadata": {
          "vector_store_key": "1705.03151-0",
          "chunk_id": 71,
          "document_id": "1705.03151",
          "start_idx": 25730,
          "end_idx": 26199
        },
        "page_content": " phones, with a sampling rate of 16kHz and a sample size of 16 bits. Each dataset was split into a training set consisting of 18 speakers, and a test set consisting of 6 speakers. For Mandarin, Cantonese, Vietnamese and Indonesian, the recording was conducted in a quiet environment. For Russian, Korean and Japanese, there are 2 recording conditions for each speaker, quiet and noisy. The average duration (including silence) of all the $12,939$ test utterances of the",
        "type": "Document"
      },
      {
        "id": "6bb58866-d827-4810-9706-e5dea8e32f98",
        "metadata": {
          "vector_store_key": "1909.06522-1",
          "chunk_id": 24,
          "document_id": "1909.06522",
          "start_idx": 8980,
          "end_idx": 9471
        },
        "page_content": "ated 10-second sound clips from YouTube videos BIBREF28. Note that in our video datasets, video lengths vary between 10 seconds and 5 minutes, with an average duration of about 2 minutes. Rather than constantly repeating the 10-second sound clip to match the original minute-long audio, we superpose each sound clip on the short utterances via audio segmentation. Specifically, we first use an initial bootstrap model to align each original long audio, and segment each audio into around 10-",
        "type": "Document"
      },
      {
        "id": "610012c5-068f-4461-9da1-ecbc2ff26944",
        "metadata": {
          "vector_store_key": "1907.05664-0",
          "chunk_id": 10,
          "document_id": "1907.05664",
          "start_idx": 3794,
          "end_idx": 4271
        },
        "page_content": " articles coupled with summaries of about three sentences. These summaries are in fact \u201chighlights\" of the articles provided by the media themselves. Articles have an average length of 780 words and the summaries of 50 words. We had 287 000 training pairs and 11 500 test pairs. Similarly to See et al. See2017, we limit during training and prediction the input text to 400 words and generate summaries of 200 words. We pad the shorter texts using an UNKNOWN token and truncate",
        "type": "Document"
      },
      {
        "id": "1523b4ed-4fb2-4b9e-8647-9a7bc686f31b",
        "metadata": {
          "vector_store_key": "2002.11268-4",
          "chunk_id": 37,
          "document_id": "2002.11268",
          "start_idx": 10875,
          "end_idx": 11324
        },
        "page_content": "K, 1M and 21M utterance-level {audio, transcript} pairs taken from anonymized, transcribed Voice Search data. These fine-tuning sets roughly correspond to 10 hours, 100 hours, 1000 hours and 21,000 hours of audio, respectively. The following data sources were used to choose scaling factors and/or evaluate the final model performance. Source-domain Eval Set (YouTube). The in-domain performance of the YouTube-trained RNN-T baseline was measured on",
        "type": "Document"
      },
      {
        "id": "0bf93796-357e-4af1-bd94-d88d9ef3745e",
        "metadata": {
          "vector_store_key": "2001.08051-6",
          "chunk_id": 31,
          "document_id": "2001.08051",
          "start_idx": 11902,
          "end_idx": 12405
        },
        "page_content": " mates or teachers often overlaps with the speech of the student speaking in her/his microphone. Also, the type of microphone depends on the equipment of the school. On average, the audio signal quality is nearly good, while the main problem is caused by a high percentage of extraneous speech. This is due to the fact that organisers decided to use a fixed duration - which depends on the question - for recording spoken utterances, so that all the recordings for a given question have the same length.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What were their results?",
    "llm_answer": "The results showed good positions in three international contests (TASS'15, SemEval'15-16, SENTIPOLC'14) and excellent results in five other languages.  Results were also up to 0.85 BLEU points better than the baseline on newstest2019, and surpassed the baseline by 0.18 points on newstest2017 (statistically significant) and 0.15 points on newstest2018 (no statistical significance).  Subjective and objective task success were similar, with dialogues resolving emergencies rated higher.  The document system obtained good results in English-French translation.\n",
    "context": [
      {
        "id": "d1c89bac-6a19-4b4e-8af7-215ed859ba92",
        "metadata": {
          "vector_store_key": "1803.02839-4",
          "chunk_id": 118,
          "document_id": "1803.02839",
          "start_idx": 45558,
          "end_idx": 45772
        },
        "page_content": " mathematical consistency of the experiments, and provided useful insights into the results. Mark prompted the investigation into potential emergent monoid structures since they appear frequently in state machines.",
        "type": "Document"
      },
      {
        "id": "b3eb1308-3e01-4903-be04-528f4258d508",
        "metadata": {
          "vector_store_key": "2002.11268-7",
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "7fed6b92-48c7-48e6-8ad9-2412ea5449bc",
        "metadata": {
          "vector_store_key": "1612.05270-0",
          "chunk_id": 4,
          "document_id": "1612.05270",
          "start_idx": 1650,
          "end_idx": 2169
        },
        "page_content": " tested in eight different languages: Spanish, English, Italian, Arabic, German, Portuguese, Russian and Swedish. We compare our approach ranking in three international contests: TASS'15, SemEval'15-16 and SENTIPOLC'14, for Spanish, English and Italian respectively; the remaining languages are compared directly with the results reported in the literature. The experimental results locate our approach in good positions for all considered competitions; and excellent results in the other five languages tested. Finally",
        "type": "Document"
      },
      {
        "id": "20198851-df5d-479e-87c6-a686a75ee069",
        "metadata": {
          "vector_store_key": "2003.05995-7",
          "chunk_id": 59,
          "document_id": "2003.05995",
          "start_idx": 23878,
          "end_idx": 24329
        },
        "page_content": " to the single lab-based WoZ study BIBREF4. Table TABREF33 gives the results from the post-task survey. We observe, that subjective and objective task success are similar in that the dialogues that resolved the emergency were rated consistently higher than the rest. Mann-Whitney-U one-tailed tests show that the scores of the Emergency Resolved Dialogues for Q1 and Q2 were significantly higher than the scores of the Emergency Not Resolved Dialogues",
        "type": "Document"
      },
      {
        "id": "18a1dacf-15c7-46e2-bc4d-ae61cde4f9e1",
        "metadata": {
          "vector_store_key": "1910.07481-4",
          "chunk_id": 30,
          "document_id": "1910.07481",
          "start_idx": 11703,
          "end_idx": 12134
        },
        "page_content": " best results, up to 0.85 BLEU points over the Baseline on the newstest2019 corpus. It also surpassed the Baselinee by 0.18 points on the newstest2017 with strong statistical significance, and by 0.15 BLEU points on the newstest2018 but this time with no statistical evidence. These encouraging results prompted us to extend experiments to another language pair: English-French. En$\\rightarrow $Fr: The Document system obtained the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the prediction accuracy of the model?",
    "llm_answer": "The model achieves 69.9% accuracy on training data, 64.7% on validation, and 65.3% on test data for binary classification.  It achieves 82.9% accuracy for synchronization offset prediction.  Other accuracies are also given for age, gender, dialect, and joint accuracy.\n",
    "context": [
      {
        "id": "87f63eab-3f9b-4e47-9a9a-d1da052f4660",
        "metadata": {
          "vector_store_key": "1907.00758-2",
          "chunk_id": 40,
          "document_id": "1907.00758",
          "start_idx": 15137,
          "end_idx": 15586
        },
        "page_content": " plateaus for 2 epochs. Upon convergence, the model achieves 0.193 training loss, 0.215 validation loss, and 0.213 test loss. By placing a threshold of 0.5 on predicted distances, the model achieves 69.9% binary classification accuracy on training samples, 64.7% on validation samples, and 65.3% on test samples. Synchronisation offset prediction: Section SECREF3 described briefly how to use our model to predict the synchronisation offset for test",
        "type": "Document"
      },
      {
        "id": "b198b932-1761-4d0f-9718-5ab4e539a320",
        "metadata": {
          "vector_store_key": "1907.00758-9",
          "chunk_id": 44,
          "document_id": "1907.00758",
          "start_idx": 16635,
          "end_idx": 17058
        },
        "page_content": " for each utterance. If the discrepancy falls within the minimum detectability range ( INLINEFORM0 125 INLINEFORM1 INLINEFORM2 INLINEFORM3 INLINEFORM4 45) then the prediction is correct. Random prediction (averaged over 1000 runs) yields 14.6% accuracy with a mean and standard deviation discrepancy of 328 INLINEFORM5 518ms. We achieve 82.9% accuracy with a mean and standard deviation discrepancy of 32 INLINEFORM6 223ms.",
        "type": "Document"
      },
      {
        "id": "881df877-2175-4d8a-ba49-6c5a5a3119c0",
        "metadata": {
          "vector_store_key": "1808.09920-2",
          "chunk_id": 72,
          "document_id": "1808.09920",
          "start_idx": 24285,
          "end_idx": 24813
        },
        "page_content": " candidate answers or the number of nodes increases. In this section we provide an error analysis for our best single model predictions. First of all, we look at which type of questions our model performs well or poorly. There are more than 150 query types in the validation set but we filtered the three with the best and with the worst accuracy that have at least 50 supporting documents and at least 5 candidates. We show results in Table 4 . We observe that questions regarding places (birth and death) are considered harder",
        "type": "Document"
      },
      {
        "id": "c0d4cfe3-437d-45df-88d6-d6f1a92b53f0",
        "metadata": {
          "vector_store_key": "1909.04181-4",
          "chunk_id": 23,
          "document_id": "1909.04181",
          "start_idx": 8006,
          "end_idx": 8418
        },
        "page_content": ", and (3) predictions from our user-level BERT model. These majority class models (i.e., our third submission) achieve best results on the official test data. We acquire 54.72% accuracy for age, 81.67% accuracy for gender, 93.75% accuracy for dialect, and 40.97% joint accuracy. In this work, we described our submitted models to the Arabic author profiling and deception detection shared task (APDA) BIBREF0. We",
        "type": "Document"
      },
      {
        "id": "d8c5e018-45c0-472f-9292-464a81880ebf",
        "metadata": {
          "vector_store_key": "1802.05322-1",
          "chunk_id": 14,
          "document_id": "1802.05322",
          "start_idx": 5026,
          "end_idx": 5479
        },
        "page_content": "16 . Accuracy is a measurement of how correct a model's predictions are and is defined as DISPLAYFORM0  . Precision is a ratio of how often positive predictions actually are positve and is defined as DISPLAYFORM0  . Recall is a measurement of how good the model is to find all true positives and is defined as DISPLAYFORM0  . BIBREF5  It has been shown that when calculating precision and recall on multi-label classifiers, it can be advantageous to use",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the dataset used in the paper?",
    "llm_answer": "Biomedical, SCI, novel-9, CNNDM, NYT50\n",
    "context": [
      {
        "id": "0d593f74-7474-41cc-9928-ce68ac96f714",
        "metadata": {
          "vector_store_key": "1802.05574-2",
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "c5eaf502-a11a-40bd-84a8-78f62e8d8d02",
        "metadata": {
          "vector_store_key": "1701.00185-9",
          "chunk_id": 10,
          "document_id": "1701.00185",
          "start_idx": 3790,
          "end_idx": 4304
        },
        "page_content": " this paper we use a new short text dataset, Biomedical, in the experiment to verify the effectiveness of our approach. Third, we put much effort on studying the influence of various different semantic features integrated in our self-taught CNN framework, which is not involved in the conference paper. For the purpose of reproducibility, we make the datasets and software used in our experiments publicly available at the website. The remainder of this paper is organized as follows: In Section SECREF2 , we first",
        "type": "Document"
      },
      {
        "id": "189f2e62-5589-4ac4-a44d-70623d5e2128",
        "metadata": {
          "vector_store_key": "1709.02271-8",
          "chunk_id": 31,
          "document_id": "1709.02271",
          "start_idx": 11255,
          "end_idx": 11734
        },
        "page_content": " We begin by introducing the datasets (Section SECREF15 ), followed by detailing the featurization methods (Section SECREF17 ), the experiments (Section SECREF22 ), and finally reporting results (Section SECREF26 ). The statistics for the three datasets used in the experiments are summarized in Table TABREF16 . novel-9. This dataset was compiled by F&H14: a collection of 19 novels by 9 nineteenth century British and American authors in the Project Gutenberg. To compare to F&",
        "type": "Document"
      },
      {
        "id": "c3435ddc-e5cd-4c15-88a6-79bf7cb93888",
        "metadata": {
          "vector_store_key": "1911.03562-5",
          "chunk_id": 4,
          "document_id": "1911.03562",
          "start_idx": 1524,
          "end_idx": 2018
        },
        "page_content": " are based on information about the papers taken directly from AA (as of June 2019) and citation information extracted from Google Scholar (as of June 2019). Thus, all subsequent papers and citations are not included in the analysis. A fresh data collection is planned for January 2020.  Interactive Visualizations: The visualizations we are developing for this work (using Tableau) are interactive\u2014so one can hover, click to select and filter, move sliders, etc. Since this work is high in the",
        "type": "Document"
      },
      {
        "id": "9270562a-8e53-4e1d-b760-74d8fd0bc872",
        "metadata": {
          "vector_store_key": "1905.06566-7",
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does the SCAN dataset evaluate compositional generalization?",
    "llm_answer": "The SCAN dataset tests compositional generalization by evaluating the ability to generalize known primitive verbs to valid unseen constructions, and by using known templates.  It includes splits with multiple examples of the held-out primitive verb.\n",
    "context": [
      {
        "id": "1a7a689f-08cb-46e4-b7af-8493e1d76659",
        "metadata": {
          "vector_store_key": "1904.09708-1",
          "chunk_id": 96,
          "document_id": "1904.09708",
          "start_idx": 35365,
          "end_idx": 35870
        },
        "page_content": " 8 and Table 4 ). Compositional generalization performance is already quite high at 1 example, and at 2 examples is almost perfect (99.997% correct). The compositional generalization splits of the SCAN dataset were originally designed to test for the ability to generalize known primitive verbs to valid unseen constructions BIBREF2 . Further work with SCAN augmented this set of tests to include compositional generalization based not on known verbs but on known templates BIBREF3 . These template splits",
        "type": "Document"
      },
      {
        "id": "39abe95c-775c-4b73-a8d1-9b0498d8bd79",
        "metadata": {
          "vector_store_key": "1904.09708-1",
          "chunk_id": 47,
          "document_id": "1904.09708",
          "start_idx": 16275,
          "end_idx": 16769
        },
        "page_content": ". These results were highly skewed, with a mean accuracy of 78.4 % but a median of 91.0 % (see supplementary materials for detailed results). Overall, this represents an improvement over the best previously reported results on this task BIBREF4 , BIBREF15 , and does so without any hand-engineered features or additional supervision. To test our hypothesis that compositional generalization requires a separation between syntax (i.e. sequential information used for alignment), and semantics (i",
        "type": "Document"
      },
      {
        "id": "c0775cc4-03e9-40bb-b090-b64251bcdd86",
        "metadata": {
          "vector_store_key": "1906.00180-0",
          "chunk_id": 4,
          "document_id": "1906.00180",
          "start_idx": 1632,
          "end_idx": 2202
        },
        "page_content": " tests for compositional generalization that address the concerns of deep learning skeptics. The paper makes three main contributions. First, we develop a protocol for automatically generating data that can be used in entailment recognition tasks. Second, we demonstrate that several deep learning architectures succeed at one such task. Third, we present and apply a number of experiments to test whether models are capable of compositional generalization. The data generation process is inspired by BIBREF13 : an artificial language is defined, sentences are generated",
        "type": "Document"
      },
      {
        "id": "0cea283b-5e67-4579-b09e-973c3f4de127",
        "metadata": {
          "vector_store_key": "1904.09708-1",
          "chunk_id": 92,
          "document_id": "1904.09708",
          "start_idx": 33775,
          "end_idx": 34287
        },
        "page_content": " did not substantially degrade compositional generalization performance, suggesting that the success of the Syntactic Attention model does not depend on the parameterization of the semantic stream with a simple linear function. The original SCAN dataset was published with compositional generalization splits that have more than one example of the held-out primitive verb BIBREF2 . The training sets in these splits of the dataset include 1, 2, 4, 8, 16, or 32 random samples of command sequences with the \"jump\"",
        "type": "Document"
      },
      {
        "id": "2a260a62-972c-4ecc-bdf1-a978c8cb287c",
        "metadata": {
          "vector_store_key": "1904.09708-1",
          "chunk_id": 1,
          "document_id": "1904.09708",
          "start_idx": 528,
          "end_idx": 970
        },
        "page_content": " when tested on unseen combinations of known elements, state-of-the-art models fail to generalize BIBREF2 , BIBREF3 , BIBREF4 . It has been suggested that this failure represents a major deficiency of current deep learning models, especially when they are compared to human learners BIBREF5 , BIBREF0 . A recently published dataset called SCAN BIBREF2 (Simplified version of the CommAI Navigation tasks), tests compositional generalization in",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much does this system outperform prior work?",
    "llm_answer": "The system outperforms the three baseline systems by 2.93% and 3.01% on CoNLL F1 score.\n",
    "context": [
      {
        "id": "cbe1ae0c-a3e6-408d-b599-2f1e42c2205e",
        "metadata": {
          "vector_store_key": "1603.04553-1",
          "chunk_id": 36,
          "document_id": "1603.04553",
          "start_idx": 10414,
          "end_idx": 10861
        },
        "page_content": " baseline with two deterministic systems, namely Stanford: the Stanford system BIBREF10 and Multigraph: the unsupervised multigraph system BIBREF26 , and one unsupervised system, namely MIR: the unsupervised system using most informative relations BIBREF27 . Our model outperforms the three baseline systems on all the evaluation metrics. Specifically, our model achieves improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford system,",
        "type": "Document"
      },
      {
        "id": "418a99a9-99e7-4ef6-ab9f-a041eff21502",
        "metadata": {
          "vector_store_key": "2001.07263-5",
          "chunk_id": 45,
          "document_id": "2001.07263",
          "start_idx": 17117,
          "end_idx": 17529
        },
        "page_content": "isher data. This model consists of 10 encoder layers, and is trained for only 50 epochs. Our overall results on the Hub5'00 and other evaluation sets are summarized in Table TABREF14. The results in Fig. FIGREF12 and Table TABREF14 show that adding more training data greatly improves the system, by around 30% relative in some cases. For comparison with others, the 2000-hour system reaches 8.7% and 7.4% WER on",
        "type": "Document"
      },
      {
        "id": "11d9b2e6-e20d-4ab9-8d7c-c35a7e2d27e6",
        "metadata": {
          "vector_store_key": "1603.07044-6",
          "chunk_id": 43,
          "document_id": "1603.07044",
          "start_idx": 14997,
          "end_idx": 15515
        },
        "page_content": " task A already had a substantial amount of training data. Table 4 gives the final comparison between different models (we only list the MAP score because it is the official score for the challenge). Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited",
        "type": "Document"
      },
      {
        "id": "555a266e-0a08-43e5-b719-8cd2fd10ebb8",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "cc119115-77ce-4722-9013-f7e4e47fd7c6",
        "metadata": {
          "vector_store_key": "2001.07263-5",
          "chunk_id": 46,
          "document_id": "2001.07263",
          "start_idx": 17384,
          "end_idx": 17775
        },
        "page_content": " others, the 2000-hour system reaches 8.7% and 7.4% WER on rt02 and rt04. We observe that the regularization techniques, which are extremely important on the 300h setup, are still beneficial but have a significantly smaller effect.  For comparison with results in the literature we refer to the Switchboard-300 results in BIBREF3, BIBREF7, BIBREF51, BIBREF52 and the Switchboard-2000 results",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the baseline systems that are compared against?",
    "llm_answer": "Baseline 1, Baseline 2, Baseline 3, Stanford, Multigraph, MIR, Tactron 2\n",
    "context": [
      {
        "id": "17d3256d-56f6-4b9d-983d-669c916f7fae",
        "metadata": {
          "vector_store_key": "1709.08299-2",
          "chunk_id": 23,
          "document_id": "1709.08299",
          "start_idx": 9435,
          "end_idx": 10003
        },
        "page_content": " questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models. The",
        "type": "Document"
      },
      {
        "id": "593127ef-0325-4226-be3e-0ed371ce653f",
        "metadata": {
          "vector_store_key": "1804.08782-7",
          "chunk_id": 37,
          "document_id": "1804.08782",
          "start_idx": 13990,
          "end_idx": 14427
        },
        "page_content": " number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 .  For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose",
        "type": "Document"
      },
      {
        "id": "cbe1ae0c-a3e6-408d-b599-2f1e42c2205e",
        "metadata": {
          "vector_store_key": "1603.04553-1",
          "chunk_id": 36,
          "document_id": "1603.04553",
          "start_idx": 10414,
          "end_idx": 10861
        },
        "page_content": " baseline with two deterministic systems, namely Stanford: the Stanford system BIBREF10 and Multigraph: the unsupervised multigraph system BIBREF26 , and one unsupervised system, namely MIR: the unsupervised system using most informative relations BIBREF27 . Our model outperforms the three baseline systems on all the evaluation metrics. Specifically, our model achieves improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford system,",
        "type": "Document"
      },
      {
        "id": "94cba751-c344-4fbb-b71b-76356d3b479e",
        "metadata": {
          "vector_store_key": "1903.07398-4",
          "chunk_id": 31,
          "document_id": "1903.07398",
          "start_idx": 11099,
          "end_idx": 11528
        },
        "page_content": " of Tactron 2 (https://github.com/NVIDIA/tacotron2) as baseline comparison. Note this open-source version is trained for much longer (around 1000 epochs) however due to our limited compute we only trained our model up to 300 epochs We decide to evaluate our model against previous baselines on two fronts, Mean Opnion Score (MOS) and training speed. Typical TTS system evaluation is done with mean opinion score (MOS). To compute",
        "type": "Document"
      },
      {
        "id": "c8c4272e-a38c-48c3-a271-2ada66b9c8b5",
        "metadata": {
          "vector_store_key": "2004.01980-1",
          "chunk_id": 63,
          "document_id": "2004.01980",
          "start_idx": 22218,
          "end_idx": 22603
        },
        "page_content": " all baselines. We use the summarization-related evaluation metrics, i.e., BLEU, ROUGE, CIDEr, and METEOR, to measure how relevant the generated headlines are to the news articles, to some extent, by comparing them to the original human-written headlines. In Table TABREF59, the first row \u201cNHG\u201d shows the performance of the current state-of-the-art summarization model on our data, and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What systems are tested?",
    "llm_answer": "Support Vector Machines (SVMs), logistic regression, Na\u00efve Bayes, Perceptron, and decision trees.  The context also mentions neural network models, but doesn't explicitly name the specific types.\n",
    "context": [
      {
        "id": "1299b1d9-1ef1-4d23-a72d-9f4d0b94f886",
        "metadata": {
          "vector_store_key": "1605.08675-1",
          "chunk_id": 12,
          "document_id": "1605.08675",
          "start_idx": 4738,
          "end_idx": 5272
        },
        "page_content": " requiring numerous evaluations may be performed swiftly; saving massive amount of time and human resources. As a test set, authentic questions from a popular Polish quiz TV show are used. Results of experiments, testing (among others) the optimal context length, a number of retrieved documents, a type of entity recognition solution, appear in section SECREF88 . To avoid overfitting, the final system evaluation is executed on a separate test set, previously unused in development, and is checked manually. The results are shown in",
        "type": "Document"
      },
      {
        "id": "c94d339e-36f5-4336-a21a-05be0b487cef",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      },
      {
        "id": "17d3256d-56f6-4b9d-983d-669c916f7fae",
        "metadata": {
          "vector_store_key": "1709.08299-2",
          "chunk_id": 23,
          "document_id": "1709.08299",
          "start_idx": 9435,
          "end_idx": 10003
        },
        "page_content": " questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models. The",
        "type": "Document"
      },
      {
        "id": "6015f6a2-fba3-4bad-a9b2-f8637220f688",
        "metadata": {
          "vector_store_key": "1911.00133-4",
          "chunk_id": 40,
          "document_id": "1911.00133",
          "start_idx": 15746,
          "end_idx": 16159
        },
        "page_content": " first experiment with a suite of non-neural models, including Support Vector Machines (SVMs), logistic regression, Na\u00efve Bayes, Perceptron, and decision trees. We tune the parameters for these models using grid search and 10-fold cross-validation, and obtain results for different combinations of input and features. For input representation, we experiment with bag-of-n-grams (for $n \\in \\lbrace 1..3\\rbrace $),",
        "type": "Document"
      },
      {
        "id": "87d9afb2-c546-418c-b5b1-18b5e1abe1e5",
        "metadata": {
          "vector_store_key": "1605.08675-6",
          "chunk_id": 134,
          "document_id": "1605.08675",
          "start_idx": 49919,
          "end_idx": 50491
        },
        "page_content": ", used at this stage, substantially differs from the one used during the development (see section SECREF80 ). A configuration for the final evaluation has been prepared based on results of the experiments. All of the tested versions share the following features: no question analysis corrections, question classification and query generation solutions which proved best in the previous experiments (see section SECREF18 ), a retrieved set of documents including 20 articles, no minimal confidence, singe sentence context with title. Tested solutions differ with respect to",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What benchmark datasets they use?",
    "llm_answer": "Stanford Sentiment Treebank (SST-2), Yelp dataset, Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), Quora Question Pairs (QQP), SQuAD 1.1, SQuAD 2.0, MRQA, QA4IE-SPAN-S/M/L, QA4IE-SEQ-S/M/L.\n",
    "context": [
      {
        "id": "304b5978-d253-43d4-92b8-7dc6acce1bd5",
        "metadata": {
          "vector_store_key": "2002.06053-5",
          "chunk_id": 123,
          "document_id": "2002.06053",
          "start_idx": 45532,
          "end_idx": 46097
        },
        "page_content": " datasets and different evaluation metrics makes the assessment of model performance challenging. Comprehensive benchmarking platforms that can assess the success of different tools are still lacking. A benchmarking environment rigorously brings together the suitable data sets and evaluation methodologies in order to provide a fair comparison between the available tools. Such environments are available for molecule generation task from MOSES BIBREF123 and GuacaMol BIBREF124. MoleculeNet is also a similar attempt to build a benchmarking platform for tasks such",
        "type": "Document"
      },
      {
        "id": "19ad6a1f-7f24-400e-9235-38467b0ba391",
        "metadata": {
          "vector_store_key": "1910.00458-7",
          "chunk_id": 51,
          "document_id": "1910.00458",
          "start_idx": 17496,
          "end_idx": 17916
        },
        "page_content": "500). For the sentiment analysis category, we used the Stanford Sentiment Treebank (SST-2) dataset from the GLUE benchmark BIBREF22 (around 60k train examples) and the Yelp dataset (around 430k train examples). For the paraphrase category, three paraphrasing datasets are used from the GLUE benchmark: Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), and Quora Question Pairs (",
        "type": "Document"
      },
      {
        "id": "e3363353-9d05-4a9e-8a9b-2d10678183c4",
        "metadata": {
          "vector_store_key": "1804.03396-9",
          "chunk_id": 41,
          "document_id": "1804.03396",
          "start_idx": 14627,
          "end_idx": 14975
        },
        "page_content": " greater than 700 as L. Therefore we obtain 6 different datasets named QA4IE-SPAN-S/M/L and QA4IE-SEQ-S/M/L. A 5/1/5 splitting of train/dev/test sets is performed. The detailed statistics of QA4IE benchmark are provided in Table 1 . We further compare our QA4IE benchmark with some existing IE and QA benchmarks in Table 2 . One can observe that QA",
        "type": "Document"
      },
      {
        "id": "8f40fad4-4ab6-4c70-b931-df5f59aec2ed",
        "metadata": {
          "vector_store_key": "1910.00458-7",
          "chunk_id": 52,
          "document_id": "1910.00458",
          "start_idx": 17758,
          "end_idx": 18087
        },
        "page_content": "antic Textual Similarity Benchmark (STS-B), and Quora Question Pairs (QQP), which are denoted as \u201cGLUE-Para.\u201d. For the span-based QA, we use the SQuAD 1.1, SQuAD 2.0 , and MRQA which is a joint dataset including six popular span-based QA datasets. Table TABREF23 summarizes the results. We see that sentiment analysis datasets do",
        "type": "Document"
      },
      {
        "id": "dac16588-f6ed-4c87-a68a-4994a2eb9a37",
        "metadata": {
          "vector_store_key": "2002.06053-5",
          "chunk_id": 124,
          "document_id": "2002.06053",
          "start_idx": 46097,
          "end_idx": 46537
        },
        "page_content": "REF124. MoleculeNet is also a similar attempt to build a benchmarking platform for tasks such as prediction of binding affinity and toxicity BIBREF82. Despite the focus on sharing datasets and source codes on popular software development platforms such as GitHub (github.com) or Zenodo (zenodo.org), it is still a challenge to use data or code from other groups. The use of FAIR (Findable, Accessible, Interoperable and Reusable) (meta)data",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they combine MonaLog with BERT?",
    "llm_answer": "MonaLog is followed by the re-trained BERT.\n",
    "context": [
      {
        "id": "0395a7c0-0848-4421-b297-f70e6d469165",
        "metadata": {
          "vector_store_key": "1910.08772-1",
          "chunk_id": 69,
          "document_id": "1910.08772",
          "start_idx": 24186,
          "end_idx": 24667
        },
        "page_content": ", i.e., MonaLog followed by the re-trained BERT, can also profit from the additional training data. Intuitively, we would expect smaller gains since MonaLog already handles a fair amount of the entailments and contradictions, i.e., those cases where BERT profits from more examples. However the experiments show that the hybrid system reaches an even higher accuracy of 87.16%, more than 2 percent points above the baseline, equivalent to roughly 100 more problems correctly solved",
        "type": "Document"
      },
      {
        "id": "efc84d95-a023-44e0-a280-05e1c54948b3",
        "metadata": {
          "vector_store_key": "1910.08772-1",
          "chunk_id": 50,
          "document_id": "1910.08772",
          "start_idx": 17205,
          "end_idx": 17639
        },
        "page_content": "ments and contradictions, as demonstrated by the high precision values, especially on the corrected SICK set (98.50 precision for E and 95.02 precision for C). The lower recall values are due to MonaLog's current inability to handle syntactic variation. Based on these results, we tested a hybrid model of MonaLog and BERT (see Table TABREF27) where we exploit MonaLog's strength: Since MonaLog has a very high precision on Entailment",
        "type": "Document"
      },
      {
        "id": "a92c7974-674d-494c-997f-a43ba1a31f41",
        "metadata": {
          "vector_store_key": "1910.08772-1",
          "chunk_id": 52,
          "document_id": "1910.08772",
          "start_idx": 17845,
          "end_idx": 18281
        },
        "page_content": "ed SICK dataset, the hybrid system performs worse than BERT. Since MonaLog is optimized for the corrected SICK, it may mislabel many E and C judgments in the uncorrected dataset. The stand-alone BERT system performs better on the uncorrected data (86.74%) than the corrected set (85.00%). The corrected set may be too inconsistent since only a part has been checked. Overall, these hybird results show that it is possible to combine our",
        "type": "Document"
      },
      {
        "id": "5f508db5-ffc5-4944-ad71-6245bfe7003c",
        "metadata": {
          "vector_store_key": "1910.08772-1",
          "chunk_id": 68,
          "document_id": "1910.08772",
          "start_idx": 23899,
          "end_idx": 24347
        },
        "page_content": "ness of BERT in the current experiment and the usefulness of the generated data. The more data we add, the better the system performs. We also see that raising the threshold to relabel uncertain cases as neutral gives a small boost, from 86.51% to 86.71%. This translates into 10 cases where the relabeling corrected the answer. Finally, we also investigated whether the hybrid system, i.e., MonaLog followed by the re-trained BERT, can also profit",
        "type": "Document"
      },
      {
        "id": "2ed25dd8-7718-495b-b13a-2d734ef2de24",
        "metadata": {
          "vector_store_key": "1902.00821-6",
          "chunk_id": 28,
          "document_id": "1902.00821",
          "start_idx": 9236,
          "end_idx": 9525
        },
        "page_content": "P]}, x_{j+1:n}, \\texttt {[SEP]})$ , where [CLS] and [SEP] are special tokens and $x_{1:n}$ is a document splited into two sides of sentences $x_{1:j}$ and $x_{j+1:n}$ . The key performance gain of BERT comes from two novel pre-training objectives: masked language model (MLM) and next text",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they select monotonicity facts?",
    "llm_answer": "They use automatically generated datasets (HELP), crowdsourced datasets, and examples from previous manually curated datasets and linguistics publications.  Some examples are excluded due to non-strict downward monotonicity.\n",
    "context": [
      {
        "id": "2f858e4c-645a-469e-a0a7-ea72b1f48075",
        "metadata": {
          "vector_store_key": "1906.06448-9",
          "chunk_id": 51,
          "document_id": "1906.06448",
          "start_idx": 18651,
          "end_idx": 19120
        },
        "page_content": " of models on monotonicity reasoning depends on the training set or the model themselves, we conducted further analysis performed by data augmentation with the automatically generated monotonicity dataset HELP BIBREF11 . HELP contains 36K monotonicity inference examples (7,784 upward examples, 21,192 downward examples, and 1,105 non-monotone examples). The size of the HELP word vocabulary is 15K, and the overlap ratio of vocabulary between HELP and MED is 15.2%. We",
        "type": "Document"
      },
      {
        "id": "2d3c4e13-0485-44e6-bee1-760fc370e748",
        "metadata": {
          "vector_store_key": "1906.06448-9",
          "chunk_id": 37,
          "document_id": "1906.06448",
          "start_idx": 13495,
          "end_idx": 14013
        },
        "page_content": " additional phrase and the gravity force is too low does not form constituents with atmosphere. Thus, such examples are not strict downward monotone inferences. In such cases as (a) and (b), we do not swap the premise and the hypothesis. In the end, we collected 4,068 examples from crowdsourced datasets. We also collect monotonicity inference problems from previous manually curated datasets and linguistics publications. The motivation is that previous linguistics publications related to monotonicity reasoning are",
        "type": "Document"
      },
      {
        "id": "bf1c8b0c-34fa-458d-90c3-bb8405385021",
        "metadata": {
          "vector_store_key": "1906.06448-0",
          "chunk_id": 10,
          "document_id": "1906.06448",
          "start_idx": 3623,
          "end_idx": 4118
        },
        "page_content": "otonicity with the training set, while they perform worse in the opposite direction. This indicates that the accuracy on monotonicity reasoning depends solely on the majority direction in the training set, and models might lack the ability to capture the structural relations between monotonicity operators and their arguments. As an example of a monotonicity inference, consider the example with the determiner every in ( \"Monotonicity\" ); here the premise $P$ entails the hypothesis $H$ .  $P$",
        "type": "Document"
      },
      {
        "id": "0d22d105-dcba-497d-b12d-76886d18bb48",
        "metadata": {
          "vector_store_key": "1906.06448-9",
          "chunk_id": 6,
          "document_id": "1906.06448",
          "start_idx": 2027,
          "end_idx": 2488
        },
        "page_content": " previous datasets containing monotonicity inference problems, FraCaS BIBREF8 and the GLUE diagnostic dataset BIBREF9 are manually-curated datasets for testing a wide range of linguistic phenomena. However, monotonicity problems are limited to very small sizes (FraCaS: 37/346 examples and GLUE: 93/1650 examples). The limited syntactic patterns and vocabularies in previous test sets are obstacles in accurately evaluating NLI models on monotonicity reasoning.",
        "type": "Document"
      },
      {
        "id": "aec13955-1042-4822-a52f-25683e23d369",
        "metadata": {
          "vector_store_key": "1906.06448-0",
          "chunk_id": 5,
          "document_id": "1906.06448",
          "start_idx": 1765,
          "end_idx": 2205
        },
        "page_content": " can be reversed again by embedding yet another downward entailing context (e.g., not in ( \"Introduction\" )), as witness the fact that ( \"Introduction\" ) entails ( \"Introduction\" ). To properly handle both directions of monotonicity, NLI models must detect monotonicity operators (e.g., all, not) and their arguments from the syntactic structure. For previous datasets containing monotonicity inference problems, FraCaS BIBREF8 and the GLUE",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the 12 categories devised?",
    "llm_answer": "The provided text describes 15 hand-coded categories and a 44-community partition, but does not explicitly list 12 categories.  The answer is not found in the context.\n",
    "context": [
      {
        "id": "a5ef44c5-13bc-4dc3-8bde-e0a34519d3b4",
        "metadata": {
          "vector_store_key": "1703.09684-2",
          "chunk_id": 29,
          "document_id": "1703.09684",
          "start_idx": 11091,
          "end_idx": 11574
        },
        "page_content": " and the answer had to be a small countable integer (1-16). The other categories were determined using regular expressions. For example, a question of the form `Are feeling ?' was classified as sentiment understanding and `What is to the right of/left of/ behind the ?' was classified as positional reasoning. Similarly, `What <OBJECT CATEGORY> is in the image?' and similar templates were used to populate subordinate object recognition questions. This method was used for questions",
        "type": "Document"
      },
      {
        "id": "dc19478a-bb9a-454c-8280-e8b18b5c121b",
        "metadata": {
          "vector_store_key": "1703.09684-2",
          "chunk_id": 28,
          "document_id": "1703.09684",
          "start_idx": 10601,
          "end_idx": 11091
        },
        "page_content": " would be imported if it contained the word `color' in it and the answer was a commonly used color. Questions were classified as activity or sports recognition questions if the answer was one of nine common sports or one of fifteen common activities and the question contained common verbs describing actions or sports, e.g., playing, throwing, etc. For counting, the question had to begin with `How many' and the answer had to be a small countable integer (1-16). The other categories were",
        "type": "Document"
      },
      {
        "id": "49d70731-d3fc-44e8-b347-8b3e9caa5f76",
        "metadata": {
          "vector_store_key": "1909.00183-5",
          "chunk_id": 93,
          "document_id": "1909.00183",
          "start_idx": 34610,
          "end_idx": 35088
        },
        "page_content": "22). Compared to the 15 hand-coded categories, this 44-community partition provides finer groupings corresponding to specific sub-themes within the generic hand-coded categories. This is apparent in the hand-coded classes `Accidents', `Medication', `Clinical assessment', `Documentation' and `Infrastructure', where a variety of meaningful subtopics are identified (see Fig. FIGREF23 for details). In other cases, however, the content clusters cut across the external categories",
        "type": "Document"
      },
      {
        "id": "77a2c284-62f0-43e0-8bf9-52a2d068ed04",
        "metadata": {
          "vector_store_key": "2003.07433-1",
          "chunk_id": 49,
          "document_id": "2003.07433",
          "start_idx": 19311,
          "end_idx": 19770
        },
        "page_content": " well as dimensions' text analysis scores. We chose three categories based on the three different surveys: 1) DOSPERT scale; 2) BSSS scale; and 3) VIAS scale. Dimension selection: We have to define the word sets (also called dimension) for each category. We chose one dimension for each of the questions under each category to reflect real survey system evaluation. Our chosen categories are state in Fig FIGREF20. Score calculation $\\alpha $-score: $\\alpha $",
        "type": "Document"
      },
      {
        "id": "4f90798a-5a29-4a3a-8935-fc0e03e7d06c",
        "metadata": {
          "vector_store_key": "1909.00154-0",
          "chunk_id": 6,
          "document_id": "1909.00154",
          "start_idx": 2487,
          "end_idx": 2981
        },
        "page_content": " categorical variables. The default paradigm is dummy variables (also known as \u201cone-hot-encoding\" in machine learning literature), which have well-known limitations, namely the explosion of dimensionality and enforced ortogonality. The former happens because we assign one new \u201cdummy\" variable to each of D-1 categories, and easily go from a small original variable specification to one with hundreds of variables, bringing problems in model estimation and analysis. This often affects the data",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what are the off-the-shelf systems discussed in the paper?",
    "llm_answer": "IBREF28, Latent-Tree, Berkeley, LaSO, Latent-Strc, Model-Stack, Non-Linear.\n",
    "context": [
      {
        "id": "577fb06e-9340-4422-93f3-91bc89574aee",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 43,
          "document_id": "2001.00137",
          "start_idx": 16781,
          "end_idx": 17197
        },
        "page_content": " the online platforms. Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector",
        "type": "Document"
      },
      {
        "id": "692e0ed1-d4ba-47e7-a034-873c65948722",
        "metadata": {
          "vector_store_key": "1603.04553-1",
          "chunk_id": 39,
          "document_id": "1603.04553",
          "start_idx": 11513,
          "end_idx": 11902
        },
        "page_content": "IBREF28 ; Latent-Tree: the latent tree model BIBREF29 obtaining the best results in the shared task; Berkeley: the Berkeley system with the final feature set BIBREF12 ; LaSO: the structured perceptron system with non-local features BIBREF30 ; Latent-Strc: the latent structure system BIBREF31 ; Model-Stack: the entity-centric system with model stacking BIBREF32 ; and Non-Linear: the non-",
        "type": "Document"
      },
      {
        "id": "a52d0dc7-7f16-45ef-a786-52c8fffa02d5",
        "metadata": {
          "vector_store_key": "1701.09123-0",
          "chunk_id": 55,
          "document_id": "1701.09123",
          "start_idx": 20683,
          "end_idx": 21114
        },
        "page_content": " the first CRF. Their \u201cbaseline\u201d system uses a similar local featureset as Ratinov and Roth's (2009) but complemented with gazetteers. Their baseline system combined with their phrase embeddings trained with infused lexicons allow them to report the best CoNLL 2003 result so far. The best system of the GermEval 2014 task built an ensemble of classifiers and pattern extractors to find the most likely tag sequence BIBREF48 . They",
        "type": "Document"
      },
      {
        "id": "18bc4047-159c-4932-9d6c-755753773398",
        "metadata": {
          "vector_store_key": "1603.04513-7",
          "chunk_id": 82,
          "document_id": "1603.04513",
          "start_idx": 26830,
          "end_idx": 27230
        },
        "page_content": "ational aspect of the system. All results in the five blocks are with respect to row 34, \u201cMVCNN (overall)\u201d; e.g., row 19 shows what happens when HLBL is removed from row 34, row 28 shows what happens when mutual learning is removed from row 34 etc. The block \u201cbaselines\u201d (1\u201318) lists some systems representative of previous work on the corresponding datasets, including the state-of-the-art systems (",
        "type": "Document"
      },
      {
        "id": "ba55c51a-836b-44cf-929b-6e8025942569",
        "metadata": {
          "vector_store_key": "1907.00854-2",
          "chunk_id": 6,
          "document_id": "1907.00854",
          "start_idx": 2637,
          "end_idx": 3157
        },
        "page_content": " REST API call. We demonstrate the system using publicly available, pre-trained models and knowledge base articles extracted from Stack Exchange sites. However, users can extend the system to any number of topics, or domains, without the need to modify the model serving code. All components of the system are open source and publicly available under a permissive Apache 2 License. The rest of the paper is organized as follows. In the next section, we provide an overview of the system logic and its modules. In Section",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many rules had to be defined?",
    "llm_answer": "Ten\n",
    "context": [
      {
        "id": "257096b4-d047-4afa-905d-fad74659fcaa",
        "metadata": {
          "vector_store_key": "1706.08032-2",
          "chunk_id": 21,
          "document_id": "1706.08032",
          "start_idx": 8281,
          "end_idx": 8786
        },
        "page_content": " and devised ten semantic rules in their hybrid approach based on the presentation of BIBREF16 . We use five rules in the semantic rules set because other five rules are only used to compute polarity of words after POS tagging or Parsing steps. We follow the same naming convention for rules utilized by BIBREF11 to represent the rules utilized in our proposed method. The rules utilized in the proposed method are displayed in Table TABREF15 in which is included examples from STS Corpus and output after",
        "type": "Document"
      },
      {
        "id": "b7f97ae4-10c4-4e1c-b7b5-1451a463ec18",
        "metadata": {
          "vector_store_key": "1909.12140-2",
          "chunk_id": 5,
          "document_id": "1909.12140",
          "start_idx": 2277,
          "end_idx": 2783
        },
        "page_content": " These patterns were heuristically determined in a comprehensive linguistic analysis and encode syntactic and lexical features that can be derived from a sentence's parse tree. Each rule specifies (1) how to split up and rephrase the input into structurally simplified sentences and (2) how to set up a semantic hierarchy between them. They are recursively applied on a given source sentence in a top-down fashion. When no more rule matches, the algorithm stops and returns the generated discourse tree. In",
        "type": "Document"
      },
      {
        "id": "191fb978-6565-4267-841b-0fdf21e5eb7a",
        "metadata": {
          "vector_store_key": "1909.05438-2",
          "chunk_id": 14,
          "document_id": "1909.05438",
          "start_idx": 5481,
          "end_idx": 5883
        },
        "page_content": " necessary. These rules are not perfect, with low coverage, and can even be incorrect for some situations. For instance, when predicting a SQL command in the first task, we have a prior knowledge that (1) WHERE values potentially have co-occurring words with table cells; (2) the words \u201cmore\u201d and \u201cgreater\u201d tend to be mapped to WHERE operator \u201c INLINEFORM2 \u201d; (3) within a WHERE clause, header and cell",
        "type": "Document"
      },
      {
        "id": "928ea19a-d8a3-49ad-952f-74736a8bc759",
        "metadata": {
          "vector_store_key": "1905.07471-3",
          "chunk_id": 18,
          "document_id": "1905.07471",
          "start_idx": 6558,
          "end_idx": 7074
        },
        "page_content": " inputs. The tool ignores questions greater than 60 characters and complex questions it cannot parse, leaving a dataset smaller than the original (see Table 1). Each rule is on average forty lines of code that traverses a dependency parse tree according to its pre-specified pattern, extracting the matching spans at each step. A master function parse() determines which rule to apply based on the question type which is categorized by nsubj presence, and the type of question (who/what/etc.). Most questions contain",
        "type": "Document"
      },
      {
        "id": "154800a9-8cc0-479d-bdb3-8f3aa6520648",
        "metadata": {
          "vector_store_key": "1906.06448-6",
          "chunk_id": 28,
          "document_id": "1906.06448",
          "start_idx": 9932,
          "end_idx": 10429
        },
        "page_content": " each set of substitutions, and each set was assigned to three workers. To remove low-quality examples, we set the minimum time it should take to complete each set to 200 seconds. The entry in our task was restricted to workers from native speaking English countries. 128 workers contributed to the task, and we created 15,339 hypotheses (7,179 upward examples and 8,160 downward examples). The gold label of each premise-hypothesis pair created in the previous task is automatically determined by",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was performance of classifiers before/after using distant supervision?",
    "llm_answer": "Training a ranker with distant supervision performs significantly better than a random baseline.  A model trained on clean and distantly-supervised data deteriorates if the distant supervision technique has a higher F1-score.\n",
    "context": [
      {
        "id": "d8880f9a-3581-484f-9905-32517ce3a321",
        "metadata": {
          "vector_store_key": "2003.08370-2",
          "chunk_id": 41,
          "document_id": "2003.08370",
          "start_idx": 13632,
          "end_idx": 14133
        },
        "page_content": " investigate this further, but it shows already the benefits that are possible through noise-handling. An evaluation of the distant supervision for Yor\u00f9b\u00e1 is given in Table TABREF14. The quality of the automatically annotated labels differs between the classes. Locations perform better than person and organization names, probably due to locations being less diverse and better covered in Wikidata. With simple date rules, we obtain already a 48% F1-score. This shows the importance of leveraging the",
        "type": "Document"
      },
      {
        "id": "1b696136-515f-48a5-bc02-4842fd7faed8",
        "metadata": {
          "vector_store_key": "2003.08370-1",
          "chunk_id": 47,
          "document_id": "2003.08370",
          "start_idx": 15969,
          "end_idx": 16447
        },
        "page_content": " higher F1-score than the distant supervision technique, the model trained on clean and distantly-supervised data deteriorates. This suggests that the BERT model overfits too much on the noise in the distant supervision. In this study, we analysed distant supervision techniques and label-noise handling for NER in Hausa and Yor\u00f9b\u00e1, two languages from developing countries. We showed that they can be successfully leveraged in a realistic low-resource scenario to double a class",
        "type": "Document"
      },
      {
        "id": "21bdd991-2357-48d5-8eeb-a90f815eb8a0",
        "metadata": {
          "vector_store_key": "2004.02393-5",
          "chunk_id": 38,
          "document_id": "2004.02393",
          "start_idx": 12674,
          "end_idx": 13193
        },
        "page_content": " best performed model first selects the tail passage $p_t$ and then the head passage $p_h$, because the number of candidates of tail is smaller ($\\sim $2 per question). Table TABREF21 shows the results. First, training a ranker with distant supervision performs significantly better than the random baseline, showing that the training process itself has a certain degree of denoising ability to distinguish the more informative signals from distant supervision labels. By introducing additional inductive bias of orders",
        "type": "Document"
      },
      {
        "id": "a23e4d6e-4282-460e-a6cd-00da265037fe",
        "metadata": {
          "vector_store_key": "2001.02380-9",
          "chunk_id": 92,
          "document_id": "2001.02380",
          "start_idx": 36012,
          "end_idx": 36552
        },
        "page_content": " across partitions, since the splits are based on document boundaries. We note again that for the training and development data, we have no signaling annotations of any kind; this is possible since the network does not actually use the human signaling annotations we will be evaluating against: its distant supervision consists solely of the RST relation labels. Although only used as an auxiliary training task, we can look at the model's performance on predicting discourse relations, which is given in Table TABREF34. Unsurprisingly, the",
        "type": "Document"
      },
      {
        "id": "6824672f-ce96-4486-bfec-5cb2c6aed439",
        "metadata": {
          "vector_store_key": "2002.05058-4",
          "chunk_id": 31,
          "document_id": "2002.05058",
          "start_idx": 12524,
          "end_idx": 12984
        },
        "page_content": " training examples for the comparative evaluator, allowing to enhance the generalization ability and introduce more informative learning signals than the standard real/fake binary discriminative evaluator. Note that when constructing a sample pair ($s_-^i$, $s_-^j$), $s_-^i$ and $s_-^j$ are sampled from the same checkpoint of the same model in order to ensure that they are of similar quality in expectation. One problem of the strong supervision approach is",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big are the datasets used?",
    "llm_answer": "The context describes various datasets with sizes ranging from tens of thousands (e.g., SST-2, MRPC) to millions (e.g., English-Italian and English-German sentence pairs).  Specific sizes are given for some datasets (e.g., CNNDM, NYT50, SCI).\n",
    "context": [
      {
        "id": "49dd09c0-f752-4a00-bc37-38c26e5e9acc",
        "metadata": {
          "vector_store_key": "1709.05404-5",
          "chunk_id": 48,
          "document_id": "1709.05404",
          "start_idx": 17579,
          "end_idx": 18022
        },
        "page_content": "-validation setup) for each dataset while holding the number of posts per class constant. Figure FIGREF26 shows the performance of each of the Gen, RQ, and Hyp datasets at intervals of 100 posts per class (up to the maximum size of 582 posts per class for Hyp, and 851 posts per class for RQ). From the graph, we can see that as a general trend, the datasets benefit from larger dataset sizes. Interestingly, the results for the RQ dataset are",
        "type": "Document"
      },
      {
        "id": "9270562a-8e53-4e1d-b760-74d8fd0bc872",
        "metadata": {
          "vector_store_key": "1905.06566-7",
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "19ad6a1f-7f24-400e-9235-38467b0ba391",
        "metadata": {
          "vector_store_key": "1910.00458-7",
          "chunk_id": 51,
          "document_id": "1910.00458",
          "start_idx": 17496,
          "end_idx": 17916
        },
        "page_content": "500). For the sentiment analysis category, we used the Stanford Sentiment Treebank (SST-2) dataset from the GLUE benchmark BIBREF22 (around 60k train examples) and the Yelp dataset (around 430k train examples). For the paraphrase category, three paraphrasing datasets are used from the GLUE benchmark: Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), and Quora Question Pairs (",
        "type": "Document"
      },
      {
        "id": "0d593f74-7474-41cc-9928-ce68ac96f714",
        "metadata": {
          "vector_store_key": "1802.05574-2",
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "6b15dc0c-6710-42c4-825a-b86937b8e657",
        "metadata": {
          "vector_store_key": "1910.10408-0",
          "chunk_id": 27,
          "document_id": "1910.10408",
          "start_idx": 10206,
          "end_idx": 10669
        },
        "page_content": " we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De). While our main goal is to verify our hypotheses on a large data condition, thus the need to include proprietary data, for the sake of reproducibility in both languages we also provide results with systems only trained on TED Talks (small data condition). When training on large scale",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What datasets are used for training/testing models? ",
    "llm_answer": "The context mentions datasets in Tables TABREF12, TABREF19, TABREF83, and TABREF10, but does not explicitly list the specific datasets used for training and testing.\n",
    "context": [
      {
        "id": "c94d339e-36f5-4336-a21a-05be0b487cef",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      },
      {
        "id": "304b5978-d253-43d4-92b8-7dc6acce1bd5",
        "metadata": {
          "vector_store_key": "2002.06053-5",
          "chunk_id": 123,
          "document_id": "2002.06053",
          "start_idx": 45532,
          "end_idx": 46097
        },
        "page_content": " datasets and different evaluation metrics makes the assessment of model performance challenging. Comprehensive benchmarking platforms that can assess the success of different tools are still lacking. A benchmarking environment rigorously brings together the suitable data sets and evaluation methodologies in order to provide a fair comparison between the available tools. Such environments are available for molecule generation task from MOSES BIBREF123 and GuacaMol BIBREF124. MoleculeNet is also a similar attempt to build a benchmarking platform for tasks such",
        "type": "Document"
      },
      {
        "id": "fa96ba68-ce4c-4bb6-996e-738362630646",
        "metadata": {
          "vector_store_key": "1701.09123-4",
          "chunk_id": 122,
          "document_id": "1701.09123",
          "start_idx": 46004,
          "end_idx": 46555
        },
        "page_content": " test data differs. Annotation guidelines: The gold annotation of the test data follows different guidelines from the training data. This is usually reflected in different named entity spans. The datasets and languages chosen for these experiments are based on the availability of both previous results and publicly distributed NERC systems to facilitate direct comparison of our system with other approaches. Table TABREF83 specifies the datasets used for each out-of-domain setting and language. Details of each dataset can be found Table TABREF10 .",
        "type": "Document"
      },
      {
        "id": "3488785c-d053-43b7-8293-c3c50f3c5473",
        "metadata": {
          "vector_store_key": "1909.00154-2",
          "chunk_id": 65,
          "document_id": "1909.00154",
          "start_idx": 24630,
          "end_idx": 25057
        },
        "page_content": " vectors) Development set: 20% of the dataset (2003 vectors) Test set: 20% of the dataset (2003 vectors) The PyLogit package BIBREF11 also uses Swissmetro as an example. Therefore, our model specifications will extend the default one from this package. We re-estimated this model with the train set and validated with testset. The results are shown in tables TABREF31 and TABREF32. Since we are comparing the models at the test",
        "type": "Document"
      },
      {
        "id": "d2d578a8-54c6-4c25-94af-53298d0dc659",
        "metadata": {
          "vector_store_key": "1811.00942-2",
          "chunk_id": 27,
          "document_id": "1811.00942",
          "start_idx": 8831,
          "end_idx": 9236
        },
        "page_content": "constrained environment, we deployed them on a Raspberry Pi 3 (Model B) running Raspbian Stretch (4.9.41-v7+). The Raspberry Pi (RPi) is not only a standard platform, but also a close surrogate to mobile phones, using the same Cortex-A7 in many phones. We then transferred the trained models to the RPi, using the same frameworks for evaluation. We plugged the RPi into a Watts Up Pro meter, a power meter",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How better is gCAS approach compared to other approaches?",
    "llm_answer": "gCAS outperforms all other methods in most metrics, especially in predicting slots consistent with the act, entity F$_1$, and success F$_1$.  Its advantage is particularly notable in the movie domain and in the restaurant domain where agents frequently have multiple slots.\n",
    "context": [
      {
        "id": "7f716fcd-0752-4adf-baa1-157b907e9bd6",
        "metadata": {
          "vector_store_key": "1908.11546-2",
          "chunk_id": 34,
          "document_id": "1908.11546",
          "start_idx": 11120,
          "end_idx": 11563
        },
        "page_content": " gCAS still outperforms all other methods. The performance difference between CAS and gCAS on frames becomes much more evident, suggesting that gCAS is more capable of predicting slots that are consistent with the act. This finding is also consistent with their Entity F$_1$ and Success F$_1$ performance. However, gCAS's act-slot pair performance is far from perfect. The most common failure case is on non-critical slots (like `genre' in the",
        "type": "Document"
      },
      {
        "id": "1a25e864-0c0a-4848-a8f2-01bbefb856d3",
        "metadata": {
          "vector_store_key": "1908.11546-2",
          "chunk_id": 29,
          "document_id": "1908.11546",
          "start_idx": 9434,
          "end_idx": 9829
        },
        "page_content": " a beam search since their inference steps are much less than Seq2Seq methods. All models use Adam optimizer BIBREF24 with a learning rate of 0.001. As shown in Table TABREF13, gCAS outperforms all other methods on Entity F$_1$ in all three domains. Compared to Seq2Seq, the performance advantage of gCAS in the taxi and restaurant domains is small, while it is more evident in the movie domain.",
        "type": "Document"
      },
      {
        "id": "6b8247c9-8412-4977-a066-b6b5837c2c2c",
        "metadata": {
          "vector_store_key": "1908.11546-2",
          "chunk_id": 35,
          "document_id": "1908.11546",
          "start_idx": 11563,
          "end_idx": 11967
        },
        "page_content": " perfect. The most common failure case is on non-critical slots (like `genre' in the example in Table TABREF4): gCAS does not predict them, while it predicts the critical ones (like `moviename' in the example in Table TABREF4). Table TABREF15 shows predictions of all methods from two emblematic examples. Example 1 is a frequent single-act multi-slots agent act. Example 2 is a complex multi-act example",
        "type": "Document"
      },
      {
        "id": "68066119-6927-4b08-844c-d366cbdc5e46",
        "metadata": {
          "vector_store_key": "1908.11546-2",
          "chunk_id": 32,
          "document_id": "1908.11546",
          "start_idx": 10415,
          "end_idx": 10854
        },
        "page_content": " (see example 2 in Table TABREF15) and this makes classification and sequence generation harder, but gCAS multi-label slots decoder handles it easily. Table TABREF14 shows the turn-level acts and frame prediction performance. CAS and gCAS outperform all other models in acts prediction in terms of F$_1$ score. The main reason is that CAS and gCAS output a tuple at each recurrent step, which makes for shorter sequences that are easier to",
        "type": "Document"
      },
      {
        "id": "10921b3d-14d2-4912-ad4e-f8936b07b161",
        "metadata": {
          "vector_store_key": "1908.11546-2",
          "chunk_id": 31,
          "document_id": "1908.11546",
          "start_idx": 10106,
          "end_idx": 10608
        },
        "page_content": " by the classification model in the taxi domain. The reason is that in the taxi domain, the agent usually informs the user at the last turn, while in all previous turns the agent usually requests information from the user. It is easy for the classification model to overfit this pattern. The advantage of gCAS in the restaurant domain is much more evident: the agent's inform act usually has multiple slots (see example 2 in Table TABREF15) and this makes classification and sequence generation harder,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is specific to gCAS cell?",
    "llm_answer": "gCAS uses a recurrent cell with separate \"continue,\" \"act,\" and \"slots\" units that are sequentially connected.\n",
    "context": [
      {
        "id": "783e92b5-012c-4349-9b76-fe6f55317c10",
        "metadata": {
          "vector_store_key": "1908.11546-3",
          "chunk_id": 27,
          "document_id": "1908.11546",
          "start_idx": 8693,
          "end_idx": 9198
        },
        "page_content": " to continue, act and slots. For each step in the sequence of CAS tuples, given the output of the GRU, continue, act and slot predictions are obtained by separate heads, each with one fully connected layer. The hidden state of the GRU and the predictions at the previous step are passed to the cell at the next step connecting them sequentially. gCAS uses our proposed recurrent cell which contains separate continue, act and slots unit that are sequentially connected. The classification architecture has",
        "type": "Document"
      },
      {
        "id": "85dfd078-1909-403f-aec4-0e83d248afdd",
        "metadata": {
          "vector_store_key": "1908.11546-2",
          "chunk_id": 41,
          "document_id": "1908.11546",
          "start_idx": 13833,
          "end_idx": 14277
        },
        "page_content": ", and proposed a novel recurrent cell, gated CAS, which allows the decoder to output a tuple at each step. Experimental results showed that gCAS is the best performing model for multi-act prediction. The CAS decoder and the gCAS cell can also be used in a user simulator and gCAS can be applied in the encoder. A few directions for improvement have also been identified: 1) improving the performance on non-critical slots, 2) tuning the decoder",
        "type": "Document"
      },
      {
        "id": "6b8247c9-8412-4977-a066-b6b5837c2c2c",
        "metadata": {
          "vector_store_key": "1908.11546-2",
          "chunk_id": 35,
          "document_id": "1908.11546",
          "start_idx": 11563,
          "end_idx": 11967
        },
        "page_content": " perfect. The most common failure case is on non-critical slots (like `genre' in the example in Table TABREF4): gCAS does not predict them, while it predicts the critical ones (like `moviename' in the example in Table TABREF4). Table TABREF15 shows predictions of all methods from two emblematic examples. Example 1 is a frequent single-act multi-slots agent act. Example 2 is a complex multi-act example",
        "type": "Document"
      },
      {
        "id": "ea5b6d46-0f6f-42cf-9ea1-f059f2a98201",
        "metadata": {
          "vector_store_key": "1908.02402-2",
          "chunk_id": 56,
          "document_id": "1908.02402",
          "start_idx": 16966,
          "end_idx": 17300
        },
        "page_content": " and requestable slots classifiers. Then, the concatenation of the context vector $c^{k^S}$ , the embedding vector of the response slot $e^{k^S}$ and the KB query vector $d_t$ are used as input to a single GRU cell. Finally, a sigmoid non-linearity is applied to the product of a weight vector $W_{y}^{S}$ and the output of the GRU $h",
        "type": "Document"
      },
      {
        "id": "0b3e4e79-4f4d-4b24-98d1-0d4eb6d4523d",
        "metadata": {
          "vector_store_key": "1811.09786-8",
          "chunk_id": 19,
          "document_id": "1811.09786",
          "start_idx": 6995,
          "end_idx": 7485
        },
        "page_content": " the controller cell is to learn gating functions in order to influence the target cell. In order to control the target cell, the controller cell constructs a forget gate and an output gate which are then used to influence the information flow of the listener cell. For each gate (output and forget), we use a separate RNN cell. As such, the controller cell comprises two cell states and an additional set of parameters. The equations of the controller cell are defined as follows: i1t = s(",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the source of external knowledge?",
    "llm_answer": "Knowledge bases for specialized domains.\n",
    "context": [
      {
        "id": "75418f4e-5ed3-44d9-a7c5-1c08a7c10ed0",
        "metadata": {
          "vector_store_key": "1905.10238-0",
          "chunk_id": 59,
          "document_id": "1905.10238",
          "start_idx": 18851,
          "end_idx": 19383
        },
        "page_content": " model. It proves that simply treating external knowledge as the feature, even though they are from the same sources, is not as effective as learning them in a joint framework. The reason behind this result is mainly from the noise in the knowledge source, e.g., parsing error, incorrectly identified relations, etc. For example, the plurality of 17% noun phrases are wrongly labeled in the test data. As a comparison, our knowledge attention might contribute to alleviate such noise when incorporating all knowledge sources. Effect",
        "type": "Document"
      },
      {
        "id": "20f2636f-9f1d-480e-bc1c-f9c08c316374",
        "metadata": {
          "vector_store_key": "1905.10238-0",
          "chunk_id": 75,
          "document_id": "1905.10238",
          "start_idx": 25430,
          "end_idx": 26025
        },
        "page_content": ", where the first layer encodes contextual information and the second layer leverages external knowledge. Particularly, a knowledge attention mechanism is proposed to selectively leverage features from different knowledge sources. As an enhancement to existing methods, the proposed model combines the advantage of conventional feature-based models and deep learning models, so that context and external knowledge can be synchronously and effectively used for this task. Experimental results and case studies demonstrate the superiority of the proposed model to state-of-the-art baselines. Since",
        "type": "Document"
      },
      {
        "id": "4af83b98-1426-42ed-80b8-5f8773cd94cb",
        "metadata": {
          "vector_store_key": "1906.03538-1",
          "chunk_id": 57,
          "document_id": "1906.03538",
          "start_idx": 22310,
          "end_idx": 22852
        },
        "page_content": " life. In this work, we attempt to facilitate the development of systems that aid in better organization and access to information, with the hope that the access to more diverse information can address over-personalization too BIBREF41 . The dataset presented here is not intended to be exhaustive, nor does it attempt to reflect a true distribution of the important claims and perspectives in the world, or to associate any of the perspective and identified evidence with levels of expertise and trustworthiness. Moreover, it is important to",
        "type": "Document"
      },
      {
        "id": "be6e0481-c3af-4d26-800e-5ddcef28338e",
        "metadata": {
          "vector_store_key": "1606.08140-0",
          "chunk_id": 3,
          "document_id": "1606.08140",
          "start_idx": 1140,
          "end_idx": 1742
        },
        "page_content": " presented here. First, additional information sources might not be available, e.g., for knowledge bases for specialized domains. Second, models that don't exploit external resources are simpler and thus typically much faster to train than the more complex models using additional information. Third, the more complex models that exploit external information are typically extensions of these simpler models, and are often initialized with parameters estimated by such simpler models, so improvements to the simpler models should yield corresponding improvements to the more complex models as well. Emb",
        "type": "Document"
      },
      {
        "id": "a84c7e1a-1d34-459d-b68c-176b2df49411",
        "metadata": {
          "vector_store_key": "1905.10238-0",
          "chunk_id": 29,
          "document_id": "1905.10238",
          "start_idx": 9591,
          "end_idx": 9802
        },
        "page_content": " a knowledge source $m$6 , its knowledge attention score is computed via  $$\\beta _i(n, n^\\prime , p) = NN_{ka}([{\\bf o}_{n,p}^i, {\\bf o}_{n^\\prime ,p}^i, {\\bf o}_{n,p}^i \\odot {\\bf o}_{n^\\prime ,p}^i]),$$   (Eq",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What were the sizes of the test sets?",
    "llm_answer": "1585 IPUs and 1841 IPUs.\n",
    "context": [
      {
        "id": "6a849beb-7f02-435c-9188-73e3f6b1d235",
        "metadata": {
          "vector_store_key": "1701.02877-1",
          "chunk_id": 27,
          "document_id": "1701.02877",
          "start_idx": 9957,
          "end_idx": 10406
        },
        "page_content": " training and testing data sizes measured in number of NEs are listed in Table 2 . Separate models are then trained on the training parts of each corpus and evaluated on the development (if available) and test parts of the same corpus. If development parts are available, as they are for CoNLL (CoNLL Test A) and MUC (MUC 7 Dev), they are not merged with the training corpora for testing, as it was permitted to do in the context of those evaluation",
        "type": "Document"
      },
      {
        "id": "9dd1bd2f-3b4b-477c-8f5b-7dedd3d4628a",
        "metadata": {
          "vector_store_key": "1801.09030-3",
          "chunk_id": 50,
          "document_id": "1801.09030",
          "start_idx": 17342,
          "end_idx": 17824
        },
        "page_content": " two test sets to do evaluation to take the advantages of both data magnitude and quality. In our experiments, we implement our models with the PyTorch toolkit . We set the embedding size of both Chinese characters in the symptoms and the herb tokens to 100. We set the hidden state size to 300, and the batch size to 20. We set the maximum length of the herb sequence to 20 because the length of nearly all the prescriptions are within this range (see Table 2 for the statistics of",
        "type": "Document"
      },
      {
        "id": "28781156-e3d3-4706-8c20-c33ac6b14c3d",
        "metadata": {
          "vector_store_key": "2002.06675-6",
          "chunk_id": 43,
          "document_id": "2002.06675",
          "start_idx": 14629,
          "end_idx": 15086
        },
        "page_content": " test sets. Thereafter, the total sizes of the development and test sets turns out to be 1585 IPUs spanning 2 hours 23 minutes and 1841 IPUs spanning 2 hours and 48 minutes respectively. The ASR model is trained with the rest data. In the speaker-open condition, all the data except for the test speaker's were used for training As it would be difficult to train the model if all of the data of speaker KM or UT were removed, experiments using their speaker",
        "type": "Document"
      },
      {
        "id": "39d0f45f-40b8-4ec9-9bc0-82bfcda170eb",
        "metadata": {
          "vector_store_key": "1908.07816-5",
          "chunk_id": 46,
          "document_id": "1908.07816",
          "start_idx": 15613,
          "end_idx": 16096
        },
        "page_content": " pairs that have at least one utterance with length greater than 30. We also reduce the frequency of those pairs whose responses appear too many times (the threshold is set to 10 for Cornell, and 5 for DailyDialog), to prevent them from dominating the learning procedure. See Table TABREF25 for the sizes of the training and validation sets. The test set consists of 100 dialogs with four turns. We give more detailed description of how we create the test set in Section SECREF31. We",
        "type": "Document"
      },
      {
        "id": "de28f623-e56b-46af-88a3-78e233bbcff1",
        "metadata": {
          "vector_store_key": "1607.06025-5",
          "chunk_id": 66,
          "document_id": "1607.06025",
          "start_idx": 24769,
          "end_idx": 25338
        },
        "page_content": " were constructed by filtering the generated datasets according to various thresholds. Thus, the generated datasets were constructed to contain enough examples, so that the filtered datasets had at least the number of examples as the original dataset. In the end, all the datasets were trimmed down to the size of the original dataset by selecting the samples sequentially from the beginning until the dataset had the right size. Also, the datasets were filtered so that each of the labels was represented equally. All the models, including classification and discrimin",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which datasets are used?",
    "llm_answer": "Stanford Sentiment Treebank (SST-2), Yelp, Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), Quora Question Pairs (QQP), SQuAD 1.1, SQuAD 2.0, MRQA, SCI (a set of 220 sentences from the scientific literature).\n",
    "context": [
      {
        "id": "19ad6a1f-7f24-400e-9235-38467b0ba391",
        "metadata": {
          "vector_store_key": "1910.00458-7",
          "chunk_id": 51,
          "document_id": "1910.00458",
          "start_idx": 17496,
          "end_idx": 17916
        },
        "page_content": "500). For the sentiment analysis category, we used the Stanford Sentiment Treebank (SST-2) dataset from the GLUE benchmark BIBREF22 (around 60k train examples) and the Yelp dataset (around 430k train examples). For the paraphrase category, three paraphrasing datasets are used from the GLUE benchmark: Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), and Quora Question Pairs (",
        "type": "Document"
      },
      {
        "id": "0d593f74-7474-41cc-9928-ce68ac96f714",
        "metadata": {
          "vector_store_key": "1802.05574-2",
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "fa96ba68-ce4c-4bb6-996e-738362630646",
        "metadata": {
          "vector_store_key": "1701.09123-4",
          "chunk_id": 122,
          "document_id": "1701.09123",
          "start_idx": 46004,
          "end_idx": 46555
        },
        "page_content": " test data differs. Annotation guidelines: The gold annotation of the test data follows different guidelines from the training data. This is usually reflected in different named entity spans. The datasets and languages chosen for these experiments are based on the availability of both previous results and publicly distributed NERC systems to facilitate direct comparison of our system with other approaches. Table TABREF83 specifies the datasets used for each out-of-domain setting and language. Details of each dataset can be found Table TABREF10 .",
        "type": "Document"
      },
      {
        "id": "c45762ac-af29-4220-8179-ad0f77cf9b48",
        "metadata": {
          "vector_store_key": "1705.03261-2",
          "chunk_id": 38,
          "document_id": "1705.03261",
          "start_idx": 13854,
          "end_idx": 14252
        },
        "page_content": " five types. We give the definitions of these types and their example sentences, as shown in table TABREF4 . This standard dataset is made up of training set and testing set. We use the same metrics as in other drug-drug interaction extraction literature BIBREF11 , BIBREF10 , BIBREF25 , BIBREF9 , BIBREF8 , BIBREF12 : the overall precision, recall, and F1 score on testing set. INLINEFORM0 denotes",
        "type": "Document"
      },
      {
        "id": "8f40fad4-4ab6-4c70-b931-df5f59aec2ed",
        "metadata": {
          "vector_store_key": "1910.00458-7",
          "chunk_id": 52,
          "document_id": "1910.00458",
          "start_idx": 17758,
          "end_idx": 18087
        },
        "page_content": "antic Textual Similarity Benchmark (STS-B), and Quora Question Pairs (QQP), which are denoted as \u201cGLUE-Para.\u201d. For the span-based QA, we use the SQuAD 1.1, SQuAD 2.0 , and MRQA which is a joint dataset including six popular span-based QA datasets. Table TABREF23 summarizes the results. We see that sentiment analysis datasets do",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much does it minimally cost to fine-tune some model according to benchmarking framework?",
    "llm_answer": "A few minutes to hours.\n",
    "context": [
      {
        "id": "91cdae72-0638-45f6-8527-6ee483926c18",
        "metadata": {
          "vector_store_key": "2002.05829-1",
          "chunk_id": 12,
          "document_id": "2002.05829",
          "start_idx": 4883,
          "end_idx": 5371
        },
        "page_content": ". After every thousand of pretraining steps, we clone the model for fine-tuning and see if the final performance can reach our cut-off level. When the level is reached, time and cost for pretraining is used for comparison. Models faster or cheaper to pretrain are recommended. For fine-tuning phase, we consider the time and cost each model requires to reach certain multi-task performance fine-tuned from given pretrained models because for each single task with different difficulty and",
        "type": "Document"
      },
      {
        "id": "4e5518dd-7ed1-49b2-b5eb-9663e7eb00e8",
        "metadata": {
          "vector_store_key": "2002.05829-1",
          "chunk_id": 11,
          "document_id": "2002.05829",
          "start_idx": 4581,
          "end_idx": 5107
        },
        "page_content": " relation between model parameter and fine-tuning efficiency and demonstrate consistency of energy efficiency between tasks for different pretrained models. For pretraining phase, the benchmark is designed to favor energy efficient models in terms of time and cost that each model takes to reach certain multi-task performance pretrained from scratch. For example, we keep track of the time and cost of a BERT model pretrained from scratch. After every thousand of pretraining steps, we clone the model for fine-tuning and see",
        "type": "Document"
      },
      {
        "id": "d5f3a977-71b7-4cdf-95ce-be91c26f08f0",
        "metadata": {
          "vector_store_key": "2002.05829-1",
          "chunk_id": 7,
          "document_id": "2002.05829",
          "start_idx": 2969,
          "end_idx": 3417
        },
        "page_content": " If a model is trained from scratch, we consider such model has no pretraining phase but fine-tuned from scratch. Typically pretraining takes several days and hundreds of dollars, according to Table TABREF1. Fine-tuning takes a few minutes to hours, costing a lot less than pretraining phase. Inference takes several milli-seconds to seconds, costing much less than fine-tuning phase. Meanwhile, pretraining is done before fine-tuning once for all,",
        "type": "Document"
      },
      {
        "id": "7b9a8767-e10b-4725-9a47-7aace75158de",
        "metadata": {
          "vector_store_key": "2002.05829-1",
          "chunk_id": 29,
          "document_id": "2002.05829",
          "start_idx": 11715,
          "end_idx": 12170
        },
        "page_content": " For computation-heavy tasks, we adopt the reported resource requirements in the original papers as the pretraining phase baselines. For fine-tuning and inference phase, we conduct extensive experiments on given hardware (GTX 2080Ti GPU) with different model settings as shown in Table TABREF8 and Table TABREF9. We also collect the devlopment set performance with time in fine-tuning to investigate in how the model are fine-tuned for different tasks. In",
        "type": "Document"
      },
      {
        "id": "de902599-d782-472a-a945-e679582bb2f8",
        "metadata": {
          "vector_store_key": "2002.05829-1",
          "chunk_id": 23,
          "document_id": "2002.05829",
          "start_idx": 9318,
          "end_idx": 9766
        },
        "page_content": " Therefore, during model pretraining, after a number of steps, we use the half-pretrained model for fine-tuning and see if the fine-tuned model can reach our cut-off performance. When it does, we count the time and cost in the pretraining process for benchmarking and analysis. For fine-tuning phase, we want to compare the general efficiency of pretrained model reaching cut-off performance on selected dataset. During fine-tuning, we evaluate the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What models are included in baseline benchmarking results?",
    "llm_answer": "Baseline 1, Baseline 2, and Baseline 3.\n",
    "context": [
      {
        "id": "304b5978-d253-43d4-92b8-7dc6acce1bd5",
        "metadata": {
          "vector_store_key": "2002.06053-5",
          "chunk_id": 123,
          "document_id": "2002.06053",
          "start_idx": 45532,
          "end_idx": 46097
        },
        "page_content": " datasets and different evaluation metrics makes the assessment of model performance challenging. Comprehensive benchmarking platforms that can assess the success of different tools are still lacking. A benchmarking environment rigorously brings together the suitable data sets and evaluation methodologies in order to provide a fair comparison between the available tools. Such environments are available for molecule generation task from MOSES BIBREF123 and GuacaMol BIBREF124. MoleculeNet is also a similar attempt to build a benchmarking platform for tasks such",
        "type": "Document"
      },
      {
        "id": "593127ef-0325-4226-be3e-0ed371ce653f",
        "metadata": {
          "vector_store_key": "1804.08782-7",
          "chunk_id": 37,
          "document_id": "1804.08782",
          "start_idx": 13990,
          "end_idx": 14427
        },
        "page_content": " number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 .  For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose",
        "type": "Document"
      },
      {
        "id": "7b9a8767-e10b-4725-9a47-7aace75158de",
        "metadata": {
          "vector_store_key": "2002.05829-1",
          "chunk_id": 29,
          "document_id": "2002.05829",
          "start_idx": 11715,
          "end_idx": 12170
        },
        "page_content": " For computation-heavy tasks, we adopt the reported resource requirements in the original papers as the pretraining phase baselines. For fine-tuning and inference phase, we conduct extensive experiments on given hardware (GTX 2080Ti GPU) with different model settings as shown in Table TABREF8 and Table TABREF9. We also collect the devlopment set performance with time in fine-tuning to investigate in how the model are fine-tuned for different tasks. In",
        "type": "Document"
      },
      {
        "id": "c8c4272e-a38c-48c3-a271-2ada66b9c8b5",
        "metadata": {
          "vector_store_key": "2004.01980-1",
          "chunk_id": 63,
          "document_id": "2004.01980",
          "start_idx": 22218,
          "end_idx": 22603
        },
        "page_content": " all baselines. We use the summarization-related evaluation metrics, i.e., BLEU, ROUGE, CIDEr, and METEOR, to measure how relevant the generated headlines are to the news articles, to some extent, by comparing them to the original human-written headlines. In Table TABREF59, the first row \u201cNHG\u201d shows the performance of the current state-of-the-art summarization model on our data, and",
        "type": "Document"
      },
      {
        "id": "dc50a5cf-b755-41a0-a173-1b561db0ed05",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "It looks like learning to paraphrase questions, a neural scoring model and a answer selection model cannot be trained end-to-end. How are they trained?",
    "llm_answer": "The entire system is trained end-to-end using question-answer pairs as a supervision signal.\n",
    "context": [
      {
        "id": "246a707b-1ced-4013-a9c2-ba023b06ac5b",
        "metadata": {
          "vector_store_key": "1708.06022-1",
          "chunk_id": 9,
          "document_id": "1708.06022",
          "start_idx": 3723,
          "end_idx": 4216
        },
        "page_content": ". A neural scoring model predicts the quality of the generated paraphrases, while learning to assign higher weights to those which are more likely to yield correct answers. The paraphrases and the original question are fed into a QA model that predicts a distribution over answers given the question. The entire system is trained end-to-end using question-answer pairs as a supervision signal. The framework is flexible, it does not rely on specific paraphrase or QA models. In fact, this plug",
        "type": "Document"
      },
      {
        "id": "8b468250-4671-4d25-a1dc-6d93ea228371",
        "metadata": {
          "vector_store_key": "1703.04617-0",
          "chunk_id": 2,
          "document_id": "1703.04617",
          "start_idx": 789,
          "end_idx": 1343
        },
        "page_content": " an end-to-end neural network framework, since we regard question understanding is of importance for such problems. We first introduced syntactic information to help encode questions. We then viewed and modelled different types of questions and the information shared among them as an adaptation problem and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results on our competitive baselines. Recent advance on reading comprehension and question answering has",
        "type": "Document"
      },
      {
        "id": "71949abf-1c48-49cc-94a4-add1151cd635",
        "metadata": {
          "vector_store_key": "1708.06022-1",
          "chunk_id": 15,
          "document_id": "1708.06022",
          "start_idx": 5527,
          "end_idx": 6017
        },
        "page_content": " we first generate candidate paraphrases $H_q$ for question $q$ . Then, a neural scoring model predicts the quality of the generated paraphrases, and assigns higher weights to the paraphrases which are more likely to obtain the correct answers. These paraphrases and the original question simultaneously serve as input to a QA model that predicts a distribution over answers for a given question. Finally, the results of these two models are fused to predict the answer. In the following we",
        "type": "Document"
      },
      {
        "id": "50761d50-be53-40f5-8f77-cef1a7199dfd",
        "metadata": {
          "vector_store_key": "1708.06022-1",
          "chunk_id": 8,
          "document_id": "1708.06022",
          "start_idx": 3178,
          "end_idx": 3723
        },
        "page_content": " not informative enough to model the quality of question paraphrases, or cannot easily generalize to unseen rewrite rules. In this paper, we present a general framework for learning paraphrases for question answering tasks. Given a natural language question, our model estimates a probability distribution over candidate answers. We first generate paraphrases for the question, which can be obtained by one or several paraphrasing systems. A neural scoring model predicts the quality of the generated paraphrases, while learning to assign higher",
        "type": "Document"
      },
      {
        "id": "568b42cb-5770-49cb-b313-b18db222dc85",
        "metadata": {
          "vector_store_key": "1703.04617-0",
          "chunk_id": 1,
          "document_id": "1703.04617",
          "start_idx": 493,
          "end_idx": 995
        },
        "page_content": " of relatively large training datasets (see Section \"Related Work\" for more details) has made it more feasible to train and estimate rather complex models in an end-to-end fashion for these problems, in which a whole model is fit directly with given question-answer tuples and the resulting model has shown to be rather effective. In this paper, we take a closer look at modeling questions in such an end-to-end neural network framework, since we regard question understanding is of importance for such",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much more accurate is the model than the baseline?",
    "llm_answer": "The model outperforms the baseline by 35% in Exact Match and 25% in Goal Match in previously seen environments.  Also, the model makes significantly fewer errors for \"What\" questions.\n",
    "context": [
      {
        "id": "a1974dac-f964-4aeb-a415-a2006317d90a",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 68,
          "document_id": "1810.00663",
          "start_idx": 26280,
          "end_idx": 26763
        },
        "page_content": " room for future work on decoding natural language instructions, our model still outperforms the baselines by a clear margin in new environments. For instance, the difference between our model and the second best model in the Test-New set is about INLINEFORM0 EM and GM. Note that the average number of actions in the ground truth output sequences is 7.07 for the Test-New set. Our model's predictions are just INLINEFORM1 edits off on average from the correct navigation plans. This",
        "type": "Document"
      },
      {
        "id": "31a0a18e-6bb4-4a10-b9fb-bd6e2c40d328",
        "metadata": {
          "vector_store_key": "1912.10435-9",
          "chunk_id": 50,
          "document_id": "1912.10435",
          "start_idx": 18889,
          "end_idx": 19382
        },
        "page_content": " of \"Which\" questions, our model goes wrong 69 times where as the baseline model goes wrong 64 times, a very small numeric difference. However, for the \"What\" questions the baseline model produces incorrect outputs for 776 examples while our model produces 30 fewer incorrect outputs. The reason for this lapse appears to be related to data augmentation where we observed that many a times \"Which\" was backtranslated as \"What\" and vice versa. Thus, the questions in these two classes are mixed",
        "type": "Document"
      },
      {
        "id": "dc50a5cf-b755-41a0-a173-1b561db0ed05",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 61,
          "document_id": "1810.00663",
          "start_idx": 23550,
          "end_idx": 24039
        },
        "page_content": " TABREF28 shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the",
        "type": "Document"
      },
      {
        "id": "eda79a06-6cb8-4f5d-8937-d699acb78fdd",
        "metadata": {
          "vector_store_key": "2001.02380-9",
          "chunk_id": 146,
          "document_id": "2001.02380",
          "start_idx": 53784,
          "end_idx": 54256
        },
        "page_content": ", and therefore prone to penalizing the baseline; many words mean more chances for a random guess to be wrong), model accuracy is substantially better in this scenario, reaching a 40% chance of hitting a signal with only one guess, exceeding 53% with two guesses, and capping at 64% for recall@3, over 20 points above baseline. Finally, the right panel in the figure shows recall when only DMs are considered. In this scenario, a random guess fares very poorly, since most",
        "type": "Document"
      },
      {
        "id": "97be5370-e788-41d2-9b17-3e7739266f06",
        "metadata": {
          "vector_store_key": "2001.11381-3",
          "chunk_id": 138,
          "document_id": "2001.11381",
          "start_idx": 31860,
          "end_idx": 32196
        },
        "page_content": "ados. Dicho lo anterior, el Modelo 1 podr\u00eda ser considerado como nuestro propio baseline. A continuaci\u00f3n presentamos un protocolo de evaluaci\u00f3n manual de los resultados obtenidos. El experimento consisti\u00f3 en la generaci\u00f3n de 15 frases por cada uno de los tres modelos propuestos. Para cada modelo, se consideraron tres queries: $Q=$ {AM",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is new state-of-the-art performance on CoNLL-2009 dataset?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "77fb1b04-2bbc-42c6-8fa9-f19cca2798a2",
        "metadata": {
          "vector_store_key": "1901.09755-3",
          "chunk_id": 64,
          "document_id": "1901.09755",
          "start_idx": 24362,
          "end_idx": 24809
        },
        "page_content": " 2014 training set is substantially larger, as detailed in Table TABREF7 . In fact, the smaller datasets seem to affect more the deep learning approaches (LSTM, WDEmb, RNCRF) where only the MIN and CMLA models obtain similar results to ours, albeit using manually added language-specific annotations. Finally, it would have been interesting to compare MIN, CNN-SenticNet and CMLA with our system on the three ABSA benchmarks, but their systems are",
        "type": "Document"
      },
      {
        "id": "83967b0b-aca8-4bd4-9e24-3ee8aa5916bf",
        "metadata": {
          "vector_store_key": "1701.09123-3",
          "chunk_id": 117,
          "document_id": "1701.09123",
          "start_idx": 44092,
          "end_idx": 44535
        },
        "page_content": "31 and our best CoNLL 2003 model (en-91-18) for comparison. First, we can observe that for every portion of the training set, both our best cluster and en-91-18 model outperform the Illinois NER system. The best cluster results are noteworthy because, as opposed to Illinois NER, it does not use gazetteers or global features for extra performance. These results are mirrored by those obtained for the rest of the languages and datasets. Thus,",
        "type": "Document"
      },
      {
        "id": "ec651bfc-61bb-46ab-a708-6779938b6cdb",
        "metadata": {
          "vector_store_key": "1701.02877-6",
          "chunk_id": 62,
          "document_id": "1701.02877",
          "start_idx": 22362,
          "end_idx": 22802
        },
        "page_content": " data gives little to no improvement, whereas adding positive examples drastically improves performance. Table UID9 shows results with size normalised precision (P), recall (R), and F1-Score (F1). The five lowest P, R and F1 values per method (CRFSuite, Stanford NER, SENNA) are in bold to highlight underperformers. Results for all corpora are summed with macro average. Comparing the different methods, the highest F1 results are achieved",
        "type": "Document"
      },
      {
        "id": "bb9fa132-e5b7-4eeb-b620-74fe74b69fab",
        "metadata": {
          "vector_store_key": "1701.09123-0",
          "chunk_id": 105,
          "document_id": "1701.09123",
          "start_idx": 39639,
          "end_idx": 40079
        },
        "page_content": " art performance with respect to the best system published up to date BIBREF24 using public data.  BIBREF24 also report 78.20 F1 with a model trained with Clark clusters induced using the Huge German Corpus (HGC). Unfortunately, the corpus or the induced clusters were not available. The best system up to date on the CoNLL 2002 dataset, originally published by BIBREF47 , is distributed as part of the Freeling library BIBREF34 . Table TAB",
        "type": "Document"
      },
      {
        "id": "255b2ebd-107f-4f70-9e4e-ec275d5b115f",
        "metadata": {
          "vector_store_key": "1603.04553-1",
          "chunk_id": 37,
          "document_id": "1603.04553",
          "start_idx": 10861,
          "end_idx": 11236
        },
        "page_content": ".93% and 3.01% on CoNLL F1 score over the Stanford system, the winner of the CoNLL 2011 shared task, on the CoNLL 2012 development and test sets, respectively. The improvements on CoNLL F1 score over the Multigraph model are 1.41% and 1.77% on the development and test sets, respectively. Comparing with the MIR model, we obtain significant improvements of 2.62% and 3.02% on",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are two strong baseline methods authors refer to?",
    "llm_answer": "PCA-based symmetric acoustic similarity measure by Lee et al. and a LSTM baseline.\n",
    "context": [
      {
        "id": "593127ef-0325-4226-be3e-0ed371ce653f",
        "metadata": {
          "vector_store_key": "1804.08782-7",
          "chunk_id": 37,
          "document_id": "1804.08782",
          "start_idx": 13990,
          "end_idx": 14427
        },
        "page_content": " number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 .  For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose",
        "type": "Document"
      },
      {
        "id": "95d982c1-df48-40f1-aea4-aa545ace30dc",
        "metadata": {
          "vector_store_key": "1909.02635-8",
          "chunk_id": 23,
          "document_id": "1909.02635",
          "start_idx": 9050,
          "end_idx": 9548
        },
        "page_content": " (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo",
        "type": "Document"
      },
      {
        "id": "899129d5-0dac-4c1e-adc0-4aae82344e56",
        "metadata": {
          "vector_store_key": "1902.00821-9",
          "chunk_id": 83,
          "document_id": "1902.00821",
          "start_idx": 25453,
          "end_idx": 25621
        },
        "page_content": " that the pre-tuning approach is highly effective and outperforms existing baselines or highly competitive with supervised baselines trained from a large-scale dataset.",
        "type": "Document"
      },
      {
        "id": "c38610ec-a58c-4ca6-b548-3b844e3bb607",
        "metadata": {
          "vector_store_key": "1712.03547-0",
          "chunk_id": 38,
          "document_id": "1712.03547",
          "start_idx": 12435,
          "end_idx": 12963
        },
        "page_content": " the baseline method. In this work, we proposed a method for inducing interpretability in KG embeddings using a coherence regularization term. We evaluated the proposed and the baseline method on the interpretability of the learned embeddings. We also evaluated the methods on different KG tasks and compared their performance. We found that the proposed method achieves better interpretability while maintaining comparable performance on KG tasks. As next steps, we plan to evaluate the generalizability of the method with more",
        "type": "Document"
      },
      {
        "id": "17d3256d-56f6-4b9d-983d-669c916f7fae",
        "metadata": {
          "vector_store_key": "1709.08299-2",
          "chunk_id": 23,
          "document_id": "1709.08299",
          "start_idx": 9435,
          "end_idx": 10003
        },
        "page_content": " questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models. The",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many category tags are considered?",
    "llm_answer": "100\n",
    "context": [
      {
        "id": "41b226f4-9a89-4360-b7b9-45836667d1d0",
        "metadata": {
          "vector_store_key": "1707.02377-0",
          "chunk_id": 56,
          "document_id": "1707.02377",
          "start_idx": 20619,
          "end_idx": 21158
        },
        "page_content": " 300,000 wikipedia pages in 100 categories. The 100 categories includes categories under sports, entertainment, literature, and politics etc. Examples of categories include American drama films, Directorial debut films, Major League Baseball pitchers and Sydney Swans players. Body texts (the second paragraph) were extracted for each page as a document. For each category, we select 1,000 documents with unique category label, and 100 documents were used for training and 900 documents for testing. The remaining documents are used as unl",
        "type": "Document"
      },
      {
        "id": "cc94e1b6-cc3d-4064-95d7-4594d54111ff",
        "metadata": {
          "vector_store_key": "2002.08899-7",
          "chunk_id": 43,
          "document_id": "2002.08899",
          "start_idx": 15442,
          "end_idx": 15949
        },
        "page_content": " and 6007 pairs, respectively. There are 9243 tokens in the input dictionary and 9486 tokens in the output dictionary. Figure FIGREF6 (c) shows some weights for four input words. They all highlight the relevant terminal, and syntactic categories that are usually associated with that word. The associated categories typically are either those of that word, the phrases headed by the category of that word, or those that select or are selected by that word. The relevant nonterminal terminology is as follows",
        "type": "Document"
      },
      {
        "id": "45453065-9121-4439-8fee-3f7ccf1a520e",
        "metadata": {
          "vector_store_key": "1909.02776-3",
          "chunk_id": 65,
          "document_id": "1909.02776",
          "start_idx": 25195,
          "end_idx": 25652
        },
        "page_content": " removal, and part of speech tagging is performed using the Hazm library BIBREF43. The majority of features have a range between zero and one. Other features are passed to a min-max scaler to transform into the same range. For the category feature which is nominal, the one-hot-encoding method applied and six flag features used instead. In assigning the target to a sentence, as mentioned in section (SECREF16), the goal is to assign a number between 0 and",
        "type": "Document"
      },
      {
        "id": "676f2c5f-d1fa-4780-8773-cbd4866831b5",
        "metadata": {
          "vector_store_key": "1909.13375-2",
          "chunk_id": 40,
          "document_id": "1909.13375",
          "start_idx": 13084,
          "end_idx": 13439
        },
        "page_content": "B\\,O}$,$\\quad $ $\\mathtt {O\\,B\\,O\\,B}$. The number of correct tag sequences can be expressed by where $s$ is the number of spans in the answer and $\\#_i$ is the number of times the $i^\\text{th}$ span appears in the text. For questions with a reasonable amount of correct tag sequences, we generate all of them before the training starts. However, there is",
        "type": "Document"
      },
      {
        "id": "a5ef44c5-13bc-4dc3-8bde-e0a34519d3b4",
        "metadata": {
          "vector_store_key": "1703.09684-2",
          "chunk_id": 29,
          "document_id": "1703.09684",
          "start_idx": 11091,
          "end_idx": 11574
        },
        "page_content": " and the answer had to be a small countable integer (1-16). The other categories were determined using regular expressions. For example, a question of the form `Are feeling ?' was classified as sentiment understanding and `What is to the right of/left of/ behind the ?' was classified as positional reasoning. Similarly, `What <OBJECT CATEGORY> is in the image?' and similar templates were used to populate subordinate object recognition questions. This method was used for questions",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What domain does the dataset fall into?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "06a8582f-cc3a-48e8-95fe-cca8b57ae377",
        "metadata": {
          "vector_store_key": "1612.06897-4",
          "chunk_id": 39,
          "document_id": "1612.06897",
          "start_idx": 12548,
          "end_idx": 13016
        },
        "page_content": "domain data. Figure 2 shows the learning curve of the ensemble of the baseline and the continue model for different sizes of in-domain training data. The used in-domain data is a randomly selected subset of the entire pool of the in-domain data available to us. We also report the result when all of the in-domain data in the pool is used. As shown in Figure 2 the translation quality of the out-of-domain test set only degrades slightly for all the different sizes of",
        "type": "Document"
      },
      {
        "id": "407faed0-32f9-4c97-83d1-f7d24f137788",
        "metadata": {
          "vector_store_key": "1907.11499-2",
          "chunk_id": 15,
          "document_id": "1907.11499",
          "start_idx": 6351,
          "end_idx": 6867
        },
        "page_content": " at hand. For example, the definition of the \u201cLifestyle\u201d domain is \u201cthe interests, opinions, behaviors, and behavioral orientations of an individual, group, or culture\u201d. Figure FIGREF5 provides an overview of our Domain Detection Network, which we call DetNet. The model comprises two modules; an encoder learns representations for words and sentences whilst incorporating prior domain information; a detector generates domain scores for words, sentences, and documents by selectively attending to previously encoded",
        "type": "Document"
      },
      {
        "id": "93868b0b-4b9f-4465-9f7e-c688fa1a5347",
        "metadata": {
          "vector_store_key": "1701.03214-0",
          "chunk_id": 11,
          "document_id": "1701.03214",
          "start_idx": 4004,
          "end_idx": 4439
        },
        "page_content": "-of-domain data is used for the in-domain data; while for the \u201cmulti domain\u201d and \u201cmixed fine tuning\u201d methods, we use a vocabulary obtained from the mixed in-domain and out-of-domain data for all the training stages. We conducted NMT domain adaptation experiments in two different settings as follows: Chinese-to-English translation was the focus of the high quality in-domain corpus setting. We utilized the resource rich patent out-of",
        "type": "Document"
      },
      {
        "id": "2be4c778-7459-49cd-97bf-8ebed4689ece",
        "metadata": {
          "vector_store_key": "1811.01734-9",
          "chunk_id": 0,
          "document_id": "1811.01734",
          "start_idx": 0,
          "end_idx": 376
        },
        "page_content": " Domain shift is a fundamental problem in machine learning, that has attracted a lot of attention in the natural language processing and vision communities BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 . To understand and address this problem, generated by the lack of labeled data in a target domain, researchers",
        "type": "Document"
      },
      {
        "id": "bea618fb-90d6-4dc7-8e60-c8e7bcf0b45f",
        "metadata": {
          "vector_store_key": "1701.03214-2",
          "chunk_id": 15,
          "document_id": "1701.03214",
          "start_idx": 5368,
          "end_idx": 5787
        },
        "page_content": " (essentially open) in-domain data. The scientific domain MT was conducted on the Chinese-Japanese paper excerpt corpus (ASPEC-CJ) BIBREF11 , which is one subtask of the workshop on Asian translation (WAT) BIBREF15 . The ASPEC-CJ task uses 672315, 2090, and 2107 sentences for training, development, and testing, respectively. The Wikipedia domain task was conducted on a Chinese-Japanese corpus automatically extracted",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are EAC evaluated?",
    "llm_answer": "Qualitative assessment using ISO 9241 (efficiency, effectiveness, satisfaction) and quantitative analysis (two techniques).\n",
    "context": [
      {
        "id": "b4985197-5d10-4e7a-a281-cd3f41fd772a",
        "metadata": {
          "vector_store_key": "1906.09774-2",
          "chunk_id": 50,
          "document_id": "1906.09774",
          "start_idx": 20313,
          "end_idx": 20820
        },
        "page_content": " and found several resources from the old one such as LIWC and Emolex to the new one, including DepecheMood and EmoWordNet. In the final part, we gather information about how to evaluate the performance of EAC, and we can classify the approach into two techniques, including qualitative and quantitative assessment. For qualitative assessment, most studies used ISO 9241, which covers several aspects such as efficiency, effectiveness, and satisfaction. While in quantitative analysis, two techniques can be",
        "type": "Document"
      },
      {
        "id": "d29ce5a1-6200-4f5e-8808-81a38602d8f2",
        "metadata": {
          "vector_store_key": "1906.09774-6",
          "chunk_id": 47,
          "document_id": "1906.09774",
          "start_idx": 19203,
          "end_idx": 19656
        },
        "page_content": " how to evaluate EAC performance. The rise of EAC was started by Parry, which uses a simple rule-based approach. Now, most of EAC are built by using a neural-based approach, by exploiting emotion classifier to detect emotion contained in the text. In the modern era, the development of EAC gains more attention since Emotion Generation Challenge shared task on NLPCC 2017. In this era, most EAC is developed by adopting encoder-decoder architecture with",
        "type": "Document"
      },
      {
        "id": "8bc052b2-ab69-4d67-a86a-d0bc1f2a0ba1",
        "metadata": {
          "vector_store_key": "1906.09774-6",
          "chunk_id": 16,
          "document_id": "1906.09774",
          "start_idx": 6366,
          "end_idx": 6844
        },
        "page_content": " section, we will try to review previous works which focus on EAC development. Table TABREF10 summarizes this information includes the objective and exploited approach of each work. In early development, EAC is designed by using a rule-based approach. However, in recent years mostly EAC exploit neural-based approach. Studies in EAC development become a hot topic start from 2017, noted by the first shared task in Emotion Generation Challenge on NLPCC 2017 BIBREF31 . Based on",
        "type": "Document"
      },
      {
        "id": "d680ee1c-079a-4633-9459-3eacd7b1b786",
        "metadata": {
          "vector_store_key": "1910.10408-7",
          "chunk_id": 34,
          "document_id": "1910.10408",
          "start_idx": 12581,
          "end_idx": 13061
        },
        "page_content": "Enc) models are evaluated in a length matching condition, i.e. output length has to match input length. We report the relative (Rel) and absolute (Abs) strategies of the approach as discussed in Section SECREF10. In the small data condition, we additionally evaluated how the fine-tuning strategy compares with a model trained from scratch. In the large data condition, we added a setting that combines both the length-token and length-encoding strategies. To evaluate all models'",
        "type": "Document"
      },
      {
        "id": "5b953cb7-9114-48c2-b577-fa0a59e4afed",
        "metadata": {
          "vector_store_key": "1804.03396-1",
          "chunk_id": 93,
          "document_id": "1804.03396",
          "start_idx": 29645,
          "end_idx": 30157
        },
        "page_content": " $= 93.53$ and the IE measurements can be found in Figure 3 . Most of the error cases are the fourth case which is acceptable by human annotators. Note that our framework takes the whole document as the input while the baseline systems take the individual sentence as the input, which means the experiment setting is much more difficult for our framework. Finally, we perform a human evaluation on our QA4IE benchmark to verify the reliability of former experiments. The evaluation metrics are as follows: Triple",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is triangulation?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "1f56cde6-b1e4-4cff-88f4-58fd3f2cbee2",
        "metadata": {
          "vector_store_key": "1601.01705-6",
          "chunk_id": 4,
          "document_id": "1601.01705",
          "start_idx": 1745,
          "end_idx": 2146
        },
        "page_content": " is the bird? might be answered in two steps: first, \u201cwhere is the bird?\u201d (fig:examplesa), second, \u201cwhat color is that part of the image?\u201d (fig:examplesc). This first step, a generic module called find, can be expressed as a fragment of a neural network that maps from image features and a lexical item (here bird) to a distribution over pixels. This operation is commonly referred to as the attention",
        "type": "Document"
      },
      {
        "id": "eda69527-12c6-40bb-817f-a9d2187c2fcb",
        "metadata": {
          "vector_store_key": "1705.03151-6",
          "chunk_id": 43,
          "document_id": "1705.03151",
          "start_idx": 16622,
          "end_idx": 17163
        },
        "page_content": " language labels), leading to a strong DNN model for phonetic feature extraction. Importantly, phone discrimination and language identification are naturally correlated (from our phonetic perspective), which means that the phonetic features learned with the strong phone/word supervision involves rich information suitable for LID. This is an example of transfer learning, where a related task (i.e., phone discrimination) is used to learn features for another task (LID). The PTN approach also involves another two transfer learning schemes",
        "type": "Document"
      },
      {
        "id": "6b8df844-0ad1-415a-a973-b82a3c84701b",
        "metadata": {
          "vector_store_key": "1807.08204-0",
          "chunk_id": 24,
          "document_id": "1807.08204",
          "start_idx": 7478,
          "end_idx": 7955
        },
        "page_content": " a nearest neighbour search problem. In this work, we use ANNS during the forward pass for considering only the most promising proof paths during the construction of the neural network. From ex:inference, we can see that the inference problem can be reduced to a nearest neighbour search problem. Given a query $g$ , the problem is finding the fact(s) in $\\mathcal {K}$ that maximise the unification score. This represents a computational bottleneck, since it is very costly to",
        "type": "Document"
      },
      {
        "id": "e8b646e6-0113-4b48-866e-a79d9c43422f",
        "metadata": {
          "vector_store_key": "1909.00183-2",
          "chunk_id": 47,
          "document_id": "1909.00183",
          "start_idx": 18176,
          "end_idx": 18644
        },
        "page_content": " BIBREF36, spectral sparsification BIBREF37, or the Relaxed Minimum Spanning Tree (RMST) BIBREF38. We have experimented with some of those methods and obtained comparable results. A detailed comparison of sparsification methods as well as the choice of distance in defining the similarity matrix $\\hat{S}$ is left for future work. Community detection encompasses various graph partitioning approaches which aim to find `good' partitions into subgraphs (or communities)",
        "type": "Document"
      },
      {
        "id": "08ad21a4-09d6-4c2b-a12d-fff691dad411",
        "metadata": {
          "vector_store_key": "1911.11744-0",
          "chunk_id": 32,
          "document_id": "1911.11744",
          "start_idx": 12320,
          "end_idx": 12859
        },
        "page_content": " low image resolution of the input image, preventing the network from reliably distinguishing the object shapes. In general, our approach is able to actuate the robot with an target error well below 5cm, given the target was correctly identified. In this work, we presented an imitation learning approach combining language, vision, and motion. A neural network architecture called Multimodal Policy Network was introduced which is able to learn the cross-modal relationships in the training data and achieve high generalization and disamb",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What languages do they use?",
    "llm_answer": "English, Spanish, French, German, Russian, Italian, Amharic, Arabic, Hindi, Somali, Basque, French, German, Hungarian, Italian, Navajo, Turkish, Quechua, English, Spanish, Zulu,  Russian, Polish, Norwegian, Romanian, Dutch, Czech, Turkish, Thai, Urdu, and potentially others from BIBREF21/22.\n",
    "context": [
      {
        "id": "1e39cb37-36b3-4b7c-8fff-408991f75381",
        "metadata": {
          "vector_store_key": "1909.09270-4",
          "chunk_id": 37,
          "document_id": "1909.09270",
          "start_idx": 13903,
          "end_idx": 14242
        },
        "page_content": "2003 shared tasks BIBREF21, BIBREF22. These are taken from newswire text, and have labelset of Person, Organization, Location, Miscellaneous. The remaining four languages come from the LORELEI project BIBREF23. These languages are: Amharic (amh: LDC2016E87), Arabic (ara: LDC2016E89), Hindi (hin: LDC2017E62), and Somali (som: LDC2016E91).",
        "type": "Document"
      },
      {
        "id": "e90f75a5-f326-4202-82c5-878736af8308",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 51,
          "document_id": "1910.04269",
          "start_idx": 19553,
          "end_idx": 20023
        },
        "page_content": ", the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Kat",
        "type": "Document"
      },
      {
        "id": "d16841ee-127f-473f-af88-1d373637d8c0",
        "metadata": {
          "vector_store_key": "1910.05456-8",
          "chunk_id": 5,
          "document_id": "1910.05456",
          "start_idx": 1974,
          "end_idx": 2423
        },
        "page_content": " on the L1, i.e., the \"native language\", in neural network models. To this goal, we select a diverse set of eight source languages from different language families \u2013 Basque, French, German, Hungarian, Italian, Navajo, Turkish, and Quechua \u2013 and three target languages \u2013 English, Spanish and Zulu. We pretrain a neural sequence-to-sequence architecture on each of the source languages and then fine-tune the resulting models on small datasets in each",
        "type": "Document"
      },
      {
        "id": "c029d921-86ad-43ab-be82-67e2b7093969",
        "metadata": {
          "vector_store_key": "1911.03562-6",
          "chunk_id": 47,
          "document_id": "1911.03562",
          "start_idx": 17539,
          "end_idx": 18028
        },
        "page_content": " as Russian, Polish, Norwegian, Romanian, Dutch, and Czech) and Asian languages (such as Turkish, Thai, and Urdu). Most of the well-represented languages are from the Indo-European language family. Yet, even in the limited landscape of the most common 122 languages, vast swathes are barren with inattention. Notable among these is the extremely low representation of languages from Africa, languages from non-Indo-European language families, and Indigenous languages from around the world",
        "type": "Document"
      },
      {
        "id": "b0d78a46-0cfa-4367-a963-2a9a8d82c9a4",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 18,
          "document_id": "1910.04269",
          "start_idx": 6853,
          "end_idx": 7342
        },
        "page_content": " comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How they evaluate their approach?",
    "llm_answer": "Evaluation using cross-domain scenarios, comparing estimates with actual attack effects, studying adversarial attack impact on evaluation metrics, exploring application to KG representation interpretability, detecting incorrect triples, and leave-one-out evaluation.  Human evaluation of NLG models is also used.\n",
    "context": [
      {
        "id": "b3eb1308-3e01-4903-be04-528f4258d508",
        "metadata": {
          "vector_store_key": "2002.11268-7",
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "be8b40f0-56ba-4775-9ef5-8844908beaec",
        "metadata": {
          "vector_store_key": "1905.00563-4",
          "chunk_id": 47,
          "document_id": "1905.00563",
          "start_idx": 13447,
          "end_idx": 13944
        },
        "page_content": " evaluate by ( \"Influence Function vs \" ) comparing estimate with the actual effect of the attacks, ( \"Robustness of Link Prediction Models\" ) studying the effect of adversarial attacks on evaluation metrics, ( \"Interpretability of Models\" ) exploring its application to the interpretability of KG representations, and ( \"Finding Errors in Knowledge Graphs\" ) detecting incorrect triples. To evaluate the quality of our approximations and compare with influence function (IF), we conduct leave one",
        "type": "Document"
      },
      {
        "id": "67c7bdd9-8926-4b32-b3b7-4b1914e74192",
        "metadata": {
          "vector_store_key": "1601.02543-1",
          "chunk_id": 2,
          "document_id": "1601.02543",
          "start_idx": 845,
          "end_idx": 1419
        },
        "page_content": " successful and unsuccessful transactions. This evaluation is then used to identifying any further improvement in the speech recognition based solution to better the overall transaction completion rates. This process of evaluation is both time consuming and expensive. For evaluation one needs to identify a set of users and also identify the set of actual usage situations and perform the test. It is also important that the set of users are able to use the system with ease meaning that even in the test conditions the performance of the system, should be good, while this",
        "type": "Document"
      },
      {
        "id": "62357a4c-d387-42b6-9ded-cfec39e88f26",
        "metadata": {
          "vector_store_key": "2002.05058-1",
          "chunk_id": 68,
          "document_id": "2002.05058",
          "start_idx": 26990,
          "end_idx": 27517
        },
        "page_content": " NLG evaluation. We further investigate the impact of imperfect metrics on training NLG models. As described in the human evaluation procedure, we perform 10 runs to test the reliability of each metric when used to perform hyperparameter tuning and early-stopping respectively. In each run, we select the best hyperparameter combination or early-stopping checkpoint based on each of the five compared metrics. Human evaluation is then employed to identify the best choice. We evaluate the performance of each metric by how many",
        "type": "Document"
      },
      {
        "id": "e2194e32-f011-4118-b3bf-35fb153b3017",
        "metadata": {
          "vector_store_key": "1804.08186-0",
          "chunk_id": 275,
          "document_id": "1804.08186",
          "start_idx": 98870,
          "end_idx": 99381
        },
        "page_content": " evaluation in research to date. In this section, we examine the literature more closely, providing a broad overview of the evaluation metrics that have been used, as well as the experimental settings in which research has been evaluated. The most common approach is to treat the task as a document-level classification problem. Given a set of evaluation documents, each having a known correct label from a closed set of labels (often referred to as the \u201cgold-standard\u201d), and a predicted label for each document",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How large is the corpus?",
    "llm_answer": "There are multiple corpora of varying sizes mentioned.  No single \"corpus\" size is definitively stated.\n",
    "context": [
      {
        "id": "2a50498c-e7d6-4306-b2ba-3b5dab7d3598",
        "metadata": {
          "vector_store_key": "1909.00183-3",
          "chunk_id": 34,
          "document_id": "1909.00183",
          "start_idx": 13387,
          "end_idx": 13902
        },
        "page_content": " of different sizes and content with different sets of hyper-parameters. . In particular, we checked the effect of corpus size by training Doc2Vec models on the full 13 million NRLS records and on randomly sampled subsets of 1 million and 2 million records. Since our target analysis has heavy medical content and specific use of words, we also tested the importance of the training corpus by generating an additional Doc2Vec model using a set of 5 million articles from the English Wikipedia representing standard,",
        "type": "Document"
      },
      {
        "id": "d54826a9-8d40-49e0-9e1e-52b33bc98304",
        "metadata": {
          "vector_store_key": "1709.05404-8",
          "chunk_id": 40,
          "document_id": "1709.05404",
          "start_idx": 14931,
          "end_idx": 15431
        },
        "page_content": " consists of 582 posts per class (1,164 posts in total). To recap, Table TABREF10 summarizes the total number of posts for each subset of our final corpus. Our primary goal is not to optimize classification results, but to explore how results vary across different subcorpora and corpus properties. We also aim to demonstrate that the quality of our corpus makes it more straightforward to achieve high classification performance. We apply both supervised learning using SVM (from Scikit-Learn BIBREF",
        "type": "Document"
      },
      {
        "id": "d7cfe2bf-feec-4faa-bfe2-cad5cf0452e6",
        "metadata": {
          "vector_store_key": "2001.08051-2",
          "chunk_id": 40,
          "document_id": "2001.08051",
          "start_idx": 15428,
          "end_idx": 15968
        },
        "page_content": " assigned either to training or evaluation sets, with proportions of $\\frac{2}{3}$ and $\\frac{1}{3}$, respectively; then training and evaluation lists were built, accordingly. Table reports statistics from the spoken data set. The id All identifies the whole data set, while Clean defines the subset in which sentences containing background voices, incomprehensible speech and word fragments were excluded. From the above description it appears that the corpus can be effectively used in many research directions. The spoken corpus features",
        "type": "Document"
      },
      {
        "id": "ddf4997a-3427-45af-aa13-0b85dd057ad1",
        "metadata": {
          "vector_store_key": "1603.04513-2",
          "chunk_id": 76,
          "document_id": "1603.04513",
          "start_idx": 24922,
          "end_idx": 25308
        },
        "page_content": "NA. Size: 130,000 word embeddings; training corpus: Wikipedia. Note that we use their 50-dimensional embeddings. (v) Word2Vec. It has no 50-dimensional embeddings available online. We use released code to train skip-gram on English Gigaword Corpus BIBREF26 with setup: window size 5, negative sampling, sampling rate $10^{-3}$ , threads 12. It is worth emphasizing that above embeddings",
        "type": "Document"
      },
      {
        "id": "53cda549-1215-4c35-a4f5-9a45363de587",
        "metadata": {
          "vector_store_key": "1909.00279-0",
          "chunk_id": 37,
          "document_id": "1909.00279",
          "start_idx": 13718,
          "end_idx": 14228
        },
        "page_content": " models, please refer to supplementary materials. Training and Validation Sets We collected a corpus of poems and a corpus of vernacular literature from online resources. The poem corpus contains 163K quatrain poems from Tang Poems and Song Poems, the vernacular literature corpus contains 337K short paragraphs from 281 famous books, the corpus covers various literary forms including prose, fiction and essay. Note that our poem corpus and a vernacular corpus are not aligned. We further split the two corpor",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How large is the dataset?",
    "llm_answer": "The answer is not found in the context.  While various dataset sizes are mentioned, there's no single \"dataset\" size given.\n",
    "context": [
      {
        "id": "0723454a-4062-463e-972a-81a58c6a504e",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "9270562a-8e53-4e1d-b760-74d8fd0bc872",
        "metadata": {
          "vector_store_key": "1905.06566-7",
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "2e1622b2-1517-4239-8581-693f3aece329",
        "metadata": {
          "vector_store_key": "1711.02013-3",
          "chunk_id": 55,
          "document_id": "1711.02013",
          "start_idx": 20658,
          "end_idx": 21116
        },
        "page_content": "ings, between recurrent layers, and on recurrent states were (0.7, 0.5, 0.5) respectively. dataset contains 17M training tokens and has a vocabulary size of 44k words. The dataset is partitioned into a training set (first 99M characters) and a development set (last 1M characters) that is used to report performance. As this dataset contains various articles from Wikipedia, the longer term information (such as current topic) plays a bigger role than in the",
        "type": "Document"
      },
      {
        "id": "0d593f74-7474-41cc-9928-ce68ac96f714",
        "metadata": {
          "vector_store_key": "1802.05574-2",
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "5b7e9180-5d63-439a-abc5-b7aa25b1e99e",
        "metadata": {
          "vector_store_key": "1804.08186-0",
          "chunk_id": 294,
          "document_id": "1804.08186",
          "start_idx": 106599,
          "end_idx": 107101
        },
        "page_content": " whether there is a universal quantity of data that is \u201cenough\u201d for any language, rather this amount appears to be affected by the particular set of languages as well as the domain of the data. The breakdown presented by BIBREF32 shows that with less than 100KB per language, there are some languages where classification accuracy is near perfect, whereas there are others where it is very poor. Another aspect that is frequently reported on is how long a sample of text needs to be before its language",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What was their perplexity score?",
    "llm_answer": "The answer is not found in the context.  While perplexity scores are discussed and referenced to tables, no specific score is given.\n",
    "context": [
      {
        "id": "28e8d703-f3c3-4cae-a595-da4bfcad4522",
        "metadata": {
          "vector_store_key": "1908.07816-4",
          "chunk_id": 58,
          "document_id": "1908.07816",
          "start_idx": 20270,
          "end_idx": 20712
        },
        "page_content": " the perplexity scores obtained by the three models on the two validation sets and the test set. As shown in the table, MEED achieves the lowest perplexity score on all three sets. We also conducted t-test on the perplexity obtained, and results show significant improvements (with $p$-value $<0.05$). Table TABREF34, TABREF35 and TABREF35 summarize the human evaluation results on the responses' grammatical correctness, contextual coherence",
        "type": "Document"
      },
      {
        "id": "a288c3fa-a09a-4d5e-9361-e1cc48c0b421",
        "metadata": {
          "vector_store_key": "1908.07816-4",
          "chunk_id": 57,
          "document_id": "1908.07816",
          "start_idx": 20024,
          "end_idx": 20472
        },
        "page_content": " or not the response is context sensitive to the previous dialog history; (3) emotional appropriateness\u2014whether or not the response conveys the right emotion and feels as if it had been produced by a human. For each criterion, the raters gave scores of either 0, 1 or 2, where 0 means bad, 2 means good, and 1 indicates neutral. Table TABREF34 gives the perplexity scores obtained by the three models on the two validation sets and the test set. As",
        "type": "Document"
      },
      {
        "id": "1f6d3a5b-867d-40f4-b223-203963a679cc",
        "metadata": {
          "vector_store_key": "1909.00578-3",
          "chunk_id": 24,
          "document_id": "1909.00578",
          "start_idx": 7983,
          "end_idx": 8378
        },
        "page_content": " the sentence-level perplexity of each summary: where $p(s_i|s_{i-1})$ is the probability that BERT assigns to the sequence of sentences $\\left< s_{i-1}, s \\right>$, and $n$ is the number of sentences in the peer summary. To evaluate our methods for a particular $\\mathcal {Q}$, we calculate the average of the predicted scores for the summaries of each particular contestant, and the average of",
        "type": "Document"
      },
      {
        "id": "b7c31cae-20c9-4579-ba0f-912b7994eeb8",
        "metadata": {
          "vector_store_key": "1811.00942-3",
          "chunk_id": 19,
          "document_id": "1811.00942",
          "start_idx": 6450,
          "end_idx": 6778
        },
        "page_content": "/3$ for all $P(\\text{``choo''}) \\le 0.5$ ; thus, perplexity can drop as low as 2 without affecting recall. We conducted our experiments on Penn Treebank (PTB; BIBREF12 ) and WikiText-103 (WT103; BIBREF13 ). Preprocessed by BIBREF14 , PTB contains 887K tokens for training, 70K for validation, and 78K for test, with a vocabulary",
        "type": "Document"
      },
      {
        "id": "79b878ed-6254-4c79-9c13-d6cd8b4efa2f",
        "metadata": {
          "vector_store_key": "2004.03744-5",
          "chunk_id": 61,
          "document_id": "2004.03744",
          "start_idx": 22301,
          "end_idx": 22694
        },
        "page_content": "55% on the test set. As reported in Table TABREF30, the overall PaE-BUTD-VE system achieves 69.40% balanced accuracy on the test set of e-SNLI-VE-2.0, which is a 3% decrease from the non-explanatory BUTD counterpart (72.52%). However, by setting $\\alpha $ to zero and selecting the model that gives the best perplexity per word at validation, the quality of explanation significantly increased",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What parallel corpus did they use?",
    "llm_answer": "Parallel corpora from the shared translation task of WMT'15 and WMT'16.\n",
    "context": [
      {
        "id": "8f7f3f3f-8638-4300-beba-dc849afcf578",
        "metadata": {
          "vector_store_key": "1704.00253-6",
          "chunk_id": 20,
          "document_id": "1704.00253",
          "start_idx": 6591,
          "end_idx": 7101
        },
        "page_content": " parallel corpus. In their work, the pseudo parallel corpus was employed in fine-tuning the target-specific attention mechanism of trained multi-way multilingual NMT BIBREF17 models, which enabled zero-resource NMT between the source and target languages. Lastly, synthetic sentence pairs have been utilized to enrich the training examples having rare or unknown translation lexicons BIBREF4 . As described in the previous section, synthetic parallel data have been widely used to boost the performance of NMT.",
        "type": "Document"
      },
      {
        "id": "53cda549-1215-4c35-a4f5-9a45363de587",
        "metadata": {
          "vector_store_key": "1909.00279-0",
          "chunk_id": 37,
          "document_id": "1909.00279",
          "start_idx": 13718,
          "end_idx": 14228
        },
        "page_content": " models, please refer to supplementary materials. Training and Validation Sets We collected a corpus of poems and a corpus of vernacular literature from online resources. The poem corpus contains 163K quatrain poems from Tang Poems and Song Poems, the vernacular literature corpus contains 337K short paragraphs from 281 famous books, the corpus covers various literary forms including prose, fiction and essay. Note that our poem corpus and a vernacular corpus are not aligned. We further split the two corpor",
        "type": "Document"
      },
      {
        "id": "0fcf38db-8036-49e9-9319-5de14182cf56",
        "metadata": {
          "vector_store_key": "1709.01256-7",
          "chunk_id": 53,
          "document_id": "1709.01256",
          "start_idx": 19942,
          "end_idx": 20473
        },
        "page_content": "e.g. addition, deletion or replacement), and save them as separate documents. These documents become revisions of the original documents. We started from an initial corpus that did not have revisions, and kept adding new documents and revising existing documents. Similar to a file system, at any moment new documents could be added and/or some of the current documents could be revised. The revision operations we used were deletion, addition and replacement of words, sentences, paragraphs, section names and document titles. The",
        "type": "Document"
      },
      {
        "id": "44bd656b-fddf-456d-b89a-2522fb4bcf99",
        "metadata": {
          "vector_store_key": "1704.00253-6",
          "chunk_id": 22,
          "document_id": "1704.00253",
          "start_idx": 7425,
          "end_idx": 7925
        },
        "page_content": " ground truth parallel corpus is established, the trained model can be improved by retraining or fine-tuning using the real parallel data. For a given translation task, we classify the existing pseudo parallel data into the following groups: Source-originated: The source sentences are from a real corpus, and the associated target sentences are synthetic. The corpus can be formed by automatically translating a source-side monolingual corpus into the target language BIBREF4 , BIBREF1 . It can also",
        "type": "Document"
      },
      {
        "id": "544a5729-15bd-4791-9613-86c42e288a64",
        "metadata": {
          "vector_store_key": "1704.00253-1",
          "chunk_id": 34,
          "document_id": "1704.00253",
          "start_idx": 12296,
          "end_idx": 12697
        },
        "page_content": "-Fr parallel corpus BIBREF19 . A beam of size 5 is used to generate synthetic sentences. Lastly, to match the size of the training data, PSEUDOmix is established by randomly sampling half of each Fr*-De and Fr-De* corpus and mixing them together. We use the parallel corpora from the shared translation task of WMT'15 and WMT'16 BIBREF27 . Using the same pivot-based technique as the previous task, Cs",
        "type": "Document"
      }
    ]
  },
  {
    "query": "In which setting they achieve the state of the art?",
    "llm_answer": "SemEval-2015 and SemEval-2016.\n",
    "context": [
      {
        "id": "8f559cb6-96ff-446d-aaf3-7673d4a1d24e",
        "metadata": {
          "vector_store_key": "1908.02402-7",
          "chunk_id": 101,
          "document_id": "1908.02402",
          "start_idx": 29500,
          "end_idx": 29733
        },
        "page_content": "-the-art models, while at the same time providing a more practical solution for real-world applications. We would like to thank Alexandros Papangelis, Janice Lam, Stefan Douglas Webb and SIGDIAL reviewers for their valuable comments.",
        "type": "Document"
      },
      {
        "id": "b3eb1308-3e01-4903-be04-528f4258d508",
        "metadata": {
          "vector_store_key": "2002.11268-7",
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "817a9fbd-0ad5-4350-9d90-c039e237a296",
        "metadata": {
          "vector_store_key": "1710.01492-5",
          "chunk_id": 53,
          "document_id": "1710.01492",
          "start_idx": 19472,
          "end_idx": 19941
        },
        "page_content": " particular, the best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks BIBREF53 , BIBREF54 . Conversely, kernel machines seem to be less frequently used than in the past, and the use of learning methods other than the ones mentioned above is at this point scarce. All these models are examples of supervised learning as they need labeled training data. Semi-supervised learning. We should note two things about the use of deep neural",
        "type": "Document"
      },
      {
        "id": "577fb06e-9340-4422-93f3-91bc89574aee",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 43,
          "document_id": "2001.00137",
          "start_idx": 16781,
          "end_idx": 17197
        },
        "page_content": " the online platforms. Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector",
        "type": "Document"
      },
      {
        "id": "77b840f6-2381-42a1-afa6-8bfe2e727051",
        "metadata": {
          "vector_store_key": "1910.08210-9",
          "chunk_id": 45,
          "document_id": "1910.08210",
          "start_idx": 17105,
          "end_idx": 17620
        },
        "page_content": " variant is able to solve the tasks\u2014it is the combination of ablated features that enables to win consistently. Qualitatively, the ablated variants converge to locally optimum policies in which the agent often picks up a random item and then attacks the correct monster, resulting in a $\\sim 50$% win rate. Table FIGREF29 shows that all models, with the exception of the CNN baseline, generalise to new evaluation environments with dynamics and world configurations not seen during training, with outperforming FiLM",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What they formulate the question generation as?",
    "llm_answer": "Automatically generating questions from various inputs like text, databases, or semantic representations.\n",
    "context": [
      {
        "id": "34bbc3de-0855-49a9-87eb-906d9255fa93",
        "metadata": {
          "vector_store_key": "1905.08949-6",
          "chunk_id": 0,
          "document_id": "1905.08949",
          "start_idx": 0,
          "end_idx": 442
        },
        "page_content": "Question Generation (QG) concerns the task of \u201cautomatically generating questions from various inputs such as raw text, database, or semantic representation\" BIBREF0 . People have the ability to ask rich, creative, and revealing questions BIBREF1 ; e.g., asking Why did Gollum betray his master Frodo Baggins? after reading the fantasy novel The Lord of the Rings. How can machines be endowed with the ability to ask relevant and to-the-point",
        "type": "Document"
      },
      {
        "id": "6202f403-8fed-4efc-b850-80b155c068f9",
        "metadata": {
          "vector_store_key": "1905.08949-5",
          "chunk_id": 44,
          "document_id": "1905.08949",
          "start_idx": 15932,
          "end_idx": 16357
        },
        "page_content": "-question should be triggered for answer \u201cthe end of the Mexican War\" while a why-question is generated by the model. A few works BIBREF49 , BIBREF53 considered question word generation separately in model design.  BIBREF49 proposed to first generate a question template that contains question word (e.g., \u201chow to #\", where # is the placeholder), before generating the rest of the question. To this end, they train two Seq2Se",
        "type": "Document"
      },
      {
        "id": "536d7b37-3d43-4781-beea-c00f9bec22dd",
        "metadata": {
          "vector_store_key": "1905.08949-5",
          "chunk_id": 10,
          "document_id": "1905.08949",
          "start_idx": 3681,
          "end_idx": 4181
        },
        "page_content": " by applying syntactic or semantic parsing, respectively, to obtain intermediate symbolic representations. Question construction then converts intermediate representations to a natural language question, taking either a tranformation- or template-based approach. The former BIBREF17 , BIBREF18 , BIBREF13 rearranges the surface form of the input sentence to produce the question; the latter BIBREF19 , BIBREF20 , BIBREF21 generates questions from pre-defined question templates. Unfortunately, such Q",
        "type": "Document"
      },
      {
        "id": "bfe2b894-c3ab-4bf1-b472-a2c31e9ba22a",
        "metadata": {
          "vector_store_key": "1806.04524-3",
          "chunk_id": 7,
          "document_id": "1806.04524",
          "start_idx": 2824,
          "end_idx": 3312
        },
        "page_content": " adapt. Our goal is to effectively provide an adaptive learning experience in terms of style and difficulty, and thus better serve users' needs BIBREF9 . In this context, we study the ability of our proposed architectures in learning to generate quizzes based on data derived of the interaction of users with the platform. The problem of fill-in-the-blank question generation has been studied in the past by several authors. Perhaps the earlies approach is by BIBREF1 , who proposed a clo",
        "type": "Document"
      },
      {
        "id": "697bb061-faa3-44c7-892d-d50c77ba4ad1",
        "metadata": {
          "vector_store_key": "1902.11049-0",
          "chunk_id": 1,
          "document_id": "1902.11049",
          "start_idx": 448,
          "end_idx": 892
        },
        "page_content": " a form of copy mechanism BIBREF5 , BIBREF6 . Such models are trained to generate a plausible question, conditioned on an input document and answer span within that document BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 . There are currently no dedicated question generation datasets, and authors have used the context-question-answer triples available in SQuAD. Only a single question is available for each context-answer pair, and models are trained",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Was the degree of offensiveness taken as how generally offensive the text was, or how personally offensive it was to the annotator?",
    "llm_answer": "The offensiveness was rated on a 6-point Likert scale.\n",
    "context": [
      {
        "id": "7a6876c5-712b-4527-8bee-eed69071ca06",
        "metadata": {
          "vector_store_key": "1910.03814-7",
          "chunk_id": 28,
          "document_id": "1910.03814",
          "start_idx": 11041,
          "end_idx": 11575
        },
        "page_content": " workers. We received a lot of valuable feedback from the annotators. Most of them had understood the task correctly, but they were worried because of its subjectivity. This is indeed a subjective task, highly dependent on the annotator convictions and sensitivity. However, we expect to get cleaner annotations the more strong the attack is, which are the publications we are more interested on detecting. We also detected that several users annotate tweets for hate speech just by spotting slur. As already said previously, just the",
        "type": "Document"
      },
      {
        "id": "02949865-784b-4c7a-8aeb-d55f6473f6fe",
        "metadata": {
          "vector_store_key": "1911.03842-4",
          "chunk_id": 10,
          "document_id": "1911.03842",
          "start_idx": 3980,
          "end_idx": 4444
        },
        "page_content": " offensive or maybe offensive, they were asked to place it in one of four categories \u2013 racist, sexist, classist, other \u2013 and to provide a reason for their response. Just over 2% of personas were flagged by at least one annotator, and these personas are removed from the dataset. We further examined gender bias in personas. Annotators were asked to label the gender of each character based on their persona description (choosing \u201cneutral\" if it was not explicit in",
        "type": "Document"
      },
      {
        "id": "432c8acd-13dd-4510-bd89-1bffa852af6b",
        "metadata": {
          "vector_store_key": "1701.08118-2",
          "chunk_id": 4,
          "document_id": "1701.08118",
          "start_idx": 1750,
          "end_idx": 2264
        },
        "page_content": " the question may also play a role. We decided to investigate the issue of reliability further by conducting a more comprehensive study across a large number of annotators, which we present in this paper. Our contribution in this paper is threefold: For the purpose of building a classifier, warner2012 define hate speech as \u201cabusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation\u201d. More recent approaches rely on lists of guidelines such as a tweet",
        "type": "Document"
      },
      {
        "id": "d2f5f9f2-44c3-42b1-8fc2-31ffb4e2ee6b",
        "metadata": {
          "vector_store_key": "1701.08118-2",
          "chunk_id": 14,
          "document_id": "1701.08118",
          "start_idx": 5758,
          "end_idx": 6241
        },
        "page_content": " which are duplicates, contain links or pictures, or are retweets or replies. As a first measurement of the frequency of hate speech in our corpus, we personally annotated them based on our previous expertise. The 541 tweets were split into six parts and each part was annotated by two out of six annotators in order to determine if hate speech was present or not. The annotators were rotated so that each pair of annotators only evaluated one part. Additionally the offensiveness of",
        "type": "Document"
      },
      {
        "id": "d1dba5d8-85f5-49e6-a4f5-8c382299fc45",
        "metadata": {
          "vector_store_key": "1701.08118-2",
          "chunk_id": 15,
          "document_id": "1701.08118",
          "start_idx": 6241,
          "end_idx": 6722
        },
        "page_content": " were rotated so that each pair of annotators only evaluated one part. Additionally the offensiveness of a tweet was rated on a 6-point Likert scale, the same scale used later in the study. Even among researchers familiar with the definitions outlined above, there was still a low level of agreement (Krippendorff's INLINEFORM0 ). This supports our claim that a clearer definition is necessary in order to be able to train a reliable classifier. The low reliability could of course",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which embeddings do they detect biases in?",
    "llm_answer": "Word embeddings.\n",
    "context": [
      {
        "id": "889e96f1-1ccc-4aa4-9593-e136e664b875",
        "metadata": {
          "vector_store_key": "1905.09866-0",
          "chunk_id": 20,
          "document_id": "1905.09866",
          "start_idx": 7284,
          "end_idx": 7764
        },
        "page_content": " as woman is to doctor (where INLINEFORM1 ). This means that the bias isn't necessarily (or at least not only) in the representations themselves, rather in the way we query them. So, what do the embedding spaces actually tell if you let them return any word in the vocabulary? We took a selection of mainstream, striking examples from the literature on embedding bias, and tested them fairly, without posing any constraint on the returned term, exactly as we did for all analogies",
        "type": "Document"
      },
      {
        "id": "df1888c5-d0ed-4ae3-bc18-eac25e74cd60",
        "metadata": {
          "vector_store_key": "1905.09866-0",
          "chunk_id": 39,
          "document_id": "1905.09866",
          "start_idx": 14595,
          "end_idx": 15095
        },
        "page_content": "ing bias in word embeddings, since bias is anyway often encoded implicitly. As a side note, we would like to mention that in an earlier version of their paper, BIBREF18 accidentally searched for the inverse of the intended query, and still managed to find biased examples. This seems to be a further, strong, indication that strategies like this are not fully suitable to demonstrate the presence of bias in embeddings. If analogies might not be the most appropriate tool to capture certain relations",
        "type": "Document"
      },
      {
        "id": "c3e90692-889f-4123-ac8f-f98b60594f07",
        "metadata": {
          "vector_store_key": "1912.05238-0",
          "chunk_id": 32,
          "document_id": "1912.05238",
          "start_idx": 12172,
          "end_idx": 12620
        },
        "page_content": "'s bias, e.g. debias the model's gender bias. They describe that the first step for debiasing word embeddings is to identify a direction (or, more generally, a subspace) of the embedding that captures the bias. To identify the gender subspace, e.g., they proposed to take the difference vectors of given gender pairs and computed its principal components (PCs) and found a single direction that explains the majority of variance in these vectors, i",
        "type": "Document"
      },
      {
        "id": "21a24ad2-d5e3-4f31-b9e1-ba288f972c04",
        "metadata": {
          "vector_store_key": "1905.09866-0",
          "chunk_id": 8,
          "document_id": "1905.09866",
          "start_idx": 2953,
          "end_idx": 3477
        },
        "page_content": " and danger of human biases in word embeddings. On the contrary: embeddings do encode human biases, and we believe that this issue deserves the full attention of the field. However, we also believe that overemphasising and specifically seeking biases to achieve sensational results is not beneficial. It is also not necessary: what we observe naturally is worrying and sensational enough. Rather, we should aim at transparency and experimental clarity so as to ensure the fairest and most efficient treatment of the problem.",
        "type": "Document"
      },
      {
        "id": "7fc35a10-4add-49f7-bcee-511868506825",
        "metadata": {
          "vector_store_key": "1912.05238-6",
          "chunk_id": 56,
          "document_id": "1912.05238",
          "start_idx": 20871,
          "end_idx": 21397
        },
        "page_content": " than previous embedding methods. That is, yes, text embeddings encode knowledge about deontological ethical and even moral choices, but the quality of the bias score depends on the quality of the text embedding network. Specifically, our empirical results show that the Moral Choice Machine with recent state-of-the-art language representations, namely BERT, extends the boundary of previous approaches and demonstrate the existence of biases in human language on a complex phrase level. Moreover, we identified for the first",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the two PharmaCoNER subtasks?",
    "llm_answer": "Annotation of pharmacological, chemical, and biomedical entities in health records; and evaluation of the quality of datasets used to evaluate systems of this type.\n",
    "context": [
      {
        "id": "6b632e18-34ea-4a79-9ccf-b4aa6b7ff40e",
        "metadata": {
          "vector_store_key": "1705.03261-5",
          "chunk_id": 17,
          "document_id": "1705.03261",
          "start_idx": 6526,
          "end_idx": 6896
        },
        "page_content": " the interested two drugs with \u201cdrug1\u201d and \u201cdrug2\u201d while the other drugs are replaced by \u201cdurg0\u201d, as in BIBREF9 did. This step is called drug blinding. For example, the sentence in figure FIGREF5 generates 3 instances after drug blinding: \u201cdrug1: an increased risk of hepatitis has been reported to result from combined use of drug2 and drug0\u201d, \u201cdrug1: an increased risk",
        "type": "Document"
      },
      {
        "id": "a531aa9e-db8f-4b9b-a33d-36f8563a7818",
        "metadata": {
          "vector_store_key": "1905.07464-8",
          "chunk_id": 17,
          "document_id": "1905.07464",
          "start_idx": 6875,
          "end_idx": 7289
        },
        "page_content": " and D, K, and U for precipitants with pharmacodynamic, pharmacokinetic, and unspecified interactions respectively. As a preprocesssing step, we identify the label drug in the sentence, if it is mentioned, and bind it to a generic entity token (e.g. \u201cLABELDRUG\u201d). We additionally account for label drug aliases, such as the generic version of a brand-name drug, and bind them to the same entity token. Table TABREF",
        "type": "Document"
      },
      {
        "id": "52df182f-f1d8-4873-af24-b3659939bcc3",
        "metadata": {
          "vector_store_key": "1905.07464-8",
          "chunk_id": 54,
          "document_id": "1905.07464",
          "start_idx": 19985,
          "end_idx": 20442
        },
        "page_content": " We also implemented several tweaks to the pre-processing and post-processing aspects of the model based on preliminary error analysis including (1) using drug class mentions (e.g., \u201cdiuretics\u201d) as proxies if the drug label is not mentioned directly; (2) removing modifiers such as moderate, strong, and potent so that output conforms to official annotation guidelines; and (3) purging predicted mentions with only stopwords or generic terms such as \u201cdrugs\ufffd",
        "type": "Document"
      },
      {
        "id": "07de900f-353e-4650-b8d1-79318ee641d3",
        "metadata": {
          "vector_store_key": "1912.09152-5",
          "chunk_id": 8,
          "document_id": "1912.09152",
          "start_idx": 2881,
          "end_idx": 3371
        },
        "page_content": " the PharmaCoNER shared task, dealing with the annotation of some of the nes in health records (namely, pharmacological, chemical and biomedical entities) using a revisited version of rule- and dictionary-based approaches; and, on the other hand, to give pause for thought about the quality of datasets (and, thus, the fairness) with which systems of this type are evaluated, and to highlight the key role of resource-based systems in the validation and consolidation of both the annotation",
        "type": "Document"
      },
      {
        "id": "00c819aa-538e-4c66-9649-22170af09c3e",
        "metadata": {
          "vector_store_key": "1801.09030-3",
          "chunk_id": 47,
          "document_id": "1801.09030",
          "start_idx": 16327,
          "end_idx": 16837
        },
        "page_content": " referred to. There are also prescriptions that refer to the name of other prescriptions. We simply substitute these names with their constituents. To make the experiment result more robust, we conduct our experiments on two separate test datasets. The first one is a subset of the data described above. We randomly split the whole data into three parts, the training data (90%), the development data (5%) and the test data (5%). The second one is a set of symptom-prescription pairs we manually extracted from",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they perform data augmentation?",
    "llm_answer": "Mirroring, random cropping, replacement of words with relevant words (synonyms), frequency masking, speed perturbation, volume perturbation, and time masking.\n",
    "context": [
      {
        "id": "16ee5a25-9f0a-4412-a242-7a702c57ef17",
        "metadata": {
          "vector_store_key": "1812.06705-7",
          "chunk_id": 0,
          "document_id": "1812.06705",
          "start_idx": 0,
          "end_idx": 467
        },
        "page_content": "Deep neural network-based models are easy to overfit and result in losing their generalization due to limited size of training data. In order to address the issue, data augmentation methods are often applied to generate more training samples. Recent years have witnessed great success in applying data augmentation in the field of speech area BIBREF0 , BIBREF1 and computer vision BIBREF2 , BIBREF3 , BIBREF4 . Data augmentation in these areas can be easily performed",
        "type": "Document"
      },
      {
        "id": "d3cca699-ddca-4533-9529-efa458dbc74e",
        "metadata": {
          "vector_store_key": "1812.06705-9",
          "chunk_id": 2,
          "document_id": "1812.06705",
          "start_idx": 789,
          "end_idx": 1246
        },
        "page_content": " by mirroring we get \u201cdoog si srotca ehT\", or by random cropping we get \u201cactors is\", both of which are meaningless. Existing data augmentation methods for text are often loss of generality, which are developed with handcrafted rules or pipelines for specific domains. A general approach for text data augmentation is replacement-based method, which generates new sentences by replacing the words in the sentences with relevant words (e.g. synonyms). However",
        "type": "Document"
      },
      {
        "id": "a872be77-b483-4ddf-9cee-412fb23e61cf",
        "metadata": {
          "vector_store_key": "1912.13109-4",
          "chunk_id": 19,
          "document_id": "1912.13109",
          "start_idx": 7657,
          "end_idx": 8136
        },
        "page_content": " above few data augmentation techniques were applied to boost the learning of the network. Before applying augmentation, a train-test split of 78%-22% was done from the original, cleansed data set. Thus, 700 tweets/messages were held out for testing. All model evaluation were done in on the test set that got generated by this process. The results presented in this report are based on the performance of the model on the test set. The training set of 2489 messages were however",
        "type": "Document"
      },
      {
        "id": "f6a7eaaa-2d91-4f42-b153-d0a213244256",
        "metadata": {
          "vector_store_key": "1909.06522-5",
          "chunk_id": 44,
          "document_id": "1909.06522",
          "start_idx": 16023,
          "end_idx": 16474
        },
        "page_content": " of the data augmentation techniques described in Section SECREF3. As in Table TABREF11, the performance gains of using frequency masking were substantial and comparable to those of using speed perturbation, where $m_F = 2$ and $F=15$ (Section SECREF12) worked best. In addition, combining both frequency masking and speed perturbation could provide further improvements. However, applying additional volume perturbation (Section SECREF4) or time mask",
        "type": "Document"
      },
      {
        "id": "9e402957-6911-4ae4-96a8-078b68758235",
        "metadata": {
          "vector_store_key": "1909.06522-5",
          "chunk_id": 9,
          "document_id": "1909.06522",
          "start_idx": 3503,
          "end_idx": 4007
        },
        "page_content": " proceed with our investigation on various data augmentation techniques to overcome the lack of training data in the above low-resource setting. Given the highly scalable neural network acoustic modeling, extensive alternatives to increasing the amount or diversity of existing training data have been explored in prior works, e.g., applying vocal tract length perturbation and speed perturbation BIBREF18, volume perturbation and normalization BIBREF19, additive noises BIBREF20, reverberation BIBREF19,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the characteristics of the city dialect?",
    "llm_answer": "The city descriptions are relatively short, Calvino's writing is filled with rare words, complex syntactic structures, and figurative language.  Capturing the essential components of each city in a single vector is not simple.\n",
    "context": [
      {
        "id": "b8018e73-70a5-465e-b8e1-e8abd21327d3",
        "metadata": {
          "vector_store_key": "1702.06777-1",
          "chunk_id": 1,
          "document_id": "1702.06777",
          "start_idx": 508,
          "end_idx": 1087
        },
        "page_content": ". Typically, the dialect forms plotted on a map appear as a geographical continuum that gradually connects places with slightly different diatopic characteristics. A dialectometric analysis aims at a computational approach to dialect distribution, providing quantitative linguistic distances between locations BIBREF1 , BIBREF2 , BIBREF3 . Dialectometric data is based upon a corpus that contains the linguistic information needed for the statistical analysis. The traditional approach is to generate these data from surveys and questionnaires that address variable types used by",
        "type": "Document"
      },
      {
        "id": "4fa78717-71c8-4693-9cf9-ea707c95c586",
        "metadata": {
          "vector_store_key": "1702.06777-1",
          "chunk_id": 0,
          "document_id": "1702.06777",
          "start_idx": 0,
          "end_idx": 508
        },
        "page_content": "Dialects are language varieties defined across space. These varieties can differ in distinct linguistic levels (phonetic, morphosyntactic, lexical), which determine a particular regional speech BIBREF0 . The extension and boundaries (always diffuse) of a dialect area are obtained from the variation of one or many features such as, e.g., the different word alternations for a given concept. Typically, the dialect forms plotted on a map appear as a geographical continuum that gradually connects places with",
        "type": "Document"
      },
      {
        "id": "ddae79cb-529f-4872-a6cd-60eaf82c3e56",
        "metadata": {
          "vector_store_key": "1804.08186-2",
          "chunk_id": 422,
          "document_id": "1804.08186",
          "start_idx": 157787,
          "end_idx": 158116
        },
        "page_content": "REF4 , BIBREF425 , Serbo-Croatian dialects BIBREF426 , English varieties BIBREF278 , BIBREF45 , Dutch\u2013Flemish BIBREF53 , Dutch dialects (including a temporal dimension) BIBREF427 , German Dialects BIBREF428 Mainland\u2013Singaporean\u2013Taiwanese Chinese BIBREF429 , Portuguese varieties BIBREF5 , BIBREF259 , Spanish varieties BIBREF70 ,",
        "type": "Document"
      },
      {
        "id": "10a4a4a2-ff41-4541-8b97-862bff6f5a8c",
        "metadata": {
          "vector_store_key": "1904.08386-0",
          "chunk_id": 8,
          "document_id": "1904.08386",
          "start_idx": 3323,
          "end_idx": 3851
        },
        "page_content": " the city descriptions is relatively short, Calvino's writing is filled with rare words, complex syntactic structures, and figurative language. Capturing the essential components of each city in a single vector is thus not as simple as it is with more standard forms of text. Nevertheless, we hope that representations from language models trained over billions of words of text can extract some meaningful semantics from these descriptions. We experiment with three different pretrained representations: ELMo BIBREF5 , BERT BIB",
        "type": "Document"
      },
      {
        "id": "68d24bcf-5c18-4555-b8da-51515cdf3f05",
        "metadata": {
          "vector_store_key": "1702.06777-1",
          "chunk_id": 32,
          "document_id": "1702.06777",
          "start_idx": 12500,
          "end_idx": 13034
        },
        "page_content": " the other hand one can now investigate which regions show similar geolectal variation, yielding well defined linguistic varieties. Those cells that have similar colors in either figure FIGREF16 or figure FIGREF17 are expected to be ascribed to the same dialect zone. Thus, we can distinguish two main regions or clusters in the maps. The purple background covers most of the map and represents rural regions with small, scattered population. Our analysis shows that this group of cells possesses more specific words in their lexicon.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the characteristics of the rural dialect?",
    "llm_answer": "Rural regions, with small, scattered populations, have more specific words in their lexicon.\n",
    "context": [
      {
        "id": "68d24bcf-5c18-4555-b8da-51515cdf3f05",
        "metadata": {
          "vector_store_key": "1702.06777-1",
          "chunk_id": 32,
          "document_id": "1702.06777",
          "start_idx": 12500,
          "end_idx": 13034
        },
        "page_content": " the other hand one can now investigate which regions show similar geolectal variation, yielding well defined linguistic varieties. Those cells that have similar colors in either figure FIGREF16 or figure FIGREF17 are expected to be ascribed to the same dialect zone. Thus, we can distinguish two main regions or clusters in the maps. The purple background covers most of the map and represents rural regions with small, scattered population. Our analysis shows that this group of cells possesses more specific words in their lexicon.",
        "type": "Document"
      },
      {
        "id": "4fa78717-71c8-4693-9cf9-ea707c95c586",
        "metadata": {
          "vector_store_key": "1702.06777-1",
          "chunk_id": 0,
          "document_id": "1702.06777",
          "start_idx": 0,
          "end_idx": 508
        },
        "page_content": "Dialects are language varieties defined across space. These varieties can differ in distinct linguistic levels (phonetic, morphosyntactic, lexical), which determine a particular regional speech BIBREF0 . The extension and boundaries (always diffuse) of a dialect area are obtained from the variation of one or many features such as, e.g., the different word alternations for a given concept. Typically, the dialect forms plotted on a map appear as a geographical continuum that gradually connects places with",
        "type": "Document"
      },
      {
        "id": "b8018e73-70a5-465e-b8e1-e8abd21327d3",
        "metadata": {
          "vector_store_key": "1702.06777-1",
          "chunk_id": 1,
          "document_id": "1702.06777",
          "start_idx": 508,
          "end_idx": 1087
        },
        "page_content": ". Typically, the dialect forms plotted on a map appear as a geographical continuum that gradually connects places with slightly different diatopic characteristics. A dialectometric analysis aims at a computational approach to dialect distribution, providing quantitative linguistic distances between locations BIBREF1 , BIBREF2 , BIBREF3 . Dialectometric data is based upon a corpus that contains the linguistic information needed for the statistical analysis. The traditional approach is to generate these data from surveys and questionnaires that address variable types used by",
        "type": "Document"
      },
      {
        "id": "ddae79cb-529f-4872-a6cd-60eaf82c3e56",
        "metadata": {
          "vector_store_key": "1804.08186-2",
          "chunk_id": 422,
          "document_id": "1804.08186",
          "start_idx": 157787,
          "end_idx": 158116
        },
        "page_content": "REF4 , BIBREF425 , Serbo-Croatian dialects BIBREF426 , English varieties BIBREF278 , BIBREF45 , Dutch\u2013Flemish BIBREF53 , Dutch dialects (including a temporal dimension) BIBREF427 , German Dialects BIBREF428 Mainland\u2013Singaporean\u2013Taiwanese Chinese BIBREF429 , Portuguese varieties BIBREF5 , BIBREF259 , Spanish varieties BIBREF70 ,",
        "type": "Document"
      },
      {
        "id": "10680036-25e6-4b98-8c56-b311fc56e84e",
        "metadata": {
          "vector_store_key": "1702.06777-0",
          "chunk_id": 23,
          "document_id": "1702.06777",
          "start_idx": 9181,
          "end_idx": 9650
        },
        "page_content": " be blank, we have instances in most of the map. Importantly, our results agree with the distribution for the concept cold reported by Gon\u00e7alves and S\u00e1nchez BD with a different corpus. The north-south bipartition of the variation suggested in figure FIGREF2 (a) also agrees with more traditional studies BIBREF27 . As a consequence, these consistencies support the validity of our data. The novelty of our approach is to further analyze this dialect distribution with a",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the performance of the models on the tasks?",
    "llm_answer": "The best model achieved 42.48% accuracy for age, 37.50% for dialect, and 57.81% for gender.  Other performance metrics (like F1 scores) are also reported, but specific performance for other tasks is not comprehensively summarized.\n",
    "context": [
      {
        "id": "0d3ffdba-9fea-4342-bb4d-40db50f44aaf",
        "metadata": {
          "vector_store_key": "1909.04181-1",
          "chunk_id": 11,
          "document_id": "1909.04181",
          "start_idx": 4044,
          "end_idx": 4423
        },
        "page_content": " of each epoch, choosing the model that performs highest accuracy on DEV as our best model. We present our best result on DEV in Table TABREF7. We report all our results using accuracy. Our best model obtains 42.48% for age, 37.50% for dialect, and 57.81% for gender. All models obtains best results with 2 epochs. For each task, we fine-tune on the BERT-Base Muultilingual Cased",
        "type": "Document"
      },
      {
        "id": "9c2af065-a3a3-474a-8098-0d02ce732ea8",
        "metadata": {
          "vector_store_key": "2003.12932-2",
          "chunk_id": 19,
          "document_id": "2003.12932",
          "start_idx": 6944,
          "end_idx": 7364
        },
        "page_content": " all the experiments, the learning rate is set to 4e-5, for optimization we use Adam optimizer with epsilon value 1e-8. We ran each of the experiments for 10 and 50 epochs. Let us discuss the results from the above mentioned experiments. We show the plots of accuracy vs noise for each of the tasks. For IMDB, we fine tune the model for the sentiment analysis task. We plot F1 score vs % of error, as shown in Figure FIG",
        "type": "Document"
      },
      {
        "id": "31e3f6a2-9e9b-4d6d-824a-22195518284a",
        "metadata": {
          "vector_store_key": "1909.02764-7",
          "chunk_id": 52,
          "document_id": "1909.02764",
          "start_idx": 20420,
          "end_idx": 20841
        },
        "page_content": "=73 % on TEC. The model trained on our AMMER corpus achieves an F1 score of 57%. This is most probably due to the small size of this data set and the class bias towards joy, which makes up more than half of the data set. These results are mostly in line with Bostan2018. Now we analyze how well the models trained in Experiment 1 perform when applied to our data set. The results are shown in column \u201cSimple\u201d in Table TAB",
        "type": "Document"
      },
      {
        "id": "424a5dc3-fec5-43a6-befb-fb7767d53200",
        "metadata": {
          "vector_store_key": "2002.05829-1",
          "chunk_id": 13,
          "document_id": "2002.05829",
          "start_idx": 5371,
          "end_idx": 5916
        },
        "page_content": "task performance fine-tuned from given pretrained models because for each single task with different difficulty and instance number, the fine-tuning characteristics may differ a lot. When pretrained models are used to deal with non-standard downstream task, especially ad hoc application in industry, the training set's difficulty cannot be accurately estimated. Therefore, it's important to compare the multi-task efficiency for model choice. For inference phase, the time and cost of each model making inference for single instance on multiple",
        "type": "Document"
      },
      {
        "id": "148ea921-3ae7-4687-bc70-85a9690fe58b",
        "metadata": {
          "vector_store_key": "1912.13109-4",
          "chunk_id": 20,
          "document_id": "1912.13109",
          "start_idx": 7921,
          "end_idx": 8471
        },
        "page_content": " on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below: We tested the performance of various model architectures by running our experiment over 100 times on a CPU based compute which later as migrated to GPU based compute to overcome the slow learning progress. Our universal metric for minimizing was the validation loss and we",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What other non-neural baselines do the authors compare to? ",
    "llm_answer": "PCA-based symmetric acoustic similarity measure by Lee et al. and Nonlinear dynamical systems-based complexity measure.\n",
    "context": [
      {
        "id": "593127ef-0325-4226-be3e-0ed371ce653f",
        "metadata": {
          "vector_store_key": "1804.08782-7",
          "chunk_id": 37,
          "document_id": "1804.08782",
          "start_idx": 13990,
          "end_idx": 14427
        },
        "page_content": " number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 .  For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose",
        "type": "Document"
      },
      {
        "id": "62c29874-db95-480a-b48f-3cc96614b9cb",
        "metadata": {
          "vector_store_key": "1804.08782-0",
          "chunk_id": 38,
          "document_id": "1804.08782",
          "start_idx": 14212,
          "end_idx": 14718
        },
        "page_content": " the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose the best performing one for reporting, thus providing an upper-bound performance. Also, for baseline 2 we choose the session with higher value of the measure as real, since it measures similarity.  As we can see in Table TABREF24 , our proposed NED measure achieves higher accuracy than all baselines on the Fisher corpus. The accuracy of our measure declines in the Suicide corpus as compared to",
        "type": "Document"
      },
      {
        "id": "ed8aaf94-c5e8-4cf0-9b3c-4a4e591733f5",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 39,
          "document_id": "1909.00542",
          "start_idx": 15059,
          "end_idx": 15516
        },
        "page_content": " the results and details of the runs submitted to BioASQ. The table uses ROUGE-SU4 Recall since this is the metric available at the time of writing this paper. However, note that, as explained in Section SECREF6, these results might differ from the final human evaluation results. Therefore we do not comment on the results, other than observing that the \u201cfirst $n$\u201d baseline produces the same results as the neural regressor. As mentioned in Section SECREF",
        "type": "Document"
      },
      {
        "id": "17d3256d-56f6-4b9d-983d-669c916f7fae",
        "metadata": {
          "vector_store_key": "1709.08299-2",
          "chunk_id": 23,
          "document_id": "1709.08299",
          "start_idx": 9435,
          "end_idx": 10003
        },
        "page_content": " questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models. The",
        "type": "Document"
      },
      {
        "id": "c8c4272e-a38c-48c3-a271-2ada66b9c8b5",
        "metadata": {
          "vector_store_key": "2004.01980-1",
          "chunk_id": 63,
          "document_id": "2004.01980",
          "start_idx": 22218,
          "end_idx": 22603
        },
        "page_content": " all baselines. We use the summarization-related evaluation metrics, i.e., BLEU, ROUGE, CIDEr, and METEOR, to measure how relevant the generated headlines are to the news articles, to some extent, by comparing them to the original human-written headlines. In Table TABREF59, the first row \u201cNHG\u201d shows the performance of the current state-of-the-art summarization model on our data, and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "On what dataset is Aristo system trained?",
    "llm_answer": "Several datasets of independently-authored science questions taken from standardized tests.\n",
    "context": [
      {
        "id": "f77b58b5-7145-480a-8cb7-4367a27497e4",
        "metadata": {
          "vector_store_key": "1909.01958-5",
          "chunk_id": 69,
          "document_id": "1909.01958",
          "start_idx": 24838,
          "end_idx": 25300
        },
        "page_content": ") that includes the correct option, the system receives 1/N points (equivalent to the asymptote of random guessing between the N). The results are summarized in Table TABREF33, showing the performance of the solvers individually, and their combination in the full Aristo system. Note that Aristo is a single system run on the five datasets (not retuned for each dataset in turn). Most notably, Aristo's scores on the Regents Exams far exceed earlier performances",
        "type": "Document"
      },
      {
        "id": "1aa63d16-fab0-4716-bcef-196ce5854fe4",
        "metadata": {
          "vector_store_key": "1909.01958-5",
          "chunk_id": 89,
          "document_id": "1909.01958",
          "start_idx": 32148,
          "end_idx": 32664
        },
        "page_content": "). This paper reports on Aristo\u2014the first system to achieve a score of over 90% on the non-diagram, multiple choice part of the New York Regents 8th Grade Science Exam, demonstrating that modern NLP methods can result in mastery of this task. Although Aristo only answers multiple choice questions without diagrams, and operates only in the domain of science, it nevertheless represents an important milestone towards systems that can read and understand. The momentum on this task has been remarkable, with accuracy",
        "type": "Document"
      },
      {
        "id": "740fd189-e219-4c07-88a5-79d242438a85",
        "metadata": {
          "vector_store_key": "1909.01958-5",
          "chunk_id": 18,
          "document_id": "1909.01958",
          "start_idx": 7153,
          "end_idx": 7578
        },
        "page_content": " dominated by information retrieval, statistical, and automated rule extraction and reasoning methods (BID24;BID25;BID26;BID27;BID28). Later work has harnessed state-of-the-art tools for large-scale language modeling and deep learning (BID29;BID30), which have come to dominate the performance of the overall system and reflects the stunning progress of the field of NLP as a whole. We now describe the architecture of Aristo",
        "type": "Document"
      },
      {
        "id": "d0e47e6b-9399-4fb8-8023-72f6bf632325",
        "metadata": {
          "vector_store_key": "1909.01958-1",
          "chunk_id": 63,
          "document_id": "1909.01958",
          "start_idx": 22619,
          "end_idx": 23113
        },
        "page_content": " Regents 8th Grade Science exams, only 38 ($<$8%) do not involve a diagram. Second, they are complex, often requiring explanation and synthesis. Both diagram and direct-answer questions are natural topics for future work. We evaluate Aristo using several datasets of independently-authored science questions taken from standardized tests. Each dataset is divided into train, development, and test partitions, the test partitions being \u201cblind\u201d, i.e., hidden to both the researchers and the Arist",
        "type": "Document"
      },
      {
        "id": "3ab714da-2deb-472b-bb89-86bf45ccb752",
        "metadata": {
          "vector_store_key": "1909.01958-5",
          "chunk_id": 17,
          "document_id": "1909.01958",
          "start_idx": 6693,
          "end_idx": 7153
        },
        "page_content": " (BID23). The metric progress of the Aristo system on the Regents 8th Grade exams (non-diagram, multiple choice part, for a hidden, held-out test set) is shown in Figure FIGREF6. The figure shows the variety of techniques attempted, and mirrors the rapidly changing trajectory of the Natural Language Processing (NLP) field in general. Early work was dominated by information retrieval, statistical, and automated rule extraction and reasoning methods (BID24;B",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many roles are proposed?",
    "llm_answer": "The context does not specify a precise number of proposed roles.  It mentions examples like \"first_element\" and \"second_element\" but doesn't give a total count.\n",
    "context": [
      {
        "id": "0e377fa0-f467-475b-8949-4937b4100ad4",
        "metadata": {
          "vector_store_key": "1911.03243-2",
          "chunk_id": 4,
          "document_id": "1911.03243",
          "start_idx": 1523,
          "end_idx": 1985
        },
        "page_content": " more roles, our coverage gain is demonstrated by evaluating against expertly annotated data and comparison with PropBank (Section SECREF4). To foster future research, we release an assessed high-quality gold dataset along with our reproducible protocol and evaluation scheme, and report the performance of the existing parser BIBREF5 as a baseline. In QA-SRL, a role question adheres to a 7-slot template, with slots corresponding to a WH-word, the verb, auxili",
        "type": "Document"
      },
      {
        "id": "c03e3db0-1240-4efb-9d94-68928455906c",
        "metadata": {
          "vector_store_key": "1911.03243-2",
          "chunk_id": 6,
          "document_id": "1911.03243",
          "start_idx": 2267,
          "end_idx": 2736
        },
        "page_content": " is then considered as the set of arguments associated with that role. This broad question-based definition of roles captures traditional cases of syntactically-linked arguments, but also additional semantic arguments clearly implied by the sentence meaning (see example (2) in Table TABREF4). The original 2015 QA-SRL dataset BIBREF4 was annotated by non-expert workers after completing a brief training procedure. They annotated 7.8K verbs, reporting an average of 2.",
        "type": "Document"
      },
      {
        "id": "a7a29f96-e131-4f32-9289-0fccf4270e98",
        "metadata": {
          "vector_store_key": "1910.14497-1",
          "chunk_id": 37,
          "document_id": "1910.14497",
          "start_idx": 12918,
          "end_idx": 13219
        },
        "page_content": "chancellor', 'chaplain', 'character', 'chef', 'chemist', 'choreographer', 'cinematographer', 'citizen', 'cleric', 'clerk', 'coach', 'collector', 'colonel', 'columnist', 'comedian', 'comic', 'commander', 'commentator', 'commissioner', 'composer', 'conductor', 'confesses', 'congressman', 'constable', '",
        "type": "Document"
      },
      {
        "id": "7ff0644f-2b65-4346-82df-63d8070fe29c",
        "metadata": {
          "vector_store_key": "1910.02339-8",
          "chunk_id": 10,
          "document_id": "1910.02339",
          "start_idx": 3480,
          "end_idx": 3825
        },
        "page_content": " are vector embeddings of the roles. In Eq. SECREF2, and below, for notational simplicity we conflate order-2 tensors and matrices. As a simple example, consider the symbolic type string, and choose roles to be $r_1 = $ first_element, $r_2 = $ second_element, etc. Then in the specific string S = cba, the first role $r_1$ is filled by c, and $r",
        "type": "Document"
      },
      {
        "id": "cb6bd1a5-2f14-4c23-a31c-ada4ec3e9204",
        "metadata": {
          "vector_store_key": "1910.14497-1",
          "chunk_id": 38,
          "document_id": "1910.14497",
          "start_idx": 13219,
          "end_idx": 13494
        },
        "page_content": "er', 'conductor', 'confesses', 'congressman', 'constable', 'consultant', 'cop', 'correspondent', 'counselor', 'critic', 'crusader', 'curator', 'dad', 'dancer', 'dean', 'dentist', 'deputy', 'detective', 'diplomat', 'director', 'doctor', 'drummer', 'economist', 'editor', 'educ",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What language technologies have been introduced in the past?",
    "llm_answer": "Statistical modeling approaches, linguistic theory based approaches, deep neural networks, LSTMs, Transformer-based architectures.\n",
    "context": [
      {
        "id": "67e9fb10-70e6-401e-8a00-a5733bd28a62",
        "metadata": {
          "vector_store_key": "1804.08186-9",
          "chunk_id": 445,
          "document_id": "1804.08186",
          "start_idx": 166375,
          "end_idx": 166790
        },
        "page_content": " such as very short documents (e.g. search engine queries), non-linguistic \u201cnoise\u201d (e.g. HTML markup), non-standard use of language (e.g. as seen in social media data), and mixed-language documents (e.g. forum posts in multilingual web forums). Modern approaches to are generally data-driven and are based on comparing new documents with models of each target language learned from data. The types of models as well",
        "type": "Document"
      },
      {
        "id": "a8519dc9-68f6-434f-a097-9534ded33e25",
        "metadata": {
          "vector_store_key": "1909.00154-8",
          "chunk_id": 4,
          "document_id": "1909.00154",
          "start_idx": 1602,
          "end_idx": 2163
        },
        "page_content": " a while two general trends were competing: the statistical modeling approaches, and the linguistic theory based approaches. The former relied on simple representations, such as vector frequencies, or dummy variables, to become practical, while the latter used domain knowledge such as grammars or logic. Until recently, neither had considerable success in making machines able to understand or generate human language, but developments in deep neural networks together with overwhelmingly massive amounts of data (i.e. the World Wide Web) brought them to a new",
        "type": "Document"
      },
      {
        "id": "bfe6fabb-3f4c-451d-a2f2-de42e6f1eeda",
        "metadata": {
          "vector_store_key": "2002.02427-2",
          "chunk_id": 20,
          "document_id": "2002.02427",
          "start_idx": 7113,
          "end_idx": 7613
        },
        "page_content": ".g., punctuation marks, positive and negative emoticons, quotations, personal pronouns, tweet's length, named entities) while others are language-dependent relying on dedicated lexicons (e.g., negation, opinion lexicons, opposition words). Several classical machine learning classifiers were tested with several feature combinations, among them Random Forest (RF) achieved the best result with all features. Neural model with monolingual embeddings. We used Convolutional Neural Network (CNN) network",
        "type": "Document"
      },
      {
        "id": "6f30b287-efa2-4fa7-9761-c2ebae0f2bc2",
        "metadata": {
          "vector_store_key": "1912.00582-2",
          "chunk_id": 9,
          "document_id": "1912.00582",
          "start_idx": 3636,
          "end_idx": 4099
        },
        "page_content": " modeling. In the last decade, we have seen two major paradigm shifts in the state of the art for language modeling. The first major shift for language modeling was the movement from statistical methods based on $n$-grams BIBREF10 to neural methods such as LSTMs BIBREF11, which directly optimize on the task of predicting the next word. More recently, Transformer-based architectures employing self-attention BIBREF12 have outperformed LSTMs at language modeling",
        "type": "Document"
      },
      {
        "id": "5b6e510f-caaf-40bc-9ea9-665c29c82167",
        "metadata": {
          "vector_store_key": "1804.08186-9",
          "chunk_id": 8,
          "document_id": "1804.08186",
          "start_idx": 3302,
          "end_idx": 3814
        },
        "page_content": " for data crawled from the World Wide Web. Another application of that predates computational methods is the detection of the language of a document for routing to a suitable translator. This application has become even more prominent due to the advent of Machine Translation (\u201cMT\u201d) methods: in order for MT to be applied to translate a document to a target language, it is generally necessary to determine the source language of the document, and this is the task of . also plays a part in providing support for",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they define local variance?",
    "llm_answer": "Local variance loss is the reciprocal of the variance of the attention distribution, calculated using the median value instead of the mean to be more robust to outliers.\n",
    "context": [
      {
        "id": "ff5a7eeb-9d35-4418-bb08-d3260b134811",
        "metadata": {
          "vector_store_key": "1910.11491-2",
          "chunk_id": 14,
          "document_id": "1910.11491",
          "start_idx": 5156,
          "end_idx": 5696
        },
        "page_content": " section SECREF1, the attention model putting most of attention weight on just a few parts of the input tends to achieve good performance. Mathematically, when only a small number of values are large, the shape of the distribution is sharp and the variance of the attention distribution is large. Drawing on the concept of variance in mathematics, local variance loss is defined as the reciprocal of its variance expecting the attention model to be able to focus on more salient parts. The standard variance calculation is based on the mean",
        "type": "Document"
      },
      {
        "id": "657f1277-4c19-43cc-8d02-e6cd00543b11",
        "metadata": {
          "vector_store_key": "1910.11491-2",
          "chunk_id": 15,
          "document_id": "1910.11491",
          "start_idx": 5696,
          "end_idx": 6154
        },
        "page_content": " model to be able to focus on more salient parts. The standard variance calculation is based on the mean of the distribution. However, as previous work BIBREF15, BIBREF16 mentioned that the median value is more robust to outliers than the mean value, we use the median value to calculate the variance of the attention distribution. Thus, local variance loss can be calculated as: where $\\hat{\\cdot }$ is a median operator and $\\epsilon $ is utilized to avoid",
        "type": "Document"
      },
      {
        "id": "9950a5b9-cb7c-46da-9e5a-6be4c82136f5",
        "metadata": {
          "vector_store_key": "1910.11491-2",
          "chunk_id": 18,
          "document_id": "1910.11491",
          "start_idx": 6756,
          "end_idx": 7263
        },
        "page_content": " difference between the sum of attention weight and maximum attention weight of $i$-th input state among all timesteps should be small. Moreover, the whole distribution of the difference over all input positions should have a flat shape. Similar to the definition of local variance loss, the global variance loss is formulated as: where $g_i$ represents the difference between the accumulated attention weight and maximum attention weight at $i$-th position. The model is firstly pre-trained to minimize the",
        "type": "Document"
      },
      {
        "id": "d32647a7-96bd-4403-a43b-4a69adc376ab",
        "metadata": {
          "vector_store_key": "1910.11491-2",
          "chunk_id": 27,
          "document_id": "1910.11491",
          "start_idx": 9922,
          "end_idx": 10236
        },
        "page_content": " achieved a gain of 0.97 ROUGE-1, 0.35 ROUGE-2, and 0.64 ROUGE-L points; the local variance loss boosts the model by 3.01 ROUGE-1, 1.6 ROUGE-2, and 2.58 ROUGE-L. As shown in Figure FIGREF22, the global variance loss helps with eliminating n-gram repetitions, which verifies its effectiveness. We also conduct human",
        "type": "Document"
      },
      {
        "id": "e851c523-1429-4cb0-9b97-02e6e0a2faaa",
        "metadata": {
          "vector_store_key": "2002.05058-2",
          "chunk_id": 65,
          "document_id": "2002.05058",
          "start_idx": 25823,
          "end_idx": 26327
        },
        "page_content": "G models). As this variance does not exist when we regard a sample as a model which always generates the same sample. As for model-level evaluation, we employ the average score of the evaluated 100 samples as each model's score and calculate their correlation with human scores. For comparative evaluator, we propose three different approaches to get an absolute score for each sample: 1) we calculate the average reference-based score (method 1 for sample-level comparison) of each sample as model-level",
        "type": "Document"
      }
    ]
  },
  {
    "query": "how do they measure discussion quality?",
    "llm_answer": "Discussion quality is measured by students' argument moves (claims and textual evidence), specificity, and knowledge domain.  Inter-rater reliability is assessed using Cohen's kappa, and validity is compared to English Education experts' evaluations.\n",
    "context": [
      {
        "id": "07e35b10-523a-4515-9a29-b432b6836f1e",
        "metadata": {
          "vector_store_key": "1909.03023-0",
          "chunk_id": 5,
          "document_id": "1909.03023",
          "start_idx": 2138,
          "end_idx": 2728
        },
        "page_content": " to discussion quality and learning opportunities: argumentation (the process of systematically reasoning in support of an idea), specificity (the quality of belonging or relating uniquely to a particular subject), and knowledge domain (area of expertise represented in the content of the talk). We demonstrate the reliability and validity of our scheme via an annotation study of five transcripts of classroom discussion. One discourse feature used to assess the quality of discussions is students' argument moves: their claims about the text, their sharing of textual evidence for claims,",
        "type": "Document"
      },
      {
        "id": "bf420fd0-2a99-48c9-9d52-0d17e3074796",
        "metadata": {
          "vector_store_key": "1910.00825-4",
          "chunk_id": 47,
          "document_id": "1910.00825",
          "start_idx": 16888,
          "end_idx": 17413
        },
        "page_content": " our method's increased performance on automatic evaluation metrics entails better human perceived quality. We randomly select 100 test samples from MultiWOZ test set for evaluation. We recruit 150 crowd workers from Amazon Mechanical Turk. For each sample, we show the conversation, reference summary, as well as summaries generated by Pointer-Generator and SPNet to three different participants. The participants are asked to score each summary on three indicators: relevance, conciseness and readability on a 1 to 5 scale,",
        "type": "Document"
      },
      {
        "id": "85583f0b-608c-434d-8a34-fb7090ae194a",
        "metadata": {
          "vector_store_key": "1909.03023-0",
          "chunk_id": 22,
          "document_id": "1909.03023",
          "start_idx": 8992,
          "end_idx": 9437
        },
        "page_content": "-based discussions were used for testing reliability after training: pair P1 annotated discussions of The Bluest Eye, Death of a Salesman, and Macbeth, while pair P2 annotated two separate discussions of Ain't I a Woman. 250 argument moves (discussed by over 40 students and consisting of over 8200 words) were annotated. Inter-rater reliability was assessed using Cohen's kappa: unweighted for argumentation and knowledge domain, but quadratic-",
        "type": "Document"
      },
      {
        "id": "090992b5-2527-485b-8522-4490a56ced1e",
        "metadata": {
          "vector_store_key": "1909.03023-0",
          "chunk_id": 3,
          "document_id": "1909.03023",
          "start_idx": 1244,
          "end_idx": 1845
        },
        "page_content": "9 . This limitation is partly due to the time-intensive work required to analyze discourse data through qualitative methods such as ethnography and discourse analysis. Thus, qualitative case studies have generated compelling theories about the specific features of student talk that lead to high-quality discussions, but few findings can be generalized and leveraged to influence instructional improvements across ELA classrooms. As a first step towards developing an automated system for detecting the features of student talk that lead to high quality discussions, we propose a new annotation scheme",
        "type": "Document"
      },
      {
        "id": "02635824-9388-4146-9d18-f7b459974cbc",
        "metadata": {
          "vector_store_key": "1909.03023-2",
          "chunk_id": 30,
          "document_id": "1909.03023",
          "start_idx": 12211,
          "end_idx": 12706
        },
        "page_content": ". We also BIBREF32 explored the validity of our coding scheme by comparing our annotations of student talk to English Education experts' evaluations (quadratic-weighted kappa of 0.544) of the discussion's quality. Using stepwise regressions, we found that the best model of discussion quality (R-squared of INLINEFORM0 ) included all three of our coding dimensions: argumentation, specificity, and knowledge domain. Our annotation scheme introduces opportunities for the educational community to",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what were the baselines?",
    "llm_answer": "Baseline 1: smooth L1 distance; Baseline 2: PCA-based symmetric acoustic similarity; Baseline 3: Nonlinear dynamical systems-based complexity measure; ANRES_Sing_R; ANRES_Sing_Z; ANRES_Seq_R; ANRES_Seq_Z\n",
    "context": [
      {
        "id": "593127ef-0325-4226-be3e-0ed371ce653f",
        "metadata": {
          "vector_store_key": "1804.08782-7",
          "chunk_id": 37,
          "document_id": "1804.08782",
          "start_idx": 13990,
          "end_idx": 14427
        },
        "page_content": " number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 .  For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose",
        "type": "Document"
      },
      {
        "id": "fe0f18bc-f5e9-4a30-bd8f-1b27de376893",
        "metadata": {
          "vector_store_key": "2004.01878-5",
          "chunk_id": 40,
          "document_id": "2004.01878",
          "start_idx": 14526,
          "end_idx": 14915
        },
        "page_content": " normal distribution $\\mathcal {N}(0, 1)$. In summary, the following four baselines are designed: ANRES_Sing_R: randomly initializing the states for each single trading day. ANRES_Sing_Z: initializing the states as zeros for each single trading day. ANRES_Seq_R: randomly initializing the first states for each trading sequence only. ANRES_Seq_Z: initializing the first states as zeros for",
        "type": "Document"
      },
      {
        "id": "17d3256d-56f6-4b9d-983d-669c916f7fae",
        "metadata": {
          "vector_store_key": "1709.08299-2",
          "chunk_id": 23,
          "document_id": "1709.08299",
          "start_idx": 9435,
          "end_idx": 10003
        },
        "page_content": " questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models. The",
        "type": "Document"
      },
      {
        "id": "0b77285e-fdf5-4273-8639-5afcd5734567",
        "metadata": {
          "vector_store_key": "2001.02380-3",
          "chunk_id": 140,
          "document_id": "2001.02380",
          "start_idx": 51470,
          "end_idx": 51956
        },
        "page_content": " baselines. The left, middle and right panels in Figure FIGREF40 correspond to measurements when all signals are included, only cases contained entirely in the head EDUs shown to the model, and only DMs, respectively. The scenario on the left is rather unreasonable and is included only for completeness: here the model is also penalized for not detecting signals such as lexical chains, part of which is outside the units that the model is being shown. An example of such a case can be",
        "type": "Document"
      },
      {
        "id": "c8c4272e-a38c-48c3-a271-2ada66b9c8b5",
        "metadata": {
          "vector_store_key": "2004.01980-1",
          "chunk_id": 63,
          "document_id": "2004.01980",
          "start_idx": 22218,
          "end_idx": 22603
        },
        "page_content": " all baselines. We use the summarization-related evaluation metrics, i.e., BLEU, ROUGE, CIDEr, and METEOR, to measure how relevant the generated headlines are to the news articles, to some extent, by comparing them to the original human-written headlines. In Table TABREF59, the first row \u201cNHG\u201d shows the performance of the current state-of-the-art summarization model on our data, and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which soft-selection approaches are evaluated?",
    "llm_answer": "Traditional attention-based soft-selection methods.\n",
    "context": [
      {
        "id": "b798ccc5-3b80-4d4a-8e3a-263f63bd1ff7",
        "metadata": {
          "vector_store_key": "1905.10238-5",
          "chunk_id": 8,
          "document_id": "1905.10238",
          "start_idx": 3291,
          "end_idx": 3866
        },
        "page_content": " the given context. Moreover, a softmax pruning is placed in between the two layers to select high confident candidates. The architecture ensures the model being able to leverage both context and external knowledge. Especially, compared with conventional approaches that simply treat external knowledge as rules or features, our model is not only more flexible and effective but also interpretable as it reflects which knowledge source has the higher weight in order to make the decision. Experiments are conducted on a widely used evaluation dataset, where the results prove",
        "type": "Document"
      },
      {
        "id": "a18aa7eb-3ea4-44f2-916a-34be5299e057",
        "metadata": {
          "vector_store_key": "2002.05058-1",
          "chunk_id": 55,
          "document_id": "2002.05058",
          "start_idx": 21738,
          "end_idx": 22296
        },
        "page_content": " three variants of the comparative evaluator where it is trained without strong supervision examples, without weak supervision examples, without fine-tuning with human preference annotations, and without transferring from BERT. As human evaluation is expensive, sample-level evaluation is performed jointly with model-level evaluation, which is also used for evaluating the ability of different metrics for performing hyperparameter search and early-stopping. Concretely, we perform 10 groups of evaluations for performing hyperparameter selecting and early-",
        "type": "Document"
      },
      {
        "id": "1e785c15-3d96-4885-8b29-043da23a7813",
        "metadata": {
          "vector_store_key": "1909.11297-4",
          "chunk_id": 67,
          "document_id": "1909.11297",
          "start_idx": 24704,
          "end_idx": 25168
        },
        "page_content": " snippet, our approach can alleviate the attention distraction problem of traditional attention-based soft-selection methods. Experimental results demonstrate the effectiveness of our method. Especially, our hard-selection approach outperforms soft-selection approaches significantly when handling multi-aspect sentences with different sentiment polarities. This work is supported by National Science and Technology Major Project, China (Grant No. 2018YFB0204304).",
        "type": "Document"
      },
      {
        "id": "33815fd5-1a09-4973-8bed-8a1685b133d7",
        "metadata": {
          "vector_store_key": "1905.10238-3",
          "chunk_id": 61,
          "document_id": "1905.10238",
          "start_idx": 19745,
          "end_idx": 20310
        },
        "page_content": " among all knowledge types, which indicates that potentially more cases in the evaluation dataset demand on the AG knowledge than others. More importantly, the results also prove the effectiveness of the knowledge attention module, which contributes to the performance gap between our model and the Feature Concatenation one. Effect of Different Pruning Thresholds We try different thresholds $t$ for the softmax pruning in selecting reliable candidates. The effects of different thresholds on reducing candidates and overall performance are shown in Figure 5 and 6",
        "type": "Document"
      },
      {
        "id": "ba42e34c-bc75-4589-9f2c-b3ad71f75e72",
        "metadata": {
          "vector_store_key": "2002.05058-1",
          "chunk_id": 56,
          "document_id": "2002.05058",
          "start_idx": 22070,
          "end_idx": 22558
        },
        "page_content": ". Concretely, we perform 10 groups of evaluations for performing hyperparameter selecting and early-stopping with five compared automated metrics. In each evaluation, each of the five compared metrics is used to select the best hyperparameter combination or early-stopping checkpoint with other variants fixed. We choose to perform score-based human evaluation for four reasons: 1) the ADEM baseline requires human-annotated score as training examples, 2) we can construct up to $\\binom{2",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is slot filing dataset?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "49fda9f6-55cd-41c4-ac7e-6f3fddd45b13",
        "metadata": {
          "vector_store_key": "1908.11546-0",
          "chunk_id": 19,
          "document_id": "1908.11546",
          "start_idx": 5797,
          "end_idx": 6239
        },
        "page_content": " of acts, slots, and act-slot pairs are also listed in Table TABREF11. Table TABREF12 shows the count of turns with multiple act annotations, which amounts to 23% of the dataset. We use MSR's dialogue management code and knowledge base to obtain the state at each turn and use it as input to every model. We evaluate the performance at the act, frame and task completion level. For a frame to be correct, both the act and all the slots should",
        "type": "Document"
      },
      {
        "id": "b42f331d-9d76-4f11-aba4-0dc1de1ca293",
        "metadata": {
          "vector_store_key": "1911.01680-0",
          "chunk_id": 45,
          "document_id": "1911.01680",
          "start_idx": 16273,
          "end_idx": 16741
        },
        "page_content": " almost 2%, which corresponds to a 12.3% error rate reduction. This improvements proves the effectiveness of using contextual information for the task of slot filling. In order to analyze the contribution of the proposed sub-tasks we also evaluate the model when we remove one of the sub-task and retrain the model. The results are reported in Table TABREF23. This table shows that all sub-tasks are required for the model to have its best performance. Among all sub-t",
        "type": "Document"
      },
      {
        "id": "02fbb1b3-afa0-486d-95f2-1669d038bf4d",
        "metadata": {
          "vector_store_key": "2002.01359-0",
          "chunk_id": 22,
          "document_id": "2002.01359",
          "start_idx": 9340,
          "end_idx": 9877
        },
        "page_content": "ents and slots which can be used for developing models which can condition their predictions on the schema. To reflect the constraints present in real-world services and APIs, we impose a few constraints on the data. Our dataset does not expose the set of all possible values for certain slots. Having such a list is impractical for slots like date or time because they have infinitely many possible values or for slots like movie or song names, for which new values are periodically added. Such slots are specifically identified as non-",
        "type": "Document"
      },
      {
        "id": "84720de1-c988-45c6-bcbe-7a0435cdf6fb",
        "metadata": {
          "vector_store_key": "1706.08032-0",
          "chunk_id": 38,
          "document_id": "1706.08032",
          "start_idx": 14264,
          "end_idx": 14661
        },
        "page_content": " datasets in Table III. for all datasets, the filter window size ( INLINEFORM0 ) is 7 with 6 feature maps each for the first wide convolution layer, the second wide convolution layer has a filter window size of 5 with 14 feature maps each. Dropout rate ( INLINEFORM1 ) is 0.5, INLINEFORM2 constraint, learning rate is 0.1 and momentum of 0.9. Mini-batch size for STS Corpus is 100 and others are 4",
        "type": "Document"
      },
      {
        "id": "193abac9-11af-4dff-8cb5-9739302307c4",
        "metadata": {
          "vector_store_key": "1909.11833-1",
          "chunk_id": 16,
          "document_id": "1909.11833",
          "start_idx": 5531,
          "end_idx": 6057
        },
        "page_content": "s\\in S$. This can be cumbersome when the ontology is large, and it suffers from the insufficient data problem: the labelled data for a single slot may not suffice to effectively train the parameters for the slot-specific neural networks structure. Therefore, we propose that in the classification process, the model needs to rely on the semantic similarity between the user's utterance and slot-value pair, with system action information. In other words, the model should have only a single global neural structure independent",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How large is the dataset they generate?",
    "llm_answer": "287,226 training documents, 13,368 validation documents, and 11,490 test documents.\n",
    "context": [
      {
        "id": "9270562a-8e53-4e1d-b760-74d8fd0bc872",
        "metadata": {
          "vector_store_key": "1905.06566-7",
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "0723454a-4062-463e-972a-81a58c6a504e",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "c40affa6-b6af-443b-beed-13b4daccfa51",
        "metadata": {
          "vector_store_key": "1705.10586-8",
          "chunk_id": 45,
          "document_id": "1705.10586",
          "start_idx": 14843,
          "end_idx": 15254
        },
        "page_content": "000 to 3,600,000 training samples, and of different text length ranging from average of 38 words for Ag News to 566 words in Sogou news as illustrated in Table 1 . The datasets are a good mix of polished (AG) and noisy (Yelp and Amazon reviews), long (Sogou) and short (DBP and AG), large (Amazon reviews) and small (AG) datasets. And thus the results over these datasets serve as good evaluation on the quality",
        "type": "Document"
      },
      {
        "id": "b41bd311-18d0-46b5-9e72-6e4924ab8a5e",
        "metadata": {
          "vector_store_key": "1912.00864-0",
          "chunk_id": 63,
          "document_id": "1912.00864",
          "start_idx": 19724,
          "end_idx": 20165
        },
        "page_content": " Oshiete-goo dataset. On average, the answers to these questions consisted of about 3.5 conclusions and supplements selected by human experts. The questions, conclusions, and supplements had average lengths of 482, 41, and 46 characters, respectively. There were 9,779 word tokens in the questions and 6,317 tokens in answers; the overlap was 4,096. We also used the Yahoo nfL6 dataset, the largest publicly available English non-factoid CQA",
        "type": "Document"
      },
      {
        "id": "49dd09c0-f752-4a00-bc37-38c26e5e9acc",
        "metadata": {
          "vector_store_key": "1709.05404-5",
          "chunk_id": 48,
          "document_id": "1709.05404",
          "start_idx": 17579,
          "end_idx": 18022
        },
        "page_content": "-validation setup) for each dataset while holding the number of posts per class constant. Figure FIGREF26 shows the performance of each of the Gen, RQ, and Hyp datasets at intervals of 100 posts per class (up to the maximum size of 582 posts per class for Hyp, and 851 posts per class for RQ). From the graph, we can see that as a general trend, the datasets benefit from larger dataset sizes. Interestingly, the results for the RQ dataset are",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the weaknesses of their proposed interpretability quantification method?",
    "llm_answer": "The context does not describe a proposed interpretability quantification method.  Therefore, weaknesses cannot be identified.\n",
    "context": [
      {
        "id": "472be461-9155-423e-a95f-0e332baa9253",
        "metadata": {
          "vector_store_key": "1804.03396-1",
          "chunk_id": 92,
          "document_id": "1804.03396",
          "start_idx": 29181,
          "end_idx": 29645
        },
        "page_content": " annotators lower the bar of accepting triples. However, the analysis on semantic roles and parsing trees cannot work very well on complicated input sentences like the 2nd and the 3rd cases. Besides, the baseline systems can hardly solve the last two cases which require inference on input sentences. Our framework works very well on this dataset with the QA measurements EM $= 91.87$ and F1 $= 93.53$ and the IE measurements can be found in Figure 3 . Most of the",
        "type": "Document"
      },
      {
        "id": "2603a753-8313-4563-b17d-50671c7769d8",
        "metadata": {
          "vector_store_key": "1811.01001-5",
          "chunk_id": 4,
          "document_id": "1811.01001",
          "start_idx": 1609,
          "end_idx": 2157
        },
        "page_content": "REF11 , BIBREF9 , BIBREF12 . These formulations follow narrow and bounded evaluation schemes though: They often define a length threshold in the test set and report the performance of the model on this fixed set. We acknowledge three unsettling issues with these formulations. First, the sequences in the training set are usually assumed to be uniformly or geometrically distributed, with little regard to the nature and complexity of the language. This assumption may undermine any conclusions drawn from empirical investigations, especially given",
        "type": "Document"
      },
      {
        "id": "2eed5d51-4dfd-4622-8141-f8d71567d458",
        "metadata": {
          "vector_store_key": "1611.03382-9",
          "chunk_id": 59,
          "document_id": "1611.03382",
          "start_idx": 21894,
          "end_idx": 22215
        },
        "page_content": " significantly speed up inference. We have demonstrated the effectiveness of our approach in the context of summarization and shown state-of-the-art performance. In the future, we plan to tackle summarization problems with large input text. We also plan to exploit our findings in other tasks such as machine translation.",
        "type": "Document"
      },
      {
        "id": "89d1ea67-80ed-45c7-9e61-036a175f2b80",
        "metadata": {
          "vector_store_key": "1905.10044-3",
          "chunk_id": 2,
          "document_id": "1905.10044",
          "start_idx": 717,
          "end_idx": 1250
        },
        "page_content": ") proposed the task of labeling candidate statements as being entailed or contradicted by a given passage. However, in practice, generating candidate statements that test for complex inferential abilities is challenging. For instance, evidence suggests BIBREF0 , BIBREF1 , BIBREF2 that simply asking human annotators to write candidate statements will result in examples that typically only require surface-level reasoning. In this paper we propose an alternative: we test models on their ability to answer naturally occurring yes/no",
        "type": "Document"
      },
      {
        "id": "9a94f7ec-d90f-49f5-938a-1010613fa503",
        "metadata": {
          "vector_store_key": "1905.10238-2",
          "chunk_id": 9,
          "document_id": "1905.10238",
          "start_idx": 3866,
          "end_idx": 4344
        },
        "page_content": " to make the decision. Experiments are conducted on a widely used evaluation dataset, where the results prove that the proposed model outperforms all baseline models by a great margin. Above all, to summarize, this paper makes the following contributions: Following the conventional setting BIBREF1 , the task of pronoun coreference resolution is defined as: for a pronoun $p$ and a candidate noun phrase set ${\\mathcal {N}}$ , the goal is to identify the correct non-pronominal",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What advantages does their proposed method of quantifying interpretability have over the human-in-the-loop evaluation they compare to?",
    "llm_answer": "The provided text describes a method for evaluating machine translation and natural language generation, not a method for quantifying interpretability.  Therefore, a comparison to a human-in-the-loop evaluation for interpretability is not found in the context.\n",
    "context": [
      {
        "id": "c1484a9f-d7af-4d07-9461-1d96d91be2bc",
        "metadata": {
          "vector_store_key": "2004.01694-7",
          "chunk_id": 4,
          "document_id": "2004.01694",
          "start_idx": 1628,
          "end_idx": 2197
        },
        "page_content": " changes in the evaluation design affect the outcome of the human evaluation. We find that for all three aspects, human translations are judged more favourably, and significantly better than MT, when we make changes that we believe strengthen the evaluation design. Based on our empirical findings, we formulate a set of recommendations for human MT evaluation in general, and assessing human\u2013machine parity in particular. All of our data are made publicly available for external validation and further analysis. We first review current methods to assess the quality of",
        "type": "Document"
      },
      {
        "id": "65420d2b-c729-4d92-a761-1ae85866f47a",
        "metadata": {
          "vector_store_key": "2004.01694-7",
          "chunk_id": 3,
          "document_id": "2004.01694",
          "start_idx": 1337,
          "end_idx": 1854
        },
        "page_content": " concur that these findings have demonstrated the need to critically re-evaluate the design of human MT evaluation. Our paper investigates three aspects of human MT evaluation, with a special focus on assessing human\u2013machine parity: the choice of raters, the use of linguistic context, and the creation of reference translations. We focus on the data shared by BIBREF3, and empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation. We find that for all three aspects",
        "type": "Document"
      },
      {
        "id": "182ba0c4-c36f-4213-95de-b4014f3146b7",
        "metadata": {
          "vector_store_key": "2002.05058-9",
          "chunk_id": 63,
          "document_id": "2002.05058",
          "start_idx": 24978,
          "end_idx": 25505
        },
        "page_content": " To ensure the computational budget to be roughly the same, we fix the number of plays in skill rating to 10,000. The experimental results are summarized in Table 1. We can see that the proposed comparative evaluator correlates far better with human judgment than BLEU and perplexity. When compared with recently proposed parameterized metrics including adversarial evaluator and ADEM, our model consistently outperforms them by a large margin, which demonstrates that our comparison-based evaluation metric is able to evaluate",
        "type": "Document"
      },
      {
        "id": "0aa4db32-cbac-4ffa-81bd-765ef03bfb78",
        "metadata": {
          "vector_store_key": "1809.08731-4",
          "chunk_id": 2,
          "document_id": "1809.08731",
          "start_idx": 812,
          "end_idx": 1267
        },
        "page_content": " evaluation, especially of fluency, causing word-overlap metrics like ROUGE BIBREF1 to correlate only weakly with human judgments BIBREF2 . As a result, fluency evaluation of NLG is often done manually, which is costly and time-consuming. Evaluating sentences on their fluency, on the other hand, is a linguistic ability of humans which has been the subject of a decade-long debate in cognitive science. In particular, the question has been raised whether",
        "type": "Document"
      },
      {
        "id": "4786aca0-d17c-410b-a05b-652f20df7fc0",
        "metadata": {
          "vector_store_key": "2002.05058-9",
          "chunk_id": 10,
          "document_id": "2002.05058",
          "start_idx": 4096,
          "end_idx": 4674
        },
        "page_content": " evaluation information provided by our pairwise comparison model. The skill rating system is more efficient and accurate than several baseline approaches. We conduct experiments on both story generation task and open domain dialogue response generation task. Experimental results show that our approach correlates better with human evaluation on both datasets. Moreover, we show that using automated metrics such as BLEU to perform hyperparameter tuning and early-stopping results in sub-optimal model and our approach helps alleviate this problem. Evaluation of NLG models has",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How was lexical diversity measured?",
    "llm_answer": "Yule's I measure and the Measure of Textual Lexical Diversity (MTLD).\n",
    "context": [
      {
        "id": "bdedd992-cbec-4e21-99f4-717309f43e18",
        "metadata": {
          "vector_store_key": "1911.00133-7",
          "chunk_id": 29,
          "document_id": "1911.00133",
          "start_idx": 11772,
          "end_idx": 12211
        },
        "page_content": " and the least in the financial domain). We also examine the overall lexical diversity of each domain by calculating Yule's I measure BIBREF13. fig:domain-yule shows the lexical diversity of our data, both for all words in the vocabulary and for only words in LIWC's \u201cnegemo\u201d word list. Yule's I measure reflects the repetitiveness of the data (as opposed to the broader coverage measured by our LIWC analysis). We notice exceptionally low",
        "type": "Document"
      },
      {
        "id": "d3f00886-553f-4e12-bfeb-c5cd15fdb4c1",
        "metadata": {
          "vector_store_key": "1703.05260-9",
          "chunk_id": 76,
          "document_id": "1703.05260",
          "start_idx": 27528,
          "end_idx": 27991
        },
        "page_content": " to measure this variance. However, this measure is known to be sensitive to text length (see e.g. Tweedie1998), which would result in very small values for InScript and relatively large ones for DeScript, given the large average difference of text lengths between the corpora. Instead, we decided to use the Measure of Textual Lexical Diversity (MTLD) (McCarthy2010, McCarthy2005), which is familiar in corpus linguistics. This metric measures the average number",
        "type": "Document"
      },
      {
        "id": "f909940b-ac30-48e6-b46a-43457b003dc6",
        "metadata": {
          "vector_store_key": "2001.05467-1",
          "chunk_id": 38,
          "document_id": "2001.05467",
          "start_idx": 14084,
          "end_idx": 14504
        },
        "page_content": " for the goal-oriented Ubuntu Dialogue Corpus\". We did not evaluate on BLEU score BIBREF8 because BIBREF17 showed that BLEU does not correlate well with dialogue quality. BIBREF18 (BIBREF18) also made similar observations on BLEU. To evaluate diversity, we employ two evaluation metrics from previous work, namely Distinct-1 and Distinct-2 BIBREF2. These are the ratios between the number of unique tokens and all tokens",
        "type": "Document"
      },
      {
        "id": "97c40357-d6ec-4d5b-ae12-ad27e05f02c5",
        "metadata": {
          "vector_store_key": "2001.05467-1",
          "chunk_id": 56,
          "document_id": "2001.05467",
          "start_idx": 20825,
          "end_idx": 21301
        },
        "page_content": " coherent and informative responses. Multiple metrics and approaches have been proposed to measure dialogue diversity. Some focus more on how similar the responses are to the ground-truth sequences, such as Word Error Rate BIBREF3 and BLEU BIBREF20, while the others explicitly have diversity in mind when being created, such as Distinct-1 and -2 BIBREF2. The key difference between AvgOut and the previous work is that first, our metric is dynamic with no feature-engineering",
        "type": "Document"
      },
      {
        "id": "65ae3d61-f756-4835-a588-203e4f89d2b0",
        "metadata": {
          "vector_store_key": "1804.08186-1",
          "chunk_id": 180,
          "document_id": "1804.08186",
          "start_idx": 64991,
          "end_idx": 65476
        },
        "page_content": " the feature weights and divided them by the number of words in the test document in order to set a threshold to detect unknown languages. Their language identifier obtained 89% precision and 94% recall when classifying documents between five languages. BIBREF192 used a weighting method combining alphabets, prefixes, suffixes and words. BIBREF233 summed up values from a word trigram ranking, basic dictionary and morphological analyzer lookup. BIBREF282 summed up language labels of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which existing models does this approach outperform?",
    "llm_answer": "Regression-based approaches, models using manually defined knowledge and features, and prior state-of-the-art NLI models (BiMPM, ESIM, Decomposable Attention Model, KIM, and BERT).\n",
    "context": [
      {
        "id": "e5b6b9e3-3575-4f19-9098-cde989337e2b",
        "metadata": {
          "vector_store_key": "2002.05058-1",
          "chunk_id": 80,
          "document_id": "2002.05058",
          "start_idx": 32135,
          "end_idx": 32299
        },
        "page_content": " models as they may lead to sub-optimal hyperparameter choice and checkpoint selection. Our model, in contrast, is much more reliable when performing these choices.",
        "type": "Document"
      },
      {
        "id": "644dd4da-698b-443e-aeae-137d54b264d9",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 23,
          "document_id": "1909.00542",
          "start_idx": 8896,
          "end_idx": 9426
        },
        "page_content": " the results are compatible with the top part of the table and prior work suggesting that classification-based approaches improve over regression-based approaches. We also experiment with the use of reinforcement learning techniques. Again these experiments are based on BIBREF2, who uses REINFORCE to train a global policy. The policy predictor uses a simple feedforward network with a hidden layer. The results reported by BIBREF2 used ROUGE Recall and indicated no improvement with respect to deep learning architectures. Human",
        "type": "Document"
      },
      {
        "id": "cb3d7b88-2e37-4e70-8b21-78f3ff9093de",
        "metadata": {
          "vector_store_key": "1905.10238-1",
          "chunk_id": 57,
          "document_id": "1905.10238",
          "start_idx": 18003,
          "end_idx": 18545
        },
        "page_content": " the best one is selected based on its performance on the development set. Table 2 compares the performance of our model with all baselines. Overall, our model performs the best with respect to all evaluation metrics. Several findings are also observed from the results. First, manually defined knowledge and features are not enough to cover rich contextual information. Deep learning models (e.g., End2end and our proposed models), which leverage text representations for context, outperform other approaches by a great margin, especially on",
        "type": "Document"
      },
      {
        "id": "7b6a405f-eb7a-4d59-8247-20fd3864055f",
        "metadata": {
          "vector_store_key": "1906.06448-5",
          "chunk_id": 47,
          "document_id": "1906.06448",
          "start_idx": 17189,
          "end_idx": 17518
        },
        "page_content": " five state-of-the-art NLI models adopting different approaches: BiMPM (Bilateral Multi-Perspective Matching Model; BIBREF31 , BIBREF31 ), ESIM (Enhanced Sequential Inference Model; BIBREF32 , BIBREF32 ), Decomposable Attention Model BIBREF33 , KIM (Knowledge-based Inference Model; BIBREF34 , BIBREF34 ), and BERT (Bidirectional",
        "type": "Document"
      },
      {
        "id": "8f559cb6-96ff-446d-aaf3-7673d4a1d24e",
        "metadata": {
          "vector_store_key": "1908.02402-7",
          "chunk_id": 101,
          "document_id": "1908.02402",
          "start_idx": 29500,
          "end_idx": 29733
        },
        "page_content": "-the-art models, while at the same time providing a more practical solution for real-world applications. We would like to thank Alexandros Papangelis, Janice Lam, Stefan Douglas Webb and SIGDIAL reviewers for their valuable comments.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What human evaluation method is proposed?",
    "llm_answer": "Annotators evaluate the summary against the source content based on the goodness of the summary.\n",
    "context": [
      {
        "id": "a1680938-1a18-4b7b-97c3-a77f4fee0bb0",
        "metadata": {
          "vector_store_key": "1805.04033-4",
          "chunk_id": 74,
          "document_id": "1805.04033",
          "start_idx": 28680,
          "end_idx": 29253
        },
        "page_content": " eliminating the less semantically-related word correspondence. The proposed human evaluation method is effective and efficient in judging the semantic consistency, which is absent in previous work but is crucial in the accurate evaluation of the text summarization systems. The proposed metric is simple to conduct and easy to interpret. It also provides an insight on how practicable the existing systems are in the real-world scenario. For human evaluation, the annotators are asked to evaluate the summary against the source content based on the goodness of the summary",
        "type": "Document"
      },
      {
        "id": "c1484a9f-d7af-4d07-9461-1d96d91be2bc",
        "metadata": {
          "vector_store_key": "2004.01694-7",
          "chunk_id": 4,
          "document_id": "2004.01694",
          "start_idx": 1628,
          "end_idx": 2197
        },
        "page_content": " changes in the evaluation design affect the outcome of the human evaluation. We find that for all three aspects, human translations are judged more favourably, and significantly better than MT, when we make changes that we believe strengthen the evaluation design. Based on our empirical findings, we formulate a set of recommendations for human MT evaluation in general, and assessing human\u2013machine parity in particular. All of our data are made publicly available for external validation and further analysis. We first review current methods to assess the quality of",
        "type": "Document"
      },
      {
        "id": "3b921cbd-e8cf-4b08-8097-48820c27b38c",
        "metadata": {
          "vector_store_key": "2004.01694-7",
          "chunk_id": 91,
          "document_id": "2004.01694",
          "start_idx": 35319,
          "end_idx": 35678
        },
        "page_content": " human evaluation of MT in general. Our recommendations have the aim of increasing the validity of MT evaluation, but we are aware of the high cost of having MT evaluation done by professional translators, and on the level of full documents. We welcome future research into alternative evaluation protocols that can demonstrate their validity at a lower cost.",
        "type": "Document"
      },
      {
        "id": "65420d2b-c729-4d92-a761-1ae85866f47a",
        "metadata": {
          "vector_store_key": "2004.01694-7",
          "chunk_id": 3,
          "document_id": "2004.01694",
          "start_idx": 1337,
          "end_idx": 1854
        },
        "page_content": " concur that these findings have demonstrated the need to critically re-evaluate the design of human MT evaluation. Our paper investigates three aspects of human MT evaluation, with a special focus on assessing human\u2013machine parity: the choice of raters, the use of linguistic context, and the creation of reference translations. We focus on the data shared by BIBREF3, and empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation. We find that for all three aspects",
        "type": "Document"
      },
      {
        "id": "67c7bdd9-8926-4b32-b3b7-4b1914e74192",
        "metadata": {
          "vector_store_key": "1601.02543-1",
          "chunk_id": 2,
          "document_id": "1601.02543",
          "start_idx": 845,
          "end_idx": 1419
        },
        "page_content": " successful and unsuccessful transactions. This evaluation is then used to identifying any further improvement in the speech recognition based solution to better the overall transaction completion rates. This process of evaluation is both time consuming and expensive. For evaluation one needs to identify a set of users and also identify the set of actual usage situations and perform the test. It is also important that the set of users are able to use the system with ease meaning that even in the test conditions the performance of the system, should be good, while this",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What languages are represented in the dataset?",
    "llm_answer": "English, French, German, Spanish, Russian, Italian.\n",
    "context": [
      {
        "id": "b0d78a46-0cfa-4367-a963-2a9a8d82c9a4",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 18,
          "document_id": "1910.04269",
          "start_idx": 6853,
          "end_idx": 7342
        },
        "page_content": " comprising of 3 languages \u2013 English, French and German. In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (",
        "type": "Document"
      },
      {
        "id": "ec4f5a1a-ee42-43ff-81a9-ab41c4ff8914",
        "metadata": {
          "vector_store_key": "1909.06522-0",
          "chunk_id": 32,
          "document_id": "1909.06522",
          "start_idx": 11616,
          "end_idx": 12081
        },
        "page_content": ": live streams from users. UserVOD (video on demand): was-live videos. For each language, the train and test set size are described in Table TABREF10, and most training data were Pages. On each language we also had a small validation set for model parameter tuning. Each monolingual ASR baseline was trained on language-specific data only. The character sets of these 7 languages have little overlap except that (i) they all include common basic Latin alphabet, and",
        "type": "Document"
      },
      {
        "id": "e90f75a5-f326-4202-82c5-878736af8308",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 51,
          "document_id": "1910.04269",
          "start_idx": 19553,
          "end_idx": 20023
        },
        "page_content": ", the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Kat",
        "type": "Document"
      },
      {
        "id": "ef088dd9-ab26-43d4-a236-5cdf234c79f7",
        "metadata": {
          "vector_store_key": "1910.11493-6",
          "chunk_id": 13,
          "document_id": "1910.11493",
          "start_idx": 5173,
          "end_idx": 5630
        },
        "page_content": " languages. Data for all but four languages (Basque, Kurmanji, Murrinhpatha, and Sorani) are extracted from English Wiktionary, a large multi-lingual crowd-sourced dictionary with morphological paradigms for many lemmata. 20 of the 100 language pairs are either distantly related or unrelated; this allows speculation into the relative importance of data quantity and linguistic relatedness. For each language, the basic data consists of triples of the form",
        "type": "Document"
      },
      {
        "id": "7b1c09b6-d8db-4d35-8a92-0acba47cc1bc",
        "metadata": {
          "vector_store_key": "1909.13362-4",
          "chunk_id": 44,
          "document_id": "1909.13362",
          "start_idx": 15624,
          "end_idx": 16095
        },
        "page_content": " language groupings within families. We selected six evaluation languages: English, Dutch, Italian, French, Basque, and Manipuri. These represent two language families (Indo-European, Sino-Tibetan), a language isolate thought to be unrelated to any existing language (Basque), and two different subfamilies within the Indo-European family (West Germanic, Romance). The primary constraint was the availability of syllabified datasets for training and testing. Table TABREF",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How faster is training and decoding compared to former models?",
    "llm_answer": "The provided text states that the model results in faster decoding time, but does not compare it to *former* models.  The answer is not found in the context.\n",
    "context": [
      {
        "id": "a3ee228f-ed03-4cc3-8058-cdbd4ce7849e",
        "metadata": {
          "vector_store_key": "1707.00110-2",
          "chunk_id": 45,
          "document_id": "1707.00110",
          "start_idx": 14339,
          "end_idx": 14828
        },
        "page_content": ". Table 3 shows that our model results in faster decoding time even on a complex dataset with a large vocabulary of 16k. We measured decoding time over the full validation set, not including time used for model setup and data loading, averaged across 10 runs. The average sequence length for examples in this data was 35, and we expect more significant speedups for tasks with longer sequences, as suggested by our experiments on toy data. Note that in our NMT examples/experiments, $K\\app",
        "type": "Document"
      },
      {
        "id": "f1726c84-30f7-42b1-9aa3-c03df09d1f30",
        "metadata": {
          "vector_store_key": "1912.06813-3",
          "chunk_id": 56,
          "document_id": "1912.06813",
          "start_idx": 18935,
          "end_idx": 19463
        },
        "page_content": ". First, when the network was trained from scratch without any pretraining, the performance was not satisfactory even with the full training set. With decoder pretraining, a performance boost in MCD was obtained, whereas the ASR results were similar. Nonetheless, as we reduced the training size, the performance dropped dramatically, a similar trend to that reported in BIBREF12. Finally, by incorporating encoder pretraining, the model exhibited a significant improvement in all objective measures, where the effectiveness was",
        "type": "Document"
      },
      {
        "id": "37eeda85-6f98-4abd-af37-24d9898c1d3b",
        "metadata": {
          "vector_store_key": "1810.03459-2",
          "chunk_id": 22,
          "document_id": "1810.03459",
          "start_idx": 8080,
          "end_idx": 8548
        },
        "page_content": "REF20 shows the performance of simply retraining the last layer using a single target language Assamese. Based on the observations from stage-1 model in section SECREF22 , we found that simply retraining the decoder towards a target language resulted in degrading %CER the performance from 45.6 to 61.3. This is mainly due to the difference in distribution across encoder and decoder. So, to alleviate this difference the encoder and decoder is once again retrained or",
        "type": "Document"
      },
      {
        "id": "a14e00d5-68d3-4bea-b9d6-6ad0992afd02",
        "metadata": {
          "vector_store_key": "1811.00942-2",
          "chunk_id": 39,
          "document_id": "1811.00942",
          "start_idx": 12674,
          "end_idx": 13079
        },
        "page_content": " over 1.2 seconds per next-word prediction. Neural models achieve perplexity drops of 60\u201380% and R@3 increases of 22\u201334%, but these improvements come at a much higher cost in latency and energy usage. In Table 2 (last two columns), the desktop yields very different results: the neural models on PTB (rows 2\u20133) are 9 $\\times $ slower than KN-5, but the absolute latency is only 8 ms/q, which is still much",
        "type": "Document"
      },
      {
        "id": "f4bab4a3-ee16-4c02-9cf2-547796dafd00",
        "metadata": {
          "vector_store_key": "1909.02480-2",
          "chunk_id": 74,
          "document_id": "1909.02480",
          "start_idx": 23944,
          "end_idx": 24420
        },
        "page_content": " iterative refinement to further improve the performance of FlowSeq has been left to future work. In this section, we compare the decoding speed (measured in average time in seconds required to decode one sentence) of FlowSeq at test time with that of the autoregressive Transformer model. We use the test set of WMT14 EN-DE for evaluation and all experiments are conducted on a single NVIDIA TITAN X GPU. First, we investigate how different decoding batch size can affect the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What datasets are used to evaluate the model?",
    "llm_answer": "Swissmetro, Oshiete-goo, and nfL6 datasets.\n",
    "context": [
      {
        "id": "3488785c-d053-43b7-8293-c3c50f3c5473",
        "metadata": {
          "vector_store_key": "1909.00154-2",
          "chunk_id": 65,
          "document_id": "1909.00154",
          "start_idx": 24630,
          "end_idx": 25057
        },
        "page_content": " vectors) Development set: 20% of the dataset (2003 vectors) Test set: 20% of the dataset (2003 vectors) The PyLogit package BIBREF11 also uses Swissmetro as an example. Therefore, our model specifications will extend the default one from this package. We re-estimated this model with the train set and validated with testset. The results are shown in tables TABREF31 and TABREF32. Since we are comparing the models at the test",
        "type": "Document"
      },
      {
        "id": "8e2a448d-c8d4-43db-9500-b6ba58f58fe9",
        "metadata": {
          "vector_store_key": "1912.00864-5",
          "chunk_id": 70,
          "document_id": "1912.00864",
          "start_idx": 22055,
          "end_idx": 22516
        },
        "page_content": " three evaluations using the Oshiete-goo dataset; we selected three different sets of 500 human-annotated test pairs from the full dataset. In each set, we trained the model by using training pairs and input questions in test pairs to the model. We repeated the experiments three times by randomly shuffling the train/test sets. For the evaluations using the nfL6 dataset, we prepared three different sets of 500 human-annotated test q-c-s triples from the full",
        "type": "Document"
      },
      {
        "id": "88bf8a44-45a8-4b6e-975d-e8b16432107d",
        "metadata": {
          "vector_store_key": "1909.05438-3",
          "chunk_id": 57,
          "document_id": "1909.05438",
          "start_idx": 22119,
          "end_idx": 22656
        },
        "page_content": " starts from examples covered by rules, which are used to initialize a semantic parser and a question generator in a back-translation paradigm. Generated examples are measured and filtered based on statistic analysis, and then used with model-agnostic meta-learning, which guarantees model's accuracy and stability on rule-covered examples, and acquires the versatility to generalize well on rule-uncovered examples. We conduct experiments on three datasets for table-based and knowledge-based question answering tasks. Results show that",
        "type": "Document"
      },
      {
        "id": "c94d339e-36f5-4336-a21a-05be0b487cef",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      },
      {
        "id": "9cdcf3d4-51f3-43d9-9cea-7a3d8cdb29a9",
        "metadata": {
          "vector_store_key": "1909.00154-2",
          "chunk_id": 61,
          "document_id": "1909.00154",
          "start_idx": 23166,
          "end_idx": 23592
        },
        "page_content": "CCM), for example. We will apply the methodology to the well-known \u201cSwissmetro\" dataset. We will compare it with a dummy variables and PCA baselines. We will follow the 3-way experimental design mentioned before: split the dataset into train, development and test sets, so that the embeddings, PCA eigenvectors and the choice model are estimated from the same train and development sets, and validate it out-of-sample. For the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the source of the dataset?",
    "llm_answer": "The dataset sources include online debate websites, the OA-STM corpus, the testing images of MS COCO dataset, and the testing questions of US DARPA.\n",
    "context": [
      {
        "id": "0d593f74-7474-41cc-9928-ce68ac96f714",
        "metadata": {
          "vector_store_key": "1802.05574-2",
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "1c380a0c-1037-4628-8ada-168d9c8247bc",
        "metadata": {
          "vector_store_key": "1707.03904-9",
          "chunk_id": 13,
          "document_id": "1707.03904",
          "start_idx": 5304,
          "end_idx": 5850
        },
        "page_content": " subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short. We plan to release these annotations along with the datasets, and our retrieved documents for each question. Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated",
        "type": "Document"
      },
      {
        "id": "401df3ea-241b-476d-97f4-1276dc131769",
        "metadata": {
          "vector_store_key": "1701.00185-9",
          "chunk_id": 99,
          "document_id": "1701.00185",
          "start_idx": 35418,
          "end_idx": 35678
        },
        "page_content": "ASQ for making the datasets available. This work is supported by the National Natural Science Foundation of China (No. 61602479, No. 61303172, No. 61403385) and the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDB02070005).",
        "type": "Document"
      },
      {
        "id": "d1adc989-abd9-4bc0-b4a1-d8160e471e4a",
        "metadata": {
          "vector_store_key": "1703.06492-1",
          "chunk_id": 28,
          "document_id": "1703.06492",
          "start_idx": 9989,
          "end_idx": 10376
        },
        "page_content": " dataset, called Basic Question Dataset (BQD), generated by our basic question generation algorithm. BQD is the first basic question dataset. Regarding the BQD, the dataset format is $\\lbrace Image,~MQ,~3~(BQ + corresponding~similarity~score)\\rbrace $ . All of our images are from the testing images of MS COCO dataset BIBREF30 , the MQ, main questions, are from the testing questions of",
        "type": "Document"
      },
      {
        "id": "eb1ce3aa-59c4-45e6-a3ea-77b0dadb95e7",
        "metadata": {
          "vector_store_key": "1906.03538-1",
          "chunk_id": 64,
          "document_id": "1906.03538",
          "start_idx": 25144,
          "end_idx": 25628
        },
        "page_content": " US Defense Advanced Research Projects Agency (DARPA). The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. We provide brief statistics on the sources of different content in our dataset in Table TABREF46 . In particular, this table shows: the size of the data collected from online debate websites (step 1). the size of the data filtered out (step 2a). the size of the perspectives added",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What were the baselines?",
    "llm_answer": "Baseline 1: smooth L1 distance; Baseline 2: PCA-based symmetric acoustic similarity; Baseline 3: Nonlinear dynamical systems-based complexity measure; ANRES_Sing_R; ANRES_Sing_Z; ANRES_Seq_R; ANRES_Seq_Z.\n",
    "context": [
      {
        "id": "593127ef-0325-4226-be3e-0ed371ce653f",
        "metadata": {
          "vector_store_key": "1804.08782-7",
          "chunk_id": 37,
          "document_id": "1804.08782",
          "start_idx": 13990,
          "end_idx": 14427
        },
        "page_content": " number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 .  For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose",
        "type": "Document"
      },
      {
        "id": "fe0f18bc-f5e9-4a30-bd8f-1b27de376893",
        "metadata": {
          "vector_store_key": "2004.01878-5",
          "chunk_id": 40,
          "document_id": "2004.01878",
          "start_idx": 14526,
          "end_idx": 14915
        },
        "page_content": " normal distribution $\\mathcal {N}(0, 1)$. In summary, the following four baselines are designed: ANRES_Sing_R: randomly initializing the states for each single trading day. ANRES_Sing_Z: initializing the states as zeros for each single trading day. ANRES_Seq_R: randomly initializing the first states for each trading sequence only. ANRES_Seq_Z: initializing the first states as zeros for",
        "type": "Document"
      },
      {
        "id": "17d3256d-56f6-4b9d-983d-669c916f7fae",
        "metadata": {
          "vector_store_key": "1709.08299-2",
          "chunk_id": 23,
          "document_id": "1709.08299",
          "start_idx": 9435,
          "end_idx": 10003
        },
        "page_content": " questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models. The",
        "type": "Document"
      },
      {
        "id": "0b77285e-fdf5-4273-8639-5afcd5734567",
        "metadata": {
          "vector_store_key": "2001.02380-3",
          "chunk_id": 140,
          "document_id": "2001.02380",
          "start_idx": 51470,
          "end_idx": 51956
        },
        "page_content": " baselines. The left, middle and right panels in Figure FIGREF40 correspond to measurements when all signals are included, only cases contained entirely in the head EDUs shown to the model, and only DMs, respectively. The scenario on the left is rather unreasonable and is included only for completeness: here the model is also penalized for not detecting signals such as lexical chains, part of which is outside the units that the model is being shown. An example of such a case can be",
        "type": "Document"
      },
      {
        "id": "c8c4272e-a38c-48c3-a271-2ada66b9c8b5",
        "metadata": {
          "vector_store_key": "2004.01980-1",
          "chunk_id": 63,
          "document_id": "2004.01980",
          "start_idx": 22218,
          "end_idx": 22603
        },
        "page_content": " all baselines. We use the summarization-related evaluation metrics, i.e., BLEU, ROUGE, CIDEr, and METEOR, to measure how relevant the generated headlines are to the news articles, to some extent, by comparing them to the original human-written headlines. In Table TABREF59, the first row \u201cNHG\u201d shows the performance of the current state-of-the-art summarization model on our data, and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What metadata is included?",
    "llm_answer": "Normalized representation of the service interface (name, tasks, entities, descriptions).\n",
    "context": [
      {
        "id": "ef45af8c-651a-4e41-807f-59234a77d578",
        "metadata": {
          "vector_store_key": "2002.01359-5",
          "chunk_id": 21,
          "document_id": "2002.01359",
          "start_idx": 9023,
          "end_idx": 9552
        },
        "page_content": " optional parameters. In addition to the dialogues, for each service used in the dataset, a normalized representation of the interface exposed is provided as the schema. The schema contains details like the name of the service, the list of tasks supported by the service (intents) and the attributes of the entities used by the service (slots). The schema also contains natural language descriptions of the service, intents and slots which can be used for developing models which can condition their predictions on the schema. To",
        "type": "Document"
      },
      {
        "id": "8163e125-ccb9-4698-a596-24c3f09767ce",
        "metadata": {
          "vector_store_key": "1909.03242-3",
          "chunk_id": 63,
          "document_id": "1909.03242",
          "start_idx": 23711,
          "end_idx": 24250
        },
        "page_content": " no metadata at all. To further investigate this, we split the test set into those instances for which no metadata is available vs. those for which metadata is available. We find that encoding metadata within the model hurts performance for domains where no metadata is available, but improves performance where it is. In practice, an ensemble of both types of models would be sensible, as well as exploring more involved methods of encoding metadata. An analysis of labels frequently confused with one another, for the largest domain `pom",
        "type": "Document"
      },
      {
        "id": "45e75b36-ee14-4963-8966-836c2b4786de",
        "metadata": {
          "vector_store_key": "1911.07228-1",
          "chunk_id": 24,
          "document_id": "1911.07228",
          "start_idx": 6150,
          "end_idx": 6611
        },
        "page_content": " corpus which are provided by VLSP 2016 - Named Entity Recognition. The dataset contains four different types of label: Location (LOC), Person (PER), Organization (ORG) and Miscellaneous - Name of an entity that do not belong to 3 types above (Table TABREF15). Although the corpus has more information about the POS and chunks, but we do not use them as features in our model. There are two folders with 267 text files of training data and 45 text files of test",
        "type": "Document"
      },
      {
        "id": "4ce440cb-dc47-4b8a-b61c-dee4751879c3",
        "metadata": {
          "vector_store_key": "1610.00879-1",
          "chunk_id": 11,
          "document_id": "1610.00879",
          "start_idx": 4217,
          "end_idx": 4654
        },
        "page_content": "damn', along with profane words. Heuristics other than these hashtags could have been used for dataset creation. For example, timestamps were a good option to account for time at which a tweet was posted. However, this could not be used because user's local times was not available, since very few users had geolocation enabled. The complete set of features is shown in Table TABREF7 . There are two sets of features: (a) N-gram features",
        "type": "Document"
      },
      {
        "id": "5827c722-8da9-4c48-9957-4bcaaa150703",
        "metadata": {
          "vector_store_key": "2001.02380-4",
          "chunk_id": 41,
          "document_id": "2001.02380",
          "start_idx": 16317,
          "end_idx": 16722
        },
        "page_content": " annotated with a large number of layers, including document layout (headings, paragraphs, figures, etc.); multiple POS tags (Penn tags, CLAWS5, Universal POS); lemmas; sentence types (e.g. imperative, wh-question etc., BIBREF35); Universal Dependencies BIBREF36; (non-)named entity types; coreference and bridging resolution; and discourse parses using Rhetorical Structure Theory BIBREF11. In particular",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Do the images have multilingual annotations or monolingual ones?",
    "llm_answer": "Monolingual.\n",
    "context": [
      {
        "id": "f694b472-42c0-46c7-8eaa-05ef948f06ee",
        "metadata": {
          "vector_store_key": "1909.09270-4",
          "chunk_id": 38,
          "document_id": "1909.09270",
          "start_idx": 14123,
          "end_idx": 14565
        },
        "page_content": "hin: LDC2017E62), and Somali (som: LDC2016E91). These come from a variety of sources including discussion forums, newswire, and social media. The labelset is Person, Organization, Location, Geo-political entity. We define train/development/test splits, taking care to keep a similar distribution of genres in each split. Data statistics for all languages are shown in Table TABREF25. We create partial annotations by perturbing gold annotated",
        "type": "Document"
      },
      {
        "id": "f25202aa-5bfc-4704-b797-a9861607c5a0",
        "metadata": {
          "vector_store_key": "2002.02427-2",
          "chunk_id": 18,
          "document_id": "2002.02427",
          "start_idx": 6346,
          "end_idx": 6847
        },
        "page_content": "-art models in monolingual ID but to investigate which of the monolingual architectures (neural or feature-based) can achieve comparable results with existing systems. The result can show which kind of features works better in the monolingual settings and can be employed to detect irony in a multilingual setting. In addition, it can show us to what extend ID is language dependent by comparing their results to multilingual results. Two models have been built, as explained below. Prior to learning,",
        "type": "Document"
      },
      {
        "id": "a02630d1-cdfe-49cd-a14b-849a9c20fa12",
        "metadata": {
          "vector_store_key": "2002.10361-3",
          "chunk_id": 24,
          "document_id": "2002.10361",
          "start_idx": 9889,
          "end_idx": 10441
        },
        "page_content": " while acknowledging that our annotations are not ground truth. We obtained the annotations from the crowdsourcing website, Figure Eight (https://figure-eight.com/). We randomly sampled 50 users whose attributes came from Face++ in each language. We anonymize the user profiles and feed the information to the crowdsourcing website. Three annotators annotated each user photo with the binary demographic categories. To select qualified annotators and ensure quality of the evaluations, we set up 5 golden standard annotation questions for each language",
        "type": "Document"
      },
      {
        "id": "08e13582-c111-4fc2-bad8-4456564e89b2",
        "metadata": {
          "vector_store_key": "1912.07025-7",
          "chunk_id": 19,
          "document_id": "1912.07025",
          "start_idx": 7473,
          "end_idx": 8015
        },
        "page_content": " annotation. Our curated selection aims to maximize the diversity of the dataset in terms of various attributes such as the extent of document degradation, script language, presence of non-textual elements (e.g. pictures, tables) and number of lines. Some images contain multiple manuscript pages stacked vertically or horizontally (see bottom-left image in Figure FIGREF1). The second source for manuscript images in our dataset is Bhoomi, an assorted collection of 315 images sourced from multiple Oriental Research Institutes and libraries",
        "type": "Document"
      },
      {
        "id": "a26d3cbf-a0d7-418e-96b7-e87b9b1b5c93",
        "metadata": {
          "vector_store_key": "1905.12260-8",
          "chunk_id": 19,
          "document_id": "1905.12260",
          "start_idx": 7428,
          "end_idx": 7934
        },
        "page_content": " this for the multilingual setting, using no human-labeled data and instead using an adversarial approach to maximize alignment between monolingual embedding spaces given their structural similarities. This method (MUSE) outperforms previous approaches and represents the state-of-the-art. We compare it to our methods in Section \"Results and Conclusions\" . There has been other work using image-text data to improve image and caption representations for image tasks and to learn word translations BIBREF22",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much important is the visual grounding in the learning of the multilingual representations?",
    "llm_answer": "The provided text states that visual grounding has been applied to visual question answering, image captioning, and the learning of sentence representations, but it doesn't explicitly state how important it is for multilingual representations.  The answer is not found in the context.\n",
    "context": [
      {
        "id": "89881d81-b8bf-4c92-bdb9-c7267a61b506",
        "metadata": {
          "vector_store_key": "1807.03367-4",
          "chunk_id": 94,
          "document_id": "1807.03367",
          "start_idx": 35549,
          "end_idx": 36052
        },
        "page_content": " shown that grounding can lead to practical improvements on various natural language understanding tasks BIBREF14 , BIBREF31 . In robotics, researchers dissatisfied with purely symbolic accounts of meaning attempted to build robotic systems with the aim of grounding meaning in physical experience of the world BIBREF44 , BIBREF46 . Recently, grounding has also been applied to the learning of sentence representations BIBREF32 , image captioning BIBREF37 , BIBREF49 , visual question answering BIBREF12",
        "type": "Document"
      },
      {
        "id": "19a2925a-0964-4145-82fb-0e76ca5755a6",
        "metadata": {
          "vector_store_key": "1809.08510-2",
          "chunk_id": 37,
          "document_id": "1809.08510",
          "start_idx": 13149,
          "end_idx": 13685
        },
        "page_content": "G representations are useful in situations when there is a lack of data in an specific language. The language agnostics properties of UG embeddings allows us to do successful zero-shot learning without needing any parallel corpus, furthermore the ability to generalize from language modeling to sentiment attests for the universal properties of these representations. Although we aren't able to improve over the state of the art in a single language we are able to learn a model that does surprisingly well on a set of languages without",
        "type": "Document"
      },
      {
        "id": "a1f0b027-a348-45c5-9e55-a216de9d54b2",
        "metadata": {
          "vector_store_key": "1905.12260-7",
          "chunk_id": 48,
          "document_id": "1905.12260",
          "start_idx": 17707,
          "end_idx": 18119
        },
        "page_content": " \u201cpain\u201d) have a shared embedding. In this sense, shared surface forms are used as a bridge between languages. This is illustrated in Figure 3 . This may be helpful in certain cases, as for English \u201cactor\u201d and Spanish \u201cactor\u201d. The image representations leverage generic image features, exactly as in Section \"Leveraging Image Understanding\" . In our language-unaware experiment, we use $m = 200$ and $n = 100$ and",
        "type": "Document"
      },
      {
        "id": "1cbb6af6-a105-466f-9dcb-276ca1f0287f",
        "metadata": {
          "vector_store_key": "1809.04686-1",
          "chunk_id": 4,
          "document_id": "1809.04686",
          "start_idx": 1578,
          "end_idx": 2066
        },
        "page_content": " or multilingual NLP, the task of transferring knowledge from one language to another, serves as a good test bed for evaluating various transfer learning approaches. For cross-lingual NLP, the most widely studied approach is to use multilingual embeddings as features in neural network models. However, research has shown that representations learned in context are more effective BIBREF5 , BIBREF6 ; therefore, we aim at doing better than just using multilingual embeddings in the cross-",
        "type": "Document"
      },
      {
        "id": "da1392f2-fe5f-4bb4-a61d-ed6547b94328",
        "metadata": {
          "vector_store_key": "1808.00265-7",
          "chunk_id": 63,
          "document_id": "1808.00265",
          "start_idx": 23875,
          "end_idx": 24409
        },
        "page_content": " final training steps, which affects the accuracy of the classification module. Figure FIGREF11 shows qualitative results of the resulting visual grounding, including also a comparison with respect to no-attn model. In this work we have proposed a new method that is able to slightly outperform current state-of-the-art VQA systems, while also providing interpretable representations in the form of an explicitly trainable visual attention mechanism. Specifically, as a main result, our experiments provide evidence that the generated",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the generative model evaluated?",
    "llm_answer": "A classifier is trained on the generated dataset, and its accuracy is evaluated on the original test set.  The accuracy is compared to the accuracy of a classifier trained on the original training set and tested on the original test set.\n",
    "context": [
      {
        "id": "b98286e7-67c6-4bd1-8f34-d04e8ccc4062",
        "metadata": {
          "vector_store_key": "1607.06025-5",
          "chunk_id": 11,
          "document_id": "1607.06025",
          "start_idx": 4370,
          "end_idx": 4877
        },
        "page_content": " for every example in the original dataset to construct a new dataset. Next, a classifier is trained on the new dataset. Finally, the classifier is evaluated on the original test set. The accuracy of the classifier is the proposed quality metric for the generative model. It can be compared to the accuracy of the classifier trained on the original training set and tested on the original test set. The generative models learn solely from the original training set to regenerate the dataset. Thus, the model",
        "type": "Document"
      },
      {
        "id": "74ffa4b1-4c00-4943-9c16-b881c9f7befa",
        "metadata": {
          "vector_store_key": "1607.06025-5",
          "chunk_id": 15,
          "document_id": "1607.06025",
          "start_idx": 6054,
          "end_idx": 6581
        },
        "page_content": " evaluation metric for NLI generative models \u2013 the accuracy of the classifier trained on the generated dataset. In Section SECREF2 we present the related work. In Section SECREF3 the considered neural networks are presented. Besides the main generative networks, we also present classification and discriminative networks, which are used for evaluation. The results are presented in Section SECREF5 , where the generative models are evaluated and compared. From the experiments we can see that the best dataset was generated by",
        "type": "Document"
      },
      {
        "id": "156d8f88-6923-45a3-b4da-b71ec6a21d37",
        "metadata": {
          "vector_store_key": "1607.06025-5",
          "chunk_id": 12,
          "document_id": "1607.06025",
          "start_idx": 4668,
          "end_idx": 5227
        },
        "page_content": " The generative models learn solely from the original training set to regenerate the dataset. Thus, the model learns the distribution of the original dataset. Furthermore, the generated dataset is just a random sample from the estimated distribution. To determine how well did the generative model learn the distribution, we observe how close does the accuracy of the classifier trained on the generated dataset approach the accuracy of classifier trained on the original dataset. Our flagship generative network EmbedDecoder works in a similar fashion as the",
        "type": "Document"
      },
      {
        "id": "527efddc-f409-4aef-8206-033487e12ce1",
        "metadata": {
          "vector_store_key": "1809.04960-8",
          "chunk_id": 53,
          "document_id": "1809.04960",
          "start_idx": 20749,
          "end_idx": 21194
        },
        "page_content": "world settings. Generative-based models directly generate comments without a candidate set. We compare the generated comments of either the retrieval-based models or the generative models with the five reference comments. We select four popular metrics in text generation to compare the model outputs with the references: BLEU BIBREF14 , METEOR BIBREF15 , ROUGE BIBREF16 , CIDEr BIBREF17 . Table TABREF32 shows the performance for our models and",
        "type": "Document"
      },
      {
        "id": "e98dd94e-ac6e-4fa7-8199-5ecf374d3db9",
        "metadata": {
          "vector_store_key": "1607.06025-5",
          "chunk_id": 10,
          "document_id": "1607.06025",
          "start_idx": 3868,
          "end_idx": 4370
        },
        "page_content": " of generative models, which is also presented in Figure FIGREF2 . First, the generative model is trained on the original training dataset. Then, the premise and label from an example in the original dataset are taken as the input to the generative model to generate a new random hypothesis. The generated hypothesis is combined with the premise and the label to form a new unseen example. This is done for every example in the original dataset to construct a new dataset. Next, a classifier is trained",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is an example of a health-related tweet?",
    "llm_answer": "The provided text describes a study analyzing tweets about diabetes, diet, exercise, and obesity, but does not give an example of a tweet.  The answer is not found in the context.\n",
    "context": [
      {
        "id": "3190abe3-0f74-4c85-9fd1-e3eb41f0bc58",
        "metadata": {
          "vector_store_key": "1901.00439-6",
          "chunk_id": 0,
          "document_id": "1901.00439",
          "start_idx": 0,
          "end_idx": 485
        },
        "page_content": "Social media plays an important role in health informatics and Twitter has been one of the most influential social media channel for mining population-level health insights BIBREF0 , BIBREF1 , BIBREF2 . These insights range from forecasting of influenza epidemics BIBREF3 to predicting adverse drug reactions BIBREF4 . A notable challenge due to the short length of Twitter messages is categorization of tweets into topics in a supervised manner, i.e., topic classification, as well as",
        "type": "Document"
      },
      {
        "id": "28c43f07-7194-4a5e-93a4-cae3389baa7c",
        "metadata": {
          "vector_store_key": "1709.07916-4",
          "chunk_id": 11,
          "document_id": "1709.07916",
          "start_idx": 4288,
          "end_idx": 4806
        },
        "page_content": " uses semantic and linguistics analyses for disclosing health characteristics of opinions in tweets containing DDEO words. The present study included three phases: data collection, topic discovery, and topic-content analysis. This phase collected tweets using Twitter's Application Programming Interfaces (API) BIBREF43 . Within the Twitter API, diabetes, diet, exercise, and obesity were selected as the related words BIBREF4 and the related health areas BIBREF19 . Twitter's APIs provides both historic and real-time",
        "type": "Document"
      },
      {
        "id": "f4de6e6f-2189-4f79-aaae-6b504fdbf6e8",
        "metadata": {
          "vector_store_key": "1709.07916-4",
          "chunk_id": 7,
          "document_id": "1709.07916",
          "start_idx": 2788,
          "end_idx": 3144
        },
        "page_content": " BIBREF20 , BIBREF21 , BIBREF22 . Twitter data has been used for a wide range of health and non-health related applications, such as stock market BIBREF23 and election analysis BIBREF24 . Some examples of Twitter data analysis for health-related topics include: flu BIBREF25 , BIBREF26 , BIBREF27 , BIBREF28 , BIBREF29 , BIBREF30 , mental health BIBREF31 ,",
        "type": "Document"
      },
      {
        "id": "9dacf51a-a3ed-46ec-8d56-e39beefff39c",
        "metadata": {
          "vector_store_key": "1709.07916-2",
          "chunk_id": 9,
          "document_id": "1709.07916",
          "start_idx": 3349,
          "end_idx": 3907
        },
        "page_content": " BIBREF42 , BIBREF21 . The previous Twitter studies have dealt with extracting common topics of one health issue discussed by the users to better understand common themes; however, this study utilizes an innovative approach to computationally analyze unstructured health related text data exchanged via Twitter to characterize health opinions regarding four common health issues, including diabetes, diet, exercise and obesity (DDEO) on a population level. This study identifies the characteristics of the most common health opinions with respect to DDEO and",
        "type": "Document"
      },
      {
        "id": "a0215ca0-bc03-414a-9e4a-a5c2f1c8d5d8",
        "metadata": {
          "vector_store_key": "1709.07916-2",
          "chunk_id": 21,
          "document_id": "1709.07916",
          "start_idx": 8063,
          "end_idx": 8472
        },
        "page_content": " find health related topics. Obesity and Diabetes showed the highest and the lowest number of tweets (51.7% and 8.0%). Diet and Exercise formed 23.7% and 16.6% of the tweets (Table TABREF6 ). Out of all 4.5 million DDEO-related tweets returned by Tweeter's API, the LDA found 425 topics. We used LIWC to filter the detected 425 topics and found 222 health-related topics. Additionally, we labeled topics based",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the challenge for other language except English",
    "llm_answer": "The context describes a challenge for Swiss students learning English, labeling spoken responses as \"accept\" or \"reject\".  It also mentions a proposed international challenge focused on non-native children's speech recognition, with an English subset.  No other language-specific challenges are described.\n",
    "context": [
      {
        "id": "1920df44-401c-4a6d-a53c-a400f8cc87f0",
        "metadata": {
          "vector_store_key": "1910.05456-2",
          "chunk_id": 27,
          "document_id": "1910.05456",
          "start_idx": 9884,
          "end_idx": 10340
        },
        "page_content": " additionally makes use of internal stem changes (e.g., o $\\rightarrow $ ue). Since English and Spanish are both Indo-European languages, and, thus, relatively similar, we further add a third, unrelated target language. We choose Zulu (ZUL), a Bantoid language. In contrast to the first two, it is strongly prefixing. For pretraining, we choose languages with different degrees of relatedness and varying morphological similarity to English, Spanish, and Z",
        "type": "Document"
      },
      {
        "id": "7fed6b92-48c7-48e6-8ad9-2412ea5449bc",
        "metadata": {
          "vector_store_key": "1612.05270-0",
          "chunk_id": 4,
          "document_id": "1612.05270",
          "start_idx": 1650,
          "end_idx": 2169
        },
        "page_content": " tested in eight different languages: Spanish, English, Italian, Arabic, German, Portuguese, Russian and Swedish. We compare our approach ranking in three international contests: TASS'15, SemEval'15-16 and SENTIPOLC'14, for Spanish, English and Italian respectively; the remaining languages are compared directly with the results reported in the literature. The experimental results locate our approach in good positions for all considered competitions; and excellent results in the other five languages tested. Finally",
        "type": "Document"
      },
      {
        "id": "349de311-6df5-4579-b11d-e65907d392f8",
        "metadata": {
          "vector_store_key": "2001.08051-9",
          "chunk_id": 13,
          "document_id": "2001.08051",
          "start_idx": 5133,
          "end_idx": 5580
        },
        "page_content": " Swiss students learning English had to answer to both written and spoken prompts. The goal of this challenge is to label students' spoken responses as \u201caccept\u201d or \u201creject\u201d. Refer to BIBREF5 for details of the challenge and of the associated data sets. Many non-native speech corpora (mostly in English as target language) have been collected during the years. A list, though not recent, as well as a brief description of most of them can be found",
        "type": "Document"
      },
      {
        "id": "5109ecda-8519-4961-b747-2d51dc342f8f",
        "metadata": {
          "vector_store_key": "1911.03562-6",
          "chunk_id": 42,
          "document_id": "1911.03562",
          "start_idx": 15671,
          "end_idx": 16139
        },
        "page_content": " world for the speakers of that language. We know that much of the research in NLP is on English or uses English datasets. Many reasons have been proffered, and we will not go into that here. Instead, we will focus on estimating how much research pertains to non-English languages. We will make use of the idea that often when work is done focusing on a non-English language, then the language is mentioned in the title. We collected a list of 122 languages indexed by",
        "type": "Document"
      },
      {
        "id": "6e2c8219-2064-439e-a6a6-de954386340e",
        "metadata": {
          "vector_store_key": "2001.08051-9",
          "chunk_id": 53,
          "document_id": "2001.08051",
          "start_idx": 20668,
          "end_idx": 21204
        },
        "page_content": " check the available human scores and - if possible - merge or map the scores according to more general performance categories (e.g. delivery, language use, topic development) and an acknowledged scale (e.g. from 0 to 4). In particular, the proposal for an international challenge focused on non-native children speech recognition is being submitted where an English subset will be released and the perspective participants are invited to propose and evaluate state-of-art techniques for dealing with the multiple issues related to this",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many categories of offensive language were there?",
    "llm_answer": "Three\n",
    "context": [
      {
        "id": "8d8374eb-969f-41be-b1a2-ee2fa1dd57ce",
        "metadata": {
          "vector_store_key": "1909.12642-0",
          "chunk_id": 0,
          "document_id": "1909.12642",
          "start_idx": 0,
          "end_idx": 540
        },
        "page_content": "In social media, abusive language denotes a text which contains any form of unacceptable language in a post or a comment. Abusive language can be divided into hate speech, offensive language and profanity. Hate speech is a derogatory comment that hurts an entire group in terms of ethnicity, race or gender. Offensive language is similar to derogatory comment, but it is targeted towards an individual. Profanity refers to any use of unacceptable language without a specific target. While profanity is the least threatening, hate speech has",
        "type": "Document"
      },
      {
        "id": "52c592f9-5804-4449-b3f0-ee13005bb479",
        "metadata": {
          "vector_store_key": "1908.04531-1",
          "chunk_id": 20,
          "document_id": "1908.04531",
          "start_idx": 8157,
          "end_idx": 8586
        },
        "page_content": "-task A the goal is to classify posts as either offensive or not. Offensive posts include insults and threats as well as any form of untargeted profanity BIBREF17 . Each sample is annotated with one of the following labels: In English this could be a post such as #TheNunMovie was just as scary as I thought it would be. Clearly the critics don't think she is terrifyingly creepy. I like how it ties in with #TheConjuring series.",
        "type": "Document"
      },
      {
        "id": "9e9a0ffd-e6a3-4366-b103-c5b1abc7963e",
        "metadata": {
          "vector_store_key": "1908.04531-1",
          "chunk_id": 19,
          "document_id": "1908.04531",
          "start_idx": 7867,
          "end_idx": 8364
        },
        "page_content": "REF0 . Our work adopts this framing of the offensive language phenomenon. Offensive content is broken into three sub-tasks to be able to effectively identify both the type and the target of the offensive posts. These three sub-tasks are chosen with the objective of being able to capture different types of offensive language, such as hate speech and cyberbullying (section \"Background\" ). In sub-task A the goal is to classify posts as either offensive or not. Offensive posts include insults and",
        "type": "Document"
      },
      {
        "id": "f983fa2a-8886-4318-a4d6-6f6b83cc5a09",
        "metadata": {
          "vector_store_key": "1908.04531-1",
          "chunk_id": 22,
          "document_id": "1908.04531",
          "start_idx": 8799,
          "end_idx": 9276
        },
        "page_content": "aggot... In sub-task B the goal is to classify the type of offensive language by determining if the offensive language is targeted or not. Targeted offensive language contains insults and threats to an individual, group, or others BIBREF17 . Untargeted posts contain general profanity while not clearly targeting anyone BIBREF17 . Only posts labeled as offensive (OFF) in sub-task A are considered in this task. Each sample is annotated with one of the following labels: Target",
        "type": "Document"
      },
      {
        "id": "fdf45ca4-e5a7-463d-b8bd-07d84c2029b1",
        "metadata": {
          "vector_store_key": "1908.04531-1",
          "chunk_id": 27,
          "document_id": "1908.04531",
          "start_idx": 10587,
          "end_idx": 10998
        },
        "page_content": " this could be e.g. \u00c5h nej! Svensk lorteret! Other (OTH): The target of the offensive language does not fit the criteria of either of the previous two categories. BIBREF17 . In English this could be a post such as And these entertainment agencies just gonna have to be an ass about it.. In Danish this could be a post such as Netto er jo et tempel over lort. One of the main concerns when it comes to collecting",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which matching features do they employ?",
    "llm_answer": "INLINEFORM6, INLINEFORM7, INLINEFORM8, INLINEFORM9, INLINEFORM10, Brown BIBREF32 clusters, Clark BIBREF33 clusters, and Word2vec BIBREF34 clusters.\n",
    "context": [
      {
        "id": "45ed9451-dcb0-4640-8136-7b45dbc76a9c",
        "metadata": {
          "vector_store_key": "1901.02222-2",
          "chunk_id": 29,
          "document_id": "1901.02222",
          "start_idx": 10251,
          "end_idx": 10759
        },
        "page_content": " , which preserves the different parts and removes the similar parts. And the INLINEFORM6 can be regarded as a sim-feature of INLINEFORM7 and INLINEFORM8 , which emphasizes on the similar parts and neglects the different parts between INLINEFORM9 and INLINEFORM10 . Each feature helps us focus on particular parts between the context vectors and the aligned vectors. These matching features are vector representations with low dimension, but containing high-order semantic information. To make further use of",
        "type": "Document"
      },
      {
        "id": "2b615793-d101-4407-ba6e-bbb66f6419af",
        "metadata": {
          "vector_store_key": "1901.09755-9",
          "chunk_id": 45,
          "document_id": "1901.09755",
          "start_idx": 17261,
          "end_idx": 17633
        },
        "page_content": ", shallow features based mostly on orthographic, word shape and n-gram features plus their context; and (ii) three types of simple clustering features, based on unigram matching: (i) Brown BIBREF32 clusters, taking the 4th, 8th, 12th and 20th node in the path; (ii) Clark BIBREF33 clusters and, (iii) Word2vec BIBREF34 clusters, based on K-means applied over the extracted",
        "type": "Document"
      },
      {
        "id": "145e5821-d708-484a-bd84-4d007becc83b",
        "metadata": {
          "vector_store_key": "1901.02222-2",
          "chunk_id": 30,
          "document_id": "1901.02222",
          "start_idx": 10545,
          "end_idx": 11058
        },
        "page_content": " are vector representations with low dimension, but containing high-order semantic information. To make further use of these matching features, we collect them to generate a matching sequence INLINEFORM11 . DISPLAYFORM0   where INLINEFORM0 . The output of this layer is the matching sequence INLINEFORM0 , which stores three kinds of matching features. The order of the matching features in INLINEFORM1 is inspired by the attention trajectory of human beings making inference on premise and hypothesis. We process",
        "type": "Document"
      },
      {
        "id": "beaea313-8252-4a6c-83d3-7d42cea5bcf6",
        "metadata": {
          "vector_store_key": "1902.09087-0",
          "chunk_id": 14,
          "document_id": "1902.09087",
          "start_idx": 5594,
          "end_idx": 6057
        },
        "page_content": " features to get the sentence level representations, which are merged via element-wise multiplication. The matching score is produced by a multi-layer perceptron (MLP) with one hidden layer based on the merged vector. The fusing and matching procedure is formulated as follows: DISPLAYFORM0  where INLINEFORM0 and INLINEFORM1 are feature vectors of question and candidate (sentence or predicate) separately encoded by CNNs, INLINEFORM2 is the sigmoid function, IN",
        "type": "Document"
      },
      {
        "id": "859a299c-1d4d-4b85-9cb5-8e8f7e2b0f8d",
        "metadata": {
          "vector_store_key": "1901.02222-0",
          "chunk_id": 32,
          "document_id": "1901.02222",
          "start_idx": 11363,
          "end_idx": 11897
        },
        "page_content": " move the attention to the different parts. Finally, we pay attention to the similar parts. In this layer, we aim to acquire inference outputs by aggregating the information in the matching sequence by multi-turn inference mechanism. We regard the inference on the matching sequence as the multi-turn interaction among various matching features. In each turn, we process one matching feature instead of all the matching features BIBREF9 , BIBREF26 . To enhance the information interaction between matching features, a memory component",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does their method outperform the multi-head attention model?",
    "llm_answer": "The context does not state how much better the described method is than a multi-head attention model.  It only notes that multi-head attention *can* outperform other methods but does not give a specific performance difference.\n",
    "context": [
      {
        "id": "d02b3119-278b-4832-85ea-abdce4f092a5",
        "metadata": {
          "vector_store_key": "1910.09295-3",
          "chunk_id": 50,
          "document_id": "1910.09295",
          "start_idx": 17498,
          "end_idx": 17963
        },
        "page_content": " considerable amount of performance, constituting 44.32% of the overall performance (a boost of 42.67% in accuracy) in the multitasking setup, and constituting 43.93% of the overall performance (a boost of 39.97%) in the standard finetuning setup. This provides evidence that the pretraining step is necessary in achieving state-of-the-art performance. An ablation study was done to establish causality between the multiheaded nature of the attention mechanisms and",
        "type": "Document"
      },
      {
        "id": "1090f256-bf33-4670-bc00-48828025fea4",
        "metadata": {
          "vector_store_key": "1908.06606-8",
          "chunk_id": 57,
          "document_id": "1908.06606",
          "start_idx": 20051,
          "end_idx": 20502
        },
        "page_content": " on both periods achieved the best performance on both EM-score and F$_1$-score. Unfortunately, applying multi-head attention on both period one and period two can not reach convergence in our experiments. This probably because it makes the model too complex to train. The difference on other two methods are the order of concatenation and multi-head attention. Applying multi-head attention on two named entity information $I_{nt}$ and $I_{nq}$ first",
        "type": "Document"
      },
      {
        "id": "7b560fe7-cfd1-4c23-9034-a2c81a68ff8b",
        "metadata": {
          "vector_store_key": "1910.09295-3",
          "chunk_id": 51,
          "document_id": "1910.09295",
          "start_idx": 17963,
          "end_idx": 18444
        },
        "page_content": " An ablation study was done to establish causality between the multiheaded nature of the attention mechanisms and state-of-the-art performance. We posit that since the model can refer to multiple context points at once, it improves in performance. For this experiment, we performed several pretraining-finetuning setups with varied numbers of attention heads using the multitask-based finetuning scheme. Using a pretrained GPT-2 model, attention heads were masked with zero-tensors",
        "type": "Document"
      },
      {
        "id": "24553ac0-2750-4ac8-ac1b-597c99567773",
        "metadata": {
          "vector_store_key": "1908.06606-8",
          "chunk_id": 60,
          "document_id": "1908.06606",
          "start_idx": 21038,
          "end_idx": 21574
        },
        "page_content": " multi-head attention method on both periods can not converge. Although Table TABREF27 shows the best integration method is concatenation, multi-head attention still has great potential. Due to the lack of computational resources, our experiment fixed the head number and hidden vector size. However, tuning these hyper parameters may have impact on the result. Tuning integration method and try to utilize larger datasets may give help to improving the performance. To investigate how shared task and shared model can benefit, we split",
        "type": "Document"
      },
      {
        "id": "d4a3d0fa-a715-42d0-9582-69d4b0d9d687",
        "metadata": {
          "vector_store_key": "1910.09295-3",
          "chunk_id": 53,
          "document_id": "1910.09295",
          "start_idx": 18749,
          "end_idx": 19277
        },
        "page_content": " the performance of 10 heads using the standard finetuning scheme. This shows that more attention heads, thereby attending to multiple different contexts at once, is important to boosting performance to state-of-the-art results. While increasing the number of attention heads improves performance, keeping on adding extra heads will not result to an equivalent boost as the performance plateaus after a number of heads. As shown in Figure FIGREF35, the performance boost of the model plateaus after 10 attention heads, which was",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How large is the corpus they use?",
    "llm_answer": "13 million NRLS records, 1 million and 2 million record subsets, 5 million articles from English Wikipedia.  Also, a 5 billion word corpus from Amazon.  Various other datasets of varying sizes are mentioned.\n",
    "context": [
      {
        "id": "2a50498c-e7d6-4306-b2ba-3b5dab7d3598",
        "metadata": {
          "vector_store_key": "1909.00183-3",
          "chunk_id": 34,
          "document_id": "1909.00183",
          "start_idx": 13387,
          "end_idx": 13902
        },
        "page_content": " of different sizes and content with different sets of hyper-parameters. . In particular, we checked the effect of corpus size by training Doc2Vec models on the full 13 million NRLS records and on randomly sampled subsets of 1 million and 2 million records. Since our target analysis has heavy medical content and specific use of words, we also tested the importance of the training corpus by generating an additional Doc2Vec model using a set of 5 million articles from the English Wikipedia representing standard,",
        "type": "Document"
      },
      {
        "id": "d7cfe2bf-feec-4faa-bfe2-cad5cf0452e6",
        "metadata": {
          "vector_store_key": "2001.08051-2",
          "chunk_id": 40,
          "document_id": "2001.08051",
          "start_idx": 15428,
          "end_idx": 15968
        },
        "page_content": " assigned either to training or evaluation sets, with proportions of $\\frac{2}{3}$ and $\\frac{1}{3}$, respectively; then training and evaluation lists were built, accordingly. Table reports statistics from the spoken data set. The id All identifies the whole data set, while Clean defines the subset in which sentences containing background voices, incomprehensible speech and word fragments were excluded. From the above description it appears that the corpus can be effectively used in many research directions. The spoken corpus features",
        "type": "Document"
      },
      {
        "id": "9270562a-8e53-4e1d-b760-74d8fd0bc872",
        "metadata": {
          "vector_store_key": "1905.06566-7",
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "f078d463-9801-4046-b7d0-43f488567e4a",
        "metadata": {
          "vector_store_key": "1909.02560-1",
          "chunk_id": 49,
          "document_id": "1909.02560",
          "start_idx": 18101,
          "end_idx": 18537
        },
        "page_content": " questions are paraphrases. We use the same dataset partition as BIBREF5, with 384,348/10,000/10,000 pairs in the training/development/test set respectively. MRPC BIBREF34: The Microsoft Research Paraphrase Corpus consists of sentence pairs collected from online news. Each pair is annotated with a label indicating whether the two sentences are semantically equivalent. There are 4,076/1,725 pairs in the training/test set respectively",
        "type": "Document"
      },
      {
        "id": "60979308-a62e-424b-aedd-1cd7aa39a11c",
        "metadata": {
          "vector_store_key": "1901.09755-3",
          "chunk_id": 25,
          "document_id": "1901.09755",
          "start_idx": 9504,
          "end_idx": 9994
        },
        "page_content": "0 5 billion word corpus extracted from Amazon BIBREF24 , POS tag features and manually developed linguistic patterns based on syntactic analysis and SenticNet BIBREF25 a concept-level knowledge based build for Sentiment Analysis applications. They only evaluate their system on the English 2014 ABSA data, obtaining best results up to date on that benchmark. More recently, BIBREF26 proposed a coupled multi-layer attention (CMLA) network where each layer consists of a couple of attentions",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many shared layers are in the system?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "e27dcfbf-d433-4e8f-b7d5-03fceb21b0ef",
        "metadata": {
          "vector_store_key": "1902.10525-9",
          "chunk_id": 61,
          "document_id": "1902.10525",
          "start_idx": 23016,
          "end_idx": 23477
        },
        "page_content": " 5 layers outperforms more shallow networks, and using more layers gives hardly any improvement. Furthermore, using 64 nodes per layer is sufficient, as wider networks give only small improvements, if at all. Finally, we show a comparison of our old and new systems with the literature on the IAM-OnDB dataset in Table TABREF49 . Our method establishes a new state of the art result when relying on closed data using IAM-OnDB, as well as when relying on our in-",
        "type": "Document"
      },
      {
        "id": "4f7009fe-338f-46f2-b7a6-b9a6f5dc6949",
        "metadata": {
          "vector_store_key": "1910.02789-9",
          "chunk_id": 71,
          "document_id": "1910.02789",
          "start_idx": 28472,
          "end_idx": 28930
        },
        "page_content": " has 8 6X6 filters with stride 3 and ReLU activation. The second convolutional layer has 16 3X3 filters with stride 2 and ReLU activation. The fully connected layers has 32 and 16 units, both of them are followed by ReLU activation. The output layer's size is the amount of actions the agent has available in the trained scenario. Used in the feature vector based agent. Naturally, some discretization is needed in order to build a feature vector, so some of",
        "type": "Document"
      },
      {
        "id": "da303aa9-bcc4-44d9-8d8e-5d1e658d2492",
        "metadata": {
          "vector_store_key": "1701.06538-5",
          "chunk_id": 41,
          "document_id": "1701.06538",
          "start_idx": 15826,
          "end_idx": 16312
        },
        "page_content": " of these models vary from 2 million to 151 million. Quality increases greatly with parameter count, as do computational costs. Results for these models form the top line of Figure FIGREF32 -right. Our models consist of two stacked LSTM layers with a MoE layer between them (see Figure FIGREF8 ). We vary the sizes of the layers and the number of experts. For full details on model architecture, training regimen, additional baselines and results, see Appendix SECREF65 . To investigate",
        "type": "Document"
      },
      {
        "id": "4f9f141c-5b05-4c09-86a6-f5813f86b80d",
        "metadata": {
          "vector_store_key": "1908.11546-3",
          "chunk_id": 28,
          "document_id": "1908.11546",
          "start_idx": 8988,
          "end_idx": 9434
        },
        "page_content": " cell which contains separate continue, act and slots unit that are sequentially connected. The classification architecture has two fully connected layers of size 128, and the remaining models have a hidden size of 64 and a teacher-forcing rate of 0.5. Seq2Seq and Copy Seq2Seq use a beam search with beam size 10 during inference. CAS and gCAS do not adopt a beam search since their inference steps are much less than Seq2Seq methods. All models",
        "type": "Document"
      },
      {
        "id": "16c78106-ea81-49ae-a500-b39a10f6061a",
        "metadata": {
          "vector_store_key": "2001.11268-1",
          "chunk_id": 34,
          "document_id": "2001.11268",
          "start_idx": 13733,
          "end_idx": 14201
        },
        "page_content": " vectors can be obtained for each of the network's layers, and multiple layers can be represented together by concatenation and pooling. We used the t-distributed Stochastic Neighbour Embedding (t-SNE) algorithm to reduce each layer-embedding into two-dimensional space, and plotted the resulting values. Additionally, we computed adjusted rand scores in order to evaluate how well each layer (or concatenation thereof, always using reduce_mean pooling) represents our",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many additional task-specific layers are introduced?",
    "llm_answer": "Two.\n",
    "context": [
      {
        "id": "82e4f80e-8f37-4c7e-a898-09038b4e4619",
        "metadata": {
          "vector_store_key": "2002.06424-9",
          "chunk_id": 10,
          "document_id": "2002.06424",
          "start_idx": 3858,
          "end_idx": 4313
        },
        "page_content": " RE tasks is not constant across all textual domains, we take the number of shared and task-specific layers to be an explicit hyperparameter of the model that can be tuned separately for different datasets. We evaluate the proposed architecture on two publicly available datasets: the Adverse Drug Events (ADE) dataset BIBREF6 and the CoNLL04 dataset BIBREF7. We show that our architecture is able to outperform the current state-of-the-art (SOTA) results",
        "type": "Document"
      },
      {
        "id": "79710ed3-7320-4d0d-83d7-7b29a5e08dd5",
        "metadata": {
          "vector_store_key": "2002.06424-9",
          "chunk_id": 31,
          "document_id": "2002.06424",
          "start_idx": 11350,
          "end_idx": 11786
        },
        "page_content": " tasks. We take the number of shared and task-specific layers to be a hyperparameter of the model. Both sets of task-specific BiRNN layers are followed by task-specific scoring and output layers. Figure FIGREF4 illustrates this architecture. Below, we use superscript $e$ for NER-specific variables and layers and superscript $r$ for RE-specific variables and layers. We obtain contextual token embeddings using the pre-trained ELMo 5.5",
        "type": "Document"
      },
      {
        "id": "f0f76fee-448e-4cd1-a46a-c69633845f8d",
        "metadata": {
          "vector_store_key": "2002.06424-9",
          "chunk_id": 76,
          "document_id": "2002.06424",
          "start_idx": 25311,
          "end_idx": 25794
        },
        "page_content": " work may explore ways to select this aspect of the model architecture in a more principled way. For example, Vandenhende et al. vandenhende2019branched propose using a measure of affinity between tasks to determine how many layers to share in MTL networks. Task affinity scores of NER and RE could be computed for different textual domains or datasets, which could then guide the decision regarding the number of shared and task-specific layers to employ for joint NER and RE models",
        "type": "Document"
      },
      {
        "id": "e34c5ed6-8485-4f28-882c-b450c9423526",
        "metadata": {
          "vector_store_key": "2002.06424-9",
          "chunk_id": 75,
          "document_id": "2002.06424",
          "start_idx": 25019,
          "end_idx": 25527
        },
        "page_content": " across domains with little domain-specific hyperparameter tuning. On the other hand, the similarity between the NER and RE tasks varies across domains, and improved performance can be achieved on these tasks by tuning the number of shared and task-specific parameters. In our work, we treated the number of shared and task-specific layers as a hyperparameter to be tuned for each dataset, but future work may explore ways to select this aspect of the model architecture in a more principled way. For example",
        "type": "Document"
      },
      {
        "id": "306be5a0-97bd-435a-a7aa-6e8d7420f408",
        "metadata": {
          "vector_store_key": "2002.06424-9",
          "chunk_id": 28,
          "document_id": "2002.06424",
          "start_idx": 10121,
          "end_idx": 10621
        },
        "page_content": " to joint NER and RE have experimented little with deep task-specificity, with the exception of those models that include additional layers for the RE task. To our knowledge, no work has considered including additional NER-specific layers beyond scoring and/or output layers. This may reflect a residual influence of the pipeline approach in which the NER task must be solved first before additional layers are used to solve the RE task. However, there is no a priori reason to think that the RE task",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many layers of self-attention does the model have?",
    "llm_answer": "10\n",
    "context": [
      {
        "id": "6d370316-f3af-42f3-a72a-523cd5fa7c9e",
        "metadata": {
          "vector_store_key": "1907.02636-4",
          "chunk_id": 42,
          "document_id": "1907.02636",
          "start_idx": 15243,
          "end_idx": 15664
        },
        "page_content": " dimensions of character embedding, hidden states of character-based token embedding LSTM, hidden states of Bi-LSTM, and sentence attention are set to 25, 25, 100 and 100, respectively. For multi-head self-attention module, we employ a stack of 6 multi-head self attention layer, each of which has 4 head and dimension of each head is set to 64. (b) All of the ANN\u2019s parameters are initialized with a uniform distribution",
        "type": "Document"
      },
      {
        "id": "bbed2dfc-debf-4dab-8113-ad250371c7e9",
        "metadata": {
          "vector_store_key": "1901.02262-9",
          "chunk_id": 39,
          "document_id": "1901.02262",
          "start_idx": 10964,
          "end_idx": 11316
        },
        "page_content": " . As in BIBREF12 , the self-attention sub-layer uses a sub-sequent mask to prevent positions from attending to subsequent positions. The second and third sub-layers perform the multi-head attention over $M^q$ and $M^{p_\\mathrm {all}}$ , respectively. The $M^{p_\\mathrm {all}}$ is the concatenated outputs of the encoder stack for the passages,  $$M^{p",
        "type": "Document"
      },
      {
        "id": "bfeae193-d31f-4ac2-bd3e-8b73b152aa2b",
        "metadata": {
          "vector_store_key": "1910.11204-4",
          "chunk_id": 35,
          "document_id": "1910.11204",
          "start_idx": 12494,
          "end_idx": 12912
        },
        "page_content": " argument and predicate in one sentence. Compared to the standard attention, in this paper, we add the dependency information into $Q$ and $V$ in each attention head, like equation (DISPLAY_FORM15) shows: where $E_D$ and $E_R$ mean the syntactic dependency head and relation information respectively. For our multi-layer multi-head self-attention model, we make this change to each head of the first $N$ self-attention",
        "type": "Document"
      },
      {
        "id": "cbeb1882-fc69-41c5-aba8-158b58f3e274",
        "metadata": {
          "vector_store_key": "1901.02262-9",
          "chunk_id": 16,
          "document_id": "1901.02262",
          "start_idx": 5146,
          "end_idx": 5553
        },
        "page_content": " It consists of two sub-layers: a self-attention layer and a position-wise feed-forward network. For the self-attention layer, we adopt the multi-head attention mechanism defined in BIBREF12 . The feed-forward network consists of two linear transformations with a GELU BIBREF18 activation in between, following OpenAI GPT BIBREF19 . Each sub-layer is placed inside a residual block BIBREF20 . For an input $",
        "type": "Document"
      },
      {
        "id": "d49cd6b5-1c32-47b2-b484-8e191e7375ea",
        "metadata": {
          "vector_store_key": "1910.11204-4",
          "chunk_id": 42,
          "document_id": "1910.11204",
          "start_idx": 14972,
          "end_idx": 15357
        },
        "page_content": "}=d_f+d_p$. The hidden dimension $d_{ff}=800$ is applied for all the experiments. We set the number of shared self-attention blocks $N=10$. The number of heads varies with $d_{model}$, but dimension of each head is 25. Besides, LISA incorporates syntax knowledge in the 5-th self-attention layer while RelAwe incorporates in the first 5 layers. We apply the similar dropout strategy as",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what are the state of the art methods?",
    "llm_answer": "DNN methods, MLP, Random Forest, Linear Support Vector, MEMM, CRF, SVM.\n",
    "context": [
      {
        "id": "b3eb1308-3e01-4903-be04-528f4258d508",
        "metadata": {
          "vector_store_key": "2002.11268-7",
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "30fae7be-f340-4047-8bcf-82ec20a73ca2",
        "metadata": {
          "vector_store_key": "1911.07228-7",
          "chunk_id": 10,
          "document_id": "1911.07228",
          "start_idx": 2445,
          "end_idx": 2848
        },
        "page_content": " used traditional machine learning methods such as Maximum Entropy Markov Model (MEMM), Support Vector Machine (SVM) and Conditional Random Field (CRF). In particular, most of the toolkits for NER task attempted to use MEMM BIBREF4, and CRF BIBREF5 to solve this problem. Nowadays, because of the increase in data, DNN methods are used a lot. They have archived great results when it comes to NER tasks,",
        "type": "Document"
      },
      {
        "id": "fe8937cb-aec4-46d8-96e0-c3f2d45369f2",
        "metadata": {
          "vector_store_key": "1910.06748-4",
          "chunk_id": 61,
          "document_id": "1910.06748",
          "start_idx": 21361,
          "end_idx": 21867
        },
        "page_content": "10. We obtained publicly-available implementations of each of these algorithms, and test them all against our three datasets. In Table TABREF33, we report each algorithm's accuracy and F1 score, the two metrics usually reported in the LID literature. We also included precision and recall values, which are necessary for computing F1 score. And finally we included the speed in number of messages handled per second. This metric is not often discussed in the LID literature, but is of particular importance",
        "type": "Document"
      },
      {
        "id": "577fb06e-9340-4422-93f3-91bc89574aee",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 43,
          "document_id": "2001.00137",
          "start_idx": 16781,
          "end_idx": 17197
        },
        "page_content": " the online platforms. Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector",
        "type": "Document"
      },
      {
        "id": "b943ee8b-1845-4ab0-8b1e-9e73667c2288",
        "metadata": {
          "vector_store_key": "1804.08186-9",
          "chunk_id": 446,
          "document_id": "1804.08186",
          "start_idx": 166790,
          "end_idx": 167290
        },
        "page_content": " on comparing new documents with models of each target language learned from data. The types of models as well as the sources of training data used in the literature are diverse, and work to date has not compared and evaluated these in a systematic manner, making it difficult to draw broader conclusions about what the \u201cbest\u201d method for actually is. We have attempted to synthesize results to date to identify a set of \u201cbest practices\u201d, but these should be treated as guidelines and should always be",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which four languages do they experiment with?",
    "llm_answer": "Spanish, English, Italian, and Russian\n",
    "context": [
      {
        "id": "e90f75a5-f326-4202-82c5-878736af8308",
        "metadata": {
          "vector_store_key": "1910.04269-5",
          "chunk_id": 51,
          "document_id": "1910.04269",
          "start_idx": 19553,
          "end_idx": 20023
        },
        "page_content": ", the languages are divided into three phyla which are Romance, Germanic and Slavic. Of the 6 languages that we selected Spanish (Es), French (Fr) and Italian (It) belong to the Romance phyla, English and German belong to Germanic phyla and Russian in Slavic phyla. Our model also confuses between languages belonging to the similar phyla which acts as an insanity check since languages in same phyla have many similar pronounced words such as cat in English becomes Kat",
        "type": "Document"
      },
      {
        "id": "7fed6b92-48c7-48e6-8ad9-2412ea5449bc",
        "metadata": {
          "vector_store_key": "1612.05270-0",
          "chunk_id": 4,
          "document_id": "1612.05270",
          "start_idx": 1650,
          "end_idx": 2169
        },
        "page_content": " tested in eight different languages: Spanish, English, Italian, Arabic, German, Portuguese, Russian and Swedish. We compare our approach ranking in three international contests: TASS'15, SemEval'15-16 and SENTIPOLC'14, for Spanish, English and Italian respectively; the remaining languages are compared directly with the results reported in the literature. The experimental results locate our approach in good positions for all considered competitions; and excellent results in the other five languages tested. Finally",
        "type": "Document"
      },
      {
        "id": "17f3676f-8f5c-4da9-b0c7-cede94c8171d",
        "metadata": {
          "vector_store_key": "1904.05862-2",
          "chunk_id": 42,
          "document_id": "1904.05862",
          "start_idx": 15343,
          "end_idx": 15454
        },
        "page_content": "letter++ experiments, and Tatiana Likhomanenko for providing convolutional language models for our experiments.",
        "type": "Document"
      },
      {
        "id": "d16841ee-127f-473f-af88-1d373637d8c0",
        "metadata": {
          "vector_store_key": "1910.05456-8",
          "chunk_id": 5,
          "document_id": "1910.05456",
          "start_idx": 1974,
          "end_idx": 2423
        },
        "page_content": " on the L1, i.e., the \"native language\", in neural network models. To this goal, we select a diverse set of eight source languages from different language families \u2013 Basque, French, German, Hungarian, Italian, Navajo, Turkish, and Quechua \u2013 and three target languages \u2013 English, Spanish and Zulu. We pretrain a neural sequence-to-sequence architecture on each of the source languages and then fine-tune the resulting models on small datasets in each",
        "type": "Document"
      },
      {
        "id": "7b1c09b6-d8db-4d35-8a92-0acba47cc1bc",
        "metadata": {
          "vector_store_key": "1909.13362-4",
          "chunk_id": 44,
          "document_id": "1909.13362",
          "start_idx": 15624,
          "end_idx": 16095
        },
        "page_content": " language groupings within families. We selected six evaluation languages: English, Dutch, Italian, French, Basque, and Manipuri. These represent two language families (Indo-European, Sino-Tibetan), a language isolate thought to be unrelated to any existing language (Basque), and two different subfamilies within the Indo-European family (West Germanic, Romance). The primary constraint was the availability of syllabified datasets for training and testing. Table TABREF",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Does DCA or GMM-based attention perform better in experiments?",
    "llm_answer": "DCA performs better.\n",
    "context": [
      {
        "id": "715a8c36-3a98-49f5-b83c-4a52e6e7c343",
        "metadata": {
          "vector_store_key": "1910.10288-8",
          "chunk_id": 49,
          "document_id": "1910.10288",
          "start_idx": 17211,
          "end_idx": 17754
        },
        "page_content": " slower alignment can sometimes lead to worse quality models, probably because the other model components are being optimized in an unaligned state for longer. Compared to GMMv2b, DCA can more easily bound its receptive field (because its prior filter numerically disallows excessive forward movement), which makes it easier to incorporate hard windowing optimizations in production. Another advantage of DCA over GMM attention is that its attention weights are normalized, which helps to stabilize the alignment, especially for coarse-grained",
        "type": "Document"
      },
      {
        "id": "2a042588-be0c-4291-94e3-c2bf45292f7f",
        "metadata": {
          "vector_store_key": "1803.09230-2",
          "chunk_id": 39,
          "document_id": "1803.09230",
          "start_idx": 13315,
          "end_idx": 13814
        },
        "page_content": "izations are also shown in Figure 3 . Visualizations demonstrate that both hybrid and DCA models perform better than vanilla Co-Attention and BiDAF attention mechanisms and reduce the losses faster and increase the dev F1/EM scores faster as well. We made a brief attempt to do a bit of hyperparameter tuning on our proposed DCA model and we report the results in Table 3 . Ideally, hyperparameter tuning for neural network architectures should be done using bayesian hyperparameter optimization but",
        "type": "Document"
      },
      {
        "id": "422bda90-3db3-48ab-883a-5a55cdce9718",
        "metadata": {
          "vector_store_key": "1803.09230-2",
          "chunk_id": 41,
          "document_id": "1803.09230",
          "start_idx": 14099,
          "end_idx": 14601
        },
        "page_content": " probably because it reduces the system complexity which makes the model easier to train. In Table 4 , we briefly provide error analysis on a small sample of results for hybrid and DCA models and try to explain the model behavior. In this paper, we studied and implemented two well known attention mechanisms namely, BiDAF and Co-Attention. We also introduced a simple combination of these two schemes called Hybrid attention mechanism that outperforms both BiDAF and Co-Attention. In addition to this,",
        "type": "Document"
      },
      {
        "id": "3e56d5cd-0c67-4b70-9df2-ad8bc3985c4e",
        "metadata": {
          "vector_store_key": "1910.10288-8",
          "chunk_id": 22,
          "document_id": "1910.10288",
          "start_idx": 7614,
          "end_idx": 8092
        },
        "page_content": " normalization isn't a significant problem in fine-grained output-to-text alignment, but becomes more of an issue for coarser-grained alignment tasks where the attention window needs to gradually move to the next index (for example in variable-length prosody transfer applications BIBREF12). Because DCA is in the energy-based attention family, it is normalized by default and should work well for a variety of monotonic alignment tasks. Another issue with GMM attention is that",
        "type": "Document"
      },
      {
        "id": "004c921a-f826-4e71-86f7-94daf0b001f8",
        "metadata": {
          "vector_store_key": "1803.09230-2",
          "chunk_id": 37,
          "document_id": "1803.09230",
          "start_idx": 12560,
          "end_idx": 13014
        },
        "page_content": " of the model performances for baseline, BiDAF, Co-Attention, Hybrid and DCA attention mechanisms in Table 1 . Notice that none of these architectures were optimized for EM/F1 scores but we are more interested in difference between these mechanisms for a fixed set of hyperparameters. Hybrid and DCA have a slight edge over plain BiDAF and Co-Attention module as per the results. Co-Attention with char embedding was giving us worse results so we put the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What evaluation metric is used?",
    "llm_answer": "Manual evaluation, automatic evaluation metrics (ROUGE, BLEU, METEOR), and task-based evaluation.\n",
    "context": [
      {
        "id": "e2194e32-f011-4118-b3bf-35fb153b3017",
        "metadata": {
          "vector_store_key": "1804.08186-0",
          "chunk_id": 275,
          "document_id": "1804.08186",
          "start_idx": 98870,
          "end_idx": 99381
        },
        "page_content": " evaluation in research to date. In this section, we examine the literature more closely, providing a broad overview of the evaluation metrics that have been used, as well as the experimental settings in which research has been evaluated. The most common approach is to treat the task as a document-level classification problem. Given a set of evaluation documents, each having a known correct label from a closed set of labels (often referred to as the \u201cgold-standard\u201d), and a predicted label for each document",
        "type": "Document"
      },
      {
        "id": "34da022a-5bec-4beb-bd76-14370cf952e1",
        "metadata": {
          "vector_store_key": "2002.05058-1",
          "chunk_id": 69,
          "document_id": "2002.05058",
          "start_idx": 27517,
          "end_idx": 28041
        },
        "page_content": " evaluation is then employed to identify the best choice. We evaluate the performance of each metric by how many times (out of 10) they succeeded in selecting the best hyperparameter combination or early-stopping checkpoint (out of 4) and the average human-annotated score for their selected models. The results are shown in Table 3. We can see that conventional automated metrics perform poorly and result in sub-optimal result when performing hyperparameter search and selecting the best performing checkpoints. Converting",
        "type": "Document"
      },
      {
        "id": "cad945f1-4338-4c8c-988a-66e53227489d",
        "metadata": {
          "vector_store_key": "1607.06025-6",
          "chunk_id": 26,
          "document_id": "1607.06025",
          "start_idx": 10088,
          "end_idx": 10607
        },
        "page_content": "REF37 , the evaluation metrics of text-generating models fall into three categories: manual evaluation, automatic evaluation metrics, task-based evaluation. In evaluation based on human judgment each generated textual example is inspected manually. The automatic evaluation metrics, like ROUGE, BLEU and METEOR, compare human texts and generated texts. BIBREF38 shows METEOR has the strongest correlation with human judgments in image description evaluation. The last category is task-based evaluation, where the impact",
        "type": "Document"
      },
      {
        "id": "62357a4c-d387-42b6-9ded-cfec39e88f26",
        "metadata": {
          "vector_store_key": "2002.05058-1",
          "chunk_id": 68,
          "document_id": "2002.05058",
          "start_idx": 26990,
          "end_idx": 27517
        },
        "page_content": " NLG evaluation. We further investigate the impact of imperfect metrics on training NLG models. As described in the human evaluation procedure, we perform 10 runs to test the reliability of each metric when used to perform hyperparameter tuning and early-stopping respectively. In each run, we select the best hyperparameter combination or early-stopping checkpoint based on each of the five compared metrics. Human evaluation is then employed to identify the best choice. We evaluate the performance of each metric by how many",
        "type": "Document"
      },
      {
        "id": "44492013-ea6a-4d0d-9984-8b621a88cebb",
        "metadata": {
          "vector_store_key": "1906.03538-4",
          "chunk_id": 45,
          "document_id": "1906.03538",
          "start_idx": 17710,
          "end_idx": 18208
        },
        "page_content": "6 and INLINEFORM7 , respectively. The metrics are averaged across all the perspectives INLINEFORM8 participating in the test set. The goal is to get estimates of the overall performance of the systems. Instead of creating a complex measure that would take all the aspects into account, we approximate the overall performance by multiplying the disjoint measures in INLINEFORM0 , INLINEFORM1 and INLINEFORM2 . While this gives an estimate on the overall quality, it ignores the pipeline structure of",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is any data-to-text generation model trained on this new corpus, what are the results?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "41218807-8815-498d-8cb0-80a78baf924d",
        "metadata": {
          "vector_store_key": "2002.10361-4",
          "chunk_id": 37,
          "document_id": "2002.10361",
          "start_idx": 15263,
          "end_idx": 15732
        },
        "page_content": " (URL, USER, HASHTAG). Documents are lowercased and tokenized using NLTK BIBREF38. The corpus is randomly split into training (70%), development (15%), and test (15%) sets. We train the models on the training set and find the optimal hyperparameters on the development set before final evaluations on the test set. We randomly shuffle the training data at the beginning of each training epoch. We implement and experiment four baseline classification models. To compare",
        "type": "Document"
      },
      {
        "id": "cd2aede0-1494-4286-af5e-7e8b64611764",
        "metadata": {
          "vector_store_key": "1905.10044-2",
          "chunk_id": 62,
          "document_id": "1905.10044",
          "start_idx": 23339,
          "end_idx": 23791
        },
        "page_content": " a language model on the Books corpus BIBREF36 . Our BERTL model fine-tunes the 24 layer 1024 dimensional transformer from BIBREF8 , which has been trained on next-sentence-selection and masked language modelling on the Book Corpus and Wikipedia. We fine-tune the BERTL and the OpenAI GPT models using the optimizers recommended by the authors, but found it important to tune the optimization parameters to achieve the best results. We use a batch size",
        "type": "Document"
      },
      {
        "id": "d7e88a3a-9f2c-444d-a064-4fc4e2448cb3",
        "metadata": {
          "vector_store_key": "1701.02877-7",
          "chunk_id": 121,
          "document_id": "1701.02877",
          "start_idx": 43692,
          "end_idx": 44145
        },
        "page_content": " the CoNLL 2003 corpus. The seed domain for the experiments here is newswire, where we use the classifier trained on the biggest NW corpus investigated in this study, i.e. OntoNotes NW. That classifier is then applied to all other corpora. The rationale is to test how suitable such a big corpus would be for improving Twitter NER, for which only small training corpora are available. Results for out-of-domain performance are reported in Table 11 . The",
        "type": "Document"
      },
      {
        "id": "06e9a4b3-9db0-4e73-892a-9b9a4c094d48",
        "metadata": {
          "vector_store_key": "1909.00694-1",
          "chunk_id": 27,
          "document_id": "1909.00694",
          "start_idx": 8898,
          "end_idx": 9330
        },
        "page_content": " CO such that it was five times larger than AL. The results are shown in Table TABREF16. We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively: . \u4f5c\u696d\u304c\u697d\u3060\u3002 The work",
        "type": "Document"
      },
      {
        "id": "58f6c10b-afb8-4fdb-aff4-91a8f4ae8848",
        "metadata": {
          "vector_store_key": "1809.10644-0",
          "chunk_id": 23,
          "document_id": "1809.10644",
          "start_idx": 8845,
          "end_idx": 9310
        },
        "page_content": " classification and a hybrid of word and character CNNs, using approximately twice the parameters we use excluding the word embeddings) and even recent work by BIBREF8 , (whose best model relies on GRUs, metadata including popularity, network reciprocity, and subscribed lists). On the SR dataset, we outperform BIBREF8 's text based model by 3 F1 points, while just falling short of the Text + Metadata Interleaved Training model. While we appreciate the potential",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are the potentially relevant text fragments identified?",
    "llm_answer": "Entities are extracted, contexts are generated for each, and a distance between question content and entity context is computed to assess relevance.\n",
    "context": [
      {
        "id": "f434de0c-800a-42f0-b3e3-f358bed443ef",
        "metadata": {
          "vector_store_key": "1605.08675-3",
          "chunk_id": 39,
          "document_id": "1605.08675",
          "start_idx": 15409,
          "end_idx": 15878
        },
        "page_content": " extracted (ENTITY RECOGNITION). For each of them, a context is generated (CONTEXT GENERATION). Finally, a distance between a question content and the entity context is computed to asses its relevance (DISTANCE MEASURE). All the mentions and their distance scores are stored and, after no more documents are left, used to select the best match (BEST ENTITY SELECTION). The system returns the entity, supplied with information about a supporting sentence and a document,",
        "type": "Document"
      },
      {
        "id": "8a6eee43-a0ea-4c27-9ed8-1016d429f821",
        "metadata": {
          "vector_store_key": "1706.04815-1",
          "chunk_id": 7,
          "document_id": "1706.04815",
          "start_idx": 2890,
          "end_idx": 3417
        },
        "page_content": " learning framework. We use the bidirectional recurrent neural networks (RNN) for the word-level representation, and then apply the attention mechanism BIBREF2 to incorporate matching information from question to passage at the word level. Next, we predict start and end positions of the evidence snippet by pointer networks BIBREF3 . Moreover, we aggregate the word-level matching information of each passage using the attention pooling, and use the passage-level representation to rank all candidate passages as an additional",
        "type": "Document"
      },
      {
        "id": "33a3a082-cd68-4c06-88ba-1979cde04b5e",
        "metadata": {
          "vector_store_key": "2001.08051-7",
          "chunk_id": 45,
          "document_id": "2001.08051",
          "start_idx": 17431,
          "end_idx": 17972
        },
        "page_content": " words fragments and spontaneous speech phenomena requires specific investigations to reduce their impact on the final performance. We believe that the particular domain and set of data pave the way to investigate into various ASR topics, such as: non-native speech, children speech, spontaneous speech, code-switching, multiple pronunciation, etc. The corpus has been (partly) annotated using the guidelines presented in Section SECREF3 on the basis of a preliminary analysis of the most common acoustic phenomena appearing in the data sets",
        "type": "Document"
      },
      {
        "id": "bbdd7f23-0ff1-471b-9830-898995d35973",
        "metadata": {
          "vector_store_key": "1907.05664-3",
          "chunk_id": 14,
          "document_id": "1907.05664",
          "start_idx": 5236,
          "end_idx": 5692
        },
        "page_content": " words with more common alternatives or repeating non-sense after the third sentence. We can see in Figure 1 an example of summary obtained compared to the target one. The \u201csummaries\" we generate are far from being valid summaries of the information in the texts but are sufficient to look at the attribution that LRP will give us. They pick up the general subject of the original text. We present in this section the Layer-Wise Relevance Propagation (LRP)",
        "type": "Document"
      },
      {
        "id": "e1ec6ad7-678e-4726-89be-d75773fe02f8",
        "metadata": {
          "vector_store_key": "1804.08186-4",
          "chunk_id": 442,
          "document_id": "1804.08186",
          "start_idx": 165159,
          "end_idx": 165656
        },
        "page_content": " only the text itself as evidence on which to base the prediction of the language. Within a text, there can sometimes be domain-specific peculiarities that can be used for . For example, BIBREF399 investigates of user-to-user messages in the eBay e-commerce portal. He finds that using only the first two and last two words of a message is sufficient for identifying the language of a message. This article has presented a comprehensive survey on language identification of digitally-encoded text.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What dataset did they use?",
    "llm_answer": "A dataset of 3 billion (query, image, weight) triples, with 900 million unique images and 220 million unique queries.  A second dataset (SCI) of 220 sentences from the scientific literature.\n",
    "context": [
      {
        "id": "47700f54-52ba-40b9-bf2e-4fde786f8ac4",
        "metadata": {
          "vector_store_key": "1912.09152-4",
          "chunk_id": 75,
          "document_id": "1912.09152",
          "start_idx": 26669,
          "end_idx": 26983
        },
        "page_content": " a true Gold Standard Dataset. We thank three anonymous reviewers of our manuscript for their careful reading and their many insightful comments and suggestions. We have made our best in providing a revised version of the manuscript that reflects their suggestions. Any remaining errors are our own responsability.",
        "type": "Document"
      },
      {
        "id": "0d593f74-7474-41cc-9928-ce68ac96f714",
        "metadata": {
          "vector_store_key": "1802.05574-2",
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "7e589466-1803-4a33-94aa-80d0575f23b4",
        "metadata": {
          "vector_store_key": "1905.12260-1",
          "chunk_id": 21,
          "document_id": "1905.12260",
          "start_idx": 8180,
          "end_idx": 8639
        },
        "page_content": " results. For example, one (query, image) pair might be \u201ccat with big ears\u201d and an image of a cat. Each (query, image) pair also has a weight corresponding to a relevance score of the image for the query. The dataset includes 3 billion (query, image, weight) triples, with 900 million unique images and 220 million unique queries. The data was prepared by first taking the query-image set, filtering to remove any personally identifiable information and adult",
        "type": "Document"
      },
      {
        "id": "401df3ea-241b-476d-97f4-1276dc131769",
        "metadata": {
          "vector_store_key": "1701.00185-9",
          "chunk_id": 99,
          "document_id": "1701.00185",
          "start_idx": 35418,
          "end_idx": 35678
        },
        "page_content": "ASQ for making the datasets available. This work is supported by the National Natural Science Foundation of China (No. 61602479, No. 61303172, No. 61403385) and the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDB02070005).",
        "type": "Document"
      },
      {
        "id": "9a66176f-fe2e-4da0-a2df-70a3d739924f",
        "metadata": {
          "vector_store_key": "1802.05322-2",
          "chunk_id": 38,
          "document_id": "1802.05322",
          "start_idx": 13905,
          "end_idx": 14341
        },
        "page_content": " explanation of the whole process needed to conduct an experiment like this. The process includes downloading a data set, web scraping for extra information, data preprocessing, model tuning and evaluation of the results. Action Adult Adventure Animation Biography Comedy Crime Documentary Drama Family Fantasy Film-Noir Game-Show History Horror Music Musical Mystery Reality-TV Romance Sci-Fi Short Sport Talk-Show Thriller War Western",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the size of the dataset?",
    "llm_answer": "The context provides various dataset sizes, but no single answer to \"the size of the dataset\".\n",
    "context": [
      {
        "id": "9270562a-8e53-4e1d-b760-74d8fd0bc872",
        "metadata": {
          "vector_store_key": "1905.06566-7",
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "0723454a-4062-463e-972a-81a58c6a504e",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "0d593f74-7474-41cc-9928-ce68ac96f714",
        "metadata": {
          "vector_store_key": "1802.05574-2",
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "84720de1-c988-45c6-bcbe-7a0435cdf6fb",
        "metadata": {
          "vector_store_key": "1706.08032-0",
          "chunk_id": 38,
          "document_id": "1706.08032",
          "start_idx": 14264,
          "end_idx": 14661
        },
        "page_content": " datasets in Table III. for all datasets, the filter window size ( INLINEFORM0 ) is 7 with 6 feature maps each for the first wide convolution layer, the second wide convolution layer has a filter window size of 5 with 14 feature maps each. Dropout rate ( INLINEFORM1 ) is 0.5, INLINEFORM2 constraint, learning rate is 0.1 and momentum of 0.9. Mini-batch size for STS Corpus is 100 and others are 4",
        "type": "Document"
      },
      {
        "id": "2e1622b2-1517-4239-8581-693f3aece329",
        "metadata": {
          "vector_store_key": "1711.02013-3",
          "chunk_id": 55,
          "document_id": "1711.02013",
          "start_idx": 20658,
          "end_idx": 21116
        },
        "page_content": "ings, between recurrent layers, and on recurrent states were (0.7, 0.5, 0.5) respectively. dataset contains 17M training tokens and has a vocabulary size of 44k words. The dataset is partitioned into a training set (first 99M characters) and a development set (last 1M characters) that is used to report performance. As this dataset contains various articles from Wikipedia, the longer term information (such as current topic) plays a bigger role than in the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the 12 AV approaches which are examined?",
    "llm_answer": "The provided text states that 12 existing AV methods were applied, but does not list them.\n",
    "context": [
      {
        "id": "4f343854-c04a-4270-a2ca-e78065d7051f",
        "metadata": {
          "vector_store_key": "1906.10551-1",
          "chunk_id": 77,
          "document_id": "1906.10551",
          "start_idx": 29331,
          "end_idx": 29815
        },
        "page_content": " 12 existing AV approaches, which have shown their potentials in the previous PAN-AV competitions BIBREF11 , BIBREF12 as well as in a number of AV studies. The methods are listed in Table TABREF33 together with their classifications regarding the AV characteristics, which we proposed in Section SECREF3 . All (optimizable) AV methods were tuned regarding their hyperparameters, according to the original procedure mentioned in the respective paper. However, in the case of the binary",
        "type": "Document"
      },
      {
        "id": "d5784ccf-49c9-4a50-8b77-accacccb70f9",
        "metadata": {
          "vector_store_key": "1906.10551-4",
          "chunk_id": 116,
          "document_id": "1906.10551",
          "start_idx": 43724,
          "end_idx": 44245
        },
        "page_content": " rated as reliable. Moreover, we clarified a number of misunderstandings in previous research works and proposed three clear criteria that allow to classify the model category of an AV method, which in turn influences its design and the way how it should be evaluated. In regard to binary-extrinsic AV approaches, we explained which challenges exist and how they affect their applicability. In an experimental setup, we applied 12 existing AV methods on three self-compiled corpora, where the intention behind each corpus",
        "type": "Document"
      },
      {
        "id": "103460d9-d0fa-4d5e-be24-ae2578497113",
        "metadata": {
          "vector_store_key": "1906.10551-1",
          "chunk_id": 12,
          "document_id": "1906.10551",
          "start_idx": 4902,
          "end_idx": 5362
        },
        "page_content": " approaches which, according to Gr\u00f6ndahl and Asokan BIBREF9 can be divided into manual, computer-assisted and automatic types, perform a variety of modifications on the texts. These include simple synonym replacements, rule-based substitutions or word order permutations. In 2016, Potthast et al. BIBREF10 presented the first large-scale evaluation of three AO approaches that aim to attack 44 AV methods, which were submitted to the PAN-AV competitions during",
        "type": "Document"
      },
      {
        "id": "8effb31e-542b-4bc0-af4b-bf493806990a",
        "metadata": {
          "vector_store_key": "1906.10551-4",
          "chunk_id": 6,
          "document_id": "1906.10551",
          "start_idx": 2469,
          "end_idx": 3019
        },
        "page_content": " contribute to the further development of this young research field. Based on the proposed properties, we investigate the applicability of 12 existing AV approaches on three self-compiled corpora, where each corpus involves a specific challenge. The rest of this paper is structured as follows. Section SECREF2 discusses the related work that served as an inspiration for our analysis. Section SECREF3 comprises the proposed criteria and properties to characterize AV methods. Section SECREF4 describes the methodology, consisting of the used corpora",
        "type": "Document"
      },
      {
        "id": "a7351c9b-1fbd-42d4-b928-d8a94dfb48bc",
        "metadata": {
          "vector_store_key": "1906.10551-4",
          "chunk_id": 117,
          "document_id": "1906.10551",
          "start_idx": 44245,
          "end_idx": 44729
        },
        "page_content": " applied 12 existing AV methods on three self-compiled corpora, where the intention behind each corpus was to focus on a different aspect of the methods applicability. Our findings regarding the examined approaches can be summarized as follows: Despite of the good performance of the five AV methods GenIM, ImpGI, Unmasking, Caravel and SPATIUM, none of them can be truly considered as reliable and therefore applicable in real forensic cases. The reason for this is not only the non-",
        "type": "Document"
      }
    ]
  },
  {
    "query": "how was annotation done?",
    "llm_answer": "Annotations were done by undergraduate students of computational linguistics, in single annotation mode, and with discussions with authors throughout the process.  A small sample was annotated by all four annotators to measure inter-annotator agreement.  Before annotation, annotators were guided through instructions and a verification step.\n",
    "context": [
      {
        "id": "ae777c6e-5f84-42f7-8843-5ba4994ac482",
        "metadata": {
          "vector_store_key": "1703.05260-8",
          "chunk_id": 49,
          "document_id": "1703.05260",
          "start_idx": 17551,
          "end_idx": 18087
        },
        "page_content": " results were discussed with the authors. Throughout the whole annotation phase, annotators could discuss any emerging issues with the authors. All annotations were done by undergraduate students of computational linguistics. The annotation was rather time-consuming due to the complexity of the task, and thus we decided for single annotation mode. To assess annotation quality, a small sample of texts was annotated by all four annotators and their inter-annotator agreement was measured (see Section \"Inter-Annotator Agreement\" ). It",
        "type": "Document"
      },
      {
        "id": "8a40ea07-4d92-474a-ac70-369ebcdccf4b",
        "metadata": {
          "vector_store_key": "1710.00341-7",
          "chunk_id": 42,
          "document_id": "1710.00341",
          "start_idx": 15082,
          "end_idx": 15541
        },
        "page_content": ".g., \u201cIt is Friday tomorrow.\u201d, \u201cIt was raining last week.\u201d), or for which the annotators were unsure. We targeted very high quality, and thus we did not use crowdsourcing for the annotation, as pilot annotations showed that the task was very difficult and that it was not possible to guarantee that Turkers would do all the necessary verification, e.g., gathering evidence from trusted sources. Instead, all examples were first annotated independently by four",
        "type": "Document"
      },
      {
        "id": "02f35bc9-8c43-4e0e-b4e7-c674de09b7e9",
        "metadata": {
          "vector_store_key": "1906.03538-8",
          "chunk_id": 17,
          "document_id": "1906.03538",
          "start_idx": 6916,
          "end_idx": 7478
        },
        "page_content": " more than 1000 finished HITs and at least a 95% acceptance rate. To ensure the diversity of responses, we do not require additional qualifications or demographic information from our annotators. For any of the annotations steps described below, the users are guided to an external platform where they first read the instructions and try a verification step to make sure they have understood the instructions. Only after successful completion are they allowed to start the annotation tasks. Throughout our annotations, it is our aim to make sure that the workers",
        "type": "Document"
      },
      {
        "id": "fa6b35d4-0fad-4da3-a6e0-9fffce8f3025",
        "metadata": {
          "vector_store_key": "1912.09152-1",
          "chunk_id": 33,
          "document_id": "1912.09152",
          "start_idx": 12001,
          "end_idx": 12474
        },
        "page_content": ", the annotation process is very simple. The program reads the input byte stream trying to identify known entities by means of a huge regexp built through the pre-processing of the available resources. If the candidate entity is ambiguous and (at least) one contextual rule exists for it, it is applied. For the rest of the nes, the system assigns them the class and sctid found in our ranked in-memory lexicon. As already mentioned in passing, the system does not tokenize",
        "type": "Document"
      },
      {
        "id": "a26c8740-0c0d-4a60-97eb-3d6b0beceeba",
        "metadata": {
          "vector_store_key": "1905.10044-9",
          "chunk_id": 25,
          "document_id": "1905.10044",
          "start_idx": 9361,
          "end_idx": 9867
        },
        "page_content": " is made before the annotator sees the Wikipedia page. Next, for good questions, annotators find a passage within the document that contains enough information to answer the question. Annotators can mark questions as \u201cnot answerable\" if the Wikipedia article does not contain the requested information. Finally, annotators mark whether the question's answer is \u201cyes\" or \u201cno\". Annotating data in this manner is quite expensive since annotators need to search entire Wikipedia documents for relevant evidence",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they measure correlation between the prediction and explanation quality?",
    "llm_answer": "By investigating how prediction accuracy is affected by jointly training the network to produce explanations, and by evaluating how well the model learns to generate explanations.  They also performed these experiments for two types of explanations.\n",
    "context": [
      {
        "id": "f84592c5-0ea5-4f08-b8eb-50f668ecf60e",
        "metadata": {
          "vector_store_key": "1708.01776-0",
          "chunk_id": 27,
          "document_id": "1708.01776",
          "start_idx": 10757,
          "end_idx": 11314
        },
        "page_content": " the average of the output vector's values), but found that a fixed threshold of .5 works best. To evaluate the model's ability to jointly learn to predict and explain its predictions we performed two experiments. First, we investigate how the prediction accuracy is affected by jointly training the network to produce explanations. Second, we evaluate how well the model learns to generate explanations. To understand the role of the explanation content in the learning process we perform both of these experiments for each of the two types of explanation:",
        "type": "Document"
      },
      {
        "id": "4f43564b-b5cd-4e8a-af36-cf0b6e58038c",
        "metadata": {
          "vector_store_key": "1708.01776-0",
          "chunk_id": 30,
          "document_id": "1708.01776",
          "start_idx": 12047,
          "end_idx": 12622
        },
        "page_content": "REF53 . Here again the performance on possible-answer explanations is significantly higher than for relevant-variable explanations. Possible-answer explanations reach an F-Score of .9, while relevant-variable explanations one of .09 only, with precision and recall only slightly deviating from the F-Score in all experiments. We would expect that explanation performance should correlate with prediction performance. Since Possible-answer knowledge is primarily needed to decide if the net has enough information to answer the challenge question without guessing and relevant",
        "type": "Document"
      },
      {
        "id": "e357d011-b2c3-48b3-9cbb-f78129fd6627",
        "metadata": {
          "vector_store_key": "2004.03744-3",
          "chunk_id": 55,
          "document_id": "2004.03744",
          "start_idx": 20038,
          "end_idx": 20622
        },
        "page_content": " of explanations and suggested human evaluation instead. We therefore manually scored the relevance of 100 explanations that were generated when the model predicted correct labels. We found that only 20% of explanations were relevant. We highlight that the relevance of explanations is in terms of whether the explanation reflects ground-truth reasons supporting the correct label. This is not to be confused with whether an explanation is correctly illustrating the inner working of the model, which is left as future work. It is also important to note that on a similar experimental",
        "type": "Document"
      },
      {
        "id": "a16913cf-a66b-46d8-872a-b22c7c3dd8ba",
        "metadata": {
          "vector_store_key": "1909.00578-1",
          "chunk_id": 33,
          "document_id": "1909.00578",
          "start_idx": 11017,
          "end_idx": 11604
        },
        "page_content": " stochastic learning algorithms are involved. An in-depth exploration of this point would involve further investigation, which will be part of future work. We propose a novel Quality Estimation model for summarization which does not require human references to estimate the quality of automatically produced summaries. Sum-QE successfully predicts qualitative aspects of summaries that recall-oriented evaluation metrics fail to approximate. Leveraging powerful BERT representations, it achieves high correlations with human scores for most linguistic qualities rated, on three different",
        "type": "Document"
      },
      {
        "id": "f691165b-75f7-4456-b6fe-497ea7084dd1",
        "metadata": {
          "vector_store_key": "2001.02380-9",
          "chunk_id": 16,
          "document_id": "2001.02380",
          "start_idx": 6369,
          "end_idx": 6911
        },
        "page_content": ", we combine the anchoring annotation data from Section SECREF3 with the model's predictions to evaluate how `human-like' its performance is, using an information retrieval approach measuring recall@k and assessing the stability of different signal types based on how the model scores them. We develop a visualization for tokenwise signaling strength and perform error analysis for some signals found by the model which were not flagged by humans and vice versa, and point out the strengths and weaknesses of the architecture. Section SECREF6",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What datasets are used to evaluate the introduced method?",
    "llm_answer": "Swissmetro, Oshiete-goo, and nfL6 datasets.\n",
    "context": [
      {
        "id": "b3eb1308-3e01-4903-be04-528f4258d508",
        "metadata": {
          "vector_store_key": "2002.11268-7",
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "9d9ab555-416a-4bc9-a24d-f0230333ee94",
        "metadata": {
          "vector_store_key": "1910.12618-4",
          "chunk_id": 49,
          "document_id": "1910.12618",
          "start_idx": 19565,
          "end_idx": 20023
        },
        "page_content": " the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014 and 2015 serve as validation set (730 days). It is used to tune the hyperparameters of the different approaches. All the data from January the 1st 2016 (974 days for France and 1,096 for the UK) is used as test set, on which the final results are presented. Grid search is applied to find the best combination of values: for each hyperparameter,",
        "type": "Document"
      },
      {
        "id": "87d9afb2-c546-418c-b5b1-18b5e1abe1e5",
        "metadata": {
          "vector_store_key": "1605.08675-6",
          "chunk_id": 134,
          "document_id": "1605.08675",
          "start_idx": 49919,
          "end_idx": 50491
        },
        "page_content": ", used at this stage, substantially differs from the one used during the development (see section SECREF80 ). A configuration for the final evaluation has been prepared based on results of the experiments. All of the tested versions share the following features: no question analysis corrections, question classification and query generation solutions which proved best in the previous experiments (see section SECREF18 ), a retrieved set of documents including 20 articles, no minimal confidence, singe sentence context with title. Tested solutions differ with respect to",
        "type": "Document"
      },
      {
        "id": "9cdcf3d4-51f3-43d9-9cea-7a3d8cdb29a9",
        "metadata": {
          "vector_store_key": "1909.00154-2",
          "chunk_id": 61,
          "document_id": "1909.00154",
          "start_idx": 23166,
          "end_idx": 23592
        },
        "page_content": "CCM), for example. We will apply the methodology to the well-known \u201cSwissmetro\" dataset. We will compare it with a dummy variables and PCA baselines. We will follow the 3-way experimental design mentioned before: split the dataset into train, development and test sets, so that the embeddings, PCA eigenvectors and the choice model are estimated from the same train and development sets, and validate it out-of-sample. For the",
        "type": "Document"
      },
      {
        "id": "8e2a448d-c8d4-43db-9500-b6ba58f58fe9",
        "metadata": {
          "vector_store_key": "1912.00864-5",
          "chunk_id": 70,
          "document_id": "1912.00864",
          "start_idx": 22055,
          "end_idx": 22516
        },
        "page_content": " three evaluations using the Oshiete-goo dataset; we selected three different sets of 500 human-annotated test pairs from the full dataset. In each set, we trained the model by using training pairs and input questions in test pairs to the model. We repeated the experiments three times by randomly shuffling the train/test sets. For the evaluations using the nfL6 dataset, we prepared three different sets of 500 human-annotated test q-c-s triples from the full",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the results achieved from the introduced method?",
    "llm_answer": "The method achieves significant improvement in ROUGE-L compared to the baseline, and outperforms the baseline on the development set in terms of ROUGE-L.  It also improves recognition performance with an ensemble effect.\n",
    "context": [
      {
        "id": "b3eb1308-3e01-4903-be04-528f4258d508",
        "metadata": {
          "vector_store_key": "2002.11268-7",
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "e47b2d6a-c071-4a61-b692-68b484884e3d",
        "metadata": {
          "vector_store_key": "1908.07816-7",
          "chunk_id": 10,
          "document_id": "1908.07816",
          "start_idx": 4017,
          "end_idx": 4509
        },
        "page_content": " relate. It is the first time such an approach is designed with consideration for the human judges. Our main goal is to increase the objectivity of the results and reduce judges' mistakes due to out-of-context dialogs they have to evaluate. The rest of the paper unfolds as follows. Section SECREF2 discusses some related work. In Section SECREF3, we give detailed description of the methodology. We present experimental results and some analysis in Section SECREF4. The paper is concluded in",
        "type": "Document"
      },
      {
        "id": "05f757b2-10bf-4327-95eb-f8e4960fee4d",
        "metadata": {
          "vector_store_key": "1905.07464-5",
          "chunk_id": 49,
          "document_id": "1905.07464",
          "start_idx": 18517,
          "end_idx": 19008
        },
        "page_content": "REF15 optimization method. These hyper-parameters were tuned during initial experiments. In this section, we present and discuss the results of our cross-validation experiments. We then describe the \u201cruns\u201d that were submitted as challenge entries and present our official challenge results. We discuss these results in Section SECREF28 . We present the results of our initial experiments in Table TABREF20 . Evaluations were produced as as result of 11-fold cross-validation over Training-22",
        "type": "Document"
      },
      {
        "id": "aaa8d642-49fb-49d7-91ec-f0838b55c145",
        "metadata": {
          "vector_store_key": "1706.04815-9",
          "chunk_id": 78,
          "document_id": "1706.04815",
          "start_idx": 21188,
          "end_idx": 21662
        },
        "page_content": " observe that our method only achieves significant improvement in terms of ROUGE-L compared with our baseline. The reason is that our synthesis model works better when the answer is short, which almost has no effect on BLEU as it is normalized with all questions. Since answers on the test set are not published, we analyze our model on the development set. Table 3 shows results on the development set in terms of ROUGE-L. As we can see, our method outperforms the baseline",
        "type": "Document"
      },
      {
        "id": "470d1db4-df8a-47b8-b003-cf518c68a17a",
        "metadata": {
          "vector_store_key": "1804.08050-3",
          "chunk_id": 9,
          "document_id": "1804.08050",
          "start_idx": 3622,
          "end_idx": 4223
        },
        "page_content": " Instead of the integration in the attention level, our proposed method uses multiple decoders for each attention and integrates their outputs to generate a final output. Furthermore, in order to make each head to capture the different modalities, different attention functions are used for each head, leading to the improvement of the recognition performance with an ensemble effect. To evaluate the effectiveness of our proposed method, we conduct an experimental evaluation using Corpus of Spontaneous Japanese. Experimental results demonstrate that our proposed method outperforms the conventional",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do they incorporate human advice?",
    "llm_answer": "By consulting domain experts and using annotator feedback and agreement information.\n",
    "context": [
      {
        "id": "77a227d9-74fa-4fa4-ad21-a2cd5640b529",
        "metadata": {
          "vector_store_key": "1709.05413-4",
          "chunk_id": 101,
          "document_id": "1709.05413",
          "start_idx": 40354,
          "end_idx": 40892
        },
        "page_content": ", with ratios of at least 4:1. By using the feature weights we derive from using our predicted dialogue acts in our outcome classification experiments, we can thus derive data-driven patterns that offer useful insight into good/bad practices. Our goal is to then use these rules as guidelines, serving as a basis for automated response planning in the customer service domain. For example, these rules can be used to recommend certain dialogue act responses given the position in a conversation, and based previous turns. This information",
        "type": "Document"
      },
      {
        "id": "f6b151c0-ffbc-4921-9e63-dae9d26d3233",
        "metadata": {
          "vector_store_key": "1808.03430-3",
          "chunk_id": 3,
          "document_id": "1808.03430",
          "start_idx": 1138,
          "end_idx": 1678
        },
        "page_content": " knowledge without human supervision from conversation records or given product introduction documents and generate proper response, which alleviates the problem of lacking dialogue corpus to train a chatbot. First, by using Apache Lucene to select top 2 sentences most relevant to the question and extracting subject-verb-object (SVO) triples from them, a set of candidate responses is generated. With regard to multi-turn conversations, we adopt a dialogue manager, including self-attention strategy to distill significant signal of utter",
        "type": "Document"
      },
      {
        "id": "a58b1d5e-430b-4761-8c11-8ddea57cba19",
        "metadata": {
          "vector_store_key": "1709.05413-4",
          "chunk_id": 107,
          "document_id": "1709.05413",
          "start_idx": 42990,
          "end_idx": 43246
        },
        "page_content": "-experts and using annotator feedback and agreement information, derive more powerful features for dialogue act prediction, and automate ranking and selection of best-practice rules based on domain requirements for automated customer service system design.",
        "type": "Document"
      },
      {
        "id": "ab5c6d47-2956-402f-b462-a406b6935381",
        "metadata": {
          "vector_store_key": "1709.05413-4",
          "chunk_id": 106,
          "document_id": "1709.05413",
          "start_idx": 42382,
          "end_idx": 42990
        },
        "page_content": " We observe interesting correlations between the dialogue acts agents use and the outcomes, offering insights into good/bad practices that are more useful for creating context-aware automated customer service systems than generating canned response templates. Future directions for this work revolve around the integration of the insights derived in the design of automated customer service systems. To this end, we aim to improve the taxonomy and annotation design by consulting domain-experts and using annotator feedback and agreement information, derive more powerful features for dialogue act prediction",
        "type": "Document"
      },
      {
        "id": "99f3bd20-c3c7-4eff-bd45-6a798ee382c4",
        "metadata": {
          "vector_store_key": "1909.00183-9",
          "chunk_id": 150,
          "document_id": "1909.00183",
          "start_idx": 56239,
          "end_idx": 56836
        },
        "page_content": " itself. Additionally, the method allows for the discovery of emerging topics or classes of incidents directly from the data when such events do not fit existing categories by using methods for anomaly detection to decide whether new topic clusters should be created. This is a direction of future work. Further work also includes the use of our method to enable comparisons across healthcare organisations and also to monitor changes in their incident reports over time. Another interesting direction is to provide online classification suggestions to users based on the text they input as an aid",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What affective-based features are used?",
    "llm_answer": "DAL Activation, ANEW Dominance, Emolex Negative, Emolex Fear, LIWC Assent, LIWC Cause, LIWC Certain, LIWC Affect.\n",
    "context": [
      {
        "id": "ee5be2f5-7a65-4166-bdf5-e38ce495f9d8",
        "metadata": {
          "vector_store_key": "1901.01911-8",
          "chunk_id": 33,
          "document_id": "1901.01911",
          "start_idx": 12612,
          "end_idx": 13054
        },
        "page_content": " of features. It is worth to be noted that in our best configuration system, not all of affective and dialogue-act features were used in our feature vector. After several optimization steps, we found that some features were not improving the system's performance. Our final list of affective and dialogue-act based features includes: DAL Activation, ANEW Dominance, Emolex Negative, Emolex Fear, LIWC Assent, LIWC Cause, LIWC Certain and LIWC",
        "type": "Document"
      },
      {
        "id": "ce4596b1-74e4-4e75-aaa7-c460d5455d37",
        "metadata": {
          "vector_store_key": "1901.01911-8",
          "chunk_id": 7,
          "document_id": "1901.01911",
          "start_idx": 2907,
          "end_idx": 3386
        },
        "page_content": " a relationship exists between the use of an affective lexicon and the communicative intention of an utterance which includes AGREE-ACCEPT (support), REJECT (deny), INFO-REQUEST (question), and OPINION (comment). They exploited several LIWC categories to analyse the role of affective content. Our results show that our model outperforms the state of the art on the Semeval-2017 benchmark dataset. Feature analysis highlights the contribution of the different feature groups, and",
        "type": "Document"
      },
      {
        "id": "0d22bb55-7e35-46b5-97a5-c533cba41b90",
        "metadata": {
          "vector_store_key": "1901.01911-8",
          "chunk_id": 23,
          "document_id": "1901.01911",
          "start_idx": 8949,
          "end_idx": 9418
        },
        "page_content": " BIBREF6 . Basically, these features are part of the affective feature group, but we present them separately because we are interested in exploring the contribution of such feature set separately. This feature set was obtained by selecting 4 communicative goals related to our classes in the stance task: agree-accept (support), reject (deny), info-request (question), and opinion (comment). The 11 LIWC categories include: Agree-accept: Assent, Certain, Affect; Reject",
        "type": "Document"
      },
      {
        "id": "734149c2-6231-4d14-a90f-daf63bb86506",
        "metadata": {
          "vector_store_key": "1901.01911-8",
          "chunk_id": 19,
          "document_id": "1901.01911",
          "start_idx": 7621,
          "end_idx": 8062
        },
        "page_content": " to true and false rumors BIBREF5 , and by the work in BIBREF6 reflecting on the role of affect in dialogue acts BIBREF6 . Multi-faceted affective features have been already proven to be effective in some related tasks BIBREF9 , including the stance detection task proposed at SemEval-2016 (Task 6). We used the following affective resources relying on different emotion models. Emolex: it contains 14,182 words associated with eight primary",
        "type": "Document"
      },
      {
        "id": "3717bc50-f2c5-47b4-bf11-345228e89964",
        "metadata": {
          "vector_store_key": "2003.07996-1",
          "chunk_id": 6,
          "document_id": "2003.07996",
          "start_idx": 2554,
          "end_idx": 3028
        },
        "page_content": " exact results. In BIBREF0, the authors give an overview of types of features, classifiers and emotional speech databases used in various SER research. Speech emotion recognition has evolved over time with regards to both the type of features and models used for classifiers. Different types of features that can be used can involve simple features like pitch and intensity BIBREF1, BIBREF2. Some studies use low-level descriptor features(LLDs) like jitter, shimmer, HNR and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is performance improvement proposed methods are used?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "b3eb1308-3e01-4903-be04-528f4258d508",
        "metadata": {
          "vector_store_key": "2002.11268-7",
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "cbe9f208-3730-4128-8f81-ac3a378ea889",
        "metadata": {
          "vector_store_key": "1707.03764-0",
          "chunk_id": 16,
          "document_id": "1707.03764",
          "start_idx": 6168,
          "end_idx": 6712
        },
        "page_content": " out to be detrimental to performance, when compared to the simpler system that we have described in Section SECREF2 . We divide our attempts according to the different ways we attempted to enhance performance: manipulating the data itself (adding more, and changing preprocessing), using a large variety of features, and changing strategies in modelling the problem by using different algorithms and paradigms. All reported results are on the PAN 2017 training data using five-fold cross-validation, unless otherwise specified. We extended the",
        "type": "Document"
      },
      {
        "id": "5ff5222e-22ba-4783-bf73-5f82b021a5b9",
        "metadata": {
          "vector_store_key": "1912.09152-0",
          "chunk_id": 74,
          "document_id": "1912.09152",
          "start_idx": 26186,
          "end_idx": 26669
        },
        "page_content": " system performance. With these methods, system development can go hand in hand with dataset refinement in a virtuous circle that let us think that maybe next time we are planning to add a new gazetteer or word embedding to our system in order to try to improve system performance, we should first look at our data and, like King Midas, turn our Human Annotated Dataset into a true Gold Standard Dataset. We thank three anonymous reviewers of our manuscript for their careful reading",
        "type": "Document"
      },
      {
        "id": "0a3b181b-177d-4c5b-a442-587d8e696c04",
        "metadata": {
          "vector_store_key": "1912.09152-5",
          "chunk_id": 73,
          "document_id": "1912.09152",
          "start_idx": 25875,
          "end_idx": 26391
        },
        "page_content": "design system can outperform a ner tagger for biomedical entities based on state-of-the-art artificial neural network technology. Thus, knowledge-based methods stand on their own merits in task resolution. But, perhaps most importantly, the other key point brought to light in this contribution is that a resource-based approach also favours a more critical stance on the dataset(s) used to evaluate system performance. With these methods, system development can go hand in hand with dataset refinement in a virtuous",
        "type": "Document"
      },
      {
        "id": "59ec1252-7f0e-4534-80eb-7357eed56662",
        "metadata": {
          "vector_store_key": "2003.07758-5",
          "chunk_id": 72,
          "document_id": "2003.07758",
          "start_idx": 23734,
          "end_idx": 24187
        },
        "page_content": " most 100 proposals per video. Notably, up to early 2017, the evaluation code had an issue which previously overestimated the performance of the algorithms in the learned proposal setting BIBREF9. Therefore, we report the results using the new evaluation code. We compare our method with five related approaches, namely Krishna BIBREF2, Wang BIBREF5, Zhou BIBREF4, Li BIBREF6, and Rahman BIBREF38. We take the performance values from the original papers",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does transfer learning improve performance on this task?",
    "llm_answer": "An average improvement of F$_1$=75% is achieved with transfer learning, compared to in-domain results.  The best result is F$_1$=76%.\n",
    "context": [
      {
        "id": "37ba11a9-5b70-4a7c-af4b-f48cbbf9e89e",
        "metadata": {
          "vector_store_key": "1909.02764-2",
          "chunk_id": 55,
          "document_id": "1909.02764",
          "start_idx": 21414,
          "end_idx": 21819
        },
        "page_content": " column \u201cTransfer L.\u201d in Table TABREF19. The confusion matrix is also depicted in Table TABREF16. With this procedure we achieve an average performance of F$_1$=75 %, being better than the results from the in-domain Experiment 1. The best performance of F$_1$=76 % is achieved with the model pre-trained on each data set, except for ISEAR. All transfer learning models clearly outperform their simple out-",
        "type": "Document"
      },
      {
        "id": "1a963965-0697-42d4-b9d1-82802643efc7",
        "metadata": {
          "vector_store_key": "1909.02764-2",
          "chunk_id": 56,
          "document_id": "1909.02764",
          "start_idx": 21819,
          "end_idx": 22276
        },
        "page_content": " each data set, except for ISEAR. All transfer learning models clearly outperform their simple out-of-domain counterpart. To ensure that this performance increase is not only due to the larger data set, we compare these results to training the model without transfer on a corpus consisting of each corpus together with AMMER (again, in leave-one-out crossvalidation). These results are depicted in column \u201cJoint C.\u201d. Thus, both settings, \u201ctransfer learning\ufffd",
        "type": "Document"
      },
      {
        "id": "8a881478-83b1-438c-9358-8332c5088311",
        "metadata": {
          "vector_store_key": "1811.02906-0",
          "chunk_id": 47,
          "document_id": "1811.02906",
          "start_idx": 19010,
          "end_idx": 19381
        },
        "page_content": " performed best suggests that this approach will also contribute beneficially to other text classification tasks such as sentiment analysis. Thus, in future work, we plan to evaluate our approach with regard to such other tasks. We also plan to evaluate more task-agnostic approaches for transfer learning, for instance employing language modeling as a pre-training task.",
        "type": "Document"
      },
      {
        "id": "52d050fa-4708-43df-ab3c-403c8a054292",
        "metadata": {
          "vector_store_key": "1910.09295-7",
          "chunk_id": 20,
          "document_id": "1910.09295",
          "start_idx": 7227,
          "end_idx": 7713
        },
        "page_content": "ifier, instead of setting it as a separate step. This produces two losses to be optimized together during training, and ensures that no task (stylometric adaptation or classification) will be prioritized over the other. This concept has been proposed and explored to improve the performance of transfer learning in multiple language tasks BIBREF9, BIBREF10. We show that this method improves performance on both BERT and GPT-2, given that it learns to adapt to the idiosyncracies of its",
        "type": "Document"
      },
      {
        "id": "f47af812-536b-464d-88c5-a8e7b327091e",
        "metadata": {
          "vector_store_key": "1809.04686-1",
          "chunk_id": 0,
          "document_id": "1809.04686",
          "start_idx": 0,
          "end_idx": 531
        },
        "page_content": "Transfer learning has been shown to work well in Computer Vision where pre-trained components from a model trained on ImageNet BIBREF0 are used to initialize models for other tasks BIBREF1 . In most cases, the other tasks are related to and share architectural components with the ImageNet task, enabling the use of such pre-trained models for feature extraction. With this transfer capability, improvements have been obtained on other image classification datasets, and on other tasks such as object detection, action recognition,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What baseline is used?",
    "llm_answer": "Baseline 1, Baseline 2, Baseline 3, ANRES_Sing_R, ANRES_Sing_Z, ANRES_Seq_R, ANRES_Seq_Z, LSTM baseline.\n",
    "context": [
      {
        "id": "593127ef-0325-4226-be3e-0ed371ce653f",
        "metadata": {
          "vector_store_key": "1804.08782-7",
          "chunk_id": 37,
          "document_id": "1804.08782",
          "start_idx": 13990,
          "end_idx": 14427
        },
        "page_content": " number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 .  For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose",
        "type": "Document"
      },
      {
        "id": "0b77285e-fdf5-4273-8639-5afcd5734567",
        "metadata": {
          "vector_store_key": "2001.02380-3",
          "chunk_id": 140,
          "document_id": "2001.02380",
          "start_idx": 51470,
          "end_idx": 51956
        },
        "page_content": " baselines. The left, middle and right panels in Figure FIGREF40 correspond to measurements when all signals are included, only cases contained entirely in the head EDUs shown to the model, and only DMs, respectively. The scenario on the left is rather unreasonable and is included only for completeness: here the model is also penalized for not detecting signals such as lexical chains, part of which is outside the units that the model is being shown. An example of such a case can be",
        "type": "Document"
      },
      {
        "id": "fe0f18bc-f5e9-4a30-bd8f-1b27de376893",
        "metadata": {
          "vector_store_key": "2004.01878-5",
          "chunk_id": 40,
          "document_id": "2004.01878",
          "start_idx": 14526,
          "end_idx": 14915
        },
        "page_content": " normal distribution $\\mathcal {N}(0, 1)$. In summary, the following four baselines are designed: ANRES_Sing_R: randomly initializing the states for each single trading day. ANRES_Sing_Z: initializing the states as zeros for each single trading day. ANRES_Seq_R: randomly initializing the first states for each trading sequence only. ANRES_Seq_Z: initializing the first states as zeros for",
        "type": "Document"
      },
      {
        "id": "95d982c1-df48-40f1-aea4-aa545ace30dc",
        "metadata": {
          "vector_store_key": "1909.02635-8",
          "chunk_id": 23,
          "document_id": "1909.02635",
          "start_idx": 9050,
          "end_idx": 9548
        },
        "page_content": " (iii) First Occurrence, where we predict the ingredient to be present in all steps following the first exact match. These latter two baselines capture natural modes of reasoning about the dataset: an ingredient is used when it is directly mentioned, or it is used in every step after it is mentioned, reflecting the assumption that a recipe is about incrementally adding ingredients to an ever-growing mixture. We also construct a LSTM baseline to evaluate the performance of ELMo embeddings (ELMo",
        "type": "Document"
      },
      {
        "id": "f86fe7c3-0c31-4435-aaa1-78adb90baabd",
        "metadata": {
          "vector_store_key": "1805.04033-7",
          "chunk_id": 59,
          "document_id": "1805.04033",
          "start_idx": 22928,
          "end_idx": 23307
        },
        "page_content": "\u65f6\u95f4\u201d (time). However, the relatedness is learned poorly in the baseline, as there is \u201cUTF8gbsn\u77e5\u9053\u201d (know), a number, and two particles in the possible substitutes considered by the baseline. Another representative example is the word \u201cUTF8gbsn\u56fe\u50cf\u201d (image), where the baseline also includes two particles in its most related words. The phenomenon shows that the baseline suffers from",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What topic clusters are identified by LDA?",
    "llm_answer": "Substance use, suicidality, and anxiety disorders.  Also, topics in a causal corpus tend to fall into three main categories, though specifics aren't given.\n",
    "context": [
      {
        "id": "80122f0c-298e-4132-b581-0ebf67dc6149",
        "metadata": {
          "vector_store_key": "1809.05752-3",
          "chunk_id": 17,
          "document_id": "1809.05752",
          "start_idx": 7163,
          "end_idx": 7672
        },
        "page_content": " of .784 compared to a baseline of .618. To perform clinical validation of the topics derived from the LDA model, they manually evaluated and annotated the topics, identifying the most informative vocabulary for the top ten topics. With their training data, they found the strongest coherence occurred in topics involving substance use, suicidality, and anxiety disorders. But given the unsupervised nature of the LDA clustering algorithm, the topic coherence they observed is not guaranteed across data sets.",
        "type": "Document"
      },
      {
        "id": "e498da5e-093f-42b9-a2a1-1aaeb51cce5e",
        "metadata": {
          "vector_store_key": "1604.05781-5",
          "chunk_id": 61,
          "document_id": "1604.05781",
          "start_idx": 21504,
          "end_idx": 21944
        },
        "page_content": " terms co-occur across a set of documents. We used the popular topic modeling method Latent Dirichlet Allocation (LDA) BIBREF39 . We ranked unigrams by how strongly associated they were with the topic. Inspecting these unigrams we found that a 10-topic model discovered meaningful topics. See Methods for full details. The top unigrams for each topic are shown in Tab. 1 . Topics in the causal corpus tend to fall into three main categories",
        "type": "Document"
      },
      {
        "id": "921d68ac-0416-4145-b46d-3ee5134e93d7",
        "metadata": {
          "vector_store_key": "1709.07916-0",
          "chunk_id": 15,
          "document_id": "1709.07916",
          "start_idx": 5826,
          "end_idx": 6267
        },
        "page_content": " identifying patterns of clinical events in a cohort of brain cancer patients BIBREF48 . Among topic models, Latent Dirichlet Allocation (LDA) BIBREF49 is the most popular effective model BIBREF50 , BIBREF19 as studies have shown that LDA is an effective computational linguistics model for discovering topics in a corpus BIBREF51 , BIBREF52 . LDA assumes that a corpus contains topics such that each word in each document can be assigned to",
        "type": "Document"
      },
      {
        "id": "73f09cc9-8253-4cda-8d46-3ea8b2c5dab3",
        "metadata": {
          "vector_store_key": "1909.00183-1",
          "chunk_id": 114,
          "document_id": "1909.00183",
          "start_idx": 42250,
          "end_idx": 42770
        },
        "page_content": " LDA topic modelling. While LDA computes topics at a pre-determined level of resolution, our method obtains partitions at all resolutions in one sweep of the Markov time, from which relevant partitions are chosen based on their robustness. The MS partitions at all resolutions are available for further investigation if so needed. Comparison of MS to other partitioning and community detection algorithms: We have partitioned the same kNN-MST graph using several well-known algorithms readily available in code libraries",
        "type": "Document"
      },
      {
        "id": "2942eae5-ba64-4eee-8480-9f424d3099b6",
        "metadata": {
          "vector_store_key": "1709.07916-0",
          "chunk_id": 16,
          "document_id": "1709.07916",
          "start_idx": 6060,
          "end_idx": 6564
        },
        "page_content": " . LDA assumes that a corpus contains topics such that each word in each document can be assigned to the topics with different degrees of membership BIBREF53 , BIBREF54 , BIBREF55 . Twitter users can post their opinions or share information about a subject to the public. Identifying the main topics of users' tweets provides an interesting point of reference, but conceptualizing larger subtopics of millions of tweets can reveal valuable insight to users' opinions. The topic discovery component of the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much do they outperform previous state-of-the-art?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "555a266e-0a08-43e5-b719-8cd2fd10ebb8",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 60,
          "document_id": "1909.13375",
          "start_idx": 20284,
          "end_idx": 20788
        },
        "page_content": " memory. Table TABREF24 shows the results on DROP's development set. Compared to our base models, our large models exhibit a substantial improvement across all metrics. We can see that our base model surpasses the NABERT+ baseline in every metric. The major improvement in multi-span performance was expected, as our multi-span head was introduced specifically to tackle this type of questions. For the other types, most of the improvement came from better preprocessing. A more detailed discussion could",
        "type": "Document"
      },
      {
        "id": "b8463b23-0372-4422-ba8d-e82788fecd2b",
        "metadata": {
          "vector_store_key": "1909.13375-0",
          "chunk_id": 61,
          "document_id": "1909.13375",
          "start_idx": 20788,
          "end_idx": 21299
        },
        "page_content": " For the other types, most of the improvement came from better preprocessing. A more detailed discussion could be found in Section (SECREF36). Notice that different BERTlarge models were used, so the comparison is less direct. Overall, our large models exhibits similar results to those of MTMSNlarge. For multi-span questions we achieve a significantly better performance. While a breakdown of metrics was only available for MTMSNlarge, notice that even when comparing these metrics to our base model, we still",
        "type": "Document"
      },
      {
        "id": "f3388551-a71b-44ec-a498-8fb27168bf88",
        "metadata": {
          "vector_store_key": "1910.09295-2",
          "chunk_id": 42,
          "document_id": "1910.09295",
          "start_idx": 14533,
          "end_idx": 14924
        },
        "page_content": " 87.47% accuracy, a 10.05% improvement over the siamese network's performance. GPT-2 finetuned to a final accuracy of 90.99%, a 13.57% improvement from the baseline performance. ULMFiT finetuning gave a final accuracy of 91.59%, an improvement of 14.17% over the baseline Siamese Network. We could see that TL techniques outperformed the siamese network baseline, which we hypothesize is due",
        "type": "Document"
      },
      {
        "id": "f5231330-5477-4f0e-86d4-50cfa2d86fb1",
        "metadata": {
          "vector_store_key": "1710.00341-1",
          "chunk_id": 35,
          "document_id": "1710.00341",
          "start_idx": 12930,
          "end_idx": 13373
        },
        "page_content": " vs. false claims extracted from snopes.com, by using information extracted from the Web. Their formulation of the task is the same as ours, but our corpora and label distributions are not the same, which makes a direct comparison impossible. Still, we can see that regarding overall classification accuracy they improve a baseline from 73.7% to 84.02% with their best model, i.e., a 39.2% relative error reduction. In our case, we go from 66.",
        "type": "Document"
      },
      {
        "id": "30f83adf-18b4-4e21-94e2-8d9a7d7fd19b",
        "metadata": {
          "vector_store_key": "1803.02839-6",
          "chunk_id": 4,
          "document_id": "1803.02839",
          "start_idx": 1481,
          "end_idx": 1849
        },
        "page_content": " , BIBREF7 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF10 , BIBREF11 , BIBREF8 , BIBREF9 . Often, the improvement simply mitigates the exploding/vanishing gradient problem BIBREF18 , BIBREF19 , but, for many tasks, the improvement is more capable of generalizing the network's training for that task. Understanding better how and why certain networks excel at certain NLP",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is the provided treebank?",
    "llm_answer": "The answer is not found in the context.  While sizes of various treebanks are mentioned, there is no single answer to the overall question.\n",
    "context": [
      {
        "id": "11cbf5e7-a658-4be4-b587-34e4b623b0fc",
        "metadata": {
          "vector_store_key": "1711.02013-1",
          "chunk_id": 60,
          "document_id": "1711.02013",
          "start_idx": 22459,
          "end_idx": 22972
        },
        "page_content": "bank Wall Street Journal section which contained 10 words or less after the removal of punctuation and null elements. Evaluation was done by seeing whether proposed constituent spans are also in the Treebank parse, measuring unlabeled F1 ( INLINEFORM0 ) of unlabeled constituent precision and recall. Constituents which could not be gotten wrong (those of span one and those spanning entire sentences) were discarded. Given the mechanism discussed in Section SECREF14 , our model generates a binary tree. Although",
        "type": "Document"
      },
      {
        "id": "bb598760-0443-4928-8612-ee08dabe7814",
        "metadata": {
          "vector_store_key": "1911.08673-0",
          "chunk_id": 68,
          "document_id": "1911.08673",
          "start_idx": 23969,
          "end_idx": 24082
        },
        "page_content": " effectiveness of our parser by achieving good performance on 27 treebanks, including the PTB and CTB benchmarks.",
        "type": "Document"
      },
      {
        "id": "5eb89519-41dd-4641-a6ae-ea0bc0967ce0",
        "metadata": {
          "vector_store_key": "1911.03894-5",
          "chunk_id": 28,
          "document_id": "1911.03894",
          "start_idx": 10104,
          "end_idx": 10550
        },
        "page_content": "IBREF30 is the second-largest treebank available for French after the FTB (described in subsection SECREF25), it contains data from blogs, news articles, reviews, and Wikipedia. The Sequoia treebank BIBREF31, BIBREF32 comprises more than 3000 sentences, from the French Europarl, the regional newspaper L\u2019Est R\u00e9publicain, the French Wikipedia and documents from the European Medicines Agency. Spoken is a corpus converted automatically from the R",
        "type": "Document"
      },
      {
        "id": "3fd0d403-b663-4f73-b8f1-cfbe94ff3a61",
        "metadata": {
          "vector_store_key": "1910.12618-4",
          "chunk_id": 48,
          "document_id": "1910.12618",
          "start_idx": 19089,
          "end_idx": 19565
        },
        "page_content": " maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014",
        "type": "Document"
      },
      {
        "id": "ee3e3597-25f0-49dc-88a5-b200c96fa10c",
        "metadata": {
          "vector_store_key": "1911.08673-0",
          "chunk_id": 48,
          "document_id": "1911.08673",
          "start_idx": 17050,
          "end_idx": 17393
        },
        "page_content": " the overall time complexity is $O$($n$log$n$) $+$ $O(n^2)$, simplifying to $O(n^2)$. We evaluate our parsing model on the English Penn Treebank (PTB), the Chinese Penn Treebank (CTB), treebanks from two CoNLL shared tasks and the Universal Dependency (UD) Treebanks, using unlabeled attachment scores (UAS) and labeled attachment scores (LAS)",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what dataset was used?",
    "llm_answer": "Multiple datasets were used:  Basic Question Dataset (BQD), SCI dataset, and a subset of the development set questions from a Gold Standard Dataset.  Also, a dataset of 3 billion (query, image, weight) triples from the MS COCO dataset.\n",
    "context": [
      {
        "id": "0d593f74-7474-41cc-9928-ce68ac96f714",
        "metadata": {
          "vector_store_key": "1802.05574-2",
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "d1adc989-abd9-4bc0-b4a1-d8160e471e4a",
        "metadata": {
          "vector_store_key": "1703.06492-1",
          "chunk_id": 28,
          "document_id": "1703.06492",
          "start_idx": 9989,
          "end_idx": 10376
        },
        "page_content": " dataset, called Basic Question Dataset (BQD), generated by our basic question generation algorithm. BQD is the first basic question dataset. Regarding the BQD, the dataset format is $\\lbrace Image,~MQ,~3~(BQ + corresponding~similarity~score)\\rbrace $ . All of our images are from the testing images of MS COCO dataset BIBREF30 , the MQ, main questions, are from the testing questions of",
        "type": "Document"
      },
      {
        "id": "47700f54-52ba-40b9-bf2e-4fde786f8ac4",
        "metadata": {
          "vector_store_key": "1912.09152-4",
          "chunk_id": 75,
          "document_id": "1912.09152",
          "start_idx": 26669,
          "end_idx": 26983
        },
        "page_content": " a true Gold Standard Dataset. We thank three anonymous reviewers of our manuscript for their careful reading and their many insightful comments and suggestions. We have made our best in providing a revised version of the manuscript that reflects their suggestions. Any remaining errors are our own responsability.",
        "type": "Document"
      },
      {
        "id": "7e589466-1803-4a33-94aa-80d0575f23b4",
        "metadata": {
          "vector_store_key": "1905.12260-1",
          "chunk_id": 21,
          "document_id": "1905.12260",
          "start_idx": 8180,
          "end_idx": 8639
        },
        "page_content": " results. For example, one (query, image) pair might be \u201ccat with big ears\u201d and an image of a cat. Each (query, image) pair also has a weight corresponding to a relevance score of the image for the query. The dataset includes 3 billion (query, image, weight) triples, with 900 million unique images and 220 million unique queries. The data was prepared by first taking the query-image set, filtering to remove any personally identifiable information and adult",
        "type": "Document"
      },
      {
        "id": "1c380a0c-1037-4628-8ada-168d9c8247bc",
        "metadata": {
          "vector_store_key": "1707.03904-9",
          "chunk_id": 13,
          "document_id": "1707.03904",
          "start_idx": 5304,
          "end_idx": 5850
        },
        "page_content": " subset of the development set questions to allow researchers to analyze the categories in which their system performs well or falls short. We plan to release these annotations along with the datasets, and our retrieved documents for each question. Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the citation intent labels in the datasets?",
    "llm_answer": "Background, Method, ResultComparison.  There are also six intent categories in a different, larger dataset.\n",
    "context": [
      {
        "id": "f6d49ea0-2706-4d39-a161-1c535c5a27f3",
        "metadata": {
          "vector_store_key": "1904.01608-3",
          "chunk_id": 34,
          "document_id": "1904.01608",
          "start_idx": 11894,
          "end_idx": 12502
        },
        "page_content": " as background information providing more context for the current research topic. More interesting intent categories are a direct use of a method or comparison of results. Therefore, our dataset provides a concise annotation scheme that is useful for navigating research topics and machine reading of scientific papers. We consider three intent categories outlined in Table 1 : Background, Method and ResultComparison. Below we describe data collection and annotation details. Citation intent of sentence extractions was labeled through the crowdsourcing platform Figure Eight. We selected a sample of papers",
        "type": "Document"
      },
      {
        "id": "6f1321c5-b9b2-4326-a343-f1fa258b5ced",
        "metadata": {
          "vector_store_key": "1904.01608-3",
          "chunk_id": 31,
          "document_id": "1904.01608",
          "start_idx": 10479,
          "end_idx": 11015
        },
        "page_content": " annotated by domain experts in the NLP field. The data was split into three standard stratified sets of train, validation, and test with 85% of data used for training and remaining 15% divided equally for validation and test. Each citation unit includes information about the immediate citation context, surrounding context, as well as information about the citing and cited paper. The data includes six intent categories outlined in Table 2 . Most existing datasets contain citation categories that are too fine-grained. Some of these",
        "type": "Document"
      },
      {
        "id": "e103b26f-605d-4098-b2bb-6e1bc4e13003",
        "metadata": {
          "vector_store_key": "1904.01608-3",
          "chunk_id": 2,
          "document_id": "1904.01608",
          "start_idx": 831,
          "end_idx": 1348
        },
        "page_content": " the problem of citation intent classification by modeling the language expressed in the citation context. A citation context includes text spans in a citing paper describing a referenced work and has been shown to be the primary signal in intent classification BIBREF8 , BIBREF9 , BIBREF7 . Existing models for this problem are feature-based, modeling the citation context with respect to a set of predefined hand-engineered features (such as linguistic patterns or cue phrases) and ignoring other signals that could",
        "type": "Document"
      },
      {
        "id": "e83891d8-9984-4bd5-88df-db2ad225769f",
        "metadata": {
          "vector_store_key": "1904.01608-3",
          "chunk_id": 33,
          "document_id": "1904.01608",
          "start_idx": 11347,
          "end_idx": 11894
        },
        "page_content": " are relatively small (less than 2,000 annotated citations). To address these limitations, we introduce SciCite, a new dataset of citation intents that is significantly larger, more coarse-grained and general-domain compared with existing datasets. Through examination of citation intents, we found out many of the categories defined in previous work such as motivation, extension or future work, can be considered as background information providing more context for the current research topic. More interesting intent categories are a direct use",
        "type": "Document"
      },
      {
        "id": "66318ea6-5c4d-4b0d-8cc2-852b362cedb2",
        "metadata": {
          "vector_store_key": "1904.01608-3",
          "chunk_id": 1,
          "document_id": "1904.01608",
          "start_idx": 545,
          "end_idx": 1091
        },
        "page_content": " Therefore, identifying the intent of citations (Figure 1 ) is critical in improving automated analysis of academic literature and scientific impact measurement BIBREF1 , BIBREF3 . Other applications of citation intent classification are enhanced research experience BIBREF4 , information retrieval BIBREF5 , summarization BIBREF6 , and studying evolution of scientific fields BIBREF7 . In this work, we approach the problem of citation intent classification by modeling the language expressed in the citation context. A citation context includes",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is quality of annotation measured?",
    "llm_answer": "By comparing annotations to gold-standard labels, measuring inter-annotator agreement, and assessing accuracy with AMT annotations.\n",
    "context": [
      {
        "id": "47101769-6c68-462d-b3b5-85609136939b",
        "metadata": {
          "vector_store_key": "1906.03538-6",
          "chunk_id": 23,
          "document_id": "1906.03538",
          "start_idx": 9308,
          "end_idx": 9838
        },
        "page_content": " the five labels into three labels, by collapsing mildly-support and mildly-oppose into support and oppose, respectively. To assess the quality of these annotations, two of the authors independently annotate a random subset of instances in the previous step (328 perspectives for 10 claims). Afterwards, the differences were adjudicated. We measure the accuracy adjudicated results with AMT annotations to estimate the quality of our annotation. This results in an accuracy of 94%, which shows high-agreement with the crowdsourced",
        "type": "Document"
      },
      {
        "id": "b24ea791-a570-4591-8aeb-7a6bd7eb7040",
        "metadata": {
          "vector_store_key": "1905.10044-3",
          "chunk_id": 29,
          "document_id": "1905.10044",
          "start_idx": 10909,
          "end_idx": 11439
        },
        "page_content": " and the kinds of reasoning abilities required to answer them. First, in order to assess annotation quality, three of the authors labelled 110 randomly chosen examples. If there was a disagreement, the authors conferred and selected a single answer by mutual agreement. We call the resulting labels \u201cgold-standard\" labels. On the 110 selected examples, the answer annotations reached 90% accuracy compared to the gold-standard labels. Of the cases where the answer annotation differed from the gold-standard, six were ambiguous or",
        "type": "Document"
      },
      {
        "id": "e2728946-0c8e-4ab5-b9c1-681f292ca675",
        "metadata": {
          "vector_store_key": "1906.09774-1",
          "chunk_id": 42,
          "document_id": "1906.09774",
          "start_idx": 17090,
          "end_idx": 17533
        },
        "page_content": "., (1: not at all, 3: somewhat, 5: very much) from around 100 different annotators. After getting all of the human judgement with different criteria, some of these studies used a t-test to get the statistical significance BIBREF28 , BIBREF39 , while some other used inter-annotator agreement measurement such as Fleiss Kappa BIBREF15 , BIBREF14 . Based on these evaluations, they can compare their system performance with baseline or any other",
        "type": "Document"
      },
      {
        "id": "ae777c6e-5f84-42f7-8843-5ba4994ac482",
        "metadata": {
          "vector_store_key": "1703.05260-8",
          "chunk_id": 49,
          "document_id": "1703.05260",
          "start_idx": 17551,
          "end_idx": 18087
        },
        "page_content": " results were discussed with the authors. Throughout the whole annotation phase, annotators could discuss any emerging issues with the authors. All annotations were done by undergraduate students of computational linguistics. The annotation was rather time-consuming due to the complexity of the task, and thus we decided for single annotation mode. To assess annotation quality, a small sample of texts was annotated by all four annotators and their inter-annotator agreement was measured (see Section \"Inter-Annotator Agreement\" ). It",
        "type": "Document"
      },
      {
        "id": "fdc53ccf-a444-4da1-816f-891b497da15f",
        "metadata": {
          "vector_store_key": "1802.05574-0",
          "chunk_id": 7,
          "document_id": "1802.05574",
          "start_idx": 2859,
          "end_idx": 3379
        },
        "page_content": ". Human annotators analyze sentences and determine correct relations to be extracted. Systems are then evaluated with respect to the overlap or similarity of their extractions to the ground truth annotations, allowing the standard metrics of precision and recall to be reported. This seems sensible but is actually problematic because of different but equivalent representations of the information in an article. For example, consider the sentence \u201cThe patient was treated with Emtricitabine, Etravirine, and Darunavir\u201d.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What accuracy score do they obtain?",
    "llm_answer": "Team 9 achieved a joint goal accuracy of 86.53%.\n",
    "context": [
      {
        "id": "562931f2-fe45-41ea-91d7-50ddfb8ddc8b",
        "metadata": {
          "vector_store_key": "1909.05246-7",
          "chunk_id": 44,
          "document_id": "1909.05246",
          "start_idx": 16417,
          "end_idx": 16962
        },
        "page_content": " accuracy measures the similarity of the system generated response versus the target response. Eric and Manning eric2017copy used this metric to evaluate their systems in which they considered their response to be correct if all tokens in the system generated response matched the corresponding token in the target response. This metric is a little bit harsh, and the results may be low since all the tokens in the generated response have to be exactly in the same position as in the target response. Per-Dialogue Accuracy: We calculate per-dial",
        "type": "Document"
      },
      {
        "id": "975b0340-cb8a-4c46-87f7-07ef48e97379",
        "metadata": {
          "vector_store_key": "2001.11268-9",
          "chunk_id": 58,
          "document_id": "2001.11268",
          "start_idx": 22809,
          "end_idx": 23352
        },
        "page_content": " precision scores in ranges between 0.92 and 0.97 are possible with F1 scores not dropping below 0.84 for the main classes of interest. In practice, the detachment between model predictions and assignment of labels means that a reviewer who wishes to switch between high recall and high precision results can do so very quickly, without obtaining new predictions from the model itself. More visualizations can be found in this project's GitHub repository , including true class labels and a detailed breakdown of true and false predictions for",
        "type": "Document"
      },
      {
        "id": "07e0c641-7653-4da3-8ad4-f080ba44d549",
        "metadata": {
          "vector_store_key": "1804.08186-0",
          "chunk_id": 276,
          "document_id": "1804.08186",
          "start_idx": 99381,
          "end_idx": 99888
        },
        "page_content": "often referred to as the \u201cgold-standard\u201d), and a predicted label for each document from the same set, the document-level accuracy is the proportion of documents that are correctly labeled over the entire evaluation collection. This is the most frequently reported metric and conveys the same information as the error rate, which is simply the proportion of documents that are incorrectly labeled (i.e. INLINEFORM0 ). Authors sometimes provide a per-language breakdown of results. There are two distinct ways",
        "type": "Document"
      },
      {
        "id": "ba59cbd1-c7b2-4e12-b4e7-dce805518c51",
        "metadata": {
          "vector_store_key": "2002.01359-7",
          "chunk_id": 55,
          "document_id": "2002.01359",
          "start_idx": 23040,
          "end_idx": 23557
        },
        "page_content": "11 shows the evaluation metrics for the different submissions obtained on the test set. It also lists the performance of different submissions on seen and unseen services, helping evaluate the effectiveness in zero-shot settings. Team 9 achieved a very high joint goal accuracy of 86.53%, around 9% higher than the second-placed team. We observed the following trends across submissions: For unseen services, performance on categorical slots is comparable to that on non-categorical slots. On the other hand, for seen",
        "type": "Document"
      },
      {
        "id": "ec651bfc-61bb-46ab-a708-6779938b6cdb",
        "metadata": {
          "vector_store_key": "1701.02877-6",
          "chunk_id": 62,
          "document_id": "1701.02877",
          "start_idx": 22362,
          "end_idx": 22802
        },
        "page_content": " data gives little to no improvement, whereas adding positive examples drastically improves performance. Table UID9 shows results with size normalised precision (P), recall (R), and F1-Score (F1). The five lowest P, R and F1 values per method (CRFSuite, Stanford NER, SENNA) are in bold to highlight underperformers. Results for all corpora are summed with macro average. Comparing the different methods, the highest F1 results are achieved",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the 12 class bilingual text?",
    "llm_answer": "The answer is not found in the context.\n",
    "context": [
      {
        "id": "dfc3db32-fd8b-4fbe-ae34-0fc06b8deda6",
        "metadata": {
          "vector_store_key": "1911.13066-2",
          "chunk_id": 4,
          "document_id": "1911.13066",
          "start_idx": 1723,
          "end_idx": 2230
        },
        "page_content": " many NLP tasks involving multilingual text. A Convolutional Neural Network (CNN) based model for sentiment classification of a multilingual dataset was proposed in BIBREF3. However, a particular record in the dataset belonged to one language only. In our case, a record can have either one or two languages. There is very little published work on this specific setting. One way to classify bilingual text is to normalize the different variations of a word to a standard spelling before training the model B",
        "type": "Document"
      },
      {
        "id": "7f1ae14e-39b0-4772-8bc8-b902df646ce6",
        "metadata": {
          "vector_store_key": "1711.11221-6",
          "chunk_id": 38,
          "document_id": "1711.11221",
          "start_idx": 12389,
          "end_idx": 12768
        },
        "page_content": " We selected corpora LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and a portion of data from the corpus LDC2004T08 (Hong Kong Hansards/Laws/News) as our bilingual training data, where document boundaries are explicitly kept. In total, our training data contain 103,236 documents and 2.80M sentences. On average, each document consists of 28.4 sentences. We chose NIST05 dataset",
        "type": "Document"
      },
      {
        "id": "0ec95a46-f4e7-4d0a-ad4b-e06d1ce3865d",
        "metadata": {
          "vector_store_key": "1905.12260-0",
          "chunk_id": 14,
          "document_id": "1905.12260",
          "start_idx": 5539,
          "end_idx": 5963
        },
        "page_content": " bilingual lexicons BIBREF13 , BIBREF4 , BIBREF6 , BIBREF14 or parallel/aligned corpora BIBREF15 , BIBREF4 , BIBREF16 , BIBREF17 . These works are also largely bilingual due to either limitations of methods or the requirement for data that exists only for a few language pairs. Bilingual embeddings are less desirable because they do not leverage the relevant resources of other languages. For example, in learning bilingual",
        "type": "Document"
      },
      {
        "id": "1e39cb37-36b3-4b7c-8fff-408991f75381",
        "metadata": {
          "vector_store_key": "1909.09270-4",
          "chunk_id": 37,
          "document_id": "1909.09270",
          "start_idx": 13903,
          "end_idx": 14242
        },
        "page_content": "2003 shared tasks BIBREF21, BIBREF22. These are taken from newswire text, and have labelset of Person, Organization, Location, Miscellaneous. The remaining four languages come from the LORELEI project BIBREF23. These languages are: Amharic (amh: LDC2016E87), Arabic (ara: LDC2016E89), Hindi (hin: LDC2017E62), and Somali (som: LDC2016E91).",
        "type": "Document"
      },
      {
        "id": "0d582531-a6b4-46f2-9565-ce7cc4858aae",
        "metadata": {
          "vector_store_key": "1911.13066-2",
          "chunk_id": 6,
          "document_id": "1911.13066",
          "start_idx": 2508,
          "end_idx": 3032
        },
        "page_content": " is not generalizable in the case of Roman Urdu text as it is an informal language with no proper grammatical rules and dictionary. More recent approach utilizes code-switching annotations to improve the predictive performance of the model, where each word is annotated with its respective language label. Such an approach is not scalable for large data as annotation task becomes tedious. In this paper, we propose a multi-cascaded deep learning network, called as McM for multi-class classification of bilingual short text",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which are the sequence model architectures this method can be transferred across?",
    "llm_answer": "LSTM seq2seq, Convolutional seq2seq, and transformer.\n",
    "context": [
      {
        "id": "7f5242a2-095b-4178-9bd9-b3e5d32cd62c",
        "metadata": {
          "vector_store_key": "1707.00110-2",
          "chunk_id": 0,
          "document_id": "1707.00110",
          "start_idx": 0,
          "end_idx": 365
        },
        "page_content": "Sequence-to-sequence models BIBREF0 , BIBREF1 have achieved state of the art results across a wide variety of tasks, including Neural Machine Translation (NMT) BIBREF2 , BIBREF3 , text summarization BIBREF4 , BIBREF5 , speech recognition BIBREF6 , BIBREF7 , image captioning BIBREF8 , and conversational modeling BIBREF9 , BIBREF10 . The most popular approaches are",
        "type": "Document"
      },
      {
        "id": "67da284e-da22-4e6f-919f-4d1ae4edb643",
        "metadata": {
          "vector_store_key": "2002.05058-3",
          "chunk_id": 51,
          "document_id": "2002.05058",
          "start_idx": 20175,
          "end_idx": 20645
        },
        "page_content": " representative sequence-to-sequence architectures: LSTM BIBREF27 seq2seq, Convolutional seq2seq BIBREF28, and transformer BIBREF1 model. We compare models with different architectures, hyperparameter choices, and early-stopping criteria with different automated metrics, as well as human evaluation. Regarding the evaluation metric (and criteria for choosing hyperparameter choice and early-stopping), we compare the proposed approach with the discriminative evaluator,",
        "type": "Document"
      },
      {
        "id": "b7880450-ffaf-48e0-b07e-4b690d9f677f",
        "metadata": {
          "vector_store_key": "1811.09786-9",
          "chunk_id": 73,
          "document_id": "1811.09786",
          "start_idx": 23896,
          "end_idx": 24139
        },
        "page_content": " are also prevalent for sequence modeling. The source code of our model can be found at https://github.com/vanzytay/NIPS2018_RCRN. We thank the anonymous reviewers and area chair from NIPS 2018 for their constructive and high quality feedback.",
        "type": "Document"
      },
      {
        "id": "e48919e9-c695-417a-84d2-6ab4b7a7cf73",
        "metadata": {
          "vector_store_key": "1907.02636-7",
          "chunk_id": 6,
          "document_id": "1907.02636",
          "start_idx": 2332,
          "end_idx": 2725
        },
        "page_content": ", and tokens assigned with IOC labels are then collected as IOCs. Recently, sequence labelling models have been utilized in many NLP tasks. Huang et al. BIBREF6 proposed using a sequence labelling model based on the bidirectional long short-term memory (LSTM) BIBREF7 for the task of named entity recognition (NER). Chiu et al. BIBREF8 and Lample et al. BIBREF9 proposed integrating LSTM encod",
        "type": "Document"
      },
      {
        "id": "f0a57f1c-3b8b-425e-a4c0-3952262e130a",
        "metadata": {
          "vector_store_key": "1810.03459-1",
          "chunk_id": 0,
          "document_id": "1810.03459",
          "start_idx": 0,
          "end_idx": 439
        },
        "page_content": "The sequence-to-sequence (seq2seq) model proposed in BIBREF0 , BIBREF1 , BIBREF2 is a neural architecture for performing sequence classification and later adopted to perform speech recognition in BIBREF3 , BIBREF4 , BIBREF5 . The model allows to integrate the main blocks of ASR such as acoustic model, alignment model and language model into a single framework. The recent ASR advancements in connectionist temporal classification (CTC) B",
        "type": "Document"
      }
    ]
  },
  {
    "query": " What percentage of improvement in inference speed is obtained by the proposed method over the newest state-of-the-art methods?",
    "llm_answer": "The answer is not found in the context.  While the text discusses inference speed improvements over specific prior methods (Lattice-LSTM and LR-CNN), it does not quantify improvement over the *newest* state-of-the-art methods.\n",
    "context": [
      {
        "id": "c99f2ff8-aa17-4b5b-9eea-83258deb06bf",
        "metadata": {
          "vector_store_key": "1908.05969-7",
          "chunk_id": 89,
          "document_id": "1908.05969",
          "start_idx": 26445,
          "end_idx": 26883
        },
        "page_content": " inference speed of our method when implementing the sequnece modeling layer with the LSTM-based, CNN-based, and Transformer-based architecture, respectively. The speed was evaluated by average sentences per second using a GPU (NVIDIA TITAN X). For a fair comparison with Lattice-LSTM and LR-CNN, we set the batch size of our method to 1 at inference time. From the table, we can see that our method has a much faster inference speed than",
        "type": "Document"
      },
      {
        "id": "ff4f7aa5-53d7-4a67-a171-86b47339c38c",
        "metadata": {
          "vector_store_key": "1908.05969-7",
          "chunk_id": 90,
          "document_id": "1908.05969",
          "start_idx": 26883,
          "end_idx": 27346
        },
        "page_content": " inference time. From the table, we can see that our method has a much faster inference speed than Lattice-LSTM when using the LSTM-based sequence modeling layer, and it was also much faster than LR-CNN, which used an CNN architecture to implement the sequence modeling layer. And as expected, our method with the CNN-based sequence modeling layer showed some advantage in inference speed than those with the LSTM-based and Transformer-based sequence model layer.",
        "type": "Document"
      },
      {
        "id": "ec6e2947-9837-4f29-91ed-b42f4d04ef26",
        "metadata": {
          "vector_store_key": "1710.00341-1",
          "chunk_id": 36,
          "document_id": "1710.00341",
          "start_idx": 13213,
          "end_idx": 13653
        },
        "page_content": "e., a 39.2% relative error reduction. In our case, we go from 66.7% to 80.0%, i.e., an almost identical 39.9% error reduction. These results are very encouraging, especially given the fact that our model is much simpler than theirs regarding the sources of information used (they model the stance of the text, the reliability of the sources, the language style of the articles, and the temporal footprint). Next, we tested the generality of",
        "type": "Document"
      },
      {
        "id": "2eed5d51-4dfd-4622-8141-f8d71567d458",
        "metadata": {
          "vector_store_key": "1611.03382-9",
          "chunk_id": 59,
          "document_id": "1611.03382",
          "start_idx": 21894,
          "end_idx": 22215
        },
        "page_content": " significantly speed up inference. We have demonstrated the effectiveness of our approach in the context of summarization and shown state-of-the-art performance. In the future, we plan to tackle summarization problems with large input text. We also plan to exploit our findings in other tasks such as machine translation.",
        "type": "Document"
      },
      {
        "id": "8e7795cd-a48b-4fcd-8dc9-00c6adee7995",
        "metadata": {
          "vector_store_key": "2002.05829-3",
          "chunk_id": 3,
          "document_id": "2002.05829",
          "start_idx": 1320,
          "end_idx": 1837
        },
        "page_content": " million inferences. The energy efficiency of inference cost could be critical to both business planning and environment impact. Previous work BIBREF10, BIBREF11 on this topic proposed new metrics like FPO (floating point operations) and new practice to report experimental results based on computing budget. Other benchmarks like BIBREF12 and BIBREF13 compares the efficiency of models on the classic reading comprehension task SQuAD and machine translation tasks. However, there has not been a concrete or practical",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the metric that is measures in this paper?",
    "llm_answer": "Premise-Hypothesis Distance, METEOR, tIoU, BIBREF16, WMD, VSM, PV-DTW.\n",
    "context": [
      {
        "id": "2c8ab506-d367-4e5d-ae14-d6b337c148ea",
        "metadata": {
          "vector_store_key": "1607.06025-0",
          "chunk_id": 75,
          "document_id": "1607.06025",
          "start_idx": 28598,
          "end_idx": 29144
        },
        "page_content": " figure is not show absolute values for each of the metrics, but to compare the metrics' curves to the curve of our main metric, the accuracy of the classifier. The first metric \u2013 Premise-Hypothesis Distance \u2013 represents the average Jaccard distance between the premise and the generated hypothesis. Datasets generated with low latent dimensions have hypotheses more similar to premises, which indicates that the generated hypotheses are more trivial and less diverse than hypothesis generated with higher latent dimensions. We also evaluated the",
        "type": "Document"
      },
      {
        "id": "78ca90a9-6244-4363-9cbe-252f39111f9b",
        "metadata": {
          "vector_store_key": "2003.07758-5",
          "chunk_id": 70,
          "document_id": "2003.07758",
          "start_idx": 23045,
          "end_idx": 23519
        },
        "page_content": "REF52. We regard the METEOR as our primary metric as it has been shown to be highly correlated with human judgement in a situation with a limited number of references (only one, in our case). We employ the official evaluation script provided in BIBREF53. Thus, the metrics are calculated if a proposed event and a ground truth location of a caption overlaps more than a specified temporal Intersection over Union (tIoU) and zero otherwise. All metric values are averaged for",
        "type": "Document"
      },
      {
        "id": "41dafdc8-0a0b-4598-93e1-e7526e9e86af",
        "metadata": {
          "vector_store_key": "1912.01046-7",
          "chunk_id": 36,
          "document_id": "1912.01046",
          "start_idx": 14275,
          "end_idx": 14679
        },
        "page_content": " BIBREF16, which measures how far away the predicted span is from the gold standard span, as a metric. The rationale behind the metric is that, in practice, it suffices to recommend a rough span which contains the answer \u2013 a difference of a few seconds would not matter much to the user. Specifically, the predicted span is counted as correct if $|pred_{start} - gt_{start}| + |pred_{end} - gt_{end}| <=$",
        "type": "Document"
      },
      {
        "id": "44bc9a3c-10b9-43b6-b19a-c95e60a6929b",
        "metadata": {
          "vector_store_key": "1709.01256-6",
          "chunk_id": 47,
          "document_id": "1709.01256",
          "start_idx": 17745,
          "end_idx": 18165
        },
        "page_content": " distance measure explained in Section SECREF23 . WMD: The Word Mover's Distance introduced in Section SECREF1 . WMD adapts the earth mover's distance to the space of documents. VSM: The similarity measure introduced in Section UID12 . PV-DTW: PV-DTW is the same as Algorithm SECREF21 except that the distance between two paragraphs is not based on Algorithm SECREF20 but rather computed as INLINEFORM0 where INLINEFORM1",
        "type": "Document"
      },
      {
        "id": "a3945c5a-0d01-4716-a840-3c14bd750a67",
        "metadata": {
          "vector_store_key": "1809.08731-9",
          "chunk_id": 37,
          "document_id": "1809.08731",
          "start_idx": 12336,
          "end_idx": 12724
        },
        "page_content": "$ , it is a non-symmetric quasi-metric. We apply it with $Y$ being the human ratings. An additional advantage as compared to Pearson is that it has an interpretable meaning: the expected error made by a given metric as compared to the human rating. As shown in Table 3 , WordSLOR and WPSLOR correlate best with human judgments: WordSLOR (respectively WPSLOR) has a $0.025$ (respectively $",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Was evaluation metrics and criteria were used to evaluate the output of the cascaded multimodal speech translation?",
    "llm_answer": "Automatic metrics, subjective human evaluation (with a focus on short translations and a head-to-head ranking of the first 10 sentences of each test talk), and document-level evaluation were used.\n",
    "context": [
      {
        "id": "35db8d81-5a4b-4599-a57e-f40009009cf2",
        "metadata": {
          "vector_store_key": "1612.06897-1",
          "chunk_id": 41,
          "document_id": "1612.06897",
          "start_idx": 13297,
          "end_idx": 13817
        },
        "page_content": " the score on the in-domain test set without seeing any significant degradation on the out-of-domain test set. In addition to evaluating on automatic metrics, we also performed a subjective human evaluation where a human annotator assigns a score based on the quality of the translation. The judgments are done by an experienced annotator (a native speaker of German and a fluent speaker of English). We ask our annotator to judge the translation output of different systems on a randomly selected in-domain sample of 50",
        "type": "Document"
      },
      {
        "id": "53d93768-f0ef-41e4-ad57-97072af55d9b",
        "metadata": {
          "vector_store_key": "1910.10408-0",
          "chunk_id": 59,
          "document_id": "1910.10408",
          "start_idx": 21469,
          "end_idx": 21983
        },
        "page_content": " goal is to be able to generate shorter translations and as close as possible to the length of the source sentences, we focused the manual evaluation on the Short output class and aimed to verify possible losses in quality with respect to the baseline system. We ran a head-to-head evaluation on the first 10 sentences of each test talk, for a total of 270 sentences, by asking annotators to blindly rank the two system outputs (ties were also permitted) in terms of quality with respect to a reference translation",
        "type": "Document"
      },
      {
        "id": "d2629f69-8007-4748-af1b-f07c12bb2ddc",
        "metadata": {
          "vector_store_key": "2004.01694-1",
          "chunk_id": 33,
          "document_id": "2004.01694",
          "start_idx": 13179,
          "end_idx": 13705
        },
        "page_content": " is the unit of evaluation. Historically, machine translation has primarily operated on the level of sentences, and so has machine translation evaluation. However, it has been remarked that human raters do not necessarily understand the intended meaning of a sentence shown out-of-context BIBREF25, which limits their ability to spot some mistranslations. Also, a sentence-level evaluation will be blind to errors related to textual cohesion and coherence. While sentence-level evaluation may be good enough when evaluating MT",
        "type": "Document"
      },
      {
        "id": "ddd6fa65-85ff-425b-80e1-5a567c3d35d7",
        "metadata": {
          "vector_store_key": "2004.01694-2",
          "chunk_id": 47,
          "document_id": "2004.01694",
          "start_idx": 18211,
          "end_idx": 18734
        },
        "page_content": "$_A$ (Table TABREF21). Our findings emphasise the importance of linguistic context in human evaluation of MT. In terms of adequacy, raters assessing documents as a whole show a significant preference for human translation, but when assessing single sentences in random order, they show no significant preference for human translation. Document-level evaluation exposes errors to raters which are hard or impossible to spot in a sentence-level evaluation, such as coherent translation of named entities. The example in Table",
        "type": "Document"
      },
      {
        "id": "9227613a-91df-489e-80cb-61d55e046e4c",
        "metadata": {
          "vector_store_key": "2004.01694-1",
          "chunk_id": 34,
          "document_id": "2004.01694",
          "start_idx": 13477,
          "end_idx": 14022
        },
        "page_content": " related to textual cohesion and coherence. While sentence-level evaluation may be good enough when evaluating MT systems of relatively low quality, we hypothesise that with additional context, raters will be able to make more nuanced quality assessments, and will also reward translations that show more textual cohesion and coherence. We believe that this aspect should be considered in evaluation, especially when making claims about human\u2013machine parity, since human translators can and do take inter-sentential context into account BIBREF15",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the two decoding functions?",
    "llm_answer": "Training decoding function:  $f_{\\text{de}}() = h(+) $\nTesting decoding function: $f_\\text{de}^{-1}() = h^{\\top}(h^{-1}() - )$\n",
    "context": [
      {
        "id": "9af3a89b-4670-4a06-af74-935d048a4cdf",
        "metadata": {
          "vector_store_key": "1912.06813-8",
          "chunk_id": 19,
          "document_id": "1912.06813",
          "start_idx": 6852,
          "end_idx": 7230
        },
        "page_content": " encoder\u2014decoder structure BIBREF4, where advanced ones are equipped with an attention mechanism BIBREF5, BIBREF6. First, an encoder ($\\text{Enc}$) maps $\\vec{x}_{1:n}$ into a sequence of hidden representations ${1:n}=(1, \\cdots , n)$. The decoding of the output sequence is autoregressive, which means that the previously generated symbols are considered an additional input at",
        "type": "Document"
      },
      {
        "id": "726e6a4f-f6a3-407c-8017-c9f7ff663d85",
        "metadata": {
          "vector_store_key": "1809.02731-2",
          "chunk_id": 25,
          "document_id": "1809.02731",
          "start_idx": 8154,
          "end_idx": 8495
        },
        "page_content": " function is simply $$1 , which is easily computed. The regularisation formula is $$2 , where $$3 is the Frobenius norm. Specifically, the update rule BIBREF23 for the regularisation is:  $$:=(1+\\beta )-\\beta (^\\top )\\nonumber $$   (Eq. 12)   The usage of the decoder during training and testing is defined as follows:  $$\\text{Training:} \\h",
        "type": "Document"
      },
      {
        "id": "888f0351-5bb9-4a63-8521-4c3ec4d1e397",
        "metadata": {
          "vector_store_key": "1901.00439-4",
          "chunk_id": 36,
          "document_id": "1901.00439",
          "start_idx": 13709,
          "end_idx": 14067
        },
        "page_content": " be considered as consisting of 2 parts, ie., the encoder and the decoder. The encoder, $f_{enc}(\\cdot )$ , is the part of the network that compresses the input, $I$ , into a latent space representation, $U$ , and the decoder, $f_{dec}(\\cdot )$ aims to reconstruct the input from the latent space representation (see equation 12 ). In essence,  $$U = f_{enc}",
        "type": "Document"
      },
      {
        "id": "3b8612e3-a536-4b87-a5eb-4a337640d556",
        "metadata": {
          "vector_store_key": "1809.02731-2",
          "chunk_id": 35,
          "document_id": "1809.02731",
          "start_idx": 10654,
          "end_idx": 10858
        },
        "page_content": ":} \\hspace{2.84544pt} & = f_{\\text{de}}() = h(+ ) \\nonumber \\\\\n\\text{Testing:} \\hspace{2.84544pt} & = f_\\text{de}^{-1}() = ^\\top (h^{-1}() - ) \\nonumber $$   (Eq. 17)  As the decoder is easily invertible,",
        "type": "Document"
      },
      {
        "id": "a1fabfef-76b3-4e41-ac89-66abf8af3ffa",
        "metadata": {
          "vector_store_key": "2002.02492-2",
          "chunk_id": 23,
          "document_id": "2002.02492",
          "start_idx": 8334,
          "end_idx": 8695
        },
        "page_content": "\\,|\\,y_{<t},C)$ for all $i < j$, and define where $V_{\\mu } = \\left\\lbrace v_1, \\cdots , v_{k_\\mu } \\right\\rbrace $ with The other family consists of deterministic decoding algorithms, where a token is selected deterministically according to a rule at each decoding step. The most naive algorithm, called greedy decoding, simply takes the most probable token at",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the domains covered in the dataset?",
    "llm_answer": "17 domains (Alarm domain not included in training), 20 domains across train, dev and test datasets.\n",
    "context": [
      {
        "id": "184ae66f-21b7-4546-b8c2-7fd69bfd9f1c",
        "metadata": {
          "vector_store_key": "1909.03242-3",
          "chunk_id": 19,
          "document_id": "1909.03242",
          "start_idx": 7530,
          "end_idx": 7943
        },
        "page_content": " of instances, as well as labels per domain, are shown in Table TABREF34 and label names in Table TABREF43 in the appendix. The dataset is split into a training part (80%) and a development and testing part (10% each) in a label-stratified manner. Note that the domains vary in the number of labels, ranging from 2 to 27. Labels include both straight-forward ratings of veracity (`correct', `incorrect'), but also",
        "type": "Document"
      },
      {
        "id": "06a8582f-cc3a-48e8-95fe-cca8b57ae377",
        "metadata": {
          "vector_store_key": "1612.06897-4",
          "chunk_id": 39,
          "document_id": "1612.06897",
          "start_idx": 12548,
          "end_idx": 13016
        },
        "page_content": "domain data. Figure 2 shows the learning curve of the ensemble of the baseline and the continue model for different sizes of in-domain training data. The used in-domain data is a randomly selected subset of the entire pool of the in-domain data available to us. We also report the result when all of the in-domain data in the pool is used. As shown in Figure 2 the translation quality of the out-of-domain test set only degrades slightly for all the different sizes of",
        "type": "Document"
      },
      {
        "id": "fbcb265a-0153-4ead-bc9a-f5d2879a4be3",
        "metadata": {
          "vector_store_key": "1909.05855-6",
          "chunk_id": 14,
          "document_id": "1909.05855",
          "start_idx": 5622,
          "end_idx": 6170
        },
        "page_content": " of multiple services per domain, are representative of these scale-related challenges. Furthermore, our evaluation sets contain many services, and consequently slots, which are not present in the training set, to help evaluate model performance on unseen services. The 17 domains (`Alarm' domain not included in training) present in our dataset are listed in Table TABREF5. We create synthetic implementations of a total of 34 services or APIs over these domains. Our simulator framework interacts with these services to generate dialogue outlines",
        "type": "Document"
      },
      {
        "id": "896d120c-d921-4200-be03-d5842da38d1e",
        "metadata": {
          "vector_store_key": "2002.01359-3",
          "chunk_id": 29,
          "document_id": "2002.01359",
          "start_idx": 12314,
          "end_idx": 12843
        },
        "page_content": " common approach utilized in other datasets BIBREF0. It has been shown that such datasets suffer from substantial annotation errors BIBREF18. Thirdly, using a simulator greatly simplifies the data collection task and instructions as only paraphrasing is needed to achieve a natural dialogue. This is particularly important for creating a large dataset spanning multiple domains. The 20 domains present across the train, dev and test datasets are listed in Table TABREF10, as are the details regarding which domains are present in",
        "type": "Document"
      },
      {
        "id": "e55a4ebd-88a8-4f4d-abe1-49dcc754f18d",
        "metadata": {
          "vector_store_key": "1909.02265-3",
          "chunk_id": 40,
          "document_id": "1909.02265",
          "start_idx": 15863,
          "end_idx": 16366
        },
        "page_content": " mixed-domain datasets through two series of experiments. These results give some useful insights in combining the approaches to improve the performance of a commercial chatbot platform which is under active development in our company. We plan to extend this current research and integrate its fruitful results into a future version of the platform. The following is three example dialogues generated by our system. The first dialogue is in single-domain.  The next two dialogues are in mixed-domains.  ",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are the two different models trained?",
    "llm_answer": "One model is trained first for coarse-grained features, freezing the parameters of the other.  Then, the other model is unfrozen, and the entire model is trained at a low learning rate for fine-grained features.  Another method involves fine-tuning the BERT model with a new prediction layer first, using a low learning rate.  Both models use Stochastic Gradient Descent with ADAM optimization and a learning rate of 1e-4, along with dropout regularization.\n",
    "context": [
      {
        "id": "e4f27389-26f3-4e80-9021-b323dd0338b3",
        "metadata": {
          "vector_store_key": "1908.06606-4",
          "chunk_id": 42,
          "document_id": "1908.06606",
          "start_idx": 14712,
          "end_idx": 15209
        },
        "page_content": " One is trained at first for coarse-graind features while freezing the parameter of the other. Then unfreeze the other one and train the entire model in a low learning rate for fetching fine-grained features. Inspired by this and due to the large amount of parameters in BERT model, to speed up the training process, we fine tune the BERT model with new prediction layer first to achieve a better contextualized representation performance. Then we deploy the proposed model and load the fine tuned",
        "type": "Document"
      },
      {
        "id": "8af7818b-bcb2-4c40-98ec-b2c855e5bf3d",
        "metadata": {
          "vector_store_key": "1902.10525-0",
          "chunk_id": 88,
          "document_id": "1902.10525",
          "start_idx": 33593,
          "end_idx": 34116
        },
        "page_content": " ). The new architecture performs between 20%\u201340% (relative) better over almost all languages. To understand how the different datasets relate to each other, we performed a set of experiments and evaluations with the goal of better characterizing the differences between the datasets. We trained a recognizer on each of the three training sets separately, then evaluated each system on all three test sets (Table TABREF65 ). The neural network architecture is the same as the one we determined earlier (5 layers bidirection",
        "type": "Document"
      },
      {
        "id": "dcb2ad75-e3d2-4819-adb7-4c81c67db732",
        "metadata": {
          "vector_store_key": "1910.08210-5",
          "chunk_id": 68,
          "document_id": "1910.08210",
          "start_idx": 25978,
          "end_idx": 26510
        },
        "page_content": " TABREF50 shows the three different settings. For each, we study the transfer behaviour of models on new environments. Figure FIGREF52 shows the learning curve when training a model on the held-out environments directly and when transferring the model trained on train environments to held-out environments. We observe that all models are significantly more sample-efficient when transferring from training environments, despite the introduction of new edges and new nodes. In Figure FIGREF51, we see that the FiLM model outperforms",
        "type": "Document"
      },
      {
        "id": "df1ca8b7-ecc2-4762-aa53-317359e0fbc6",
        "metadata": {
          "vector_store_key": "2002.05829-1",
          "chunk_id": 31,
          "document_id": "2002.05829",
          "start_idx": 12450,
          "end_idx": 12894
        },
        "page_content": " model converges and can reach expected performance as soon as possible with parameter searching. As shown in Figure FIGREF15, the fine-tuning performance curve differs a lot among pretrained models. The x-axis denoting time consumed is shown in log-scale for better comparison of different models. None of the models acutally take the lead in all tasks. However, if two pretrained models are in the same family, such as BERTBASE and BERTLARGE,",
        "type": "Document"
      },
      {
        "id": "b34bd58d-6789-4f17-8573-75e40897f7e2",
        "metadata": {
          "vector_store_key": "1910.11949-7",
          "chunk_id": 27,
          "document_id": "1910.11949",
          "start_idx": 10675,
          "end_idx": 11085
        },
        "page_content": " Both models are trained using Stochastic Gradient Descent with ADAM optimization BIBREF28 and a learning rate of 1e-4. Besides, we use dropout regularization BIBREF29 which prevents from over-fitting by dropping some units of the network. The VQG encoder is composed of 2048 neuron cells, while the VQG decoder has an attention layer of 512 followed by an embedding layer of 512 and a LSTM with the same size.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How long is the dataset?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "2e1622b2-1517-4239-8581-693f3aece329",
        "metadata": {
          "vector_store_key": "1711.02013-3",
          "chunk_id": 55,
          "document_id": "1711.02013",
          "start_idx": 20658,
          "end_idx": 21116
        },
        "page_content": "ings, between recurrent layers, and on recurrent states were (0.7, 0.5, 0.5) respectively. dataset contains 17M training tokens and has a vocabulary size of 44k words. The dataset is partitioned into a training set (first 99M characters) and a development set (last 1M characters) that is used to report performance. As this dataset contains various articles from Wikipedia, the longer term information (such as current topic) plays a bigger role than in the",
        "type": "Document"
      },
      {
        "id": "0723454a-4062-463e-972a-81a58c6a504e",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      },
      {
        "id": "9270562a-8e53-4e1d-b760-74d8fd0bc872",
        "metadata": {
          "vector_store_key": "1905.06566-7",
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "a2c17c40-d003-46d0-ba08-8966b42e8e23",
        "metadata": {
          "vector_store_key": "1906.03538-7",
          "chunk_id": 31,
          "document_id": "1906.03538",
          "start_idx": 12505,
          "end_idx": 12830
        },
        "page_content": ". We now provide a brief summary of [wave]390P[wave]415e[wave]440r[wave]465s[wave]485p[wave]525e[wave]535c[wave]595t[wave]610r[wave]635u[wave]660m. The dataset contains about INLINEFORM0 claims with a significant length diversity (Table TABREF19 ). Additionally, the dataset comes with INLINEFORM1 perspectives, most of which",
        "type": "Document"
      },
      {
        "id": "0d593f74-7474-41cc-9928-ce68ac96f714",
        "metadata": {
          "vector_store_key": "1802.05574-2",
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big are negative effects of proposed techniques on high-resource tasks?",
    "llm_answer": "The context states performance was comparable on high-resource tasks.  No negative effects are mentioned.\n",
    "context": [
      {
        "id": "8c3ea0e6-6305-4ec4-bd68-b0986047a3e4",
        "metadata": {
          "vector_store_key": "1909.06434-0",
          "chunk_id": 31,
          "document_id": "1909.06434",
          "start_idx": 11976,
          "end_idx": 12470
        },
        "page_content": " the optimization process. Compared to single-task baselines, performance improved on the low-resource En-De task and was comparable on high-resource En-Fr task. For future work, in order to increase the utility of adaptive schedulers, it would be beneficial to explore their use on a much larger number of simultaneous tasks. In this scenario, they may prove more useful as hyper-parameter search over fixed schedules would become cumbersome. In this appendix, we present the impact of various",
        "type": "Document"
      },
      {
        "id": "0a3b181b-177d-4c5b-a442-587d8e696c04",
        "metadata": {
          "vector_store_key": "1912.09152-5",
          "chunk_id": 73,
          "document_id": "1912.09152",
          "start_idx": 25875,
          "end_idx": 26391
        },
        "page_content": "design system can outperform a ner tagger for biomedical entities based on state-of-the-art artificial neural network technology. Thus, knowledge-based methods stand on their own merits in task resolution. But, perhaps most importantly, the other key point brought to light in this contribution is that a resource-based approach also favours a more critical stance on the dataset(s) used to evaluate system performance. With these methods, system development can go hand in hand with dataset refinement in a virtuous",
        "type": "Document"
      },
      {
        "id": "1c433e33-8148-4309-82c5-9b3e3f2ac9eb",
        "metadata": {
          "vector_store_key": "1707.06939-3",
          "chunk_id": 5,
          "document_id": "1707.06939",
          "start_idx": 2233,
          "end_idx": 2805
        },
        "page_content": " population of workers. Our results indicate that the AUI treatment did not affect quality, and did not help workers perform more quickly or achieve greater response consensus. Instead, workers with the AUI were significantly slower and their responses were more diverse than workers in the non-AUI control group. An important goal of crowdsourcing research is achieving efficient scalability of the crowd to very large sets of tasks. Efficiency in crowdsourcing manifests both in receiving more effective information per worker and in making individual workers faster and",
        "type": "Document"
      },
      {
        "id": "ce881912-da18-448f-9be6-915e352583f5",
        "metadata": {
          "vector_store_key": "2002.05829-3",
          "chunk_id": 4,
          "document_id": "2002.05829",
          "start_idx": 1602,
          "end_idx": 2139
        },
        "page_content": " comprehension task SQuAD and machine translation tasks. However, there has not been a concrete or practical reference for accurate estimation on NLP model pretraining, fine-tunning and inference considering multi-task energy efficiency. Energy efficiency can be reflected in many metrics including carbon emission, electricity usage, time consumption, number of parameters and FPO as shown in BIBREF10. Carbon emission and electricity are intuitive measures yet either hard to track or hardware-dependent. Number of parameteres does not",
        "type": "Document"
      },
      {
        "id": "5153c35c-c530-4300-a1da-d6e8163a0eec",
        "metadata": {
          "vector_store_key": "1707.03764-0",
          "chunk_id": 41,
          "document_id": "1707.03764",
          "start_idx": 15735,
          "end_idx": 16277
        },
        "page_content": "performing out of the 22 systems submitted in this shared task. Using additional training data, `smart' features, and hand-crafted resources hurts rather than helps performance. A possible lesson to take from this would be that manually crafting features serves only to hinder a machine learning algorithm's ability to find patterns in a dataset, and perhaps it is better to focus one's efforts on parameter optimisation instead of feature engineering. However, we believe that this is too strong a conclusion to draw from this limited study,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Are this techniques used in training multilingual models, on what languages?",
    "llm_answer": "English, Russian, Arabic, Chinese, German, Spanish, French, and Asturian (using Spanish as a source).\n",
    "context": [
      {
        "id": "1cbb6af6-a105-466f-9dcb-276ca1f0287f",
        "metadata": {
          "vector_store_key": "1809.04686-1",
          "chunk_id": 4,
          "document_id": "1809.04686",
          "start_idx": 1578,
          "end_idx": 2066
        },
        "page_content": " or multilingual NLP, the task of transferring knowledge from one language to another, serves as a good test bed for evaluating various transfer learning approaches. For cross-lingual NLP, the most widely studied approach is to use multilingual embeddings as features in neural network models. However, research has shown that representations learned in context are more effective BIBREF5 , BIBREF6 ; therefore, we aim at doing better than just using multilingual embeddings in the cross-",
        "type": "Document"
      },
      {
        "id": "51bd57fd-5cf8-4e72-a219-541eb5b26d2d",
        "metadata": {
          "vector_store_key": "1809.08510-5",
          "chunk_id": 47,
          "document_id": "1809.08510",
          "start_idx": 17158,
          "end_idx": 17644
        },
        "page_content": ", Russian, Arabic, Chinese, German, Spanish, French. We maintain the same procedure as described above. The hidden size of the language model was increased to 1024 with 16K BPE tokens being used. The first model was trained on English Russian, second was trained on English Russian Arabic and so on. For arabic we still trained from left to right even though naturally the language is read from right to left. We report the results in Figure 5 . As the number of languages increases the",
        "type": "Document"
      },
      {
        "id": "94b7dfa9-8286-4dfc-a97f-25dec6bd36cd",
        "metadata": {
          "vector_store_key": "1911.03894-6",
          "chunk_id": 12,
          "document_id": "1911.03894",
          "start_idx": 4523,
          "end_idx": 4978
        },
        "page_content": " have been released: ELMo models for Japanese, Portuguese, German and Basque and BERT for Simplified and Traditional Chinese and German. However, to the best of our knowledge, no particular effort has been made toward training models for languages other than English, at a scale similar to the latest English models (e.g. RoBERTa trained on more than 100GB of data). Our approach is based on RoBERTa BIBREF9, which replicates and improves the initial BERT",
        "type": "Document"
      },
      {
        "id": "df131e48-331f-45fd-b2ca-622b9ba6dd49",
        "metadata": {
          "vector_store_key": "1910.11493-1",
          "chunk_id": 8,
          "document_id": "1910.11493",
          "start_idx": 3314,
          "end_idx": 3804
        },
        "page_content": " that models be trained by fine-tuning. Joint modeling or any number of methods may be explored instead. The model will have access to type-level data in a low-resource target language, plus a high-resource source language. We give an example here of Asturian as the target language with Spanish as the source language.  We score the output of each system in terms of its predictions' exact-match accuracy and the average Levenshtein distance between the predictions and their corresponding",
        "type": "Document"
      },
      {
        "id": "dfc3db32-fd8b-4fbe-ae34-0fc06b8deda6",
        "metadata": {
          "vector_store_key": "1911.13066-2",
          "chunk_id": 4,
          "document_id": "1911.13066",
          "start_idx": 1723,
          "end_idx": 2230
        },
        "page_content": " many NLP tasks involving multilingual text. A Convolutional Neural Network (CNN) based model for sentiment classification of a multilingual dataset was proposed in BIBREF3. However, a particular record in the dataset belonged to one language only. In our case, a record can have either one or two languages. There is very little published work on this specific setting. One way to classify bilingual text is to normalize the different variations of a word to a standard spelling before training the model B",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What metric is used to measure performance?",
    "llm_answer": "BIBREF16, accuracy, precision, recall, F1-score.\n",
    "context": [
      {
        "id": "34da022a-5bec-4beb-bd76-14370cf952e1",
        "metadata": {
          "vector_store_key": "2002.05058-1",
          "chunk_id": 69,
          "document_id": "2002.05058",
          "start_idx": 27517,
          "end_idx": 28041
        },
        "page_content": " evaluation is then employed to identify the best choice. We evaluate the performance of each metric by how many times (out of 10) they succeeded in selecting the best hyperparameter combination or early-stopping checkpoint (out of 4) and the average human-annotated score for their selected models. The results are shown in Table 3. We can see that conventional automated metrics perform poorly and result in sub-optimal result when performing hyperparameter search and selecting the best performing checkpoints. Converting",
        "type": "Document"
      },
      {
        "id": "41dafdc8-0a0b-4598-93e1-e7526e9e86af",
        "metadata": {
          "vector_store_key": "1912.01046-7",
          "chunk_id": 36,
          "document_id": "1912.01046",
          "start_idx": 14275,
          "end_idx": 14679
        },
        "page_content": " BIBREF16, which measures how far away the predicted span is from the gold standard span, as a metric. The rationale behind the metric is that, in practice, it suffices to recommend a rough span which contains the answer \u2013 a difference of a few seconds would not matter much to the user. Specifically, the predicted span is counted as correct if $|pred_{start} - gt_{start}| + |pred_{end} - gt_{end}| <=$",
        "type": "Document"
      },
      {
        "id": "b314840d-73c6-45e5-8f7a-829f1a9eea09",
        "metadata": {
          "vector_store_key": "1911.13066-4",
          "chunk_id": 37,
          "document_id": "1911.13066",
          "start_idx": 14008,
          "end_idx": 14543
        },
        "page_content": " the standard metrics that are widely adapted in the literature for measuring multi-class classification performance. These metrics are accuracy, precision, recall, and F1-score, where latter three can be computed using micro-average or macro-average strategies BIBREF15. In micro-average strategy, each instance holds equal weight and outcomes are aggregated across all classes to compute a particular metric. This essentially means that the outcome would be influenced by the frequent class, if class distribution is skewed. In macro",
        "type": "Document"
      },
      {
        "id": "44492013-ea6a-4d0d-9984-8b621a88cebb",
        "metadata": {
          "vector_store_key": "1906.03538-4",
          "chunk_id": 45,
          "document_id": "1906.03538",
          "start_idx": 17710,
          "end_idx": 18208
        },
        "page_content": "6 and INLINEFORM7 , respectively. The metrics are averaged across all the perspectives INLINEFORM8 participating in the test set. The goal is to get estimates of the overall performance of the systems. Instead of creating a complex measure that would take all the aspects into account, we approximate the overall performance by multiplying the disjoint measures in INLINEFORM0 , INLINEFORM1 and INLINEFORM2 . While this gives an estimate on the overall quality, it ignores the pipeline structure of",
        "type": "Document"
      },
      {
        "id": "9be7f7cc-0715-4e93-a5c2-8e42fafca7ad",
        "metadata": {
          "vector_store_key": "1804.08186-0",
          "chunk_id": 280,
          "document_id": "1804.08186",
          "start_idx": 100985,
          "end_idx": 101460
        },
        "page_content": "score or F-measure) was developed in IR to measure the effectiveness of retrieval with respect to a user who attaches different relative importance to precision and recall BIBREF376 . When used as an evaluation metric for classification tasks, it is common to place equal weight on precision and recall (hence \u201cF1\u201d-score, in reference to the INLINEFORM1 hyper-parameter, which equally weights precision and recall when INLINEFORM2 ). In addition to evaluating performance for",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How do Zipf and Herdan-Heap's laws differ?",
    "llm_answer": "The provided text states that Heaps-Herdan law and Zipf's law are consequences of Zipf's law and describe the frequency of words, but not their order.  Therefore, the difference is not explicitly described.\n",
    "context": [
      {
        "id": "9c2347e6-0881-4e39-8181-da269524f082",
        "metadata": {
          "vector_store_key": "1911.08915-8",
          "chunk_id": 2,
          "document_id": "1911.08915",
          "start_idx": 637,
          "end_idx": 1041
        },
        "page_content": " as linguistsBIBREF12, BIBREF13, BIBREF14. Indeed, a large amount of effort has been devoted to try to understand the origin of Zipf's law, in some cases arguing that it arises from the fact that texts carry information BIBREF15, all the way to arguing that it is the result of mere chance BIBREF16, BIBREF17. Another interesting characterization of texts is the Heaps-Herdan law, which describes how the",
        "type": "Document"
      },
      {
        "id": "56b43130-6e62-4dcd-94cb-d819f3720ef9",
        "metadata": {
          "vector_store_key": "1911.08915-8",
          "chunk_id": 47,
          "document_id": "1911.08915",
          "start_idx": 16743,
          "end_idx": 17225
        },
        "page_content": " accurately as it describes texts written in real languages. Further, Heaps-Herdan law and the degree distribution of the adjacency network, appear to be consequences of Zipf's law, and are, thus, as universal. In this work we studied 91 texts in seven different languages, as well as random texts constructed by randomizing the spacings between words without altering the order of the letters in the text. We find that they are all well described by the universal laws. However, we",
        "type": "Document"
      },
      {
        "id": "004ec379-af00-4d08-a5d0-121f717137a8",
        "metadata": {
          "vector_store_key": "1911.08915-8",
          "chunk_id": 46,
          "document_id": "1911.08915",
          "start_idx": 16228,
          "end_idx": 16743
        },
        "page_content": " Spanish and Turkish, similar for Spanish and French, and very different for all languages and random. Zipf's law is one of the most universal statistics of natural languages. However, it may be too universal. While it may not strictly apply to sequences of independent random symbols with random spacings BIBREF32, it appears to describe random texts that conserve most of the correlations between successive symbols, as accurately as it describes texts written in real languages. Further, Heaps-Herdan law and the",
        "type": "Document"
      },
      {
        "id": "355240bc-8996-4062-b485-321840b3a646",
        "metadata": {
          "vector_store_key": "1911.08915-8",
          "chunk_id": 19,
          "document_id": "1911.08915",
          "start_idx": 7331,
          "end_idx": 7689
        },
        "page_content": " plots for some of the texts, including the random texts constructed as described previously. It is clear that all the texts reproduce convincingly Zipf's law: $f(n)\\sim 1/n^\\alpha $ where $n=1,2,...N_{tot}$ is the word rank, $N_{tot}$ is the size of the vocabulary and $f(n)$ is its frequency. This is in contrast to previous work in which it is argued that",
        "type": "Document"
      },
      {
        "id": "395ee3ff-3f86-466e-92a8-d2d19aa29e3a",
        "metadata": {
          "vector_store_key": "1911.08915-8",
          "chunk_id": 29,
          "document_id": "1911.08915",
          "start_idx": 10312,
          "end_idx": 10824
        },
        "page_content": " again in close agreement with what we observe. Thus far, our results confirm that the all our texts exhibit the expected universal statistics observed in natural languages. Actually, it could be argued that these laws may be \"too universal\", not being able to clearly distinguish texts written in real languages from our random texts. Further, all these laws appear to be consequence of Zipf's law, and this law reflects only the frequency of words, not their order. Thus, all three laws would still hold if the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are the synthetic examples generated?",
    "llm_answer": "The generative model is trained on the original training dataset.  Then, the premise and label from an example are used as input to generate a new hypothesis.  This hypothesis is combined with the premise and label to create a new unseen example.  This process is repeated for every example in the original dataset.  Synthetic data is also generated using simple rules like dropping non-initial vowels and replacing consonants.\n",
    "context": [
      {
        "id": "0ce3a1e7-1796-4008-b2b0-65fae04b9b4f",
        "metadata": {
          "vector_store_key": "1909.13668-2",
          "chunk_id": 53,
          "document_id": "1909.13668",
          "start_idx": 16998,
          "end_idx": 17465
        },
        "page_content": " sentences are generated by sampling $z\\sim p(z)$ and decoding the samples into sentences. The synthetic sequences are then used to train a language model (an LSTM with the parametrisation of our decoder). The FCE score is estimated by reporting the negative log likelihood (NLL) of the trained LM on the set of human generated sentences. We generated synthetic corpora using trained models from Table TABREF12 with different C and decoding schemes and using the same",
        "type": "Document"
      },
      {
        "id": "e98dd94e-ac6e-4fa7-8199-5ecf374d3db9",
        "metadata": {
          "vector_store_key": "1607.06025-5",
          "chunk_id": 10,
          "document_id": "1607.06025",
          "start_idx": 3868,
          "end_idx": 4370
        },
        "page_content": " of generative models, which is also presented in Figure FIGREF2 . First, the generative model is trained on the original training dataset. Then, the premise and label from an example in the original dataset are taken as the input to the generative model to generate a new random hypothesis. The generated hypothesis is combined with the premise and the label to form a new unseen example. This is done for every example in the original dataset to construct a new dataset. Next, a classifier is trained",
        "type": "Document"
      },
      {
        "id": "05da1bac-9a5e-4829-aeac-277c08ed03e9",
        "metadata": {
          "vector_store_key": "1804.05868-3",
          "chunk_id": 18,
          "document_id": "1804.05868",
          "start_idx": 6795,
          "end_idx": 7281
        },
        "page_content": " (4,29,715) synthetically generated from the English vocabulary. We use simple rules such as dropping non-initial vowels and replacing consonants based on their phonological proximity to generate synthetic data for normalization. Figure \"Supplemental Material\" shows some of the noisy forms generated from standard word forms using simple and finite rules which include vowel elision (please $\\rightarrow $ pls), interchanging similar consonants and vowels (cousin $\\rightarrow $ couzin",
        "type": "Document"
      },
      {
        "id": "75143fde-282e-480b-b66d-8de5709987c7",
        "metadata": {
          "vector_store_key": "1607.06025-5",
          "chunk_id": 3,
          "document_id": "1607.06025",
          "start_idx": 1312,
          "end_idx": 1803
        },
        "page_content": " examples are presented in Table TABREF1 . The proposed generative networks are trained to generate a hypothesis given a premise and a label, which allow us to construct new, unseen examples. Some generative models are build to generate a single optimal response given the input. Such models have been applied to machine translation BIBREF5 , image caption generation BIBREF6 , or dialogue systems BIBREF7 . Another type of generative models are autoencoders that generate a stream of random",
        "type": "Document"
      },
      {
        "id": "8023e66d-7ff1-4065-abc5-15a9e4a04769",
        "metadata": {
          "vector_store_key": "1910.09399-4",
          "chunk_id": 8,
          "document_id": "1910.09399",
          "start_idx": 3211,
          "end_idx": 3731
        },
        "page_content": " (GANs) consist of two neural networks paired with a discriminator and a generator. These two models compete with one another, with the generator attempting to produce synthetic/fake samples that will fool the discriminator and the discriminator attempting to differentiate between real (genuine) and synthetic samples. Because GANs' adversarial training aims to cause generators to produce images similar to the real (training) images, GANs can naturally be used to generate synthetic images (image synthesis), and this",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does the new parser outperform the current state-of-the-art?",
    "llm_answer": "The context states the new parser is \"competitive with the state-of-the-art on PTB\" and \"gives better or comparable results\" to state-of-the-art parsers in transition and graph types, but does not quantify the difference.\n",
    "context": [
      {
        "id": "0e54a715-2936-4ee8-95ee-fe3e4bd6e78d",
        "metadata": {
          "vector_store_key": "1911.08673-0",
          "chunk_id": 67,
          "document_id": "1911.08673",
          "start_idx": 23641,
          "end_idx": 24173
        },
        "page_content": "building actions, and it also supports non-projective parsing when introducing two extra non-projective arc-building actions. Compared to graph-based and transition-based parsers, our parser achieves a better tradeoff between parsing accuracy and efficiency by taking advantages of both graph-based models' training methods and transition-based models' linear time decoding strategies. Experimental results on 28 treebanks show the effectiveness of our parser by achieving good performance on 27 treebanks, including the PTB and CTB",
        "type": "Document"
      },
      {
        "id": "ba50995b-00d1-412f-8665-466c69f6c100",
        "metadata": {
          "vector_store_key": "1911.08673-0",
          "chunk_id": 9,
          "document_id": "1911.08673",
          "start_idx": 3681,
          "end_idx": 4273
        },
        "page_content": " From the evaluation results on the benchmark treebanks, our proposed model gives significant improvements when compared to the baseline parser. In summary, our contributions are thus: $\\bullet $ We integrate the arc scoring mechanism of graph-based parsers and the linear time complexity inference approach of transition parsing models, which, by replacing stepwise local feature scoring, significantly alleviates the drawbacks of these models, improving their moderate performance caused by error propagation and increasing their training speeds resulting from their lack of parallelism. $\\",
        "type": "Document"
      },
      {
        "id": "c3be80c3-608c-4a69-8dce-98ec1f08218c",
        "metadata": {
          "vector_store_key": "1911.08673-9",
          "chunk_id": 59,
          "document_id": "1911.08673",
          "start_idx": 20643,
          "end_idx": 21073
        },
        "page_content": "based parsers, our model is also competitive with the state-of-the-art on PTB when considering the UAS metric. Compared to state-of-the-art parsers in transition and graph types, BIAF and STACKPTR, respectively, our model gives better or comparable results but with much faster training and decoding. Additionally, with the help of pre-trained language models, ELMo or BERT, our model can achieve even greater results. In order to",
        "type": "Document"
      },
      {
        "id": "2ba5d4f8-d9b4-40cf-8068-824b40d9fd0a",
        "metadata": {
          "vector_store_key": "1710.09340-2",
          "chunk_id": 27,
          "document_id": "1710.09340",
          "start_idx": 10044,
          "end_idx": 10515
        },
        "page_content": " a comparison between the Covington parser and the novel variant developed here. The NL-Covington parser outperforms the original version in all datasets tested, with all improvements statistically significant ( INLINEFORM0 ). Table TABREF12 compares our novel system with other state-of-the-art transition-based dependency parsers on the PT-SD. Greedy parsers are in the first block, beam-search and dynamic programming parsers in the second block. The third block shows",
        "type": "Document"
      },
      {
        "id": "bb598760-0443-4928-8612-ee08dabe7814",
        "metadata": {
          "vector_store_key": "1911.08673-0",
          "chunk_id": 68,
          "document_id": "1911.08673",
          "start_idx": 23969,
          "end_idx": 24082
        },
        "page_content": " effectiveness of our parser by achieving good performance on 27 treebanks, including the PTB and CTB benchmarks.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What experimental evaluation is used?",
    "llm_answer": "Cross-domain evaluation, human evaluation with criteria of relevance, attraction, fluency, and style strength, and automatic evaluation.\n",
    "context": [
      {
        "id": "b3eb1308-3e01-4903-be04-528f4258d508",
        "metadata": {
          "vector_store_key": "2002.11268-7",
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "67c7bdd9-8926-4b32-b3b7-4b1914e74192",
        "metadata": {
          "vector_store_key": "1601.02543-1",
          "chunk_id": 2,
          "document_id": "1601.02543",
          "start_idx": 845,
          "end_idx": 1419
        },
        "page_content": " successful and unsuccessful transactions. This evaluation is then used to identifying any further improvement in the speech recognition based solution to better the overall transaction completion rates. This process of evaluation is both time consuming and expensive. For evaluation one needs to identify a set of users and also identify the set of actual usage situations and perform the test. It is also important that the set of users are able to use the system with ease meaning that even in the test conditions the performance of the system, should be good, while this",
        "type": "Document"
      },
      {
        "id": "e2194e32-f011-4118-b3bf-35fb153b3017",
        "metadata": {
          "vector_store_key": "1804.08186-0",
          "chunk_id": 275,
          "document_id": "1804.08186",
          "start_idx": 98870,
          "end_idx": 99381
        },
        "page_content": " evaluation in research to date. In this section, we examine the literature more closely, providing a broad overview of the evaluation metrics that have been used, as well as the experimental settings in which research has been evaluated. The most common approach is to treat the task as a document-level classification problem. Given a set of evaluation documents, each having a known correct label from a closed set of labels (often referred to as the \u201cgold-standard\u201d), and a predicted label for each document",
        "type": "Document"
      },
      {
        "id": "60e45509-683c-4639-be3b-33b2eab42183",
        "metadata": {
          "vector_store_key": "2004.01980-7",
          "chunk_id": 55,
          "document_id": "2004.01980",
          "start_idx": 18999,
          "end_idx": 19503
        },
        "page_content": " style corpus, and the sampling strategy follows the uniform distribution with the probability being equal to $\\lambda $. The human evaluation is to have a comprehensive measurement of the performances. We conduct experiments on four criteria, relevance, attraction, fluency, and style strength. We summarize the human evaluation results on the first three criteria in Table TABREF51, and the last criteria in Table TABREF57. Note that through automatic evaluation, the baselines NST, Fine-tuned, and Gig",
        "type": "Document"
      },
      {
        "id": "9a66176f-fe2e-4da0-a2df-70a3d739924f",
        "metadata": {
          "vector_store_key": "1802.05322-2",
          "chunk_id": 38,
          "document_id": "1802.05322",
          "start_idx": 13905,
          "end_idx": 14341
        },
        "page_content": " explanation of the whole process needed to conduct an experiment like this. The process includes downloading a data set, web scraping for extra information, data preprocessing, model tuning and evaluation of the results. Action Adult Adventure Animation Biography Comedy Crime Documentary Drama Family Fantasy Film-Noir Game-Show History Horror Music Musical Mystery Reality-TV Romance Sci-Fi Short Sport Talk-Show Thriller War Western",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the architecture fault-tolerant?",
    "llm_answer": "The architecture is fault-tolerant because it is Spark-based, which acts as an in-memory data store and allows for persistent and real-time handling of large volumes of incoming data.\n",
    "context": [
      {
        "id": "f691165b-75f7-4456-b6fe-497ea7084dd1",
        "metadata": {
          "vector_store_key": "2001.02380-9",
          "chunk_id": 16,
          "document_id": "2001.02380",
          "start_idx": 6369,
          "end_idx": 6911
        },
        "page_content": ", we combine the anchoring annotation data from Section SECREF3 with the model's predictions to evaluate how `human-like' its performance is, using an information retrieval approach measuring recall@k and assessing the stability of different signal types based on how the model scores them. We develop a visualization for tokenwise signaling strength and perform error analysis for some signals found by the model which were not flagged by humans and vice versa, and point out the strengths and weaknesses of the architecture. Section SECREF6",
        "type": "Document"
      },
      {
        "id": "3262c4bf-cded-4089-8c6e-911417da25e6",
        "metadata": {
          "vector_store_key": "2003.04967-0",
          "chunk_id": 27,
          "document_id": "2003.04967",
          "start_idx": 11675,
          "end_idx": 12186
        },
        "page_content": " scalable, being 100x faster than Hadoop on large datasets, and provides out of the box libraries for both streaming and machine learning. The growth of the volume of data inspired us to opt for a big data architecture which can not only handle the prediction algorithms but also the streaming and increasing volume of data in a fault tolerant way. Figure FIGREF2 gives an overview of the architecture design. Central to this design is Apache Spark which acts as an in-memory data store and allows us to perform",
        "type": "Document"
      },
      {
        "id": "fe00c902-a86c-4c44-bf90-86d008b61c2b",
        "metadata": {
          "vector_store_key": "2003.04967-0",
          "chunk_id": 5,
          "document_id": "2003.04967",
          "start_idx": 2255,
          "end_idx": 2850
        },
        "page_content": " learning algorithms, which introduces an additional difficulty. These challenges call for learning platforms based on big data architectures that can not only handle heterogeneous volumes of data but also be fault tolerant and persistent in real time. In this paper we provide a novel real-time and adaptive cryptocurrency price prediction platform based on Twitter sentiments. The integrative and modular platform copes with the three aforementioned challenges in several ways. Firstly, it provides a Spark-based architecture which handles the large volume of incoming data in a persistent and",
        "type": "Document"
      },
      {
        "id": "57be0cd5-a3bd-41a5-aa1a-2887b6846ad0",
        "metadata": {
          "vector_store_key": "1808.09409-2",
          "chunk_id": 45,
          "document_id": "1808.09409",
          "start_idx": 17134,
          "end_idx": 17611
        },
        "page_content": "-based one. Finally, a concrete study on the instances in the output is conducted, as to validate conclusions in the previous step. We employ 6 oracle transformations designed by he2017deep to fix various prediction errors sequentially (see details in Table TABREF19 ), and observe the relative improvements after each operation, as to obtain fine-grained error types. Figure FIGREF21 compares two systems in terms of different mistakes on ENG-L2 and JPN-L2 respectively. After",
        "type": "Document"
      },
      {
        "id": "881df877-2175-4d8a-ba49-6c5a5a3119c0",
        "metadata": {
          "vector_store_key": "1808.09920-2",
          "chunk_id": 72,
          "document_id": "1808.09920",
          "start_idx": 24285,
          "end_idx": 24813
        },
        "page_content": " candidate answers or the number of nodes increases. In this section we provide an error analysis for our best single model predictions. First of all, we look at which type of questions our model performs well or poorly. There are more than 150 query types in the validation set but we filtered the three with the best and with the worst accuracy that have at least 50 supporting documents and at least 5 candidates. We show results in Table 4 . We observe that questions regarding places (birth and death) are considered harder",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which elements of the platform are modular?",
    "llm_answer": "Question answering platform, bitcoin price prediction platform, and dialog system architecture.\n",
    "context": [
      {
        "id": "877648e7-9b67-4429-bb7a-b4e3f038ce69",
        "metadata": {
          "vector_store_key": "1907.00854-0",
          "chunk_id": 37,
          "document_id": "1907.00854",
          "start_idx": 14715,
          "end_idx": 15274
        },
        "page_content": " portable because it is built on cloud native technologies (i.e., Docker and Kubernetes) and can be deployed to any cloud or on-premise environment. It is modular because it is composed of four configurable modules that collectively enable identification of questions, classification of those questions into topics, a search of knowledge base articles, and reading comprehension. Initial usage of the system indicates that it provides a flexible and developer friendly way to enable question answering functionality for multiple topics or domains via REST API",
        "type": "Document"
      },
      {
        "id": "f99af5e2-8d00-4257-961b-b24f59707b57",
        "metadata": {
          "vector_store_key": "2003.04967-0",
          "chunk_id": 7,
          "document_id": "2003.04967",
          "start_idx": 3154,
          "end_idx": 3699
        },
        "page_content": " cope with new prices and sentiments. Finally, the platform is modular and integrative in the sense that it combines these different solutions to provide novel real-time tool support for bitcoin price prediction that is more scalable, data-rich, and proactive, and can help accelerate decision-making, uncover new opportunities and provide more timely insights based on the available and ever-larger financial data volume and variety. The rest of the paper is organized as follows. Section 2 discusses the related work proposed in the literature",
        "type": "Document"
      },
      {
        "id": "ef00a6bb-45b9-4ec9-aed7-d0473d81d005",
        "metadata": {
          "vector_store_key": "1707.05853-2",
          "chunk_id": 2,
          "document_id": "1707.05853",
          "start_idx": 719,
          "end_idx": 1211
        },
        "page_content": " a modular architecture: First, an ASR system converts the spoken user utterance into text. Then, a spoken language understanding (SLU) module extracts the user's intent and coarse-grained semantic information. Next, a dialog state tracking (DST) component maintains a distribution over the state of the dialog, updating it in every turn. Given this information, the dialog policy manager decides on the next action of the system. Finally, a natural language generation (NLG) module forms the",
        "type": "Document"
      },
      {
        "id": "692e0ed1-d4ba-47e7-a034-873c65948722",
        "metadata": {
          "vector_store_key": "1603.04553-1",
          "chunk_id": 39,
          "document_id": "1603.04553",
          "start_idx": 11513,
          "end_idx": 11902
        },
        "page_content": "IBREF28 ; Latent-Tree: the latent tree model BIBREF29 obtaining the best results in the shared task; Berkeley: the Berkeley system with the final feature set BIBREF12 ; LaSO: the structured perceptron system with non-local features BIBREF30 ; Latent-Strc: the latent structure system BIBREF31 ; Model-Stack: the entity-centric system with model stacking BIBREF32 ; and Non-Linear: the non-",
        "type": "Document"
      },
      {
        "id": "1f7e477a-c648-4b9b-9be5-48670d678938",
        "metadata": {
          "vector_store_key": "1911.11744-4",
          "chunk_id": 34,
          "document_id": "1911.11744",
          "start_idx": 13205,
          "end_idx": 13620
        },
        "page_content": " stochastic network outputs to get a distribution over the belief. The modularity of our architecture allows us to easily exchange parts of the network. This can be utilized for transfer learning between different tasks in the semantic network or transfer between different robots by transferring the policy translation network to different robots in simulation, or to bridge the gap between simulation and reality.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Could you tell me more about the metrics used for performance evaluation?",
    "llm_answer": "The context describes various evaluation metrics, including: recall rate, F1 score, overall accuracy, task completion ratio, user satisfaction degree (with scores ranging from -2 to 2), response fluency (with scores of -1, 0, 1), and number of dialogue turns.  Performance of automated metrics was evaluated by success rate in selecting the best hyperparameter combination or early-stopping checkpoint, and average human-annotated score.\n",
    "context": [
      {
        "id": "34da022a-5bec-4beb-bd76-14370cf952e1",
        "metadata": {
          "vector_store_key": "2002.05058-1",
          "chunk_id": 69,
          "document_id": "2002.05058",
          "start_idx": 27517,
          "end_idx": 28041
        },
        "page_content": " evaluation is then employed to identify the best choice. We evaluate the performance of each metric by how many times (out of 10) they succeeded in selecting the best hyperparameter combination or early-stopping checkpoint (out of 4) and the average human-annotated score for their selected models. The results are shown in Table 3. We can see that conventional automated metrics perform poorly and result in sub-optimal result when performing hyperparameter search and selecting the best performing checkpoints. Converting",
        "type": "Document"
      },
      {
        "id": "62357a4c-d387-42b6-9ded-cfec39e88f26",
        "metadata": {
          "vector_store_key": "2002.05058-1",
          "chunk_id": 68,
          "document_id": "2002.05058",
          "start_idx": 26990,
          "end_idx": 27517
        },
        "page_content": " NLG evaluation. We further investigate the impact of imperfect metrics on training NLG models. As described in the human evaluation procedure, we perform 10 runs to test the reliability of each metric when used to perform hyperparameter tuning and early-stopping respectively. In each run, we select the best hyperparameter combination or early-stopping checkpoint based on each of the five compared metrics. Human evaluation is then employed to identify the best choice. We evaluate the performance of each metric by how many",
        "type": "Document"
      },
      {
        "id": "44492013-ea6a-4d0d-9984-8b621a88cebb",
        "metadata": {
          "vector_store_key": "1906.03538-4",
          "chunk_id": 45,
          "document_id": "1906.03538",
          "start_idx": 17710,
          "end_idx": 18208
        },
        "page_content": "6 and INLINEFORM7 , respectively. The metrics are averaged across all the perspectives INLINEFORM8 participating in the test set. The goal is to get estimates of the overall performance of the systems. Instead of creating a complex measure that would take all the aspects into account, we approximate the overall performance by multiplying the disjoint measures in INLINEFORM0 , INLINEFORM1 and INLINEFORM2 . While this gives an estimate on the overall quality, it ignores the pipeline structure of",
        "type": "Document"
      },
      {
        "id": "638d3f9b-929b-4fa6-b0d4-efe44cd66255",
        "metadata": {
          "vector_store_key": "1912.13109-2",
          "chunk_id": 25,
          "document_id": "1912.13109",
          "start_idx": 10203,
          "end_idx": 10681
        },
        "page_content": " metrics - Recall rate, F1 score and Overall accuracy. Choice of model parameters were in the above models were inspired from previous work done but then were tuned to the best performance of the Test dataset. Following parameters were considered for tuning. Learning rate: Based on grid search the best performance was achieved when learning rate was set to 0.01. This value was arrived by a grid search on lr parameter. Number of Bidirectional LSTM units: A set of 32, 64, 128",
        "type": "Document"
      },
      {
        "id": "4786191e-fed1-4406-89b6-7b8835719e5f",
        "metadata": {
          "vector_store_key": "1709.10217-2",
          "chunk_id": 18,
          "document_id": "1709.10217",
          "start_idx": 6394,
          "end_idx": 6850
        },
        "page_content": " evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the number of total tasks. User satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied, neutral, satisfied and very satisfied, respectively. Response fluency: There are three scores -1, 0, 1, which indicate nonfluency, neutral, fluency. Number of dialogue turns: The number of utterances in a task",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the tasks that this method has shown improvements?",
    "llm_answer": "The context does not specify the tasks the method improves upon.  It only mentions evaluation using cross-domain scenarios for establishing general effectiveness.\n",
    "context": [
      {
        "id": "b3eb1308-3e01-4903-be04-528f4258d508",
        "metadata": {
          "vector_store_key": "2002.11268-7",
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "cbe9f208-3730-4128-8f81-ac3a378ea889",
        "metadata": {
          "vector_store_key": "1707.03764-0",
          "chunk_id": 16,
          "document_id": "1707.03764",
          "start_idx": 6168,
          "end_idx": 6712
        },
        "page_content": " out to be detrimental to performance, when compared to the simpler system that we have described in Section SECREF2 . We divide our attempts according to the different ways we attempted to enhance performance: manipulating the data itself (adding more, and changing preprocessing), using a large variety of features, and changing strategies in modelling the problem by using different algorithms and paradigms. All reported results are on the PAN 2017 training data using five-fold cross-validation, unless otherwise specified. We extended the",
        "type": "Document"
      },
      {
        "id": "b399f64c-63e5-4e97-a851-965f9e036093",
        "metadata": {
          "vector_store_key": "2001.07263-7",
          "chunk_id": 35,
          "document_id": "2001.07263",
          "start_idx": 13259,
          "end_idx": 13753
        },
        "page_content": " Sorting the results by decreasing number of absolute errors on Hub5'00, Table TABREF7 indicates that each regularization method contributes to the improved WER. SpecAugment is by far the most important method, while using $\\Delta $ and $\\Delta \\Delta $ features or switching off the curriculum learning in the later stage of training have marginal but positive effects. Other direct input level perturbation steps (speed/tempo perturbation and sequence noise injection) are also key techniques",
        "type": "Document"
      },
      {
        "id": "fd3bbd76-414c-4bda-b471-807bcd4c762c",
        "metadata": {
          "vector_store_key": "1701.09123-8",
          "chunk_id": 14,
          "document_id": "1701.09123",
          "start_idx": 5380,
          "end_idx": 5887
        },
        "page_content": " a clear methodology for choosing and combining them. Third, we demonstrate that our system still obtains very competitive results even when the supervised data is reduced by half (even less in some cases), alleviating the dependency on costly hand annotated data. These three main contributions are based on: A simple and shallow robust set of features across languages and datasets, even in out-of-domain evaluations. The lack of linguistic motivated features, even for languages with agglutinative (e.g.,",
        "type": "Document"
      },
      {
        "id": "5ff5222e-22ba-4783-bf73-5f82b021a5b9",
        "metadata": {
          "vector_store_key": "1912.09152-0",
          "chunk_id": 74,
          "document_id": "1912.09152",
          "start_idx": 26186,
          "end_idx": 26669
        },
        "page_content": " system performance. With these methods, system development can go hand in hand with dataset refinement in a virtuous circle that let us think that maybe next time we are planning to add a new gazetteer or word embedding to our system in order to try to improve system performance, we should first look at our data and, like King Midas, turn our Human Annotated Dataset into a true Gold Standard Dataset. We thank three anonymous reviewers of our manuscript for their careful reading",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Why does the model improve in monolingual spaces as well? ",
    "llm_answer": "The context does not explain why the model improves in monolingual spaces.\n",
    "context": [
      {
        "id": "259d0539-f533-49c5-9cb2-a64908fda471",
        "metadata": {
          "vector_store_key": "1810.03459-3",
          "chunk_id": 17,
          "document_id": "1810.03459",
          "start_idx": 6376,
          "end_idx": 6908
        },
        "page_content": "ization across languages. Languages with limited data when trained with other languages allows them to be robust and helps in improving the recognition performance. In spite of being simple, the model has limitations in keeping the target language data unseen during training. Table TABREF16 shows the recognition performance of naive multilingual approach using BLSTMP and VGG model against a monolingual model trained with BLSTMP. The results clearly indicate that having a better architecture such as VGG-BLSTM helps in improving",
        "type": "Document"
      },
      {
        "id": "9d387c2c-4c55-41cc-ba62-84ac5a00fe57",
        "metadata": {
          "vector_store_key": "1710.09589-4",
          "chunk_id": 29,
          "document_id": "1710.09589",
          "start_idx": 11047,
          "end_idx": 11545
        },
        "page_content": " multilingual model shows promising, overall (macro average) outperforming the single language-specific models. However, the multilingual model does not consistently fare better than single models, for example on French a monolingual model would be more beneficial. Adding POS tags did not help (cf. Table TABREF31 ), actually dropped performance. We disregard this feature for the final official runs. We trained the final models on the concatenation of Train and Dev data. The results on the test",
        "type": "Document"
      },
      {
        "id": "e9c85a76-ea3c-4f25-9285-929cc7c228a1",
        "metadata": {
          "vector_store_key": "1710.09589-4",
          "chunk_id": 31,
          "document_id": "1710.09589",
          "start_idx": 11811,
          "end_idx": 12338
        },
        "page_content": " models on the two languages (Spanish and Japanese) which have the least amount of training data. Overall results between the monolingual and multilingual model are close, but the advantage of our multilingual All-in-1 approach is that it is a single model that can be applied to all four languages. Second, automatic translation harms, the performance of the EN model on the translated data is substantially lower than the respective in-language model. We could investigate this as the organizers provided us with translations",
        "type": "Document"
      },
      {
        "id": "bfabd9c2-75ee-4ceb-8bfa-b890b0511dbf",
        "metadata": {
          "vector_store_key": "1710.09589-4",
          "chunk_id": 28,
          "document_id": "1710.09589",
          "start_idx": 10557,
          "end_idx": 11047
        },
        "page_content": " performance of the character+embedding model. However, the bilingual models are still only able to handle pairs of languages. We therefore mapped all embeddings to a common space and train a single multilingual All-in-1 model on the union of all training data. This is the second model that we submitted to the shared task. As we can see from the development data, on average the multilingual model shows promising, overall (macro average) outperforming the single language-specific models",
        "type": "Document"
      },
      {
        "id": "7f2738a8-9d5a-449f-a096-666cfd62ba7a",
        "metadata": {
          "vector_store_key": "1810.03459-2",
          "chunk_id": 18,
          "document_id": "1810.03459",
          "start_idx": 6686,
          "end_idx": 7098
        },
        "page_content": " The results clearly indicate that having a better architecture such as VGG-BLSTM helps in improving multilingual performance. Except Pashto, Georgian and Tokpisin, the multilingual VGG-BLSTM model gave 8.8 % absolute gain in average over monolingual model. In case of multilingual BLSTMP, except Pashto and Georgian an absolute gain of 5.0 % in average is observed over monolingual model. Even though the VGG-BL",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is annotation projection done when languages have different word order?",
    "llm_answer": "The context states that the approach projects target-language word embeddings to the source-language embedding space preserving word order.  This is expected to work better for languages with similar word order to the source language (English, SVO).\n",
    "context": [
      {
        "id": "e7003dc7-f174-43b3-b750-34e756cc8d7c",
        "metadata": {
          "vector_store_key": "1911.00069-4",
          "chunk_id": 76,
          "document_id": "1911.00069",
          "start_idx": 24530,
          "end_idx": 24998
        },
        "page_content": ".9 $F_1$ points, except for Arabic. Since our approach projects the target-language word embeddings to the source-language embedding space preserving the word order, it is expected to work better for a target language that has more similar word order as the source language. This has been verified by our experiments. The source language, English, belongs to the SVO (Subject, Verb, Object) language family where in a sentence the subject comes first, the verb second,",
        "type": "Document"
      },
      {
        "id": "f1d78584-356c-4141-9a77-85caa5331c3a",
        "metadata": {
          "vector_store_key": "1704.04539-6",
          "chunk_id": 35,
          "document_id": "1704.04539",
          "start_idx": 13333,
          "end_idx": 13834
        },
        "page_content": " noisy word alignments in annotation projection: the parser learns what words are likely to trigger a node (or a set of nodes) in the AMR by looking at their AMR alignments (which are induced by the word alignments). If an important word consistently remains unaligned, the parser will erroneously learn to discard it. More accurate alignments are therefore crucial in order to achieve better parsing results. We computed the percentage of words in the training data that are learned to be non-content",
        "type": "Document"
      },
      {
        "id": "a0e50662-b18b-408c-89ec-da91125d7391",
        "metadata": {
          "vector_store_key": "1904.09708-3",
          "chunk_id": 27,
          "document_id": "1904.09708",
          "start_idx": 10045,
          "end_idx": 10321
        },
        "page_content": "}$ . The syntactic information (or \"annotations\" BIBREF12 ) of each word $x_j$ is determined by the two vectors $\\overrightarrow{h_{j-1}}$ , $\\overleftarrow{h_{j+1}}$ corresponding to the words surrounding it:  $$h_j = [\\overrightarrow{h_{j-1}};\\overleftarrow{h_{j+1}}]$$   (E",
        "type": "Document"
      },
      {
        "id": "3895ac78-b491-4e19-8650-55d008035878",
        "metadata": {
          "vector_store_key": "1704.04539-5",
          "chunk_id": 12,
          "document_id": "1704.04539",
          "start_idx": 4890,
          "end_idx": 5371
        },
        "page_content": " We think of English labels for the graph nodes as ones from an independent language, which incidentally looks similar to English. However, in order to train state-of-the-art AMR parsers, we also need to project the alignments between AMR nodes and words in the sentence (henceforth called AMR alignments). We use word alignments, similarly to other annotation projection work, to project the AMR alignments to the target languages. Our approach depends on an underlying assumption",
        "type": "Document"
      },
      {
        "id": "15f70939-1a06-47c8-9270-5a52ba1532a1",
        "metadata": {
          "vector_store_key": "1612.06897-1",
          "chunk_id": 42,
          "document_id": "1612.06897",
          "start_idx": 13614,
          "end_idx": 14122
        },
        "page_content": " annotator to judge the translation output of different systems on a randomly selected in-domain sample of 50 sentences (maximum sentence length 50). Each source sentence is presented to the annotator with all 3 different translations (baseline/ continue/ ensemble). The translation are presented in a blind fashion (i.e., the annotator is not aware of which system is which) and shuffled in random order. The evaluation is presented to the annotator via a web-page interface with all these translation pairs",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What's the precision of the system?",
    "llm_answer": "Precision scores in ranges between 0.92 and 0.97 are possible.\n",
    "context": [
      {
        "id": "562931f2-fe45-41ea-91d7-50ddfb8ddc8b",
        "metadata": {
          "vector_store_key": "1909.05246-7",
          "chunk_id": 44,
          "document_id": "1909.05246",
          "start_idx": 16417,
          "end_idx": 16962
        },
        "page_content": " accuracy measures the similarity of the system generated response versus the target response. Eric and Manning eric2017copy used this metric to evaluate their systems in which they considered their response to be correct if all tokens in the system generated response matched the corresponding token in the target response. This metric is a little bit harsh, and the results may be low since all the tokens in the generated response have to be exactly in the same position as in the target response. Per-Dialogue Accuracy: We calculate per-dial",
        "type": "Document"
      },
      {
        "id": "588591da-a708-48ca-876c-78b81a6ea8a4",
        "metadata": {
          "vector_store_key": "1802.05574-1",
          "chunk_id": 34,
          "document_id": "1802.05574",
          "start_idx": 13420,
          "end_idx": 13921
        },
        "page_content": " if we have 100% inter-annotator agreement that a triple was correctly extracted we label it as correct. Table 2 show the results for the combinations of systems and data sources. The Correct Triples column contains the number of triples that are labelled as being correct by all annotators. Total Triples are the total number of triples extracted by the given systems over the specified data. Precision is calculated as typical where Correct Triples are treated as true positives. On average, 3.1 tri",
        "type": "Document"
      },
      {
        "id": "fd6b681c-fce8-4449-a2c9-adc6a9af7ee1",
        "metadata": {
          "vector_store_key": "1804.03396-5",
          "chunk_id": 81,
          "document_id": "1804.03396",
          "start_idx": 25766,
          "end_idx": 26030
        },
        "page_content": " to calculate the precision and recall under the threshold $\\delta $ . Specifically, we try two confidence scores calculated as follows: $$\\begin{split}\n{\\rm Score_{mul}} = \\prod _{t=1}^L{\\rm P}(a^t_{i_t}),~~~{\\rm Score_{avg}} = \\sum _{t=1}^L{\\rm P}(a^t_{i_t}) / L",
        "type": "Document"
      },
      {
        "id": "ff224634-5e59-4333-8bf6-50641a781ae8",
        "metadata": {
          "vector_store_key": "1911.03243-4",
          "chunk_id": 27,
          "document_id": "1911.03243",
          "start_idx": 10852,
          "end_idx": 11313
        },
        "page_content": "Bank. To examine this, we analyzed 100 false positive arguments. Only 32 of those were due to wrong or incomplete QA annotations in our gold, while most others were outside of PropBank's scope, capturing either implied arguments or roles not covered in PropBank. Extrapolating from this manual analysis estimates our true precision (on all roles) to be about 91%, which is consistent with the 88% precision figure in Table TABREF19. Compared with 2015, our QA-S",
        "type": "Document"
      },
      {
        "id": "975b0340-cb8a-4c46-87f7-07ef48e97379",
        "metadata": {
          "vector_store_key": "2001.11268-9",
          "chunk_id": 58,
          "document_id": "2001.11268",
          "start_idx": 22809,
          "end_idx": 23352
        },
        "page_content": " precision scores in ranges between 0.92 and 0.97 are possible with F1 scores not dropping below 0.84 for the main classes of interest. In practice, the detachment between model predictions and assignment of labels means that a reviewer who wishes to switch between high recall and high precision results can do so very quickly, without obtaining new predictions from the model itself. More visualizations can be found in this project's GitHub repository , including true class labels and a detailed breakdown of true and false predictions for",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which of the two ensembles yields the best performance?",
    "llm_answer": "The ensemble model using a sum of probabilities at each position yields the best performance for ROUGE-L and BLEU-1.\n",
    "context": [
      {
        "id": "6e5dd4a7-27b3-4d21-8529-fac0bc4ba1f2",
        "metadata": {
          "vector_store_key": "1901.02257-9",
          "chunk_id": 60,
          "document_id": "1901.02257",
          "start_idx": 18849,
          "end_idx": 19405
        },
        "page_content": " Our ensemble model surpasses the current state-of-the-art model with an accuracy of 84.84%. We got the final ensemble result by voting on 4 single models. Every single model uses the same architecture but different parameters. To study the effectiveness of each perspective, we conduct several experiments on the three single perspectives and their combination perspective. Table 3 presents their comparison results. The first group of models are based on the three single perspectives, and we can observe that the union perspective performs best compared",
        "type": "Document"
      },
      {
        "id": "26fdd01d-b2b0-4f77-9a8b-bcc85191347d",
        "metadata": {
          "vector_store_key": "2001.06354-0",
          "chunk_id": 60,
          "document_id": "2001.06354",
          "start_idx": 20528,
          "end_idx": 20984
        },
        "page_content": ". As also shown in Table TABREF46, the ensemble model seems to take the best results from each model. Specifically, the NDCG score of the ensemble model is comparable to that of the image-only model and the scores of other metrics are comparable to those of the image-history joint model. From this experiment, we can confirm that the two models are in complementary relation. For the evaluation on the test-standard dataset of VisDial v1.0, we try 6 image",
        "type": "Document"
      },
      {
        "id": "8e78f160-6197-468e-99be-9ef9a20b7059",
        "metadata": {
          "vector_store_key": "1804.08186-3",
          "chunk_id": 259,
          "document_id": "1804.08186",
          "start_idx": 92868,
          "end_idx": 93327
        },
        "page_content": "-based classifiers with different features at the 2017 DSL shared task. A voting ensemble gave better results on the closed track than a singular word-based perplexity classifier (0.9025 weighted F1-score over 0.9013), but worse results on the open track (0.9016 with ensemble and 0.9065 without). In a highest probability ensemble, the winner is simply the language which is given the highest probability by any of the individual classifiers in the ensemble.",
        "type": "Document"
      },
      {
        "id": "bd9eb22f-a864-4a04-80a3-86e3b7b45731",
        "metadata": {
          "vector_store_key": "1804.08186-3",
          "chunk_id": 263,
          "document_id": "1804.08186",
          "start_idx": 94355,
          "end_idx": 94863
        },
        "page_content": " several ensemble methods and found that the mean probability ensemble attained better results than plurality voting, median probability, product, highest confidence, or Borda count ensembles. In a median probability ensemble, the medians over the probabilities given by the individual classifiers are calculated for each language. BIBREF270 and BIBREF171 used a median probability rule ensemble over SVM classifiers. Consistent with the results of BIBREF270 , BIBREF171 found that a mean ensemble was better",
        "type": "Document"
      },
      {
        "id": "eca3e33d-5f05-4892-a7b3-e2dc3930d807",
        "metadata": {
          "vector_store_key": "1706.04815-9",
          "chunk_id": 76,
          "document_id": "1706.04815",
          "start_idx": 20517,
          "end_idx": 20959
        },
        "page_content": " We sum the probability at each position of each single model to decide the ensemble result. Finally we select 13 models for ensemble, which achieves 42.92 and 44.97 in terms of ROUGE-L and BLEU-1, respectively, which achieves the state-of-the-art results of the extraction model. Then we test our synthesis model based on the extracted evidence. Our synthesis model achieves 3.78% and 3.73% improvement on the single model and ensemble model",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the new initialization method proposed in this paper?",
    "llm_answer": "Multichannel initialization.\n",
    "context": [
      {
        "id": "904822e7-928f-4301-a223-cad8a3da7977",
        "metadata": {
          "vector_store_key": "1603.04513-4",
          "chunk_id": 63,
          "document_id": "1603.04513",
          "start_idx": 20319,
          "end_idx": 20747
        },
        "page_content": " the \u201cNCE\u201d layer (\u201con\u201d in the figure). In this work, we use multichannel initialization for case (i) and random initialization for cases (ii) and (iii). Only fine-tuned multichannel representations (case (i)) are kept for subsequent supervised training. The rationale for this pretraining is similar to auto-encoder: for an object composed of smaller-granular elements, the representations of the whole object and its components",
        "type": "Document"
      },
      {
        "id": "886b3954-cf7d-44b2-9e36-75e394bfa1dc",
        "metadata": {
          "vector_store_key": "2002.10832-2",
          "chunk_id": 67,
          "document_id": "2002.10832",
          "start_idx": 22843,
          "end_idx": 23334
        },
        "page_content": ". Again, we additionally report the results obtained bypassing the intermediate steps 1 and 2: for the model denoted as Step 3 (from scratch) (last row of Table ), $\\Theta $ parameters are initialized with the original weights from pre-trained BERT, while the $W$ matrix is randomly initialized. Under this experimental condition, we observe lower performances, a finding that consolidates the importance of the multi-step training procedure we adopted. In Table , we report quantitative VQG",
        "type": "Document"
      },
      {
        "id": "c2657524-04df-4584-ab1b-ccf38b598c19",
        "metadata": {
          "vector_store_key": "1910.10288-1",
          "chunk_id": 11,
          "document_id": "1910.10288",
          "start_idx": 4037,
          "end_idx": 4421
        },
        "page_content": "y}_i$, from the decoder RNN state, $\\mathbf {d}_i$. An early sequence-to-sequence attention mechanism was proposed by Graves in BIBREF0. This approach is a purely location-based mechanism that uses an unnormalized mixture of $K$ Gaussians to produce the attention weights, $\\mathbf {\\alpha }_i$, for each encoder state. The general form of this type of attention is shown in (DISPLAY_",
        "type": "Document"
      },
      {
        "id": "75b12547-cc23-4e29-9622-0d0605c72a9e",
        "metadata": {
          "vector_store_key": "1910.11491-4",
          "chunk_id": 19,
          "document_id": "1910.11491",
          "start_idx": 7263,
          "end_idx": 7654
        },
        "page_content": " weight at $i$-th position. The model is firstly pre-trained to minimize the maximum-likelihood loss, which is widely used in sequence generation tasks. We define $y^* = \\lbrace y^*_1, \\cdots , y_T^*\\rbrace $ as the ground-truth output sequence for a given input sequence $x$, then the loss function is formulated as: After converging, the model is further optimized with local variance loss",
        "type": "Document"
      },
      {
        "id": "6539a7e2-47a7-4926-bb13-5e92ad1a0d8f",
        "metadata": {
          "vector_store_key": "2004.01878-5",
          "chunk_id": 39,
          "document_id": "2004.01878",
          "start_idx": 14189,
          "end_idx": 14714
        },
        "page_content": " As the first set of development experiments, we try different ways to initialize the noisy recurrent states of our ANRES model to find a suitable approach. For each trading day, we compare the results whether states transitions are modeled or not. Besides, we also compare the methods of random initialization and zero initialization. Note that the random initialization method we use here returns a tensor filled with random numbers from the standard normal distribution $\\mathcal {N}(0, 1)$. In summary, the following four",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How was a quality control performed so that the text is noisy but the annotations are accurate?",
    "llm_answer": "A small sample of texts was annotated by all four annotators, and inter-annotator agreement was measured.  For each annotator and each question, the probability of agreement with the majority response was computed. Annotators whose probability was more than two standard deviations away from the mean were discarded.\n",
    "context": [
      {
        "id": "ae777c6e-5f84-42f7-8843-5ba4994ac482",
        "metadata": {
          "vector_store_key": "1703.05260-8",
          "chunk_id": 49,
          "document_id": "1703.05260",
          "start_idx": 17551,
          "end_idx": 18087
        },
        "page_content": " results were discussed with the authors. Throughout the whole annotation phase, annotators could discuss any emerging issues with the authors. All annotations were done by undergraduate students of computational linguistics. The annotation was rather time-consuming due to the complexity of the task, and thus we decided for single annotation mode. To assess annotation quality, a small sample of texts was annotated by all four annotators and their inter-annotator agreement was measured (see Section \"Inter-Annotator Agreement\" ). It",
        "type": "Document"
      },
      {
        "id": "07872878-bec6-490d-9987-8ecc6566f9e2",
        "metadata": {
          "vector_store_key": "1909.00124-7",
          "chunk_id": 3,
          "document_id": "1909.00124",
          "start_idx": 1276,
          "end_idx": 1646
        },
        "page_content": ", the crowdsourced annotations can be quite noisy. Research on learning with noisy labels dates back to 1980s BIBREF4. It is still vibrant today BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11, BIBREF12 as it is highly challenging. We will discuss the related work in the next section. This paper studies the problem of learning with noisy labels for SSC",
        "type": "Document"
      },
      {
        "id": "f1d78584-356c-4141-9a77-85caa5331c3a",
        "metadata": {
          "vector_store_key": "1704.04539-6",
          "chunk_id": 35,
          "document_id": "1704.04539",
          "start_idx": 13333,
          "end_idx": 13834
        },
        "page_content": " noisy word alignments in annotation projection: the parser learns what words are likely to trigger a node (or a set of nodes) in the AMR by looking at their AMR alignments (which are induced by the word alignments). If an important word consistently remains unaligned, the parser will erroneously learn to discard it. More accurate alignments are therefore crucial in order to achieve better parsing results. We computed the percentage of words in the training data that are learned to be non-content",
        "type": "Document"
      },
      {
        "id": "e0cc4ab9-eb02-4d7b-a1bc-754cf2f089ff",
        "metadata": {
          "vector_store_key": "2001.08051-4",
          "chunk_id": 34,
          "document_id": "2001.08051",
          "start_idx": 13177,
          "end_idx": 13680
        },
        "page_content": " the annotation, which were used by 5 researchers to manually transcribe about 20 minutes of audio data. This experience led to a discussion, from which a second set of guidelines originated, aiming at reaching a reasonable trade-off between transcription accuracy and speed. As a consequence, we decided to apply the following transcription rules: only the main speaker has to be transcribed; presence of other voices (schoolmates, teacher) should be reported only with the label \u201c@voices\u201d, presence of",
        "type": "Document"
      },
      {
        "id": "5c28e61d-ac4b-4de3-aea4-d30240c8a3b0",
        "metadata": {
          "vector_store_key": "1912.03184-1",
          "chunk_id": 48,
          "document_id": "1912.03184",
          "start_idx": 17696,
          "end_idx": 18237
        },
        "page_content": " annotations that were done in less than 10 seconds and Phase 2 annotations that were done in less than 70 seconds. After we collected all annotations, we found unreliable annotators for both phases in the following way: for each annotator and for each question, we compute the probability with which the annotator agrees with the response chosen by the majority. If the computed probability is more than two standard deviations away from the mean we discard all annotations done by that annotator. On average, 310 distinct annotators needed",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is it a neural model? How is it trained?",
    "llm_answer": "Yes.  Training details vary depending on the specific model (e.g., neural regressor, neural classifier), but methods include using classification labels, cross-entropy loss, and training on data.\n",
    "context": [
      {
        "id": "76df7fe7-d53f-4a67-b7d1-6f3e7704a95e",
        "metadata": {
          "vector_store_key": "2002.11402-2",
          "chunk_id": 15,
          "document_id": "2002.11402",
          "start_idx": 5576,
          "end_idx": 5982
        },
        "page_content": " as explained above before passing to the network. Architecture is built using tensorflow/keras. Coding inspiration taken from BERT-keras and for CRF layer keras-contrib. If one is more comfortable in pytorch there are many examples available on github, but pytorch-bert-crf-ner is better for an easy start. We used BERT-Multilingual model so that we can train and fine-tune the same model for other Indian",
        "type": "Document"
      },
      {
        "id": "069c986a-be4b-4061-bfbb-a829143cc502",
        "metadata": {
          "vector_store_key": "1710.00341-4",
          "chunk_id": 2,
          "document_id": "1710.00341",
          "start_idx": 857,
          "end_idx": 1406
        },
        "page_content": " modeling. Indeed, the system makes very few assumptions about the task, and looks for supportive information directly on the Web. Our system works fully automatically. It does not use any heavy feature engineering and can be easily used in combination with task-specific approaches as well, as a core subsystem. Finally, it combines the representational strength of recurrent neural networks with kernel-based classification. The system starts with a claim to verify. First, we automatically convert the claim into a query, which we execute against",
        "type": "Document"
      },
      {
        "id": "28ba9973-bbf8-4367-a731-785e03cca374",
        "metadata": {
          "vector_store_key": "1910.11949-4",
          "chunk_id": 18,
          "document_id": "1910.11949",
          "start_idx": 7091,
          "end_idx": 7577
        },
        "page_content": " its features. The core of our chatbot model is a sequence-to-sequence BIBREF24. This architecture uses a Recurrent Neural Network (RNN) to encode a variable-length sequence to obtain a large fixed dimensional vector representation and another RNN to decode the vector into a variable-length sequence. The encoder iterates through the input sentence one word at each time step producing an output vector and a hidden state vector. The hidden state vector is passed to the next time step",
        "type": "Document"
      },
      {
        "id": "10ee7c8c-8492-4d26-9ee2-5244b2aa61f4",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 20,
          "document_id": "1909.00542",
          "start_idx": 7743,
          "end_idx": 8155
        },
        "page_content": " layer, and use cross-entropy as the loss function. The complete architecture is shown in Fig. FIGREF28. The bottom section of Table TABREF26 shows the results of several variants of the neural architecture. The table includes a neural regressor (NNR) and a neural classifier (NNC). The neural classifier is trained in two set ups: \u201cNNC top 5\u201d uses classification labels as described in Section SECREF3, and \u201cNNC",
        "type": "Document"
      },
      {
        "id": "dbf24675-827e-4c27-9d8a-87caba4ecf75",
        "metadata": {
          "vector_store_key": "1706.07179-2",
          "chunk_id": 7,
          "document_id": "1706.07179",
          "start_idx": 2809,
          "end_idx": 3302
        },
        "page_content": " in this section. Figure 1 provides a high-level view of the model. The model is sequential in nature, consisting of the following steps: read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer. We model the dynamic memory in a fashion similar to Recurrent Entity Networks BIBREF17 and then equip it with an additional relational memory. There are three main components to the model: 1) input encoder 2) dynamic memory, and 3)",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How they evaluate quality of generated output?",
    "llm_answer": "Through summarization quality (BLEU, METEOR, ROUGE) and language fluency.  Human evaluation of style strength is also used.\n",
    "context": [
      {
        "id": "8179a06f-adf0-4ac6-bc02-f09225699a49",
        "metadata": {
          "vector_store_key": "2004.01980-7",
          "chunk_id": 51,
          "document_id": "2004.01980",
          "start_idx": 17590,
          "end_idx": 18113
        },
        "page_content": " measure the generation quality through two conventional aspects: summarization quality and language fluency. Note that the purpose of this two-way automatic evaluation is to confirm that the performance of our model is in an acceptable range. Good automatic evaluation performances are necessary proofs to compliment human evaluations on the model effectiveness. We use the standard automatic evaluation metrics for summarization with the original headlines as the reference: BLEU BIBREF47, METEOR BIBREF48, ROUGE BIBREF49",
        "type": "Document"
      },
      {
        "id": "f089ebde-5eb8-47ef-814d-01a25f7bd385",
        "metadata": {
          "vector_store_key": "1909.02776-5",
          "chunk_id": 70,
          "document_id": "1909.02776",
          "start_idx": 27191,
          "end_idx": 27684
        },
        "page_content": " was produced, and compared by ROUGE. Averaging these ROUGE scores over each document and then over the dataset, the overall quality of summaries produced by the model can be obtained. The same process was repeated with a random regressor that needed no training, and which simply assigns a random number between zero and one to any given sample. Apart from measuring the performance of this regressor on the test set, the quality of summaries produced is evaluated and reported as a baseline.",
        "type": "Document"
      },
      {
        "id": "618b40e6-3113-4b7c-9edb-fc714b6bcc5d",
        "metadata": {
          "vector_store_key": "2004.01980-7",
          "chunk_id": 50,
          "document_id": "2004.01980",
          "start_idx": 17033,
          "end_idx": 17590
        },
        "page_content": " scores as the final score. In addition, we have another independent human evaluation task about the style strength \u2013 we present the generated headlines from TitleStylist and baselines to the human judges and let them choose the one that most conforms to the target style such as humor. Then we define the style strength score as the proportion of choices. Apart from the comprehensive human evaluation, we use automatic evaluation to measure the generation quality through two conventional aspects: summarization quality and language fluency. Note that the",
        "type": "Document"
      },
      {
        "id": "c1c83419-2165-4f1e-8678-3fe52ffd95fd",
        "metadata": {
          "vector_store_key": "2002.05058-9",
          "chunk_id": 64,
          "document_id": "2002.05058",
          "start_idx": 25291,
          "end_idx": 25823
        },
        "page_content": "forms them by a large margin, which demonstrates that our comparison-based evaluation metric is able to evaluate sample quality more accurately. In addition, we find that evaluating generated samples by comparing it with a set of randomly selected samples or using sample-level skill rating performs almost equally well. This is not surprising as the employed skill rating is able to handle the inherent variance of players (i.e. NLG models). As this variance does not exist when we regard a sample as a model which always generates",
        "type": "Document"
      },
      {
        "id": "fe8b179a-29d1-4533-9d3c-177402a1ca5c",
        "metadata": {
          "vector_store_key": "2004.01980-7",
          "chunk_id": 62,
          "document_id": "2004.01980",
          "start_idx": 21664,
          "end_idx": 22218
        },
        "page_content": " human evaluation of the overall generation quality on four criteria, we also conducted a conventional automatic assessment to gauge only the summarization quality. This evaluation does not take other measures such as the style strength into consideration, but it serves as important complimentary proof to ensure that the model has an acceptable level of summarization ability. Table TABREF59 summarizes the automatic evaluation results of our proposed TitleStylist model and all baselines. We use the summarization-related evaluation metrics, i.e., BLE",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the four forums the data comes from?",
    "llm_answer": "BIBREF, Reddit, Twitter, Facebook\n",
    "context": [
      {
        "id": "e4374424-74e2-4265-a87d-f390b05d4eb7",
        "metadata": {
          "vector_store_key": "1708.09609-2",
          "chunk_id": 3,
          "document_id": "1708.09609",
          "start_idx": 1347,
          "end_idx": 1894
        },
        "page_content": " of a forum in terms of what products it deals with, identify trends over time, associate users with particular activity profiles, and connect to price information to better understand the marketplace. Some of these analyses only require post-level information (what is the product being bought or sold in this post?) whereas other analyses might require token-level references; we annotate at the token level to make our annotation as general as possible. Our dataset has already proven enabling for case studies on these particular forums BIBREF",
        "type": "Document"
      },
      {
        "id": "572d2157-e52c-4480-9cc1-a73d6c58d96c",
        "metadata": {
          "vector_store_key": "1812.07023-1",
          "chunk_id": 7,
          "document_id": "1812.07023",
          "start_idx": 2828,
          "end_idx": 3309
        },
        "page_content": " , we discuss existing literature on end-to-end dialogue systems with a special focus on multi-modal dialogue systems. Section SECREF3 describes the avsd dataset. In Section SECREF4 , we present the architecture of our modelname model. We describe our evaluation and experimental setup in Section SECREF5 and then conclude in Section SECREF6 . With the availability of large conversational corpora from sources like Reddit and Twitter, there has been a lot of recent work on end-to",
        "type": "Document"
      },
      {
        "id": "52033256-2bdc-44e0-8fc0-dd0c987ca334",
        "metadata": {
          "vector_store_key": "1908.04531-4",
          "chunk_id": 29,
          "document_id": "1908.04531",
          "start_idx": 11374,
          "end_idx": 11919
        },
        "page_content": ". Twitter. Twitter has been used extensively as a source of user-generated content and it was the first source considered in our initial data collection phase. The platform provides excellent interface for developers making it easy to gather substantial amounts of data with limited efforts. However, Twitter was not a suitable source of data for our task. This is due to the fact that Twitter has limited usage in Denmark, resulting in low quality data with many classes of interest unrepresented. Facebook. We next considered Facebook, and the",
        "type": "Document"
      },
      {
        "id": "4b7f28ff-10f8-48db-942e-1c9dcb77f4e2",
        "metadata": {
          "vector_store_key": "2002.01359-1",
          "chunk_id": 30,
          "document_id": "2002.01359",
          "start_idx": 12645,
          "end_idx": 13213
        },
        "page_content": " datasets are listed in Table TABREF10, as are the details regarding which domains are present in each of the datasets. We create synthetic implementations of a total of 45 services or APIs over these domains. Our simulator framework interacts with these services to generate dialogue outlines, which are structured representations of dialogue semantics. We then use a crowd-sourcing procedure to paraphrase these outlines to natural language utterances. Our novel crowd-sourcing procedure preserves all annotations obtained from the simulator and does not require any",
        "type": "Document"
      },
      {
        "id": "e55a4ebd-88a8-4f4d-abe1-49dcc754f18d",
        "metadata": {
          "vector_store_key": "1909.02265-3",
          "chunk_id": 40,
          "document_id": "1909.02265",
          "start_idx": 15863,
          "end_idx": 16366
        },
        "page_content": " mixed-domain datasets through two series of experiments. These results give some useful insights in combining the approaches to improve the performance of a commercial chatbot platform which is under active development in our company. We plan to extend this current research and integrate its fruitful results into a future version of the platform. The following is three example dialogues generated by our system. The first dialogue is in single-domain.  The next two dialogues are in mixed-domains.  ",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How are sentence embeddings incorporated into the speech recognition system?",
    "llm_answer": "The provided text describes using pre-trained embeddings (like fastText and BERT) for conversational context, but does not detail how sentence embeddings are incorporated into the *speech recognition* system.  The text focuses on using these embeddings for *other* NLP tasks (e.g. domain classification and sentiment classification).\n",
    "context": [
      {
        "id": "2a676b64-7d5b-412a-9491-70d756ee05ff",
        "metadata": {
          "vector_store_key": "1906.11604-1",
          "chunk_id": 6,
          "document_id": "1906.11604",
          "start_idx": 2151,
          "end_idx": 2635
        },
        "page_content": " text resources, (i.e. fastText, BERT) so that the model can learn better conversational-context representations. So far, the use of such pre-trained embeddings have found limited success in the speech recognition task. We also add a gating mechanism to the decoder network that can integrate all the available embeddings (word, speech, conversational-context) efficiently with increase representational power using multiplicative interactions. Additionally, we explore a way to train",
        "type": "Document"
      },
      {
        "id": "b47eb24e-554c-49db-afc1-a49279bccaa0",
        "metadata": {
          "vector_store_key": "2001.00137-7",
          "chunk_id": 17,
          "document_id": "2001.00137",
          "start_idx": 7115,
          "end_idx": 7610
        },
        "page_content": " by an embedding layer used for input representation, with the final input embedding being a sum of token embedddings, segmentation embeddings and position embeddings. The first one, token embedding layer, uses a vocabulary dictionary to convert each token into a more representative embedding. The segmentation embedding layer indicates which tokens constitute a sentence by signaling either 1 or 0. In our case, since our data are formed of single sentences, the segment is 1 until the first `",
        "type": "Document"
      },
      {
        "id": "5565cedf-e8d9-4e1b-947e-259521751114",
        "metadata": {
          "vector_store_key": "1804.08139-0",
          "chunk_id": 1,
          "document_id": "1804.08139",
          "start_idx": 428,
          "end_idx": 1019
        },
        "page_content": " There are also pre-trained word embeddings, which can easily used in downstream tasks. However, on sentence level, there is still no generic sentence representation which is suitable for various NLP tasks. Currently, most of sentence encoding models are trained specifically for a certain task in a supervised way, which results to different representations for the same sentence in different tasks. Taking the following sentence as an example for domain classification task and sentiment classification task,  general text classification models always learn two representations separately.",
        "type": "Document"
      },
      {
        "id": "ba719cf5-ce97-42c3-855e-fdaab2ca5b30",
        "metadata": {
          "vector_store_key": "1909.06937-4",
          "chunk_id": 19,
          "document_id": "1909.06937",
          "start_idx": 6576,
          "end_idx": 7018
        },
        "page_content": " As shown in Figure FIGREF16, the input utterance is firstly encoded with the Embedding Layer, and then is transformed by multiple CM-blocks with the assistance of slot and intent memories, and finally make predictions in the Inference Layer. The pre-trained word embeddings has been indicated as a de-facto standard of neural network architectures for various NLP tasks. We adapt the cased, 300d Glove BIBREF17 to initialize word embeddings,",
        "type": "Document"
      },
      {
        "id": "577707d8-3352-40d1-baa1-b60e9f24da55",
        "metadata": {
          "vector_store_key": "1804.05868-5",
          "chunk_id": 13,
          "document_id": "1804.05868",
          "start_idx": 5060,
          "end_idx": 5536
        },
        "page_content": " embedding (English dictionary flag and word length flag with length bins of 0-3, 4-6, 7-10, and 10-all). These concatenated vectors are passed to a Bi-LSTM network to generate a sequence of hidden representations which encode the contextual information spread across the sentence. Finally, output layer uses the feed-forward neural network with a softmax function for a probability distribution over the language tags. We train the network on our CS training set concatenated",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How different is the dataset size of source and target?",
    "llm_answer": "The source dataset is significantly larger than the target dataset.  For example, 25% of MovieQA (2462 examples) is still larger than MC160 (280 examples).\n",
    "context": [
      {
        "id": "62d86165-dc99-4b04-9b9c-3124c1a25e97",
        "metadata": {
          "vector_store_key": "1711.05345-0",
          "chunk_id": 48,
          "document_id": "1711.05345",
          "start_idx": 16243,
          "end_idx": 16669
        },
        "page_content": " how large the source dataset should be to make transfer learning feasible. The results are shown in Table 4 . We find that even a small amount of source data can help. For example, by using only 25% of MovieQA for pre-training, the accuracy increases 6.3% on MC160. This is because 25% of MovieQA training set (2,462 examples) is still much larger than the MC160 training set (280 examples). As the size of the source dataset",
        "type": "Document"
      },
      {
        "id": "54f5728b-0a4f-408d-a76a-c78cc3de85eb",
        "metadata": {
          "vector_store_key": "1910.06748-3",
          "chunk_id": 71,
          "document_id": "1910.06748",
          "start_idx": 25247,
          "end_idx": 25756
        },
        "page_content": " in accuracy and F1 score. Looking at the choice of datasets, it can be seen that training with the large-scale dataset leads to a nearly 1% improvement compared to the medium-sized dataset, which also gives a 1% improvement compared to the small-scale dataset. While it is expected that using more training data will lead to a better system and better results, the small improvement indicates that even our small-scale dataset has sufficient messages to allow the network training to converge. We can further",
        "type": "Document"
      },
      {
        "id": "f722e190-d4f4-40ff-850f-30b3b60e19c4",
        "metadata": {
          "vector_store_key": "1804.08186-0",
          "chunk_id": 310,
          "document_id": "1804.08186",
          "start_idx": 113568,
          "end_idx": 114102
        },
        "page_content": " datasets are critical for replicable and comparable research in . Where a subset of data is used from a larger collection, researchers should include details of the specific subset, including any breakdown into training and test data, or partitions for cross-validation. Where data from a new source is used, justification should be given for its inclusion, as well as some means for other researchers to replicate experiments on the same dataset. To address specific sub-problems in , a number of shared tasks have been organized on",
        "type": "Document"
      },
      {
        "id": "4582f30b-ade5-4cd6-92a4-a9a939fae323",
        "metadata": {
          "vector_store_key": "1711.05345-0",
          "chunk_id": 7,
          "document_id": "1711.05345",
          "start_idx": 2670,
          "end_idx": 3189
        },
        "page_content": " the same model on the other MCQA dataset, which is referred to as the target task, that we actually care about, but that usually contains much less training data. The effectiveness of transfer learning is evaluated by the model's performance on the target task. In supervised transfer learning, both the source and target datasets provide the correct answer to each question during pre-training and fine-tuning, and the QA model is guided by the correct answer to optimize its objective function in a supervised manner",
        "type": "Document"
      },
      {
        "id": "49dd09c0-f752-4a00-bc37-38c26e5e9acc",
        "metadata": {
          "vector_store_key": "1709.05404-5",
          "chunk_id": 48,
          "document_id": "1709.05404",
          "start_idx": 17579,
          "end_idx": 18022
        },
        "page_content": "-validation setup) for each dataset while holding the number of posts per class constant. Figure FIGREF26 shows the performance of each of the Gen, RQ, and Hyp datasets at intervals of 100 posts per class (up to the maximum size of 582 posts per class for Hyp, and 851 posts per class for RQ). From the graph, we can see that as a general trend, the datasets benefit from larger dataset sizes. Interestingly, the results for the RQ dataset are",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What type of documents are supported by the annotation platform?",
    "llm_answer": "PDFs and Microsoft Word\n",
    "context": [
      {
        "id": "7a436576-c431-415f-ba13-4c0acac286a6",
        "metadata": {
          "vector_store_key": "2002.01861-3",
          "chunk_id": 33,
          "document_id": "2002.01861",
          "start_idx": 12971,
          "end_idx": 13505
        },
        "page_content": " features: First, it provides an annotation interface that allows users to define content elements, upload documents, and annotate documents; a screenshot is shown in Figure FIGREF12. We have invested substantial effort in making the interface as easy to use as possible; for example, annotating content elements is as easy as selecting text from the document. Our platform is able to ingest documents in a variety of formats, including PDFs and Microsoft Word, and converts these formats into plain text before presenting them to the",
        "type": "Document"
      },
      {
        "id": "b65c5a7e-da34-44bf-9c3f-3b4e2fc794a6",
        "metadata": {
          "vector_store_key": "1912.07025-6",
          "chunk_id": 63,
          "document_id": "1912.07025",
          "start_idx": 23832,
          "end_idx": 24368
        },
        "page_content": "aged thereby improving performance over time. Our proposed web-based annotator system, although designed for Indic manuscripts, is flexible, and could be reused for similar manuscripts from Asian subcontinent. We intend to expand the capabilities of our annotator system in many useful ways. For instance, the layout estimated by our deep-network could be provided to annotators for correction, thus reducing annotation efforts. Finally, we plan to have our dataset, instance segmentation system and annotator system publicly available.",
        "type": "Document"
      },
      {
        "id": "5827c722-8da9-4c48-9957-4bcaaa150703",
        "metadata": {
          "vector_store_key": "2001.02380-4",
          "chunk_id": 41,
          "document_id": "2001.02380",
          "start_idx": 16317,
          "end_idx": 16722
        },
        "page_content": " annotated with a large number of layers, including document layout (headings, paragraphs, figures, etc.); multiple POS tags (Penn tags, CLAWS5, Universal POS); lemmas; sentence types (e.g. imperative, wh-question etc., BIBREF35); Universal Dependencies BIBREF36; (non-)named entity types; coreference and bridging resolution; and discourse parses using Rhetorical Structure Theory BIBREF11. In particular",
        "type": "Document"
      },
      {
        "id": "272674f8-fd4a-4092-891b-be5cfc8e68de",
        "metadata": {
          "vector_store_key": "1912.07025-6",
          "chunk_id": 28,
          "document_id": "1912.07025",
          "start_idx": 11135,
          "end_idx": 11635
        },
        "page_content": "forations in the document interior. It may be important to identify such degradations before attempting lexically-focused tasks such as OCR or word-spotting. Keeping the aforementioned challenges in mind, we introduce a new browser-based annotation tool (see Figure FIGREF10). The tool is designed to operate both stand-alone and as a web-service. The web-service mode enables features such as distributed parallel sessions by registered annotators, dashboard-based live session monitoring and a wide",
        "type": "Document"
      },
      {
        "id": "ac1a10fb-adea-4b22-b9d0-2fc69b92a68e",
        "metadata": {
          "vector_store_key": "2002.01861-3",
          "chunk_id": 32,
          "document_id": "2002.01861",
          "start_idx": 12437,
          "end_idx": 12971
        },
        "page_content": " each type of document along with the content elements that are extracted by our BERT models. We provide both the original source Chinese texts as well as English translations to provide the reader with a general sense of the source documents and how well our models behave. All the capabilities described in this paper come together in an end-to-end cloud-based platform that we have built. The platform has two main features: First, it provides an annotation interface that allows users to define content elements, upload documents,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the strong baselines you have?",
    "llm_answer": "The context does not specify which baselines are considered \"strong.\"  It only describes different baselines and their performance.\n",
    "context": [
      {
        "id": "593127ef-0325-4226-be3e-0ed371ce653f",
        "metadata": {
          "vector_store_key": "1804.08782-7",
          "chunk_id": 37,
          "document_id": "1804.08782",
          "start_idx": 13990,
          "end_idx": 14427
        },
        "page_content": " number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 .  For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose",
        "type": "Document"
      },
      {
        "id": "9028b9ee-b8b5-41cb-b460-733f242d6911",
        "metadata": {
          "vector_store_key": "1911.13066-4",
          "chunk_id": 40,
          "document_id": "1911.13066",
          "start_idx": 15067,
          "end_idx": 15591
        },
        "page_content": " dataset. Table TABREF23 presents results of baseline and all variations of our experiments. We focus our discussion on F1-score as accuracy is often misleading for dataset with unbalanced class distribution. However, for completeness sake, all measures are reported. It is observed from the results that baseline model performs worst among all the experiments. The reason behind this degradation in performance can be traced back to the nature of the texts in the datasets (i.e., datasets used in original paper of baseline",
        "type": "Document"
      },
      {
        "id": "25bb2afe-7835-48e9-a6f8-dbdbbde2116d",
        "metadata": {
          "vector_store_key": "1902.06843-8",
          "chunk_id": 91,
          "document_id": "1902.06843",
          "start_idx": 35139,
          "end_idx": 35416
        },
        "page_content": "s by -0.27. Baselines: To test the efficacy of our multi-modal framework for detecting depressed users, we compare it against existing content, content-network, and image-based models (based on the aforementioned general image feature, facial presence, and facial expressions.)",
        "type": "Document"
      },
      {
        "id": "f189eac9-0435-44a6-90b3-1a428fecf8da",
        "metadata": {
          "vector_store_key": "1707.03904-4",
          "chunk_id": 53,
          "document_id": "1707.03904",
          "start_idx": 20086,
          "end_idx": 20524
        },
        "page_content": " \u2013 finding the few most relevant ones will allow the reader to work best. In Tables 2 and 3 we compare all baselines when the context size is tuned to maximize the overall accuracy on the validation set. For Quasar-S the best performing baseline is the BiRNN language model, which achieves $33.6\\%$ accuracy. The GA model achieves $48.3\\%$ accuracy on the set of instances for which the answer is in context, however, a search accuracy of",
        "type": "Document"
      },
      {
        "id": "c8c4272e-a38c-48c3-a271-2ada66b9c8b5",
        "metadata": {
          "vector_store_key": "2004.01980-1",
          "chunk_id": 63,
          "document_id": "2004.01980",
          "start_idx": 22218,
          "end_idx": 22603
        },
        "page_content": " all baselines. We use the summarization-related evaluation metrics, i.e., BLEU, ROUGE, CIDEr, and METEOR, to measure how relevant the generated headlines are to the news articles, to some extent, by comparing them to the original human-written headlines. In Table TABREF59, the first row \u201cNHG\u201d shows the performance of the current state-of-the-art summarization model on our data, and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are causal attribution networks?",
    "llm_answer": "A causal attribution dataset is a collection of text pairs reflecting cause-effect relationships proposed by humans.  These networks map out the underlying or latent causal attribution network held collectively by humans.  Nodes are defined by written descriptions, and edges represent cause-effect relationships.\n",
    "context": [
      {
        "id": "c9742cc7-72ba-4daf-893f-dc22eadc6969",
        "metadata": {
          "vector_store_key": "1812.06038-4",
          "chunk_id": 0,
          "document_id": "1812.06038",
          "start_idx": 0,
          "end_idx": 494
        },
        "page_content": "In this work we compare causal attribution networks derived from three datasets. A causal attribution dataset is a collection of text pairs that reflect cause-effect relationships proposed by humans (for example, \u201cvirus causes sickness\u201d). These written statements identify the nodes of the network (see also our graph fusion algorithm for dealing with semantically equivalent statements) while cause-effect relationships form the directed edges (\u201cvirus\u201d $\\rightarrow $ \u201csickness\u201d) of the causal",
        "type": "Document"
      },
      {
        "id": "125e9765-1234-47d5-8e4a-60d8f649fd3c",
        "metadata": {
          "vector_store_key": "1812.06038-4",
          "chunk_id": 55,
          "document_id": "1812.06038",
          "start_idx": 18228,
          "end_idx": 18801
        },
        "page_content": " adjectives than ConceptNet and IPRnet. Overall, shorter, noun-heavy sentences may either help or harm the ability to combine causal attribution networks, depending on their ambiguity relative to longer, typical written statements. These causal attributions networks are separate efforts to map out the underlying or latent causal attribution network held collectively by humans. It is natural to then ask if these different efforts can be combined in an effective way. Fusing these networks together can provide a single causal attribution network for researchers to study",
        "type": "Document"
      },
      {
        "id": "3386cdc6-951a-4f4b-95b8-26cce8a11589",
        "metadata": {
          "vector_store_key": "1812.06038-4",
          "chunk_id": 1,
          "document_id": "1812.06038",
          "start_idx": 494,
          "end_idx": 921
        },
        "page_content": "\ufffdvirus\u201d $\\rightarrow $ \u201csickness\u201d) of the causal attribution network. We collected causal attribution networks from three sources of data: English Wikidata BIBREF11 , English ConceptNet BIBREF10 , and IPRnet BIBREF12 . Wikidata and ConceptNet, are large knowledge graphs that contain semantic links denoting many types of interactions, one of which is causal attribution, while IPRnet comes from an Amazon Mechanical Turk study",
        "type": "Document"
      },
      {
        "id": "cf8623fa-ef6d-4774-9d68-3480660bf375",
        "metadata": {
          "vector_store_key": "1812.06038-4",
          "chunk_id": 42,
          "document_id": "1812.06038",
          "start_idx": 12945,
          "end_idx": 13533
        },
        "page_content": " cause-effect relationships held by humans. We perform a descriptive analysis of the three datasets, comparing and contrasting their features and properties. We focus on two aspects, the network structure and the text information (the written descriptions associated with each node in the network). Understanding these data at these levels can inform efforts to combine different causal attribution networks (Sec. \"Fusing causal networks\" ). Table 1 and Fig. 2 summarize network characteristics for the three causal attribution networks. We focus on standard measures of network structure",
        "type": "Document"
      },
      {
        "id": "46fd1c9a-4ff8-4422-ae1a-aabdf1e57659",
        "metadata": {
          "vector_store_key": "1812.06038-4",
          "chunk_id": 40,
          "document_id": "1812.06038",
          "start_idx": 12063,
          "end_idx": 12629
        },
        "page_content": " . Here we use network and text analysis tools to compare causal attribution networks (Sec. \"Comparing causal networks\" ). Crucially, nodes in these networks are defined only by their written descriptions, and multiple written descriptions can represent the same conceptual entity. Thus, to understand how causal attribution networks can be combined, we introduce and analyze a method for fusing networks (Sec. \"Fusing causal networks\" ) that builds off both the network structure and associated text information and explicitly incorporates conceptual equivalencies.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "how did they ask if a tweet was racist?",
    "llm_answer": "Participants were asked to answer \"yes\" or \"no\" if they considered the tweet hate speech, either based on a definition or their personal opinion.\n",
    "context": [
      {
        "id": "df7b997b-2152-494e-b4b2-a63faee41c48",
        "metadata": {
          "vector_store_key": "1701.08118-4",
          "chunk_id": 19,
          "document_id": "1701.08118",
          "start_idx": 7852,
          "end_idx": 8358
        },
        "page_content": ". Participants were finally asked (3) to answer how offensive they thought the tweet was on a 6-point Likert scale from 1 (Not offensive at all) to 6 (Very offensive). If they answered 4 or higher, the participants had the option to state which particular words they found offensive. After the annotation of the 20 tweets, participants were asked to voluntarily answer an open question regarding the definition of hate speech. In the survey with the definition, they were asked if the definition of Twitter",
        "type": "Document"
      },
      {
        "id": "a3dbbd35-487d-4101-8371-c9dca7b97eea",
        "metadata": {
          "vector_store_key": "1701.08118-4",
          "chunk_id": 18,
          "document_id": "1701.08118",
          "start_idx": 7381,
          "end_idx": 7852
        },
        "page_content": " of 20 tweets presented in a random order. For each tweet, each participant was asked three questions. Depending on the survey, participants were asked (1) to answer (yes/no) if they considered the tweet hate speech, either based on the definition or based on their personal opinion. Afterwards they were asked (2) to answer (yes/no) if the tweet should be banned from Twitter. Participants were finally asked (3) to answer how offensive they thought the tweet was on a 6",
        "type": "Document"
      },
      {
        "id": "7fea94b1-2c2d-417d-89ed-68b61ae68612",
        "metadata": {
          "vector_store_key": "1901.01911-1",
          "chunk_id": 44,
          "document_id": "1901.01911",
          "start_idx": 16609,
          "end_idx": 17070
        },
        "page_content": " people in #Ferguson were shouting \u201ckill the police\", local reporting has refuted your ugly racism  Here the misclassified tweet is a reply including an explicit expression of agreement with the author of the source tweet (\u201cI join you\u201d). Tweet (s2) is one of the rare cases of source tweets denying the rumor (source tweets in the RumourEval17 dataset are mostly supporting the rumor at issue). Our hypothesis is that it is difficult for a system to detect such",
        "type": "Document"
      },
      {
        "id": "70226e9e-d673-47fb-983c-4c3ab1909fae",
        "metadata": {
          "vector_store_key": "1811.02906-2",
          "chunk_id": 13,
          "document_id": "1811.02906",
          "start_idx": 5683,
          "end_idx": 6214
        },
        "page_content": "izing the same tweets into either `insult', `profanity', `abuse', or `other'. The training data contains 5,008 manually labeled tweets sampled from Twitter from selected accounts that are suspected to contain a high share of offensive language. Manual inspection reveals a high share of political tweets among those labeled as offensive. These tweets range from offending single Twitter users, politicians and parties to degradation of whole social groups such as Muslims, migrants or refugees. The test data contains 3,532 tweets.",
        "type": "Document"
      },
      {
        "id": "88093cca-69d6-49bf-93c4-2a8b10e15809",
        "metadata": {
          "vector_store_key": "1910.03814-1",
          "chunk_id": 24,
          "document_id": "1910.03814",
          "start_idx": 9411,
          "end_idx": 9950
        },
        "page_content": ", tweets containing less than three words and tweets containing porn related terms. From that selection, we kept the ones that included images and downloaded them. Twitter applies hate speech filters and other kinds of content control based on its policy, although the supervision is based on users' reports. Therefore, as we are gathering tweets from real-time posting, the content we get has not yet passed any filter. We aim to create a multimodal hate speech database where all the instances contain visual and textual information that",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does the model compute the likelihood of executing to the correction semantic denotation?",
    "llm_answer": "The parser is trained to maximize the total log likelihood of consistent logical forms, where consistent logical forms are those that yield the correct denotation when executed.\n",
    "context": [
      {
        "id": "f5c707d4-d9b6-450d-9885-07e0a9fdaa77",
        "metadata": {
          "vector_store_key": "1808.07625-2",
          "chunk_id": 44,
          "document_id": "1808.07625",
          "start_idx": 13753,
          "end_idx": 14137
        },
        "page_content": " term computes the reconstruction likelihood $p(x|y)$ ; and the second term is the entropy of the approximated posterior $q(y|x) $ for regularization. Specifically, we use the semantic parser to compute the approximated posterior $q(y|x)$ . The reconstruction likelihood $p(x|y)$ is computed with an inverse parser which recovers utterance $x$ from its logical form $y$ . We use $p(x|",
        "type": "Document"
      },
      {
        "id": "b455043e-7c67-46b1-954e-927f5a4c9f3d",
        "metadata": {
          "vector_store_key": "1809.02731-5",
          "chunk_id": 13,
          "document_id": "1809.02731",
          "start_idx": 4791,
          "end_idx": 5057
        },
        "page_content": "+1}}\\log P(w_j|S_i)  \\nonumber $$   (Eq. 5)   where $\\theta $ and $\\phi $ contain the parameters in the encoder $f_\\text{en}(S_i;\\theta )$ and the decoder $f_\\text{de}(_i;\\phi )$ respectively. The forward computation of our model for a given sentence pair $\\lbrace S",
        "type": "Document"
      },
      {
        "id": "d2a7dc8e-8502-469b-a4b8-ed2d3a7d2cc1",
        "metadata": {
          "vector_store_key": "1808.07625-2",
          "chunk_id": 29,
          "document_id": "1808.07625",
          "start_idx": 9486,
          "end_idx": 9719
        },
        "page_content": ". Then, the parser is trained to maximize the total log likelihood of these consistent logical forms:  $$\\begin{split}\n& \\sum _{y \\in Y_c(x)} \\log p(y|x) = \\\\\n& \\sum _{y \\in Y_c(x)} \\log p(f_1,\\cdots , f_k, l_1, \\cdots , l_o|x)\n\\end{",
        "type": "Document"
      },
      {
        "id": "f0e82722-7fb4-4803-a385-85935e76bc0e",
        "metadata": {
          "vector_store_key": "1808.07625-4",
          "chunk_id": 36,
          "document_id": "1808.07625",
          "start_idx": 11252,
          "end_idx": 11757
        },
        "page_content": " equates to 1 in this case. Training of the neural parser-ranker system involves the following steps. Given an input utterance, the parser first generates a list of candidate logical forms via beam search. The logical forms are then executed and those which yield the correct denotation are marked as consistent. The parser is trained to optimize the total likelihood of consistent logical forms (Equation ( 21 )), while the ranker is trained to optimize the marginal likelihood of denotations (Equation (",
        "type": "Document"
      },
      {
        "id": "a954b3fa-b110-40f3-af02-3f11cd382202",
        "metadata": {
          "vector_store_key": "2001.05467-5",
          "chunk_id": 12,
          "document_id": "2001.05467",
          "start_idx": 4836,
          "end_idx": 5327
        },
        "page_content": " this, during training, we average out all the output probability distributions for each time step of the decoder for the whole mini-batch. The resulting vector $D^{\\prime }$ will reflect each token's probability of being generated from the model's perspective. Note that we do not use discrete ground-truth tokens to evaluate the model's bias, because there is a fine distinction between the two: a statistics of frequency on ground-truth tokens is an evaluation of the corpus's bias, while",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are state of the art methods authors compare their work with? ",
    "llm_answer": "GenIM, ImpGI, Unmasking, Caravel, and SPATIUM.\n",
    "context": [
      {
        "id": "116cbe96-deb5-4d3e-b2c7-613c794a2057",
        "metadata": {
          "vector_store_key": "1804.08186-0",
          "chunk_id": 301,
          "document_id": "1804.08186",
          "start_idx": 109612,
          "end_idx": 110179
        },
        "page_content": " different authors have used to evaluate methods. BIBREF32 emphasize this by demonstrating how the performance of a system can vary according to the data used for evaluation. This implies that comparisons of results reported by different authors may not be meaningful, as a strong result in one paper may not translate into a strong result on the dataset used in a different paper. In other areas of research, authors have proposed standardized corpora to allow for the objective comparison of different methods. Some authors have released datasets to accompany their",
        "type": "Document"
      },
      {
        "id": "b3eb1308-3e01-4903-be04-528f4258d508",
        "metadata": {
          "vector_store_key": "2002.11268-7",
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "14d03e05-90b4-4b04-8f6f-76f718dcba54",
        "metadata": {
          "vector_store_key": "1906.10551-4",
          "chunk_id": 7,
          "document_id": "1906.10551",
          "start_idx": 3019,
          "end_idx": 3554
        },
        "page_content": " properties to characterize AV methods. Section SECREF4 describes the methodology, consisting of the used corpora, examined AV methods, selected performance measures and experiments. Finally, Section SECREF5 concludes the work and outlines future work. Over the years, researchers in the field of authorship analysis identified a number of challenges and limitations regarding existing studies and approaches. Azarbonyad et al. BIBREF8 , for example, focused on the questions if the writing styles of authors of short texts change over",
        "type": "Document"
      },
      {
        "id": "a7351c9b-1fbd-42d4-b928-d8a94dfb48bc",
        "metadata": {
          "vector_store_key": "1906.10551-4",
          "chunk_id": 117,
          "document_id": "1906.10551",
          "start_idx": 44245,
          "end_idx": 44729
        },
        "page_content": " applied 12 existing AV methods on three self-compiled corpora, where the intention behind each corpus was to focus on a different aspect of the methods applicability. Our findings regarding the examined approaches can be summarized as follows: Despite of the good performance of the five AV methods GenIM, ImpGI, Unmasking, Caravel and SPATIUM, none of them can be truly considered as reliable and therefore applicable in real forensic cases. The reason for this is not only the non-",
        "type": "Document"
      },
      {
        "id": "4b8a152c-f365-4d84-b274-574f22da01b4",
        "metadata": {
          "vector_store_key": "1804.08186-0",
          "chunk_id": 162,
          "document_id": "1804.08186",
          "start_idx": 58399,
          "end_idx": 58929
        },
        "page_content": "hing. Witten-Bell proved to be clearly better than Kneser-Ney. In recent years there has been a tendency towards attempting to combine several different types of features into one classifier or classifier ensemble. Many recent studies use readily available classifier implementations and simply report how well they worked with the feature set used in the context of their study. There are many methods presented in this article that are still not available as out of the box implementations, however. There are many studies which",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much improvement do they get?",
    "llm_answer": "The improvement varies.  2% for using Doc2Vec instead of TF-IDF, 2.6 points for CNN2-DE (global) model with RST features, about 1 point for discourse-enhanced SVM2-PVs.  Other improvements are mentioned but not quantified.\n",
    "context": [
      {
        "id": "a982cc1e-0dc1-47ba-b4e9-e78baae67bef",
        "metadata": {
          "vector_store_key": "1610.00879-0",
          "chunk_id": 19,
          "document_id": "1610.00879",
          "start_idx": 7038,
          "end_idx": 7487
        },
        "page_content": "2%. This degradation implies that our features capture a subset of drunk tweets and that there are properties of drunk tweets that may be more subtle. When both N-gram and stylistic features are used, there is negligible improvement. The accuracy for Dataset 2 increases from 77.9% to 78.1%. Precision/Recall metrics do not change significantly either. The best accuracy of our classifier is 78.1% for all features, and 75.6% for stylistic features.",
        "type": "Document"
      },
      {
        "id": "567aa9a2-5797-4ddd-af49-4f7bd63326a2",
        "metadata": {
          "vector_store_key": "2003.06279-6",
          "chunk_id": 56,
          "document_id": "2003.06279",
          "start_idx": 21752,
          "end_idx": 22053
        },
        "page_content": "e. $\\max \\Gamma _+ = \\max _p \\Gamma _+(p)$) for GloVe. We also show the average difference in performance ($\\langle \\Gamma _+ - \\Gamma _0 \\rangle $) and the total number of cases in which an improvement in performance was observed ($N_+$). $N_+$ ranges in the interval $0 \\le N_+ \\le 20$. Table TABREF",
        "type": "Document"
      },
      {
        "id": "dda4f549-a635-4044-b0b0-194cecf97d9a",
        "metadata": {
          "vector_store_key": "1909.00183-3",
          "chunk_id": 129,
          "document_id": "1909.00183",
          "start_idx": 47837,
          "end_idx": 48317
        },
        "page_content": "). The improvement is larger for finer clusterings into 70 and 45 communities, which contain enough detail that can be associated with levels of risk (e.g., type of accident). This supports the value of the multi-resolution groupings we have extracted through our analysis. We also studied the impact of using Doc2Vec vectors as features. Interestingly, the comparison between columns 2 and 11 shows that there is only a slight improvement of 2% when using Doc2Vec instead of TF-i",
        "type": "Document"
      },
      {
        "id": "23959b6b-88e5-4c0e-9f38-3f369af42815",
        "metadata": {
          "vector_store_key": "1709.02271-9",
          "chunk_id": 49,
          "document_id": "1709.02271",
          "start_idx": 17361,
          "end_idx": 17785
        },
        "page_content": " a noticeable amount, with the maximal improvement seen in the CNN2-DE (global) model with RST features (by 2.6 points). In contrast, the discourse-enhanced SVM2-PVs increase F1 by about 1 point, with overall much lower scores in comparison to the CNNs. In general, RST features work better than GR features. The results of the varying-sizes experiments are plotted in Figure FIGREF27 . Again, we observe the overall pattern",
        "type": "Document"
      },
      {
        "id": "efb1237b-ae0c-478c-a553-dfbddd271f36",
        "metadata": {
          "vector_store_key": "1706.04815-9",
          "chunk_id": 83,
          "document_id": "1706.04815",
          "start_idx": 23311,
          "end_idx": 23745
        },
        "page_content": ". In other categories, our synthesis model achieves more or less improvement. For the question whose answer can be almost found in the passage (ROUGE-L $\\ge $ 0.8), our model achieves 0.2 improvement even though the space that can be raised is limited. For the question whose upper performance via answer extraction is between 0.6 and 0.8, our model achieves a large improvement of 2.0. Part of questions in the last category (ROUGE-L",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which languages do they test on?",
    "llm_answer": "Dutch, Spanish, English, German, Basque, French, Italian, German, Finnish, Hindi, Portuguese, Japanese, Thai, Vietnamese, Arabic, Russian.\n",
    "context": [
      {
        "id": "e8496824-d6ce-4c1b-bc89-11f6ad8ad7ac",
        "metadata": {
          "vector_store_key": "1701.09123-8",
          "chunk_id": 17,
          "document_id": "1701.09123",
          "start_idx": 6692,
          "end_idx": 7180
        },
        "page_content": " our approach is much simpler than most, we obtain the best results for Dutch, Spanish and English and comparable results in German (on CoNLL 2002 and 2003). We also report best results for German using the GermEval 2014 shared task data and for Basque using the Egunkaria testset BIBREF23 . We report out-of-domain evaluations in three languages (Dutch, English and Spanish) using four different datasets to compare our system with the best publicly available systems for those languages",
        "type": "Document"
      },
      {
        "id": "0ecb2ed4-1b4b-4bfa-b0da-4b7981a45091",
        "metadata": {
          "vector_store_key": "1808.08780-4",
          "chunk_id": 36,
          "document_id": "1808.08780",
          "start_idx": 13409,
          "end_idx": 13926
        },
        "page_content": ". We test our bilingual embedding refinement approach on both intrinsic and extrinsic tasks. In Section \"Cross-lingual embeddings training\" we describe the common training setup for all experiments and language pairs. The languages we considered are English, Spanish, Italian, German and Finnish. Throughout all the experiments we use publicly available resources in order to make comparisons and reproducibility of our experiments easier. Corpora. In our experiments we make use of web-extracted corpora. For English",
        "type": "Document"
      },
      {
        "id": "d00af03c-0e2e-45ca-a17f-37d25292ea85",
        "metadata": {
          "vector_store_key": "1909.13695-6",
          "chunk_id": 33,
          "document_id": "1909.13695",
          "start_idx": 12139,
          "end_idx": 12554
        },
        "page_content": " came from the Linguaskill test, of which there are 6 major L1s each with more than 100 candidates: Hindi, Portuguese, Japanese, Spanish, Thai and Vietnamese. Each of the training set and two test sets was fairly gender balanced, with approximately one third of candidates graded as B1, one third graded as B2, and the rest graded as A1, A2, C1, or C2, according to CEFR ability levels. For each test set candidate,",
        "type": "Document"
      },
      {
        "id": "4199c59c-1927-47cb-857b-9c1bed5dae20",
        "metadata": {
          "vector_store_key": "1911.03894-7",
          "chunk_id": 44,
          "document_id": "1911.03894",
          "start_idx": 15623,
          "end_idx": 16121
        },
        "page_content": " is also machine translated for all languages. The dataset is composed of 122k train, 2490 valid and 5010 test examples. As usual, NLI performance is evaluated using accuracy. To evaluate a model on a language other than English (such as French), we consider the two following settings: TRANSLATE-TEST: The French test set is machine translated into English, and then used with an English classification model. This setting provides a reasonable, although imperfect, way to circumvent the fact that",
        "type": "Document"
      },
      {
        "id": "77ad9c8e-30eb-484f-9026-bfde7c947210",
        "metadata": {
          "vector_store_key": "1809.04686-0",
          "chunk_id": 25,
          "document_id": "1809.04686",
          "start_idx": 9144,
          "end_idx": 9613
        },
        "page_content": "40 chose a subset of the SNLI test set (1332 sentences) and professionally translated it into four major languages - Arabic, French, Russian, and Spanish. We use the French test set for evaluation in Section \"Zero-Shot Classification Results\" and \"Analyses\" . Here, we first describe the model and training details of the base multilingual NMT model whose encoder is reused in all other tasks. Then we provide details about the task-specific classifiers. For each task,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What limitations are mentioned?",
    "llm_answer": "The context mentions limitations in the assumption of uniformly or geometrically distributed sequences in training sets, and the narrow and bounded evaluation schemes that often define a length threshold in the test set.\n",
    "context": [
      {
        "id": "df8143e2-10f2-44d4-a133-fc0ee02cc8e5",
        "metadata": {
          "vector_store_key": "1709.08299-3",
          "chunk_id": 19,
          "document_id": "1709.08299",
          "start_idx": 7668,
          "end_idx": 8137
        },
        "page_content": "OO, SBV, VOB, HED, FOB, IOB, POB, and only preserve the parts that have dependencies. Further Filtering: Only preserve SBV, VOB and restrict the related words not to be pronouns and verbs. Frequency Restriction: After calculating word frequencies, only word frequency that greater than 2 is valid for generating question. Question Restriction: Only five questions can be extracted within one passage. Apart from the automatically generated large-scale training data, we",
        "type": "Document"
      },
      {
        "id": "db81677a-b6fa-41e6-84b4-5e9b232778ed",
        "metadata": {
          "vector_store_key": "1606.04631-8",
          "chunk_id": 49,
          "document_id": "1606.04631",
          "start_idx": 18364,
          "end_idx": 18507
        },
        "page_content": " . In the future we will make more effort to fix these limitations and exploit the linguistic domain knowledge in visual content understanding.",
        "type": "Document"
      },
      {
        "id": "098856a9-f173-4e6f-8688-57cd5364b28d",
        "metadata": {
          "vector_store_key": "1606.04631-5",
          "chunk_id": 31,
          "document_id": "1606.04631",
          "start_idx": 11739,
          "end_idx": 12163
        },
        "page_content": "ively when the sum of the number of frames and words exceed the limit. If the number of words is within 40, we arbitrarily truncate the frames to satisfy the maximum length. When the length of sentence is more than 40, we discard the words that beyond the length and take video frames with a maximum number of 40. Bidirectional S2VT: Similar to BIBREF19 , we implement several S2VT-based models: S2VT, bidirectional S2VT and",
        "type": "Document"
      },
      {
        "id": "2603a753-8313-4563-b17d-50671c7769d8",
        "metadata": {
          "vector_store_key": "1811.01001-5",
          "chunk_id": 4,
          "document_id": "1811.01001",
          "start_idx": 1609,
          "end_idx": 2157
        },
        "page_content": "REF11 , BIBREF9 , BIBREF12 . These formulations follow narrow and bounded evaluation schemes though: They often define a length threshold in the test set and report the performance of the model on this fixed set. We acknowledge three unsettling issues with these formulations. First, the sequences in the training set are usually assumed to be uniformly or geometrically distributed, with little regard to the nature and complexity of the language. This assumption may undermine any conclusions drawn from empirical investigations, especially given",
        "type": "Document"
      },
      {
        "id": "69d22448-ad9d-4b5e-a63a-d0f312f5d16d",
        "metadata": {
          "vector_store_key": "1606.04631-5",
          "chunk_id": 30,
          "document_id": "1606.04631",
          "start_idx": 11300,
          "end_idx": 11739
        },
        "page_content": " with 16 video-sentence pairs. We note that over 99% of the descriptions in MSVD and COCO 2014 contain no more than 40 words, and in BIBREF19 , Venugopalan et al. pointed out that 94% of the YouTube training videos satisfy our maximum length limit. To ensure sufficient visual content, we adopt two ways to truncate the videos and sentences adaptively when the sum of the number of frames and words exceed the limit. If the number of words",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the baselines?",
    "llm_answer": "Baseline 1: smooth L1 distance; Baseline 2: PCA-based acoustic similarity; Baseline 3: Nonlinear dynamical systems complexity; ANRES_Sing_R; ANRES_Sing_Z; ANRES_Seq_R; ANRES_Seq_Z.\n",
    "context": [
      {
        "id": "593127ef-0325-4226-be3e-0ed371ce653f",
        "metadata": {
          "vector_store_key": "1804.08782-7",
          "chunk_id": 37,
          "document_id": "1804.08782",
          "start_idx": 13990,
          "end_idx": 14427
        },
        "page_content": " number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 .  For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose",
        "type": "Document"
      },
      {
        "id": "9028b9ee-b8b5-41cb-b460-733f242d6911",
        "metadata": {
          "vector_store_key": "1911.13066-4",
          "chunk_id": 40,
          "document_id": "1911.13066",
          "start_idx": 15067,
          "end_idx": 15591
        },
        "page_content": " dataset. Table TABREF23 presents results of baseline and all variations of our experiments. We focus our discussion on F1-score as accuracy is often misleading for dataset with unbalanced class distribution. However, for completeness sake, all measures are reported. It is observed from the results that baseline model performs worst among all the experiments. The reason behind this degradation in performance can be traced back to the nature of the texts in the datasets (i.e., datasets used in original paper of baseline",
        "type": "Document"
      },
      {
        "id": "fe0f18bc-f5e9-4a30-bd8f-1b27de376893",
        "metadata": {
          "vector_store_key": "2004.01878-5",
          "chunk_id": 40,
          "document_id": "2004.01878",
          "start_idx": 14526,
          "end_idx": 14915
        },
        "page_content": " normal distribution $\\mathcal {N}(0, 1)$. In summary, the following four baselines are designed: ANRES_Sing_R: randomly initializing the states for each single trading day. ANRES_Sing_Z: initializing the states as zeros for each single trading day. ANRES_Seq_R: randomly initializing the first states for each trading sequence only. ANRES_Seq_Z: initializing the first states as zeros for",
        "type": "Document"
      },
      {
        "id": "0b77285e-fdf5-4273-8639-5afcd5734567",
        "metadata": {
          "vector_store_key": "2001.02380-3",
          "chunk_id": 140,
          "document_id": "2001.02380",
          "start_idx": 51470,
          "end_idx": 51956
        },
        "page_content": " baselines. The left, middle and right panels in Figure FIGREF40 correspond to measurements when all signals are included, only cases contained entirely in the head EDUs shown to the model, and only DMs, respectively. The scenario on the left is rather unreasonable and is included only for completeness: here the model is also penalized for not detecting signals such as lexical chains, part of which is outside the units that the model is being shown. An example of such a case can be",
        "type": "Document"
      },
      {
        "id": "17d3256d-56f6-4b9d-983d-669c916f7fae",
        "metadata": {
          "vector_store_key": "1709.08299-2",
          "chunk_id": 23,
          "document_id": "1709.08299",
          "start_idx": 9435,
          "end_idx": 10003
        },
        "page_content": " questions for each passage The answer should be better in the type of nouns, named entities to be fully evaluated Too long or too short paragraphs should be skipped In this section, we will give several baseline systems for evaluating our datasets as well as presenting several top-ranked systems in the competition. We set several baseline systems for testing basic performance of our datasets and provide meaningful comparisons to the participant systems. In this paper, we provide four baseline systems, including two simple ones and two neural network models. The",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What semantic features help in detecting whether a piece of text is genuine or generated? of ",
    "llm_answer": "The context discusses semantic relevance as an important criterion for text-to-image synthesis, but not for detecting genuine vs. generated text.  No semantic features for authorship verification are mentioned.\n",
    "context": [
      {
        "id": "42f00a38-0611-4b86-b510-6a797ebfa4d1",
        "metadata": {
          "vector_store_key": "1710.00341-0",
          "chunk_id": 3,
          "document_id": "1710.00341",
          "start_idx": 1406,
          "end_idx": 1923
        },
        "page_content": " a claim to verify. First, we automatically convert the claim into a query, which we execute against a search engine in order to obtain a list of potentially relevant documents. Then, we take both the snippets and the most relevant sentences in the full text of these Web documents, and we compare them to the claim. The features we use are dense representations of the claim, of the snippets and of related sentences from the Web pages, which we automatically train for the task using Long Short-Term Memory networks",
        "type": "Document"
      },
      {
        "id": "0f03d853-acab-4392-8e0f-225a0a31ac83",
        "metadata": {
          "vector_store_key": "1910.09399-6",
          "chunk_id": 67,
          "document_id": "1910.09399",
          "start_idx": 26295,
          "end_idx": 26820
        },
        "page_content": " methods of each category. black Semantic relevance is one the of most important criteria of the text-to-image synthesis. For most GNAs discussed in this survey, they are required to generate images semantically related to the text descriptions. However, the semantic relevance is a rather subjective measure, and images are inherently rich in terms of its semantics and interpretations. Therefore, many GANs are further proposed to enhance the text-to-image synthesis from different perspectives. In this subsection, we will",
        "type": "Document"
      },
      {
        "id": "fcb5a971-e13d-49f1-b748-77699aa1ca03",
        "metadata": {
          "vector_store_key": "1906.10551-6",
          "chunk_id": 114,
          "document_id": "1906.10551",
          "start_idx": 42880,
          "end_idx": 43427
        },
        "page_content": " loud) or \u201ck\u201d (ok), smileys and specific obscene words. Therefore, we assume that the verification results of the examined methods are mainly caused by the similar vocabularies between the texts. We highlighted the problem that underlying characteristics of authorship verification approaches have not been paid much attention in the past research and that these affect the applicability of the methods in real forensic settings. Then, we proposed several properties that enable a better characterization and by this a better comparison between AV",
        "type": "Document"
      },
      {
        "id": "8ecabcf8-60d7-4459-9a62-312fc6d57b3d",
        "metadata": {
          "vector_store_key": "1910.11493-8",
          "chunk_id": 0,
          "document_id": "1910.11493",
          "start_idx": 0,
          "end_idx": 599
        },
        "page_content": "While producing a sentence, humans combine various types of knowledge to produce fluent output\u2014various shades of meaning are expressed through word selection and tone, while the language is made to conform to underlying structural rules via syntax and morphology. Native speakers are often quick to identify disfluency, even if the meaning of a sentence is mostly clear. Automatic systems must also consider these constraints when constructing or processing language. Strong enough language models can often reconstruct common syntactic structures, but are insufficient to properly model morphology.",
        "type": "Document"
      },
      {
        "id": "647b23c7-8fdc-40c0-b5ff-4ec974c14e97",
        "metadata": {
          "vector_store_key": "1610.00879-1",
          "chunk_id": 2,
          "document_id": "1610.00879",
          "start_idx": 758,
          "end_idx": 1216
        },
        "page_content": " datasets by using different strategies that are related to the use of hashtags. We then present SVM-based classifiers that use N-gram and stylistic features such as capitalisation, spelling errors, etc. Through our experiments, we make subtle points related to: (a) the performance of our features, (b) how our approach compares against human ability to detect drunk-texting, (c) most discriminative stylistic features, and (d) an error analysis that points",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Is the assumption that natural language is stationary and ergodic valid?",
    "llm_answer": "Manning and Sch\u00fctze argue that language can be modeled as stationary and ergodic, and the text follows this assumption.\n",
    "context": [
      {
        "id": "813e8fff-46e0-49b8-a564-29e210e6177b",
        "metadata": {
          "vector_store_key": "2002.03438-4",
          "chunk_id": 26,
          "document_id": "2002.03438",
          "start_idx": 9108,
          "end_idx": 9486
        },
        "page_content": " we assume this stationary ergodic random process with finite alphabet $\\mathcal {A}$ denoted $X = \\lbrace X_i, -\\infty < i < \\infty \\rbrace $ is non-null in the sense that always $P(x_{-m}^{-1}) > 0$ and This is sometimes called the smoothing requirement. We further introduce an additional property of random processes that we assume for language $L$. We define the continuity",
        "type": "Document"
      },
      {
        "id": "69bb2b6f-7410-4e58-a8a5-98fc8144ce1b",
        "metadata": {
          "vector_store_key": "2002.03438-4",
          "chunk_id": 25,
          "document_id": "2002.03438",
          "start_idx": 8831,
          "end_idx": 9278
        },
        "page_content": " among the class of models that are $k$-order Markov approximations of language $L$, which is simply the empirical plug-in estimate. Manning and Sch\u00fctze argue that, even though not quite correct, language text can be modeled as stationary, ergodic random processes BIBREF29, an assumption that we follow. Moreover, given the diversity of language production, we assume this stationary ergodic random process with finite alphabet $\\mathcal {A}$ den",
        "type": "Document"
      },
      {
        "id": "cd14ba3c-827d-4bc7-95fb-2550490c23a3",
        "metadata": {
          "vector_store_key": "2002.03438-4",
          "chunk_id": 28,
          "document_id": "2002.03438",
          "start_idx": 9681,
          "end_idx": 10158
        },
        "page_content": " of smoothing and continuity are taken from the literature on estimation of stationary, ergodic random processes BIBREF30. As such, the hypothesis test we aim to consider here is between a non-null, stationary, ergodic process with summable continuity rate (genuine language) and its empirical $k$-order Markov approximation based on training data (language model output). We think of the setting where the language model is trained on data with many tokens, a sequence of very",
        "type": "Document"
      },
      {
        "id": "9e519201-7926-4b4f-b812-48b5cefa5204",
        "metadata": {
          "vector_store_key": "2002.03438-4",
          "chunk_id": 27,
          "document_id": "2002.03438",
          "start_idx": 9486,
          "end_idx": 9912
        },
        "page_content": " further introduce an additional property of random processes that we assume for language $L$. We define the continuity rate of the process $X$ as: We further let $\\gamma = \\sum _{k=1}^{\\infty } \\gamma (k)$, and If $\\gamma < \\infty $, then the process has summable continuity rate. These specific technical notions of smoothing and continuity are taken from the literature on estimation of stationary, ergodic random processes",
        "type": "Document"
      },
      {
        "id": "878a2cf2-e6c5-47a9-b5a8-f3e8352cb848",
        "metadata": {
          "vector_store_key": "2002.03438-4",
          "chunk_id": 5,
          "document_id": "2002.03438",
          "start_idx": 1946,
          "end_idx": 2415
        },
        "page_content": " $k$-order Markov approximations of stationary, ergodic random processes. Results on estimation of such random processes are revisited in the context of the error probability, using a conjectured extension of the reverse Pinsker inequality. In closing, we discuss how the semantics of generated text may be a form of side information in detection. Consider a language $L$ like English, which has tokens drawn from a finite alphabet $\\mathcal {A}$; tokens can be letters",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which models do they try out?",
    "llm_answer": "Support Vector Machines (SVMs), logistic regression, Na\u00efve Bayes, Perceptron, and decision trees.\n",
    "context": [
      {
        "id": "f4579321-9662-40a1-8a0f-48bd08c87d37",
        "metadata": {
          "vector_store_key": "1909.00154-1",
          "chunk_id": 56,
          "document_id": "1909.00154",
          "start_idx": 21118,
          "end_idx": 21619
        },
        "page_content": " different experiments in the paper (and report the respective mean and standard deviation statistics). Whenever we want to analyse a particular model (e.g. to check the coefficients of a choice model), we select the one with the highest likelihood at the development set (i.e. in practice, its out-of-sample generalization performance), and report its performance on the test set. Since a choice model will typically involve other variables than the categorical ones that we learn the embeddings for,",
        "type": "Document"
      },
      {
        "id": "c94d339e-36f5-4336-a21a-05be0b487cef",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      },
      {
        "id": "08676cf7-5cf4-4a2c-b9d7-ddbe513c2062",
        "metadata": {
          "vector_store_key": "1810.00663-2",
          "chunk_id": 66,
          "document_id": "1810.00663",
          "start_idx": 25415,
          "end_idx": 25983
        },
        "page_content": " In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models. The previous section evaluated model performance on new instructions (and corresponding navigation plans) for environments that were previously seen at training time. Here, we examine whether the trained models succeed on environments that are completely new. The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as",
        "type": "Document"
      },
      {
        "id": "7a1a6801-fc34-4dd4-b446-2a2341879753",
        "metadata": {
          "vector_store_key": "1904.09708-2",
          "chunk_id": 102,
          "document_id": "1904.09708",
          "start_idx": 37855,
          "end_idx": 38357
        },
        "page_content": " is shown for each of the model's outputs (in the rows of the image) for some illustrative examples. Darker blue indicates a higher weight. The examples are shown in pairs for a model trained and tested on the add-jump split, with one example drawn from the training set and a corresponding example drawn from the test set. Examples are shown in increasing complexity, with a failure mode depicted in Figure 15 . In general, it can be seen that although the attention distributions on the test examples",
        "type": "Document"
      },
      {
        "id": "6015f6a2-fba3-4bad-a9b2-f8637220f688",
        "metadata": {
          "vector_store_key": "1911.00133-4",
          "chunk_id": 40,
          "document_id": "1911.00133",
          "start_idx": 15746,
          "end_idx": 16159
        },
        "page_content": " first experiment with a suite of non-neural models, including Support Vector Machines (SVMs), logistic regression, Na\u00efve Bayes, Perceptron, and decision trees. We tune the parameters for these models using grid search and 10-fold cross-validation, and obtain results for different combinations of input and features. For input representation, we experiment with bag-of-n-grams (for $n \\in \\lbrace 1..3\\rbrace $),",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are the competing models?",
    "llm_answer": "BiMPM, ESIM, Decomposable Attention Model, KIM, BERT, Latent-Tree, Berkeley, LaSO, Latent-Strc, Model-Stack, Non-Linear, Random Forest (with reduced features and full vocabulary), and an aggregated forecaster (Agg).\n",
    "context": [
      {
        "id": "7b6a405f-eb7a-4d59-8247-20fd3864055f",
        "metadata": {
          "vector_store_key": "1906.06448-5",
          "chunk_id": 47,
          "document_id": "1906.06448",
          "start_idx": 17189,
          "end_idx": 17518
        },
        "page_content": " five state-of-the-art NLI models adopting different approaches: BiMPM (Bilateral Multi-Perspective Matching Model; BIBREF31 , BIBREF31 ), ESIM (Enhanced Sequential Inference Model; BIBREF32 , BIBREF32 ), Decomposable Attention Model BIBREF33 , KIM (Knowledge-based Inference Model; BIBREF34 , BIBREF34 ), and BERT (Bidirectional",
        "type": "Document"
      },
      {
        "id": "692e0ed1-d4ba-47e7-a034-873c65948722",
        "metadata": {
          "vector_store_key": "1603.04553-1",
          "chunk_id": 39,
          "document_id": "1603.04553",
          "start_idx": 11513,
          "end_idx": 11902
        },
        "page_content": "IBREF28 ; Latent-Tree: the latent tree model BIBREF29 obtaining the best results in the shared task; Berkeley: the Berkeley system with the final feature set BIBREF12 ; LaSO: the structured perceptron system with non-local features BIBREF30 ; Latent-Strc: the latent structure system BIBREF31 ; Model-Stack: the entity-centric system with model stacking BIBREF32 ; and Non-Linear: the non-",
        "type": "Document"
      },
      {
        "id": "4eac1735-025c-4156-adb1-6423efb4e079",
        "metadata": {
          "vector_store_key": "1910.12618-9",
          "chunk_id": 61,
          "document_id": "1910.12618",
          "start_idx": 24043,
          "end_idx": 24512
        },
        "page_content": " the gradient descent in the NNs for instance. In order to tone it down and to increase the consistency of our results, the different models are run $B=10$ times. The results presented hereafter correspond to the average and standard-deviation on those runs. The RF model denoted as \"sel\" is the one with the reduced number of features, whereas the other RF uses the full vocabulary. We also considered an aggregated forecaster (abridged Agg), consisting of the average",
        "type": "Document"
      },
      {
        "id": "577fb06e-9340-4422-93f3-91bc89574aee",
        "metadata": {
          "vector_store_key": "2001.00137-4",
          "chunk_id": 43,
          "document_id": "2001.00137",
          "start_idx": 16781,
          "end_idx": 17197
        },
        "page_content": " the online platforms. Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector",
        "type": "Document"
      },
      {
        "id": "8f559cb6-96ff-446d-aaf3-7673d4a1d24e",
        "metadata": {
          "vector_store_key": "1908.02402-7",
          "chunk_id": 101,
          "document_id": "1908.02402",
          "start_idx": 29500,
          "end_idx": 29733
        },
        "page_content": "-the-art models, while at the same time providing a more practical solution for real-world applications. We would like to thank Alexandros Papangelis, Janice Lam, Stefan Douglas Webb and SIGDIAL reviewers for their valuable comments.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is the input triple translated to a slot-filling task?",
    "llm_answer": "The context describes slot filling as a sequence labeling problem, where a parameterized mapping function learns to map input words to slot tags.\n",
    "context": [
      {
        "id": "9f154dee-ce6e-4595-a0a7-dfb17607a90c",
        "metadata": {
          "vector_store_key": "1909.06937-0",
          "chunk_id": 11,
          "document_id": "1909.06937",
          "start_idx": 4332,
          "end_idx": 4674
        },
        "page_content": " $N$ words and its corresponding slot tags $Y^{slot} = \\lbrace y_1, y_2, \\cdots , y_N \\rbrace $, the slot filling task aims to learn a parameterized mapping function $f_{\\theta } : X \\rightarrow Y $ from input words to slot tags. For the intent detection, it is designed to predict the intent label $\\hat{y}^{int}$ for the entire utterance $X",
        "type": "Document"
      },
      {
        "id": "271e7835-d520-4dc3-aa53-958dc6cba939",
        "metadata": {
          "vector_store_key": "1911.01680-0",
          "chunk_id": 6,
          "document_id": "1911.01680",
          "start_idx": 2291,
          "end_idx": 2654
        },
        "page_content": "% error rate reduction. The task of Slot Filling is formulated as a sequence labeling problem. Deep learning has been extensively employed for this task (BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11). The prior work has mainly utilized the recurrent neural network as the encoder to extract features per word and Cond",
        "type": "Document"
      },
      {
        "id": "f5170471-b356-4ae7-b58f-f50daf1b6f8f",
        "metadata": {
          "vector_store_key": "1911.01680-0",
          "chunk_id": 0,
          "document_id": "1911.01680",
          "start_idx": 0,
          "end_idx": 403
        },
        "page_content": "Slot Filling (SF) is the task of identifying the semantic concept expressed in natural language utterance. For instance, consider a request to edit an image expressed in natural language: \u201cRemove the blue ball on the table and change the color of the wall to brown\u201d. Here, the user asks for an \"Action\" (i.e., removing) on one \u201cObject\u201d (blue ball on the table) in the image and changing an \u201cAttribute\u201d (",
        "type": "Document"
      },
      {
        "id": "2eb12bca-8707-408a-9a4b-0f9c3819845a",
        "metadata": {
          "vector_store_key": "1911.01680-0",
          "chunk_id": 47,
          "document_id": "1911.01680",
          "start_idx": 17090,
          "end_idx": 17449
        },
        "page_content": " of Slot Filling. In a multi-task setting, our model increase the mutual information between word representations and its context, improve the label information in the context and predict which concepts are expressed in the given sentence. Our experiments on an image edit request corpus shows that our model achieves state-of-the-art results on this dataset.",
        "type": "Document"
      },
      {
        "id": "2f50db15-1c22-4c72-802a-899b5863468d",
        "metadata": {
          "vector_store_key": "1910.12129-0",
          "chunk_id": 44,
          "document_id": "1910.12129",
          "start_idx": 16331,
          "end_idx": 16890
        },
        "page_content": " just a single slot, given the option. Leaving the slot selection to crowdworkers yields a frequency distribution of all slot combinations, which presumably indicates the suitability of different slots to be mentioned together in a sentence. This meta-information can be made use of in a system's dialogue manager to sample from the observed slot combination distributions instead of sampling randomly or hard-coding the combinations. Figure FIGREF30 shows the distributions of the 8 slot pairs most commonly mentioned together in different DAs. These account",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is module that analyzes behavioral state trained?",
    "llm_answer": "The behavior model is pre-trained to identify the presence of each behavior from a sequence of words using a multi-label classification scheme.  The pre-trained portion of the behavior model was implemented using a single layer RNN with LSTM units with dimension size 50.\n",
    "context": [
      {
        "id": "f91852f3-ff11-4a3d-9242-807a82fdccbc",
        "metadata": {
          "vector_store_key": "1909.00107-2",
          "chunk_id": 31,
          "document_id": "1909.00107",
          "start_idx": 12029,
          "end_idx": 12563
        },
        "page_content": " model. The behavior states are used as gating mechanism for a typical RNN based language model. We show through our experiments that the proposed technique improves language modeling perplexity specifically in the case of behavior-rich scenarios. Finally, we show improvements on the previous state-of-the-art benchmark model with Penn Tree Bank Corpus to underline the affect of behavior states in language modeling. In future, we plan to incorporate the behavior-gated language model into the task of automatic speech recognition (",
        "type": "Document"
      },
      {
        "id": "c25456e2-83aa-4000-88c2-07c93dcac008",
        "metadata": {
          "vector_store_key": "1909.00107-2",
          "chunk_id": 22,
          "document_id": "1909.00107",
          "start_idx": 8441,
          "end_idx": 8945
        },
        "page_content": "ivity, and Sadness to train our models. This is motivated from previous works which showed good separability in these behaviors as well as being easy to interpret. The behavior model is pre-trained to identify the presence of each behavior from a sequence of words using a multi-label classification scheme. The pre-trained portion of the behavior model was implemented using a single layer RNN with LSTM units with dimension size 50. We augmented previous RNN language model architectures by BIBREF1 and",
        "type": "Document"
      },
      {
        "id": "cbee24eb-6c63-48e7-bf4a-9ac62a9715ce",
        "metadata": {
          "vector_store_key": "1810.00663-4",
          "chunk_id": 10,
          "document_id": "1810.00663",
          "start_idx": 3977,
          "end_idx": 4584
        },
        "page_content": " collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation. The dataset opens up opportunities to explore data-driven methods for grounding navigation commands into high-level motion plans. We conduct extensive experiments to study the generalization capabilities of the proposed model for following natural language instructions. We investigate both generalization to new instructions in known and in new environments. We conclude this paper by discussing the benefits of",
        "type": "Document"
      },
      {
        "id": "c0d6d384-55a2-4aa5-a1fb-55dada08099b",
        "metadata": {
          "vector_store_key": "1810.00663-4",
          "chunk_id": 76,
          "document_id": "1810.00663",
          "start_idx": 29565,
          "end_idx": 30130
        },
        "page_content": " applications, including information retrieval. We proposed an end-to-end system to translate user instructions to a high-level navigation plan. Our model utilized an attention mechanism to merge relevant information from the navigation instructions with a behavioral graph of the environment. The model then used a decoder to predict a sequence of navigation behaviors that matched the input commands. As part of this effort, we contributed a new dataset of 11,051 pairs of user instructions and navigation plans from 100 different environments. Our model achieved",
        "type": "Document"
      },
      {
        "id": "5541e6ef-767d-441c-a466-e34abce86484",
        "metadata": {
          "vector_store_key": "1810.00663-4",
          "chunk_id": 77,
          "document_id": "1810.00663",
          "start_idx": 29906,
          "end_idx": 30510
        },
        "page_content": " of 11,051 pairs of user instructions and navigation plans from 100 different environments. Our model achieved the best performance in this dataset in comparison to a two-step baseline approach for interpreting navigation instructions, and a sequence-to-sequence model that does not consider the behavioral graph. Our quantitative and qualitative results suggest that attention mechanisms can help leverage the behavioral graph as a relevant knowledge base to facilitate the translation of free-form navigation instructions. Overall, our approach demonstrated practical form of learning for a complex and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Can the model add new relations to the knowledge graph, or just new entities?",
    "llm_answer": "The context discusses adding new triples, implying both entities and relations can be added.\n",
    "context": [
      {
        "id": "f798979f-f8fd-4759-b4c4-09d4d8c6e7db",
        "metadata": {
          "vector_store_key": "1910.03891-7",
          "chunk_id": 11,
          "document_id": "1910.03891",
          "start_idx": 4084,
          "end_idx": 4666
        },
        "page_content": " knowledge, most existing knowledge graph embedding methods just leverage relation triples of KGs while ignoring a large number of attribute triples. Therefore, these methods easily suffer from sparseness and incompleteness of knowledge graph. Even worse, structure information usually cannot distinguish the different meanings of relations and entities in different triples. We believe that these rich information encoded in attribute triples can help explore rich semantic information and further improve the performance of knowledge graph. For example, we can learn date of birth",
        "type": "Document"
      },
      {
        "id": "6ac29cfc-94b4-4556-92a3-520b0124e77d",
        "metadata": {
          "vector_store_key": "1910.03891-7",
          "chunk_id": 1,
          "document_id": "1910.03891",
          "start_idx": 363,
          "end_idx": 743
        },
        "page_content": "}, relation, \\textit {tail entity})$ (denoted $(h, r, t)$ in this study) through the Resource Description Framework, e.g.,$(\\textit {Donald Trump}, Born In, \\textit {New York City})$. Figure FIGREF2 shows the subgraph of knowledge graph about the family of Donald Trump. In many KGs, we can observe that some relations indicate attributes of entities, such as the $\\textit {Born}$",
        "type": "Document"
      },
      {
        "id": "568866fb-3b04-4e40-a1ea-f1e729dbc92f",
        "metadata": {
          "vector_store_key": "1706.07179-3",
          "chunk_id": 0,
          "document_id": "1706.07179",
          "start_idx": 0,
          "end_idx": 428
        },
        "page_content": "Reasoning about entities and their relations is an important problem for achieving general artificial intelligence. Often such problems are formulated as reasoning over graph-structured representation of knowledge. Knowledge graphs, for example, consist of entities and relations between them BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . Representation learning BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 and reasoning BIBREF8 , BIBREF9 ,",
        "type": "Document"
      },
      {
        "id": "92e33c2a-bf16-4999-a706-82a9464b0c51",
        "metadata": {
          "vector_store_key": "1808.09920-1",
          "chunk_id": 25,
          "document_id": "1808.09920",
          "start_idx": 9216,
          "end_idx": 9743
        },
        "page_content": " of relation (COMPLEMENT edge) between any two nodes that are not connected with any of the other relations. We can think of these edges as those in the complement set of the entity graph with respect to a fully connected graph. Our model then approaches multi-step reasoning by transforming node representations (Section \"Node annotations\" for details) with a differentiable message passing algorithm that propagates information through the entity graph. The algorithm is parameterized by a graph convolutional network (GCN) B",
        "type": "Document"
      },
      {
        "id": "f69d4760-2435-4490-80d4-271c2a934c58",
        "metadata": {
          "vector_store_key": "1711.03438-7",
          "chunk_id": 6,
          "document_id": "1711.03438",
          "start_idx": 2121,
          "end_idx": 2419
        },
        "page_content": " {E}$ , $\\mathbf {R}$ , and $\\mathbf {T}$ are the entity set, relationship set, and triple set respectively, Closed-World Knowledge Graph Completion completes $\\mathcal {G}$ by finding a set of missing triples $\\mathbf {T^\\prime }=\\lbrace \\langle h,r,t\\rangle | h\\in \\mathbf {E}, r \\in \\mathbf {R},",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Do twitter users tend to tweet about the DOS attack when it occurs? How much data supports this assumption?",
    "llm_answer": "Yes, the context states that legitimate users are willing to reveal information about service outages on social media platforms, including Twitter, and that user feedback can indicate the severity of service outages.  The authors collected 35,214 tweets related to DDoS attacks on banks.\n",
    "context": [
      {
        "id": "c0321f5c-03c0-494a-b0fa-e5822be76f4b",
        "metadata": {
          "vector_store_key": "1909.05890-3",
          "chunk_id": 13,
          "document_id": "1909.05890",
          "start_idx": 5331,
          "end_idx": 5840
        },
        "page_content": " Twitter during DoS attacks on a variety organizations from differing domains such as banking (like Bank Of America) and technology. A weakly-supervised approach to identifying detect likely DoS service related events on twitter in real-time. A score to measure impact of the DoS attack based on the frequency of user complaints about the event. The rest of this paper is organized as follows: In section 2, previous work regarding DDoS attack detection and new event detection will be discussed. In section 3",
        "type": "Document"
      },
      {
        "id": "dc22a9eb-416c-40ab-bca1-d7120076b78f",
        "metadata": {
          "vector_store_key": "1909.05890-3",
          "chunk_id": 4,
          "document_id": "1909.05890",
          "start_idx": 1577,
          "end_idx": 2133
        },
        "page_content": " of service attack always results in legitimate users being unable to access the network system or slowing down their access and they are usually willing to reveal this information on social media plaforms. Thus legitimate user feedback can be a reliable indicator about the severity level of the service outage. Thus we split this problem into two parts namely by first isolating the tweet stream that is likely related to a DoS attack and then measuring the impact of attack by analyzing the extracted tweets. A central challenge to measure the impact is",
        "type": "Document"
      },
      {
        "id": "3113e510-a220-4dd1-ae6e-b8d993f2e9ad",
        "metadata": {
          "vector_store_key": "1909.05890-2",
          "chunk_id": 12,
          "document_id": "1909.05890",
          "start_idx": 4781,
          "end_idx": 5331
        },
        "page_content": " tweet in the time window is computed using the distribution of topics for the tweet and the score of the topics. We're looking for tweets on new topics through time. While the experiments show promising results, precision can be further increased by adding a layer of a supervised classifier trained with attack data at the expense of recall. Following are the contributions in this paper: A dataset of annotated tweets extracted from Twitter during DoS attacks on a variety organizations from differing domains such as banking (like Bank Of America",
        "type": "Document"
      },
      {
        "id": "1413a21f-0dbe-4fae-bf80-d28e71855d74",
        "metadata": {
          "vector_store_key": "1909.05890-2",
          "chunk_id": 50,
          "document_id": "1909.05890",
          "start_idx": 18412,
          "end_idx": 18750
        },
        "page_content": " attack days ($D_a$) are manually annotated and only 50 percent of those tweets are actually about a DDoS attack. The 5 tweets that have the highest relative score in the dataset are: jiwa mines and miner u.s. bancorp, pnc latest bank websites to face access issues: (reuters) - some u.s. bancorp... http://bit.ly/p5xpmz u.s. bancorp, pnc",
        "type": "Document"
      },
      {
        "id": "9421e02e-5247-4ad7-afa2-357c17958596",
        "metadata": {
          "vector_store_key": "1909.05890-2",
          "chunk_id": 42,
          "document_id": "1909.05890",
          "start_idx": 15475,
          "end_idx": 15995
        },
        "page_content": " positive. Recall: Out of all of the actual attack tweets, the percentage of tweets that are labeled as attack tweets. Or true positive over true positive plus false negative. We collected tweets related to five different DDoS attacks on three different American banks. For each attack, all the tweets containing the bank's name posted from one week before the attack until the attack day were collected. There are in total 35214 tweets in the dataset. Then the collected tweets were preprocessed as mentioned in the pre",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the training and test data used?",
    "llm_answer": "The training data consists of all previous rounds of data, the Wikipedia Toxic Comments data, and the newly collected adversarial data from the \"break it\" round.  Two test sets (Test Set 1 and 2) of drug labels, with gold standard annotations, are used.  A further external dataset (NLM-180 BIBREF5) is also utilized for training.  A separate dataset with labels for age, dialect, and gender is used for training.\n",
    "context": [
      {
        "id": "c3e4cbc0-16d7-402d-bc14-7d7e21a2b655",
        "metadata": {
          "vector_store_key": "1908.06083-3",
          "chunk_id": 31,
          "document_id": "1908.06083",
          "start_idx": 12013,
          "end_idx": 12455
        },
        "page_content": " the newly collected adversarial data from the \u201cbreak it\" round. The training data consists of all previous rounds of data, so that model $A_i$ is trained on all rounds $n$ for $n \\le i$, as well as the Wikipedia Toxic Comments data. We split each round of data into train, validation, and test partitions. The validation set is used for hyperparameter selection. The test sets are used to measure how robust we are to new adversarial attacks",
        "type": "Document"
      },
      {
        "id": "4da14f73-dbf5-4b5d-8e89-f621bbabfba5",
        "metadata": {
          "vector_store_key": "1911.00133-6",
          "chunk_id": 36,
          "document_id": "1911.00133",
          "start_idx": 14430,
          "end_idx": 14888
        },
        "page_content": " content. This results in a total of 2,838 train data points (51.6% labeled stressful) and 715 test data points (52.4% labeled stressful). Because our data is relatively small, we train our traditional supervised models with 10-fold cross-validation; for our neural models, we break off a further random 10% of the training data for validation and average the predictions of 10 randomly-initialized trained models. In addition to the words of the posts (both",
        "type": "Document"
      },
      {
        "id": "94280401-a043-4e48-82e4-88f9b5ce4d3e",
        "metadata": {
          "vector_store_key": "1905.07464-5",
          "chunk_id": 10,
          "document_id": "1905.07464",
          "start_idx": 3930,
          "end_idx": 4425
        },
        "page_content": " labels, referred to as Training-22, with gold standard annotations. Two test sets of 57 and 66 drug labels, referred to as Test Set 1 and 2 respectively, with gold standard annotations are used to evaluate participating systems. As Training-22 is a relatively small dataset, we additionally utilize an external dataset with 180 annotated drug labels dubbed NLM-180 BIBREF5 (more later). We provide summary statistics about these datasets in Table TABREF3 . Test Set 1 closely resembles Training",
        "type": "Document"
      },
      {
        "id": "c94d339e-36f5-4336-a21a-05be0b487cef",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      },
      {
        "id": "b173c67e-93ea-4929-9561-19a2d01ec631",
        "metadata": {
          "vector_store_key": "1909.04181-0",
          "chunk_id": 3,
          "document_id": "1909.04181",
          "start_idx": 1283,
          "end_idx": 1837
        },
        "page_content": " experimental conditions and results. We then provide a literature review and conclude. For the purpose of our experiments, we use data released by the APDA shared task organizers. The dataset is divided into train and test by organizers. The training set is distributed with labels for the three tasks of age, dialect, and gender. Following the standard shared tasks set up, the test set is distributed without labels and participants were expected to submit their predictions on test. The shared task predictions are expected by organizers at the level",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What writing styles are present in the corpus?",
    "llm_answer": "Humor, newswire, letters, journal, non-fiction, MSA (politics, economy, sports, society), classical Arabic (religious texts).\n",
    "context": [
      {
        "id": "55e81d9d-52e3-4901-b473-e90bb0ad44c9",
        "metadata": {
          "vector_store_key": "2004.01980-9",
          "chunk_id": 18,
          "document_id": "2004.01980",
          "start_idx": 6486,
          "end_idx": 6805
        },
        "page_content": " _{i=1}^N$. The target corpus $T=\\lbrace \\mathbf {t^{(i)}}\\rbrace _{i=1}^{M}$ comprises of sentences $\\mathbf {t}$ written in a specific style (e.g., humor). We assume that it conforms to the distribution $P(T)$. Note that the target corpus $T$ only contains style-carrying sentences, not necessarily headlines \u2014 it can",
        "type": "Document"
      },
      {
        "id": "4fb20063-88d1-4f0a-a49c-446bb40296af",
        "metadata": {
          "vector_store_key": "1809.08731-7",
          "chunk_id": 24,
          "document_id": "1809.08731",
          "start_idx": 8424,
          "end_idx": 8817
        },
        "page_content": " two-sentence paragraphs from the Open American National Corpus (OANC), which belong to 4 genres: newswire, letters, journal, and non-fiction. Gold references are manually created and the outputs of 4 compression systems (ILP (extractive), NAMAS (abstractive), SEQ2SEQ (extractive), and T3 (abstractive); cf. toutanova2016dataset for details) for the test data are provided. Each example has 3",
        "type": "Document"
      },
      {
        "id": "068b4ec2-919f-45b7-8b0a-ccdad02d33e2",
        "metadata": {
          "vector_store_key": "1909.00279-4",
          "chunk_id": 18,
          "document_id": "1909.00279",
          "start_idx": 6941,
          "end_idx": 7337
        },
        "page_content": " model relies on a vernacular texts corpus $\\textbf {\\emph {S}}$ and a poem corpus $\\textbf {\\emph {T}}$. We denote $S$ and $T$ as instances in $\\textbf {\\emph {S}}$ and $\\textbf {\\emph {T}}$ respectively. The training of our model relies on three procedures, namely parameter initialization, language modeling and back-translation. We will give detailed introduction to each procedure. Parameter",
        "type": "Document"
      },
      {
        "id": "baf11d30-d577-42ea-a5b8-1bfd18a01cd0",
        "metadata": {
          "vector_store_key": "1906.10551-7",
          "chunk_id": 74,
          "document_id": "1906.10551",
          "start_idx": 28096,
          "end_idx": 28581
        },
        "page_content": " regarding the individual's writing style. As a third corpus, we compiled INLINEFORM0 , which is a collection of 200 aggregated postings crawled from the Reddit platform. Overall, the postings were written by 100 Reddit users and stem from a variety of subreddits. In order to construct the Y-cases, we selected exactly two postings from disjoint subreddits for each user such that both the known and unknown document INLINEFORM1 and INLINEFORM2 differ in their topic. Regarding the N-",
        "type": "Document"
      },
      {
        "id": "e44a060c-271a-44bc-9eb0-286e74dc3406",
        "metadata": {
          "vector_store_key": "1710.06700-1",
          "chunk_id": 22,
          "document_id": "1710.06700",
          "start_idx": 7098,
          "end_idx": 7457
        },
        "page_content": " unique surface words. About 73% of the corpus is in MSA and covers variety of genres like politics, economy, sports, society, etc. and the remaining part is mostly religious texts written in classical Arabic (CA). The effectiveness of using this corpus in building state-of-the-art diacritizer was proven in BIBREF10 .For example, the word \u00d9\u0088\u00d8\u00a8\u00d9\u0086\u00d9\u0088\u00d8\u00af> (wbnwd",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What meta-information is being transferred?",
    "llm_answer": "Relation-specific meta information, including relation meta and gradient meta.\n",
    "context": [
      {
        "id": "e0f3c5a6-3e7b-4e90-8e10-5640e654e71b",
        "metadata": {
          "vector_store_key": "1909.01515-0",
          "chunk_id": 6,
          "document_id": "1909.01515",
          "start_idx": 2143,
          "end_idx": 2735
        },
        "page_content": "-specific meta information related to the relation CEOof or CountryCapital will be extracted and transferred by MetaR from a few existing instances to incomplete triples. The relation-specific meta information is helpful in the following two perspectives: 1) transferring common relation information from observed triples to incomplete triples, 2) accelerating the learning process within one task by observing only a few instances. Thus we propose two kinds of relation-specific meta information: relation meta and gradient meta corresponding to afore mentioned two perspectives respectively",
        "type": "Document"
      },
      {
        "id": "7f20bae6-9748-48cf-8d2c-d4babca021d2",
        "metadata": {
          "vector_store_key": "1909.01515-1",
          "chunk_id": 30,
          "document_id": "1909.01515",
          "start_idx": 10253,
          "end_idx": 10790
        },
        "page_content": " loss function via gradient descending, thus gradients reveal how the model's parameters should be changed. Intuitively, we believe that gradients are valuable source to accelerate learning process. Based on these thoughts, we propose two kinds of meta information which are shared between support set and query set to deal with above problems: In order to extract relation meta and gradient mate and incorporate them with knowledge graph embedding to solve few-shot link prediction, our proposal, MetaR, mainly contains two modules: The",
        "type": "Document"
      },
      {
        "id": "70117fba-7609-4e10-b7d5-cb4bf1c94341",
        "metadata": {
          "vector_store_key": "2002.01030-9",
          "chunk_id": 13,
          "document_id": "2002.01030",
          "start_idx": 5577,
          "end_idx": 6081
        },
        "page_content": " dataset are chosen from more natural contexts such as Facebook posts, tweets, political debates, etc. They proposed neural network architecture for taking advantage of text and meta-data together. The model consists of a Convolutional Neural Network (CNN) for feature extraction from the text and a Bi-directional Long Short Term Memory (BiLSTM) network for feature extraction from the meta-data and feeds the concatenation of these two features into a fully connected softmax layer for making the final",
        "type": "Document"
      },
      {
        "id": "5330ad25-4c4c-4737-aaca-bcabae904d5e",
        "metadata": {
          "vector_store_key": "1909.01515-0",
          "chunk_id": 5,
          "document_id": "1909.01515",
          "start_idx": 1827,
          "end_idx": 2386
        },
        "page_content": " try to accomplish few-shot link prediction from another perspective based on the intuition that the most important information to be transferred from a few existing instances to incomplete triples should be the common and shared knowledge within one task. We call such information relation-specific meta information and propose a new framework Meta Relational Learning (MetaR) for few-shot link prediction. For example, in Figure 1 , relation-specific meta information related to the relation CEOof or CountryCapital will be extracted and transferred by Meta",
        "type": "Document"
      },
      {
        "id": "475f39b7-6950-436b-b06c-a722fb24022d",
        "metadata": {
          "vector_store_key": "1909.01515-0",
          "chunk_id": 9,
          "document_id": "1909.01515",
          "start_idx": 3561,
          "end_idx": 4034
        },
        "page_content": "-shot link prediction tasks. In summary, main contributions of our work are three-folds: One target of MetaR is to learn the representation of entities fitting the few-shot link prediction task and the learning framework is inspired by knowledge graph embedding methods. Furthermore, using loss gradient as one kind of meta information is inspired by MetaNet BIBREF12 and MAML BIBREF13 which explore methods for few-shot learning by meta-learning. From these two points, we",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What datasets are used to evaluate the approach?",
    "llm_answer": "Swissmetro dataset.\n",
    "context": [
      {
        "id": "b3eb1308-3e01-4903-be04-528f4258d508",
        "metadata": {
          "vector_store_key": "2002.11268-7",
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "c57177cf-018a-4129-9e33-25e62794c7f6",
        "metadata": {
          "vector_store_key": "1711.06351-7",
          "chunk_id": 62,
          "document_id": "1711.06351",
          "start_idx": 21596,
          "end_idx": 22086
        },
        "page_content": " which we evaluate quantitatively. Second, it was tasked with generating genuinely novel questions that were not present in the data set, which we evaluate qualitatively. To make predictions, the different candidate models were fit to 15 contexts and asked to predict the remaining one (i.e., leave one out cross-validation). This results in 64 different model fits (i.e., 4 models $\\times $ 16 fits). First, we verify that compositionality is an essential ingredient in an account of human",
        "type": "Document"
      },
      {
        "id": "9cdcf3d4-51f3-43d9-9cea-7a3d8cdb29a9",
        "metadata": {
          "vector_store_key": "1909.00154-2",
          "chunk_id": 61,
          "document_id": "1909.00154",
          "start_idx": 23166,
          "end_idx": 23592
        },
        "page_content": "CCM), for example. We will apply the methodology to the well-known \u201cSwissmetro\" dataset. We will compare it with a dummy variables and PCA baselines. We will follow the 3-way experimental design mentioned before: split the dataset into train, development and test sets, so that the embeddings, PCA eigenvectors and the choice model are estimated from the same train and development sets, and validate it out-of-sample. For the",
        "type": "Document"
      },
      {
        "id": "9a66176f-fe2e-4da0-a2df-70a3d739924f",
        "metadata": {
          "vector_store_key": "1802.05322-2",
          "chunk_id": 38,
          "document_id": "1802.05322",
          "start_idx": 13905,
          "end_idx": 14341
        },
        "page_content": " explanation of the whole process needed to conduct an experiment like this. The process includes downloading a data set, web scraping for extra information, data preprocessing, model tuning and evaluation of the results. Action Adult Adventure Animation Biography Comedy Crime Documentary Drama Family Fantasy Film-Noir Game-Show History Horror Music Musical Mystery Reality-TV Romance Sci-Fi Short Sport Talk-Show Thriller War Western",
        "type": "Document"
      },
      {
        "id": "9d9ab555-416a-4bc9-a24d-f0230333ee94",
        "metadata": {
          "vector_store_key": "1910.12618-4",
          "chunk_id": 49,
          "document_id": "1910.12618",
          "start_idx": 19565,
          "end_idx": 20023
        },
        "page_content": " the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014 and 2015 serve as validation set (730 days). It is used to tune the hyperparameters of the different approaches. All the data from January the 1st 2016 (974 days for France and 1,096 for the UK) is used as test set, on which the final results are presented. Grid search is applied to find the best combination of values: for each hyperparameter,",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much is performance hurt when using too small amount of layers in encoder?",
    "llm_answer": "Reducing the $enc_{src \\rightarrow mt}$ encoder block's depth reduces performance for all four scores.\n",
    "context": [
      {
        "id": "a0b37614-1a42-4a1b-a021-aa4b56a878d4",
        "metadata": {
          "vector_store_key": "1908.06151-4",
          "chunk_id": 86,
          "document_id": "1908.06151",
          "start_idx": 25676,
          "end_idx": 26094
        },
        "page_content": " Exp. 5.1 shows that decreasing the number of layers on the decoder side does not hurt the performance. In fact, in the case of test2016, we got some improvement, while for test2017, the scores got slightly worse. In contrast, reducing the $enc_{src \\rightarrow mt}$ encoder block's depth (Exp. 5.2) does indeed reduce the performance for all four scores, showing the importance of this second encoder. In Table TABREF",
        "type": "Document"
      },
      {
        "id": "b1bf4d3b-637d-4398-86b8-43383ad52204",
        "metadata": {
          "vector_store_key": "1911.02711-8",
          "chunk_id": 46,
          "document_id": "1911.02711",
          "start_idx": 16199,
          "end_idx": 16671
        },
        "page_content": " increasing layer numbers from 1 to 2. More layers do not increase the accuracy on development set. We thus set 2 as the number of review encoder layers in the experiments. The best performing model size is comparable to that of the BiLSTM self-attention, demonstrating that the number of parameters is not the key factor to models' performance. Table TABREF34 and Table TABREF35 show the final results. Our model outperforms all the baseline models and the top-performing",
        "type": "Document"
      },
      {
        "id": "418a99a9-99e7-4ef6-ab9f-a041eff21502",
        "metadata": {
          "vector_store_key": "2001.07263-5",
          "chunk_id": 45,
          "document_id": "2001.07263",
          "start_idx": 17117,
          "end_idx": 17529
        },
        "page_content": "isher data. This model consists of 10 encoder layers, and is trained for only 50 epochs. Our overall results on the Hub5'00 and other evaluation sets are summarized in Table TABREF14. The results in Fig. FIGREF12 and Table TABREF14 show that adding more training data greatly improves the system, by around 30% relative in some cases. For comparison with others, the 2000-hour system reaches 8.7% and 7.4% WER on",
        "type": "Document"
      },
      {
        "id": "1583c8f5-46f0-435d-9bc3-d77468b4e00d",
        "metadata": {
          "vector_store_key": "1908.06151-4",
          "chunk_id": 49,
          "document_id": "1908.06151",
          "start_idx": 15443,
          "end_idx": 15749
        },
        "page_content": " ($enc_{src \\rightarrow mt}$), compared to the source encoder ($enc_{src}$) and the decoder ($dec_{pe}$), by reducing and expanding the amount of layers in the encoders and the decoder. Our standard setup, which we use for fine-tuning, ensembling etc., is fixed to 6-6-6 for $N_{src}$-$N_{mt}$-$N_{pe}$ (cf",
        "type": "Document"
      },
      {
        "id": "a16514e5-02fe-4977-bcf6-e63e32040a02",
        "metadata": {
          "vector_store_key": "1910.10408-8",
          "chunk_id": 28,
          "document_id": "1910.10408",
          "start_idx": 10454,
          "end_idx": 10921
        },
        "page_content": " also provide results with systems only trained on TED Talks (small data condition). When training on large scale data we use Transformer with layer size of 1024, hidden size of 4096 on feed forward layers, 16 heads in the multi-head attention, and 6 layers in both encoder and decoder. When training only on TED talks, we set layer size of 512, hidden size of 2048 for the feed forward layers, multi-head attention with 8 heads and again 6 layers in both encoder and",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What neural machine translation models can learn in terms of transfer learning?",
    "llm_answer": "Neural machine translation models can learn to build a statistical model that aims to find the most probable target sequence for an input sequence by learning from a corpus of examples.  They can also optimize parameter sharing to maximize model capacity, potentially learning linguistic properties of the languages involved.\n",
    "context": [
      {
        "id": "8c4703cb-f09d-4f29-8b94-ceca3436426e",
        "metadata": {
          "vector_store_key": "1707.06945-0",
          "chunk_id": 42,
          "document_id": "1707.06945",
          "start_idx": 15780,
          "end_idx": 16358
        },
        "page_content": " of the specialisation and clustering algorithms may also result in improved verb class induction. We have presented a novel cross-lingual transfer model which enables the automatic induction of VerbNet-style verb classifications across multiple languages. The transfer is based on a word vector space specialisation framework, utilised to directly model the assumption of cross-linguistic validity of VerbNet-style classifications. Our results indicate strong improvements in verb classification accuracy across all six target languages explored. All automatically induced Verb",
        "type": "Document"
      },
      {
        "id": "1cbb6af6-a105-466f-9dcb-276ca1f0287f",
        "metadata": {
          "vector_store_key": "1809.04686-1",
          "chunk_id": 4,
          "document_id": "1809.04686",
          "start_idx": 1578,
          "end_idx": 2066
        },
        "page_content": " or multilingual NLP, the task of transferring knowledge from one language to another, serves as a good test bed for evaluating various transfer learning approaches. For cross-lingual NLP, the most widely studied approach is to use multilingual embeddings as features in neural network models. However, research has shown that representations learned in context are more effective BIBREF5 , BIBREF6 ; therefore, we aim at doing better than just using multilingual embeddings in the cross-",
        "type": "Document"
      },
      {
        "id": "7ab489f5-8d5a-4341-b90e-1bc75c84b5f7",
        "metadata": {
          "vector_store_key": "2002.06053-9",
          "chunk_id": 107,
          "document_id": "2002.06053",
          "start_idx": 39210,
          "end_idx": 39720
        },
        "page_content": " products). Machine translation is a challenging task because the syntactic and semantic dependencies of each language differ from one another and this may give rise to ambiguities. Neural Machine Translation (NMT) models benefit from the potential of deep learning architectures to build a statistical model that aims to find the most probable target sequence for an input sequence by learning from a corpus of examples BIBREF110, BIBREF111. The main advantage of NMT models is that they provide an end-to-end",
        "type": "Document"
      },
      {
        "id": "638b0a58-33b1-4a8f-ae4a-84d00211b966",
        "metadata": {
          "vector_store_key": "1811.02906-0",
          "chunk_id": 10,
          "document_id": "1811.02906",
          "start_idx": 4294,
          "end_idx": 4859
        },
        "page_content": " Transfer learning recently has gained a lot of attention since it can be easily applied to neural network learning architectures. For instance, Howard.2018 propose a generic transfer learning setup for text classification based on language modeling for pre-training neural models with large background corpora. To improve offensive language detection for English social media texts, a transfer learning approach was recently introduced by Felbo.2017. Their `deepmoji' approach relies on the idea to pre-train a neural network model for an actual offensive language",
        "type": "Document"
      },
      {
        "id": "b942ee80-7286-45ad-9259-5ed935a367b7",
        "metadata": {
          "vector_store_key": "1802.00273-4",
          "chunk_id": 28,
          "document_id": "1802.00273",
          "start_idx": 11704,
          "end_idx": 12295
        },
        "page_content": " on the objective to translate between all of them with a single model. The training procedure learns to map closely related languages near to each other in order to increase knowledge transfer between them. This development is very encouraging and demonstrates the ability of the neural network model to optimise parameter sharing to make most out of the model's capacity. An interesting question coming out of this study is whether such multilingual translation models can be used to learn linguistic properties of the languages involved. Making it possible to measure the distance between",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How does the semi-automatic construction process work?",
    "llm_answer": "Researchers use crowdworkers to create sentiment and semantic role-labeling systems, then construct examples to find their faults.  Models are retrained to improve them, and the build, break, and fix sequence is repeated multiple times.\n",
    "context": [
      {
        "id": "7154efd0-b0d2-4332-8c05-a3ce3b40f54f",
        "metadata": {
          "vector_store_key": "1601.01705-3",
          "chunk_id": 22,
          "document_id": "1601.01705",
          "start_idx": 8846,
          "end_idx": 9269
        },
        "page_content": " models in reverse order. We first imagine that $z$ is always observed, and in sec:model:modules describe how to evaluate and learn modules parameterized by $\\theta _e$ within fixed structures. In sec:model:assemblingNetworks, we move to the real scenario, where $z$ is unknown. We describe how to predict layouts from questions and learn $\\theta _e$ and $\\theta _\\ell $ jointly without layout supervision. Given a layout $",
        "type": "Document"
      },
      {
        "id": "e25e7a35-449b-4b1e-b26b-fad35c7f5d51",
        "metadata": {
          "vector_store_key": "1908.06083-2",
          "chunk_id": 4,
          "document_id": "1908.06083",
          "start_idx": 1550,
          "end_idx": 2021
        },
        "page_content": " researchers, \u201cbuilders\u201d and \u201cbreakers\u201d were used to first create sentiment and semantic role-labeling systems and then construct examples that find their faults. In this work we instead fully automate such an approach using crowdworkers as the humans-in-the-loop, and also apply a fixing stage where models are retrained to improve them. Finally, we repeat the whole build, break, and fix sequence over a number of iterations. We show that such an approach provides more",
        "type": "Document"
      },
      {
        "id": "75b12547-cc23-4e29-9622-0d0605c72a9e",
        "metadata": {
          "vector_store_key": "1910.11491-4",
          "chunk_id": 19,
          "document_id": "1910.11491",
          "start_idx": 7263,
          "end_idx": 7654
        },
        "page_content": " weight at $i$-th position. The model is firstly pre-trained to minimize the maximum-likelihood loss, which is widely used in sequence generation tasks. We define $y^* = \\lbrace y^*_1, \\cdots , y_T^*\\rbrace $ as the ground-truth output sequence for a given input sequence $x$, then the loss function is formulated as: After converging, the model is further optimized with local variance loss",
        "type": "Document"
      },
      {
        "id": "069c986a-be4b-4061-bfbb-a829143cc502",
        "metadata": {
          "vector_store_key": "1710.00341-4",
          "chunk_id": 2,
          "document_id": "1710.00341",
          "start_idx": 857,
          "end_idx": 1406
        },
        "page_content": " modeling. Indeed, the system makes very few assumptions about the task, and looks for supportive information directly on the Web. Our system works fully automatically. It does not use any heavy feature engineering and can be easily used in combination with task-specific approaches as well, as a core subsystem. Finally, it combines the representational strength of recurrent neural networks with kernel-based classification. The system starts with a claim to verify. First, we automatically convert the claim into a query, which we execute against",
        "type": "Document"
      },
      {
        "id": "783b961b-1ded-417c-b692-72cf6e75bb89",
        "metadata": {
          "vector_store_key": "1911.11744-0",
          "chunk_id": 27,
          "document_id": "1911.11744",
          "start_idx": 10434,
          "end_idx": 10948
        },
        "page_content": " language generator, we conducted an human-subject study to collect sentence templates of a placement task as well as common words and synonyms for each of the used features. By utilising these data, we are able to generate over 180,000 unique sentences, depending on the generated scenario. The generated parameters of the low-level DMP controller \u2013 the weights and goal position \u2013 must be sufficiently accurate in order to successfully deliver the object to the specified bin. On the right side of Figure FIGREF4",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What does \"explicitly leverages their probabilistic correlation to guide the training process of both models\" mean?",
    "llm_answer": "The framework uses the probabilistic relationship between Question Answering (QA) and Question Generation (QG) as a regularization term during training.\n",
    "context": [
      {
        "id": "84213115-083d-40ea-aeeb-19398e1695df",
        "metadata": {
          "vector_store_key": "1706.02027-6",
          "chunk_id": 6,
          "document_id": "1706.02027",
          "start_idx": 2013,
          "end_idx": 2458
        },
        "page_content": " framework that exploits the duality of QA and QG to improve both tasks. There might be different ways of exploiting the duality of QA and QG. In this work, we leverage the probabilistic correlation between QA and QG as the regularization term to influence the training process of both tasks. Specifically, the training objective of our framework is to jointly learn the QA model parameterized by $\\theta _{qa}$ and the QG model parameterized by",
        "type": "Document"
      },
      {
        "id": "5f8d3401-af16-4eb3-94f4-6610aef34d07",
        "metadata": {
          "vector_store_key": "2004.02393-5",
          "chunk_id": 30,
          "document_id": "2004.02393",
          "start_idx": 9949,
          "end_idx": 10347
        },
        "page_content": " example, we train the Ranker and Reasoner alternatively as a cooperative game: Reasoner Step: Given the first passage $p_t$ selected by the trained Ranker, the Reasoner predicts the probability of each entity $e$ appearing in $p_t$. The Reasoner is trained with the cross-entropy loss:  Ranker Step: Given the Reasoner's top-1 predicted linking entity $e$, the reward for Ranker at the $2^{\\textrm",
        "type": "Document"
      },
      {
        "id": "3403420d-7275-4475-96c4-892e57999916",
        "metadata": {
          "vector_store_key": "1910.09399-9",
          "chunk_id": 31,
          "document_id": "1910.09399",
          "start_idx": 12524,
          "end_idx": 12961
        },
        "page_content": " to become increasingly adept at its specific task until a threshold is reached. This is analogous to a min-max game between the two models, according to the following equation: In Eq. (DISPLAY_FORM10), $x$ denotes a multi-dimensional sample, e.g., an image, and $z$ denotes a multi-dimensional latent space vector, e.g., a multidimensional data point following a predefined distribution function such as that of normal distributions. $D",
        "type": "Document"
      },
      {
        "id": "ae663228-13cc-4cbf-83ab-393c494a2a00",
        "metadata": {
          "vector_store_key": "2001.02380-1",
          "chunk_id": 98,
          "document_id": "2001.02380",
          "start_idx": 38384,
          "end_idx": 38880
        },
        "page_content": " see in Section SECREF5 that model signal predictions overlap considerably with human judgments, even when the predicted relation is incorrect. Intuitively, for similar relations, such as concession or contrast, both of which are adversative, the model may notice a relevant cue (e.g. `but', or contrasting lexical items) despite choosing the wrong one. Second, as we will see below, we will be analyzing the model's behavior with respect to the probability of the correct relation, regardless of",
        "type": "Document"
      },
      {
        "id": "b0eceb09-5970-4ccd-b96c-5e7fac605833",
        "metadata": {
          "vector_store_key": "1902.00330-2",
          "chunk_id": 14,
          "document_id": "1902.00330",
          "start_idx": 5719,
          "end_idx": 6104
        },
        "page_content": " will be independently trained before the joint training process starts. In the following, we will introduce the technical details of these modules. Before introducing our model, we firstly define the entity linking task. Formally, given a document $D$ with a set of mentions $M = \\lbrace m_1, m_2,...,m_k\\rbrace $ , each mention $ m_t \\in D$ has a set of candidate entities $C_{m_t} =",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is WNGT 2019 shared task?",
    "llm_answer": "A shared task focused on reducing model size in translation performance.\n",
    "context": [
      {
        "id": "4a635258-53d2-4417-a82e-3c6f1b312499",
        "metadata": {
          "vector_store_key": "1910.07134-1",
          "chunk_id": 3,
          "document_id": "1910.07134",
          "start_idx": 1305,
          "end_idx": 1774
        },
        "page_content": ". NDNLP's submission to the 2019 WNGT Efficiency shared task uses a standard, recommended baseline Transformer network. Following BIBREF2, we investigate the application of auto-sizing to various portions of the network. Differing from their work, the shared task used a significantly larger training dataset from WMT 2014 BIBREF4, as well as the goal of reducing model size even if it impacted translation performance. Our best system was able to prune over 25% of the",
        "type": "Document"
      },
      {
        "id": "03e2632a-ede9-499a-8a49-64e026431bf3",
        "metadata": {
          "vector_store_key": "1804.08186-0",
          "chunk_id": 312,
          "document_id": "1804.08186",
          "start_idx": 114392,
          "end_idx": 114938
        },
        "page_content": " . Shared tasks are important for because they provide datasets and standardized evaluation methods that serve as benchmarks for the community. We summarize all shared tasks organized to date in sharedtasks. Generally, datasets for shared tasks have been made publicly available after the conclusion of the task, and are a good source of standardized evaluation data. However, the shared tasks to date have tended to target specific sub-problems in , and no general, broad-coverage datasets have been compiled. Widespread interest in over closely",
        "type": "Document"
      },
      {
        "id": "2e2f38b6-f339-4735-b9ba-8817249f68fd",
        "metadata": {
          "vector_store_key": "1909.04181-0",
          "chunk_id": 4,
          "document_id": "1909.04181",
          "start_idx": 1596,
          "end_idx": 2096
        },
        "page_content": " were expected to submit their predictions on test. The shared task predictions are expected by organizers at the level of users. The distribution has 100 tweets for each user, and so each tweet is distributed with a corresponding user id. As such, in total, the distributed training data has 2,250 users, contributing a total of 225,000 tweets. The official task test set contains 720,00 tweets posted by 720 users. For our experiments, we split the training data released by organizers into 90% TRA",
        "type": "Document"
      },
      {
        "id": "2ae49a36-4e88-4c8d-966d-48e1553b58f8",
        "metadata": {
          "vector_store_key": "1910.11493-1",
          "chunk_id": 30,
          "document_id": "1910.11493",
          "start_idx": 11324,
          "end_idx": 11824
        },
        "page_content": "ger as an additional attribute. The model uses hard monotonic attention instead of standard soft attention, along with a dynamic programming based training scheme. The SIGMORPHON 2019 shared task received 30 submissions\u201414 for task 1 and 16 for task 2\u2014from 23 teams. In addition, the organizers' baseline systems were evaluated. Five teams participated in the first Task, with a variety of methods aimed at leveraging the cross-lingual data to improve system performance. The University of Alberta (U",
        "type": "Document"
      },
      {
        "id": "b173c67e-93ea-4929-9561-19a2d01ec631",
        "metadata": {
          "vector_store_key": "1909.04181-0",
          "chunk_id": 3,
          "document_id": "1909.04181",
          "start_idx": 1283,
          "end_idx": 1837
        },
        "page_content": " experimental conditions and results. We then provide a literature review and conclude. For the purpose of our experiments, we use data released by the APDA shared task organizers. The dataset is divided into train and test by organizers. The training set is distributed with labels for the three tasks of age, dialect, and gender. Following the standard shared tasks set up, the test set is distributed without labels and participants were expected to submit their predictions on test. The shared task predictions are expected by organizers at the level",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Was any variation in results observed based on language typology?",
    "llm_answer": "The context mentions different results for different languages, but does not explicitly link these variations to language typology.  The answer is not found in the context.\n",
    "context": [
      {
        "id": "c2978644-732a-46d1-90c5-4ac082dfefa3",
        "metadata": {
          "vector_store_key": "1808.08780-6",
          "chunk_id": 53,
          "document_id": "1808.08780",
          "start_idx": 19773,
          "end_idx": 20259
        },
        "page_content": "ual and cross-lingual settings, we can notice that our models generally outperform the corresponding baselines. Moreover, in cases where no improvement is obtained, the differences tend to be minimal, with the exception of RG-65, but this is a very small test set for which larger variations can thus be expected. In contrast, there are a few cases where substantial gains were obtained by using our model. This is most notable for English WordSim and SimLex in the monolingual setting.",
        "type": "Document"
      },
      {
        "id": "683ec3a3-76b9-4898-ba08-4fbb51c5b45a",
        "metadata": {
          "vector_store_key": "1702.06777-4",
          "chunk_id": 37,
          "document_id": "1702.06777",
          "start_idx": 14710,
          "end_idx": 15305
        },
        "page_content": ", i.e., mostly villages and less industrialized regions. Furthermore, we have checked that the different metrics used here lead to similar results in the analysis of the lexical variation for a representative concept and provide a reasonable description to language variation in Twitter. We remark that the small amount of tweets generated after matching the lexical variations of concepts within our automatic corpus puts a limit to the quantitative analysis, making the differences between regions small. Our work might be improved by similarly examining Spanish tweets worldwide, specially in",
        "type": "Document"
      },
      {
        "id": "ecfa89ea-0dc7-427b-a302-4f91d149e29a",
        "metadata": {
          "vector_store_key": "1909.13362-7",
          "chunk_id": 61,
          "document_id": "1909.13362",
          "start_idx": 22056,
          "end_idx": 22556
        },
        "page_content": " differences in the French results can be considered negligible because the accuracies are all near $100\\%$. The Small model performed best on Manipuri, which may suggest that reducing the number of parameters of the Base model leads to better accuracy on smaller datasets. When comparing our model with previous syllabifiers, we consider the Base model exclusively. In Table TABREF26, a side-by-side comparison of our Base model to a selection of published syllabifiers shows that Base is near state",
        "type": "Document"
      },
      {
        "id": "c0037d28-11de-4a69-bcbc-17d195d64e36",
        "metadata": {
          "vector_store_key": "1911.08915-7",
          "chunk_id": 12,
          "document_id": "1911.08915",
          "start_idx": 4478,
          "end_idx": 5047
        },
        "page_content": " less similar for all the texts of the same language, provided the networks are of the same size. However, it is apparent that the distributions are different between different languages. We also compare the clustering coefficient distributions with those of the null hypothesis. The data show that the strongest differences between languages occur for the fractions of nodes with clustering coefficients 0 and 1. We build a scatter plot for these fractions for all the texts in each language. Though there is overlap between some languages, other languages are clearly",
        "type": "Document"
      },
      {
        "id": "3ce1076a-aedc-4312-83c7-87298b6454f0",
        "metadata": {
          "vector_store_key": "1710.09589-4",
          "chunk_id": 32,
          "document_id": "1710.09589",
          "start_idx": 12114,
          "end_idx": 12643
        },
        "page_content": " lower than the respective in-language model. We could investigate this as the organizers provided us with translations of French, Spanish and Japanese back to English. Averaged over all languages our system ranked first, cf. Table TABREF34 for the results of the top 5 submissions. The multilingual model reaches the overall best exact accuracy, for two languages training a in-language model would be slightly more beneficial at the cost of maintaining a separate model. The similarity-based baseline provided by the organizers",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Can the approach be generalized to other technical domains as well? ",
    "llm_answer": "The context states that models should generalize well for all domains, implying the approach *can* be generalized to other technical domains.\n",
    "context": [
      {
        "id": "06ea5aa2-4426-4c5a-be84-23196a7cf470",
        "metadata": {
          "vector_store_key": "1804.08186-0",
          "chunk_id": 448,
          "document_id": "1804.08186",
          "start_idx": 167598,
          "end_idx": 168133
        },
        "page_content": " solved problem, aspects of make it an archetypal learning task with subtleties that could be tackled by future work on supervised learning, representation learning, multi-task learning, domain adaptation, multi-label classification and other subfields of machine learning. We hope that this paper can serve as a reference point for future work in the area, both for providing insight into work to date, as well as pointing towards the key aspects that merit further investigation. This research was supported in part by the Australian",
        "type": "Document"
      },
      {
        "id": "cdb49bf5-b71c-4a98-8cc0-6f7d9d0560a7",
        "metadata": {
          "vector_store_key": "1905.06906-2",
          "chunk_id": 6,
          "document_id": "1905.06906",
          "start_idx": 2449,
          "end_idx": 2912
        },
        "page_content": " in $\\textit {Electronics}$ domain. Hence, models should generalize well for all domains. Several methods have been introduced for performing Domain Adaptation. Blitzer BIBREF12 proposed Structural Correspondence Learning (SCL) which relies on pivot features between source and target domains. Pan BIBREF13 performed Domain Adaptation using Spectral Feature Alignment (SFA) that aligns features across different domains. Glorot BIBREF14 proposed Stacked Denoising",
        "type": "Document"
      },
      {
        "id": "da57eefb-b22d-4b31-aaf3-7e7f2928c014",
        "metadata": {
          "vector_store_key": "1804.08186-0",
          "chunk_id": 447,
          "document_id": "1804.08186",
          "start_idx": 167092,
          "end_idx": 167598
        },
        "page_content": " set of \u201cbest practices\u201d, but these should be treated as guidelines and should always be considered in the broader context of a target application. Existing work on serves to illustrate that the scope and depth of the problem are much greater than they may first appear. In openissues, we discussed open issues in , identifying the key challenges, and outlining opportunities for future research. Far from being a solved problem, aspects of make it an archetypal learning task with subtleties that could be",
        "type": "Document"
      },
      {
        "id": "d5fd3822-c1e5-4d52-a60a-d0507a24043f",
        "metadata": {
          "vector_store_key": "1909.02265-3",
          "chunk_id": 10,
          "document_id": "1909.02265",
          "start_idx": 4138,
          "end_idx": 4670
        },
        "page_content": " domains. Most approaches have difficulty scaling up with multiple domains due to the dependency of their model parameters on the underlying knowledge bases. Recently, Ramadan et al. BIBREF9 has introduced a novel approach which utilizes semantic similarity between dialogue utterances and knowledge base terms, allowing the information to be shared across domains. This method has been shown not only to scale well to multi-domain dialogues, but also outperform existing state-of-the-art models in single-domain tracking tasks. The",
        "type": "Document"
      },
      {
        "id": "0a3b181b-177d-4c5b-a442-587d8e696c04",
        "metadata": {
          "vector_store_key": "1912.09152-5",
          "chunk_id": 73,
          "document_id": "1912.09152",
          "start_idx": 25875,
          "end_idx": 26391
        },
        "page_content": "design system can outperform a ner tagger for biomedical entities based on state-of-the-art artificial neural network technology. Thus, knowledge-based methods stand on their own merits in task resolution. But, perhaps most importantly, the other key point brought to light in this contribution is that a resource-based approach also favours a more critical stance on the dataset(s) used to evaluate system performance. With these methods, system development can go hand in hand with dataset refinement in a virtuous",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What dataset they use for evaluation?",
    "llm_answer": "16 Amazon review datasets, Stanford Sentiment TreeBank (SST-5/SST-2), and the IMDb Sentiment dataset.  Also, TREC question data.\n",
    "context": [
      {
        "id": "e2194e32-f011-4118-b3bf-35fb153b3017",
        "metadata": {
          "vector_store_key": "1804.08186-0",
          "chunk_id": 275,
          "document_id": "1804.08186",
          "start_idx": 98870,
          "end_idx": 99381
        },
        "page_content": " evaluation in research to date. In this section, we examine the literature more closely, providing a broad overview of the evaluation metrics that have been used, as well as the experimental settings in which research has been evaluated. The most common approach is to treat the task as a document-level classification problem. Given a set of evaluation documents, each having a known correct label from a closed set of labels (often referred to as the \u201cgold-standard\u201d), and a predicted label for each document",
        "type": "Document"
      },
      {
        "id": "9c717b01-065f-4b01-bbba-e1a5027dd747",
        "metadata": {
          "vector_store_key": "1811.09786-4",
          "chunk_id": 37,
          "document_id": "1811.09786",
          "start_idx": 12018,
          "end_idx": 12427
        },
        "page_content": " concretely, we use 16 Amazon review datasets from BIBREF32 , the well-established Stanford Sentiment TreeBank (SST-5/SST-2) BIBREF33 and the IMDb Sentiment dataset BIBREF34 . All tasks are binary classification tasks with the exception of SST-5. The metric is the accuracy score. The goal of this task is to classify questions into fine-grained categories such as number or location. We use the TREC question",
        "type": "Document"
      },
      {
        "id": "116cbe96-deb5-4d3e-b2c7-613c794a2057",
        "metadata": {
          "vector_store_key": "1804.08186-0",
          "chunk_id": 301,
          "document_id": "1804.08186",
          "start_idx": 109612,
          "end_idx": 110179
        },
        "page_content": " different authors have used to evaluate methods. BIBREF32 emphasize this by demonstrating how the performance of a system can vary according to the data used for evaluation. This implies that comparisons of results reported by different authors may not be meaningful, as a strong result in one paper may not translate into a strong result on the dataset used in a different paper. In other areas of research, authors have proposed standardized corpora to allow for the objective comparison of different methods. Some authors have released datasets to accompany their",
        "type": "Document"
      },
      {
        "id": "b3eb1308-3e01-4903-be04-528f4258d508",
        "metadata": {
          "vector_store_key": "2002.11268-7",
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "6e6ace1d-41d7-457e-a210-1e7d19864073",
        "metadata": {
          "vector_store_key": "1909.00124-3",
          "chunk_id": 39,
          "document_id": "1909.00124",
          "start_idx": 12394,
          "end_idx": 12855
        },
        "page_content": " during training. Summary statistics of the training, validation, and test data are shown in Table TABREF9. Noisy-labeled Training Datasets. For the above three domains (movie, laptop, and restaurant), we collected 2,000 reviews for each domain from the same review source. We extracted sentences from each review and assigned review's label to its sentences. Like previous work, we treat 4 or 5 stars as positive and 1 or 2 stars as negative. The data is noisy",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Which regions of the United States do they consider?",
    "llm_answer": "The Northeast, Southeast, Midwest, and West Coast.\n",
    "context": [
      {
        "id": "73283bf7-a4cf-46ab-9252-5a003d2fa964",
        "metadata": {
          "vector_store_key": "1708.05873-1",
          "chunk_id": 43,
          "document_id": "1708.05873",
          "start_idx": 17963,
          "end_idx": 18387
        },
        "page_content": " of regions: Latin America and the Caribbean (LCN), South Asia (SAS), Sub-Saharan Africa (SSA), Europe and Central Asia (ECS), Middle East and North Africa (MEA), East Asia and the Pacific (EAS), North America (NAC). The figure shows that states in South Asia, and Latin America and the Caribbean are likely to discuss Topic 2 the most. States in South Asia and East Asia and the Pacific discuss Topic 7 the most. The figure",
        "type": "Document"
      },
      {
        "id": "51f48907-afa6-4f5b-b629-d7308838d58f",
        "metadata": {
          "vector_store_key": "1904.07342-4",
          "chunk_id": 18,
          "document_id": "1904.07342",
          "start_idx": 6890,
          "end_idx": 7375
        },
        "page_content": " on a subset with geographic user information and compare results to the prior literature. In Figure FIGREF3 , we map 4-clustering results on three dimensions: predicted sentiments, latitude, and longitude. The clusters correspond to four major regions of the U.S.: the Northeast (green), Southeast (yellow), Midwest (blue), and West Coast (purple); centroids are designated by crosses. Average sentiments within each cluster confirm prior knowledge BIBREF1 : the Southeast and Midwest",
        "type": "Document"
      },
      {
        "id": "a4b3be77-3a88-46ec-b9a4-e6ac8632bd46",
        "metadata": {
          "vector_store_key": "1909.01247-6",
          "chunk_id": 33,
          "document_id": "1909.01247",
          "start_idx": 11077,
          "end_idx": 11441
        },
        "page_content": ".  U.S.A. ramane indiferent\u0103 amenin\u021b\u0103rilor Coreei de Nord. green!55!blueU.S.A. remains indifferent to North Korea's threats.  Non-geo-political locations: mountains, seas, lakes, streets, neighbourhoods, addresses, continents, regions that are not GPEs. We include regions such as Middle East, \"continents\" like Central America or East Europe. Such regions include",
        "type": "Document"
      },
      {
        "id": "ea72e437-ec4e-47e8-b6bd-d9e6d434a1df",
        "metadata": {
          "vector_store_key": "1809.02494-1",
          "chunk_id": 21,
          "document_id": "1809.02494",
          "start_idx": 8157,
          "end_idx": 8743
        },
        "page_content": " highlight the creation of models of geographical descriptors, comparing models between both subject groups, studying combinations of models of cardinal directions, and researching on geographical referring expression generation. Furthermore, insights about the semantics of geographical concepts could be inferred under a more thorough analysis. One of the inconveniences that our data sets present is the appearance of the issues described in Sec. SECREF10 . It could be necessary to filter some of the answers according to different criteria (e.g., deviation of the centroid location",
        "type": "Document"
      },
      {
        "id": "ec923dda-1bf1-4f70-ba19-513d0a0c84e8",
        "metadata": {
          "vector_store_key": "1601.01705-5",
          "chunk_id": 74,
          "document_id": "1601.01705",
          "start_idx": 25331,
          "end_idx": 25785
        },
        "page_content": "e.g. north-of, capital-of). Here we take the world representation to consist of two pieces: a set of category features (used by the find module) and a different set of relational features (used by the relate module). For our experiments, we use a subset of the features originally used by Krishnamurthy et al. The original dataset includes no quantifiers, and treats the questions What cities are in Texas? and Are there any cities in Texas? identically.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How is performance measured?",
    "llm_answer": "Performance is measured by the number of times a metric selects the best hyperparameter combination or checkpoint (out of 10 trials), and the average human-annotated score for selected models.  Speech recognition performance is evaluated by analyzing successful and unsuccessful transactions in logs.  Classifier performance varies depending on the competition (e.g., SemEval, TASS, SENTIPOLC) using metrics like accuracy or custom metrics.\n",
    "context": [
      {
        "id": "34da022a-5bec-4beb-bd76-14370cf952e1",
        "metadata": {
          "vector_store_key": "2002.05058-1",
          "chunk_id": 69,
          "document_id": "2002.05058",
          "start_idx": 27517,
          "end_idx": 28041
        },
        "page_content": " evaluation is then employed to identify the best choice. We evaluate the performance of each metric by how many times (out of 10) they succeeded in selecting the best hyperparameter combination or early-stopping checkpoint (out of 4) and the average human-annotated score for their selected models. The results are shown in Table 3. We can see that conventional automated metrics perform poorly and result in sub-optimal result when performing hyperparameter search and selecting the best performing checkpoints. Converting",
        "type": "Document"
      },
      {
        "id": "4995b0bb-117b-4711-ba2c-cbd10a77affc",
        "metadata": {
          "vector_store_key": "2004.04696-5",
          "chunk_id": 89,
          "document_id": "2004.04696",
          "start_idx": 28641,
          "end_idx": 29104
        },
        "page_content": "ing. We run training and evaluation in parallel: we run the evaluation every 1,500 steps and store the checkpoint that performs best on a held-out validation set (more details on the data splits and our choice of metrics in the following sections). We use Google Cloud TPUs v2 for learning, and Nvidia Tesla V100 accelerators for evaluation and test. Our code uses Tensorflow 1.15 and Python 2.7. The metrics used to compare the evaluation systems vary across the",
        "type": "Document"
      },
      {
        "id": "51a7992d-48ab-413d-af66-94de3136c637",
        "metadata": {
          "vector_store_key": "1601.02543-1",
          "chunk_id": 1,
          "document_id": "1601.02543",
          "start_idx": 478,
          "end_idx": 1101
        },
        "page_content": " of words for enabling a transaction. Before commercial deployment of a speech solution it is imperative to have a quantitative measure of the performance of the speech solution which is primarily based on the speech recognition accuracy of the speech engine used. Generally, the recognition performance of any speech recognition based solution is quantitatively evaluated by putting it to actual use by the people who are the intended users and then analyzing the logs to identify successful and unsuccessful transactions. This evaluation is then used to identifying any further improvement in the speech recognition based",
        "type": "Document"
      },
      {
        "id": "a3786646-1165-434a-8ae0-56260ffdae8d",
        "metadata": {
          "vector_store_key": "1612.05270-0",
          "chunk_id": 39,
          "document_id": "1612.05270",
          "start_idx": 15129,
          "end_idx": 15565
        },
        "page_content": " to test the classifier. The performance of the classifier is presented using different metrics depending the competition. SemEval uses the average of score INLINEFORM0 of positive and negative labels, TASS uses the accuracy and SENTIPOLC uses a custom metric (see BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 ). We tested our framework on two kinds of datasets. On one hand, we compare our performance on three languages having well known",
        "type": "Document"
      },
      {
        "id": "67c7bdd9-8926-4b32-b3b7-4b1914e74192",
        "metadata": {
          "vector_store_key": "1601.02543-1",
          "chunk_id": 2,
          "document_id": "1601.02543",
          "start_idx": 845,
          "end_idx": 1419
        },
        "page_content": " successful and unsuccessful transactions. This evaluation is then used to identifying any further improvement in the speech recognition based solution to better the overall transaction completion rates. This process of evaluation is both time consuming and expensive. For evaluation one needs to identify a set of users and also identify the set of actual usage situations and perform the test. It is also important that the set of users are able to use the system with ease meaning that even in the test conditions the performance of the system, should be good, while this",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is novel in author's approach?",
    "llm_answer": "The authors propose an AA approach based on time-aware language models that incorporate the temporal changes of the writing style of authors.  Their experiments focus on recognizing authorial style despite large time spans between documents.\n",
    "context": [
      {
        "id": "b3a51a4f-253b-4283-9a0a-9a10e9a110da",
        "metadata": {
          "vector_store_key": "1901.02262-2",
          "chunk_id": 84,
          "document_id": "1901.02262",
          "start_idx": 23149,
          "end_idx": 23457
        },
        "page_content": " in the answer sentence; our model will be promising for controlling other styles such as length and speaking styles. Future work will involve exploring the potential of hybrid models combining extractive and abstractive approaches and improving the passage re-ranking and answerable question identification.",
        "type": "Document"
      },
      {
        "id": "4a923d6f-83c5-4656-8465-8e3af2b5720f",
        "metadata": {
          "vector_store_key": "1906.10551-8",
          "chunk_id": 8,
          "document_id": "1906.10551",
          "start_idx": 3340,
          "end_idx": 3874
        },
        "page_content": "8 , for example, focused on the questions if the writing styles of authors of short texts change over time and how this affects AA. To answer these questions, the authors proposed an AA approach based on time-aware language models that incorporate the temporal changes of the writing style of authors. In one of our experiments, we focus on a similar question, namely, whether it is possible to recognize the writing style of authors, despite of large time spans between their documents. However, there are several differences between",
        "type": "Document"
      },
      {
        "id": "f02a90ba-bad9-41a6-b5cd-76344f5673c2",
        "metadata": {
          "vector_store_key": "1904.08386-1",
          "chunk_id": 23,
          "document_id": "1904.08386",
          "start_idx": 9489,
          "end_idx": 10035
        },
        "page_content": " we demonstrate that NLP techniques can be used to support literary analyses and obtain new insights, they also have clear limitations (e.g., in understanding abstract themes). As text representation methods become more powerful, we hope that (1) computational tools will become useful for analyzing novels with more conventional structures, and (2) literary criticism will be used as a testbed for evaluating representations. We thank the anonymous reviewers for their insightful comments. Additionally, we thank Nader Akoury, Garrett Bernstein,",
        "type": "Document"
      },
      {
        "id": "9f316cac-7c7b-4e57-8d7e-a550b2ef1825",
        "metadata": {
          "vector_store_key": "1904.08386-3",
          "chunk_id": 21,
          "document_id": "1904.08386",
          "start_idx": 8660,
          "end_idx": 9186
        },
        "page_content": " specific literary criticism about a single narrative. There has been other computational work that focuses on just a single book or a small number of books, much of it focused on network analysis: BIBREF35 extract character social networks from Alice in Wonderland, while BIBREF36 recover social networks from 19th century British novels. BIBREF37 disentangles multiple narrative threads within the novel Infinite Jest, while BIBREF7 provides several automated statistical methods for close reading and test them on the award",
        "type": "Document"
      },
      {
        "id": "56412562-e0ff-40d5-8040-4b1eec0e0c86",
        "metadata": {
          "vector_store_key": "2003.06279-8",
          "chunk_id": 85,
          "document_id": "2003.06279",
          "start_idx": 31919,
          "end_idx": 32298
        },
        "page_content": "\\langle S_L \\rangle $), measured in number of words. The following authors were considered: Hector Hugh (HH), Thomas Hardy (TH), Daniel Defoe (DD), Allan Poe (AP), Bram Stoker (BS), Mark Twain (MT), Charles Dickens (CD), Pelham Grenville (PG), Charles Darwin (CD), Arthur Doyle (AD), George Eliot (GE), Jane Austen (JA), and Joseph Conrad (JC). In this section we show additional",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How large is the Dialog State Tracking Dataset?",
    "llm_answer": "Over 16,000 dialogues.\n",
    "context": [
      {
        "id": "62ead5ea-9ceb-4e09-8a3d-4819eab9f141",
        "metadata": {
          "vector_store_key": "1909.11833-1",
          "chunk_id": 8,
          "document_id": "1909.11833",
          "start_idx": 3127,
          "end_idx": 3573
        },
        "page_content": " than the previously best model GLAD, with only 22% of the number of parameters. On DSTC2 dataset, SIM achieves comparable performance with previous best models with only 19% of the model size. As outlined in BIBREF9, the dialogue state tracking task is formulated as follows: at each turn of dialogue, the user's utterance is semantically decoded into a set of slot-value pairs. There are two types of slots. Goal slots indicate the category, e.",
        "type": "Document"
      },
      {
        "id": "1274389f-4ac9-41cc-84bf-27d3a584b81c",
        "metadata": {
          "vector_store_key": "1707.05853-1",
          "chunk_id": 19,
          "document_id": "1707.05853",
          "start_idx": 6842,
          "end_idx": 7275
        },
        "page_content": " provided for the second Dialog State Tracking Challenge (DSTC2) BIBREF22 that consists of user interactions with an SDS in the restaurant domain. It encompasses 1612, 506, 1117 dialogs for training, development and testing, respectively. Every dialog turn is annotated with its dialog state encompassing the three goals for area (7 values), food (93 values) and price range (5 values) and 8 requestable slots, e.g. phone and address",
        "type": "Document"
      },
      {
        "id": "318f767d-561b-42bd-84b9-757129ef6caf",
        "metadata": {
          "vector_store_key": "2002.01359-4",
          "chunk_id": 34,
          "document_id": "2002.01359",
          "start_idx": 14455,
          "end_idx": 14981
        },
        "page_content": "ased into a natural conversation by crowd workers. We ensure that the annotations for the dialogue state and slots generated by the simulator are preserved and hence need no other annotation. We omit details for brevity: please refer to BIBREF19 for more details. The entire dataset consists of over 16K dialogues spanning multiple domains. Overall statistics of the dataset and comparison with other datasets can be seen in Table TABREF9. Figure FIGREF8 shows the details of the distribution of dialogue lengths across single",
        "type": "Document"
      },
      {
        "id": "1953582a-9bbe-4f0b-a91f-46babe3fdc12",
        "metadata": {
          "vector_store_key": "1909.05855-5",
          "chunk_id": 32,
          "document_id": "1909.05855",
          "start_idx": 13044,
          "end_idx": 13583
        },
        "page_content": " more data with the same resources. Furthermore, it is important to note that this entire procedure preserves all other annotations obtained from the simulator including the dialogue state. Hence, no further annotation is needed. With over 16000 dialogues in the training set, the Schema-Guided Dialogue dataset is the largest publicly available annotated task-oriented dialogue dataset. The annotations include the active intents and dialogue states for each user utterance and the system actions for every system utterance. We have a few",
        "type": "Document"
      },
      {
        "id": "8d74cda5-d178-4a3f-9c55-bad03c7d6f24",
        "metadata": {
          "vector_store_key": "2002.11893-2",
          "chunk_id": 86,
          "document_id": "2002.11893",
          "start_idx": 32022,
          "end_idx": 32535
        },
        "page_content": " filled, and all cross-domain informable slots are filled by real values. Result Analysis: During the evaluation, we initialized the user state of the simulator using the previous gold user state. The input to the simulator is the gold system dialogue acts. We used joint state accuracy (percentage of exact matching) to evaluate user state prediction and F1 score to evaluate the prediction of user dialogue acts. The results are presented in Table TABREF31. We can observe that the performance on complex dialog",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What dataset is used for train/test of this method?",
    "llm_answer": "The provided text describes using multiple datasets:  Wikipedia Toxic Comments data,  \"break it\" round data (multiple rounds), Training-22, NLM-180, Test Set 1 and Test Set 2.\n",
    "context": [
      {
        "id": "c94d339e-36f5-4336-a21a-05be0b487cef",
        "metadata": {
          "vector_store_key": "1909.02764-1",
          "chunk_id": 50,
          "document_id": "1909.02764",
          "start_idx": 19785,
          "end_idx": 20208
        },
        "page_content": " experiments: (1) in-domain, (2) out-of-domain and (3) transfer learning. For all experiments we train on the classes anger/annoyance, fear/insecurity and joy. Table TABREF19 shows all results for the comparison of these experimental settings. We first set a baseline by validating our models on established corpora. We train the baseline model on 60 % of each data set listed in Table TABREF12 and evaluate that model with",
        "type": "Document"
      },
      {
        "id": "b3eb1308-3e01-4903-be04-528f4258d508",
        "metadata": {
          "vector_store_key": "2002.11268-7",
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "c3e4cbc0-16d7-402d-bc14-7d7e21a2b655",
        "metadata": {
          "vector_store_key": "1908.06083-3",
          "chunk_id": 31,
          "document_id": "1908.06083",
          "start_idx": 12013,
          "end_idx": 12455
        },
        "page_content": " the newly collected adversarial data from the \u201cbreak it\" round. The training data consists of all previous rounds of data, so that model $A_i$ is trained on all rounds $n$ for $n \\le i$, as well as the Wikipedia Toxic Comments data. We split each round of data into train, validation, and test partitions. The validation set is used for hyperparameter selection. The test sets are used to measure how robust we are to new adversarial attacks",
        "type": "Document"
      },
      {
        "id": "c8978676-7096-4886-9737-ff5b0d065d87",
        "metadata": {
          "vector_store_key": "1909.02635-8",
          "chunk_id": 72,
          "document_id": "1909.02635",
          "start_idx": 27869,
          "end_idx": 27993
        },
        "page_content": " testbed supported by the National Science Foundation. Thanks as well to the anonymous reviewers for their helpful comments.",
        "type": "Document"
      },
      {
        "id": "94280401-a043-4e48-82e4-88f9b5ce4d3e",
        "metadata": {
          "vector_store_key": "1905.07464-5",
          "chunk_id": 10,
          "document_id": "1905.07464",
          "start_idx": 3930,
          "end_idx": 4425
        },
        "page_content": " labels, referred to as Training-22, with gold standard annotations. Two test sets of 57 and 66 drug labels, referred to as Test Set 1 and 2 respectively, with gold standard annotations are used to evaluate participating systems. As Training-22 is a relatively small dataset, we additionally utilize an external dataset with 180 annotated drug labels dubbed NLM-180 BIBREF5 (more later). We provide summary statistics about these datasets in Table TABREF3 . Test Set 1 closely resembles Training",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much is the gap between using the proposed objective and using only cross-entropy objective?",
    "llm_answer": "The context states that the proposed mixed objective converges quickly and outperforms the purely cross-entropy model.  However, it does not quantify the gap.\n",
    "context": [
      {
        "id": "9672c115-ca12-4684-aca6-cb1ee96dbb6c",
        "metadata": {
          "vector_store_key": "1711.00106-6",
          "chunk_id": 29,
          "document_id": "1711.00106",
          "start_idx": 9499,
          "end_idx": 9980
        },
        "page_content": " span produced by the model and the ground truth answer span. We note that there is a disconnect between the cross entropy optimization objective and the evaluation metrics. For example, suppose we are given the answer estimates $A$ and $B$ , neither of which match the ground truth positions. However, $A$ has an exact string match with the ground truth answer whereas $B$ does not. The cross entropy objective penalizes $A$ and $B$ equally, despite the former being correct under",
        "type": "Document"
      },
      {
        "id": "1d3d64c5-9a09-4a11-a94a-108f70aea91b",
        "metadata": {
          "vector_store_key": "1711.00106-6",
          "chunk_id": 55,
          "document_id": "1711.00106",
          "start_idx": 16878,
          "end_idx": 17429
        },
        "page_content": " without mixing in the cross entropy loss, it is extremely difficult to learn the policy. When we combine the cross entropy loss with the reinforcement learning objective, we find that the model initially performs worse early on as it begins policy learning from scratch (shown in Figure 4 ). However, with the addition of cross entropy loss, the model quickly learns a reasonable policy and subsequently outperforms the purely cross entropy model (shown in Figure 4 ). Figure 5 compares predictions by and by the baseline on the development set. Both",
        "type": "Document"
      },
      {
        "id": "bfc64272-f1e7-4121-abf6-6e070dd2a1d4",
        "metadata": {
          "vector_store_key": "1711.00106-6",
          "chunk_id": 30,
          "document_id": "1711.00106",
          "start_idx": 9782,
          "end_idx": 10226
        },
        "page_content": " cross entropy objective penalizes $A$ and $B$ equally, despite the former being correct under both evaluation metrics. In the less extreme case where $A$ does not have exact match but has some degree of word overlap with the ground truth, the F1 metric still prefers $A$ over $B$ despite its wrongly predicted positions. We encode this preference using reinforcement learning, using the F1 score as the reward function. Let $\\hat{s_t} \\sim _t$",
        "type": "Document"
      },
      {
        "id": "b032ac97-cc7f-4e8d-a510-5f93e2c32d74",
        "metadata": {
          "vector_store_key": "1711.00106-6",
          "chunk_id": 3,
          "document_id": "1711.00106",
          "start_idx": 1258,
          "end_idx": 1826
        },
        "page_content": " traditional cross entropy based training it is no better than the span \u201chistory\u201d. To address this problem, we propose a mixed objective that combines traditional cross entropy loss over positions with a measure of word overlap trained with reinforcement learning. We obtain the latter objective using self-critical policy learning in which the reward is based on word overlap between the proposed answer and the ground truth answer. Our mixed objective brings two benefits: (i) the reinforcement learning objective encourages answers that are textually similar to the",
        "type": "Document"
      },
      {
        "id": "9d2797af-37f2-4bb6-8d0e-0e00f470767d",
        "metadata": {
          "vector_store_key": "1711.00106-6",
          "chunk_id": 4,
          "document_id": "1711.00106",
          "start_idx": 1618,
          "end_idx": 2146
        },
        "page_content": " brings two benefits: (i) the reinforcement learning objective encourages answers that are textually similar to the ground truth answer and discourages those that are not; (ii) the cross entropy objective significantly facilitates policy learning by encouraging trajectories that are known to be correct. The resulting objective is one that is both faithful to the evaluation metric and converges quickly in practice. In addition to our mixed training objective, we extend the Dynamic Coattention Network (DCN) by BIBREF0 with a",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many domains of ontologies do they gather data from?",
    "llm_answer": "17\n",
    "context": [
      {
        "id": "184ae66f-21b7-4546-b8c2-7fd69bfd9f1c",
        "metadata": {
          "vector_store_key": "1909.03242-3",
          "chunk_id": 19,
          "document_id": "1909.03242",
          "start_idx": 7530,
          "end_idx": 7943
        },
        "page_content": " of instances, as well as labels per domain, are shown in Table TABREF34 and label names in Table TABREF43 in the appendix. The dataset is split into a training part (80%) and a development and testing part (10% each) in a label-stratified manner. Note that the domains vary in the number of labels, ranging from 2 to 27. Labels include both straight-forward ratings of veracity (`correct', `incorrect'), but also",
        "type": "Document"
      },
      {
        "id": "09ed3a1e-6632-4d06-835d-1a20fa2f8026",
        "metadata": {
          "vector_store_key": "1811.09529-2",
          "chunk_id": 17,
          "document_id": "1811.09529",
          "start_idx": 6119,
          "end_idx": 6486
        },
        "page_content": "T-Box). As a result we selected 5 ontologies with 234 competency questions in total. Table TABREF8 summarizes our dataset size and source of each ontology. The Software Ontology (SWO) BIBREF5 is included because its set of CQs is of substantial size and it was part of Ren et al.'s set of analysed CQs. The CQ sets of Dem@Care BIBREF8 and OntoDT BIBREF9 were included",
        "type": "Document"
      },
      {
        "id": "16a1c7fa-0b69-4d22-bda3-fec2e0c59828",
        "metadata": {
          "vector_store_key": "1909.00754-8",
          "chunk_id": 55,
          "document_id": "1909.00754",
          "start_idx": 15273,
          "end_idx": 15680
        },
        "page_content": ", WoZ2.0 BIBREF19 . It consists of 1,200 dialogues from the restaurant reservation domain with three pre-defined slots: food, price range, and area. Since the name slot rarely occurs in the dataset, it is not included in our experiments, following previous literature BIBREF3 , BIBREF20 . Our model is also tested on the multi-domain dataset, MultiWoZ BIBREF9 . It has a more complex ontology with 7 domains",
        "type": "Document"
      },
      {
        "id": "bdf6f5d2-ea30-458f-be20-3cda0397fbf5",
        "metadata": {
          "vector_store_key": "1903.00172-0",
          "chunk_id": 64,
          "document_id": "1903.00172",
          "start_idx": 22362,
          "end_idx": 22720
        },
        "page_content": " Kumar. 2018. Enriching domain ontologies using question-answer datasets. In Proc. CoDS-COMAD '18, pages 329\u2013332. Swarnadeep Saha, Harinder Pal, et al. 2017. Bootstrapping for numerical open ie. In Proc. ACL '17, pages 317\u2013323. Denis Savenkov, Wei-Lwun Lu, Jeff Dalton, and Eugene Agichtein. 2015. Relation extraction from community generated question-answer",
        "type": "Document"
      },
      {
        "id": "fbcb265a-0153-4ead-bc9a-f5d2879a4be3",
        "metadata": {
          "vector_store_key": "1909.05855-6",
          "chunk_id": 14,
          "document_id": "1909.05855",
          "start_idx": 5622,
          "end_idx": 6170
        },
        "page_content": " of multiple services per domain, are representative of these scale-related challenges. Furthermore, our evaluation sets contain many services, and consequently slots, which are not present in the training set, to help evaluate model performance on unseen services. The 17 domains (`Alarm' domain not included in training) present in our dataset are listed in Table TABREF5. We create synthetic implementations of a total of 34 services or APIs over these domains. Our simulator framework interacts with these services to generate dialogue outlines",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what is the practical application for this paper?",
    "llm_answer": "The practical application includes testing generalization more rigorously, better performance on a different meaning representation language, solving representative problems from a synthetic dataset, helping with low-resource language modeling problems, and improving confidence measures in related applications like information retrieval and speaker adaptation.  Also,  the paper addresses biomedical text analysis.\n",
    "context": [
      {
        "id": "4bfeaa8b-12e6-44d4-b764-f2ca5e9ab27e",
        "metadata": {
          "vector_store_key": "2003.06279-0",
          "chunk_id": 11,
          "document_id": "2003.06279",
          "start_idx": 4620,
          "end_idx": 5004
        },
        "page_content": " present Zipf's Law BIBREF21 and the analysis of linguistic properties of natural and unknown texts BIBREF22, BIBREF23. Applications of network science in text mining and text classification encompasses applications in semantic analysis BIBREF24, BIBREF25, BIBREF26, BIBREF27, authorship attribution BIBREF28, BIBREF29 and stylometry BIBREF28, BIBREF30, BIBREF31. Here we focus in the",
        "type": "Document"
      },
      {
        "id": "b165fe68-0cc6-435a-a29a-ef5094d6fa9b",
        "metadata": {
          "vector_store_key": "1904.09545-2",
          "chunk_id": 7,
          "document_id": "1904.09545",
          "start_idx": 2867,
          "end_idx": 3427
        },
        "page_content": " on a different split of the data designed to test generalization more rigorously, and better performance on a different meaning representation language. Outside of semantic parsing, it solves two representative problems from the scan dataset of BIBREF8 that are synthetic but precise in the notion of compositionality they test. Finally, it helps with some (unconditional) low-resource language modeling problems in a typologically diverse set of languages. Recent years have seen tremendous success at natural language transduction and generation tasks using",
        "type": "Document"
      },
      {
        "id": "c5eaf502-a11a-40bd-84a8-78f62e8d8d02",
        "metadata": {
          "vector_store_key": "1701.00185-9",
          "chunk_id": 10,
          "document_id": "1701.00185",
          "start_idx": 3790,
          "end_idx": 4304
        },
        "page_content": " this paper we use a new short text dataset, Biomedical, in the experiment to verify the effectiveness of our approach. Third, we put much effort on studying the influence of various different semantic features integrated in our self-taught CNN framework, which is not involved in the conference paper. For the purpose of reproducibility, we make the datasets and software used in our experiments publicly available at the website. The remainder of this paper is organized as follows: In Section SECREF2 , we first",
        "type": "Document"
      },
      {
        "id": "3b9bc27b-8604-4608-9f48-314a80fd898e",
        "metadata": {
          "vector_store_key": "1912.03457-5",
          "chunk_id": 43,
          "document_id": "1912.03457",
          "start_idx": 18264,
          "end_idx": 18855
        },
        "page_content": " are accessible to the population, the technology must be effectively deployed. Specialized applications are useless if they are not deployed properly in a way that accesses their end-users. When deploying a specially developed technology, the application must be deployed with consideration of the existing community dynamics. For any deployment to be successful, it usually must be able to be purposefully integrated into the lifestyle of community members - or have strong utilization incentives if it is a transformative technology. In this section, we will review examples of technology",
        "type": "Document"
      },
      {
        "id": "cb29be02-4115-407e-acc6-30f01d24b5c2",
        "metadata": {
          "vector_store_key": "1810.13024-9",
          "chunk_id": 69,
          "document_id": "1810.13024",
          "start_idx": 21048,
          "end_idx": 21224
        },
        "page_content": "ices. Many related applications like information retrieval, speaker adaptation, keyword spotting and semi-supervised training will benefit from the improved confidence measure.",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What's the method used here?",
    "llm_answer": "A multi-task learning framework with passage ranking for evidence extraction and a sequence-to-sequence model for answer synthesis.\n",
    "context": [
      {
        "id": "a47a2b60-4b1a-43d0-b971-5f0833a0421f",
        "metadata": {
          "vector_store_key": "2002.11268-2",
          "chunk_id": 20,
          "document_id": "2002.11268",
          "start_idx": 6009,
          "end_idx": 6300
        },
        "page_content": " target domain posterior as: with $k(X) = p_{\\psi }(X) / p_{\\tau }(X)$ shared by all hypotheses $W$, and the ratio $P_{\\tau }(W) / {P_{\\psi }(W)}$ (really a probablity mass ratio) giving the proposed method its name. In essence, this model is just an application of Bayes' rule to end-to-end",
        "type": "Document"
      },
      {
        "id": "d3ecd185-dec9-402a-9bf7-0f559407d4c1",
        "metadata": {
          "vector_store_key": "1706.04815-0",
          "chunk_id": 45,
          "document_id": "1706.04815",
          "start_idx": 12072,
          "end_idx": 12345
        },
        "page_content": " (1-y_i)\\log (1-\\hat{g}_i)]$$   (Eq. 23)  where $k$ is the number of passages. $y_i \\in \\lbrace 0,1\\rbrace $ denotes a label. $y_i=1$ means $P_i$ is the correct passage, otherwise $y_i=0$ . The evident extraction part is trained by minimizing joint objective functions:  $$",
        "type": "Document"
      },
      {
        "id": "14db9e0e-4a42-467f-aa63-2247a10ce336",
        "metadata": {
          "vector_store_key": "1909.09270-9",
          "chunk_id": 19,
          "document_id": "1909.09270",
          "start_idx": 7446,
          "end_idx": 7869
        },
        "page_content": " training process (line 4) is weighted, using weights $V$. At the start, these can be all 1 (Raw), or can be initialized with prior knowledge. The learned model is then used to predict on all of $T$ (line 5). In the inference step (line 6), we take the predictions from the prior round and the constraints $C$ and produce a new labeling on $T$, and a new set of weights $V$. The details of this inference step are presented",
        "type": "Document"
      },
      {
        "id": "0c6f06cc-2953-4285-84a9-5100ee127f17",
        "metadata": {
          "vector_store_key": "1905.13497-3",
          "chunk_id": 24,
          "document_id": "1905.13497",
          "start_idx": 8250,
          "end_idx": 8720
        },
        "page_content": ", and the matrices involved. We evaluate our method on two commonsense reasoning tasks, PDP and WSC. On the former task, we use the original set of 60 questions (PDP-60) as the main benchmark. The second task (WSC-273) is qualitatively much more difficult. The recent best reported result are not much above random guess. This task consists of 273 questions and is designed to work against traditional linguistic techniques, common heuristics or simple statistical tests",
        "type": "Document"
      },
      {
        "id": "6870b31f-0951-46f9-aa38-84e0db2093cb",
        "metadata": {
          "vector_store_key": "1706.04815-1",
          "chunk_id": 90,
          "document_id": "1706.04815",
          "start_idx": 25770,
          "end_idx": 26320
        },
        "page_content": " The extraction model aims to match the question and passage and predict most important sub-spans in the passage related to the question as evidence. Then, the synthesis model synthesizes the question information and the evidence snippet to generate the final answer. We propose a multi-task learning framework to improve the evidence extraction model by passage ranking to extract the evidence snippet, and use the sequence-to-sequence model for answer synthesis. We conduct experiments on the MS-MARCO dataset. Results demonstrate that our approach",
        "type": "Document"
      }
    ]
  },
  {
    "query": "By how much does their method outperform state-of-the-art OOD detection?",
    "llm_answer": "More than 17 points in Precision@K on average.\n",
    "context": [
      {
        "id": "4eafd4b1-3e24-4421-a16c-d71e7f3450c0",
        "metadata": {
          "vector_store_key": "1905.10247-1",
          "chunk_id": 34,
          "document_id": "1905.10247",
          "start_idx": 12345,
          "end_idx": 12839
        },
        "page_content": " turns to foster further research. In the presence of OOD utterances, our method outperforms state-of-the-art dialog models equipped with an OOD detection mechanism by a large margin \u2014 more than 17 points in Precision@K on average \u2014 while minimizing performance trade-off on in-domain test data. The detailed analysis sheds light on the difficulty of optimizing context-independent OOD detection and justifies the necessity of context-aware OOD handling models. We plan to explore other ways of",
        "type": "Document"
      },
      {
        "id": "f6ebdacb-65c9-47c7-83cf-f1287511d1d7",
        "metadata": {
          "vector_store_key": "1905.10247-1",
          "chunk_id": 33,
          "document_id": "1905.10247",
          "start_idx": 12036,
          "end_idx": 12546
        },
        "page_content": " superior stability nicely contrasts with the high sensitivity of AE-HCN-Indep with regard to threshold values as shown in Table TABREF25 . We proposed a novel OOD detection method that does not require OOD data without any restrictions by utilizing counterfeit OOD turns in the context of a dialog. We also release new dialog datasets which are three publicly available dialog corpora augmented with natural OOD turns to foster further research. In the presence of OOD utterances, our method outperforms state",
        "type": "Document"
      },
      {
        "id": "6632cff0-a1f5-4499-9358-55cd641fd8c6",
        "metadata": {
          "vector_store_key": "1905.10247-3",
          "chunk_id": 28,
          "document_id": "1905.10247",
          "start_idx": 10154,
          "end_idx": 10567
        },
        "page_content": " on in-domain training data. AE-HCN(-CNN) outperforms HCN on Test-OOD by a large margin about 17(20) points on average while keeping the minimum performance trade-off compared to Test. Interestingly, AE-HCN-CNN has even better performance than HCN on Test, indicating that, with the CNN encoder, counterfeit OOD augmentation acts as an effective regularization. In contrast, AE-HCN-Indep failed to robustly detect",
        "type": "Document"
      },
      {
        "id": "516a333c-94d2-49fb-91fb-4fc15f5b57fa",
        "metadata": {
          "vector_store_key": "1905.10247-3",
          "chunk_id": 31,
          "document_id": "1905.10247",
          "start_idx": 11300,
          "end_idx": 11764
        },
        "page_content": " Table TABREF25 shows the search for the best threshold value for AE-HCN-Indep on the bAbI6 task when given actual OOD utterances (which is highly unrealistic for the real-world scenario). Note that the best performance achieved at 9 is still not as good as that of AE-HCN(-CNN). This implies that we can perform better OOD detection by jointly considering other context features. Finally, we conduct a sensitivity analysis by varying counterfeit OOD probabilities",
        "type": "Document"
      },
      {
        "id": "06218e60-04f5-4943-8dd6-e3b1e2702661",
        "metadata": {
          "vector_store_key": "1905.10247-1",
          "chunk_id": 27,
          "document_id": "1905.10247",
          "start_idx": 9866,
          "end_idx": 10321
        },
        "page_content": " are multiple actions that are appropriate for a given dialog context, we use per-utterance Precision@K as performance metric. We also report f1-score for OOD detection to measure the balance between precision and recall. The performances of HCN on Test-OOD are about 15 points down on average from those on Test, showing the detrimental impact of OOD utterances to such models only trained on in-domain training data. AE-HCN(-CNN) outperforms HCN on Test",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What are dilated convolutions?",
    "llm_answer": "Dilated convolutions skip some input values so that the convolution kernel is applied over a larger area than its own, increasing the receptive field without increasing the number of parameters.\n",
    "context": [
      {
        "id": "5a7504ee-5ee2-4a24-9fe8-cdb505b1d0b6",
        "metadata": {
          "vector_store_key": "1811.07684-2",
          "chunk_id": 11,
          "document_id": "1811.07684",
          "start_idx": 4383,
          "end_idx": 4825
        },
        "page_content": " increase in computational cost yielded by larger receptive fields. Dilated convolutions skip some input values so that the convolution kernel is applied over a larger area than its own. The network therefore operates on a larger scale, without the downside of increasing the number of parameters. The receptive field $r$ of a network made of stacked convolutions indeed reads: $r = \\sum _i d_i (s_i - 1),$  where $d_i$ refers to the dilation",
        "type": "Document"
      },
      {
        "id": "6d6ccdc3-f1ab-4f29-b21a-7ab9e23a496a",
        "metadata": {
          "vector_store_key": "1910.08210-3",
          "chunk_id": 30,
          "document_id": "1910.08210",
          "start_idx": 11739,
          "end_idx": 12233
        },
        "page_content": " visual inputs with height $H$, width $W$, and $_$ channels. Let $$ denote a convolution layer. Let + and * symbols denote element-wise addition and multiplication operations that broadcast over spatial dimensions. We first modulate visual features using text features: Unlike FiLM, we additionally modulate text features using visual features: The output of the layer consists of the sum of the modulated features $$, as well as a max-pooled summary $$ over this sum across spatial dimensions.",
        "type": "Document"
      },
      {
        "id": "83d38b91-209d-48e0-a8e7-1468cdde7a42",
        "metadata": {
          "vector_store_key": "1811.07684-2",
          "chunk_id": 12,
          "document_id": "1811.07684",
          "start_idx": 4658,
          "end_idx": 5030
        },
        "page_content": " (s_i - 1),$  where $d_i$ refers to the dilation rate ( $d_i=1$ for normal convolutions) and $s_i$ the filter size of the $i^{th}$ layer. Additionally, causal convolutions kernels ensure a causal ordering of input frames: the prediction emitted at time $t$ only depends on previous time stamps. It allows to reduce the latency at inference time. As mentioned in BIBREF10 ,",
        "type": "Document"
      },
      {
        "id": "702c3b7e-b313-4cb0-bf2f-2dbe41ce0ce8",
        "metadata": {
          "vector_store_key": "2001.06888-9",
          "chunk_id": 42,
          "document_id": "2001.06888",
          "start_idx": 15719,
          "end_idx": 16083
        },
        "page_content": ",k)}], \\dots , [c_{(n,0)}, c_{(n,1)}, \\dots , c_{(n,k)}] \\rangle $ and in order to apply one dimensional convolution, it is required to be in a fixed length. $k$ shows the fixed length of the character sequence representing each word. Rather than using the one-hot representation of characters, a randomly initialized (uniform distribution) embedding layer is used",
        "type": "Document"
      },
      {
        "id": "bffa02b5-a73c-4aa2-8157-459ab070df48",
        "metadata": {
          "vector_store_key": "1603.04513-8",
          "chunk_id": 25,
          "document_id": "1603.04513",
          "start_idx": 9584,
          "end_idx": 9897
        },
        "page_content": " {R}^{d}$ denotes the embedding of word $w_i$ ; a convolution layer uses sliding filters to extract local features of that sentence. The filter width $l$ is a parameter. We first concatenate the initialized embeddings of $l$ consecutive words ( $\\mathbf {w}_{i-l+1},\n\\ldots , \\mathbf {w}_i$ ) as $\\mathbf {c}_i\\in",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what are the three methods presented in the paper?",
    "llm_answer": "The provided texts describe multiple methods, but not three specific ones.  The answer is not found in the context.\n",
    "context": [
      {
        "id": "b3eb1308-3e01-4903-be04-528f4258d508",
        "metadata": {
          "vector_store_key": "2002.11268-7",
          "chunk_id": 66,
          "document_id": "2002.11268",
          "start_idx": 20893,
          "end_idx": 21138
        },
        "page_content": " fine-tuning data is used. Evaluation using a variety of cross-domain evaluation scenarios is needed to establish the general effectiveness of the method. The authors thank Matt Shannon and Khe Chai Sim for valuable feedback regarding this work.",
        "type": "Document"
      },
      {
        "id": "3eeea765-3316-41b9-a57f-b8efe1a730a5",
        "metadata": {
          "vector_store_key": "1908.08419-0",
          "chunk_id": 10,
          "document_id": "1908.08419",
          "start_idx": 3629,
          "end_idx": 4094
        },
        "page_content": " summarized as follows: The rest of this paper is organized as follows. Section SECREF2 briefly reviews the related work on CWS and active learning. Section SECREF3 presents an active learning method for CWS. We experimentally evaluate our proposed method in Section SECREF4 . Finally, Section SECREF5 concludes the paper and envisions on future work. In past decades, researches on CWS have a long history and various methods have been proposed BIBREF13 , BIBREF14",
        "type": "Document"
      },
      {
        "id": "8effb31e-542b-4bc0-af4b-bf493806990a",
        "metadata": {
          "vector_store_key": "1906.10551-4",
          "chunk_id": 6,
          "document_id": "1906.10551",
          "start_idx": 2469,
          "end_idx": 3019
        },
        "page_content": " contribute to the further development of this young research field. Based on the proposed properties, we investigate the applicability of 12 existing AV approaches on three self-compiled corpora, where each corpus involves a specific challenge. The rest of this paper is structured as follows. Section SECREF2 discusses the related work that served as an inspiration for our analysis. Section SECREF3 comprises the proposed criteria and properties to characterize AV methods. Section SECREF4 describes the methodology, consisting of the used corpora",
        "type": "Document"
      },
      {
        "id": "ad7bfde5-e163-4e4d-afc4-8231ffb2df82",
        "metadata": {
          "vector_store_key": "1909.02265-0",
          "chunk_id": 16,
          "document_id": "1909.02265",
          "start_idx": 6691,
          "end_idx": 7196
        },
        "page_content": " on. Next, Section SECREF3 presents experimental settings and results. Finally, Section SECREF4 concludes the paper and gives some directions for future work. In this section, we present briefly two methods that we use in our experiments which have been mentioned in the previous section. The first method is the Sequicity framework and the second one is the state-of-the-art multi-domain dialogue state tracking approach. Figure FIGREF1 shows the architecture of the Sequicity framework as described in B",
        "type": "Document"
      },
      {
        "id": "e47b2d6a-c071-4a61-b692-68b484884e3d",
        "metadata": {
          "vector_store_key": "1908.07816-7",
          "chunk_id": 10,
          "document_id": "1908.07816",
          "start_idx": 4017,
          "end_idx": 4509
        },
        "page_content": " relate. It is the first time such an approach is designed with consideration for the human judges. Our main goal is to increase the objectivity of the results and reduce judges' mistakes due to out-of-context dialogs they have to evaluate. The rest of the paper unfolds as follows. Section SECREF2 discusses some related work. In Section SECREF3, we give detailed description of the methodology. We present experimental results and some analysis in Section SECREF4. The paper is concluded in",
        "type": "Document"
      }
    ]
  },
  {
    "query": "what datasets did the authors use?",
    "llm_answer": "The authors used three datasets: a first dataset, a SCI dataset of 220 sentences from the scientific literature, and a novel-9 dataset of 19 novels.\n",
    "context": [
      {
        "id": "0d593f74-7474-41cc-9928-ce68ac96f714",
        "metadata": {
          "vector_store_key": "1802.05574-2",
          "chunk_id": 17,
          "document_id": "1802.05574",
          "start_idx": 6551,
          "end_idx": 7065
        },
        "page_content": " the dataset. This choice allows for a rough comparison between our results and theirs. The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. This corpus is derived from the 10 most published in disciplines. It includes 11 articles each from the following domains: agriculture, astronomy, biology, chemistry, computer science, earth science, engineering, materials science, math, and medicine. The article text is made freely available",
        "type": "Document"
      },
      {
        "id": "189f2e62-5589-4ac4-a44d-70623d5e2128",
        "metadata": {
          "vector_store_key": "1709.02271-8",
          "chunk_id": 31,
          "document_id": "1709.02271",
          "start_idx": 11255,
          "end_idx": 11734
        },
        "page_content": " We begin by introducing the datasets (Section SECREF15 ), followed by detailing the featurization methods (Section SECREF17 ), the experiments (Section SECREF22 ), and finally reporting results (Section SECREF26 ). The statistics for the three datasets used in the experiments are summarized in Table TABREF16 . novel-9. This dataset was compiled by F&H14: a collection of 19 novels by 9 nineteenth century British and American authors in the Project Gutenberg. To compare to F&",
        "type": "Document"
      },
      {
        "id": "47700f54-52ba-40b9-bf2e-4fde786f8ac4",
        "metadata": {
          "vector_store_key": "1912.09152-4",
          "chunk_id": 75,
          "document_id": "1912.09152",
          "start_idx": 26669,
          "end_idx": 26983
        },
        "page_content": " a true Gold Standard Dataset. We thank three anonymous reviewers of our manuscript for their careful reading and their many insightful comments and suggestions. We have made our best in providing a revised version of the manuscript that reflects their suggestions. Any remaining errors are our own responsability.",
        "type": "Document"
      },
      {
        "id": "7fe095b0-eec5-46a0-b9ba-5537c38f62c3",
        "metadata": {
          "vector_store_key": "1909.05855-5",
          "chunk_id": 69,
          "document_id": "1909.05855",
          "start_idx": 26527,
          "end_idx": 26688
        },
        "page_content": "-of-the-art results. The authors thank Guan-Lin Chao for help with model design and implementation, and Amir Fayazi and Maria Wang for help with data collection.",
        "type": "Document"
      },
      {
        "id": "c3435ddc-e5cd-4c15-88a6-79bf7cb93888",
        "metadata": {
          "vector_store_key": "1911.03562-5",
          "chunk_id": 4,
          "document_id": "1911.03562",
          "start_idx": 1524,
          "end_idx": 2018
        },
        "page_content": " are based on information about the papers taken directly from AA (as of June 2019) and citation information extracted from Google Scholar (as of June 2019). Thus, all subsequent papers and citations are not included in the analysis. A fresh data collection is planned for January 2020.  Interactive Visualizations: The visualizations we are developing for this work (using Tableau) are interactive\u2014so one can hover, click to select and filter, move sliders, etc. Since this work is high in the",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How much performance improvements they achieve on SQuAD?",
    "llm_answer": "F1 score improved by 11.8 points in Japanese SQuAD.\n",
    "context": [
      {
        "id": "771cc327-fd2d-413c-8bc2-a1b2fa71292b",
        "metadata": {
          "vector_store_key": "1912.10435-2",
          "chunk_id": 4,
          "document_id": "1912.10435",
          "start_idx": 1471,
          "end_idx": 1945
        },
        "page_content": " augmented the SQUAD2.0 dataset with additional backtranslated examples. This augmented dataset will be publicly available on our github BIBREF4 on the completion of this course. After performing hyperparameter tuning, we ensembled our two best networks to get F1 and EM scores of 82.317 and 79.442 respectively. The experiments took around 300 GPU hours to train. The SQUAD2.0 creators proposed this dataset as a means for networks to actually understand the text they were",
        "type": "Document"
      },
      {
        "id": "dc199ef9-84f9-4aae-9f86-07e6e302124e",
        "metadata": {
          "vector_store_key": "2001.11268-7",
          "chunk_id": 74,
          "document_id": "2001.11268",
          "start_idx": 29425,
          "end_idx": 29920
        },
        "page_content": " use of original SQuAD domains to enrich our data. This was done in order to save computing resources, as an addition of 100 SQuAD domains resulted in training time increases of two hours, depending on various other parameter settings. Adjusted parameters include increased batch size, and decreased maximal context length in order to reduce training time. With this paper we aimed to explore state-of-the-art NLP methods to advance systematic review (semi)automation. Both of the presented fine",
        "type": "Document"
      },
      {
        "id": "a6919a6a-45c0-433b-8b40-024f70b24ca0",
        "metadata": {
          "vector_store_key": "1902.00821-2",
          "chunk_id": 78,
          "document_id": "1902.00821",
          "start_idx": 23708,
          "end_idx": 24133
        },
        "page_content": " evaluation of SQuAD 2.0, CoQA script reports turn-level Exact Match (EM) and F1 scores for all turns in all dialogues. EM requires the answers to have exact string match with human annotated answer spans. F1 score is the averaged F1 scores of individual answers, which is typically higher than EM and is the major metric. As shown in Table 3 , BERT+Pre-tuning has significant performance gains over many baselines. To answer",
        "type": "Document"
      },
      {
        "id": "4c1919f6-562d-42db-aa2a-998a2ba8e7ce",
        "metadata": {
          "vector_store_key": "1902.00821-2",
          "chunk_id": 77,
          "document_id": "1902.00821",
          "start_idx": 23287,
          "end_idx": 23708
        },
        "page_content": " better results. We fine-tune 6 epochs, though most runs converged just within 3 epochs due to the pre-trained/tuned weights of BERT. Results are reported as averages of 3 runs of fine-tuning (3 different random seeds for tuning batch generation). To be consistent with existing research, we leverage the same evaluation script from CoQA. Similar to the evaluation of SQuAD 2.0, CoQA script reports turn-level Exact Match",
        "type": "Document"
      },
      {
        "id": "f1cd90f7-c5d6-4a2d-9478-0de113d2dbe5",
        "metadata": {
          "vector_store_key": "1909.00361-7",
          "chunk_id": 55,
          "document_id": "1909.00361",
          "start_idx": 20160,
          "end_idx": 20623
        },
        "page_content": "en}}$) is higher than previous work (Back-Translation BIBREF10), when using SimpleMatch to extract original passage span could obtain competitive of even larger improvements. In Japanese SQuAD, the F1 score improved by 9.6 in BIBREF10 using NMT attention, while we obtain larger improvement with 11.8 points demonstrating the effectiveness of the proposed method. BERT with pre-trained SQuAD weights yields the best performance among these systems, as it does not",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the baseline?",
    "llm_answer": "The context describes several different baselines, including ones using smooth L1 distance, PCA-based acoustic similarity, nonlinear dynamical systems, and different initialization methods for ANRES.  It also mentions content, content-network, and image-based models as baselines.\n",
    "context": [
      {
        "id": "593127ef-0325-4226-be3e-0ed371ce653f",
        "metadata": {
          "vector_store_key": "1804.08782-7",
          "chunk_id": 37,
          "document_id": "1804.08782",
          "start_idx": 13990,
          "end_idx": 14427
        },
        "page_content": " number of baseline measures: Baseline 1: smooth L1 distance directly computed between turn-level features ( INLINEFORM0 and INLINEFORM1 ) Baseline 2: PCA-based symmetric acoustic similarity measure by Lee et al. BIBREF10  Baseline 3: Nonlinear dynamical systems-based complexity measure BIBREF6 .  For the baselines, we conduct the classification experiments in a similar manner. Since Baseline 1 and 2 have multiple measures, we choose",
        "type": "Document"
      },
      {
        "id": "9028b9ee-b8b5-41cb-b460-733f242d6911",
        "metadata": {
          "vector_store_key": "1911.13066-4",
          "chunk_id": 40,
          "document_id": "1911.13066",
          "start_idx": 15067,
          "end_idx": 15591
        },
        "page_content": " dataset. Table TABREF23 presents results of baseline and all variations of our experiments. We focus our discussion on F1-score as accuracy is often misleading for dataset with unbalanced class distribution. However, for completeness sake, all measures are reported. It is observed from the results that baseline model performs worst among all the experiments. The reason behind this degradation in performance can be traced back to the nature of the texts in the datasets (i.e., datasets used in original paper of baseline",
        "type": "Document"
      },
      {
        "id": "0b77285e-fdf5-4273-8639-5afcd5734567",
        "metadata": {
          "vector_store_key": "2001.02380-3",
          "chunk_id": 140,
          "document_id": "2001.02380",
          "start_idx": 51470,
          "end_idx": 51956
        },
        "page_content": " baselines. The left, middle and right panels in Figure FIGREF40 correspond to measurements when all signals are included, only cases contained entirely in the head EDUs shown to the model, and only DMs, respectively. The scenario on the left is rather unreasonable and is included only for completeness: here the model is also penalized for not detecting signals such as lexical chains, part of which is outside the units that the model is being shown. An example of such a case can be",
        "type": "Document"
      },
      {
        "id": "fe0f18bc-f5e9-4a30-bd8f-1b27de376893",
        "metadata": {
          "vector_store_key": "2004.01878-5",
          "chunk_id": 40,
          "document_id": "2004.01878",
          "start_idx": 14526,
          "end_idx": 14915
        },
        "page_content": " normal distribution $\\mathcal {N}(0, 1)$. In summary, the following four baselines are designed: ANRES_Sing_R: randomly initializing the states for each single trading day. ANRES_Sing_Z: initializing the states as zeros for each single trading day. ANRES_Seq_R: randomly initializing the first states for each trading sequence only. ANRES_Seq_Z: initializing the first states as zeros for",
        "type": "Document"
      },
      {
        "id": "25bb2afe-7835-48e9-a6f8-dbdbbde2116d",
        "metadata": {
          "vector_store_key": "1902.06843-8",
          "chunk_id": 91,
          "document_id": "1902.06843",
          "start_idx": 35139,
          "end_idx": 35416
        },
        "page_content": "s by -0.27. Baselines: To test the efficacy of our multi-modal framework for detecting depressed users, we compare it against existing content, content-network, and image-based models (based on the aforementioned general image feature, facial presence, and facial expressions.)",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the network architecture?",
    "llm_answer": "The network architecture consists of five main modules: an input module, an attention module, a reasoning module, a modeling module, and an output module.  It is built using tensorflow/keras, drawing inspiration from BERT-keras and keras-contrib.\n",
    "context": [
      {
        "id": "2e27377a-5499-4f47-9c90-c8a9a21ce330",
        "metadata": {
          "vector_store_key": "1909.08859-1",
          "chunk_id": 11,
          "document_id": "1909.08859",
          "start_idx": 4603,
          "end_idx": 5140
        },
        "page_content": " with an explicit reasoning module that acts on entity-specific relational memory units. Fig. FIGREF4 shows an overview of the network architecture. It consists of five main modules: An input module, an attention module, a reasoning module, a modeling module, and an output module. Note that the question answering tasks we consider here are multimodal in that while the context is a procedural text, the question and the multiple choice answers are composed of images. Input Module extracts vector representations of inputs at different",
        "type": "Document"
      },
      {
        "id": "76df7fe7-d53f-4a67-b7d1-6f3e7704a95e",
        "metadata": {
          "vector_store_key": "2002.11402-2",
          "chunk_id": 15,
          "document_id": "2002.11402",
          "start_idx": 5576,
          "end_idx": 5982
        },
        "page_content": " as explained above before passing to the network. Architecture is built using tensorflow/keras. Coding inspiration taken from BERT-keras and for CRF layer keras-contrib. If one is more comfortable in pytorch there are many examples available on github, but pytorch-bert-crf-ner is better for an easy start. We used BERT-Multilingual model so that we can train and fine-tune the same model for other Indian",
        "type": "Document"
      },
      {
        "id": "2479a645-a808-4a2b-bd3c-d81c6ee08cf8",
        "metadata": {
          "vector_store_key": "1912.13109-4",
          "chunk_id": 23,
          "document_id": "1912.13109",
          "start_idx": 9357,
          "end_idx": 9891
        },
        "page_content": " Vo et al and Mathur et al. This was also used as a baseline model but we did not get appreciable performance on such architecture due to FC networks not being able to capture local and long term dependencies. Convolution based architectures: Architecture and hyperparameter choices were chosen from the past study Deon on the subject. We were able to boost the performance as compared to only FC based network but we noticed better performance from architectures that are suitable to sequences such as text messages or any timeseries",
        "type": "Document"
      },
      {
        "id": "ba658f37-20f9-42c3-af8c-3f377ad4cd03",
        "metadata": {
          "vector_store_key": "1901.00570-9",
          "chunk_id": 16,
          "document_id": "1901.00570",
          "start_idx": 6305,
          "end_idx": 6842
        },
        "page_content": " networks are (1) content analysis, where the textual content of each post is analyzed using natural language processing to identify the topic or the sentiment of the authors. (2) Network structure analysis, where the relation between the users are described in a tree structure for the follower-followee patterns, or in a graph structure for friendship and interaction patterns. These patterns can be used to know the political preference of people prior to elections. (3) Behavioural analysis of each user including sentiment, response",
        "type": "Document"
      },
      {
        "id": "e3cd6a68-f763-489d-8ef4-89b597712f14",
        "metadata": {
          "vector_store_key": "1710.09340-3",
          "chunk_id": 36,
          "document_id": "1710.09340",
          "start_idx": 13393,
          "end_idx": 13706
        },
        "page_content": "ES-UDC (FFI2014-51978-C2-2-R) and ANSWER-ASAP (TIN2017-85160-C2-1-R) projects from MINECO, and from Xunta de Galicia (ED431B 2017/01). We provide more details of the neural network architecture used in this paper, which is taken from Qi2017. The model consists of two blocks of 2-layered bidirectional long short-",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What does recurrent deep stacking network do?",
    "llm_answer": "RDSN stacks and concatenates the outputs of previous frames into the input features of the current frame.  It utilizes phoneme information in previous frames as additional inputs to the raw features, transforming the Acoustic Model into a hybrid model with an N-gram Language Model.\n",
    "context": [
      {
        "id": "c4e62949-3ced-46f5-842d-98c9aab7a319",
        "metadata": {
          "vector_store_key": "1612.04675-0",
          "chunk_id": 4,
          "document_id": "1612.04675",
          "start_idx": 1527,
          "end_idx": 2023
        },
        "page_content": " Model into a hybrid model consisted of an Acoustic Model and a simple N-gram Language Model on phoneme level. In the next section, we will explain the framework of RDSN and tricks to compress the outputs. Then we will show the experimental results and make a conclusion. As indicated in its name, Recurrent Deep Stacking Network stacks and concatenates the outputs of previous frames into the input features of the current frame. If we view acoustic models in ASR systems as functions projecting",
        "type": "Document"
      },
      {
        "id": "7c090fb4-84ab-405a-8eca-9c1ae3d85cc3",
        "metadata": {
          "vector_store_key": "1612.04675-1",
          "chunk_id": 11,
          "document_id": "1612.04675",
          "start_idx": 3695,
          "end_idx": 4220
        },
        "page_content": " other output dimensions that have the same corresponding monophone. This process compresses the dimension from thousands to about forty. Compressing the output dimensions enables us to add multiple recurrent outputs and keep the input vector dense. Originated from a similar idea as of RDSN, a BiPass Stacking Network (BPSN) takes as input both conventional features and the outputs of previous frames. But the way BiPass Stacking Network generates the representations of previous frames is through a two-pass scheme similar",
        "type": "Document"
      },
      {
        "id": "92b5399b-c2b4-429a-866f-0fd282847bcf",
        "metadata": {
          "vector_store_key": "1612.04675-0",
          "chunk_id": 3,
          "document_id": "1612.04675",
          "start_idx": 1221,
          "end_idx": 1729
        },
        "page_content": " The models in this paper belong to the back-end methods. Inspired by recent progresses in Natural Language Processing BIBREF5 , we proposed the Recurrent Deep Stacking Network (RDSN) and successfully applied it to Speech Enhancement tasks. RDSN utilizes the phoneme information in previous frames as additional inputs to the raw features. From another perspective of view, this framework transformed the Acoustic Model into a hybrid model consisted of an Acoustic Model and a simple N-gram Language Model on",
        "type": "Document"
      },
      {
        "id": "46fcef9a-4476-41b8-a623-774679caa1c3",
        "metadata": {
          "vector_store_key": "1612.09113-1",
          "chunk_id": 2,
          "document_id": "1612.09113",
          "start_idx": 815,
          "end_idx": 1357
        },
        "page_content": " Recurrent Neural Network (RNN), where junior tasks in the hierarchy are supervised on inner layers and the parameters are jointly optimised during training. Joint optimisation within a hierarchical network acts as a form of regularisation in two ways: first, it forces the network to learn general representations within the parameters of the shared hidden layers BIBREF0 ; second, there is a penalty on the supervised junior layers for forming a representation and making predictions that are inconsistent with senior tasks. Intuitively, we",
        "type": "Document"
      },
      {
        "id": "a9bf9d07-ee9c-4e7d-998f-feebc0538416",
        "metadata": {
          "vector_store_key": "1612.04675-0",
          "chunk_id": 18,
          "document_id": "1612.04675",
          "start_idx": 6261,
          "end_idx": 6709
        },
        "page_content": " in the first epoch, the weights corresponding to the additional inputs were all random. In this paper, we proposed a Recurrent Deep Stacking Network (RDSN) based speech recognition system and an efficient substitute to RDSN, BiPass Stacking Network (BPSN). These two models convert the pure acoustic model into a hybrid structure consisted of both an acoustic model and a phoneme-level N-gram model. Note that both RDSN and BPSN can be extended to",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What is the reward model for the reinforcement learning appraoch?",
    "llm_answer": "A balanced reward: maximizing the probability of finding an answer to the generated question within the input paragraph but not inside the source sentence.\n",
    "context": [
      {
        "id": "9a146d33-54fe-4db7-8283-3acad4704dc2",
        "metadata": {
          "vector_store_key": "1702.03274-2",
          "chunk_id": 55,
          "document_id": "1702.03274",
          "start_idx": 20589,
          "end_idx": 21098
        },
        "page_content": " for the system to continue to learn autonomously using reinforcement learning (RL). With RL, each turn receives a measurement of goodness called a reward; the agent explores different sequences of actions in different situations, and makes adjustments so as to maximize the expected discounted sum of rewards, which is called the return, denoted $G$ . For optimization, we selected a policy gradient approach BIBREF34 , which has been successfully applied to dialog systems BIBREF35 , robotics BIBREF36 , and",
        "type": "Document"
      },
      {
        "id": "8bb9a9aa-4293-48a9-8f97-cfc281e5a121",
        "metadata": {
          "vector_store_key": "1911.03350-5",
          "chunk_id": 52,
          "document_id": "1911.03350",
          "start_idx": 19179,
          "end_idx": 19669
        },
        "page_content": " the models appear to learn to exploit the weaknesses of the reward source. To overcome this issue, we propose to use a balanced reward: thus maximizing the probability of finding an answer to the generated question within the input paragraph but not inside the source sentence. In our experiments, we follow the approach proposed by BIBREF18, BIBREF29, considering a mixed loss $L_{ml+rl}$ which combines supervised and reinforcement learning schemes: where the maximum likelihood $L_{ml}$",
        "type": "Document"
      },
      {
        "id": "20b8ec83-cb62-4e9d-bd50-91b5234703f2",
        "metadata": {
          "vector_store_key": "1909.00542-1",
          "chunk_id": 43,
          "document_id": "1909.00542",
          "start_idx": 16697,
          "end_idx": 17106
        },
        "page_content": " for the reward function used by a reinforcement learning system. Reinforcement learning gives promising results, especially in human evaluations made on the runs submitted to BioASQ 6b. This year we introduced very small changes to the runs using reinforcement learning, and will aim to explore more complex reinforcement learning strategies and more complex neural models in the policy and value estimators.",
        "type": "Document"
      },
      {
        "id": "51efb65b-2c96-40aa-8bcb-81cc71a95e2f",
        "metadata": {
          "vector_store_key": "1902.00330-5",
          "chunk_id": 101,
          "document_id": "1902.00330",
          "start_idx": 34293,
          "end_idx": 34753
        },
        "page_content": " state to help disambiguation. In the last few years, reinforcement learning has emerged as a powerful tool for solving complex sequential decision-making problems. It is well known for its great success in the game field, such as Go BIBREF37 and Atari games BIBREF38 . Recently, reinforcement learning has also been successfully applied to many natural language processing tasks and achieved good performance BIBREF12 , BIBREF39 , BIBREF5 . Feng et al. BIBREF",
        "type": "Document"
      },
      {
        "id": "d3172ba9-84aa-4c03-9c11-e7bfb19ebcfe",
        "metadata": {
          "vector_store_key": "1909.09484-5",
          "chunk_id": 44,
          "document_id": "1909.09484",
          "start_idx": 15129,
          "end_idx": 15611
        },
        "page_content": " get the rewards at each decoding step, we sample some results $A_t$ after choosing $y_j$, and the reward of $y_j$ is set as the average of all the sampled results' rewards. In order to ensure that the model's performance is stable during the fine-tuning phase of reinforcement learning, we freeze the parameters of user utterance and dialogue belief tracker, only the parameters of the dialogue policy maker will be optimized by reinforcement learning. Policy gradient algorithm RE",
        "type": "Document"
      }
    ]
  },
  {
    "query": "Does this paper propose a new task that others can try to improve performance on?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "cbe9f208-3730-4128-8f81-ac3a378ea889",
        "metadata": {
          "vector_store_key": "1707.03764-0",
          "chunk_id": 16,
          "document_id": "1707.03764",
          "start_idx": 6168,
          "end_idx": 6712
        },
        "page_content": " out to be detrimental to performance, when compared to the simpler system that we have described in Section SECREF2 . We divide our attempts according to the different ways we attempted to enhance performance: manipulating the data itself (adding more, and changing preprocessing), using a large variety of features, and changing strategies in modelling the problem by using different algorithms and paradigms. All reported results are on the PAN 2017 training data using five-fold cross-validation, unless otherwise specified. We extended the",
        "type": "Document"
      },
      {
        "id": "4b141b0d-d314-40fc-9296-a5e710f9793b",
        "metadata": {
          "vector_store_key": "1910.06748-4",
          "chunk_id": 10,
          "document_id": "1910.06748",
          "start_idx": 3889,
          "end_idx": 4391
        },
        "page_content": " benchmarks on our Twitter datasets, our proposed model consistently achieves new state-of-the-art results with an improvement of 5% in accuracy and F1 score and a competitive inference time. The rest of this paper is structured as follows. After a background review in the next section, we will present our Twitter dataset in Section SECREF3. Our novel LID algorithm will be the topic of Section SECREF4. We will then present and analyze some experiments we conducted with our algorithm in Section SEC",
        "type": "Document"
      },
      {
        "id": "6e9ecc0b-428f-4a10-b5ea-73fb0f5fbfe5",
        "metadata": {
          "vector_store_key": "1810.12885-0",
          "chunk_id": 5,
          "document_id": "1810.12885",
          "start_idx": 1760,
          "end_idx": 2277
        },
        "page_content": " something complicated when they read'. If a system performs as well as a human on a task, the conclusion shouldn't immediately be that the task is too easy, it should more subtly be that new datasets are then needed to see if the inference mechanisms hold up, where the creation of the datasets can be based based on an explicitly stated intuition that humans may rely on more than pattern matching. It is a hypothesis at this point in the Introduction, that systems doing well on earlier datasets won't also do well",
        "type": "Document"
      },
      {
        "id": "2eed5d51-4dfd-4622-8141-f8d71567d458",
        "metadata": {
          "vector_store_key": "1611.03382-9",
          "chunk_id": 59,
          "document_id": "1611.03382",
          "start_idx": 21894,
          "end_idx": 22215
        },
        "page_content": " significantly speed up inference. We have demonstrated the effectiveness of our approach in the context of summarization and shown state-of-the-art performance. In the future, we plan to tackle summarization problems with large input text. We also plan to exploit our findings in other tasks such as machine translation.",
        "type": "Document"
      },
      {
        "id": "d1a71d42-d3ab-42c6-b1ae-2992a6a73474",
        "metadata": {
          "vector_store_key": "1909.05016-7",
          "chunk_id": 19,
          "document_id": "1909.05016",
          "start_idx": 6972,
          "end_idx": 7434
        },
        "page_content": " will advance faster. This provides us an incremental and iterative approach to build our architecture starting with the most important components, always testing included components before advancing to new ones. Our main novelties include: [topsep=2pt,itemsep=-1ex,partopsep=1ex,parsep=1ex] Using self-play strategies to train a neural response ranker. Computing a large number of metrics for both input and response, and specifically optimizing some models for",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How big is their dataset?",
    "llm_answer": "18,200,000 tweets; 34,918 claims; 287,226 documents; 0.83 million words.\n",
    "context": [
      {
        "id": "c3e97266-86f6-41f0-b6ae-ddeedb461cc3",
        "metadata": {
          "vector_store_key": "1910.06748-3",
          "chunk_id": 37,
          "document_id": "1910.06748",
          "start_idx": 14072,
          "end_idx": 14512
        },
        "page_content": "860,000 tweets. Large-scale dataset: Once again, we increased tenfold the number of tweets per language, and kept the 14 languages that had sufficient tweets in our initial 900 million tweet corpus. This gives us a dataset where each language has 700,000 tweets in its training set, 300,000 tweets in its validation set, and 300,000 tweets in its testing set, for a total 18,200,000 tweets. Referring to Table TABREF6, this dataset includes",
        "type": "Document"
      },
      {
        "id": "2dfbaa6e-5764-43fa-b43f-8fedf238753c",
        "metadata": {
          "vector_store_key": "1909.00107-1",
          "chunk_id": 19,
          "document_id": "1909.00107",
          "start_idx": 7348,
          "end_idx": 7859
        },
        "page_content": ". The dataset comprises of approximately 0.83 million words with 10,000 unique entries of which 0.5 million is used for training (0.24m for dev and 88k for test). Cancer Couples Interaction Dataset: This dataset was gathered as part of a observational study of couples coping with advanced cancer. Advanced cancer patients and their spouse caregivers were recruited from clinics and asked to interact with each other in two structured discussions: neutral discussion and cancer related. Interactions were audio-",
        "type": "Document"
      },
      {
        "id": "9270562a-8e53-4e1d-b760-74d8fd0bc872",
        "metadata": {
          "vector_store_key": "1905.06566-7",
          "chunk_id": 63,
          "document_id": "1905.06566",
          "start_idx": 16990,
          "end_idx": 17407
        },
        "page_content": "REF9 , and the New York Times dataset BIBREF37 , BIBREF38 . For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of see:2017:acl. The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following BIBREF38 , BIBREF37 , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from",
        "type": "Document"
      },
      {
        "id": "172793cb-e8d7-4755-9456-221033401a8a",
        "metadata": {
          "vector_store_key": "1909.03242-4",
          "chunk_id": 67,
          "document_id": "1909.03242",
          "start_idx": 25291,
          "end_idx": 25806
        },
        "page_content": " for bigrams. This means that the model has an easy time succeeding for instances where the claims are short, where specific topics tend to co-occur with certain veracities, and where evidence documents are highly informative. Instances with longer, more complex claims where evidence is ambiguous remain challenging. We present a new, real-world fact checking dataset, currently the largest of its kind. It consists of 34,918 claims collected from 26 fact checking websites, rich metadata and 10 retrieved evidence",
        "type": "Document"
      },
      {
        "id": "0723454a-4062-463e-972a-81a58c6a504e",
        "metadata": {
          "vector_store_key": "1809.04960-3",
          "chunk_id": 36,
          "document_id": "1809.04960",
          "start_idx": 14228,
          "end_idx": 14615
        },
        "page_content": "000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100. The weight INLINEFORM1 in Eq. EQREF26 is 1.0 under the semi-super",
        "type": "Document"
      }
    ]
  },
  {
    "query": "What task do they evaluate on?",
    "llm_answer": "Task 1 and Task 2.\n",
    "context": [
      {
        "id": "dc58289f-90f2-4f31-9a31-296d00bb01a9",
        "metadata": {
          "vector_store_key": "1709.10217-2",
          "chunk_id": 11,
          "document_id": "1709.10217",
          "start_idx": 4141,
          "end_idx": 4657
        },
        "page_content": ", in which only the released data can be used for training and developing. The other is an open evaluation that allow to collect external data for training and developing. For task 1, we use F1-score as evaluation metric. For the task-oriented dialogue systems, the best way for evaluation is to use the online human-computer dialogue. After finishing an online human-computer dialogue with a dialogue system, the human then manually evaluate the system by using the metrics of user satisfaction degree, dialogue flu",
        "type": "Document"
      },
      {
        "id": "67c7bdd9-8926-4b32-b3b7-4b1914e74192",
        "metadata": {
          "vector_store_key": "1601.02543-1",
          "chunk_id": 2,
          "document_id": "1601.02543",
          "start_idx": 845,
          "end_idx": 1419
        },
        "page_content": " successful and unsuccessful transactions. This evaluation is then used to identifying any further improvement in the speech recognition based solution to better the overall transaction completion rates. This process of evaluation is both time consuming and expensive. For evaluation one needs to identify a set of users and also identify the set of actual usage situations and perform the test. It is also important that the set of users are able to use the system with ease meaning that even in the test conditions the performance of the system, should be good, while this",
        "type": "Document"
      },
      {
        "id": "463039b8-3cd5-420e-81d9-33915772d993",
        "metadata": {
          "vector_store_key": "1906.03538-8",
          "chunk_id": 40,
          "document_id": "1906.03538",
          "start_idx": 15736,
          "end_idx": 16245
        },
        "page_content": " range of natural language understanding tasks. Human performance provides us with an estimate of the best achievable results on datasets. We use human annotators to measure human performance for each task. We randomly sample 10 claims from the test set, and instruct two expert annotators to solve each of T1 to T4. We perform evaluations on four different subtasks in our dataset. In all of the following evaluations, the systems are given the two pools of perspectives INLINEFORM0 and evidences INLINEFORM1",
        "type": "Document"
      },
      {
        "id": "daf340a9-7c75-42f4-aad2-16f270bcf92c",
        "metadata": {
          "vector_store_key": "1709.10217-2",
          "chunk_id": 17,
          "document_id": "1709.10217",
          "start_idx": 5875,
          "end_idx": 6394
        },
        "page_content": " the same. The tester then begin to converse to each system. A dialogue is finished if the system successfully returns the information which the user inquires or the number of dialogue turns is larger than 30 for a single task. For building the dialogue systems of participants, we release an example set of complete user intent and three data files of flight, train and hotel in JSON format. There are five evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the",
        "type": "Document"
      },
      {
        "id": "cad945f1-4338-4c8c-988a-66e53227489d",
        "metadata": {
          "vector_store_key": "1607.06025-6",
          "chunk_id": 26,
          "document_id": "1607.06025",
          "start_idx": 10088,
          "end_idx": 10607
        },
        "page_content": "REF37 , the evaluation metrics of text-generating models fall into three categories: manual evaluation, automatic evaluation metrics, task-based evaluation. In evaluation based on human judgment each generated textual example is inspected manually. The automatic evaluation metrics, like ROUGE, BLEU and METEOR, compare human texts and generated texts. BIBREF38 shows METEOR has the strongest correlation with human judgments in image description evaluation. The last category is task-based evaluation, where the impact",
        "type": "Document"
      }
    ]
  },
  {
    "query": "How many feature maps are generated for a given triple?",
    "llm_answer": "Not found in the context.\n",
    "context": [
      {
        "id": "50d87359-707e-4d91-8e5d-e140d7041382",
        "metadata": {
          "vector_store_key": "1804.03396-7",
          "chunk_id": 35,
          "document_id": "1804.03396",
          "start_idx": 12619,
          "end_idx": 13113
        },
        "page_content": " we prefer the articles with more golden triples involved by assuming that Wikipedia articles with more annotated triples are more informative and better annotated. Therefore, we figure out the distribution of the number of golden triples in articles and decide to discard the articles with less than 6 golden triples (account for about 80%). After this step, we obtain about 200K articles and 1.4M triples with 636 relation types. Query and Answer Assignment. For each golden triple $\\lbrace e",
        "type": "Document"
      },
      {
        "id": "6d346439-fc1e-4767-bbf3-d8fee5152e46",
        "metadata": {
          "vector_store_key": "1804.03396-7",
          "chunk_id": 34,
          "document_id": "1804.03396",
          "start_idx": 12119,
          "end_idx": 12619
        },
        "page_content": "64 DBpedia relations. Then we gather all the DBpedia triples with the first entity is corresponding to one of the above 3.5M articles and the relation is one of the projected 148 relations. After the same clipping process as above and removing the repetitive triples, we obtain 394K additional triples in 302K existing Wikipedia articles. Distillation. Since our benchmark is for IE, we prefer the articles with more golden triples involved by assuming that Wikipedia articles with more annotated tri",
        "type": "Document"
      },
      {
        "id": "12b98520-ef86-4acb-837d-84533afb10cd",
        "metadata": {
          "vector_store_key": "1804.03396-7",
          "chunk_id": 32,
          "document_id": "1804.03396",
          "start_idx": 11405,
          "end_idx": 11801
        },
        "page_content": ". We discard the triples with multiple entities (or values) for $e_j$ (account for about 6%, e.g., a book may have multiple authors). Besides, we discard the triples with any word in $e_j$ out of the corresponding article (account for about 50%). After this step, we obtain about 3.5M articles and 9M triples with 636 relation types. Incorporating DBpedia. Unlike WikiData, DBpedia is constructed",
        "type": "Document"
      },
      {
        "id": "fed58bb7-6137-4086-afd4-6f2ff0dd3f9d",
        "metadata": {
          "vector_store_key": "1802.05574-1",
          "chunk_id": 36,
          "document_id": "1802.05574",
          "start_idx": 14230,
          "end_idx": 14702
        },
        "page_content": " 5 judgements this excludes 9 triples. We indicate not only the pair-wise inter-annotator agreement but also the number of annotators who have judged a triple to be correct. For example, at the 40% agreement level at least 3 annotators have agreed that a triple is true. The figure separates the results by extractor and by data source. We see that as expected the amount of triples agreed to as correct grows larger as we relax the requirement for agreement. For example,",
        "type": "Document"
      },
      {
        "id": "69a71c81-f872-4ca4-b343-a23ede361c47",
        "metadata": {
          "vector_store_key": "1802.05574-1",
          "chunk_id": 35,
          "document_id": "1802.05574",
          "start_idx": 13921,
          "end_idx": 14410
        },
        "page_content": " calculated as typical where Correct Triples are treated as true positives. On average, 3.1 triples were extracted per sentence. Figure 1 shows the performance of extractors in terms of precision as inter-annotator agreement decreases. In this figure, we look only at agreement on triples where the majority agree that the triple is correct. Furthermore, to ease comparison, we only consider triples with 5 judgements this excludes 9 triples. We indicate not only the pair-wise inter-annot",
        "type": "Document"
      }
    ]
  }
]