qa_id,query,ground_truth,llm_answer,rouge1_p,rouge1_r,rouge1_f,rouge2_p,rouge2_r,rouge2_f,rougeL_p,rougeL_r,rougeL_f,bleu,cosine_sim,self_checker,llm_eval@1,llm_eval@3,llm_eval@5
q0,What is the seed lexicon?,a vocabulary of positive and negative predicates that helps determine the polarity score of an event,"A seed lexicon consists of 15 positive words and 15 negative words, as shown in Section SECREF27.  It also contains positive and negative predicates.
",0.25,0.375,0.3,0.13043478260869565,0.2,0.15789473684210528,0.25,0.375,0.3,0.09186672262412118,0.3561064898967743,No,0.0,1.0,1.0
q1,What are the results?,"Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. 
Using a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.","The models performed considerably better than random baselines.  Performance gaps with semi-supervised counterparts were less than 7%.  BiGRU encoder trained with AL+CA+CO dataset achieved the highest score.
",0.2,0.057692307692307696,0.08955223880597014,0.06896551724137931,0.019417475728155338,0.030303030303030304,0.16666666666666666,0.04807692307692308,0.07462686567164178,0.006035928682349959,0.42641690373420715,No,0.0,1.0,1.0
q2,How are relations used to propagate polarity?,"based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event ","Discourse relations like Cause (same polarity propagated) and Concession (reverse polarity propagated) are used to propagate polarity from seed predicates to other events.
",0.21739130434782608,0.23809523809523808,0.22727272727272724,0.045454545454545456,0.05,0.04761904761904762,0.21739130434782608,0.23809523809523808,0.22727272727272724,0.018884748972625875,0.557393491268158,Yes,1.0,1.0,1.0
q3,How big is the Japanese data?,"7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus","The context mentions a training set of 0.6 million events.
",0.2727272727272727,0.14285714285714285,0.18749999999999997,0.0,0.0,0.0,0.2727272727272727,0.14285714285714285,0.18749999999999997,0.016395955393726066,0.41435930132865906,No,0.0,0.0,1.0
q4,How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?,3%,"The context states that the method is effective when labeled data are small, but does not provide a specific comparison of improvements between supervised learning on small labeled data with and without the proposed approach.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0051898290403187275,No,0.0,0.0,0.0
q5,How does their model learn using mostly raw data?,by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity,"The model learns complex phenomena through label propagation.  Seed lexicon matches are used to assign scores to event pairs based on discourse relations (Cause or Concession).  These scores are used as reference scores during training.
",0.17142857142857143,0.42857142857142855,0.24489795918367346,0.029411764705882353,0.07692307692307693,0.0425531914893617,0.08571428571428572,0.21428571428571427,0.12244897959183673,0.02185068324154416,0.3750339448451996,Yes,0.0,1.0,1.0
q6,How big is seed lexicon used for training?,30 words,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18316617608070374,No,0.0,1.0,1.0
q7,How do the various social phenomena examined manifest in different types of communities?,"Dynamic communities have substantially higher rates of monthly user retention than more stable communities. More distinctive communities exhibit moderately higher monthly retention rates than more generic communities. There is also a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community - a short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content.
","Radically contrasting engagement patterns emerge in different parts of a multi-community space, and this variation is partly explained by the type of identity each community fosters.  For example, established members of distinctive communities engage more with volatile content than outsiders, while the opposite is true in generic communities.
",0.40816326530612246,0.22727272727272727,0.291970802919708,0.0625,0.034482758620689655,0.044444444444444446,0.22448979591836735,0.125,0.16058394160583941,0.025235499573567268,0.5023714303970337,Yes,0.0,0.0,1.0
q8,How did the select the 300 Reddit communities for comparison?,They selected all the subreddits from January 2013 to December 2014 with at least 500 words in the vocabulary and at least 4 months of the subreddit's history. They also removed communities with the bulk of the contributions are in foreign language.,"The answer is not found in the context.
",0.375,0.06976744186046512,0.1176470588235294,0.14285714285714285,0.023809523809523808,0.04081632653061224,0.375,0.06976744186046512,0.1176470588235294,0.002159928550151633,0.10400967299938202,No,0.0,0.0,0.0
q9,How is the clinical text structuring task defined?,CTS is extracting structural data from medical research data (unstructured). Authors define QA-CTS task that aims to discover most related text from original text.,"Clinical text structuring (CTS) is extracting or generating a key-value pair where the key is a query term and the value is a result of that query term according to the paragraph text.
",0.23529411764705882,0.32,0.2711864406779661,0.06060606060606061,0.08333333333333333,0.07017543859649122,0.17647058823529413,0.24,0.20338983050847456,0.03646650581707163,0.7197161912918091,Yes,0.0,1.0,1.0
q10,"Is all text in this dataset a question, or are there unrelated sentences in between questions?",the dataset consists of pathology reports including sentences and questions and answers about tumor size and resection margins so it does include additional sentences ,"Not found in the context.
",0.2,0.041666666666666664,0.06896551724137931,0.0,0.0,0.0,0.2,0.041666666666666664,0.06896551724137931,0.004041065926152656,0.0966578871011734,No,0.0,0.0,0.0
q11,How many questions are in the dataset?,"2,714 ","2,714
",1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,Yes,0.0,1.0,1.0
q12,What aspects have been compared between various language models?,"Quality measures using perplexity and recall, and performance measured using latency and energy usage. ","Perplexity, computational complexity (measured in FLOPs), latency, and energy usage.
",0.6,0.42857142857142855,0.5,0.3333333333333333,0.23076923076923078,0.27272727272727276,0.6,0.42857142857142855,0.5,0.1892240568795935,0.6573885083198547,Yes,0.0,1.0,1.0
q13,How many attention layers are there in their model?,one,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0340510718524456,No,0.0,0.0,0.0
q14,What are the three measures of bias which are reduced in experiments?,"RIPA, Neighborhood Metric, WEAT","Geometric bias, WEAT, and neighborhood bias.
",0.3333333333333333,0.5,0.4,0.0,0.0,0.0,0.16666666666666666,0.25,0.2,0.10552670315936317,0.4970637559890747,No,0.0,1.0,1.0
q15,How big is the dataset?,903019 references,"The answer is not found in the context.  While various dataset sizes are mentioned, there is no single answer to ""How big is the dataset?"".
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07880421727895737,No,0.0,1.0,1.0
q16,How is the intensity of the PTSD established?,"Given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively, the estimated intensity  is established as mean  squared error.","The intensity of PTSD is established by weekly survey results from three clinical assessment tools (DOSPERT, BSSS, and VIAS).  High risk is determined by scores exceeding thresholds on all three tools; moderate risk by scores exceeding thresholds on any two; low risk by exceeding thresholds on one; and no PTSD by not exceeding thresholds on any.
",0.30357142857142855,0.4722222222222222,0.3695652173913044,0.09090909090909091,0.14285714285714285,0.1111111111111111,0.125,0.19444444444444445,0.15217391304347827,0.017430763978945937,0.7398414611816406,No,0.0,1.0,1.0
q17,how is quality measured?,Accuracy and the macro-F1 (averaged F1 over positive and negative classes) are used as a measure of quality.,"By validating saliency maps by incrementally deleting ""important"" words from input text and observing changes in generated summaries;  using Cronbach's alpha to assess internal consistency; and using EM-score and F$_1$-score to evaluate deep neural network models.
",0.075,0.15789473684210525,0.10169491525423728,0.0,0.0,0.0,0.075,0.15789473684210525,0.10169491525423728,0.01045953676514853,0.45377781987190247,No,0.0,0.0,0.0
q18,What is the accuracy reported by state-of-the-art methods?,"Answer with content missing: (Table 1)
Previous state-of-the art on same dataset: ResNet50 89% (6 languages), SVM-HMM 70% (4 languages)","86%
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.24144577980041504,No,0.0,0.0,0.0
q19,How do the authors define or exemplify 'incorrect words'?,typos in spellings or ungrammatical words,"The authors exemplify ""incorrect words"" as missing or incorrectly typed words in sentences.  They also refer to incorrect sentences formatted by native English speakers as part of the dataset creation process.
",0.0967741935483871,0.5,0.16216216216216214,0.0,0.0,0.0,0.06451612903225806,0.3333333333333333,0.1081081081081081,0.013897369362318638,0.5894215703010559,Yes,0.0,0.0,0.0
q20,By how much do they outperform other models in the sentiment in intent classification tasks?,In the sentiment classification task by 6% to 8% and in the intent classification task by 0.94% on average,"The context states that the proposed model outperformed baseline models by 6% to 8% in the Twitter Sentiment Classification task, but no performance difference is mentioned for intent classification tasks.
",0.43333333333333335,0.65,0.5199999999999999,0.27586206896551724,0.42105263157894735,0.3333333333333333,0.3333333333333333,0.5,0.4,0.17347584066405097,0.6461379528045654,No,0.0,0.0,1.0
q21,Which experiments are perfomed?,They used BERT-based models to detect subjective language in the WNC corpus,"Experiments on subjective language detection using BERT-based models (BERT, RoBERTa, ALBERT);  experiments on clinical text structuring using rule-based methods, task-specific end-to-end methods, and pipeline methods; experiments on classifying perceptually similar languages (German and English) using GMM and RPLP features; experiments evaluating a proposed model for Chinese pathology report analysis.
",0.14814814814814814,0.6153846153846154,0.23880597014925373,0.07547169811320754,0.3333333333333333,0.12307692307692308,0.1111111111111111,0.46153846153846156,0.17910447761194032,0.01680969088963095,0.5823036432266235,Yes,0.0,1.0,1.0
q22,Is ROUGE their only baseline?,"No, other baseline metrics they use besides ROUGE-L are n-gram overlap, negative cross-entropy, perplexity, and BLEU.","No.  The context mentions a ""Random+Seed baseline"".
",0.25,0.10526315789473684,0.14814814814814814,0.0,0.0,0.0,0.25,0.10526315789473684,0.14814814814814814,0.02197090046478247,0.35524260997772217,No,0.0,0.0,0.0
q23,By how much does their system outperform the lexicon-based models?,"Under the retrieval evaluation setting, their proposed model + IR2 had better MRR than NVDM by 0.3769, better MR by 4.6, and better Recall@10 by  20 . 
Under the generative evaluation setting the proposed model + IR2 had better BLEU by 0.044 , better CIDEr by 0.033, better ROUGE by 0.032, and better METEOR by 0.029","The provided text states that UniSent outperforms lexicon-based models, but does not give a specific quantitative measure of the difference.
",0.14285714285714285,0.05084745762711865,0.07500000000000001,0.0,0.0,0.0,0.14285714285714285,0.05084745762711865,0.07500000000000001,0.0035258346230071192,0.38950544595718384,No,0.0,0.0,0.0
q24,How are the main international development topics that states raise identified?," They focus on exclusivity and semantic coherence measures: Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive. They select select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence.","By analyzing discussion of international development in annual UN General Debate statements, focusing on Topic 2 (economic development and the global system) and Topic 7 (sustainable development).
",0.2962962962962963,0.13559322033898305,0.18604651162790697,0.0,0.0,0.0,0.14814814814814814,0.06779661016949153,0.09302325581395349,0.00716236985297766,0.27377307415008545,No,0.0,0.0,0.0
q25,Is the semantic hierarchy representation used for any task?,"Yes, Open IE","Yes.
",1.0,0.3333333333333333,0.5,0.0,0.0,0.0,1.0,0.3333333333333333,0.5,0.0,0.3891817331314087,Yes,1.0,1.0,1.0
q26,Is the model evaluated?,the English version is evaluated. The German version evaluation is in progress ,"Yes.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09660522639751434,Yes,0.0,0.0,0.0
q27,How better is accuracy of new model compared to previously reported models?,"Average accuracy of proposed model vs best prevous result:
Single-task Training: 57.57 vs 55.06
Multi-task Training: 50.17 vs 50.59","The new model outperforms previous models in both EM and F1 scores, with improvements ranging from 3.69% to 6.39% in F1 and 5.64% to 6.39% in EM score.  It also demonstrates greater robustness to noise in data.
",0.024390243902439025,0.04,0.030303030303030304,0.0,0.0,0.0,0.024390243902439025,0.04,0.030303030303030304,0.008321783954793126,0.4553341567516327,No,0.0,0.0,0.0
q28,How does the active learning model work?,"Active learning methods has a learning engine (mainly used for training of classification problems) and the selection engine (which chooses samples that need to be relabeled by annotators from unlabeled data). Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively.","Active learning involves a learning engine (a classifier) and a selection engine (based on a sampling strategy).  The selection engine chooses samples needing annotation from unlabeled data.  These labeled samples are added to the training set, and the classifier is retrained.  This process iterates until desired accuracy is reached or a predefined threshold is met.  Different scoring models, like information entropy, or scoring models combining word score, link score, and sequence score (and using neural networks) can be used in the selection engine.
",0.4819277108433735,0.5633802816901409,0.5194805194805195,0.21951219512195122,0.2571428571428571,0.2368421052631579,0.37349397590361444,0.43661971830985913,0.40259740259740256,0.10867172900215763,0.8774538636207581,Yes,1.0,1.0,1.0
q29,Did the annotators agreed and how much?,"For event types and participant types, there was a moderate to substantial level of agreement using the Fleiss' Kappa. For coreference chain annotation, there was average agreement of 90.5%.","Inter-annotator agreement was found to be sufficiently high, and moderate to substantial, as measured by Fleiss' Kappa.  A qualified majority vote (at least 3 annotators agreeing) on coreference pairs reached 90.5%.
",0.3939393939393939,0.43333333333333335,0.4126984126984127,0.125,0.13793103448275862,0.13114754098360656,0.2727272727272727,0.3,0.28571428571428564,0.09343174834465218,0.7799333333969116,Yes,0.0,1.0,1.0
q30,What datasets are used to evaluate this approach?," Kinship and Nations knowledge graphs, YAGO3-10 and WN18KGs knowledge graphs ","A Chinese dataset (198,112 news articles from Tencent News), a dataset comprising 3 languages (English, French, German), and a dataset annotated based on Chinese pathology reports (1,899 training instances, 815 test instances).
",0.029411764705882353,0.09090909090909091,0.04444444444444444,0.0,0.0,0.0,0.029411764705882353,0.09090909090909091,0.04444444444444444,0.009896324688555468,0.30813026428222656,No,0.0,0.0,0.0
q31,How was the dataset collected?,"They crawled travel information from the Web to build a database, created a multi-domain goal generator from the database, collected dialogue between workers an automatically annotated dialogue acts. ","The datasets were collected from Tencent News (comments, articles), Shuguang Hospital Affiliated to Shanghai University of Traditional Chinese Medicine (EHRs), and Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital.  Dialogue data was collected through worker conversations.
",0.175,0.2413793103448276,0.2028985507246377,0.0,0.0,0.0,0.125,0.1724137931034483,0.14492753623188406,0.011860506110174321,0.2805941104888916,No,0.0,0.0,1.0
q32,What models other than standalone BERT is new model compared to?,Only Bert base and Bert large are compared to proposed approach.,"RoBERTa, ALBERT, and distilled BERT-based models.
",0.42857142857142855,0.2727272727272727,0.33333333333333326,0.16666666666666666,0.1,0.125,0.2857142857142857,0.18181818181818182,0.2222222222222222,0.04062582855427254,0.5639514923095703,No,0.0,1.0,1.0
q33,How big is the performance difference between this method and the baseline?,"Comparing with the highest performing baseline: 1.3 points on ACE2004 dataset, 0.6 points on CWEB dataset, and 0.86 points in the average of all scores.","The context states that the proposed methodology outperforms the baselines for all metrics.  No specific performance difference is given.
",0.2631578947368421,0.17857142857142858,0.2127659574468085,0.0,0.0,0.0,0.15789473684210525,0.10714285714285714,0.1276595744680851,0.01754711160376357,0.4101121723651886,No,0.0,0.0,0.0
q34,What approaches without reinforcement learning have been tried?,"classification, regression, neural methods","Regression systems (NNR).
",0.3333333333333333,0.25,0.28571428571428575,0.0,0.0,0.0,0.3333333333333333,0.25,0.28571428571428575,0.0,0.4798702001571655,Yes,0.0,0.0,0.0
q35,Which languages do they validate on?,"Ar, Bg, Ca, Cs, Da, De, En, Es, Eu, Fa, Fi, Fr, Ga, He, Hi, Hu, It, La, Lt, Lv, Nb, Nl, Nn, PL, Pt, Ro, Ru, Sl, Sv, Tr, Uk, Ur","English, French, and German.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.6199127516221904e-05,0.5641132593154907,No,0.0,0.0,0.0
q36,What is the baseline method for the task?,"For the emotion recognition from text they use described neural network as baseline.
For audio and face there is no baseline.","TF-IDF (Lexical, Non-Neural) and LDA (Topic, Non-Neural) are unsupervised baseline models.
",0.21428571428571427,0.14285714285714285,0.17142857142857143,0.0,0.0,0.0,0.21428571428571427,0.14285714285714285,0.17142857142857143,0.021409907800309075,0.40187060832977295,No,0.0,0.0,0.0
q37,what amounts of size were used on german-english?,"Training data with 159000, 80000, 40000, 20000, 10000 and 5000 sentences, and 7584 sentences for development","159000 parallel sentences for training, and 7584 for development.
",0.7777777777777778,0.4375,0.56,0.375,0.2,0.26086956521739135,0.6666666666666666,0.375,0.4800000000000001,0.08261093701076819,0.6193320751190186,Yes,0.0,1.0,1.0
q38,How big is the dataset?,Resulting dataset was 7934 messages for train and 700 messages for test.,"The sizes of the datasets vary.  There are 287,000 training pairs and 11,500 test pairs for one dataset. Another dataset has 191,502 training, 5,000 validation, and 1,610 test pieces of news.  A third dataset contains 198,112 news articles.  A fourth dataset has 2489 training messages and 700 test tweets/messages.
",0.14285714285714285,0.6666666666666666,0.23529411764705882,0.01818181818181818,0.09090909090909091,0.0303030303030303,0.10714285714285714,0.5,0.17647058823529413,0.017702581747699578,0.616782546043396,No,0.0,0.0,1.0
q39,What MC abbreviate for?,machine comprehension,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14238585531711578,No,0.0,0.0,0.0
q40,What are their correlation results?,High correlation results range from 0.472 to 0.936,"The text describes correlation calculations (Pearson, Kendall, Spearman, revised Kendall) between ROUGE metrics (recall, precision, F1 for ROUGE-2 and ROUGE-SU4) and human scores, and between predicted and manual scores for various models and domains.  Specific correlation values are presented in tables (TABREF23 and TABREF38), but not explicitly stated here.
",0.0196078431372549,0.1,0.032786885245901634,0.0,0.0,0.0,0.0196078431372549,0.1,0.032786885245901634,0.0056577596562095355,0.4212336242198944,No,0.0,0.0,0.0
q41,What simpler models do they look at?,"BiGRUs with attention, ROUGE, Language model, and next sentence prediction ","Topic models with between 3 and 50 topics.
",0.375,0.3,0.33333333333333326,0.0,0.0,0.0,0.25,0.2,0.22222222222222224,0.036353588668522965,0.2689388692378998,No,0.0,0.0,0.0
q42,What linguistic quality aspects are addressed?,"Grammaticality, non-redundancy, referential clarity, focus, structure & coherence","Fluency.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3204004466533661,No,0.0,0.0,0.0
q43,What dataset do they use?,A parallel corpus where the source is an English expression of code and the target is Python code.,"AIDA-Train, Wikipedia, AIDA-B, AIDA-CoNLL, Stanford Question Answering Dataset (SQuAD), MCTest, CNN/Daily Mail, Children's Book Test (CBT), HEOT, FigureEight, ISEAR, Twitter Emotion Corpus (TEC).
",0.03571428571428571,0.05555555555555555,0.043478260869565216,0.0,0.0,0.0,0.03571428571428571,0.05555555555555555,0.043478260869565216,0.008522452865706931,0.21445518732070923,No,0.0,0.0,0.0
q44,What is typical GAN architecture for each text-to-image synhesis group?,"Semantic Enhancement GANs: DC-GANs, MC-GAN
Resolution Enhancement GANs: StackGANs, AttnGAN, HDGAN
Diversity Enhancement GANs: AC-GAN, TAC-GAN etc.
Motion Enhancement GAGs: T2S, T2V, StoryGAN","The context describes the *motivations* and *contributions* of different GAN architectures, but does not explicitly state a typical architecture for each group.  It mentions *semantic enhancement GANs* using a neural network to encode text, but doesn't detail a typical architecture for the other groups (resolution, diversity, or motion enhancement).
",0.16,0.2962962962962963,0.20779220779220778,0.061224489795918366,0.11538461538461539,0.08,0.14,0.25925925925925924,0.18181818181818182,0.008998901289965151,0.6269466876983643,No,1.0,1.0,1.0
q45,What language do the agents talk in?,English,"Natural language.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4631931781768799,No,1.0,1.0,1.0
q46,How much better is performance of proposed method than state-of-the-art methods in experiments?,"Accuracy of best proposed method KANE (LSTM+Concatenation) are 0.8011, 0.8592, 0.8605 compared to best state-of-the art method R-GCN + LR 0.7721, 0.8193, 0.8229 on three datasets respectively.","The proposed method achieved 93.7% accuracy, outperforming the Coppersmith et al. method which achieved 86%.  Other proposed methods (mixup augmentation, attention mechanism, recurrent layers) also achieved higher accuracy than the baseline Coppersmith et al. method, but the context does not explicitly quantify these improvements.
",0.1111111111111111,0.1388888888888889,0.12345679012345678,0.022727272727272728,0.02857142857142857,0.02531645569620253,0.1111111111111111,0.1388888888888889,0.12345679012345678,0.01940539184851615,0.5324263572692871,No,0.0,0.0,0.0
q47,What baseline model is used?,Human evaluators,"TF-IDF, LDA, Random+Seed
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0003543058701325208,No,0.0,0.0,0.0
q48,What stylistic features are used to detect drunk texts?,"LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors,  Spelling errors, Repeated characters, Capitalisation, Length,  Emoticon (Presence/Count ) 
 and Sentiment Ratio","Capitalization, spelling errors.
",0.6666666666666666,0.08695652173913043,0.15384615384615383,0.5,0.045454545454545456,0.08333333333333334,0.6666666666666666,0.08695652173913043,0.15384615384615383,0.00014148640667517308,0.21510757505893707,Yes,0.0,0.0,0.0
q49,What is the accuracy of the proposed technique?,51.7 and 51.6 on 4th and 8th grade question sets with no curated knowledge. 47.5 and 48.0 on 4th and 8th grade question sets when both solvers are given the same knowledge,"96%
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.33898890018463135,No,0.0,0.0,0.0
q50,Which retrieval system was used for baselines?,The dataset comes with a ranked set of relevant documents. Hence the baselines do not use a retrieval system.,"Sliding Window (SW), Sliding Window + Distance (SW+D), TF-IDF, LDA, IR.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.017272237992167868,0.10584023594856262,No,1.0,1.0,1.0
q51,How much better was the BLSTM-CNN-CRF than the BLSTM-CRF?,Best BLSTM-CNN-CRF had F1 score 86.87 vs 86.69 of best BLSTM-CRF ,"Answer not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.00016737543046474457,No,0.0,1.0,1.0
q52,What supplemental tasks are used for multitask learning?,"Multitask learning is used for the task of predicting relevance of a comment on a different question to a given question, where the supplemental tasks are predicting relevance between the questions, and between the comment and the corresponding question","Task A and Task B.
",0.8,0.10256410256410256,0.18181818181818182,0.0,0.0,0.0,0.6,0.07692307692307693,0.13636363636363635,0.00023768123945074746,0.3728804588317871,No,0.0,0.0,0.0
q53,How much performance gap between their approach and the strong handcrafted method?,"0.007 MAP on Task A, 0.032 MAP on Task B, 0.055 MAP on Task C","The context mentions outperforming Coppersmith et al.'s method, but does not quantify the performance gap.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.020244462660665506,0.23140138387680054,No,0.0,0.0,0.0
q54,How big is their model?,Proposed model has 1.16 million parameters and 11.04 MB.,"The answer is not found in the context.  While the context describes various models and their components, it does not specify the overall size of *any* model.
",0.07407407407407407,0.18181818181818182,0.10526315789473684,0.0,0.0,0.0,0.07407407407407407,0.18181818181818182,0.10526315789473684,0.015266081935888859,0.46315768361091614,No,0.0,0.0,0.0
q55,How many emotions do they look at?,9,"3 (joy, annoyance/anger, insecurity/fear)
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11760774254798889,No,0.0,0.0,0.0
q56,What approach did previous models use for multi-span questions?,Only MTMSM specifically tried to tackle the multi-span questions. Their approach consisted of two parts: first train a dedicated categorical variable to predict the number of spans to extract and the second was to generalize the single-span head method of extracting a span,"Only one model, BIBREF4, directly targeted multi-span questions.
",0.4444444444444444,0.08888888888888889,0.14814814814814814,0.25,0.045454545454545456,0.07692307692307693,0.4444444444444444,0.08888888888888889,0.14814814814814814,0.006815048332167776,0.6070787906646729,No,0.0,1.0,1.0
q57,What is difference in peformance between proposed model and state-of-the art on other question types?,"For single-span questions, the proposed LARGE-SQUAD improve performance of the MTMSNlarge baseline for 2.1 EM and 1.55 F1.
For number type question,  MTMSNlarge baseline  have improvement over LARGE-SQUAD for 3,11  EM and  2,98 F1. 
For date question,  LARGE-SQUAD have improvements in 2,02 EM but MTMSNlarge have improvement of 4,39 F1.","The provided text states that the ""proposed+IR"" model achieves the best performance and better accuracy in discriminating plausible from correct comments compared to other models (IR, TF-IDF, and S2S).  However, it does not give specific performance numbers for different question types compared to the other models.
",0.22916666666666666,0.18333333333333332,0.2037037037037037,0.02127659574468085,0.01694915254237288,0.01886792452830189,0.14583333333333334,0.11666666666666667,0.12962962962962965,0.012324865325777161,0.3641304075717926,No,0.0,0.0,0.0
q58,What is the performance of proposed model on entire DROP dataset?,"The proposed model achieves  EM 77,63 and F1 80,73  on the test and EM  76,95 and  F1 80,25 on the dev","The proposed model slightly eclipses the current state-of-the-art results on the entire DROP dataset.
",0.35294117647058826,0.24,0.28571428571428564,0.1875,0.125,0.15,0.35294117647058826,0.24,0.28571428571428564,0.08629921818897687,0.44843533635139465,No,0.0,0.0,0.0
q59,How accurate is the aspect based sentiment classifier trained only using the XR loss?,"BiLSTM-XR-Dev Estimation accuracy is 83.31 for SemEval-15 and 87.68 for SemEval-16.
BiLSTM-XR accuracy is 83.31 for SemEval-15 and 88.12 for SemEval-16.
","The context states the classifier is *accurate*.
",0.14285714285714285,0.03125,0.05128205128205128,0.0,0.0,0.0,0.14285714285714285,0.03125,0.05128205128205128,0.013599471925976851,0.2575058341026306,No,1.0,1.0,1.0
q60,What were the non-neural baselines used for the task?,The Lemming model in BIBREF17,"TF-IDF and LDA.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09291186928749084,No,0.0,0.0,0.0
q61,What are the models evaluated on?,They evaluate F1 score and agent's test performance on their own built interactive datasets (iSQuAD and iNewsQA),"UN General Debate statements;  ""what"", ""how"", ""who"", ""when"", ""which"", ""where"", ""why"", ""be"", ""whose"", and ""whom"" questions.  Also, a test set (for QRNNs, first 350 words; for KN-5, entire test set).  A development set.
",0.058823529411764705,0.1111111111111111,0.07692307692307691,0.0,0.0,0.0,0.058823529411764705,0.1111111111111111,0.07692307692307691,0.0070930906528519485,0.15020820498466492,No,0.0,0.0,0.0
q62,What is the results of multimodal compared to unimodal models?,"Unimodal LSTM vs Best Multimodal (FCM)
- F score: 0.703 vs 0.704
- AUC: 0.732 vs 0.734 
- Mean Accuracy: 68.3 vs 68.4 ","The multimodal models (like TKM) end up using almost only the text input and produce results similar to textual models.
",0.05,0.038461538461538464,0.043478260869565216,0.0,0.0,0.0,0.05,0.038461538461538464,0.043478260869565216,0.015756171878170463,0.3735285997390747,No,0.0,0.0,0.0
q63,What were their performance results?,"On SearchSnippets dataset ACC 77.01%, NMI 62.94%, on StackOverflow dataset ACC 51.14%, NMI 49.08%, on Biomedical dataset ACC 43.00%, NMI 38.18%","The models achieved a 12.2 absolute improvement in EM and a 2.3 improvement in F1 compared to the base model.  Large-squad showed a 29.7 EM and 15.1 F1 improvement compared to MTMSNlarge.  A simple model trained in Experiment 1 had an average F$_1$ of 48%.  BERT achieved a 0.604 micro-F1 score in benchmark tests.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.005749785925016119,0.33573535084724426,No,0.0,0.0,0.0
q64,By how much did they outperform the other methods?,"on SearchSnippets dataset by 6.72% in ACC, by 6.94% in NMI; on Biomedical dataset by 5.77% in ACC, 3.91% in NMI","The provided text states that proposed methods outperformed baselines for all metrics, but does not give specific performance differences.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.013012817232777963,0.37629297375679016,No,0.0,0.0,0.0
q65,What is the state of the art?,"POS and DP task: CONLL 2018
NER task: (no extensive work) Strong baselines CRF and BiLSTM-CRF
NLI task: mBERT or XLM (not clear from text)","The state-of-the-art baselines achieve 33.6% and 28.5% accuracy on Quasar-S and Quasar-T, respectively.  Human performance is 50% and 60.6% on the same tasks.  Deep learning models, particularly ensembles of convolutional neural networks, are a strong trend in image and video recognition.  In the automotive domain, FACS (Facial Action Coding System) using support vector machines is still popular.  Word embedding methods using hash tokens are also being explored to address out-of-vocabulary issues.
",0.06172839506172839,0.19230769230769232,0.09345794392523364,0.0,0.0,0.0,0.04938271604938271,0.15384615384615385,0.07476635514018692,0.006257182452954464,0.22122567892074585,No,0.0,0.0,0.0
q66,"What difficulties does sentiment analysis on Twitter have, compared to sentiment analysis in other domains?","Tweets noisy nature, use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, short (length limited) text","The context states that Twitter messages are generally short, often length-limited, and that this presents a challenge requiring specialized research.
",0.19047619047619047,0.16666666666666666,0.17777777777777778,0.05,0.043478260869565216,0.046511627906976744,0.14285714285714285,0.125,0.13333333333333333,0.02648202738127081,0.4703013002872467,Yes,0.0,0.0,0.0
q67,How many sentence transformations on average are available per unique sentence in dataset?,27.41 transformation on average of single seed sentence is available in dataset.,"15
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09500506520271301,No,0.0,0.0,0.0
q68,What annotations are available in the dataset?,"For each source sentence, transformation sentences that are transformed according to some criteria (paraphrase, minimal change etc.)","Dialogue acts and states (Joy, Sadness, Anger, Fear, Anticipation, Surprise, Love, Disgust, Neutral);  MissScrEv/MissScrPart (missing script event/participant); coreference information for noun phrases; question text, context document, candidate solutions, and correct solution.
",0.030303030303030304,0.058823529411764705,0.04,0.0,0.0,0.0,0.030303030303030304,0.058823529411764705,0.04,0.010072487110349271,0.2838897109031677,No,0.0,0.0,0.0
q69,"How are possible sentence transformations represented in dataset, as new sentences?","Yes, as new sentences.","The context describes *possible* sentence transformations, but does not specify how they are *represented* as *new* sentences in the dataset.  It only mentions that the dataset illustrates patterns like formalness, concreteness, and speaker confidence.
",0.08823529411764706,0.75,0.15789473684210525,0.06060606060606061,0.6666666666666666,0.1111111111111111,0.08823529411764706,0.75,0.15789473684210525,0.012157709822493184,0.4136589467525482,No,0.0,1.0,1.0
q70,What are all 15 types of modifications ilustrated in the dataset?,"- paraphrase 1
- paraphrase 2
- different meaning
- opposite meaning
- nonsense
- minimal change
- generalization
- gossip
- formal sentence
- non-standard sentence
- simple sentence
- possibility
- ban
- future
- past","The context does not list 15 types of modifications.  It mentions several, including changes to subject, predicate, and object, changes in word order, transformations to interrogative/imperative sentences, fairy-tale style, excessive use of diminutives/vulgarisms, dadaism, and swap of roles.  Specific modifications like $\langle s, r^{\prime }, o^{\prime }\rangle $ and $\langle s, r^{\prime }, o\rangle $ are mentioned, but not counted as individual types.
",0.030303030303030304,0.08333333333333333,0.04444444444444444,0.0,0.0,0.0,0.030303030303030304,0.08333333333333333,0.04444444444444444,0.0,0.3875386416912079,No,0.0,0.0,0.0
q71,What were their results on the three datasets?,"accuracy of 86.63 on STS, 85.14 on Sanders and 80.9 on HCR","The context mentions that HDGAN performed relatively better on CUB and Oxford datasets, and AttnGAN on COCO.  Specific quantitative results for IS, FID, FCN, or human classifiers are not provided.
",0.1,0.2,0.13333333333333333,0.0,0.0,0.0,0.1,0.2,0.13333333333333333,0.014500372328365223,0.3646788001060486,No,0.0,0.0,0.0
q72,What semantic rules are proposed?,rules that compute polarity of words after POS tagging or parsing steps,"Five semantic rules.
",0.3333333333333333,0.08333333333333333,0.13333333333333333,0.0,0.0,0.0,0.3333333333333333,0.08333333333333333,0.13333333333333333,0.021617886496312456,0.3471823036670685,No,0.0,1.0,1.0
q73,What is the performance of the model?,"Experiment 1: ACC around 0.5 with 50% noise rate in worst case - clearly higher than baselines for all noise rates
Experiment 2: ACC on real noisy datasets: 0.7 on Movie, 0.79 on Laptop, 0.86 on Restaurant (clearly higher than baselines in almost all cases)","The context describes improvements in model performance across various datasets (DROP, Quasar-S, Quasar-T, and others), with large models significantly outperforming base models and baselines.  Specific results are given for different models and metrics, like EM, F1, and accuracy.
",0.125,0.10416666666666667,0.11363636363636363,0.0,0.0,0.0,0.075,0.0625,0.06818181818181818,0.01188474235736749,0.18873077630996704,No,0.0,0.0,0.0
q74,What was the performance of both approaches on their dataset?,ERR of 19.05 with i-vectors and 15.52 with x-vectors,"The context describes performance on DROP's test set, with the author's model being the best overall.  It also describes ablation studies on the development set.  Performance on other datasets or tasks is mentioned, but no overall comparison between the two approaches (author's and MTMSN) on their dataset is provided.
",0.038461538461538464,0.15384615384615385,0.06153846153846154,0.0,0.0,0.0,0.038461538461538464,0.15384615384615385,0.06153846153846154,0.007716270058689812,0.042944684624671936,No,0.0,0.0,0.0
q75,What genres are covered?,"genre, entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement","Entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation, advertisement.
",1.0,0.8571428571428571,0.923076923076923,0.9090909090909091,0.7692307692307693,0.8333333333333333,1.0,0.8571428571428571,0.923076923076923,0.7985145893632312,0.9057595729827881,Yes,1.0,1.0,1.0
q76,Which of the two speech recognition models works better overall on CN-Celeb?,x-vector,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10085505992174149,No,0.0,0.0,0.0
q77,By how much is performance on CN-Celeb inferior to performance on VoxCeleb?,"For i-vector system, performances are 11.75% inferior to voxceleb. For x-vector system, performances are 10.74% inferior to voxceleb","The context states that performance on CN-Celeb(E) is much worse than performance on SITW and SITW(S), and that a re-trained model on VoxCeleb(L) performs better on SITW than the same-source CN-Celeb(E).  However, it does not provide specific performance numbers for a direct comparison.  Therefore, the answer is not found in the context.
",0.06666666666666667,0.18181818181818182,0.0975609756097561,0.0,0.0,0.0,0.05,0.13636363636363635,0.07317073170731708,0.00679364123713465,0.5777836441993713,No,0.0,0.0,0.0
q78,How do the authors measure performance?,Accuracy across six datasets,"The authors use F<sub>1</sub> score, EM-score, and training/testing curves (averaged over 3 random seeds).  Human evaluation is also used, as well as comparison against baselines.  Summarization is evaluated using Pyramid and ROUGE metrics.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2555687129497528,No,0.0,0.0,0.0
q79,What is the latest paper covered by this survey?,Kim et al. (2019),"Wei, Jason and Zou, Kai EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)
",0.024390243902439025,0.25,0.04444444444444444,0.0,0.0,0.0,0.024390243902439025,0.25,0.04444444444444444,0.010700143107304574,0.3947550654411316,No,0.0,0.0,0.0
q80,What learning paradigms do they cover in this survey?,"Considering ""What"" and ""How"" separately versus jointly optimizing for both.","Supervised learning, semi-supervised learning, and using distant supervision.
",0.1111111111111111,0.1,0.10526315789473685,0.0,0.0,0.0,0.1111111111111111,0.1,0.10526315789473685,0.030981749906855875,0.2093956023454666,No,0.0,0.0,0.0
q81,What are all the input modalities considered in prior work in question generation?,"Textual inputs, knowledge bases, and images.","Raw text, database, or semantic representation.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06274655310994741,0.5580601096153259,No,0.0,0.0,0.0
q82,How do this framework facilitate demographic inference from social media?,Demographic information is predicted using weighted lexicon of terms.,"The framework uses supervised models with features like screen names, profile descriptions, content from external resources, emoticons, online behaviors, and life stage information.  It also incorporates visual imagery (profile and shared images) and textual content (weighted lexicons of terms) for demographic prediction.  Analysis of language cues and homophily interaction information are also employed.
",0.1509433962264151,0.8888888888888888,0.25806451612903225,0.057692307692307696,0.375,0.09999999999999999,0.09433962264150944,0.5555555555555556,0.16129032258064518,0.014004371144601919,0.5901303291320801,Yes,0.0,0.0,1.0
q83,How is the data annotated?,The data are self-reported by Twitter users and then verified by two human experts.,"Workers made dialogues, received feedback, and then were paired to converse.  Workers annotated user and system states.  Rules were used for automatic dialogue act annotation based on states and history.  Three experts manually annotated 50 dialogues to evaluate automatic annotation quality.  The scheme was modified to include categories like Joy, Sadness, Anger, Fear, etc., and a binary label for semantic tuples.  Three experts also manually annotated dialogue acts and states for 50 dialogues (806 utterances) in the modified scheme.
",0.06329113924050633,0.3333333333333333,0.10638297872340426,0.02564102564102564,0.14285714285714285,0.043478260869565216,0.0379746835443038,0.2,0.06382978723404255,0.009510195519005135,0.18290606141090393,No,0.0,0.0,0.0
q84,Where does the information on individual-level demographics come from?,From Twitter profile descriptions of the users.,"User-generated content (screen names, full names, profile descriptions, content on external resources, emoticons, acronyms, slangs, punctuation, capitalization, sentence length, links/images), online behavior (number of friends, post time, commenting activity), and life stage information (secondary school, college, employee).  Also, self-reported data from Facebook users.
",0.10869565217391304,0.7142857142857143,0.18867924528301885,0.022222222222222223,0.16666666666666666,0.0392156862745098,0.08695652173913043,0.5714285714285714,0.15094339622641512,0.015438133484929223,0.40685492753982544,No,0.0,0.0,0.0
q85,What is the source of the user interaction data? ,Sociability from ego-network on Twitter,"A driving simulator.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.03405246511101723,No,0.0,0.0,0.0
q86,What is the source of the textual data? ,Users' tweets,"English Wiktionary, Stack Overflow, SemEval-2016 (Laptop and Restaurant datasets), electronic health records (EHRs).
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.058065567165613174,No,0.0,0.0,0.0
q87,What is the source of the visual data? ,Profile pictures from the Twitter users' profiles.,"Image data captured from photo-ready devices (like smartphones) and online social media services.
",0.07142857142857142,0.14285714285714285,0.09523809523809523,0.0,0.0,0.0,0.07142857142857142,0.14285714285714285,0.09523809523809523,0.02908317710573758,0.5360140204429626,Yes,0.0,0.0,0.0
q88,What result from experiments suggest that natural language based agents are more robust?,Average reward across 5 seeds show that NLP representations are robust to changes in the environment as well task-nuisances,"Natural language representation achieved better performance, even on difficult tasks or tasks with task-nuisances.
",0.2,0.15,0.17142857142857143,0.07142857142857142,0.05263157894736842,0.06060606060606061,0.2,0.15,0.17142857142857143,0.020274685852177114,0.4689246714115143,Yes,0.0,0.0,0.0
q89,Which datasets are used in the paper?,"Google N-grams
COHA
Moral Foundations Dictionary (MFD)
","Image datasets:  CUB, MSCOCO, Oxford-102, MNIST, CUB, Oxford, COCO, CIFAR-10.
Question Answering datasets: Stanford Question Answering Dataset (SQuAD), MCTest, CNN/Daily Mail.
Other datasets: BioASQ challenge data, SearchSnippets, StackOverflow, Chinese dataset from Tencent News.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.007860561859628806,0.17911696434020996,No,0.0,0.0,0.0
q90,How do they quantify moral relevance?,By complementing morally relevant seed words with a set of morally irrelevant seed words based on the notion of valence,"They compute diachronic moral relevance scores using the probability of a concept being in a moral category given a word, and then perform a linear regression to measure the rate of change.  They also compare model predictions to human ratings of moral relevance and acceptability from the Pew Research Center's survey.
",0.15384615384615385,0.4,0.2222222222222222,0.0392156862745098,0.10526315789473684,0.05714285714285714,0.1346153846153846,0.35,0.19444444444444445,0.00952597016640547,0.35546213388442993,No,0.0,0.0,0.0
q91,How much is proposed model better in perplexity and BLEU score than typical UMT models?,"Perplexity of the best model is 65.58 compared to best baseline 105.79.
Bleu of the best model is 6.57 compared to best baseline 5.50.","The answer is not found in the context.  While the text discusses improvements over a ""naive transformer"" and other models, it does not compare the proposed model to typical UMT (universal machine translation) models.
",0.20588235294117646,0.25,0.22580645161290322,0.0,0.0,0.0,0.17647058823529413,0.21428571428571427,0.1935483870967742,0.01456486634235763,0.31642621755599976,No,0.0,0.0,0.0
q92,Do they train a different training method except from scheduled sampling?,"Answer with content missing: (list missing) 
Scheduled sampling: In our experiments, we found that models trained with scheduled sampling performed better (about 0.004 BLEU-4 on validation set) than the ones trained using teacher-forcing for the AVSD dataset. Hence, we use scheduled sampling for all the results we report in this paper.

Yes.","The answer is not found in the context.
",0.625,0.09090909090909091,0.15873015873015875,0.0,0.0,0.0,0.375,0.05454545454545454,0.09523809523809523,0.00016713125187167043,0.08405481278896332,No,0.0,0.0,0.0
q93,How do they define upward and downward reasoning?,"Upward reasoning is defined as going from one specific concept to a more general one. Downward reasoning is defined as the opposite, going from a general concept to one that is more specific.","The context describes upward entailing inferences as those where adding a phrase to the premise strengthens the conclusion, and downward entailing inferences as those where adding a phrase to the premise weakens the conclusion.  However, a formal definition of upward and downward reasoning is not explicitly given.
",0.23404255319148937,0.3333333333333333,0.275,0.043478260869565216,0.0625,0.05128205128205128,0.1702127659574468,0.24242424242424243,0.19999999999999998,0.022362916402160152,0.8323651552200317,No,0.0,0.0,0.0
q94,Do they annotate their own dataset or use an existing one?,Use an existing one,"They annotate their own dataset for some tasks (e.g., software questions, dialogue acts, and entity linking), and use existing public datasets (e.g., AIDA, Wikipedia) for other tasks.
",0.06896551724137931,0.5,0.1212121212121212,0.0,0.0,0.0,0.06896551724137931,0.5,0.1212121212121212,0.008321783954793126,-0.0018251222791150212,Yes,1.0,1.0,1.0
q95,What kind of neural network architecture do they use?,CNN,"Bi-directional attention flow (BiDAF) with an explicit reasoning module, a hybrid convolutional recurrent network at the character level, and a Gated Combination Neural Network (GCNN).  Also uses a Dynamic Convolutional Neural Network (DCNN).
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.24785694479942322,No,0.0,0.0,0.0
q96,What web and user-generated NER datasets are used for the analysis?,"MUC, CoNLL, ACE, OntoNotes, MSM, Ritter, UMBC","UMBC Twitter corpus BIBREF33, Web content from ACE BIBREF30 and OntoNotes BIBREF29, and Usenet newsgroups from ACE BIBREF30.
",0.16666666666666666,0.42857142857142855,0.24,0.0,0.0,0.0,0.1111111111111111,0.2857142857142857,0.16,0.027156804039319287,0.4252539277076721,No,1.0,1.0,1.0
q97,Which unlabeled data do they pretrain with?,1000 hours of WSJ audio data,"Unlabeled audio data.
",0.6666666666666666,0.3333333333333333,0.4444444444444444,0.5,0.2,0.28571428571428575,0.6666666666666666,0.3333333333333333,0.4444444444444444,0.1937692912686648,0.4484156370162964,No,0.0,0.0,0.0
q98,How many convolutional layers does their model have?,wav2vec has 12 convolutional layers,"Two
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1546301394701004,No,0.0,0.0,0.0
q99,How big are the datasets?,"In-house dataset consists of  3716 documents 
ACE05 dataset consists of  1635 documents","Not found in the context.  The context describes *types* of datasets, but not sizes.
",0.21428571428571427,0.23076923076923078,0.22222222222222224,0.0,0.0,0.0,0.21428571428571427,0.23076923076923078,0.22222222222222224,0.020244462660665506,0.3301977813243866,No,0.0,0.0,0.0
q100,What baselines did they compare their model with?,the baseline where path generation uses a standard sequence-to-sequence model augmented with attention mechanism and path verification uses depth-first search,"NABERT+ baseline.
",0.5,0.043478260869565216,0.08,0.0,0.0,0.0,0.5,0.043478260869565216,0.08,0.002925662791955391,0.3605397343635559,No,0.0,0.0,0.0
q101,What was the performance of their model?,"For test-repeated set, EM score of 61.17, F1 of 93.54, ED of 0.75 and GM of 61.36. For test-new set, EM score of 41.71, F1 of 91.02, ED of 1.22 and GM of 41.81","Their large models exhibited a substantial improvement across all metrics compared to base models, surpassing the NABERT+ baseline.  Their best model (large-squad) showed a huge improvement of 29.7 in EM and 15.1 in F1 compared to MTMSNlarge.  They achieved a 12.2 absolute improvement in EM and a 2.3 improvement in F1 compared to their base model, even when comparing to MTMSNlarge.
",0.12121212121212122,0.18181818181818182,0.14545454545454545,0.0,0.0,0.0,0.06060606060606061,0.09090909090909091,0.07272727272727272,0.009315599761292813,0.4531148076057434,No,0.0,0.0,0.0
q102,What evaluation metrics are used?,"exact match, f1 score, edit distance and goal match","Human ratings (Likert scale), BLEU, METEOR, ROUGE, Pyramid, F<sub>1</sub> score, Pearson correlation, Mean Squared Error (MSE), Cronbach's alpha.
",0.045454545454545456,0.1111111111111111,0.06451612903225805,0.0,0.0,0.0,0.045454545454545456,0.1111111111111111,0.06451612903225805,0.020877077807874116,0.22338224947452545,No,0.0,0.0,0.0
q103,How were the navigation instructions collected?,using Amazon Mechanical Turk using simulated environments with topological maps,"Collected through Mechanical Turk using 100 simulated environments with corresponding topological maps.
",0.6666666666666666,0.8,0.7272727272727272,0.45454545454545453,0.5555555555555556,0.5,0.6666666666666666,0.8,0.7272727272727272,0.2197281387499716,0.5839466452598572,Yes,0.0,1.0,1.0
q104,What language is the experiment done in?,english language,"English
",1.0,0.5,0.6666666666666666,0.0,0.0,0.0,1.0,0.5,0.6666666666666666,0.0,0.8033708333969116,Yes,1.0,1.0,1.0
q105,What additional features are proposed for future work?,distinguishing between clinically positive and negative phenomena within each risk factor domain and accounting for structured data collected on the target cohort,"Additional features for future work include: deeper analysis of clinical narratives in EHRs, distinguishing between clinically positive and negative phenomena within each risk factor domain,  incorporation of available metadata, preprocessing data with morphological normalization and out-of-vocabulary mappings, word-by-word and history-aware attention mechanisms, and more semantic features.
",0.29411764705882354,0.6818181818181818,0.4109589041095891,0.22,0.5238095238095238,0.3098591549295775,0.2549019607843137,0.5909090909090909,0.35616438356164387,0.21440097943968883,0.4559409022331238,Yes,1.0,1.0,1.0
q106,What are their initial results on this task?,"Achieved the highest per-domain scores on Substance (F1  0.8) and the lowest scores on Interpersonal and Mood (F1  0.5), and show consistency in per-domain performance rankings between MLP and RBF models.","The context states that the ""first n"" baseline produced the same results as the neural regressor, but provides no further details on specific initial results.  No initial performance metrics are given.
",0.12903225806451613,0.11428571428571428,0.12121212121212123,0.0,0.0,0.0,0.12903225806451613,0.11428571428571428,0.12121212121212123,0.014764228628822696,0.2632143497467041,No,0.0,0.0,0.0
q107,How is morphology knowledge implemented in the method?,A BPE model is applied to the stem after morpheme segmentation.,"A morphological word segmentation method segments complex words into morphemes, reducing vocabulary size.  Different segmentation strategies (stem with combined suffix, stem with singular suffix, BPE on stem with combined suffix, BPE on stem with singular suffix) are investigated.
",0.13157894736842105,0.45454545454545453,0.2040816326530612,0.0,0.0,0.0,0.07894736842105263,0.2727272727272727,0.12244897959183672,0.01188432823684058,0.5666888356208801,Yes,0.0,0.0,0.0
q108,How is the performance on the task evaluated?,Comparison of test accuracies of neural network models on an inflection task and qualitative analysis of the errors,"The performance is evaluated using F<sub>1</sub> score for question answering, training and testing curves (averaged over 3 random seeds),  test performance corresponding to best validation performance, F<sub>1info</sub> score, Search Accuracy, Reading Accuracy, Overall Accuracy, average accuracy of predictions, NoOffer rate, Multi-query rate, Goal change rate, and dialogue length distribution.
",0.08928571428571429,0.2777777777777778,0.13513513513513514,0.01818181818181818,0.058823529411764705,0.027777777777777783,0.07142857142857142,0.2222222222222222,0.10810810810810811,0.00600452721165399,0.28180745244026184,No,0.0,0.0,0.0
q109,what datasets were used?,"IWSLT14 German-English, IWSLT14 Turkish-English, WMT14 English-German","Laptop, Restaurant, avsd, Charades, CUB, MSCOCO, Oxford-102, MNIST, Oxford, COCO, CIFAR-10.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.020540268312306344,0.2912618815898895,No,0.0,0.0,0.0
q110,How do they obtain human judgements?,Using crowdsourcing ,"Human judgments are obtained via valence ratings for nearly 14,000 English nouns collected by BIBREF28, and for acceptability judgments in the context of dialogue model evaluation.  For the latter, human judgments are the ground-truth comments of corresponding news.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12407352030277252,No,0.0,0.0,1.0
q111,Does this approach perform better in the multi-domain or single-domain setting?,single-domain setting,"Multi-domain.
",0.5,0.3333333333333333,0.4,0.0,0.0,0.0,0.5,0.3333333333333333,0.4,0.0,0.752661943435669,No,0.0,0.0,0.0
q112,How many samples did they generate for the artificial language?,"70,000","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08871352672576904,No,0.0,0.0,0.0
q113,Why does not the approach from English work on other languages?,"Because, unlike other languages, English does not mark grammatical genders","The approach struggles with subtle relationships between words, particularly conjunctions, and lacks co-reference information.  It also performs less well on languages with different sentence structures (SOV, VSO) compared to those with SVO structure.  Finally,  the approach may not handle the different morphological features of other languages as effectively as it does English.
",0.09433962264150944,0.5,0.15873015873015875,0.019230769230769232,0.1111111111111111,0.03278688524590165,0.05660377358490566,0.3,0.09523809523809525,0.016257798650627393,0.3468281328678131,No,0.0,0.0,0.0
q114,How do they measure grammaticality?,by calculating log ratio of grammatical phrase over ungrammatical phrase,"The provided text describes a method for evaluating fluency, not grammaticality.  No method for measuring grammaticality is described.
",0.05555555555555555,0.1,0.07142857142857142,0.0,0.0,0.0,0.05555555555555555,0.1,0.07142857142857142,0.0,0.4470407962799072,No,0.0,0.0,0.0
q115,What is the difference in recall score between the systems?,"Between the model and Stanford, Spacy and Flair the differences are 42.91, 25.03, 69.8 with Traditional NERs as reference and  49.88, 43.36, 62.43 with Wikipedia titles as reference.","The answer is not found in the context.  While the text discusses recall scores for different models and scenarios, it does not provide a direct comparison of recall scores *between* systems.
",0.1935483870967742,0.17647058823529413,0.18461538461538463,0.03333333333333333,0.030303030303030304,0.03174603174603175,0.12903225806451613,0.11764705882352941,0.12307692307692307,0.01533227700920213,0.37529098987579346,No,0.0,0.0,0.0
q116,What is their f1 score and recall?,"F1 score and Recall are 68.66, 80.08 with Traditional NERs as reference and 59.56, 69.76 with Wikipedia titles as reference.","The provided text discusses F1 scores and recall in relation to various tasks and datasets, but does not give a single, overall F1 score and recall for a specific model or dataset.  The answer is not found in the context.
",0.125,0.20833333333333334,0.15625,0.07692307692307693,0.13043478260869565,0.0967741935483871,0.125,0.20833333333333334,0.15625,0.03828564254373387,0.7271133661270142,No,0.0,0.0,0.0
q117,How many layers does their system have?,4 layers,"The provided texts describe various architectures with differing numbers of layers.  A precise total layer count is not given for any single, unified system.
",0.041666666666666664,0.5,0.07692307692307693,0.0,0.0,0.0,0.041666666666666664,0.5,0.07692307692307693,0.01387819277861591,0.5250334143638611,No,1.0,1.0,1.0
q118,What context modelling methods are evaluated?,"Concat
Turn
Gate
Action Copy
Tree Copy
SQL Attn
Concat + Action Copy
Concat + Tree Copy
Concat + SQL Attn
Turn + Action Copy
Turn + Tree Copy
Turn + SQL Attn
Turn + SQL Attn + Action Copy","The provided text mentions context modeling methods, but does not explicitly list them.  It does describe some, such as ""concatenation method,"" but doesn't enumerate all evaluated methods.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14004459977149963,No,1.0,1.0,1.0
q119,Is the baseline a non-heirarchical model like BERT?,There were hierarchical and non-hierarchical baselines; BERT was one of those baselines,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11696356534957886,No,0.0,0.0,0.0
q120,How better are results compared to baseline models?,F1 score of best authors' model is 55.98 compared to BiLSTM and FastText that have F1 score slighlty higher than 46.61.,"The model significantly outperforms baseline models in most cases,  especially when considering sufficient training data.  Results vary depending on the specific task and data set.
",0.08,0.08695652173913043,0.08333333333333333,0.0,0.0,0.0,0.08,0.08695652173913043,0.08333333333333333,0.017574239156849022,0.3964156210422516,No,0.0,0.0,0.0
q121,How big is dataset used for training/testing?,"4,261  days for France and 4,748 for the UK","The answer is not found in the context.  While various datasets and their splits are described, there's no single, overall answer to the size of the training/testing dataset.
",0.06666666666666667,0.18181818181818182,0.0975609756097561,0.0,0.0,0.0,0.06666666666666667,0.18181818181818182,0.0975609756097561,0.012944532586971997,0.1674509048461914,No,0.0,0.0,0.0
q122,What geometric properties do embeddings display?,Winter and summer words formed two separate clusters. Week day and week-end day words also formed separate clusters.,"The embeddings display geometric properties pertaining to the behavior of the time series, as observed in the t-SNE projection.  Thematic clusters, such as winter/summer and weekdays/weekends, are evident.
",0.12903225806451613,0.21052631578947367,0.15999999999999998,0.0,0.0,0.0,0.0967741935483871,0.15789473684210525,0.12000000000000001,0.014899946568663818,0.45067858695983887,Yes,0.0,1.0,1.0
q123,How accurate is model trained on text exclusively?,Relative error is less than 5%,"The answer is not found in the context.
",0.125,0.16666666666666666,0.14285714285714288,0.0,0.0,0.0,0.125,0.16666666666666666,0.14285714285714288,0.04767707020457095,0.1734209656715393,No,0.0,0.0,0.0
q124,What was their result on Stance Sentiment Emotion Corpus?,F1 score of 66.66%,"The provided text evaluates the approach on the Stance Sentiment Emotion Corpus (SSEC), but does not state any specific result.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.17472073435783386,No,0.0,0.0,0.0
q125,What performance did they obtain on the SemEval dataset?,F1 score of 82.10%,"Our system improved the existing best system of SemEval 2016 task 6 by 3.2 F-score points for sentiment analysis.
",0.09523809523809523,0.4,0.15384615384615385,0.0,0.0,0.0,0.047619047619047616,0.2,0.07692307692307693,0.019146030690102515,0.45289885997772217,No,0.0,0.0,1.0
q126,What are the state-of-the-art systems?,"For sentiment analysis UWB, INF-UFRGS-OPINION-MINING, LitisMind, pkudblab and SVM + n-grams + sentiment and for emotion analysis MaxEnt, SVM, LSTM, BiLSTM and CNN","Deep convolutional networks.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13859516382217407,No,0.0,0.0,0.0
q127,What is the size of their collected dataset?,3347 unique utterances ,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.059737760573625565,No,0.0,0.0,0.0
q128,"What kind of features are used by the HMM models, and how interpretable are those?","A continuous emission HMM uses the hidden states of a 2-layer LSTM as features and a discrete emission HMM uses data as features. 
The interpretability of the model is shown in Figure 2. ","The HMM models identify spaces, indentation, and special characters (like comment symbols in Linux data).  The interpretability is described as being high enough for the model's features to be visualized and understood.
",0.36363636363636365,0.35294117647058826,0.35820895522388063,0.0625,0.06060606060606061,0.06153846153846154,0.21212121212121213,0.20588235294117646,0.20895522388059704,0.05101599307902365,0.41793468594551086,No,0.0,0.0,0.0
q129,What kind of information do the HMMs learn that the LSTMs don't?,The HMM can identify punctuation or pick up on vowels.,"The context states that HMMs and LSTMs learn *different* things about spaces and comments in Linux data, and punctuation in Shakespeare data.  However, it does not specify *what* the HMMs specifically learn that LSTMs don't.
",0.08333333333333333,0.3,0.13043478260869565,0.02857142857142857,0.1111111111111111,0.04545454545454545,0.08333333333333333,0.3,0.13043478260869565,0.01121617874316939,0.5624568462371826,No,1.0,1.0,1.0
q130,How large is the gap in performance between the HMMs and the LSTMs?,"With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower.","The hybrid algorithm performs a bit better than the standalone LSTM on all text datasets.  The effect of the HMM diminishes as the LSTM size increases.
",0.2692307692307692,0.2,0.22950819672131148,0.0,0.0,0.0,0.2692307692307692,0.2,0.22950819672131148,0.02548659823692312,0.4493614137172699,No,0.0,0.0,0.0
q131,what was their system's f1 performance?,"Proposed model achieves 0.86, 0.924, 0.71 F1 score on SR, HATE, HAR datasets respectively.","The system achieved an F1 score of 78.38% on the whole development set.
",0.2857142857142857,0.23529411764705882,0.2580645161290323,0.07692307692307693,0.0625,0.06896551724137931,0.2857142857142857,0.23529411764705882,0.2580645161290323,0.047873443378439184,0.6584826707839966,No,0.0,0.0,0.0
q132,How much more coverage is in the new dataset?,278 more annotations,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.047633085399866104,No,0.0,0.0,0.0
q133,How was quality measured?,"Inter-annotator agreement, comparison against expert annotation, agreement with PropBank Data annotations.","Human evaluation using a 5-point Likert scale, average rank, percentage of best-ranked questions, Pearson correlation with human judgments, and mean squared error (MSE).  Evaluation of question quality also used a validation step where workers judged the validity of questions, not answers.
",0.023255813953488372,0.08333333333333333,0.03636363636363636,0.0,0.0,0.0,0.023255813953488372,0.08333333333333333,0.03636363636363636,0.009903417847796372,0.25844067335128784,No,0.0,0.0,0.0
q134,What is different in the improved annotation protocol?,a trained worker consolidates existing annotations ,"The improved annotation protocol uses a template with scenario-specific event and participant labels, annotates all NP heads with participant labels and all verbs with event labels, and includes coreference chains between NPs.  It also uses a single annotation mode.
",0.05,0.3333333333333333,0.08695652173913045,0.0,0.0,0.0,0.05,0.3333333333333333,0.08695652173913045,0.008522452865706931,0.4437605142593384,Yes,0.0,0.0,0.0
q135,What data were they used to train the multilingual encoder?,WMT 2014 En-Fr parallel corpus,"En  Fr and Fr  En examples.
",0.3333333333333333,0.3333333333333333,0.3333333333333333,0.2,0.2,0.20000000000000004,0.3333333333333333,0.3333333333333333,0.3333333333333333,0.0,0.21363796293735504,No,0.0,0.0,0.0
q136,From when are many VQA datasets collected?,late 2014,"Late 2014.
",1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.8534937500953674,Yes,1.0,1.0,1.0
q137,What is task success rate achieved? ,96-97.6% using the objects color or shape and 79% using shape alone,"Not found in the context.
",0.2,0.07142857142857142,0.10526315789473682,0.0,0.0,0.0,0.2,0.07142857142857142,0.10526315789473682,0.015330462064343475,-0.03624602407217026,No,0.0,0.0,0.0
q138,Does proposed end-to-end approach learn in reinforcement or supervised learning manner?,supervised learning,"Reinforcement learning.
",0.5,0.5,0.5,0.0,0.0,0.0,0.5,0.5,0.5,0.0,0.5885686278343201,No,0.0,0.0,0.0
q139,How is performance of this system measured?,using the BLEU score as a quantitative metric and human evaluation for quality,"Mean rank, ratings (rank converted to 7-i), student t-test, accuracy (Quasar-S and Quasar-T), system combination (average rank score), MAPE, RMSE, MAE, $R^2$, EM-score, F$_1$-score.
",0.0625,0.15384615384615385,0.08888888888888889,0.0,0.0,0.0,0.03125,0.07692307692307693,0.044444444444444446,0.008327748566971627,0.3139597177505493,No,0.0,1.0,1.0
q140,How big dataset is used for training this system?,"For the question generation model 15,000 images with 75,000 questions. For the chatbot model, around 460k utterances over 230k dialogues.","7934 messages (after augmentation) for the training set.  The WSJ corpus (81 hours), an 80 hour subset of clean Librispeech, the full 960 hour Librispeech training set, or a combination of all of them are used for pre-training.
",0.10256410256410256,0.18181818181818182,0.13114754098360654,0.02631578947368421,0.047619047619047616,0.03389830508474576,0.07692307692307693,0.13636363636363635,0.09836065573770493,0.011622974570266495,0.42028072476387024,No,0.0,0.0,0.0
q141,How do they obtain word lattices from words?,By considering words as vertices and generating directed edges between neighboring words within a sentence,"They construct word lattices from an existing lookup vocabulary, or from various word segmentations with different strategies, including using character sequence.
",0.09523809523809523,0.13333333333333333,0.1111111111111111,0.0,0.0,0.0,0.09523809523809523,0.13333333333333333,0.1111111111111111,0.0,0.5008617639541626,No,1.0,1.0,1.0
q142,How better is proposed method than baselines perpexity wise?,Perplexity of proposed MEED model is 19.795 vs 19.913 of next best result on test set.,"The answer is not found in the context.
",0.125,0.05555555555555555,0.07692307692307691,0.0,0.0,0.0,0.125,0.05555555555555555,0.07692307692307691,0.023309208103306348,0.1447410136461258,No,0.0,0.0,0.0
q143,Who manually annotated the semantic roles for the set of learner texts?,Authors,"Two senior students majoring in Applied Linguistics.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2775115370750427,No,0.0,1.0,1.0
q144,How do they obtain region descriptions and object annotations?,they are available in the Visual Genome dataset,"From the Visual Genome dataset.
",0.8,0.5,0.6153846153846154,0.75,0.42857142857142855,0.5454545454545454,0.8,0.5,0.6153846153846154,0.36409302398068727,0.8206645250320435,Yes,1.0,1.0,1.0
q145,Which training dataset allowed for the best generalization to benchmark sets?,MultiNLI,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06096215546131134,No,0.0,0.0,0.0
q146,Which models were compared?,"BiLSTM-max, HBMP, ESIM, KIM, ESIM + ELMo, and BERT","RF model (with reduced features), RF model (full vocabulary), aggregated forecaster (Agg), and neural network methods (reduced vocabulary).  BERT, ESIM + ELMo, and MEED are also mentioned as specific models.
",0.13793103448275862,0.4444444444444444,0.21052631578947367,0.07142857142857142,0.25,0.11111111111111112,0.10344827586206896,0.3333333333333333,0.15789473684210528,0.11381305436860424,0.3536589741706848,No,0.0,0.0,1.0
q147,What is private dashboard?,Private dashboard is leaderboard where competitors can see results after competition is finished - on hidden part of test set (private test set).,"The answer is not found in the context.
",0.125,0.045454545454545456,0.06666666666666667,0.0,0.0,0.0,0.125,0.045454545454545456,0.06666666666666667,0.008574978451193194,0.040497105568647385,No,0.0,0.0,0.0
q148,What is public dashboard?,"Public dashboard where competitors can see their results during competition, on part of the test set (public test set).","Not found in the context.
",0.2,0.05263157894736842,0.08333333333333333,0.0,0.0,0.0,0.2,0.05263157894736842,0.08333333333333333,0.0056772216224713025,0.07751435786485672,No,0.0,0.0,0.0
q149,What dataset do they use?,They used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings and dataset provided in HSD task to train model (details not mentioned in paper).,"MS COCO, Bing, Flickr, avsd, Charades, CUB, MSCOCO, Oxford-102, MNIST, CIFAR-10, AIDA-Train, Wikipedia, AIDA-B, AIDA-CoNLL.
",0.05,0.04,0.044444444444444446,0.0,0.0,0.0,0.05,0.04,0.044444444444444446,0.01286713095960916,0.20936459302902222,No,0.0,0.0,0.0
q150,Do the use word embeddings alone or they replace some previous features of the model with word embeddings?,They use it as addition to previous model - they add new edge between words if word embeddings are similar.,"Word embeddings replace some previous features of the model with word embeddings.
",0.4166666666666667,0.2631578947368421,0.3225806451612903,0.09090909090909091,0.05555555555555555,0.06896551724137931,0.3333333333333333,0.21052631578947367,0.2580645161290323,0.04198387697463197,0.7320820689201355,No,0.0,1.0,1.0
q151,How many natural language explanations are human-written?,Totally 6980 validation and test image-sentence pairs have been corrected.,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03527502360630139,0.19304700195789337,No,0.0,0.0,0.0
q152,What is the dataset used as input to the Word2Vec algorithm?,Italian Wikipedia and Google News extraction producing final vocabulary of 618224 words,"A dump of Italian Wikipedia (2019.04.01), main categories of Italian Google News (WORLD, NATION, BUSINESS, TECHNOLOGY, ENTERTAINMENT, SPORTS, SCIENCE, HEALTH), and anonymized chats between users and a customer care chatbot (Laila).
",0.18181818181818182,0.5,0.26666666666666666,0.0625,0.18181818181818182,0.09302325581395349,0.12121212121212122,0.3333333333333333,0.17777777777777776,0.02433002721900354,0.6049412488937378,No,0.0,0.0,1.0
q153,What methodology is used to compensate for limited labelled data?,Influential tweeters ( who they define as individuals certain to have a classifiable sentiment regarding the topic at hand) is used to label tweets in bulk in the absence of manually-labeled tweets.,"Lightly supervised learning, self-training, and label regularization.
",0.125,0.03125,0.05,0.0,0.0,0.0,0.125,0.03125,0.05,0.004526870957721524,0.20491063594818115,No,0.0,0.0,0.0
q154,What are the baseline state of the art models?,"Stanford NER, BiLSTM+CRF, LSTM+CNN+CRF, T-NER and BiLSTM+CNN+Co-Attention","Pre-trained BERT<sub>base</sub> model fine-tuned on the CNNDM dataset, and a hierarchical transformer summarization model (HeriTransfomer) without pre-training.  Also, linear or multi-layer perceptron (MLP) classifiers.
",0.03225806451612903,0.07142857142857142,0.044444444444444446,0.0,0.0,0.0,0.03225806451612903,0.07142857142857142,0.044444444444444446,0.012090878038257867,0.3379170596599579,No,0.0,0.0,0.0
q155,How do they extract causality from text?,"They identify documents that contain the unigrams 'caused', 'causing', or 'causes'","They constructed ""cause-trees,"" binary trees rooted at ""caused,"" ""causes,"" or ""causing,"" illustrating the most frequent n-grams beginning or ending with those root words.
",0.24,0.5454545454545454,0.3333333333333333,0.125,0.3,0.17647058823529413,0.2,0.45454545454545453,0.2777777777777778,0.01533227700920213,0.4702325463294983,No,1.0,1.0,1.0
q156,"What is the source of the ""control"" corpus?","Randomly selected from a Twitter dump, temporally matched to causal documents","Randomly selected documents, matched temporally to causal documents.
",0.875,0.6363636363636364,0.7368421052631579,0.42857142857142855,0.3,0.3529411764705882,0.75,0.5454545454545454,0.631578947368421,0.18085427584453792,0.7904418110847473,Yes,1.0,1.0,1.0
q157,"What are the selection criteria for ""causal statements""?","Presence of only the exact unigrams 'caused', 'causing', or 'causes'","The selection criteria are not explicitly stated.  The text describes *how* causal statements were identified and analyzed but not *why* or *how* they were chosen in the first place.
",0.06896551724137931,0.2,0.10256410256410257,0.0,0.0,0.0,0.06896551724137931,0.2,0.10256410256410257,0.01184949136625139,0.3193996846675873,No,0.0,0.0,0.0
q158,"Do they use expert annotations, crowdsourcing, or only automatic methods to analyze the corpora?",Only automatic methods,"Expert annotations, crowdsourcing, and automatic methods are used.
",0.25,0.6666666666666666,0.36363636363636365,0.14285714285714285,0.5,0.22222222222222224,0.25,0.6666666666666666,0.36363636363636365,0.07495553473355841,0.2978677451610565,No,0.0,1.0,1.0
q159,how do they collect the comparable corpus?,Randomly from a Twitter dump,"A corpus of poems and a corpus of vernacular literature were collected from online resources.  The poem corpus contains 163K quatrain poems from Tang Poems and Song Poems, and the vernacular literature corpus contains 337K short paragraphs from 281 famous books.
",0.04878048780487805,0.4,0.08695652173913045,0.0,0.0,0.0,0.024390243902439025,0.2,0.04347826086956522,0.009896324688555468,0.07855533808469772,No,0.0,0.0,0.0
q160,How do they collect the control corpus?,Randomly from Twitter,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.01484298799186945,No,0.0,0.0,0.0
q161,What are resolution model variables?,"Variables in the set {str, prec, attr} indicating in which mode the mention should be resolved.","The provided text describes how different features are used in a learning algorithm, but it does not define ""resolution model variables.""
",0.14285714285714285,0.1875,0.16216216216216214,0.0,0.0,0.0,0.09523809523809523,0.125,0.1081081081081081,0.01982256626710344,0.21576352417469025,No,0.0,0.0,0.0
q162,Is the model presented in the paper state of the art?,"No, supervised models perform better for this task.","Yes.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4601079821586609,No,0.0,0.0,0.0
q163,What was the result of the highest performing system?,"For task 1 best F1 score was 0.9391 on closed and 0.9414 on open test.
For task2 best result had: Ratio 0.3175 , Satisfaction 64.53, Fluency 0, Turns -1 and Guide 2","Human
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.026075851172208786,No,0.0,0.0,0.0
q164,What do they mean by answer styles?,well-formed sentences vs concise answers,"Answer styles are different forms of expressing an answer, such as concise phrases without context, or well-formed sentences that make sense without context.  They also include the expected form of the answer (e.g., text span, human-generated, multiple choice).
",0.12195121951219512,0.8333333333333334,0.21276595744680848,0.05,0.4,0.0888888888888889,0.0975609756097561,0.6666666666666666,0.1702127659574468,0.0161391821076343,0.49444809556007385,Yes,1.0,1.0,1.0
q165,What are the baselines that Masque is compared against?,"BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D","""first n"" baseline and the neural regressor.  MTMSNlarge and large-squad models.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18329332768917084,No,1.0,1.0,1.0
q166,What is the performance achieved on NarrativeQA?,"Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1005011796951294,No,0.0,0.0,0.0
q167,"What is an ""answer style""?",well-formed sentences vs concise answers,"Answer styles are related to the context of the question in the answer sentence, and can also include length and speaking styles.
",0.09090909090909091,0.3333333333333333,0.14285714285714288,0.0,0.0,0.0,0.045454545454545456,0.16666666666666666,0.07142857142857144,0.0,0.39917173981666565,No,1.0,1.0,1.0
q168,How are the EAU text spans annotated?,Answer with content missing: (Data and pre-processing section) The data is suited for our experiments because the annotators were explicitly asked to provide annotations on a clausal level.,"Three annotations per instance are collected.  Workers choose a label, highlight words in the hypothesis, and use at least half of the highlighted words to write an explanation for their decision.
",0.22580645161290322,0.2413793103448276,0.23333333333333334,0.0,0.0,0.0,0.0967741935483871,0.10344827586206896,0.09999999999999999,0.017176188460170133,0.5556186437606812,No,0.0,0.0,0.0
q169,Which Twitter corpus was used to train the word vectors?,They collected tweets in Russian language using a heuristic query specific to Russian,"Russian Twitter stream.
",0.3333333333333333,0.07692307692307693,0.125,0.0,0.0,0.0,0.3333333333333333,0.07692307692307693,0.125,0.016836026931676892,0.6832237839698792,Yes,0.0,1.0,1.0
q170,How does proposed word embeddings compare to Sindhi fastText word representations?,"Proposed SG model vs SINDHI FASTTEXT:
Average cosine similarity score: 0.650 vs 0.388
Average semantic relatedness similarity score between countries and their capitals: 0.663 vs 0.391","The proposed Sindhi word embeddings surpassed SdfastText in intrinsic evaluation.
",0.2,0.06666666666666667,0.1,0.0,0.0,0.0,0.2,0.06666666666666667,0.1,0.0,0.5168033242225647,Yes,1.0,1.0,1.0
q171,How many uniue words are in the dataset?,908456 unique words are available in collected corpus.,"618,224
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2953603267669678,No,0.0,0.0,0.0
q172,Which baseline methods are used?,standard parametrized attention and a non-attention baseline,"Sliding Window (SW), Sliding Window + Distance (SW+D), language model baselines (n-gram, RNN), K-means, Skip-thought Vectors, Recursive Neural Network, Paragraph Vector, TF-IDF, LDA, i-vector, x-vector, direct methods, hierarchical methods, iterative methods.
",0.02702702702702703,0.125,0.044444444444444446,0.0,0.0,0.0,0.02702702702702703,0.125,0.044444444444444446,0.0,0.19357171654701233,No,0.0,0.0,0.0
q173,How much is the BLEU score?,Ranges from 44.22 to 100.00 depending on K and the sequence length.,"The answer is not found in the context.  While the text discusses BLEU scores, it does not give a specific numerical value for one.
",0.041666666666666664,0.07142857142857142,0.052631578947368425,0.0,0.0,0.0,0.041666666666666664,0.07142857142857142,0.052631578947368425,0.01650404559570942,0.3754120469093323,No,1.0,1.0,1.0
q174,Which datasets are used in experiments?,Sequence Copy Task and WMT'17,"CUB, MSCOCO, Oxford-102, MNIST, CUB, Oxford, COCO, CIFAR-10, SQuAD, MCTest, CNN/Daily Mail, MS COCO, Bing, Flickr, AIDA-Train, Wikipedia, AIDA-B, AIDA-CoNLL.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.02760728821158409,No,0.0,0.0,0.0
q175,What are new best results on standard benchmark?,"New best results of accuracy (P@1) on Vecmap:
Ours-GeoMMsemi: EN-IT 50.00 IT-EN 42.67 EN-DE 51.60 DE-EN 47.22 FI-EN 39.62 EN-ES 39.47 ES-EN 36.43","BERT BIBREF11 achieved a 0.604 micro-F1 score.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.26821431517601013,No,0.0,0.0,0.0
q176,How better is performance compared to competitive baselines?,"Proposed method vs best baseline result on Vecmap (Accuracy P@1):
EN-IT: 50 vs 50
IT-EN: 42.67 vs 42.67
EN-DE: 51.6 vs 51.47
DE-EN: 47.22 vs 46.96
EN-FI: 35.88 vs 36.24
FI-EN: 39.62 vs 39.57
EN-ES: 39.47 vs 39.30
ES-EN: 36.43 vs 36.06","Our model significantly outperforms baseline models.  The best baselines achieve 33.6% and 28.5% on Quasar-S and Quasar-T, respectively, while human performance is 50% and 60.6%.  Our model is also superior to heuristic models on Quasar-T.
",0.12195121951219512,0.07692307692307693,0.09433962264150944,0.025,0.015625,0.019230769230769232,0.12195121951219512,0.07692307692307693,0.09433962264150944,0.009165534871082124,0.39321663975715637,No,0.0,0.0,0.0
q177,What 6 language pairs is experimented on?,"EN<->ES
EN<->DE
EN<->IT
EN<->EO
EN<->MS
EN<->FI","English, French, Spanish, Russian, Italian, and German.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4679754376411438,No,0.0,0.0,0.0
q178,How do they enrich the positional embedding with length information,They introduce new trigonometric encoding which besides information about position uses additional length information (abs or relative).,"The first variant replaces the variable `pos` in positional encoding equations with the difference `len - pos`, where `len` is the sentence length.  This new length encoding is only in the decoder network.
",0.125,0.23529411764705882,0.16326530612244897,0.0,0.0,0.0,0.0625,0.11764705882352941,0.08163265306122448,0.012350387043213578,0.3746833801269531,Yes,1.0,1.0,1.0
q179,How do they condition the output to a given target-source class?,"They use three groups short/normal/long translation classes to learn length token, which is in inference used to bias network to generate desired length group.","By adding conditions to the training samples, such that the GAN is trained with respect to the underlying conditions.  The condition vector, which can be text, is fed to both the generator and discriminator.
",0.17647058823529413,0.23076923076923078,0.20000000000000004,0.0,0.0,0.0,0.14705882352941177,0.19230769230769232,0.16666666666666669,0.02580262591983483,0.3986779749393463,No,0.0,0.0,0.0
q180,Is this library implemented into Torch or is framework agnostic?,It uses deep learning framework (pytorch),"The library is implemented into PyTorch.
",0.16666666666666666,0.16666666666666666,0.16666666666666666,0.0,0.0,0.0,0.16666666666666666,0.16666666666666666,0.16666666666666666,0.0,0.6590545773506165,Yes,0.0,1.0,1.0
q181,How does this compare to traditional calibration methods like Platt Scaling?,No reliability diagrams are provided and no explicit comparison is made between confidence scores or methods.,"The answer is not found in the context.
",0.125,0.0625,0.08333333333333333,0.0,0.0,0.0,0.125,0.0625,0.08333333333333333,0.023309208103306348,0.061772335320711136,Yes,0.0,0.0,0.0
q182,What's the input representation of OpenIE tuples into the model?,word embeddings,"One-hot encoded vectors of dimensionality determined by the number of nodes and edges in the graph.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12094598263502121,No,0.0,1.0,1.0
q183,"Is CRWIZ already used for data collection, what are the results?","Yes, CRWIZ has been used for data collection and its initial use resulted in 145 dialogues. The average time taken for the task was close to the estimate of 10 minutes, 14 dialogues (9.66%) resolved the emergency in the scenario, and these dialogues rated consistently higher in subjective and objective ratings than those which did not resolve the emergency. Qualitative results showed that participants believed that they were interacting with an automated assistant.","Yes, CRWIZ is used for data collection.  A dataset was collected and compared to one collected in a controlled lab setting.  The advantages/disadvantages of both approaches are discussed.
",0.41379310344827586,0.16216216216216217,0.23300970873786403,0.17857142857142858,0.0684931506849315,0.099009900990099,0.3448275862068966,0.13513513513513514,0.1941747572815534,0.03091009701575783,0.6971855163574219,Yes,0.0,0.0,1.0
q184,What contextual features are used?,The words that can indicate the characteristics of the neighbor words as contextual keywords and generate it from the automatically extracted contextual keywords.,"The contextual feature vector is defined by the frequency of a token in the whole corpus and the frequency of contextual keywords from windowed portions of text centered on the token.  Contextual keywords are automatically extracted and satisfy specific conditions related to manually annotated IOCs and a frequency lower bound.
",0.26,0.5652173913043478,0.3561643835616438,0.061224489795918366,0.13636363636363635,0.08450704225352113,0.2,0.43478260869565216,0.273972602739726,0.027091441261496742,0.4761238098144531,Yes,1.0,1.0,1.0
q185,How is the data in RAFAEL labelled?,"Using a set of annotation tools such as Morfeusz, PANTERA, Spejd, NERF and Liner","The knowledge base is indexed by a search engine, annotated using NLP tools (possibly also during the answering stage), and annotated with named entities (NER) using NERF, Liner2, Quant, and DeepER.  The UD schema also annotates for features interpretable as derivational.
",0.17073170731707318,0.5,0.2545454545454546,0.0,0.0,0.0,0.12195121951219512,0.35714285714285715,0.18181818181818182,0.011777214967444383,0.37977299094200134,No,0.0,1.0,1.0
q186,How is the fluctuation in the sense of the word and its neighbors measured?,"Our method performs a statistical test to determine whether a given word is used polysemously in the text, according to the following steps:
1) Setting N, the size of the neighbor.
2) Choosing N neighboring words ai in the order whose angle with the vector of the given word w is the smallest.
3) Computing the surrounding uniformity for ai(0 < i  N) and w.
4) Computing the mean m and the sample variance  for the uniformities of ai .
5) Checking whether the uniformity of w is less than m  3. If the value is less than m  3, we may regard w as a polysemic word.","Surrounding uniformity, calculated from the statistical fluctuation in vectors corresponding to words in the neighbor.
",0.7333333333333333,0.102803738317757,0.18032786885245902,0.21428571428571427,0.02830188679245283,0.05,0.4,0.056074766355140186,0.09836065573770492,0.0001794484463692311,0.6280112266540527,Yes,0.0,1.0,1.0
q187,By how much does SPNet outperforms state-of-the-art abstractive summarization methods on evaluation metrics?,"SPNet vs best baseline:
ROUGE-1: 90.97 vs 90.68
CIC: 70.45 vs 70.25","SPNet outperforms other methods in all automatic evaluation metrics with a large margin.
",0.07692307692307693,0.058823529411764705,0.06666666666666667,0.0,0.0,0.0,0.07692307692307693,0.058823529411764705,0.06666666666666667,0.026440681349450875,0.5507586598396301,No,0.0,0.0,1.0
q188,"Is it expected to have speaker role, semantic slot and dialog domain annotations in real world datasets?","Not at the moment, but summaries can be additionaly extended with this annotations.","The context mentions that speaker role and dialog domain annotations are often available in datasets, but semantic slot annotations are less common and can be relaxed to important entities.
",0.1724137931034483,0.38461538461538464,0.23809523809523808,0.03571428571428571,0.08333333333333333,0.05,0.13793103448275862,0.3076923076923077,0.1904761904761905,0.03754162526467109,0.4199337959289551,No,1.0,1.0,1.0
q189,How does new evaluation metric considers critical informative entities?,Answer with content missing: (formula for CIC) it accounts for the most important information within each dialog domain. CIC can be applied to any summarization task with predefined essential entities,"CIC (Critical Information Completeness) is a recall metric of semantic slot information between a candidate and reference summary.
",0.1111111111111111,0.06666666666666667,0.08333333333333334,0.0,0.0,0.0,0.1111111111111111,0.06666666666666667,0.08333333333333334,0.014622771495305926,0.533576250076294,Yes,1.0,1.0,1.0
q190,What are state of the art methods MMM is compared to?,"FTLM++, BERT-large, XLNet","MTMSN, NABERT+, MLP, MCB, NMN
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.092875289995668,0.25994715094566345,No,0.0,0.0,0.0
q191,What are the problems related to ambiguity in PICO sentence prediction tasks?,Some sentences are associated to ambiguous dimensions in the hidden state output,"Mapping emotions to facial expressions, personal taste, and lack of consensus on terms like ""many people"" or ""crowded"".
",0.05555555555555555,0.08333333333333333,0.06666666666666667,0.0,0.0,0.0,0.05555555555555555,0.08333333333333333,0.06666666666666667,0.015061893230938673,0.09556866437196732,No,0.0,0.0,1.0
q192,How is knowledge stored in the memory?,entity memory and relational memory.,"Knowledge is stored in a memory matrix, $\mathbf{E} \in \mathbb{R}^{d_E \times K}$, where each row represents an entity (ingredient) and is updated after each recipe step.  Entity representations ($\mathbf{e}_i$) are initialized with CharCNN and pre-trained GloVe embeddings.
",0.06818181818181818,0.6,0.1224489795918367,0.0,0.0,0.0,0.045454545454545456,0.4,0.0816326530612245,0.007195175746258358,0.49961185455322266,No,0.0,0.0,0.0
q193,How do they measure the diversity of inferences?,by number of distinct n-grams,"The provided text does not describe how diversity of inferences is measured.  It discusses diversity of entities and corpora, not inferences.
",0.047619047619047616,0.16666666666666666,0.07407407407407407,0.0,0.0,0.0,0.047619047619047616,0.16666666666666666,0.07407407407407407,0.015732934811145335,0.16158093512058258,No,0.0,0.0,0.0
q194,By how much do they improve the accuracy of inferences over state-of-the-art methods?,"ON Event2Mind, the accuracy of proposed method is improved by  absolute BLUE  2.9,  10.87, 1.79 for xIntent, xReact and oReact respectively.
On Atomic dataset, the accuracy of proposed method is improved by  absolute BLUE 3.95.   4.11, 4.49 for xIntent, xReact and oReact.respectively.","The answer is not found in the context.  While improvements in accuracy for various tasks are discussed, no comparison to ""state-of-the-art methods"" is made in terms of inference improvement.
",0.34375,0.22448979591836735,0.2716049382716049,0.03225806451612903,0.020833333333333332,0.02531645569620253,0.25,0.16326530612244897,0.19753086419753088,0.010469408829546487,0.4168602526187897,No,0.0,0.0,0.0
q195,How does the context-aware variational autoencoder learn event background information?," CWVAE is trained on an auxiliary dataset to learn the event background information by using the context-aware latent variable.  Then, in finetute stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target.","In the pretraining stage, the model is trained on an auxiliary dataset with rich event context information.  The context-acquiring latent variable ($z_c$) is directly conditioned on the context, allowing it to learn background knowledge.  This knowledge is then transferred to a context-aware latent variable ($z_{c}$) by minimizing the distance between them.
",0.4727272727272727,0.5531914893617021,0.5098039215686275,0.2037037037037037,0.2391304347826087,0.22000000000000003,0.32727272727272727,0.3829787234042553,0.35294117647058826,0.1164745508151451,0.6901190280914307,Yes,1.0,1.0,1.0
q196,How much improvement does their method get over the fine tuning baseline?,"0.08 points on the 2011 test set, 0.44 points on the 2012 test set, 0.42 points on the 2013 test set for IWSLT-CE.","The context states that the proposed iterative learning approach significantly outperforms non-iterative settings, and that calibrating confidence using binary classification loss improves both reranking and assertion generation compared to the base model.  It also mentions performance augmentation is over 9% for BERT-Base and BERT-Large models compared to baselines.  However, the exact improvement over *fine-tuning* baselines is not specified.
",0.08064516129032258,0.18518518518518517,0.11235955056179775,0.0,0.0,0.0,0.08064516129032258,0.18518518518518517,0.11235955056179775,0.008917334597397072,0.26892945170402527,No,0.0,0.0,0.0
q197,By how much do they outpeform previous results on the word discrimination task?,Their best average precision tops previous best result by 0.202,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.01928919367492199,No,0.0,0.0,0.0
q198,How many paraphrases are generated per question?,"10*n paraphrases, where n depends on the number of paraphrases that contain the entity mention spans","The answer is not found in the context.
",0.25,0.11764705882352941,0.15999999999999998,0.0,0.0,0.0,0.25,0.11764705882352941,0.15999999999999998,0.015694957190569982,0.15200206637382507,No,0.0,0.0,0.0
q199,How strong was the correlation between exercise and diabetes?,weak correlation with p-value of 0.08,"The context states that exercising is associated with controlling diabetes and assisting with meal planning.  However, it does not quantify the strength of the correlation.
",0.12,0.375,0.18181818181818182,0.0,0.0,0.0,0.08,0.25,0.12121212121212122,0.017574239156849022,0.20639249682426453,No,0.0,0.0,0.0
q200,How were topics of interest about DDEO identified?,using topic modeling model Latent Dirichlet Allocation (LDA),"Tweets using the words ""diabetes, diet, exercise, and obesity"" were collected using Twitter's API.  A limited number of queries were used to select the initial pool of tweets.
",0.034482758620689655,0.125,0.05405405405405405,0.0,0.0,0.0,0.034482758620689655,0.125,0.05405405405405405,0.010559717178170818,0.04903822019696236,No,0.0,1.0,1.0
q201,How do their train their embeddings?,"The embeddings are learned several times using the training set, then the average is taken.","The context describes various embedding training methods: CBOW embeddings trained on comments (with different tokenization methods), sentence embeddings trained using the RoBERTa architecture, and pre-trained embeddings like GloVe and contextual embeddings (BERT).  Some teams trained their own embeddings.
",0.1282051282051282,0.3333333333333333,0.18518518518518517,0.02631578947368421,0.07142857142857142,0.038461538461538464,0.1282051282051282,0.3333333333333333,0.18518518518518517,0.02126286556148234,0.429923415184021,No,0.0,0.0,0.0
q202,How do they model travel behavior?,The data from collected travel surveys is used to model travel behavior.,"The model uses a mapping that preserves semantic distance relative to a choice problem, analogous to word embeddings.  It also uses attention mechanisms to correlate navigation instructions with a topological map of the environment to predict navigation plans.
",0.10526315789473684,0.3333333333333333,0.16,0.0,0.0,0.0,0.07894736842105263,0.25,0.11999999999999998,0.012663302792223932,0.46766477823257446,No,0.0,0.0,0.0
q203,How do their interpret the coefficients?,The coefficients are projected back to the dummy variable space.,"The sign of the coefficient $\beta_w$ indicates the impact on the time series.  A positive coefficient ($\beta_w > 0$) indicates a rise in the time series, and a negative coefficient ($\beta_w < 0$) indicates a decline.
",0.08108108108108109,0.3,0.12765957446808512,0.027777777777777776,0.1111111111111111,0.04444444444444444,0.08108108108108109,0.3,0.12765957446808512,0.008237054296530683,0.2674567699432373,No,0.0,1.0,1.0
q204,By how much do they outperform previous state-of-the-art models?,"Proposed ORNN has 0.769, 1.238, 0.818, 0.772 compared to 0.778, 1.244, 0.813, 0.781 of best state of the art result on Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.)","Our best model improves CoNLL F1 score by 2.93% and 3.01% over Stanford, 1.41% and 1.77% over Multigraph, and 2.62% and 3.02% over MIR.
",0.13333333333333333,0.08,0.1,0.0,0.0,0.0,0.1,0.06,0.075,0.007410183148990832,0.3127855658531189,No,0.0,0.0,0.0
q205,What is the performance difference between proposed method and state-of-the-arts on these datasets?,Difference is around 1 BLEU score lower on average than state of the art methods.,"The proposed model outperformed other state-of-the-art methods overall, and in two categories (Person and Miscellaneous).  TRADE, a state-of-the-art model, performed poorly on the dataset.
",0.2,0.4,0.26666666666666666,0.13793103448275862,0.2857142857142857,0.18604651162790697,0.16666666666666666,0.3333333333333333,0.2222222222222222,0.016961343903963387,0.3033694624900818,No,0.0,0.0,0.0
q206,What benchmarks are created?,Answer with content missing: (formula) The accuracy is defined as the ratio # of correct chains predicted to # of evaluation samples,"SQuAD, UMBC corpus for Twitter NER, Twitter NER corpus (BIBREF21), Making Sense of Microposts (MSM) initiative, OntoNotes, ACE, MUC 7, Ritter NER corpus, MSM 2013 corpus, and UMBC Twitter corpus.
",0.03333333333333333,0.05,0.04,0.0,0.0,0.0,0.03333333333333333,0.05,0.04,0.01095208360430825,0.004670694936066866,No,0.0,0.0,0.0
q207,What percentage fewer errors did professional translations make?,36%,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.17132845520973206,No,0.0,0.0,0.0
q208,What was the weakness in Hassan et al's evaluation design?,"MT developers to which crowd workers were compared are usually not professional translators, evaluation of sentences in isolation prevents raters from detecting translation errors, used not originally written Chinese test set
","Non-experts disregarded translation nuances, leading to a more tolerant judgment of MT systems and lower inter-annotator agreement.  Human rater performance may vary over time due to learning or fatigue, and the unit of evaluation was a concern.
",0.15384615384615385,0.1935483870967742,0.17142857142857143,0.0,0.0,0.0,0.07692307692307693,0.0967741935483871,0.08571428571428572,0.01456486634235763,0.6405585408210754,No,0.0,0.0,0.0
q209,What evidence do they present that the model attends to shallow context clues?,Using model gradients with respect to input features they presented that the most important model inputs are verbs associated with entities which shows that the model attends to shallow context clues,"Figure FIGREF37 shows that the model's most important inputs are verbs possibly associated with the entity butter, in addition to the entity's mentions themselves.  It further shows that the model learns to extract shallow clues of identifying actions exerted upon only the entity being tracked.
",0.40425531914893614,0.6129032258064516,0.48717948717948717,0.17391304347826086,0.26666666666666666,0.2105263157894737,0.3617021276595745,0.5483870967741935,0.43589743589743596,0.09668023126159721,0.6853685975074768,Yes,0.0,0.0,1.0
q210,In what way is the input restructured?,"In four entity-centric ways - entity-first, entity-last, document-level and sentence-level","The input sentence is recursively transformed into structurally simplified sentences and a semantic hierarchy is established between them using a set of hand-crafted grammar rules.
",0.07692307692307693,0.14285714285714285,0.1,0.0,0.0,0.0,0.038461538461538464,0.07142857142857142,0.05,0.014445809981770858,0.46546077728271484,No,0.0,0.0,0.0
q211,What language is the Twitter content in?,English,"English, Russian
",0.5,1.0,0.6666666666666666,0.0,0.0,0.0,0.5,1.0,0.6666666666666666,0.0,0.6316617727279663,No,1.0,1.0,1.0
q212,What evaluations did the authors use on their system?,"BLEU scores, exact matches of words in both translations and topic cache, and cosine similarities of adjacent sentences for coherence.","Human evaluation of translations, head-to-head ranking of system outputs, and human evaluation of Chinese human-computer dialogue.
",0.21052631578947367,0.2,0.20512820512820512,0.0,0.0,0.0,0.21052631578947367,0.2,0.20512820512820512,0.044866567316469146,0.48835331201553345,No,0.0,0.0,0.0
q213,What accuracy does CNN model achieve?,Combined per-pixel accuracy for character line segments is 74.79,"The context states that CNN accuracy drops from around 0.8 to 0.5 when noise rate increases from 0 to 0.5, but NetAb outperforms CNN.  No specific CNN accuracy is given.
",0.06060606060606061,0.18181818181818182,0.09090909090909091,0.0,0.0,0.0,0.06060606060606061,0.18181818181818182,0.09090909090909091,0.013355980882200826,0.35611265897750854,No,0.0,0.0,0.0
q214,How many documents are in the Indiscapes dataset?,508,"508
",1.0,1.0,1.0,0.0,0.0,0.0,1.0,1.0,1.0,0.0,1.0,Yes,0.0,1.0,1.0
q215,What are simulated datasets collected?,There are 6 simulated datasets collected which is initialised with a corpus of size 550 and simulated by generating new documents from Wikipedia extracts and replacing existing documents,"Simulated datasets contain documents and their revisions, generated through operations like deletion, addition, and replacement of words, sentences, paragraphs, and section names.  They include corpora (Corpus 0-5) and revisions from specific time periods.  Also included are datasets mapping pictures and questions (MS COCO, Bing, Flickr), and an open-domain conversation dataset.
",0.21153846153846154,0.39285714285714285,0.275,0.0392156862745098,0.07407407407407407,0.05128205128205129,0.11538461538461539,0.21428571428571427,0.15,0.00864906706425476,0.6423072218894958,No,0.0,0.0,1.0
q216,What human evaluation metrics were used in the paper?,rating questions on a scale of 1-5 based on fluency of language used and relevance of the question to the context,"Task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, partner collaboration, information ease, task ease, user expertise, and a 5-point Likert scale for question quality.
",0.2413793103448276,0.3181818181818182,0.27450980392156865,0.0,0.0,0.0,0.13793103448275862,0.18181818181818182,0.1568627450980392,0.015863148730547675,0.5027071237564087,No,0.0,0.0,1.0
q217,"For the purposes of this paper, how is something determined to be domain specific knowledge?",reviews under distinct product categories are considered specific domain knowledge,"Domain-specific knowledge is described by the dominant subject matter of text, which might give specialized vocabulary or specific, unusual word senses.  Examples include ""financial text"" or ""popular culture.""
",0.10344827586206896,0.3,0.15384615384615385,0.0,0.0,0.0,0.06896551724137931,0.2,0.10256410256410257,0.012193311110766474,0.3475876748561859,No,0.0,0.0,0.0
q218,What type of model are the ELMo representations used in?,A bi-LSTM with max-pooling on top of it,"CNNs and Bi-directional LSTMs.
",0.4,0.2,0.26666666666666666,0.0,0.0,0.0,0.4,0.2,0.26666666666666666,0.0,0.4516381025314331,No,1.0,1.0,1.0
q219,By how much does using phonetic feedback improve state-of-the-art systems?,Improved AECNN-T by 2.1 and AECNN-T-SM BY 0.9,"The context states that the model using phonetic feedback outperforms the best character-based result in the literature (Deep Speech 2) on the WSJ benchmark.
",0.04,0.07692307692307693,0.052631578947368425,0.0,0.0,0.0,0.04,0.07692307692307693,0.052631578947368425,0.0,0.28477147221565247,No,1.0,1.0,1.0
q220,what are the baselines?,"AS Reader, GA Reader, CAS Reader","i-vector system BIBREF3, x-vector system BIBREF10, Sliding Window (SW), Sliding Window + Distance (SW+D), Majority Class, Exact Match, First Occurrence, LSTM, ELMo$_{token}$, ELMo$_{sent}$, GPT, 3-gram, 4-gram, 5-gram, bidirectional RNN language model.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.006138747833681335,0.24897709488868713,No,0.0,0.0,0.0
q221,What are the results of the experiment?,"They were able to create a language model from the dataset, but did not test.","The provided text describes experiments, but does not summarize results in a concise, generalizable way.  Specific results are presented for different experiments, but not a unified ""result"" for all experiments.  For example, increasing documents retrieved in a search can increase recall but decrease precision.  The text also discusses participant evaluations and system performance, but again, no single, overarching result is given.  The answer is not found in the context.
",0.057971014492753624,0.26666666666666666,0.09523809523809525,0.0,0.0,0.0,0.057971014492753624,0.26666666666666666,0.09523809523809525,0.011419539161359998,0.2632347047328949,No,0.0,0.0,0.0
q222,How was the dataset collected?,extracted text from Sorani Kurdish books of primary school and randomly created sentences,"The Polish quiz show dataset was downloaded as a single database dump.  Laptop and Restaurant datasets were collected from SemEval-2016.  The HEOT dataset was from a previous study by Mathur et al.  An English tweet dataset was from a study by Davidson et al.
",0.044444444444444446,0.15384615384615385,0.06896551724137931,0.0,0.0,0.0,0.022222222222222223,0.07692307692307693,0.034482758620689655,0.009044512123285879,0.273893266916275,No,0.0,0.0,0.0
q223,How many annotators participated?,1,"4
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.360342413187027,No,0.0,0.0,0.0
q224,How do they show their model discovers underlying syntactic structure?,By visualizing syntactic distance estimated by the parsing network,"The model's unsupervised parsing outperforms some strong baseline models, demonstrating that the structure found by their model is similar to the intrinsic structure provided by human experts.
",0.14285714285714285,0.4444444444444444,0.21621621621621623,0.0,0.0,0.0,0.07142857142857142,0.2222222222222222,0.10810810810810811,0.01693409667719809,0.36335429549217224,No,0.0,0.0,0.0
q225,How do they measure performance of language model tasks?,"BPC, Perplexity","Task finish rate (simulations), BLEU score, accuracy (NLI), human evaluation (ranking translations against a reference).
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.015061893230938673,0.07353753596544266,No,0.0,0.0,0.0
q226,How are content clusters used to improve the prediction of incident severity?,they are used as additional features in a supervised classification task,"Content clusters, combined with externally imposed classifications, improve the performance of a supervised classifier predicting incident harm severity.  The clusters provide complementary information, especially in cases with highly homogeneous medical content, and mitigate human error in category selection.
",0.10526315789473684,0.36363636363636365,0.16326530612244897,0.02702702702702703,0.1,0.0425531914893617,0.05263157894736842,0.18181818181818182,0.08163265306122448,0.01841913568484358,0.3963543772697449,Yes,1.0,1.0,1.0
q227,What cluster identification method is used in this paper?,"A combination of Minimum spanning trees, K-Nearest Neighbors and Markov Stability BIBREF15, BIBREF16, BIBREF17, BIBREF18","Markov Stability (MS), a multiscale community detection algorithm.
",0.375,0.1875,0.25,0.14285714285714285,0.06666666666666667,0.09090909090909091,0.25,0.125,0.16666666666666666,0.0418278227513131,0.47379881143569946,No,0.0,1.0,1.0
q228,Why did they think this was a good idea?,They think it will help human TCM practitioners make prescriptions.,"This approach allowed for guided dialogues, which led to better data for training machine learning models, reduced the amount of data needed, and improved the accuracy of the model's performance.  It also saved computing resources.
",0.027777777777777776,0.1,0.04347826086956522,0.0,0.0,0.0,0.027777777777777776,0.1,0.04347826086956522,0.00918708060432945,0.3196808993816376,No,0.0,0.0,0.0
q229,What QA models were used?,"A pointer network decodes the answer from a bidirectional LSTM with attention flow layer and self-matching layer, whose inputs come from word and character embeddings of the query and input text fed through a highway layer.","Pointer Networks, Self-Matching Networks, QA-BERT, and QA-DQN.
",0.5,0.13513513513513514,0.21276595744680854,0.2222222222222222,0.05555555555555555,0.08888888888888888,0.5,0.13513513513513514,0.21276595744680854,0.004237000139470399,0.44120773673057556,No,1.0,1.0,1.0
q230,How do their results compare against other competitors in the PAN 2017 shared task on Author Profiling?,"They achieved best result in the PAN 2017 shared task with accuracy for Variety prediction task 0.0013 more than the 2nd best baseline, accuracy for Gender prediction task 0.0029 more than 2nd best baseline and accuracy for Joint prediction task 0.0101 more than the 2nd best baseline","N-GrAM was the best-performing system out of 22 submitted.
",0.18181818181818182,0.04,0.06557377049180327,0.0,0.0,0.0,0.18181818181818182,0.04,0.06557377049180327,0.000938703292510158,0.31861254572868347,Yes,1.0,1.0,1.0
q231,On which task does do model do worst?,Gender prediction task,"""ZUL""
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.008581547066569328,No,0.0,0.0,0.0
q232,On which task does do model do best?,Variety prediction task,"The context does not specify a task on which the model performs best.  It describes various models and analyses, but no comparison of performance across tasks is made.
",0.03571428571428571,0.3333333333333333,0.06451612903225806,0.0,0.0,0.0,0.03571428571428571,0.3333333333333333,0.06451612903225806,0.011993481292529619,0.1887831687927246,No,0.0,0.0,0.0
q233,How does counterfactual data augmentation aim to tackle bias?,The training dataset is augmented by swapping all gendered words by their other gender counterparts,"By swapping gendered words.
",1.0,0.26666666666666666,0.4210526315789474,0.6666666666666666,0.14285714285714285,0.23529411764705882,1.0,0.26666666666666666,0.4210526315789474,0.031998052130773655,0.6494800448417664,Yes,0.0,1.0,1.0
q234,"In the targeted data collection approach, what type of data is targetted?",Gendered characters in the dataset,"Sentiment polarities of a sentence over ""opinion targets"" that explicitly appear in the sentence.
",0.14285714285714285,0.4,0.21052631578947364,0.07692307692307693,0.25,0.11764705882352941,0.14285714285714285,0.4,0.21052631578947364,0.045739135561238015,0.1434703916311264,No,0.0,0.0,0.0
q235,How do they determine which words are informative?,Informative are those that will not be suppressed by regularization performed.,"Informative words are nouns and verbs that are matched if their raw text, lemmatizations, WordNet synsets, or aliases are the same.
",0.14285714285714285,0.2727272727272727,0.18749999999999997,0.0,0.0,0.0,0.14285714285714285,0.2727272727272727,0.18749999999999997,0.021300733682208986,0.5209739208221436,No,0.0,0.0,0.0
q236,What improvement does the MOE model make over the SOTA on language modelling?,Perpexity is improved from 34.7 to 28.0.,"The MoE model achieves lower test perplexity and better BLEU scores on 11 of 12 language pairs than the multilingual GNMT model, and even beats monolingual GNMT models on 8 of 12.  It also has a lower test perplexity than the best published model by a specific amount (INLINEFORM2).
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.006735878102046052,0.27410393953323364,No,0.0,0.0,0.0
q237,What is the difference in performance between the interpretable system (e.g. vectors and cosine distance) and LSTM with ELMo system?,Accuracy of best interpretible system was 0.3945 while accuracy of LSTM-ELMo net was 0.6818.,"The context states that if interpretability and explainability are required, a system combining vector distance and edit distance might be better than the ELMo-based LSTM system.  It does not quantify the difference in performance.
",0.11428571428571428,0.23529411764705882,0.15384615384615385,0.0,0.0,0.0,0.08571428571428572,0.17647058823529413,0.11538461538461539,0.01184949136625139,0.48428741097450256,No,0.0,1.0,1.0
q238,Which language-pair had the better performance?,French-English,"English-French
",1.0,1.0,1.0,0.0,0.0,0.0,0.5,0.5,0.5,0.0,0.9834061861038208,No,0.0,0.0,0.0
q239,Which psycholinguistic and basic linguistic features are used?,"Emotion Sensor Feature, Part of Speech, Punctuation, Sentiment Analysis, Empath, TF-IDF Emoticon features","Emoticons, punctuation, modifiable inflectional features, fixed lexical features, part-of-speech (POS), POS ratios, discourse connectors, spelling errors, repeated characters, and sentiment-based features.
",0.3333333333333333,0.5714285714285714,0.4210526315789474,0.13043478260869565,0.23076923076923078,0.16666666666666669,0.25,0.42857142857142855,0.3157894736842105,0.017577459356554814,0.767737865447998,Yes,0.0,1.0,1.0
q240,How have the differences in communication styles between Twitter and Facebook increase the complexity of the problem?,Systems do not perform well both in Facebook and Twitter texts,"The context does not contain information about Facebook communication styles, so the answer is not found in the context.
",0.15789473684210525,0.2727272727272727,0.19999999999999998,0.0,0.0,0.0,0.10526315789473684,0.18181818181818182,0.13333333333333333,0.023901021968803136,0.3830249309539795,No,0.0,0.0,0.0
q241,What data/studies do the authors provide to support the assertion that the majority of aggressive conversations contain code-mixed languages?,None,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.21160225570201874,No,0.0,0.0,0.0
q242,What datasets are used to evaluate the model?,"WN18, FB15k","AIDA-B and other datasets.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19672156870365143,No,0.0,0.0,0.0
q243,What baseline models do they compare against?,"SLQA, Rusalka, HMA Model (single), TriAN (single), jiangnan (ensemble), MITRE (ensemble), TriAN (ensemble), HMA Model (ensemble)","BIBREF11, BIBREF14, BIBREF21, and NABERT+
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.003491092613871826,0.3363056480884552,No,1.0,1.0,1.0
q244,What are the differences with previous applications of neural networks for this task?,This approach considers related images,"The context describes newer applications using sequence-to-sequence transformer models to avoid overfitting and capture distant character information, subword information integrated with Lattice LSTM, and gated recursive neural networks with reset and update gates to handle complex character combinations.  Previous methods, like those using LSTM to capture previous important information or deep layers to learn character representations, are mentioned but not contrasted in detail.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08368978649377823,No,0.0,0.0,0.0
q245,How much improvement is gained from the proposed approaches?,It eliminates non-termination in some models fixing for some models up to 6% of non-termination ratio.,"The provided text describes improvements in overall performance, outperformance in two categories, and  superiority to other methods in detecting PTSD users.  However, it does not quantify the *amount* of improvement.  Therefore, the answer is not found in the context.
",0.10256410256410256,0.2222222222222222,0.14035087719298245,0.0,0.0,0.0,0.07692307692307693,0.16666666666666666,0.10526315789473684,0.010755798168739934,0.2643623650074005,No,0.0,0.0,0.0
q246,Is infinite-length sequence generation a result of training with maximum likelihood?,There are is a strong conjecture that it might be the reason but it is not proven.,"Not found in the context.
",0.4,0.11764705882352941,0.1818181818181818,0.0,0.0,0.0,0.2,0.058823529411764705,0.0909090909090909,0.013063150080561919,0.1049891859292984,No,1.0,1.0,1.0
q247,How big is dataset for this challenge?,"133,287 images","There are 26,700 question-comment pairs for task A, 2,670 question-question pairs for task B, and 26,700 question-comment pairs for task C.  The test dataset includes 50 questions, 500 related questions, and 5,000 comments.  Additionally, there are 20,000 paper titles, with a 10% development set.  There are 70,000 to 170,000 raw tweets,  12,350 labeled ads, and  5,000 balanced validation and 10,000 test tweets.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2546500861644745,No,0.0,0.0,0.0
q248,How better is performance of proposed model compared to baselines?,"Proposed model achive 66+-22 win rate, baseline CNN 13+-1  and baseline FiLM 32+-3 .","The proposed model leads to improvements ranging from 0.5% to 3.0% in accuracy, and outperforms baselines in new environments by a clear margin.
",0.2,0.3125,0.24390243902439027,0.041666666666666664,0.06666666666666667,0.05128205128205127,0.16,0.25,0.19512195121951217,0.01962672844874697,0.5318109393119812,No,1.0,1.0,1.0
q249,What DCGs are used?,Author's own DCG rules are defined from scratch.,"DGC-ave, DGC-max, DGC-gated.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04923026124015933,0.3573300838470459,No,0.0,0.0,0.0
q250,What is the performance difference of using a generated summary vs. a user-written one?,2.7 accuracy points,"The models consistently perform better with a user-written (gold) summary than with a system-generated summary.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1839207112789154,Yes,0.0,1.0,1.0
q251,What evaluation metrics did they look at?,accuracy with standard deviation,"Partner collaboration, information ease, task ease, user expertise, task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, F<sub>1</sub> score, training curves, test performance.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12135467678308487,No,0.0,0.0,0.0
q252,What are the datasets used for the task?,"Datasets used are Celex (English, Dutch), Festival (Italian), OpenLexuque (French), IIT-Guwahati (Manipuri), E-Hitz (Basque)","HEOT (cleaned tweets), Davidson et al. (English tweets), MS COCO, Bing, Flickr, SST-2, Yelp, MRPC, STS-B, QQP (GLUE-Para), SQuAD 1.1, SQuAD 2.0, MRQA.
",0.03571428571428571,0.0625,0.045454545454545456,0.0,0.0,0.0,0.03571428571428571,0.0625,0.045454545454545456,0.03772657799190421,0.37717360258102417,No,0.0,0.0,0.0
q253,What is the accuracy of the model for the six languages tested?,"Authors report their best models have following accuracy: English CELEX (98.5%), Dutch CELEX (99.47%), Festival (99.990%), OpenLexique (100%), IIT-Guwahat (95.4%), E-Hitz (99.83%)","91.2% for English, French, and German; 94.9  0.3% for Manipuri.  Other accuracies are mentioned for specific language combinations, but not a single ""six-language"" overall accuracy.
",0.06896551724137931,0.06896551724137931,0.06896551724137931,0.0,0.0,0.0,0.034482758620689655,0.034482758620689655,0.034482758620689655,0.01263284914106771,0.5300987362861633,No,0.0,0.0,0.0
q254,Which models achieve state-of-the-art performances?,"CELEX (Dutch and English) - SVM-HMM
Festival, E-Hitz and OpenLexique - Liang hyphenation
IIT-Guwahat - Entropy CRF","KIM, and possibly HUN and ITA (for SPA).
",0.25,0.11764705882352941,0.15999999999999998,0.0,0.0,0.0,0.25,0.11764705882352941,0.15999999999999998,0.02599942497302821,0.3391548693180084,No,0.0,0.0,0.0
q255,Which competitive relational classification models do they test?,For relation prediction they test TransE and for relation extraction they test position aware neural sequence model,"The context does not mention any competitive relational classification models.  It describes baseline models using Logistic Regression.
",0.11764705882352941,0.11764705882352941,0.11764705882352941,0.0,0.0,0.0,0.11764705882352941,0.11764705882352941,0.11764705882352941,0.0,0.37091511487960815,No,0.0,0.0,0.0
q256,How do they gather human judgements for similarity between relations?,By assessing similarity of 360 pairs of relations from a subset of Wikidata using an integer similarity score from 0 to 4,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10664244741201401,No,0.0,0.0,0.0
q257,What text classification task is considered?,To classify a text as belonging to one of the ten possible classes.,"Incomplete text classification.
",0.3333333333333333,0.07692307692307693,0.125,0.0,0.0,0.0,0.3333333333333333,0.07692307692307693,0.125,0.015592777802775041,0.6039221286773682,No,0.0,0.0,0.0
q258,What novel class of recurrent-like networks is proposed?,"A network, whose learned functions satisfy a certain equation. The  network contains RNN cells with either nested internal memories or dependencies that extend temporally beyond the immediately previous hidden state.","A class of recurrent-like neural networks satisfying the differential equation DISPLAYFORM0.
",0.4166666666666667,0.16666666666666666,0.23809523809523808,0.0,0.0,0.0,0.3333333333333333,0.13333333333333333,0.19047619047619044,0.008322437662428014,0.4417955279350281,No,0.0,0.0,1.0
q259,How much bigger is Switchboard-2000 than Switchboard-300 database?,Switchboard-2000 contains 1700 more hours of speech data.,"The context states that Switchboard-2000 has 2000 hours of speech data, and Switchboard-300 has 300 hours.  Therefore, Switchboard-2000 is approximately 6.67 times larger.
",0.2222222222222222,0.6666666666666666,0.3333333333333333,0.15384615384615385,0.5,0.23529411764705882,0.2222222222222222,0.6666666666666666,0.3333333333333333,0.09615094003919297,0.7911010384559631,Yes,0.0,0.0,0.0
q260,What domains are detected in this paper?,"Answer with content missing: (Experimental setup not properly rendered) In our experiments we used seven target domains: Business and Commerce (BUS), Government and Politics (GOV), Physical and Mental Health (HEA), Law and Order (LAW),
Lifestyle (LIF), Military (MIL), and General Purpose (GEN). Exceptionally, GEN does
not have a natural root category.","The context mentions ""Lifestyle"" as an example domain, but does not list all detected domains.
",0.26666666666666666,0.0784313725490196,0.1212121212121212,0.07142857142857142,0.02,0.03125,0.2,0.058823529411764705,0.0909090909090909,0.0022815111601008904,0.4783070385456085,No,0.0,1.0,1.0
q261,Why do they think this task is hard?  What is the baseline performance?,"1. there may be situations where more than one action is reasonable, and also because writers tell a story playing with elements such as surprise or uncertainty.
2. Macro F1 = 14.6 (MLR, length 96 snippet)
Weighted F1 = 31.1 (LSTM, length 128 snippet)","Users with mild cognitive impairment found the task challenging due to the effort required to remember answers to generated questions.  The baseline performance is not explicitly stated but initial policy training with additional complexity resulted in significantly worse performance.
",0.05128205128205128,0.045454545454545456,0.04819277108433735,0.0,0.0,0.0,0.05128205128205128,0.045454545454545456,0.04819277108433735,0.009222398050743635,0.22669799625873566,No,0.0,0.0,0.0
q262,How do they generate the synthetic dataset?,using generative process,"The simulated data sets are generated by opening existing documents, making changes (addition, deletion, replacement), and saving them as revisions.  This process starts with an initial corpus without revisions, adding new documents and revising existing ones, mimicking a file system.  Revisions include changes to words, sentences, paragraphs, section names, and titles.
",0.0392156862745098,0.6666666666666666,0.07407407407407407,0.0,0.0,0.0,0.0392156862745098,0.6666666666666666,0.07407407407407407,0.005482271666382152,0.2101266086101532,No,0.0,0.0,0.0
q263,What is the average length of the claims?,Average claim length is 8.9 tokens.,"The answer is not found in the context.
",0.125,0.14285714285714285,0.13333333333333333,0.0,0.0,0.0,0.125,0.14285714285714285,0.13333333333333333,0.05669791110976001,0.0089884577319026,No,0.0,0.0,0.0
q264,What debate topics are included in the dataset?,"Ethics, Gender, Human rights, Sports, Freedom of Speech, Society, Religion, Philosophy, Health, Culture, World, Politics, Environment, Education, Digital Freedom, Economy, Science and Law","Politics, World, and Society.  The text also mentions 10+ other topics.
",0.36363636363636365,0.17391304347826086,0.2352941176470588,0.0,0.0,0.0,0.18181818181818182,0.08695652173913043,0.1176470588235294,0.02845716632337669,0.7236053347587585,No,0.0,1.0,1.0
q265,"By how much, the proposed method improves BiDAF and DCN on SQuAD dataset?","In terms of F1 score, the Hybrid approach improved by 23.47% and 1.39% on BiDAF and DCN respectively. The DCA approach improved by 23.2% and 1.12% on BiDAF and DCN respectively.","The context states that Hybrid and DCA models perform better than BiDAF and Co-Attention, but does not quantify the improvement.  Therefore, the answer is not found in the context.
",0.3,0.2571428571428571,0.2769230769230769,0.034482758620689655,0.029411764705882353,0.03174603174603175,0.23333333333333334,0.2,0.2153846153846154,0.034327809485034944,0.725942075252533,No,0.0,0.0,0.0
q266,What are the linguistic differences between each class?,"Each class has different patterns in adjectives, adverbs and verbs for sarcastic and non-sarcastic classes","English and Spanish are suffixing languages with Spanish having larger verbal paradigms and internal stem changes.  Zulu is a strongly prefixing language.  Basque is a language isolate with frequent use of prefixes and suffixes.  The other languages mentioned (French, German, Hungarian, Italian, Navajo, Turkish, and Quechua) are included in the study but their specific linguistic differences are not detailed.
",0.06779661016949153,0.25,0.10666666666666667,0.0,0.0,0.0,0.03389830508474576,0.125,0.05333333333333334,0.007195175746258358,0.27070367336273193,No,0.0,0.0,0.0
q267,what genres do they songs fall under?,"Gospel, Sertanejo, MPB, Forr, Pagode, Rock, Samba, Pop, Ax, Funk-carioca, Infantil, Velha-guarda, Bossa-nova and Jovem-guarda","The songs fall under 14 genres.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5677310824394226,No,0.0,0.0,0.0
q268,To what other competitive baselines is this approach compared?,"LSTMs with and without attention, HRED, VHRED with and without attention, MMI and Reranking-RL","CNN baseline, and various BERT-based models (RoBERTa, DistillRoBERTa, and ensembles of these).  Also, a baseline model for the dialogue task is mentioned.
",0.08695652173913043,0.13333333333333333,0.10526315789473685,0.0,0.0,0.0,0.08695652173913043,0.13333333333333333,0.10526315789473685,0.01856467050507842,0.17590823769569397,No,0.0,0.0,0.0
q269,"How is human evaluation performed, what was the criteria?",Through Amazon MTurk annotators to determine plausibility and content richness of the response,"Human evaluators graded generated poems from four perspectives: Fluency, Semantic coherence, Semantic preservability, and Poeticness.  For dialogs, evaluators rated generated responses based on criteria including emotional positivity/negativity, and the first three utterances of each dialog were used as input.  Another group of English-speaking students evaluated generated responses from dialogs.
",0.0784313725490196,0.3076923076923077,0.125,0.0,0.0,0.0,0.058823529411764705,0.23076923076923078,0.09375,0.007955340858308131,0.27685028314590454,No,0.0,0.0,0.0
q270,How much better were results of the proposed models than base LSTM-RNN model?,on diversity 6.87 and on relevance 4.6 points higher,"The context states that the Base model performed the best, except on French and Manipuri datasets.  It also says the proposed LSTM models achieved similar test perplexity to previous work.  It does *not* state how much better the proposed models are than the base model.
",0.044444444444444446,0.18181818181818182,0.07142857142857142,0.0,0.0,0.0,0.044444444444444446,0.18181818181818182,0.07142857142857142,0.008496070958802303,0.28300219774246216,No,0.0,0.0,0.0
q271,Which one of the four proposed models performed best?,the hybrid model MinAvgOut + RL,"Hybrid model
",1.0,0.4,0.5714285714285715,1.0,0.25,0.4,1.0,0.4,0.5714285714285715,0.0,0.5566774606704712,Yes,0.0,0.0,1.0
q272,How much is proposed model better than baselines in performed experiments?,"most of the models have similar performance on BPRA: DSTC2 (+0.0015), Maluuba (+0.0729)
GDP achieves the best performance in APRA: DSTC2 (+0.2893), Maluuba (+0.2896)
GDP significantly outperforms the baselines on BLEU: DSTC2 (+0.0791), Maluuba (+0.0492)","The proposed model outperforms baselines by 0.5% to 3.0% in accuracy, 0.18 points on newstest2017 (statistically significant), 0.15 BLEU points on newstest2018 (no statistical evidence), 1.09 BLEU and 0.85 TER points on tst2015, 0.75 BLEU and 0.76 TER points on tst2014, 0.48 BLEU and 0.68 TER points on tst2013, and a clear margin in new environments.
",0.21212121212121213,0.34146341463414637,0.26168224299065423,0.0,0.0,0.0,0.16666666666666666,0.2682926829268293,0.20560747663551404,0.01989420802348714,0.5859696269035339,No,0.0,0.0,0.0
q273,By how much is precission increased?,"ROUGE-1 increases by 0.05, ROUGE-2 by 0.06 and ROUGE-L by 0.09","Answer not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14521369338035583,No,0.0,0.0,0.0
q274,What labels are in the dataset?,binary label of stress or not stress,"Location (LOC), Person (PER), Organization (ORG), Miscellaneous (TABREF15), Entailment, Contradiction, Neutral.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.17337116599082947,No,0.0,0.0,0.0
q275,"How are customer satisfaction, customer frustration and overall problem resolution data collected?",By annotators on Amazon Mechanical Turk.,"Conversations were labeled with majority consensus, excluding those with majority ""can't tell"" labels.  The inherent imbalance in the data was retained.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.014445809981770858,0.09904451668262482,No,0.0,0.0,0.0
q276,How many improvements on the French-German translation benchmark?,one,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06584003567695618,No,0.0,0.0,0.0
q277,How do they prevent the model complexity increasing with the increased number of slots?,They exclude slot-specific parameters and incorporate better feature representation of user utterance and dialogue states using syntactic information and convolutional neural networks (CNN).,"The model avoids slot-specific parameters and incorporates better feature representation using syntactic information and CNNs, relying on semantic similarity between slots and utterances instead.
",0.6,0.625,0.6122448979591836,0.4166666666666667,0.43478260869565216,0.425531914893617,0.52,0.5416666666666666,0.5306122448979592,0.174932986566737,0.7949756979942322,Yes,1.0,1.0,1.0
q278,How do they measure model size?,By the number of parameters.,"The context describes model size by the number of experts (32, 256, 1024, etc.) in the MoE layers and the number of parameters.
",0.21739130434782608,1.0,0.3571428571428571,0.18181818181818182,1.0,0.3076923076923077,0.21739130434782608,1.0,0.3571428571428571,0.11622111816655836,0.31882190704345703,Yes,0.0,0.0,0.0
q279,Which model architecture do they for sentence encoding?,"Answer with content missing: (Skip-thought vectors-Natural Language Inference paragraphs) The encoder for the current sentence and the decoders for the previous (STP) and next sentence (STN) are typically parameterized as separate RNNs
- RNN","BiLSTM-max, ESIM, KIM, ESIM + ELMo, BERT
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08286994695663452,No,0.0,0.0,0.0
q280,Which data sources do they use?,"- En-Fr (WMT14)
- En-De (WMT15)
- Skipthought (BookCorpus)
- AllNLI (SNLI + MultiNLI)
- Parsing (PTB + 1-billion word)","News and encyclopedia corpus, AIDA-Train and Wikipedia datasets, AIDA-CoNLL, Stanford Sentiment Treebank (SST-2), Yelp dataset, Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), Quora Question Pairs (QQP), SQuAD 1.1, SQuAD 2.0, MRQA.
",0.025,0.0625,0.03571428571428572,0.0,0.0,0.0,0.025,0.0625,0.03571428571428572,0.011548422250985523,0.4277553856372833,No,0.0,0.0,0.0
q281,How were breast cancer related posts compiled from the Twitter streaming API?,"By using  keywords `breast' AND `cancer' in tweet collecting process. 
","Tweets mentioning ""breast"" AND ""cancer"" were collected from a primary Twitter endpoint.
",0.4166666666666667,0.5,0.45454545454545453,0.18181818181818182,0.2222222222222222,0.19999999999999998,0.3333333333333333,0.4,0.3636363636363636,0.02719665272174911,0.7999901175498962,Yes,0.0,1.0,1.0
q282,What machine learning and NLP methods were used to sift tweets relevant to breast cancer experiences?,"ML  logistic regression classifier combined with a Convolutional Neural Network (CNN) to identify self-reported diagnostic tweets.
NLP methods:  tweet conversion to numeric word vector,  removing tweets containing hyperlinks, removing ""retweets"", removing all tweets containing horoscope indicators,  lowercasing and  removing punctuation.","A context classifier, a Convolutional Neural Network (CNN) with Google's Tensorflow interface, and natural language processing.
",0.47058823529411764,0.1951219512195122,0.27586206896551724,0.25,0.1,0.14285714285714288,0.4117647058823529,0.17073170731707318,0.24137931034482757,0.0773644802207357,0.41588786244392395,Yes,0.0,0.0,1.0
q283,What approach performs better in experiments global latent or sequence of fine-grained latent variables?,"PPL: SVT
Diversity: GVT
Embeddings Similarity: SVT
Human Evaluation: SVT","Global latent variable.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.030608147382736206,No,1.0,1.0,1.0
q284,Which translation system do they use to translate to English?,Attention-based translation model with convolution sequence to sequence model,"Machine translation.
",0.5,0.1,0.16666666666666669,0.0,0.0,0.0,0.5,0.1,0.16666666666666669,0.0,0.3398295044898987,No,0.0,0.0,0.0
q285,Which pre-trained English NER model do they use?,Bidirectional LSTM based NER model of Flair,"Flair's default NER model.
",0.6,0.42857142857142855,0.5,0.25,0.16666666666666666,0.2,0.4,0.2857142857142857,0.3333333333333333,0.14320952289897704,0.6513205170631409,Yes,1.0,1.0,1.0
q286,How much of the ASR grapheme set is shared between languages?,Little overlap except common basic Latin alphabet and that Hindi and Marathi languages use same script.,"The character sets of the 7 languages have little overlap, except for the common basic Latin alphabet and the use of Devanagari script by Hindi and Marathi.
",0.5185185185185185,0.875,0.6511627906976744,0.3076923076923077,0.5333333333333333,0.3902439024390244,0.4074074074074074,0.6875,0.5116279069767441,0.1852972751417938,0.7873966097831726,Yes,1.0,1.0,1.0
q287,What are the languages used to test the model?,"Hindi, English and German (German task won)","English, French, German, Spanish, Russian, Italian.
",0.3333333333333333,0.2857142857142857,0.30769230769230765,0.0,0.0,0.0,0.3333333333333333,0.2857142857142857,0.30769230769230765,0.04456882760699064,0.6182389259338379,No,1.0,1.0,1.0
q288,Which language has the lowest error rate reduction?,thai,"ParTUT (0.28% for tagging, 1.65% for parsing)
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0324917733669281,No,0.0,0.0,0.0
q289,How is moral bias measured?,"Answer with content missing: (formula 1) bias(q, a, b) = cos(a, q)  cos(b, q)
Bias is calculated as substraction of cosine similarities of question and some answer for two opposite answers.","The Moral Choice Machine computes cosine similarity in a sentence embedding space of an arbitrary action embedded in question/answer pairs.  Several question/answer prompts are conflated into a template, the element of interest is inserted, and the resulting biases are averaged to an overall bias value.
",0.2553191489361702,0.36363636363636365,0.29999999999999993,0.021739130434782608,0.03125,0.02564102564102564,0.1276595744680851,0.18181818181818182,0.14999999999999997,0.013038751133433215,0.43473219871520996,No,0.0,0.0,1.0
q290,What metrics are used for evaluation?,word error rate,"BLEU, BIBREF41, METEOR, ROUGE, Pyramid, ROUGE, Pearson correlation, MSE, MAPE, RMSE, MAE, and $R^2$ coefficient.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13463588058948517,No,0.0,0.0,0.0
q291,How much training data is used?,"163,110,000 utterances","Tens of thousands to several million per script.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.42341703176498413,No,0.0,0.0,0.0
q292,How does their model differ from BERT?,Their model does not differ from BERT.,"Their model uses pre-trained [CLS] and [SEP] tokens, and is optimized using techniques from BIBREF6 and BIBREF7.  It also potentially uses distilled BERT models (BIBREF8).
",0.15384615384615385,0.5714285714285714,0.24242424242424246,0.04,0.16666666666666666,0.06451612903225806,0.15384615384615385,0.5714285714285714,0.24242424242424246,0.027374353552268266,0.5788612365722656,No,0.0,1.0,1.0
q293,How does explicit constraint on the KL divergence term that authors propose looks like?,"Answer with content missing: (Formula 2) Formula 2 is an answer: 
\big \langle\! \log p_\theta({x}|{z}) \big \rangle_{q_\phi({z}|{x})}  -  \beta |D_{KL}\big(q_\phi({z}|{x}) || p({z})\big)-C|","$|D_{KL}\big (q_\phi ({z}|{x}) || p({z})\big )-C|$  where $C \ge 0$.
",0.7333333333333333,0.3055555555555556,0.43137254901960786,0.7142857142857143,0.2857142857142857,0.40816326530612246,0.7333333333333333,0.3055555555555556,0.43137254901960786,0.260985971676289,0.6525278687477112,Yes,1.0,1.0,1.0
q294,what was the baseline?,There is no baseline.,"A Kaldi i-vector system (BIBREF3) and an x-vector system (BIBREF10) were baselines.  A Majority Class baseline, an Exact Match baseline, and a First Occurrence baseline were also used.  Finally, a LSTM baseline was used to evaluate the performance of ELMo embeddings.  A ""Baseline"" system was also used for a machine translation task.
",0.018518518518518517,0.25,0.034482758620689655,0.0,0.0,0.0,0.018518518518518517,0.25,0.034482758620689655,0.006622258249686733,0.5788087248802185,No,1.0,1.0,1.0
q295,How big is dataset used?,"553,451 documents","The context mentions datasets of various sizes:  70,000 to 170,000 raw data, 12,350 labeled ads, 15,000 images with 75,000 questions, and tens of thousands to several million training samples per script.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.35852065682411194,No,0.0,0.0,0.0
q296,How much better does this baseline neural model do?,"The model outperforms at every point in the
implicit-tuples PR curve reaching almost 0.8 in recall","The RelNet model achieves a mean error of 0.285%, better than the EntNet model.  MMM leads to improvements ranging from 0.5% to 3.0% in accuracy.
",0.17857142857142858,0.2777777777777778,0.21739130434782608,0.0,0.0,0.0,0.17857142857142858,0.2777777777777778,0.21739130434782608,0.016961343903963387,0.3205711543560028,No,0.0,0.0,0.0
q297,What are the baseline models?,"MC-CNN
MVCNN
CNN","Baseline models:  Random, Modelo 1, TF-IDF, LDA, Majority Class, Exact Match, First Occurrence, LSTM, i-vector, x-vector.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12356417626142502,No,0.0,0.0,0.0
q298,By how much of MGNC-CNN out perform the baselines?,"In terms of Subj the Average MGNC-CNN is better than the average score of baselines by 0.5.  Similarly, Scores of SST-1, SST-2, and TREC where MGNC-CNN has similar improvements. 
In case of Irony the difference is about 2.0. 
","The context states that MGNC-CNN and MG-CNN always outperform baseline methods (including C-CNN).  No specific performance difference is given.
",0.36363636363636365,0.18181818181818182,0.24242424242424246,0.09523809523809523,0.046511627906976744,0.0625,0.3181818181818182,0.1590909090909091,0.2121212121212121,0.017393445458541913,0.7383814454078674,No,0.0,0.0,0.0
q299,What is increase in percentage of humor contained in headlines generated with TitleStylist method (w.r.t. baselines)?,"Humor in headlines (TitleStylist vs Multitask baseline):
Relevance: +6.53% (5.87 vs 5.51)
Attraction: +3.72% (8.93 vs 8.61)
Fluency: 1,98% (9.29 vs 9.11)","Answer not found in the context.
",0.16666666666666666,0.03225806451612903,0.05405405405405405,0.0,0.0,0.0,0.16666666666666666,0.03225806451612903,0.05405405405405405,0.0006792646002484808,0.0888647586107254,No,0.0,1.0,1.0
q300,What are the languages they consider in this paper?,"The languages considered were English, Chinese, German, Russian, Arabic, Spanish, French","English, French, German, Spanish, Russian, Italian, Bengali, Hindi, Telegu, Kannada, Malayalam, Sinhala, Tamil, and Marathi.
",0.3333333333333333,0.45454545454545453,0.3846153846153846,0.0,0.0,0.0,0.2,0.2727272727272727,0.23076923076923075,0.12335881744794525,0.641706109046936,No,0.0,1.0,1.0
q301,Did they experiment with tasks other than word problems in math?,They experimented with sentiment analysis and natural language inference task,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.26264309883117676,No,0.0,0.0,0.0
q302,What evaluation metrics are used?,Accuracy on each dataset and the average accuracy on all datasets.,"Sum-QE, Pyramid, ROUGE, F<sub>1</sub> score, BPRA, number of turns, task completion ratio.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01387819277861591,0.245351642370224,No,0.0,0.0,1.0
q303,What kind of Youtube video transcripts did they use?,"youtube video transcripts on news covering different topics like technology, human rights, terrorism and politics","Manual transcriptions of newscasts, interviews, reports, and round tables.
",0.2222222222222222,0.13333333333333333,0.16666666666666669,0.0,0.0,0.0,0.2222222222222222,0.13333333333333333,0.16666666666666669,0.029886628689621778,0.41098296642303467,No,1.0,1.0,1.0
q304,What makes it a more reliable metric?,It takes into account the agreement between different systems,"Pearson correlation, along with mean squared error, is used because simple accuracy metrics can be unreliable, particularly when evaluating outputs of similar quality.  Mean per-class accuracy is also used in image recognition tasks to compensate for unbalanced categories.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15916313230991364,No,0.0,0.0,0.0
q305,How much in experiments is performance improved for models trained with generated adversarial examples?,"Answer with content missing: (Table 1) The performance of all the target models raises significantly, while that on the original
examples remain comparable (e.g. the overall accuracy of BERT on modified examples raises from 24.1% to 66.0% on Quora)","The context states that performance of target models *raises significantly* after adversarial training.
",0.6153846153846154,0.19047619047619047,0.2909090909090909,0.3333333333333333,0.0975609756097561,0.15094339622641506,0.5384615384615384,0.16666666666666666,0.2545454545454545,0.011197524280228974,0.48486489057540894,Yes,1.0,1.0,1.0
q306,How is the delta-softmax calculated?,Answer with content missing: (Formula) Formula is the answer.,"The delta-softmax metric ${\Delta }_s$ is calculated by comparing the ${\Delta }_s$ for a particular token to the maximum ${\Delta }_s$ achieved by any token in the current pair and the maximum ${\Delta }_s$ achieved by any token in the current document.  It depends on the true relation, tokens in the sequence, and the input sequence with a masked position.
",0.04918032786885246,0.3333333333333333,0.0857142857142857,0.0,0.0,0.0,0.03278688524590164,0.2222222222222222,0.05714285714285715,0.005780829745424562,0.11343038082122803,No,0.0,0.0,0.0
q307,Which two datasets does the resource come from?,two surveys by two groups - school students and meteorologists to draw on a map a polygon representing a given geographical descriptor,"English Wiktionary and Wikipedia.
",0.25,0.047619047619047616,0.08,0.0,0.0,0.0,0.25,0.047619047619047616,0.08,0.0035649911537502725,0.13761018216609955,No,0.0,0.0,0.0
q308,What is the size of the second dataset?,1 000 labeled dialogues for training and 240 unlabeled dialogues for evaluation,"141 symptom-prescription pairs
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16394396126270294,No,0.0,0.0,0.0
q309,How large is the first dataset?,1 000 labeled dialogues for training and 240 unlabeled dialogues for evaluation,"12,350 ads
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.21802332997322083,No,0.0,0.0,0.0
q310,Who was the top-scoring team?,IDEA,"N-GrAM
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03956492617726326,No,0.0,0.0,0.0
q311,Why is big data not appropriate for this task?,Training embeddings from small-corpora can increase the performance of some tasks,"""Big"" datasets are not always available, particularly in computational social science NLP applications, where the data of interest are often from sources like press releases, academic journals, books, and transcripts of recorded speech, rather than large-scale internet or social media sources.
",0.07142857142857142,0.25,0.11111111111111112,0.0,0.0,0.0,0.047619047619047616,0.16666666666666666,0.07407407407407407,0.00959641509636166,0.1493104249238968,No,1.0,1.0,1.0
q312,What is an example of a computational social science NLP task?,Visualization of State of the union addresses,"The answer is not found in the context.
",0.125,0.14285714285714285,0.13333333333333333,0.0,0.0,0.0,0.125,0.14285714285714285,0.13333333333333333,0.04767707020457095,0.06527452915906906,No,0.0,0.0,0.0
q313,Which major geographical regions are studied?,"Northeast U.S, South U.S., West U.S. and Midwest U.S.","Latin America and the Caribbean, South Asia, Sub-Saharan Africa, Europe and Central Asia, Middle East and North Africa, East Asia and the Pacific, North America.
",0.07692307692307693,0.15384615384615385,0.10256410256410257,0.0,0.0,0.0,0.07692307692307693,0.15384615384615385,0.10256410256410257,0.029171735901692596,0.6391904354095459,No,0.0,0.0,0.0
q314,How strong is the correlation between the prevalence of the #MeToo movement and official reports [of sexual harassment]?,0.9098 correlation,"The correlation is statistically significant ($p-value<0.05$).  A change in reported cases corresponds to a substantial change in #MeToo users.
",0.09090909090909091,0.6666666666666666,0.16,0.0,0.0,0.0,0.045454545454545456,0.3333333333333333,0.08,0.013353534059549444,0.5272191166877747,No,0.0,0.0,0.0
q315,How are the topics embedded in the #MeToo tweets extracted?,Using Latent Dirichlet Allocation on TF-IDF transformed from the corpus,"Latent Dirichlet Allocation (LDA) is used to label universal topics demonstrated by the users.  The optimal topic number is determined by selecting the one with the highest coherence score.
",0.1724137931034483,0.45454545454545453,0.25000000000000006,0.07142857142857142,0.2,0.10526315789473682,0.13793103448275862,0.36363636363636365,0.2,0.04492398578415928,0.42359021306037903,Yes,1.0,1.0,1.0
q316,Which geographical regions correlate to the trend?,"Northeast U.S., West U.S. and South U.S.","South Asia, Latin America and the Caribbean, East Asia and the Pacific, North America.
",0.14285714285714285,0.2,0.16666666666666666,0.0,0.0,0.0,0.07142857142857142,0.1,0.08333333333333333,0.030372940354383413,0.7071542143821716,No,0.0,0.0,0.0
q317,What two components are included in their proposed framework?,evidence extraction and answer synthesis,"Deep learning classifier and a tweet processor.
",0.14285714285714285,0.2,0.16666666666666666,0.0,0.0,0.0,0.14285714285714285,0.2,0.16666666666666666,0.05522397783539471,0.12251779437065125,No,0.0,0.0,0.0
q318,Which modifications do they make to well-established Seq2seq architectures?,"Replacing attention mechanism to query-key attention, and adding a loss to make the attention mask as diagonal as possible","1. Using a soft loss function.
2. Modifying the Seq2Seq attention mechanism to be mostly monotonic, suitable for TTS.
",0.3157894736842105,0.3,0.3076923076923077,0.1111111111111111,0.10526315789473684,0.10810810810810811,0.21052631578947367,0.2,0.20512820512820512,0.07238177794755686,0.44173404574394226,No,0.0,0.0,0.0
q319,How was speed measured?,how long it takes the system to lemmatize a set number of words,"A navigation system with voice commands and information on the screen kept participants on a predefined track.  The time needed for the task would be compared to other participants.
",0.13793103448275862,0.3076923076923077,0.1904761904761905,0.0,0.0,0.0,0.06896551724137931,0.15384615384615385,0.09523809523809525,0.016961343903963387,0.21955709159374237,No,0.0,0.0,0.0
q320,What were their accuracy results on the task?,97.32%,"Recall and precision measures were used for evaluating the algorithm's performance on questions from the development and evaluation sets.  High accuracy was achieved on ""absurd"" questions, moderate accuracy on subordinate object recognition, and low accuracy on counting questions.  More analysis is needed for other question types.  The best accuracy rate for GloVe is shown in Table TABREF17.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2393941581249237,No,0.0,0.0,0.0
q321,What two types the Chinese reading comprehension dataset consists of?,cloze-style reading comprehension and user query reading comprehension questions,"People Daily news datasets and Children's Fairy Tale datasets.
",0.1,0.1,0.10000000000000002,0.0,0.0,0.0,0.1,0.1,0.10000000000000002,0.04196114906296548,0.20782171189785004,No,1.0,1.0,1.0
q322,For which languages most of the existing MRC datasets are created?,English,"English.
",1.0,1.0,1.0,0.0,0.0,0.0,1.0,1.0,1.0,0.0,0.8476008176803589,Yes,1.0,1.0,1.0
q323,Which sentiment analysis tasks are addressed?,12 binary-class classification and multi-class classification of reviews based on rating,"Tweet-level sentiment, aspect-based sentiment analysis, sentiment analysis of figurative language, implicit event polarity, stance in tweets, out-of-context sentiment intensity, emotion detection, and clinical sentiment analysis.
",0.10344827586206896,0.23076923076923078,0.14285714285714288,0.0,0.0,0.0,0.034482758620689655,0.07692307692307693,0.04761904761904762,0.013355980882200826,0.17750947177410126,No,0.0,0.0,0.0
q324,Which 3 NLP areas are cited the most?,"machine translation, statistical machine, sentiment analysis","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11865322291851044,No,0.0,0.0,1.0
q325,Which journal and conference are cited the most in recent years?,CL Journal and EMNLP conference,"CL journal.
",1.0,0.4,0.5714285714285715,1.0,0.25,0.4,1.0,0.4,0.5714285714285715,0.0,0.6662901043891907,Yes,1.0,1.0,1.0
q326,Which 5 languages appear most frequently in AA paper titles?,"English, Chinese, French, Japanese and Arabic","The provided text states that Chinese, Arabic, Korean, Japanese, and Hindi are among the most widely spoken Asian languages well-represented in AA papers, and French, German, Swedish, Spanish, Portuguese, and Italian are well-represented European languages.  However, it does not provide a ranked list of the *five* most frequent.  The answer is not found in the context.
",0.08620689655172414,0.8333333333333334,0.15625000000000003,0.017543859649122806,0.2,0.03225806451612903,0.05172413793103448,0.5,0.09375,0.018670064559469356,0.571308970451355,No,0.0,0.0,1.0
q327,How much F1 was improved after adding skip connections?,"Simple Skip improves F1 from 74.34 to 74.81
Transformer Skip improes F1 from 74.34 to 74.95 ","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12683187425136566,No,0.0,0.0,0.0
q328,How much gain does the model achieve with pretraining MVCNN?,0.8 points on Binary; 0.7 points on Fine-Grained; 0.6 points on Senti140; 0.7 points on Subj,"The context does not contain a specific number for the gain achieved by pretraining MVCNN.  It only states that pretraining is important for words without initialization and that MVCNN uses pretraining.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19991615414619446,No,0.0,0.0,0.0
q329,What is the highest accuracy score achieved?,82.0%,"84.84%
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9147586822509766,No,0.0,1.0,1.0
q330,What are the three datasets used in the paper?,Data released for APDA shared task contains 3 datasets.,"France, UK, and laptop/restaurant datasets.
",0.16666666666666666,0.1111111111111111,0.13333333333333333,0.0,0.0,0.0,0.16666666666666666,0.1111111111111111,0.13333333333333333,0.08392229812593097,0.3554950952529907,No,0.0,0.0,0.0
q331,What is improvement in accuracy for short Jokes in relation other types of jokes?,It had the highest accuracy comparing to all datasets 0.986% and It had the highest improvement comparing to previous methods on the same dataset by 8%,"A jump of 8 percent from the most recent CNN work.
",0.18181818181818182,0.07407407407407407,0.10526315789473684,0.0,0.0,0.0,0.09090909090909091,0.037037037037037035,0.05263157894736842,0.010615710980041333,0.3943278193473816,Yes,1.0,1.0,1.0
q332,What baseline did they compare Entity-GCN to?,"Human, FastQA, BiDAF, Coref-GRU, MHPGM, Weaver / Jenga, MHQA-GRN","BiDAF and FastQA.
",0.6666666666666666,0.2,0.30769230769230765,0.0,0.0,0.0,0.3333333333333333,0.1,0.15384615384615383,0.012143667563059622,0.5615100860595703,Yes,0.0,1.0,1.0
q333,How did they get relations between mentions?,"Assign a value to the relation based on whether mentions occur in the same document, if mentions are identical, or if mentions are in the same coreference chain.","They used i) co-occurrence within the same document (DOC-BASED), ii) exact matching (MATCH), and iii) coreference predicted by an external system (COREF).
",0.20833333333333334,0.17857142857142858,0.1923076923076923,0.08695652173913043,0.07407407407407407,0.07999999999999999,0.16666666666666666,0.14285714285714285,0.15384615384615383,0.05007816165264321,0.5331400036811829,Yes,1.0,1.0,1.0
q334,How did they detect entity mentions?,Exact matches to the entity string and predictions from a coreference resolution system,"The basic rules used to detect mentions are similar to those of Lee:2013:CL, except that their system uses filtering rules.  The described system keeps partitives, quantified noun phrases, and bare NP mentions, but discards pleonastic it and other spurious mentions.  Mentions are also selected by taking surrounding words and encoding them using a pre-trained lookup table.
",0.0847457627118644,0.38461538461538464,0.1388888888888889,0.0,0.0,0.0,0.06779661016949153,0.3076923076923077,0.1111111111111111,0.0076347001877086085,0.2132137417793274,No,0.0,1.0,1.0
q335,What performance does the Entity-GCN get on WIKIHOP?,"During testing: 67.6 for single model without coreference, 66.4 for single model with coreference, 71.2 for ensemble of 5 models","Entity-GCN achieves over 2% improvement over the best previously-published results on WikiHop, and an ensemble improves this further by 3.6% and only 3% below human performance.
",0.10344827586206896,0.13043478260869565,0.11538461538461538,0.0,0.0,0.0,0.06896551724137931,0.08695652173913043,0.07692307692307693,0.014262733286728255,0.4092727601528168,No,1.0,1.0,1.0
q336,What document context was added?,Preceding and following sentence of each metaphor and paraphrase are added as document context,"A preceding and following sentence, forming a document context of a sentence pair.
",0.5384615384615384,0.5,0.5185185185185186,0.3333333333333333,0.3076923076923077,0.32,0.46153846153846156,0.42857142857142855,0.4444444444444445,0.12874330508144843,0.6566329598426819,No,0.0,0.0,0.0
q337,What were the results of the first experiment?,Best performance achieved is 0.72 F1 score,"The context describes the setup for six data sets and multiple experiments, but does not contain results for a specific ""first experiment"".
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.22645847499370575,No,0.0,0.0,0.0
q338,By how much does their model outperform existing methods?,Answer with content missing: (Table II) Proposed model has F1 score of  0.7220 compared to 0.7148 best state-state-of-the-art result.,"The model outperforms existing methods by 1.4% in joint goal accuracy and 0.2% in turn request accuracy on the WoZ dataset.  It also shows significant improvements in EM and F1 scores on other datasets.
",0.1388888888888889,0.2,0.1639344262295082,0.02857142857142857,0.041666666666666664,0.03389830508474576,0.08333333333333333,0.12,0.09836065573770492,0.012753998269755913,0.5645400285720825,No,0.0,0.0,1.0
q339,What is the performance of their model?,Answer with content missing: (Table II) Proposed model has F1 score of  0.7220.,"Their large models substantially improved across all metrics compared to base models, surpassing the NABERT+ baseline.  Their best model (large-squad) showed a huge improvement of 29.7 in EM and 15.1 in F1 compared to MTMSNlarge.
",0.07894736842105263,0.21428571428571427,0.11538461538461539,0.0,0.0,0.0,0.05263157894736842,0.14285714285714285,0.07692307692307693,0.014014248243844825,0.3519439399242401,No,0.0,0.0,0.0
q340,How do they damage different neural modules?,"Damage to neural modules is done by randomly initializing their weights, causing the loss of all learned information.","The context describes techniques like weight decay, dropout, dropconnect, and zoneout for regularization, but doesn't detail how they damage different neural modules.  It explains *how* these techniques are used to *improve* generalization and prevent overfitting.  No information about damaging neural modules is provided.
",0.18181818181818182,0.4444444444444444,0.2580645161290322,0.046511627906976744,0.11764705882352941,0.06666666666666667,0.11363636363636363,0.2777777777777778,0.16129032258064516,0.030989439600214986,0.6848194599151611,No,0.0,0.0,0.0
q341,How long are the datasets?,"Travel dataset contains 4100 raw samples, 11291 clauses, Hotel dataset contains 3825 raw samples, 11264 clauses, and the Mobile dataset contains 3483 raw samples and 8118 clauses","The answer is not found in the context.  While the context describes various dataset *splits* and *sizes* related to specific parameters (like article length or sequence length), it does not provide a single answer for the *overall* length of any datasets.
",0.0975609756097561,0.14814814814814814,0.11764705882352941,0.0,0.0,0.0,0.0975609756097561,0.14814814814814814,0.11764705882352941,0.009903417847796372,0.350599080324173,No,0.0,0.0,0.0
q342,What are the sources of the data?,"User reviews written in Chinese collected online for hotel, mobile phone, and travel domains","Twitter.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14381207525730133,No,0.0,0.0,0.0
q343,What is the new labeling strategy?,They use a two-stage labeling strategy where in the first stage single annotators label a large number of short texts with relatively pure sentiment orientations and in the second stage multiple annotators label few text samples with mixed sentiment orientations,"A two-level network utilizing two labeled data sets (pure and mixed sentiment orientations) is introduced.  Lexical cues and a new encoding strategy (INLINEFORM0-hot) are used.
",0.4074074074074074,0.2682926829268293,0.3235294117647059,0.11538461538461539,0.075,0.09090909090909093,0.2962962962962963,0.1951219512195122,0.23529411764705882,0.04179407535312319,0.5898931622505188,No,1.0,1.0,1.0
q344,How are their changes evaluated?,The changes are evaluated based on accuracy of intent and entity recognition on SNIPS dataset,"The context mentions evaluating changes in prediction scores if an observed fact is absent during embedding learning, but doesn't describe a specific evaluation method for other modifications.
",0.10714285714285714,0.2,0.13953488372093023,0.0,0.0,0.0,0.10714285714285714,0.2,0.13953488372093023,0.015301683686839006,0.407465398311615,No,0.0,0.0,0.0
q345,What are the six target languages?,"Answer with content missing: (3 Experimental Setup) We experiment with six target languages: French (FR), Brazilian Portuguese (PT), Italian (IT), Polish (PL), Croatian (HR), and Finnish (FI).","English, Spanish, French, German, Russian, and Italian.
",0.42857142857142855,0.1111111111111111,0.17647058823529413,0.0,0.0,0.0,0.2857142857142857,0.07407407407407407,0.11764705882352941,0.00564283261088931,0.6280084848403931,No,0.0,0.0,0.0
q346,What is the size of the released dataset?,"440 sentences, 2247 triples extracted from those sentences, and 11262 judgements on those triples.","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.019600629536473145,0.12985602021217346,No,0.0,0.0,0.0
q347,Which OpenIE systems were used?,OpenIE4 and MiniIE,"OpenIE-4, OpenIE 5, MiniIE.
",0.2,0.3333333333333333,0.25,0.0,0.0,0.0,0.2,0.3333333333333333,0.25,0.06567274736060395,0.8062064051628113,No,0.0,1.0,1.0
q348,how are the bidirectional lms obtained?,"They pre-train forward and backward LMs separately, remove top layer softmax, and concatenate to obtain the bidirectional LMs.","A bidirectional LSTM (BiLSTM) is formed by concatenating a forward LSTM and a backward LSTM.
",0.3333333333333333,0.2631578947368421,0.29411764705882354,0.0,0.0,0.0,0.2,0.15789473684210525,0.17647058823529413,0.02718516209546605,0.6035863757133484,No,0.0,0.0,0.0
q349,what metrics are used in evaluation?,micro-averaged F1,"ROUGE, BLEU, METEOR, F<sub>1</sub> score, Pyramid,  average rank, percentage of best-ranked questions, number of turns, ratio of task completion.
",0.043478260869565216,0.3333333333333333,0.07692307692307691,0.0,0.0,0.0,0.043478260869565216,0.3333333333333333,0.07692307692307691,0.0,0.12191320210695267,No,0.0,0.0,1.0
q350,what results do they achieve?,91.93% F1 score on CoNLL 2003 NER task and 96.37% F1 score on CoNLL 2000 Chunking task,"The provided text describes experiments, but does not state specific achieved results.  It mentions that classification approaches outperformed regression approaches, and that the ""first $n$"" baseline produced the same results as the neural regressor, but does not give numerical results.
",0.025,0.05263157894736842,0.03389830508474576,0.0,0.0,0.0,0.025,0.05263157894736842,0.03389830508474576,0.007445292335166879,0.23207539319992065,No,0.0,0.0,0.0
q351,what previous systems were compared to?,"Chiu and Nichols (2016), Lample et al. (2016), Ma and Hovy (2016), Yang et al. (2017), Hashimoto et al. (2016), Sgaard and Goldberg (2016) ","IMS, Latent-Tree, Berkeley, LaSO, Latent-Strc, Model-Stack, Non-Linear, and Human.
",0.07692307692307693,0.04,0.052631578947368425,0.0,0.0,0.0,0.07692307692307693,0.04,0.052631578947368425,0.007599248433208697,0.4027050733566284,No,0.0,0.0,0.0
q352,Are this models usually semi/supervised or unsupervised?,"Both supervised and unsupervised, depending on the task that needs to be solved.","Semi-supervised.
",0.5,0.07692307692307693,0.13333333333333336,0.0,0.0,0.0,0.5,0.07692307692307693,0.13333333333333336,0.0,0.6440650224685669,No,1.0,1.0,1.0
q353,"When they say ""comparable performance"", how much of a performance drop do these new embeddings result in?","Performance was comparable, with the proposed method quite close and sometimes exceeding performance of baseline method.","The context states the new embeddings achieve ""comparable performance"" on KG tasks, but does not quantify the performance drop.  Answer not found in the context.
",0.16,0.25,0.19512195121951217,0.0,0.0,0.0,0.12,0.1875,0.14634146341463414,0.01856467050507842,0.493014931678772,No,0.0,0.0,0.0
q354,What types of word representations are they evaluating?,GloVE; SGNS,"Word embeddings.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.020067742094397545,No,0.0,1.0,1.0
q355,What type of recurrent layers does the model use?,GRU,"Quasi-recurrent neural networks (QRNNs).
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0193665511906147,No,0.0,0.0,1.0
q356,What is a word confusion network?,It is a network used to encode speech lattices to maintain a rich hypothesis space.,"The answer is not found in the context.
",0.125,0.06666666666666667,0.08695652173913045,0.0,0.0,0.0,0.125,0.06666666666666667,0.08695652173913045,0.02604848453271721,0.2145993411540985,No,0.0,0.0,0.0
q357,What evaluation metrics were used in the experiment?,"For sentence-level prediction they used tolerance accuracy, for segment retrieval accuracy and MRR and for the pipeline approach they used overall accuracy","Triple Accuracy, Contextual Consistency, MSE, R2, ROUGE, human evaluation (manual inspection), ROUGE, BLEU, METEOR, perplexity.
",0.06666666666666667,0.043478260869565216,0.052631578947368425,0.0,0.0,0.0,0.06666666666666667,0.043478260869565216,0.052631578947368425,0.01387819277861591,0.6384308338165283,No,0.0,0.0,0.0
q358,What kind of instructional videos are in the dataset?,tutorial videos for a photo-editing software,"Tutorial videos about image editing software.
",0.6666666666666666,0.5714285714285714,0.6153846153846153,0.4,0.3333333333333333,0.3636363636363636,0.6666666666666666,0.5714285714285714,0.6153846153846153,0.07809849842300641,0.9190176129341125,Yes,1.0,1.0,1.0
q359,What baseline algorithms were presented?,"a sentence-level prediction algorithm, a segment retrieval algorithm and a pipeline segment retrieval algorithm","Fully Connected dense networks,  K-means, Skip-thought Vectors, Recursive Neural Network, Paragraph Vector, Majority Class, Exact Match, First Occurrence, LSTM, bidirectional RNN, MLP, Random Forest, Linear Support Vector Classifier, Regularized linear classifier, Stochastic Gradient Descent, Nearest Centroid.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0070027739171006995,0.44420766830444336,No,0.0,0.0,0.0
q360,How does TP-N2F compare to LSTM-based Seq2Seq in terms of training and inference speed?,"Full Testing Set accuracy: 84.02
Cleaned Testing Set accuracy: 93.48","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04697878658771515,No,0.0,0.0,0.0
q361,What is the performance proposed model achieved on AlgoList benchmark?,"Full Testing Set Accuracy: 84.02
Cleaned Testing Set Accuracy: 93.48","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04697878658771515,No,0.0,0.0,0.0
q362,What is the performance proposed model achieved on MathQA?,"Operation accuracy: 71.89
Execution accuracy: 55.95","The proposed model achieved execution accuracy and operation sequence accuracy on the MathQA dataset, but specific numbers are not provided.
",0.2,0.5,0.28571428571428575,0.05263157894736842,0.14285714285714285,0.07692307692307693,0.1,0.25,0.14285714285714288,0.020540268312306344,0.6890245676040649,No,0.0,0.0,0.0
q363,What previous methods is the proposed method compared against?,"BLSTM+Attention+BLSTM
Hierarchical BLSTM-CRF
CRF-ASN
Hierarchical CNN (window 4)
mLSTM-RNN
DRLM-Conditional
LSTM-Softmax
RCNN
CNN
CRF
LSTM
BERT","State-of-the-art methods, baseline methods, Coppersmith et al.'s method, Shallow Fusion.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2364102154970169,No,1.0,1.0,1.0
q364,What is the baseline model used?,"The baseline models used are DrQA modified to support answering no answer questions, DrQA+CoQA which is pre-tuned on CoQA dataset, vanilla BERT, BERT+review tuned on domain reviews, BERT+CoQA tuned on the supervised CoQA data","Baseline1, which represents a span of sentences by concatenating the vectors of the starting and ending sentences.
",0.17647058823529413,0.07894736842105263,0.10909090909090909,0.0,0.0,0.0,0.11764705882352941,0.05263157894736842,0.07272727272727272,0.0071473732085562255,0.2504170835018158,No,0.0,0.0,0.0
q365,What domains are present in the data?,"Alarm, Banks, Buses, Calendar, Events, Flights, Homes, Hotels, Media, Messaging, Movies, Music, Payment, Rental Cars, Restaurants, Ride Sharing, Services, Train, Travel, Weather","Lifestyle, hotel, attraction, restaurant, metro, taxi.
",0.3333333333333333,0.09090909090909091,0.14285714285714288,0.0,0.0,0.0,0.3333333333333333,0.09090909090909091,0.14285714285714288,0.004518022605007096,0.6097150444984436,No,1.0,1.0,1.0
q366,"How many texts/datapoints are in the SemEval, TASS and SENTIPOLC datasets?","Total number of annotated data:
Semeval'15: 10712
Semeval'16: 28632
Tass'15: 69000
Sentipol'14: 6428","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.20116351544857025,No,0.0,0.0,0.0
q367,In which languages did the approach outperform the reported results?,"Arabic, German, Portuguese, Russian, Swedish","English, French, Spanish, Italian, Arabic, German, Portuguese, Russian, and Swedish.
",0.5,1.0,0.6666666666666666,0.3333333333333333,0.75,0.46153846153846156,0.5,1.0,0.6666666666666666,0.37754323999245865,0.8859562277793884,No,0.0,0.0,0.0
q368,Which is the baseline model?,"The three baseline models are the i-vector model, a standard RNN LID system and a multi-task RNN LID system. ","The context describes multiple baselines, including a general system trained on shared data, a Baseline1 model using bi-LSTMs, and a LSTM baseline using ELMo embeddings.  It doesn't identify a single, overarching ""the"" baseline.
",0.22857142857142856,0.38095238095238093,0.2857142857142857,0.029411764705882353,0.05,0.03703703703703704,0.17142857142857143,0.2857142857142857,0.21428571428571427,0.03184652052227926,0.502792239189148,No,0.0,0.0,0.0
q369,What is the main contribution of the paper? ,"Proposing an improved RNN model, the phonetic temporal neural LID approach, based on phonetic features that results in better performance","The paper demonstrates how text analytic approaches can help understand different aspects of global governance, highlighting international development issues prioritized by governments in the UN and key country-specific factors associated with discussing development issues.
",0.08571428571428572,0.15,0.10909090909090909,0.0,0.0,0.0,0.08571428571428572,0.15,0.10909090909090909,0.013494116947566302,0.1303258091211319,No,0.0,0.0,0.0
q370,How do they get the formal languages?,These are well-known formal languages some of which was used in the literature to evaluate the learning capabilities of RNNs.,"The answer is not found in the context.
",0.375,0.14285714285714285,0.20689655172413796,0.14285714285714285,0.05,0.07407407407407408,0.25,0.09523809523809523,0.13793103448275862,0.027816536947595952,0.11267686635255814,No,0.0,0.0,0.0
q371,What is a confusion network or lattice?,"graph-like structures where arcs connect nodes representing multiple hypothesized words, thus allowing multiple incoming arcs unlike 1-best sequences","A confusion network (CN) and lattice are graph-like structures where arcs connected by nodes represent hypothesized words.  Each node in a lattice marks a timestamp in an utterance, and each arc represents a hypothesized word with its corresponding acoustic and language model scores.
",0.25,0.55,0.34375,0.16279069767441862,0.3684210526315789,0.22580645161290322,0.25,0.55,0.34375,0.06084398257751886,0.4554698169231415,Yes,1.0,1.0,1.0
q372,How close do clusters match to ground truth tone categories?,"NMI between cluster assignments and ground truth tones for all sylables is:
Mandarin: 0.641
Cantonese: 0.464","In Mandarin, the clusters closely match the ground truth tone categories.  In Cantonese, the model found 5 clusters, which is one fewer than the 6 phonemic tones.
",0.25925925925925924,0.3888888888888889,0.3111111111111111,0.07692307692307693,0.11764705882352941,0.0930232558139535,0.18518518518518517,0.2777777777777778,0.22222222222222224,0.03053216387177772,0.7042528986930847,No,1.0,1.0,1.0
q373,what are the evaluation metrics?,"Precision, Recall, F1","Manual evaluation, automatic evaluation metrics (ROUGE, BLEU, METEOR), task-based evaluation;  Task completion ratio, User satisfaction degree, Response fluency, Number of dialogue turns, BPRA (Belief Per-Response Accuracy);  number of turns in a dialogue, ratio of task completion.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.008327748566971627,0.18142274022102356,No,0.0,0.0,0.0
q374,which datasets were used in evaluation?,"CoNLL 2003, GermEval 2014, CoNLL 2002, Egunkaria, MUC7, Wikigold, MEANTIME, SONAR-1, Ancora 2.0","Laptop, Restaurant, CMRC-2017 (Cloze and User Query Tracks), avsd.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.023183636251438454,0.31932273507118225,No,0.0,0.0,0.0
q375,what are the baselines?,Perceptron model using the local features.,"The baselines include:  neural baselines, ANRES_Sing_R, ANRES_Sing_Z, ANRES_Seq_R, ANRES_Seq_Z, i-vector system, x-vector system, Sliding Window (SW), Sliding Window + Distance (SW+D), 3, 4, and 5-gram language models, and a bidirectional Recurrent Neural Network (RNN) language model.
",0.0425531914893617,0.3333333333333333,0.07547169811320754,0.0,0.0,0.0,0.02127659574468085,0.16666666666666666,0.03773584905660377,0.005644235767491028,0.20765507221221924,No,0.0,0.0,0.0
q376,What monolingual word representations are used?,"AraVec for Arabic, FastText for French, and Word2vec Google News for English.","Google's Word2Vec and Stanford's GloVe.
",0.42857142857142855,0.25,0.3157894736842105,0.0,0.0,0.0,0.14285714285714285,0.08333333333333333,0.10526315789473685,0.021537493400170523,0.6431131958961487,No,0.0,0.0,0.0
q377,Do they build one model per topic or on all topics?,One model per topic.,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08116697886877475,0.049132879823446274,No,0.0,0.0,0.0
q378,Do they quantitavely or qualitatively evalute the output of their low-rank approximation to verify the grouping of lexical items?,They evaluate quantitatively.,"Quantitatively, using Recall@k, Mean Rank (MR), and Mean Reciprocal Rank (MRR).  The context also describes qualitative evaluation via human annotation (highlighting semantically-equivalent phrases).
",0.08,0.6666666666666666,0.14285714285714285,0.0,0.0,0.0,0.04,0.3333333333333333,0.07142857142857142,0.010253311603116816,0.44237303733825684,Yes,1.0,1.0,1.0
q379,How well does their system perform on the development set of SRE?,"EER 16.04, Cmindet 0.6012, Cdet 0.6107","The system showed an 11% relative improvement compared to the primary system on the SRE 2016 development set, but the results may be misleading due to consistent recording conditions for all speakers.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010559717178170818,0.15816648304462433,No,0.0,0.0,0.0
q380,Which of the classifiers showed the best performance?,Logistic regression,"AttEmbedDecoder
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.06267746537923813,No,0.0,0.0,0.0
q381,How are the keywords associated with events such as protests selected?,"By using a Bayesian approach  and by using word-pairs, where they extract all the pairs of co-occurring words within each tweet.  They search for the words that achieve the highest number of spikes matching the days of events.","Keywords are selected by searching for word-pairs that achieve the highest number of spikes matching the days of events.  The method uses the Jaccard similarity metric to value spikes matching events and penalize other cases.
",0.5833333333333334,0.525,0.5526315789473685,0.37142857142857144,0.3333333333333333,0.35135135135135137,0.4444444444444444,0.4,0.4210526315789474,0.3183232290992116,0.7109853029251099,No,1.0,1.0,1.0
q382,How many speeches are in the dataset?,5575 speeches,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12845686078071594,No,0.0,0.0,0.0
q383,What is different in BERT-gen from standard BERT?,"They use a left-to-right attention mask so that the input tokens can only attend to other input tokens, and the target tokens can only attend to the input tokens and already generated target tokens.","BERT-gen allows for direct text generation from pre-trained encoders, either in mono- or multi-modal setups, whereas standard BERT is fine-tuned.
",0.041666666666666664,0.027777777777777776,0.03333333333333333,0.0,0.0,0.0,0.041666666666666664,0.027777777777777776,0.03333333333333333,0.011127350279702089,0.19112783670425415,No,0.0,0.0,0.0
q384,How are multimodal representations combined?,The image feature vectors are mapped into BERT embedding dimensions and treated like a text sequence afterwards.,"The feature vectors from different embedding sets are concatenated.
",0.5555555555555556,0.29411764705882354,0.3846153846153846,0.125,0.0625,0.08333333333333333,0.4444444444444444,0.23529411764705882,0.30769230769230765,0.049627465510420604,0.5269306898117065,Yes,0.0,0.0,0.0
q385,What is the problem with existing metrics that they are trying to address?,"Answer with content missing: (whole introduction) However, recent
studies observe the limits of ROUGE and find in
some cases, it fails to reach consensus with human.
judgment (Paulus et al., 2017; Schluter, 2017).","Existing metrics fail to properly address biases in datasets, leading algorithms to exploit them, resulting in poor real-world performance;  they also don't correlate well with human judgment in some areas (e.g., machine translation).  Some metrics are not suited for unbalanced categories in datasets.
",0.15217391304347827,0.21212121212121213,0.17721518987341772,0.08888888888888889,0.125,0.10389610389610389,0.10869565217391304,0.15151515151515152,0.12658227848101267,0.031528588518789175,0.1716684252023697,No,0.0,0.0,0.0
q386,How are discourse embeddings analyzed?,They perform t-SNE clustering to analyze discourse embeddings,"t-SNE clustering is performed on the discourse embeddings, examining the closest neighbors of each embedding to observe whether similar discourse relations group together (e.g., explanation and interpretation).
",0.2413793103448276,0.7777777777777778,0.368421052631579,0.10714285714285714,0.375,0.16666666666666666,0.1724137931034483,0.5555555555555556,0.26315789473684215,0.03158092042483704,0.863402247428894,Yes,0.0,0.0,1.0
q387,How are discourse features incorporated into the model?,They derive entity grid with grammatical relations and RST discourse relations and concatenate them with pooling vector for the char-bigrams before feeding to the resulting vector to the softmax layer.,"Discourse features are embedded in a parallel CNN at the input end, or used as a feature vector concatenated to the output layer.  Global featurization of discourse embeddings is also used.
",0.22580645161290322,0.22580645161290322,0.22580645161290322,0.03333333333333333,0.03333333333333333,0.03333333333333333,0.1935483870967742,0.1935483870967742,0.1935483870967742,0.03541060769394014,0.49374666810035706,No,0.0,1.0,1.0
q388,What discourse features are used?,Entity grid with grammatical relations and RST discourse relations.,"The provided context describes discourse embeddings, but does not explicitly list the discourse features used.
",0.06666666666666667,0.1111111111111111,0.08333333333333334,0.0,0.0,0.0,0.06666666666666667,0.1111111111111111,0.08333333333333334,0.02719665272174911,0.3567611277103424,No,0.0,0.0,0.0
q389,What are proof paths?,A sequence of logical statements represented in a computational graph,"Proof paths are paths from the thesis to leaf nodes in an argument tree, representing a particular line of reasoning.
",0.2,0.4,0.26666666666666666,0.0,0.0,0.0,0.1,0.2,0.13333333333333333,0.022731543567022873,0.2648046910762787,Yes,0.0,0.0,0.0
q390,What external sources are used?,"Raw data from Gigaword, Automatically segmented text from Gigaword, Heterogenous training data from People's Daily, POS data from People's Daily","Twitter, NASA (via Vermont Advanced Computing Core), NSF (via BIGDATA grant), Google, and DARPA.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.021671320168371845,0.22655655443668365,No,0.0,0.0,0.0
q391,How much better peformance is achieved in human evaluation when model is trained considering proposed metric?,"Pearson correlation to human judgement - proposed vs next best metric
Sample level comparison:
- Story generation: 0.387 vs 0.148
- Dialogue: 0.472 vs 0.341
Model level comparison:
- Story generation:  0.631 vs 0.302
- Dialogue: 0.783 vs 0.553","The provided text states the proposed comparative evaluator correlates better with human judgment than BLEU and perplexity, and outperforms other parameterized metrics like the adversarial evaluator and ADEM.  It does *not* quantify the *amount* of improvement.
",0.1111111111111111,0.09523809523809523,0.10256410256410256,0.0,0.0,0.0,0.08333333333333333,0.07142857142857142,0.07692307692307691,0.009674362658702076,0.4740614593029022,No,1.0,1.0,1.0
q392,How much transcribed data is available for for Ainu language?,Transcribed data is available for duration of 38h 54m 38s for 8 speakers.,"Most of the recordings are not transcribed.
",0.2857142857142857,0.15384615384615385,0.2,0.0,0.0,0.0,0.14285714285714285,0.07692307692307693,0.1,0.031021609279760064,0.5782769322395325,No,1.0,1.0,1.0
q393,What baseline approaches do they compare against?,"HotspotQA: Yang, Ding, Muppet
Fever: Hanselowski, Yoneda, Nie","ILP and ILP+MC.  (and various neural baselines)
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09667649120092392,No,0.0,0.0,0.0
q394,how many domains did they experiment with?,2,"20
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2847701907157898,No,1.0,1.0,1.0
q395,How long is the dataset?,8000,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08488640934228897,No,0.0,0.0,0.0
q396,What is a string kernel?,String kernel is a technique that uses character n-grams to measure the similarity of strings,"A string kernel is a kernel function used to measure the pairwise similarity between text samples, based on character n-grams.
",0.5714285714285714,0.75,0.6486486486486486,0.35,0.4666666666666667,0.4,0.42857142857142855,0.5625,0.4864864864864864,0.12653094028246673,0.8869720697402954,Yes,1.0,1.0,1.0
q397,How do they correlate NED with emotional bond levels?,They compute Pearsons correlation between NED measure for patient-to-therapist and patient-perceived emotional bond rating and NED measure for therapist-to-patient and patient-perceived emotional bond rating,"They compute Pearson's correlation coefficients between the proposed NED measure (both patient-to-therapist and therapist-to-patient) and patient-perceived emotional bond ratings.  They also correlate the emotional bond with baseline measures.
",0.7058823529411765,0.7741935483870968,0.7384615384615385,0.5151515151515151,0.5666666666666667,0.5396825396825397,0.6176470588235294,0.6774193548387096,0.6461538461538462,0.12489558671106762,0.9053001403808594,Yes,1.0,1.0,1.0
q398,What was their F1 score on the Bengali NER corpus?,52.0%,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1726384460926056,No,0.0,0.0,0.0
q399,What is the size of the dataset?,"300,000 sentences with 1.5 million single-quiz questions","The answer is not found in the context.  While various dataset sizes are mentioned (e.g., 2,557 days, 12,350 ads), there is no single figure for the overall size of *one* dataset.
",0.029411764705882353,0.1,0.04545454545454545,0.0,0.0,0.0,0.029411764705882353,0.1,0.04545454545454545,0.0,0.2917250394821167,No,0.0,0.0,0.0
q400,How many examples do they have in the target domain?,"Around 388k examples, 194k from tst2013 (in-domain) and 194k from newstest2014 (out-of-domain)","The answer is not found in the context.
",0.125,0.06666666666666667,0.08695652173913045,0.0,0.0,0.0,0.125,0.06666666666666667,0.08695652173913045,0.0,0.12108388543128967,No,0.0,0.0,0.0
q401,What is the baseline model?,a RNN-based seq2seq VC model called ATTS2S based on the Tacotron model,"Baseline1, a model that takes two inputs (transcript and question) and outputs span scores.  A general system built using shared training data, chosen based on cloze validation set performance, and further tuned with 10-fold cross-validation on user query validation data.  Model 1 is also presented as a baseline.
",0.1,0.38461538461538464,0.15873015873015872,0.02040816326530612,0.08333333333333333,0.032786885245901634,0.1,0.38461538461538464,0.15873015873015872,0.01488603753237641,0.2717186510562897,No,0.0,0.0,0.0
q402,Where did they get training data?,AmazonQA and ConciergeQA datasets,"Data released by the APDA shared task organizers.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.35785186290740967,No,0.0,0.0,0.0
q403,What extraction model did they use?,"Multi-Encoder, Constrained-Decoder model","A multi-task learning framework with a sequence-to-sequence model for answer synthesis.
",0.14285714285714285,0.4,0.21052631578947364,0.0,0.0,0.0,0.14285714285714285,0.4,0.21052631578947364,0.033864985683445356,0.20471736788749695,No,0.0,0.0,0.0
q404,Which datasets did they experiment on?,ConciergeQA and AmazonQA,"novel-9, novel-50, IMDB62, SCI, Stat2015, Stat2016, CS2016
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08037684857845306,No,0.0,0.0,0.0
q405,How do slot binary classifiers improve performance?,by adding extra supervision to generate the slots that will be present in the response,"Slot binary classifiers predict the probability of a slot being requested by the user, improving performance by providing information for the candidate tracker and the candidate classifier.
",0.18518518518518517,0.3333333333333333,0.23809523809523808,0.0,0.0,0.0,0.14814814814814814,0.26666666666666666,0.19047619047619047,0.01693409667719809,0.3943033814430237,Yes,0.0,0.0,0.0
q406,What baselines have been used in this work?,"NDM, LIDM, KVRN, and TSCP/RL","Neural baselines (with different models), ANRES_Sing_R, ANRES_Sing_Z, ANRES_Seq_R, ANRES_Seq_Z, i-vector system, and x-vector system.
",0.041666666666666664,0.16666666666666666,0.06666666666666667,0.0,0.0,0.0,0.041666666666666664,0.16666666666666666,0.06666666666666667,0.02243388007668207,0.3682689368724823,No,0.0,0.0,0.0
q407,what bottlenecks were identified?,Confusion in recognizing the words that are active at a given node by a speech recognition solution developed for Indian Railway Inquiry System.,"The need for universalized benchmarks and metrics, reproducibility of published methodologies, bias in available data, and biological/chemical interpretability/explainability of solutions.  Also, the use of different datasets and evaluation metrics makes model performance assessment challenging, and comprehensive benchmarking platforms are lacking.
",0.11904761904761904,0.21739130434782608,0.15384615384615383,0.0,0.0,0.0,0.07142857142857142,0.13043478260869565,0.0923076923076923,0.010683243644556363,0.03803955763578415,No,0.0,0.0,0.0
q408,By how much do they outperform BiLSTMs in Sentiment Analysis?,Proposed RCRN outperforms ablative baselines BiLSTM by +2.9% and 3L-BiLSTM by +1.1% on average across 16 datasets.,"The provided text states that the proposed model significantly outperforms BiLSTMs in sentiment analysis, achieving new state-of-the-art performance on multiple datasets.  It also outperforms other complex models, including those from BIBREF4, BIBREF6, and even BIBREF8 on specific datasets.  However, a precise numerical difference isn't given.
",0.12244897959183673,0.3,0.17391304347826086,0.0,0.0,0.0,0.12244897959183673,0.3,0.17391304347826086,0.017268919138039227,0.5783291459083557,No,0.0,1.0,1.0
q409,what state of the accuracy did they obtain?,51.5,"Word recognition accuracy reached about 80% in the speaker-closed setting and 60% on average in the speaker-open setting.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15395745635032654,No,0.0,0.0,0.0
q410,what models did they compare to?,"High-order CNN, Tree-LSTM, DRNN, DCNN, CNN-MC, NBoW and SVM ","MatchZoo models, Arc1, CDSSM, CNN-jieba/PKU/CTB, BIBREF15, ESIM, Decomposable Attention Model, KIM, BERT, and models pretrained on Wikipedia and BookCorpus.
",0.09090909090909091,0.16666666666666666,0.11764705882352942,0.0,0.0,0.0,0.09090909090909091,0.16666666666666666,0.11764705882352942,0.017577459356554814,0.4587818682193756,No,0.0,0.0,0.0
q411,which benchmark tasks did they experiment on?, They used Stanford Sentiment Treebank benchmark for sentiment classification task and   AG English news corpus for the text classification task.,"Molecule generation, prediction of binding affinity and toxicity, question answering (QA), named entity recognition (NER).
",0.06666666666666667,0.05,0.05714285714285715,0.0,0.0,0.0,0.06666666666666667,0.05,0.05714285714285715,0.019582248325011244,0.33377382159233093,No,0.0,0.0,0.0
q412,How is the proficiency score calculated?,"They used 6 indicators for proficiency (same for written and spoken) each marked by bad, medium or good by one expert.","The proficiency score is computed by summing the scores related to 6 indicators (from 3 to 6 in some campaigns), where each indicator can be 0, 1, or 2 (bad, medium, good).
",0.28125,0.42857142857142855,0.3396226415094339,0.06451612903225806,0.1,0.0784313725490196,0.1875,0.2857142857142857,0.22641509433962265,0.05190619769166794,0.6155295372009277,Yes,0.0,1.0,1.0
q413,What proficiency indicators are used to the score the utterances?,"6 indicators:
- lexical richness
- pronunciation and fluency
- syntactical correctness
- fulfillment of delivery
- coherence and cohesion
- communicative, descriptive, narrative skills","6 indicators in 2017/2018 (from 3 to 6 in the 2016 campaign).
",0.15384615384615385,0.10526315789473684,0.125,0.08333333333333333,0.05555555555555555,0.06666666666666667,0.15384615384615385,0.10526315789473684,0.125,0.023948255115961062,0.3364753723144531,No,1.0,1.0,1.0
q414,What accuracy is achieved by the speech recognition system?,"Accuracy not available: WER results are reported 42.6 German, 35.9 English","91.2% accuracy on utterances containing words, sentences, and conversations.  Word recognition accuracy reached about 80% in the speaker-closed setting and 60% on average in the speaker-open setting.
",0.03333333333333333,0.07692307692307693,0.046511627906976744,0.0,0.0,0.0,0.03333333333333333,0.07692307692307693,0.046511627906976744,0.010885011049519644,0.36235475540161133,No,0.0,0.0,0.0
q415,How is the speech recognition system evaluated?,Speech recognition system is evaluated using WER metric.,"Generally, by putting the solution to actual use by intended users, analyzing logs to identify successful/unsuccessful transactions, and using this data to identify areas for improvement in recognition.  A Wizard of Oz (WoZ) approach is often employed to maintain user experience during testing.
",0.06818181818181818,0.375,0.11538461538461538,0.0,0.0,0.0,0.045454545454545456,0.25,0.07692307692307693,0.009903417847796372,0.32527512311935425,No,0.0,0.0,0.0
q416,How many of the utterances are transcribed?,Total number of transcribed utterances including Train and Test for both Eng and Ger language is 5562 (2188 cleaned),"2000
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08342097699642181,No,0.0,0.0,0.0
q417,How many utterances are in the corpus?,Total number of utterances available is: 70607 (37344 ENG + 33263 GER),"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.24490493535995483,No,0.0,0.0,0.0
q418,By how much does their model outperform both the state-of-the-art systems?,w.r.t Rouge-1 their model outperforms by 0.98% and w.r.t Rouge-L their model outperforms by 0.45%,"The model significantly outperforms state-of-the-art systems in overall performance, with improvements in specific categories (Person and Miscellaneous).  Specific percentage improvements are given for various comparisons but not a single overall figure.
",0.08823529411764706,0.13043478260869565,0.10526315789473685,0.0,0.0,0.0,0.08823529411764706,0.13043478260869565,0.10526315789473685,0.01533227700920213,0.4539671242237091,No,0.0,0.0,0.0
q419,What is the state-of-the art?,neural attention model with a convolutional encoder with an RNN decoder and RNN encoder-decoder,"In recent years, deep learning methods, particularly deep convolutional networks, dominate sentiment analysis.  For coreference resolution, systems like IMS, Latent-Tree, and Berkeley are state-of-the-art supervised models.  In dialogue systems, RNNs pre-trained using GloVe word embeddings are achieving high accuracy.  Various other models like MLP, SVM, and Random Forest are used for classification.
",0.07017543859649122,0.26666666666666666,0.11111111111111112,0.0,0.0,0.0,0.05263157894736842,0.2,0.08333333333333333,0.006622258249686733,0.26784834265708923,No,0.0,0.0,0.0
q420,What was the performance on the self-collected corpus?,F1 scores of 86.16 on slot filling and 94.56 on intent detection,"The best result achieved was a cosine similarity of 0.598.
",0.09090909090909091,0.07142857142857142,0.08,0.0,0.0,0.0,0.09090909090909091,0.07142857142857142,0.08,0.034220976227266095,0.0878634825348854,No,0.0,0.0,0.0
q421,What is the size of their dataset?,"10,001 utterances","Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08109205216169357,No,0.0,0.0,0.0
q422,how was the dataset built?,"Questions are gathered from anonymized, aggregated queries to the Google search engine. Queries that are likely to be yes/no questions are heuristically identified: we found selecting queries where the first word is in a manually constructed set of indicator words and are of sufficient length, to be effective. Questions are only kept if a Wikipedia page is returned as one of the first five results, in which case the question and Wikipedia page are given to a human annotator for further processing. Annotators label question/article pairs in a three-step process. First, they decide if the question is good, meaning it is comprehensible, unambiguous, and requesting factual information. This judgment is made before the annotator sees the Wikipedia page. Next, for good questions, annotators find a passage within the document that contains enough information to answer the question. Annotators can mark questions as not answerable"" if the Wikipedia article does not contain the requested information. Finally, annotators mark whether the question's answer is yes"" or no""","The dataset was built in three steps: scraping PDF files from the Lok Sabha website, converting the PDFs to text files, and converting the unstructured information into a structured format.  Other datasets (novel-9, novel-50, IMDB62) were constructed using different methods, described in the text.
",0.32608695652173914,0.08875739644970414,0.1395348837209302,0.022222222222222223,0.005952380952380952,0.009389671361502348,0.21739130434782608,0.05917159763313609,0.09302325581395349,0.001894973012594604,0.25491634011268616,No,1.0,1.0,1.0
q423,what processing was done on the speeches before being parsed?,Remove numbers and interjections,"Manual transcription, segmentation based on clear instructions to evaluators, and structuring into a relational database.
",0.06666666666666667,0.25,0.10526315789473685,0.0,0.0,0.0,0.06666666666666667,0.25,0.10526315789473685,0.021476912089159056,0.0954485833644867,No,0.0,0.0,0.0
q424,What is the performance change of the textual semantic similarity task when no error and maximum errors (noise) are present?,"10 Epochs: pearson-Spearman correlation  drops  60 points when error increase by 20%
50 Epochs: pearson-Spearman correlation  drops  55 points when error increase by 20%","Very good numbers when no error (0.0% error); significant drop in performance as error percentage increases.
",0.29411764705882354,0.19230769230769232,0.23255813953488372,0.0,0.0,0.0,0.29411764705882354,0.19230769230769232,0.23255813953488372,0.018402902615276922,0.29981356859207153,No,1.0,1.0,1.0
q425,Which sentiment analysis data set has a larger performance drop when a 10% error is introduced?,SST-2 dataset,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.053742747753858566,No,0.0,0.0,0.0
q426,How much is pre-training loss increased in Low/Medium/Hard level of pruning?,"The increase is linearly from lowest on average 2.0 , medium around 3.5, and the largest is 6.0","The context states that pre-training loss increases as weights necessary for fitting the pre-training data are pruned (medium levels).  No specific values of loss increase are given.
",0.13793103448275862,0.2,0.16326530612244897,0.0,0.0,0.0,0.10344827586206896,0.15,0.12244897959183673,0.016961343903963387,0.3749895393848419,No,0.0,1.0,1.0
q427,What is the average length of the recordings?,40 minutes,"4.74 seconds
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6508417725563049,No,0.0,0.0,0.0
q428,What were their results?,Best model achieved F-score 74.7 on NALCS and F-score of 70.0 on LMS on test set,"The system surpassed baselines in machine translation (English-French), achieving better BLEU and TER scores on various test sets.  Answer prediction scores were also higher, with improvements in EM and F1.  FEVER scores also improved substantially.  Human evaluation of incident descriptions showed surprisingly low agreement with recorded harm levels.  No major improvements were found in this human evaluation aspect and results varied between runs.
",0.125,0.4,0.19047619047619047,0.015873015873015872,0.05263157894736842,0.024390243902439025,0.09375,0.3,0.14285714285714285,0.0070930906528519485,0.45581480860710144,No,0.0,0.0,0.0
q429,What is the prediction accuracy of the model?,"mean prediction accuracy 0.99582651
S&P 500 Accuracy 0.99582651","82.0%
",0.5,0.09090909090909091,0.15384615384615385,0.0,0.0,0.0,0.5,0.09090909090909091,0.15384615384615385,0.0,0.37881022691726685,No,0.0,0.0,0.0
q430,What is the dataset used in the paper?,"historical S&P 500 component stocks
 306242 news articles","SCI, novel-9, novel-50, IMDB62.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16591747105121613,No,0.0,0.0,0.0
q431,How does the SCAN dataset evaluate compositional generalization?,"it systematically holds out inputs in the training set containing basic primitive verb, ""jump"", and tests on sequences containing that verb.","The SCAN dataset evaluates compositional generalization by testing the ability to generalize known primitive verbs to valid unseen constructions and, later, by including compositional generalization based on known templates (e.g., ""Jump around right"").
",0.20588235294117646,0.3333333333333333,0.2545454545454545,0.030303030303030304,0.05,0.03773584905660378,0.14705882352941177,0.23809523809523808,0.18181818181818185,0.024240945815868543,0.5581567883491516,Yes,0.0,0.0,1.0
q432,How much does this system outperform prior work?,"The system outperforms by 27.7% the LSTM model, 38.5% the RL-SPINN model and 41.6% the Gumbel Tree-LSTM","The system achieves new state-of-the-art results on HotpotQA, with large-margin improvements on all metrics, doubling the joint EM and increasing EM by ~8 points and F1 by ~9 points.  It also outperforms a strong feature-rich system for Task A when sufficient training data is available.
",0.14,0.3181818181818182,0.19444444444444445,0.02040816326530612,0.047619047619047616,0.02857142857142857,0.1,0.22727272727272727,0.1388888888888889,0.01954373830911292,0.41085055470466614,No,0.0,0.0,1.0
q433,What are the baseline systems that are compared against?,"The system is compared to baseline models: LSTM, RL-SPINN and Gumbel Tree-LSTM","Participant systems, AoA Reader, AS Reader
",0.16666666666666666,0.07142857142857142,0.1,0.0,0.0,0.0,0.16666666666666666,0.07142857142857142,0.1,0.0260859600387516,0.07163272798061371,No,0.0,0.0,1.0
q434,What systems are tested?,"BULATS i-vector/PLDA
BULATS x-vector/PLDA
VoxCeleb x-vector/PLDA
PLDA adaptation (X1)
 Extractor fine-tuning (X2) ","Systems developed by 74 participants (final 28), and 43 submitted systems.  Baseline systems, and the framework's system are also tested.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01717906001229923,0.11300257593393326,No,0.0,0.0,0.0
q435,What benchmark datasets they use?,VQA and GeoQA,"Airline Travel Information Systems (ATIS), SNIPS Natural Language Understanding, SQuAD, OntoNotes, ACE, MUC 7, Ritter NER, MSM 2013, UMBC Twitter.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07444790750741959,No,0.0,0.0,0.0
q436,How do they combine MonaLog with BERT?,They use Monalog for data-augmentation to fine-tune BERT on this task,"The hybrid model uses MonaLog's high precision on Entailment and Contradiction.  If MonaLog predicts E or C, it's trusted; otherwise, BERT is used.
",0.16,0.3076923076923077,0.2105263157894737,0.041666666666666664,0.08333333333333333,0.05555555555555555,0.12,0.23076923076923078,0.15789473684210525,0.01588011771404737,0.5466133952140808,No,0.0,0.0,1.0
q437,How do they select monotonicity facts?,They derive it from Wordnet,"They collected examples from the GLUE diagnostic dataset, FraCaS, and 11 linguistics publications.  They reclassified problems from GLUE using their own criteria, and crowdsourced and collected data from linguistics publications.
",0.06666666666666667,0.4,0.1142857142857143,0.0,0.0,0.0,0.06666666666666667,0.4,0.1142857142857143,0.012557690800697192,0.3320710062980652,No,0.0,0.0,0.0
q438,What are the 12 categories devised?,"Economics, Genocide, Geography, History, Human Rights, Kurdish, Kurdology, Philosophy, Physics, Theology, Sociology, Social Study","Activity/sports recognition, sentiment understanding, positional reasoning, subordinate object recognition, and Appreciation, Call for Action, Issue, Blaming.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.023497215931088396,0.3457717299461365,No,0.0,0.0,0.0
q439,what are the off-the-shelf systems discussed in the paper?,"Answer with content missing: (Names of many identifiers missing) TextCat, ChromeCLD, LangDetect, langid.py, whatlang, whatthelang, YALI, LDIG, Polyglot 3000, Lextek Language Identifier and Open Xerox Language Identifier.","Random Guess, Top Frequency, AS Reader.  i-vector system and x-vector system.
",0.07692307692307693,0.03571428571428571,0.04878048780487805,0.0,0.0,0.0,0.07692307692307693,0.03571428571428571,0.04878048780487805,0.006495782380497174,-0.006534476764500141,No,0.0,0.0,0.0
q440,How many rules had to be defined?,"WikiSQL - 2 rules (SELECT, WHERE)
SimpleQuestions - 1 rule
SequentialQA - 3 rules (SELECT, WHERE, COPY)","5
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15045775473117828,No,0.0,0.0,0.0
q441,What was performance of classifiers before/after using distant supervision?,"Bi-LSTM: For low resource <17k clean data: Using distant supervision resulted in huge boost of F1 score (1k eg. ~9 to ~36 wit distant supervision)
BERT: <5k clean data boost of F1 (1k eg. ~32 to ~47 with distant supervision)","Distant supervision *improved* classifier performance in some cases, but also *deteriorated* performance in other cases, particularly when the model was already trained on a good amount of clean data.  The improvement was more pronounced in the low-resource languages and with smaller models.
",0.20930232558139536,0.21951219512195122,0.2142857142857143,0.07142857142857142,0.075,0.07317073170731707,0.16279069767441862,0.17073170731707318,0.16666666666666669,0.01866140150304229,0.43651723861694336,No,1.0,1.0,1.0
q442,How big are the datasets used?,"Evaluation datasets used:
CMRC 2018 - 18939 questions, 10 answers
DRCD - 33953 questions, 5 answers
NIST MT02/03/04/05/06/08 Chinese-English - Not specified

Source language train data:
SQuAD - Not specified","The provided text mentions various datasets with sizes like 2,557 days, 2,922 days, 730 days, 19 novels, 50 authors, 250 novels, 5,000 images, 15,000 images, 75,000 questions.  More sizes are referenced, but not provided in full detail.
",0.09523809523809523,0.125,0.1081081081081081,0.0,0.0,0.0,0.07142857142857142,0.09375,0.08108108108108107,0.010755798168739934,0.3326747715473175,No,0.0,0.0,0.0
q443,What datasets are used for training/testing models? ,"Microsoft Research dataset containing movie, taxi and restaurant domains.","MS COCO, Bing, Flickr (question generation); AIDA-Train, Wikipedia (entity linking); CUB, MSCOCO, Oxford-102 (text-to-image synthesis);  BIBREF4 small diagnostic, Geoquery, Wall Street Journal, Tatoeba (other tasks).  AIDA-B, other datasets (evaluation).
",0.029411764705882353,0.1111111111111111,0.04651162790697674,0.0,0.0,0.0,0.029411764705882353,0.1111111111111111,0.04651162790697674,0.008165967685677476,0.31686264276504517,No,0.0,0.0,0.0
q444,How better is gCAS approach compared to other approaches?,"For entity  F1 in the movie, taxi and restaurant domain it results in scores of 50.86, 64, and 60.35. For success, it results it outperforms in the movie and restaurant domain with scores of 77.95 and 71.52","gCAS outperforms other models in act prediction, frame prediction, entity F$_1$, and Success F$_1$ in most domains, especially in movie and restaurant domains.  It handles multi-label slots easily, making it better at generating shorter, more accurate sequences.
",0.35,0.34146341463414637,0.34567901234567905,0.10256410256410256,0.1,0.10126582278481013,0.2,0.1951219512195122,0.19753086419753088,0.042300040297579194,0.43280503153800964,Yes,0.0,0.0,1.0
q445,What is specific to gCAS cell?,"It has three sequentially connected units to output continue, act and slots generating multi-acts in a doble recurrent manner.","gCAS uses a proposed recurrent cell with separate continue, act, and slots units sequentially connected.
",0.6,0.45,0.5142857142857143,0.2857142857142857,0.21052631578947367,0.24242424242424243,0.26666666666666666,0.2,0.2285714285714286,0.11136278656778544,0.5863015055656433,Yes,1.0,1.0,1.0
q446,What is the source of external knowledge?,counts of predicate-argument tuples from English Wikipedia,"Manually defined rules and world knowledge, such as number/gender requirements of pronouns, selectional preference, and eventuality knowledge.
",0.05555555555555555,0.125,0.07692307692307691,0.0,0.0,0.0,0.05555555555555555,0.125,0.07692307692307691,0.016466642419110007,0.22724206745624542,No,0.0,0.0,0.0
q447,What were the sizes of the test sets?,Test set 1 contained 57 drug labels and 8208 sentences and test set 2 contained 66 drug labels and 4224 sentences,"1841 IPUs
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.040125928819179535,No,0.0,0.0,0.0
q448,Which datasets are used?,"ABSA SemEval 2014-2016 datasets
Yelp Academic Dataset
Wikipedia dumps","SST-2, Yelp, MRPC, STS-B, QQP, SQuAD 1.1, SQuAD 2.0, MRQA, MS COCO, Bing, Flickr, novel-9, novel-50, IMDB62, avsd, Charades.
",0.04,0.1,0.05714285714285714,0.0,0.0,0.0,0.04,0.1,0.05714285714285714,0.010559717178170818,0.39029568433761597,No,0.0,0.0,0.0
q449,How much does it minimally cost to fine-tune some model according to benchmarking framework?,"$1,728","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08053942024707794,No,0.0,0.0,0.0
q450,What models are included in baseline benchmarking results?,"BERT, XLNET RoBERTa, ALBERT, DistilBERT","Neural baselines, AoA Reader, AS Reader, and participant systems.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04065425428798724,0.05492805689573288,No,0.0,0.0,0.0
q451,"It looks like learning to paraphrase questions, a neural scoring model and a answer selection model cannot be trained end-to-end. How are they trained?",using multiple pivot sentences,"The entire system, including paraphrase generation, scoring, and answer selection, is trained end-to-end using question-answer pairs.
",0.05263157894736842,0.25,0.08695652173913043,0.0,0.0,0.0,0.05263157894736842,0.25,0.08695652173913043,0.018160849415439308,0.105234295129776,No,0.0,0.0,0.0
q452,How much more accurate is the model than the baseline?,"For the Oshiete-goo dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, Trans, by 0.021, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.037.  For the nfL6 dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, CLSTM, by 0.028, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.040. Human evaluation of the NAGM's generated outputs for the Oshiete-goo dataset had 47% ratings of (1), the highest rating, while CLSTM only received 21% ratings of (1). For the nfL6 dataset, the comparison of (1)'s was NAGM's 50% to CLSTM's 30%. ","Improvements range from 0.5% to 3.0% in accuracy, and over 9% for BERT-Base and BERT-Large.
",0.3157894736842105,0.048,0.08333333333333334,0.0,0.0,0.0,0.21052631578947367,0.032,0.05555555555555555,0.00021109410374182597,0.29092657566070557,No,0.0,0.0,0.0
q453,What is new state-of-the-art performance on CoNLL-2009 dataset?,In closed setting 84.22 F1 and in open 87.35 F1.,"The answer is not found in the context.
",0.125,0.08333333333333333,0.1,0.0,0.0,0.0,0.125,0.08333333333333333,0.1,0.045400138092837265,0.3038182556629181,No,0.0,0.0,0.0
q454,What are two strong baseline methods authors refer to?,Marcheggiani and Titov (2017) and Cai et al. (2018),"AoA Reader and AS Reader
",0.2,0.1111111111111111,0.14285714285714285,0.0,0.0,0.0,0.2,0.1111111111111111,0.14285714285714285,0.01765751677720686,0.07246077805757523,No,0.0,0.0,0.0
q455,How many category tags are considered?,14 categories,"100
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05683956667780876,No,0.0,0.0,0.0
q456,What domain does the dataset fall into?,YouTube videos,"The dataset contains questions about entertainment media, sports, and general factual knowledge.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16208122670650482,No,0.0,0.0,0.0
q457,How are EAC evaluated?,"Qualitatively through efficiency, effectiveness and satisfaction aspects and quantitatively through metrics such as precision, recall, accuracy, BLEU score and even human judgement.","EAC is evaluated qualitatively (using ISO 9241) and quantitatively (using automatic evaluation like perplexity and manual evaluation),  with specific metrics depending on the task.  For question answering, F$_1$ score is used.  For RL agents, training and testing curves are reported, and the agent's best validation performance is used for test evaluation.  Other metrics include precision, recall, F$_1$ score, BLEU, EMR, and Success F$_1$ score.
",0.14705882352941177,0.45454545454545453,0.22222222222222224,0.029850746268656716,0.09523809523809523,0.045454545454545456,0.11764705882352941,0.36363636363636365,0.17777777777777776,0.03759614704253158,0.5448870658874512,Yes,1.0,1.0,1.0
q458,What is triangulation?,"Answer with content missing: (Chapter 3) The concept can be easily explained with an example, visualized in Figure 1. Consider the Portuguese (Pt) word trabalho which, according to the MUSE PtEn dictionary, has the words job and work as possible En translations. In turn, these two En words can be translated to 4 and 5 Czech (Cs) words respectively. By utilizing the transitive property (which translation should exhibit) we can identify the set of 7 possible Cs translations for the Pt word trabalho.","The answer is not found in the context.
",0.5,0.047619047619047616,0.08695652173913042,0.0,0.0,0.0,0.375,0.03571428571428571,0.06521739130434782,2.7392066425764806e-06,0.19508090615272522,No,0.0,0.0,0.0
q459,What languages do they use?,"Train languages are: Cantonese, Bengali, Pashto, Turkish, Vietnamese, Haitian, Tamil, Kurdish, Tokpisin and Georgian, while Assamese, Tagalog, Swahili, Lao are used as target languages.","Amharic, Arabic, Hindi, Somali, Chinese, Korean, Japanese, French, German, Swedish, Spanish, Portuguese, Italian, Russian, Polish, Norwegian, Romanian, Dutch, Czech, Turkish, Thai, Urdu, Mandarin, Cantonese, Indonesian, Japanese, Russian, Korean, Vietnamese, English, Spanish, French, German, Russian, Italian, Bengali, Hindi, and Telegu.
",0.1282051282051282,0.20833333333333334,0.15873015873015872,0.0,0.0,0.0,0.07692307692307693,0.125,0.09523809523809525,0.04911522364075463,0.6979992985725403,No,0.0,0.0,0.0
q460,How they evaluate their approach?,"They  evaluate newly proposed models in several low-resource settings across different languages with real, distantly supervised data with non-synthetic noise","They evaluate their approach by: 1) using human evaluators to assess the balance of positive/negative sentiment, topic range, and relatability of dialogs; 2) having human experts evaluate the quality of prescriptions, considering potential curative effect, not just literal similarity; 3) comparing model performance based on how often a metric selected the best hyperparameters/early-stopping checkpoint, and the average human score for those selected models; 4) using human experts familiar with the Oshiete-goo QA community to evaluate AI system outputs (content match, grammar, suitability) on unseen questions.
",0.0449438202247191,0.18181818181818182,0.07207207207207207,0.011363636363636364,0.047619047619047616,0.01834862385321101,0.0449438202247191,0.18181818181818182,0.07207207207207207,0.008428337811633772,0.42022788524627686,No,0.0,0.0,0.0
q461,How large is the corpus?,"It contains 106,350 documents","9.7 million words
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.48809054493904114,No,0.0,0.0,0.0
q462,How large is the dataset?,over 104k documents,"2.6 GB of raw text.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.39479249715805054,No,0.0,0.0,0.0
q463,What was their perplexity score?,Perplexity score 142.84 on dev and 138.91 on test,"The answer is not found in the context.  While the text discusses perplexity and its use in evaluating models, it does not provide a specific perplexity score for any model.
",0.1,0.2727272727272727,0.14634146341463417,0.034482758620689655,0.1,0.05128205128205129,0.06666666666666667,0.18181818181818182,0.0975609756097561,0.013355980882200826,0.6519774794578552,No,0.0,1.0,1.0
q464,What parallel corpus did they use?,Parallel monolingual corpus in English and Mandarin,"Europarl Fr-En and En-De parallel corpora.
",0.25,0.2857142857142857,0.26666666666666666,0.0,0.0,0.0,0.125,0.14285714285714285,0.13333333333333333,0.06567274736060395,0.2785693109035492,No,0.0,0.0,0.0
q465,In which setting they achieve the state of the art?,in open-ended task esp. for counting-type questions ,"SemEval-2015 and SemEval-2016.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.058763508032616325,0.04884079843759537,No,0.0,0.0,0.0
q466,What they formulate the question generation as?,LASSO optimization problem,"The task of automatically generating questions from various inputs such as raw text, database, or semantic representation.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.03947804868221283,No,0.0,0.0,0.0
q467,"Was the degree of offensiveness taken as how generally offensive the text was, or how personally offensive it was to the annotator?",Personal thought of the annotator.,"The context states that offensiveness was rated on a 6-point Likert scale, but does not specify whether it was judged as general or personal offensiveness.
",0.07692307692307693,0.4,0.12903225806451613,0.0,0.0,0.0,0.038461538461538464,0.2,0.06451612903225806,0.01286713095960916,0.11491791158914566,No,1.0,1.0,1.0
q468,Which embeddings do they detect biases in?,Word embeddings trained on GoogleNews and Word embeddings trained on Reddit dataset,"Word embeddings.
",1.0,0.16666666666666666,0.2857142857142857,1.0,0.09090909090909091,0.16666666666666669,1.0,0.16666666666666666,0.2857142857142857,0.0,0.6588382720947266,No,0.0,1.0,1.0
q469,What are the two PharmaCoNER subtasks?,Entity identification with offset mapping and concept indexing,"Named Entity Recognition (NER) and Relation Extraction (RE).
",0.25,0.25,0.25,0.0,0.0,0.0,0.25,0.25,0.25,0.03673526562988939,0.33381667733192444,No,0.0,0.0,0.0
q470,How do they perform data augmentation?,They randomly sample sentences from Wikipedia that contains an object RC and add them to training data,"For text, they use a replacement-based method, replacing words with synonyms, and contextual augmentation using a label-conditional bidirectional language model.  They also simulate real-world revisions by adding, deleting, or replacing words, sentences, paragraphs, section names, and document titles.
",0.07317073170731707,0.17647058823529413,0.10344827586206896,0.0,0.0,0.0,0.07317073170731707,0.17647058823529413,0.10344827586206896,0.009798555761147508,0.4267779290676117,No,0.0,0.0,0.0
q471,What are the characteristics of the city dialect?,Lexicon of the cities tend to use most forms of a particular concept,"City dialects exhibit similar linguistic distances, unlike rural dialects, which are more homogeneous.  City dialects are characterized by a more uniform vocabulary, including most keywords, in contrast to rural dialects that use more specific or unique utterances.
",0.13513513513513514,0.38461538461538464,0.2,0.0,0.0,0.0,0.08108108108108109,0.23076923076923078,0.12,0.01205256842736819,0.4964464008808136,No,0.0,0.0,0.0
q472,What are the characteristics of the rural dialect?,It uses particular forms of a concept rather than all of them uniformly,"Rural regions have small, scattered populations and possess more specific words in their lexicon.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.21648430824279785,No,0.0,0.0,0.0
q473,What is the performance of the models on the tasks?,"Overall accuracy per model is: 5-gram (60.5), LSTM (68.9), TXL (68.7), GPT-2 (80.1)","The models achieve 42.48% accuracy for age, 37.50% for dialect, and 57.81% for gender.  In evidence extraction, single and ensemble models achieve 42.23 and 44.11 ROUGE-L, respectively.  Answer synthesis models improve by 3.72% and 3.65% in ROUGE-L.  Passage selection accuracy is 38.9%.  A German sub-task achieved a macro F1 score of 0.62.  Other results are reported in tables TABREF20, TABREF21, TABREF27, and FIGREF15.
",0.05333333333333334,0.21052631578947367,0.08510638297872342,0.0,0.0,0.0,0.05333333333333334,0.21052631578947367,0.08510638297872342,0.006568156923422029,0.6186646223068237,No,0.0,1.0,1.0
q474,What other non-neural baselines do the authors compare to? ,"bag of words, tf-idf, bag-of-means","TF-IDF (Lexical) and LDA (Topic).
",0.3333333333333333,0.25,0.28571428571428575,0.2,0.14285714285714285,0.16666666666666666,0.3333333333333333,0.25,0.28571428571428575,0.0,0.48853766918182373,No,0.0,0.0,0.0
q475,On what dataset is Aristo system trained?,"Aristo Corpus
Regents 4th
Regents 8th
Regents `12th
ARC-Easy
ARC-challenge ","Training partition of the five datasets.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06755182892084122,No,0.0,1.0,1.0
q476,How many roles are proposed?,12,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11060941219329834,No,0.0,0.0,0.0
q477,What language technologies have been introduced in the past?,"- Font & Keyboard
- Speech-to-Text
- Text-to-Speech
- Text Prediction
- Spell Checker
- Grammar Checker
- Text Search
- Machine Translation
- Voice to Text Search
- Voice to Speech Search","whatthelang, an off-the-shelf classifier; a Twitter-specific tool for language identification.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.45900964736938477,No,0.0,0.0,0.0
q478,How do they define local variance?,The reciprocal of the variance of the attention distribution,"Local variance loss is the reciprocal of the variance of the attention distribution, calculated using the median value, expecting the model to focus on salient parts.
",0.34615384615384615,1.0,0.5142857142857142,0.32,1.0,0.48484848484848486,0.34615384615384615,1.0,0.5142857142857142,0.23299898819244838,0.5637585520744324,Yes,1.0,1.0,1.0
q479,how do they measure discussion quality?,"Measuring three aspects: argumentation, specificity and knowledge domain.","Comparing annotations of student talk to English Education experts' evaluations (quadratic-weighted kappa).  The best model (R-squared) included argumentation, specificity, and knowledge domain.
",0.20833333333333334,0.625,0.3125,0.17391304347826086,0.5714285714285714,0.26666666666666666,0.20833333333333334,0.625,0.3125,0.11240537083690315,0.5287158489227295,Yes,1.0,1.0,1.0
q480,what were the baselines?,"2008 Punyakanok et al. 
2009 Zhao et al. + ME 
2008 Toutanova et al. 
2010 Bjorkelund et al.  
2015 FitzGerald et al. 
2015 Zhou and Xu 
2016 Roth and Lapata 
2017 He et al. 
2017 Marcheggiani et al.
2017 Marcheggiani and Titov 
2018 Tan et al. 
2018 He et al. 
2018 Strubell et al. 
2018 Cai et al. 
2018 He et al. 
2018 Li et al. 
","Neural baselines, i-vector system, x-vector system, ANRES_Sing_R, ANRES_Sing_Z, ANRES_Seq_R, ANRES_Seq_Z, Baseline1.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0030038765516682054,0.18645545840263367,No,0.0,0.0,0.0
q481,Which soft-selection approaches are evaluated?,LSTM and BERT ,"BERT-Soft.  ATAE-LSTM.
",0.5,0.6666666666666666,0.5714285714285715,0.0,0.0,0.0,0.25,0.3333333333333333,0.28571428571428575,0.0,0.7395720481872559,Yes,0.0,0.0,0.0
q482,How big is slot filing dataset?,"Dataset has 1737 train, 497 dev and 559 test sentences.","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03416211359799978,0.20938529074192047,No,0.0,0.0,0.0
q483,How large is the dataset they generate?,4.756 million sentences,"The generated datasets were trimmed down to the size of the original dataset.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14625601470470428,No,0.0,0.0,0.0
q484,What are the weaknesses of their proposed interpretability quantification method?,can be biased by dataset used and may generate categories which are suboptimal compared to human designed categories,"The word intrusion test is expensive, requiring human effort for each evaluation, and only yields a binary decision (interpretable/not interpretable), not a measure of the degree of interpretability.
",0.06896551724137931,0.1111111111111111,0.0851063829787234,0.0,0.0,0.0,0.034482758620689655,0.05555555555555555,0.0425531914893617,0.012193311110766474,0.2252417504787445,No,0.0,0.0,0.0
q485,What advantages does their proposed method of quantifying interpretability have over the human-in-the-loop evaluation they compare to?,it is less expensive and quantifies interpretability using continuous values rather than binary evaluations,"Their proposed method is automated and provides continuous values for interpretability, whereas the word intrusion test is expensive and yields only binary decisions.
",0.30434782608695654,0.5,0.3783783783783784,0.09090909090909091,0.15384615384615385,0.1142857142857143,0.21739130434782608,0.35714285714285715,0.27027027027027023,0.04899864500847875,0.3904832601547241,Yes,0.0,1.0,1.0
q486,How was lexical diversity measured?,By computing number of unique responses and number of responses divided by the number of unique responses to that question for each of the questions,"MTLD, Distinct-1, Distinct-2, Diversity-32, Diversity-AUC, Yule's I, and vector comparison measures.
",0.0625,0.04,0.048780487804878044,0.0,0.0,0.0,0.0625,0.04,0.048780487804878044,0.014557256720013054,0.220256507396698,No,0.0,0.0,0.0
q487,Which existing models does this approach outperform?,"RNN-context, SRB, CopyNet, RNN-distract, DRGD","State-of-the-art methods, VHRED (attn), and Reranking-RL.  Also, all other models by a large margin (with a hybrid model).
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.018264728862681344,0.20376774668693542,No,0.0,0.0,0.0
q488,What human evaluation method is proposed?,comparing the summary with the text instead of the reference and labeling the candidate bad if it is incorrect or irrelevant,"Score-based human evaluation.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.29457083344459534,No,0.0,1.0,1.0
q489,What languages are represented in the dataset?,"EN, JA, ES, AR, PT, KO, TH, FR, TR, RU, IT, DE, PL, NL, EL, SV, FA, VI, FI, CS, UK, HI, DA, HU, NO, RO, SR, LV, BG, UR, TA, MR, BN, IN, KN, ET, SL, GU, CY, ZH, CKB, IS, LT, ML, SI, IW, NE, KM, MY, TL, KA, BO","Amharic, Arabic, Hindi, Somali, English, French, German, Russian, Italian, Bengali, Telegu, Mandarin, Cantonese, Indonesian, Japanese, Korean, and Vietnamese.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.00302628723515443,0.5274969935417175,No,0.0,0.0,0.0
q490,How faster is training and decoding compared to former models?,"Proposed vs best baseline:
Decoding: 8541 vs 8532 tokens/sec
Training: 8h vs 8h","Training is similar in speed to BIAF, but decoding is fastest, followed by BIAF. STACKPTR is significantly slower for both.
",0.1,0.14285714285714285,0.11764705882352941,0.0,0.0,0.0,0.05,0.07142857142857142,0.058823529411764705,0.015732934811145335,0.42376428842544556,No,0.0,0.0,0.0
q491,What datasets are used to evaluate the model?,WN18 and FB15k,"novel-50, IMDB62, AIDA-B, and other datasets (news and encyclopedia corpus).
",0.08333333333333333,0.3333333333333333,0.13333333333333333,0.0,0.0,0.0,0.08333333333333333,0.3333333333333333,0.13333333333333333,0.024455939372403628,0.13865774869918823,No,0.0,0.0,1.0
q492,What is the source of the dataset?,"Online sites tagged as fake news site by Verafiles and NUJP and news website in the Philippines, including Pilipino Star Ngayon, Abante, and Bandera","The SCI dataset is from the OA-STM corpus, derived from the 10 most published disciplines, with sentences randomly selected from articles.  The avsd dataset uses videos from the Charades dataset.
",0.03225806451612903,0.041666666666666664,0.03636363636363637,0.0,0.0,0.0,0.03225806451612903,0.041666666666666664,0.03636363636363637,0.014325480172973542,0.16835498809814453,No,0.0,0.0,0.0
q493,What were the baselines?,"Siamese neural network consisting of an embedding layer, a LSTM layer and a feed-forward layer with ReLU activations","ANRES_Sing_R, ANRES_Sing_Z, ANRES_Seq_R, ANRES_Seq_Z,  i-vector system, x-vector system.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.012414943415352929,0.23190005123615265,No,0.0,0.0,0.0
q494,What metadata is included?,"besides claim, label and claim url, it also includes a claim ID, reason, category, speaker, checker, tags, claim entities, article title, publish data and claim date","Service name, supported tasks (intents), entity attributes (slots), natural language descriptions of service, intents, and slots.
",0.125,0.07692307692307693,0.09523809523809525,0.0,0.0,0.0,0.125,0.07692307692307693,0.09523809523809525,0.015390168378161205,0.5181943774223328,No,0.0,0.0,0.0
q495,Do the images have multilingual annotations or monolingual ones?,monolingual,"The context does not state whether the images have multilingual or monolingual annotations.
",0.07692307692307693,1.0,0.14285714285714288,0.0,0.0,0.0,0.07692307692307693,1.0,0.14285714285714288,0.028398387225677894,0.4980044662952423,No,0.0,0.0,0.0
q496,How much important is the visual grounding in the learning of the multilingual representations?,performance is significantly degraded without pixel data,"The context does not explicitly state the importance of visual grounding in learning multilingual representations.  It mentions visual grounding in the context of localization, but not in the context of multilingual representation learning.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13330179452896118,No,0.0,0.0,0.0
q497,How is the generative model evaluated?,Comparing BLEU score of model with and without attention,"The generative model is evaluated by training a classifier on the generated dataset and comparing the accuracy of that classifier to the accuracy of a classifier trained on the original dataset.  A discriminative error rate is also calculated, comparing the generated hypotheses to the original hypotheses.
",0.08695652173913043,0.4444444444444444,0.14545454545454548,0.0,0.0,0.0,0.043478260869565216,0.2222222222222222,0.07272727272727274,0.009798555761147508,0.1640673279762268,No,0.0,0.0,0.0
q498,What is an example of a health-related tweet?,"The health benefits of alcohol consumption are more limited than previously thought, researchers say","The provided context lists examples of health-related topics discussed on Twitter (flu, mental health, etc.), but does not give an example of an *actual* tweet.  The answer is not found in the context.
",0.08823529411764706,0.21428571428571427,0.125,0.0,0.0,0.0,0.08823529411764706,0.21428571428571427,0.125,0.01205256842736819,0.18407146632671356,No,0.0,0.0,0.0
q499,What is the challenge for other language except English,not researched as much as English,"Morphological richness and differing prefixing/suffixing structures.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16771335899829865,No,0.0,0.0,0.0
q500,How many categories of offensive language were there?,3,"Three
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.85391765832901,Yes,1.0,1.0,1.0
q501,Which matching features do they employ?,Matching features from matching sentences from various perspectives.,"diff-feature, sim-feature, joint-feature
",0.16666666666666666,0.125,0.14285714285714288,0.0,0.0,0.0,0.16666666666666666,0.125,0.14285714285714288,0.0,0.32878419756889343,No,0.0,1.0,1.0
q502,By how much does their method outperform the multi-head attention model?,Their average improvement in Character Error Rate over the best MHA model was 0.33 percent points.,"Their method (SAW Reader (mul)) outperforms the Attention Sum Reader (AS Reader) baseline by 7.57%.
",0.125,0.11764705882352941,0.12121212121212122,0.0,0.0,0.0,0.125,0.11764705882352941,0.12121212121212122,0.021671320168371845,0.40557703375816345,No,0.0,0.0,0.0
q503,How large is the corpus they use?,449050,"9.7 million words
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.24046744406223297,No,0.0,0.0,0.0
q504,How many shared layers are in the system?,1,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13674989342689514,No,1.0,1.0,1.0
q505,How many additional task-specific layers are introduced?,2 for the ADE dataset and 3 for the CoNLL04 dataset,"Additional NER-specific layers and RE-specific BiRNN layers are introduced, but the exact number is not specified.
",0.1111111111111111,0.18181818181818182,0.13793103448275862,0.0,0.0,0.0,0.1111111111111111,0.18181818181818182,0.13793103448275862,0.0255404966647159,0.41603535413742065,No,1.0,1.0,1.0
q506,How many layers of self-attention does the model have?,"1, 4, 8, 16, 32, 64","6
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.27301695942878723,No,0.0,0.0,1.0
q507,what are the state of the art methods?,"S2VT, RGB (VGG), RGB (VGG)+Flow (AlexNet), LSTM-E (VGG), LSTM-E (C3D) and Yao et al.","RNN pre-trained using GloVe word embeddings, LSTM with dropout and ReLU activation, and a dense layer with sigmoid activation using an Adam optimizer on the binary crossentropy loss.  Also, Cavnar and Trenkle's method, implemented as  and achieving 99.8% accuracy.
",0.047619047619047616,0.11764705882352941,0.06779661016949151,0.0,0.0,0.0,0.047619047619047616,0.11764705882352941,0.06779661016949151,0.012438555540747177,0.4279593229293823,No,0.0,0.0,0.0
q508,Which four languages do they experiment with?,"German, English, Italian, Chinese","German, English, Chinese, and English.
",0.6,0.75,0.6666666666666665,0.25,0.3333333333333333,0.28571428571428575,0.6,0.75,0.6666666666666665,0.35494810560100537,0.8795689940452576,No,0.0,0.0,0.0
q509,Does DCA or GMM-based attention perform better in experiments?,About the same performance,"DCA performs better than GMM-based attention in the experiments, as visualized results show better performance and faster loss reduction.
",0.1,0.5,0.16666666666666669,0.0,0.0,0.0,0.1,0.5,0.16666666666666669,0.021597011339333432,0.30582934617996216,No,1.0,1.0,1.0
q510,What evaluation metric is used?,F1 and Weighted-F1,"Accuracy (Acc), weighted F1 score (F1-w), macro F1 score (F1-m), area under the ROC curve (AUC), equality differences (ED).
",0.14285714285714285,0.75,0.24,0.05,0.3333333333333333,0.08695652173913045,0.09523809523809523,0.5,0.16,0.00969094302009921,0.3103383481502533,No,0.0,0.0,1.0
q511,"Is any data-to-text generation model trained on this new corpus, what are the results?","Yes, Transformer based seq2seq is evaluated with average BLEU 0.519, METEOR 0.388, ROUGE 0.631 CIDEr 2.531 and SER 2.55%.","Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.004041065926152656,0.08463679999113083,No,0.0,0.0,0.0
q512,How are the potentially relevant text fragments identified?," Generate a query out of the claim and querying a search engine, rank the words by means of TF-IDF, use IBM's AlchemyAPI to identify named entities, generate queries of 510 tokens, which execute against a search engine, and collect the snippets and the URLs in the results, skipping any result that points to a domain that is considered unreliable.","A search query, derived from a question model, is used to find potentially relevant documents.  Entity mentions compatible with a question type are extracted from each document, and context is generated around each mention.  A distance measure is then used to assess the relevance of the document content to the question.
",0.3333333333333333,0.27419354838709675,0.3008849557522124,0.04,0.03278688524590164,0.036036036036036036,0.21568627450980393,0.1774193548387097,0.19469026548672566,0.02176393157854695,0.4510795474052429,No,0.0,0.0,0.0
q513,What dataset did they use?,"weibo-100k, Ontonotes, LCQMC and XNLI","novel-9, novel-50, IMDB62, Stat2015, Stat2016, CS2016, Friends, EmotionPush, SCI, Eng
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.024074859035470342,0.17986641824245453,No,0.0,0.0,0.0
q514,What is the size of the dataset?,3029,"The answer is not found in the context.  While various dataset sizes are referenced (e.g., 2,557 days, 730 days, 12,350 ads), a single, overall dataset size is not provided.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12140114605426788,No,0.0,0.0,0.0
q515,What are the 12 AV approaches which are examined?,"MOCC, OCCAV, COAV, AVeer, GLAD, DistAV, Unmasking, Caravel, GenIM, ImpGI, SPATIUM and NNCD","The answer is not found in the context.  While the text mentions 12 AV methods were applied, the names of the specific methods are not listed.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01286713095960916,0.30664488673210144,No,1.0,1.0,1.0
q516,how was annotation done?,Annotation was done with the help of annotators from Amazon Mechanical Turk on snippets of conversations,"Annotators were presented with image-sentence pairs.  They chose a label, highlighted words in the sentence, and explained their decision using at least half of the highlighted words.  Each assignment contained 10 image-sentence pairs.  Workers had to have a previous approval rate above 90%.
",0.08888888888888889,0.25,0.13114754098360656,0.0,0.0,0.0,0.08888888888888889,0.25,0.13114754098360656,0.00959641509636166,0.5526428818702698,No,0.0,0.0,0.0
q517,How do they measure correlation between the prediction and explanation quality?,They look at the performance accuracy of explanation and the prediction performance,"The text states that the quality of the explanations strongly correlates with the quality of the predictions, but does not describe the *method* used to measure this correlation.
",0.17857142857142858,0.4166666666666667,0.25,0.037037037037037035,0.09090909090909091,0.05263157894736842,0.17857142857142858,0.4166666666666667,0.25,0.015266081935888859,0.7045300602912903,No,0.0,0.0,0.0
q518,What datasets are used to evaluate the introduced method?,"They used a dataset from Taobao which contained a collection of conversation records between customers and customer service staffs. It contains over five kinds of conversations,
including chit-chat, product and discount consultation, querying delivery progress and after-sales feedback. ","SNLI dataset, and three self-compiled corpora.
",0.2857142857142857,0.05,0.0851063829787234,0.0,0.0,0.0,0.2857142857142857,0.05,0.0851063829787234,0.0009831150070693118,0.12123522907495499,No,0.0,0.0,0.0
q519,What are the results achieved from the introduced method?,"Their model resulted in values of 0.476, 0.672 and 0.893 for recall at position 1,2 and 5 respectively in 10 candidates.","The introduced method yields more robust predictions with limited training data, a more comprehensive summary consistent with the source content, improved Word Error Rate (WER), and overall superiority to state-of-the-art methods in speech recognition, with outperformance in two categories (Person and Miscellaneous).
",0.1111111111111111,0.2,0.14285714285714285,0.0,0.0,0.0,0.044444444444444446,0.08,0.05714285714285715,0.011181458201080076,0.2748832404613495,No,0.0,0.0,0.0
q520,How do they incorporate human advice?,by converting human advice to first-order logic format and use as an input to calculate gradient,"They use human evaluators to assess the balance of positive/negative sentiment, range of topics, and relevance to judges in test dialogs.  They also consult domain experts and use annotator feedback and agreement information to improve the taxonomy and annotation design.
",0.12195121951219512,0.29411764705882354,0.17241379310344826,0.025,0.0625,0.03571428571428572,0.12195121951219512,0.29411764705882354,0.17241379310344826,0.019986978024115483,0.18588028848171234,No,0.0,0.0,0.0
q521,What affective-based features are used?,"affective features provided by different emotion models such as Emolex, EmoSenticNet, Dictionary of Affect in Language, Affective Norms for English Words and Linguistics Inquiry and Word Count","DAL Activation, ANEW Dominance, Emolex Negative, Emolex Fear, LIWC Assent, LIWC Cause, LIWC Certain, LIWC Sad.
",0.0625,0.037037037037037035,0.046511627906976744,0.0,0.0,0.0,0.0625,0.037037037037037035,0.046511627906976744,0.01732810698034988,0.4638558626174927,No,0.0,1.0,1.0
q522,How big is performance improvement proposed methods are used?,"Data augmentation (es)  improved Adv es by 20% comparing to baseline 
Data augmentation (cs) improved Adv cs by 16.5% comparing to baseline
Data augmentation (cs+es) improved both Adv cs and Adv es by at least 10% comparing to baseline 
All models show improvements over adversarial sets  
","The context describes improvements ranging from nearly 50% for decision trees with 1,000 words and 18% for Naive Bayes and 12% for SVM, to 11% for a domain adaptation technique.
",0.0967741935483871,0.0625,0.0759493670886076,0.0,0.0,0.0,0.0967741935483871,0.0625,0.0759493670886076,0.008954957330387979,0.4371229410171509,No,0.0,0.0,0.0
q523,By how much does transfer learning improve performance on this task?,"In task 1 best transfer learning strategy improves F1 score by 4.4% and accuracy score by 3.3%, in task 2 best transfer learning strategy improves F1 score by 2.9% and accuracy score by 1.7%","Transfer learning achieves an average F of 75%, which is better than the in-domain Experiment 1 results.
",0.2222222222222222,0.10526315789473684,0.14285714285714285,0.058823529411764705,0.02702702702702703,0.03703703703703704,0.2222222222222222,0.10526315789473684,0.14285714285714285,0.017611100178010486,0.6925500631332397,No,0.0,0.0,0.0
q524,What baseline is used?,SVM,"Neural baselines, i-vector system, x-vector system, Majority Class, Exact Match, First Occurrence, and a LSTM baseline.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13151399791240692,No,0.0,0.0,0.0
q525,What topic clusters are identified by LDA?,"Clusters of Twitter user ids from accounts of American or German political actors, musicians, media websites or sports club","Substance use, suicidality, anxiety disorders, harassment types (verbal, physical, and visual abuse) and context (peer-to-peer, school employee or work employer, and third-parties), sexual harassment.
",0.037037037037037035,0.05263157894736842,0.043478260869565216,0.0,0.0,0.0,0.037037037037037035,0.05263157894736842,0.043478260869565216,0.013113617852675475,0.1628222018480301,No,0.0,0.0,0.0
q526,How much do they outperform previous state-of-the-art?,"On subtask 3 best proposed model has F1 score of 92.18 compared to best previous F1 score of 88.58.
On subtask 4 best proposed model has 85.9, 89.9 and 95.6 compared to best previous results of 82.9, 84.0 and 89.9 on 4-way, 3-way and binary aspect polarity.","Our model achieves improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford system, 1.41% and 1.77% over the Multigraph model, and 2.62% and 3.02% over the MIR model.
",0.2972972972972973,0.19298245614035087,0.23404255319148934,0.027777777777777776,0.017857142857142856,0.021739130434782608,0.21621621621621623,0.14035087719298245,0.1702127659574468,0.01927647157532704,0.5512213706970215,No,0.0,0.0,0.0
q527,How big is the provided treebank?,"1448 sentences more than the dataset from Bhat et al., 2017","2.4M distinct questions
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.31798478960990906,No,0.0,0.0,0.0
q528,what dataset was used?,The dataset from a joint ADAPT-Microsoft project,"SCI, novel-9, novel-50, IMDB62, Stat2015, Stat2016, CS2016, avsd, HEOT.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15042860805988312,No,0.0,0.0,0.0
q529,What are the citation intent labels in the datasets?,"Background, extends, uses, motivation, compare/contrast, and future work for the ACL-ARC dataset. Background, method, result comparison for the SciCite dataset.","Background, Method, ResultComparison
",0.6666666666666666,0.09090909090909091,0.16,0.5,0.047619047619047616,0.08695652173913042,0.6666666666666666,0.09090909090909091,0.16,0.001304311127864991,0.25538402795791626,No,0.0,1.0,1.0
q530,How is quality of annotation measured?,Annotators went through various phases to make sure their annotations did not deviate from the mean.,"Independent annotator adjudication, comparison with AMT annotations, confidence scores, expert annotator review, and inter-annotator agreement ().
",0.125,0.125,0.125,0.0,0.0,0.0,0.125,0.125,0.125,0.019582248325011244,0.511654794216156,Yes,0.0,0.0,1.0
q531,What accuracy score do they obtain?,the best performing model obtained an accuracy of 0.86,"The answer is not found in the context.  While accuracy is discussed and a formula is given, no specific accuracy score is reported.
",0.08695652173913043,0.2,0.12121212121212122,0.0,0.0,0.0,0.08695652173913043,0.2,0.12121212121212122,0.01717906001229923,0.5930761098861694,No,0.0,0.0,0.0
q532,What is the 12 class bilingual text?,"Appreciation, Satisfied, Peripheral complaint, Demanded inquiry, Corruption, Lagged response, Unresponsive, Medicine payment, Adverse behavior, Grievance ascribed and Obnoxious/irrelevant","The context does not contain a description of a 12-class bilingual text.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09278765320777893,No,0.0,0.0,0.0
q533,Which are the sequence model architectures this method can be transferred across?,The sequence model architectures which this method is transferred to are: LSTM and Transformer-based models,"Pipelined and end-to-end architectures.
",0.5,0.1875,0.2727272727272727,0.0,0.0,0.0,0.16666666666666666,0.0625,0.09090909090909091,0.014075678340715925,0.3256679177284241,No,0.0,0.0,0.0
q534, What percentage of improvement in inference speed is obtained by the proposed method over the newest state-of-the-art methods?,"Across 4 datasets, the best performing proposed model (CNN) achieved an average of 363% improvement over the state of the art method (LR-CNN)","The answer is not found in the context.
",0.25,0.08333333333333333,0.125,0.0,0.0,0.0,0.25,0.08333333333333333,0.125,0.005166669851080904,-0.022495266050100327,No,0.0,0.0,0.0
q535,What is the metric that is measures in this paper?,error rate in a minimal pair ABX discrimination task,"Premise-Hypothesis Distance, ROUGE-L, METEOR, $\Delta_s$, Accuracy (ACC), Normalized Mutual Information (NMI), BLEU, Time$_{full}$, Time$_{DP}$.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.25821101665496826,No,0.0,0.0,0.0
q536,Was evaluation metrics and criteria were used to evaluate the output of the cascaded multimodal speech translation?,BLEU scores,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01374494843184948,No,0.0,0.0,0.0
q537,What are the two decoding functions?,"a linear projection and a bijective function with continuous transformation though  affine coupling layer of (Dinh et al.,2016). ","A linear projection with a trainable weight matrix and bias term, and a decoder employing a GRU with an embedding function.
",0.3333333333333333,0.3684210526315789,0.35,0.15,0.16666666666666666,0.15789473684210525,0.2857142857142857,0.3157894736842105,0.3,0.05538696232597744,0.32998114824295044,No,0.0,1.0,1.0
q538,What are the domains covered in the dataset?,"Alarm
Bank
Bus
Calendar
Event
Flight
Home
Hotel
Media
Movie
Music
RentalCar
Restaurant
RideShare
Service
Travel
Weather","The 20 domains are listed in Table TABREF10, and 17 domains (excluding the ""Alarm"" domain) are listed in Table TABREF5.  The datasets also include domains from the New York Times.
",0.03333333333333333,0.058823529411764705,0.0425531914893617,0.0,0.0,0.0,0.03333333333333333,0.058823529411764705,0.0425531914893617,0.009964194812460634,0.14594481885433197,No,1.0,1.0,1.0
q539,How are the two different models trained?,They pre-train the models using 600000 articles as an unsupervised dataset and then fine-tune the models on small training set.,"Different models are trained with different methods, including fine-tuning with self-inference, question-generation, back-translation data, and sometimes with a quality controller and meta-learning.  Some models are initialized with rule-covered training data.  Baseline models use entity embeddings and Logistic Regression.  Other models use different features and EM algorithm.
",0.15384615384615385,0.36363636363636365,0.21621621621621623,0.0392156862745098,0.09523809523809523,0.05555555555555555,0.09615384615384616,0.22727272727272727,0.13513513513513511,0.010072487110349271,0.42894822359085083,No,0.0,0.0,0.0
q540,How long is the dataset?,"645, 600000","The answer is not found in the context.  While various durations and sizes are mentioned, there's no single ""length"" for a single dataset.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.013353534059549444,0.2037898749113083,No,0.0,0.0,0.0
q541,How big are negative effects of proposed techniques on high-resource tasks?,The negative effects were insignificant.,"The context does not contain information about negative effects on high-resource tasks.
",0.23076923076923078,0.6,0.33333333333333337,0.08333333333333333,0.25,0.125,0.23076923076923078,0.6,0.33333333333333337,0.07347053125977879,0.4898521602153778,No,0.0,0.0,0.0
q542,"Are this techniques used in training multilingual models, on what languages?",English to French and English to German,"Multilingual BERT was trained on Wikipedia data for 104 languages.  XLM was also trained for machine translation.  ELMo models were trained for Japanese, Portuguese, German, and Basque. BERT models were trained for Simplified and Traditional Chinese and German.
",0.05263157894736842,0.2857142857142857,0.08888888888888889,0.0,0.0,0.0,0.05263157894736842,0.2857142857142857,0.08888888888888889,0.009668670750672485,0.3790585398674011,Yes,0.0,0.0,0.0
q543,What metric is used to measure performance?,"Accuracy and F1 score for supervised tasks, Pearson's and Spearman's correlation for unsupervised tasks","Accuracy (Acc), weighted F1 score (F1-w), macro F1 score (F1-m), area under the ROC curve (AUC), and equality differences (ED).
",0.18181818181818182,0.25,0.2105263157894737,0.047619047619047616,0.06666666666666667,0.05555555555555555,0.18181818181818182,0.25,0.2105263157894737,0.02372094012055542,0.37588265538215637,No,0.0,0.0,1.0
q544,How do Zipf and Herdan-Heap's laws differ?,"Zipf's law describes change of word frequency rate, while Heaps-Herdan describes different word number in large texts (assumed that Hepas-Herdan is consequence of Zipf's)","The provided text states that Heaps-Herdan's law describes how vocabulary grows with text size (a power law relationship), while Zipf's law describes the rank-frequency distribution of words in a text (also a power law).  The text also suggests Heaps-Herdan's law might be a consequence of Zipf's law.
",0.35185185185185186,0.6785714285714286,0.4634146341463415,0.1509433962264151,0.2962962962962963,0.2,0.24074074074074073,0.4642857142857143,0.3170731707317073,0.05510788236970714,0.8576423525810242,Yes,1.0,1.0,1.0
q545,How are the synthetic examples generated?,"Random perturbation of Wikipedia sentences using mask-filling with BERT, backtranslation and randomly drop out","The premises and labels from the original dataset are used as input for the generative model.  The model then generates new hypotheses.
",0.09090909090909091,0.13333333333333333,0.10810810810810811,0.0,0.0,0.0,0.045454545454545456,0.06666666666666667,0.05405405405405406,0.015732934811145335,0.20239435136318207,No,0.0,0.0,0.0
q546,By how much does the new parser outperform the current state-of-the-art?,Proposed method achieves 94.5 UAS and 92.4 LAS  compared to 94.3 and 92.2 of best state-of-the -art greedy based parser. Best state-of-the art parser overall achieves 95.8 UAS and 94.6 LAS.,"The context states the new parser outperforms several other models, but does not give a single overall comparison against the current absolute state-of-the-art.
",0.3076923076923077,0.1951219512195122,0.23880597014925378,0.12,0.075,0.0923076923076923,0.2692307692307692,0.17073170731707318,0.20895522388059704,0.01439413740718659,0.6197635531425476,Yes,0.0,0.0,0.0
q547,What experimental evaluation is used?,root mean square error between the actual and the predicted price of Bitcoin for every minute,"Human evaluation on QA4IE benchmark, using Triple Accuracy and Contextual Consistency metrics.  10 runs for hyperparameter tuning/early-stopping, using 5 compared metrics, with human evaluation to identify the best choice.
",0.0967741935483871,0.1875,0.12765957446808507,0.0,0.0,0.0,0.06451612903225806,0.125,0.0851063829787234,0.013494116947566302,0.05140991881489754,No,0.0,0.0,0.0
q548,How is the architecture fault-tolerant?,By using Apache Spark which stores all executions in a lineage graph and recovers to the previous steady state from any fault,"The architecture is fault-tolerant because Spark RDDs store execution steps in a lineage graph, allowing recovery from faults like memory overload or system crashes.  Duplicate copies of RDDs in Apache Hive and the stored model state allow recovery to the previous steady state.
",0.36363636363636365,0.7272727272727273,0.4848484848484849,0.16279069767441862,0.3333333333333333,0.21875,0.29545454545454547,0.5909090909090909,0.393939393939394,0.13663447533001424,0.5467984080314636,Yes,1.0,1.0,1.0
q549,Which elements of the platform are modular?,"handling large volume incoming data, sentiment analysis on tweets and predictive online learning","The dialog system, the platform for bitcoin price prediction, Katecheo, and Torch-Struct are modular.
",0.13333333333333333,0.15384615384615385,0.14285714285714288,0.0,0.0,0.0,0.06666666666666667,0.07692307692307693,0.07142857142857144,0.0255404966647159,0.04966963455080986,No,0.0,1.0,1.0
q550,Could you tell me more about the metrics used for performance evaluation?,"BLUE utilizes different metrics for each of the tasks: Pearson correlation coefficient, F-1 scores, micro-averaging, and accuracy","The metrics used for performance evaluation include: accuracy (Acc), weighted F1 score (F1-w), macro F1 score (F1-m), area under the ROC curve (AUC), equality differences (ED) of true positive/negative and false positive/negative rates, BLEU, perplexity, ROUGE, METEOR, BPRA (Belief Per-Response Accuracy).  The F1 score combines precision and recall.  There are also task-based evaluations and manual evaluations.
",0.14516129032258066,0.47368421052631576,0.2222222222222222,0.0,0.0,0.0,0.0967741935483871,0.3157894736842105,0.14814814814814814,0.007185920544260915,0.5306190252304077,No,0.0,0.0,0.0
q551,What are the tasks that this method has shown improvements?,"bilingual dictionary induction, monolingual and cross-lingual word similarity, and cross-lingual hypernym discovery","Sentiment analysis, language identification of tweets, predictors for renal failure, political affiliation classification, and hate-speech detection.
",0.058823529411764705,0.07142857142857142,0.06451612903225808,0.0,0.0,0.0,0.058823529411764705,0.07142857142857142,0.06451612903225808,0.04019656738893367,0.1544829159975052,No,0.0,0.0,0.0
q552,Why does the model improve in monolingual spaces as well? ,because word pair similarity increases if the two words translate to similar parts of the cross-lingual embedding space,"By averaging representations from different languages, the impact of language-specific phenomena and corpus-specific biases is reduced, resulting in more ""neutral"" monolingual embeddings.
",0.125,0.15789473684210525,0.13953488372093023,0.0,0.0,0.0,0.125,0.15789473684210525,0.13953488372093023,0.01650404559570942,0.5696429014205933,No,0.0,0.0,0.0
q553,How is annotation projection done when languages have different word order?,"Word alignments are generated for parallel text, and aligned words are assumed to also share AMR node alignments.","The context states that translation divergences exist between languages, making projection sometimes difficult without loss of information.  However, it doesn't describe *how* the projection handles different word orders.  The answer is not found in the context.
",0.02702702702702703,0.05555555555555555,0.03636363636363636,0.0,0.0,0.0,0.02702702702702703,0.05555555555555555,0.03636363636363636,0.010134961585174011,0.3313150107860565,No,0.0,0.0,1.0
q554,What's the precision of the system?,"0.8320 on semantic typing, 0.7194 on entity matching","The precision of the system varies, ranging from 25-40% up to a confidence level of 0.75, with a drop to 0.35% in recall at that level.  A precision of 91% is also mentioned in the context, but is qualified as an estimate for a different aspect of the system.
",0.038461538461538464,0.2,0.06451612903225806,0.0,0.0,0.0,0.038461538461538464,0.2,0.06451612903225806,0.006149917424924613,0.31372228264808655,No,0.0,0.0,0.0
q555,Which of the two ensembles yields the best performance?,Answer with content missing: (Table 2) CONCAT ensemble,"The ensemble model for the synthesis task achieves the best performance (46.65 ROUGE-L and 44.78 BLEU-1).  The ensemble model for the language classification task (84.84% accuracy) also outperforms other models.
",0.02857142857142857,0.125,0.04651162790697674,0.0,0.0,0.0,0.02857142857142857,0.125,0.04651162790697674,0.013113617852675475,0.17632712423801422,No,0.0,0.0,0.0
q556,What is the new initialization method proposed in this paper?,They initialize their word and entity embeddings with vectors pre-trained over a large corpus of unlabeled data.,"The provided text does not describe a new initialization method.  It discusses various existing methods, but does not propose its own.
",0.09523809523809523,0.1111111111111111,0.10256410256410256,0.0,0.0,0.0,0.047619047619047616,0.05555555555555555,0.05128205128205128,0.018709718017288025,0.23599404096603394,No,0.0,0.0,0.0
q557,How was a quality control performed so that the text is noisy but the annotations are accurate?,The authors believe that the Wikilinks corpus  contains ground truth annotations while being noisy. They discard mentions that cannot have ground-truth verified by comparison with Wikipedia.,"Annotators were limited to 120 headlines per annotator, geographically restricted to the U.S., and required to pass a 95% test on 1,100/500 questions.  Annotations were excluded if made in less than 10/70 seconds (Phase 1/2).  Reliability was assessed by computing the probability of agreement with the majority.  Independent annotation of a subset of instances was followed by adjudication, and the accuracy was measured.  Instances with confidence scores  0.7 were discarded.  A subset was re-annotated by an expert, and agreement with crowdsourced workers was verified.
",0.07692307692307693,0.25925925925925924,0.11864406779661017,0.0,0.0,0.0,0.06593406593406594,0.2222222222222222,0.1016949152542373,0.005245232563907101,0.3262149393558502,No,0.0,1.0,1.0
q558,Is it a neural model? How is it trained?,"No, it is a probabilistic model trained by finding feature weights through gradient ascent","Yes.  Trained end-to-end using CTC loss.
",0.125,0.07142857142857142,0.09090909090909091,0.0,0.0,0.0,0.125,0.07142857142857142,0.09090909090909091,0.0,0.4827076494693756,No,0.0,0.0,0.0
q559,How they evaluate quality of generated output?,Through human evaluation where they are asked to evaluate the generated output on a likert scale.,"Human evaluation, automatic evaluation (summarization quality, language fluency), task-based evaluation, discriminative evaluation, and grading by experts and amateurs.
",0.15789473684210525,0.1875,0.17142857142857143,0.05555555555555555,0.06666666666666667,0.0606060606060606,0.15789473684210525,0.1875,0.17142857142857143,0.01717906001229923,0.5765293836593628,Yes,1.0,1.0,1.0
q560,What are the four forums the data comes from?,"Darkode,  Hack Forums, Blackhat and Nulled.","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04767707020457095,0.04089486971497536,No,0.0,0.0,0.0
q561,How are sentence embeddings incorporated into the speech recognition system?,BERT generates sentence embeddings that represent words in context. These sentence embeddings are merged into a single conversational-context vector that is used to calculate a gated embedding and is later combined with the output of the decoder h to provide the gated activations for the next hidden layer.,"External word and/or sentence embeddings (e.g., fastText, BERT) are used to create conversational-context representations.  A gating mechanism integrates these embeddings (along with speech embeddings) into the decoder network.
",0.6129032258064516,0.3877551020408163,0.4749999999999999,0.16666666666666666,0.10416666666666667,0.12820512820512822,0.3870967741935484,0.24489795918367346,0.3,0.03131660566257573,0.5976610779762268,Yes,1.0,1.0,1.0
q562,How different is the dataset size of source and target?,the training dataset is large while the target dataset is usually much smaller,"The answer is not found in the context.
",0.375,0.23076923076923078,0.2857142857142857,0.0,0.0,0.0,0.375,0.23076923076923078,0.2857142857142857,0.036353588668522965,0.07102110981941223,No,0.0,0.0,0.0
q563,What type of documents are supported by the annotation platform?,"Variety of formats supported (PDF, Word...), user can define content elements of document","PDFs and Microsoft Word.
",0.5,0.15384615384615385,0.23529411764705882,0.0,0.0,0.0,0.5,0.15384615384615385,0.23529411764705882,0.01063668370007461,0.562411904335022,No,1.0,1.0,1.0
q564,What are the strong baselines you have?,optimize single task with no synthetic data,"The neural baseline with ELMo contextual vectors (F1 score of 82.6) and AoA Reader are strong baselines.
",0.05555555555555555,0.14285714285714285,0.08,0.0,0.0,0.0,0.05555555555555555,0.14285714285714285,0.08,0.019146030690102515,0.0975147932767868,No,0.0,0.0,0.0
q565,What are causal attribution networks?,"networks where nodes represent causes and effects, and directed edges represent cause-effect relationships proposed by humans","A causal attribution network is a collection of text pairs reflecting cause-effect relationships proposed by humans, where the nodes represent the concepts and directed edges represent the relationships (e.g., ""virus""  ""sickness"").
",0.42424242424242425,0.8235294117647058,0.5599999999999999,0.28125,0.5625,0.375,0.2727272727272727,0.5294117647058824,0.36,0.1479253412382805,0.6725110411643982,No,1.0,1.0,1.0
q566,how did they ask if a tweet was racist?,"if it includes  negative utterances, negative generalizations and insults concerning ethnicity, nationality, religion and culture.","The context does not describe how participants were asked if a tweet was racist.  It discusses offensiveness in general, but not specific types of offenses.
",0.12,0.2,0.15,0.0,0.0,0.0,0.12,0.2,0.15,0.017574239156849022,0.4875926673412323,No,0.0,0.0,0.0
q567,How does the model compute the likelihood of executing to the correction semantic denotation?,By treating logical forms as a latent variable and training a discriminative log-linear model over logical form y given x.,"The model computes the reconstruction likelihood $p(x|y)$ using an inverse parser that recovers the utterance $x$ from its logical form $y$.  This measures how well the logical form reflects the utterance meaning.
",0.20588235294117646,0.3333333333333333,0.2545454545454545,0.09090909090909091,0.15,0.11320754716981132,0.11764705882352941,0.19047619047619047,0.14545454545454542,0.022241100841129926,0.41356053948402405,No,0.0,0.0,0.0
q568,What are state of the art methods authors compare their work with? ,"ISOT dataset: LLVM
Liar dataset: Hybrid CNN and LSTM with attention","Supervised systems learning CRF, SVM, Maximum Entropy or Averaged Perceptron models; and other state of the art methods.
",0.05555555555555555,0.09090909090909091,0.06896551724137931,0.0,0.0,0.0,0.05555555555555555,0.09090909090909091,0.06896551724137931,0.017272237992167868,0.20555344223976135,No,0.0,0.0,0.0
q569,How much improvement do they get?,Their GTRS approach got an improvement of 3.89% compared to SVM and 27.91% compared to Pawlak.,"The context provides various improvements, including:  a 50% enhancement in decision tree accuracy,  significant gains for Naive Bayes and SVM, improvements for kNN, a 2.1 point gain in number handling, a 5.4 EM improvement in multi-span questions, a 1.5 EM improvement in single-span questions, a 12.2 absolute improvement in EM and 2.3 in F1, and a 29.7 EM and 15.1 F1 improvement.
",0.056338028169014086,0.2222222222222222,0.08988764044943821,0.0,0.0,0.0,0.04225352112676056,0.16666666666666666,0.06741573033707865,0.007395108857228917,0.41655412316322327,No,0.0,0.0,0.0
q570,Which languages do they test on?,"Answer with content missing: (Applications section) We use Wikipedia articles
in five languages
(Kinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English) as well as the Na dataset of Adams
et al. (2017).
Select:
Kinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English","English, French, German, Spanish, Russian, Italian, Bengali, Hindi, Telegu, Chinese.
",0.1,0.022222222222222223,0.03636363636363636,0.0,0.0,0.0,0.1,0.022222222222222223,0.03636363636363636,0.003965935206499725,0.5958448648452759,No,0.0,0.0,1.0
q571,What limitations are mentioned?,"deciding publisher partisanship, risk annotator bias because of short description text provided to annotators","Limited slot value sets for certain attributes, limited combinations of slot values supported by real-world services, and maximum length constraints for descriptions and video/sentence content.  Syntactic agreement across object relative clauses is also identified as a potential limitation for LSTM-LMs.
",0.046511627906976744,0.14285714285714285,0.07017543859649122,0.0,0.0,0.0,0.046511627906976744,0.14285714285714285,0.07017543859649122,0.009451258377819046,0.17903953790664673,No,0.0,0.0,0.0
q572,What are the baselines?,"CNN, LSTM, BERT","Neural baselines, ANRES_Sing_R, ANRES_Sing_Z, ANRES_Seq_R, ANRES_Seq_Z, an i-vector system, and an x-vector system.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.012193311110766474,0.1776743084192276,No,0.0,0.0,1.0
q573,What semantic features help in detecting whether a piece of text is genuine or generated? of ,"No feature is given, only discussion that semantic features are use in practice and yet to be discovered how to embed that knowledge into statistical decision theory framework.","Semantic inconsistencies in different structural parts of a satirical news tweet, captured by decomposing the syntactic tree and using word vectors.
",0.19047619047619047,0.14285714285714285,0.16326530612244897,0.0,0.0,0.0,0.14285714285714285,0.10714285714285714,0.12244897959183672,0.01717684794195263,0.11820508539676666,No,0.0,0.0,0.0
q574,Is the assumption that natural language is stationary and ergodic valid?,It is not completely valid for natural languages because of diversity of language - this is called smoothing requirement.,"Manning and Schtze argue that language can be modeled as a stationary, ergodic random process, an assumption the text follows.
",0.047619047619047616,0.05555555555555555,0.05128205128205128,0.0,0.0,0.0,0.047619047619047616,0.05555555555555555,0.05128205128205128,0.019582248325011244,0.2886008024215698,No,1.0,1.0,1.0
q575,Which models do they try out?,"DocQA, SAN, QANet, ASReader, LM, Random Guess","The models mentioned are:  models, SVMs (SVM2 and SVM2-PV).
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.024455939372403628,0.2540287971496582,No,0.0,0.0,1.0
q576,What are the competing models?,"TEACHER FORCING (TF), SCHEDULED SAMPLING (SS),  SEQGAN, RANKGAN, LEAKGAN.","ESIM, 600D MIMN, 600D MIMN-memory, LSTMN, ESIM, Bilateral Multi-Perspective Matching Model, Enhanced Sequential Inference Model, Decomposable Attention Model, KIM, BERT (pretrained on Wikipedia and BookCorpus), BERT (trained with SNLI and MultiNLI).
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.023150760452861335,0.5160163640975952,No,0.0,0.0,0.0
q577,How is the input triple translated to a slot-filling task?,"The relation R(x,y) is mapped onto a question q whose answer is y","The input triple is translated to a slot-filling task by querifying the relation for each entity, treating the entity as a variable, and instantiating a question template with the relevant entities.
",0.15625,0.3333333333333333,0.2127659574468085,0.06451612903225806,0.14285714285714285,0.08888888888888889,0.125,0.26666666666666666,0.1702127659574468,0.02865096034594708,0.33128052949905396,Yes,0.0,0.0,1.0
q578,How is module that analyzes behavioral state trained?,pre-trained to identify the presence of behavior from a sequence of word using the Couples Therapy Corpus,"The behavior model is pre-trained to identify the presence of each behavior from a sequence of words using a multi-label classification scheme.  The pre-trained portion used a single layer RNN with LSTM units with dimension size 50.
",0.375,0.8333333333333334,0.5172413793103448,0.3076923076923077,0.7058823529411765,0.42857142857142866,0.375,0.8333333333333334,0.5172413793103448,0.20918707282385296,0.5794128179550171,No,0.0,0.0,1.0
q579,"Can the model add new relations to the knowledge graph, or just new entities?",The model does not add new relations to the knowledge graph.,"The model can add a fourth type of relation (COMPLEMENT edge) to the knowledge graph.
",0.5333333333333333,0.7272727272727273,0.6153846153846153,0.2857142857142857,0.4,0.3333333333333333,0.5333333333333333,0.7272727272727273,0.6153846153846153,0.23909453161355018,0.7911447286605835,No,0.0,0.0,1.0
q580,Do twitter users tend to tweet about the DOS attack when it occurs? How much data supports this assumption?,The dataset contains about 590 tweets about DDos attacks.,"Yes, the context states that legitimate users often reveal issues with network access on social media platforms, including Twitter, during a denial-of-service attack.  The paper describes collecting tweets about specific attacks from one week before the attack to the day of the attack, which implicitly assumes tweeting about the attack occurs.
",0.09433962264150944,0.5555555555555556,0.16129032258064518,0.019230769230769232,0.125,0.03333333333333334,0.09433962264150944,0.5555555555555556,0.16129032258064518,0.016771058848336243,0.5874245762825012,Yes,0.0,0.0,1.0
q581,What is the training and test data used?,"Tweets related to a Bank of America DDos attack were used as training data. The test datasets contain tweets related to attacks to Bank of America, PNC and Wells Fargo.","The provided text describes multiple training and test data sets with different splits and augmentations.  No single, unified training and test set is described.
",0.20833333333333334,0.16666666666666666,0.1851851851851852,0.0,0.0,0.0,0.125,0.1,0.11111111111111112,0.01868940725720814,0.3110208213329315,No,0.0,0.0,0.0
q582,What writing styles are present in the corpus?,"current news, historical news, free time, sports, juridical news pieces, personal adverts, editorials.","News, letters, journal, non-fiction, poems, vernacular literature, prose, fiction, essay, short stories, sports news, historical writings, novels, stories, books, news columns, and tweets.
",0.20833333333333334,0.38461538461538464,0.2702702702702703,0.043478260869565216,0.08333333333333333,0.057142857142857134,0.125,0.23076923076923078,0.16216216216216217,0.05237131831423431,0.7261043190956116,No,0.0,0.0,0.0
q583,What meta-information is being transferred?,"high-order representation of a relation, loss gradient of relation meta","Relation meta and gradient meta.
",0.6,0.2727272727272727,0.37499999999999994,0.25,0.1,0.14285714285714288,0.6,0.2727272727272727,0.37499999999999994,0.041949309054502545,0.6192680597305298,Yes,1.0,1.0,1.0
q584,What datasets are used to evaluate the approach?,"NELL-One, Wiki-One","Three self-compiled corpora, and a dataset called BIBREF31 with 3,778 training instances and 2,032 test instances.  Data from 2016 (974 days for France and 1,096 days for the UK) is also used as a test set.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.008954307276600085,0.09463553875684738,No,0.0,0.0,0.0
q585,How much is performance hurt when using too small amount of layers in encoder?,"comparing to the results from reducing the number of layers in the decoder, the BLEU score was 69.93 which is less than 1% in case of test2016 and in case of test2017 it was less by 0.2 %. In terms of TER it had higher score by 0.7 in case of test2016 and 0.1 in case of test2017. ","Reducing the $enc_{src \rightarrow mt}$ encoder block's depth (Exp. 5.2) reduces performance.
",0.2,0.04918032786885246,0.07894736842105264,0.07142857142857142,0.016666666666666666,0.02702702702702703,0.2,0.04918032786885246,0.07894736842105264,0.003976243626767296,0.34578272700309753,Yes,0.0,0.0,0.0
q586,What neural machine translation models can learn in terms of transfer learning?,Multilingual Neural Machine Translation Models,"Multilingual NMT systems can generalize to an unseen language direction (zero-shot translation).
",0.15384615384615385,0.4,0.2222222222222222,0.0,0.0,0.0,0.15384615384615385,0.4,0.2222222222222222,0.02627961710408444,0.5858657360076904,Yes,0.0,0.0,1.0
q587,How does the semi-automatic construction process work?,Automatic transcription of 5000 tokens through sequential neural models trained on the annotated part of the corpus,"The context describes a process using a layout model and an execution model to transform sentences recursively based on hand-crafted grammar rules.  It also details data generation for entailment recognition tasks using a defined artificial language and FOL.  No semi-automatic construction process is explicitly described.
",0.0851063829787234,0.23529411764705882,0.125,0.0,0.0,0.0,0.0425531914893617,0.11764705882352941,0.0625,0.007605497822189857,0.3781832456588745,No,0.0,0.0,0.0
q588,"What does ""explicitly leverages their probabilistic correlation to guide the training process of both models"" mean?","The framework jointly learns parametrized QA and QG models subject to the constraint in equation 2. In more detail, they minimize QA and QG loss functions, with a third dual loss for regularization.","The training objective minimizes loss functions for both QA and QG models while enforcing a constraint based on the probabilistic correlation between them.
",0.5217391304347826,0.36363636363636365,0.42857142857142855,0.18181818181818182,0.125,0.14814814814814814,0.2608695652173913,0.18181818181818182,0.21428571428571427,0.08103631810436425,0.6758409738540649,Yes,1.0,1.0,1.0
q589,What is WNGT 2019 shared task?,efficiency task aimed  at reducing the number of parameters while minimizing drop in performance,"The answer is not found in the context.
",0.25,0.14285714285714285,0.18181818181818182,0.0,0.0,0.0,0.25,0.14285714285714285,0.18181818181818182,0.0325306204478917,0.05154506489634514,No,0.0,0.0,0.0
q590,Was any variation in results observed based on language typology?,It is observed some variability - but not significant. Bert does not seem to gain much more syntax information than with type level information.,"No.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1914031207561493,No,0.0,0.0,0.0
q591,Can the approach be generalized to other technical domains as well? ,"There is no reason to think that this approach wouldn't also be successful for other technical domains. Technical terms are replaced with tokens, therefore so as long as there is a corresponding process for identifying and replacing technical terms in the new domain this approach could be viable.","The answer is not found in the context.
",0.375,0.061224489795918366,0.10526315789473682,0.14285714285714285,0.020833333333333332,0.03636363636363636,0.375,0.061224489795918366,0.10526315789473682,0.0010663247073194868,0.02684328518807888,No,0.0,1.0,1.0
q592,What dataset they use for evaluation?,The same 2K set from Gigaword used in BIBREF7,"Cochrane Schizophrenia Group data, CMRC-2017, MSCOCO, and Quora duplicate question dataset.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14064398407936096,No,0.0,0.0,0.0
q593,Which regions of the United States do they consider?,all regions except those that are colored black,"The Northeast, Southeast, Midwest, and West Coast.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5081115961074829,No,0.0,0.0,0.0
q594,How is performance measured?,they use ROC curves and cross-validation,"Performance is measured by success rate in selecting the best hyperparameter combination or early-stopping checkpoint (out of 10 runs), average human-annotated score, means and standard deviations of accuracy (with 5 runs and different random seeds, except for a specific split where 25 runs and median are used), and time and cost to reach a specific task-specific performance level.
",0.03278688524590164,0.2857142857142857,0.05882352941176471,0.0,0.0,0.0,0.03278688524590164,0.2857142857142857,0.05882352941176471,0.0053985482750549645,0.34397634863853455,Yes,0.0,0.0,0.0
q595,What is novel in author's approach?,"They use self-play learning , optimize the model for specific metrics, train separate models per user, use model  and response classification predictors, and filter the dataset to obtain higher quality training data.","Using recent advances in text representation learning to test a single literary theory about a novel, focusing on a single book, rather than large collections of stories.
",0.1111111111111111,0.09375,0.10169491525423728,0.0,0.0,0.0,0.1111111111111111,0.09375,0.10169491525423728,0.015199466663267451,0.11997774243354797,No,0.0,0.0,0.0
q596,How large is the Dialog State Tracking Dataset?,"1,618 training dialogs, 500 validation dialogs, and 1,117 test dialogs","Over 16000 dialogues in the training set.
",0.14285714285714285,0.08333333333333333,0.10526315789473685,0.0,0.0,0.0,0.14285714285714285,0.08333333333333333,0.10526315789473685,0.033495035708457806,0.6161818504333496,No,0.0,1.0,1.0
q597,What dataset is used for train/test of this method?,Training datasets: TTS System dataset and embedding selection dataset. Evaluation datasets: Common Prosody Errors dataset and LFR dataset.,"UXTD, UXSSD, UPX, and BIBREF31.
",0.2,0.05555555555555555,0.08695652173913045,0.0,0.0,0.0,0.2,0.05555555555555555,0.08695652173913045,0.013373737883952701,0.34100380539894104,No,0.0,0.0,0.0
q598,How much is the gap between using the proposed objective and using only cross-entropy objective?,The mixed objective improves EM by 2.5% and F1 by 2.2%,"The context states that the proposed mixed objective significantly outperforms a purely cross-entropy model, but does not quantify the gap.
",0.14285714285714285,0.23076923076923078,0.17647058823529413,0.05,0.08333333333333333,0.0625,0.14285714285714285,0.23076923076923078,0.17647058823529413,0.038229746997386335,0.4897002577781677,No,0.0,1.0,1.0
q599,How many domains of ontologies do they gather data from?,"5 domains: software, stuff, african wildlife, healthcare, datatypes","20
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10751072317361832,No,1.0,1.0,1.0
q600,what is the practical application for this paper?,Improve existing NLP methods. Improve linguistic analysis. Measure impact of word normalization tools.,"Understanding different aspects of global governance; empowering rural communities; text generation tasks (e.g., natural language generation, sentence compression); language identification of tweets; predictors for renal failure; political affiliation classifiers; hate-speech detection.
",0.030303030303030304,0.07692307692307693,0.043478260869565216,0.0,0.0,0.0,0.030303030303030304,0.07692307692307693,0.043478260869565216,0.011498052049318417,0.6043316125869751,No,0.0,0.0,1.0
q601,What's the method used here?,Two neural networks: an extractor based on an encoder (BERT) and a decoder (LSTM Pointer Network BIBREF22) and an abstractor identical to the one proposed in BIBREF8.,"A method for question answering using specific rules, falling back to generic rules, and throwing out QA pairs if no output is found.  The appropriate sentence is then selected based on the index, and the tuple is aligned with a sentence from the input passage.
",0.15555555555555556,0.25925925925925924,0.19444444444444445,0.022727272727272728,0.038461538461538464,0.028571428571428574,0.1111111111111111,0.18518518518518517,0.13888888888888887,0.020624038231695518,0.13160653412342072,No,0.0,0.0,0.0
q602,By how much does their method outperform state-of-the-art OOD detection?,"AE-HCN outperforms by 17%, AE-HCN-CNN outperforms by 20% on average","More than 17 points in Precision@K on average.
",0.3333333333333333,0.23076923076923078,0.27272727272727276,0.125,0.08333333333333333,0.1,0.3333333333333333,0.23076923076923078,0.27272727272727276,0.06916141792551135,0.2601360082626343,No,1.0,1.0,1.0
q603,What are dilated convolutions?,Similar to standard convolutional networks but instead they skip some input values effectively operating on a broader scale.,"Dilated convolutions skip some input values so that the convolution kernel is applied over a larger area than its own, operating on a larger scale without increasing the number of parameters.
",0.2903225806451613,0.5,0.3673469387755102,0.16666666666666666,0.29411764705882354,0.21276595744680848,0.2903225806451613,0.5,0.3673469387755102,0.10827962061377729,0.3768901228904724,Yes,1.0,1.0,1.0
q604,what are the three methods presented in the paper?,"Optimized TF-IDF, iterated TF-IDF, BERT re-ranking.","An active learning method for CWS, a method for evaluation without testing, and a speech recognition based solution.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.023901021968803136,0.1789337396621704,No,0.0,0.0,0.0
q605,what datasets did the authors use?,"Kaggle
Subversive Kaggle
Wikipedia
Subversive Wikipedia
Reddit
Subversive Reddit ","novel-9, novel-50, IMDB62, SCI, Stat2015, Stat2016, CS2016
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.17744415998458862,No,0.0,0.0,0.0
q606,How much performance improvements they achieve on SQuAD?,Compared to baselines SAN (Table 1) shows  improvement of 1.096% on EM and 0.689% F1. Compared to other published SQuAD results (Table 2) SAN is ranked second. ,"A 0.72 F1 score on the original SQuAD development set, and an improvement across the board in different question types (with notable improvements for ""What"" questions).  The context also mentions an 8% F1 score increase for I entities when adding additional data.
",0.13953488372093023,0.20689655172413793,0.16666666666666666,0.0,0.0,0.0,0.06976744186046512,0.10344827586206896,0.08333333333333333,0.021807265362294385,0.654832124710083,No,0.0,0.0,0.0
q607,What is the baseline?,The baseline is a multi-task architecture inspired by another paper.,"Neural baselines, built using shared training data, chosen based on cloze validation set performance, and further tuned using user query validation data with 10-fold cross-validations.  Specific examples include a neural baseline with ELMo contextual vectors, BIBREF7, i-vector and x-vector systems.  Also, majority class, exact match, first occurrence, and LSTM baselines are mentioned.
",0.03571428571428571,0.18181818181818182,0.05970149253731343,0.0,0.0,0.0,0.03571428571428571,0.18181818181818182,0.05970149253731343,0.007215075272071308,0.44924667477607727,No,0.0,0.0,1.0
q608,What is the network architecture?,"The network architecture has a multi-task Bi-Directional Recurrent Neural Network, with an unsupervised sequence labeling task and a low-dimensional embedding layer between tasks. There is a hidden layer after each successive task with skip connections to the senior supervised layers.","The architecture is based on BiDAF with an explicit reasoning module, five main modules (input, attention, reasoning, modeling, output), and multimodal input (procedural text, image question/answers).  Other architectures mentioned include a multi-view network, a DQN regression network, and a PPO agent with actor and critic networks.  RNN-based embedding models are also part of the architecture.
",0.25862068965517243,0.3488372093023256,0.297029702970297,0.05263157894736842,0.07142857142857142,0.06060606060606061,0.15517241379310345,0.20930232558139536,0.1782178217821782,0.019781629410068426,0.47513940930366516,No,0.0,0.0,0.0
q609,What does recurrent deep stacking network do?,Stacks and joins outputs of previous frames with inputs of the current frame,"Stacks and concatenates the outputs of previous frames into the input features of the current frame.
",0.6875,0.8461538461538461,0.7586206896551724,0.4666666666666667,0.5833333333333334,0.5185185185185186,0.6875,0.8461538461538461,0.7586206896551724,0.3146660996956415,0.7719307541847229,Yes,1.0,1.0,1.0
q610,What is the reward model for the reinforcement learning appraoch?,"reward 1 for successfully completing the task, with a discount by the number of turns, and reward 0 when fail","The reward can be a QA-based metric (number of overlapping words between ground truth and predicted answer), or a balanced reward maximizing the probability of finding an answer to the generated question within the input paragraph but not inside the source sentence.
",0.18604651162790697,0.4,0.25396825396825395,0.023809523809523808,0.05263157894736842,0.03278688524590164,0.13953488372093023,0.3,0.1904761904761905,0.023150760452861335,0.3715192973613739,No,0.0,0.0,0.0
q611,Does this paper propose a new task that others can try to improve performance on?,"No, there has been previous work on recognizing social norm violation.","Yes.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3446711003780365,No,0.0,1.0,1.0
q612,How big is their dataset?,"3 million webpages processed with a CCG parser for training, 220 queries for development, and 307 queries for testing","More than 0.3 million records, 0.83 million words, 230,000 word tokens, 12,350 ads, and a Web-crawled corpus of 5 x 10^10 tokens.
",0.14285714285714285,0.21052631578947367,0.1702127659574468,0.037037037037037035,0.05555555555555555,0.04444444444444444,0.10714285714285714,0.15789473684210525,0.1276595744680851,0.032359121225441656,0.4438574016094208,No,0.0,0.0,0.0
q613,What task do they evaluate on?,Fill-in-the-blank natural language questions,"Task-oriented dialogue systems, specifically a 31-categories classification task (chit-chat and task-oriented dialogue).  A conceptualization task is also mentioned.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.23203076422214508,No,0.0,0.0,0.0
q614,How many feature maps are generated for a given triple?,3 feature maps for a given tuple,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0015766919823363423,No,0.0,0.0,0.0
