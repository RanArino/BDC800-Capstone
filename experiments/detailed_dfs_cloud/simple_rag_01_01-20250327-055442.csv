qa_id,query,ground_truth,llm_answer,rouge1_p,rouge1_r,rouge1_f,rouge2_p,rouge2_r,rouge2_f,rougeL_p,rougeL_r,rougeL_f,bleu,cosine_sim,self_checker,llm_eval@1,llm_eval@3,llm_eval@5
q0,What is the seed lexicon?,a vocabulary of positive and negative predicates that helps determine the polarity score of an event,"A seed lexicon consisting of 15 positive words and 15 negative words.
",0.4166666666666667,0.3125,0.35714285714285715,0.0,0.0,0.0,0.4166666666666667,0.3125,0.35714285714285715,0.03468317761317914,0.4117070138454437,No,0.0,1.0,1.0
q1,What are the results?,"Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. 
Using a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.","Results are shown in Tables TABREF16 and TABREF23.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.8406778562015323e-08,0.04819546639919281,No,0.0,0.0,0.0
q2,How are relations used to propagate polarity?,"based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event ","The reverse of one event's polarity can be propagated to another event based on the discourse relation (Cause or Concession) between them.  If the relation is Concession, the polarities are assumed to be reversed.
",0.42857142857142855,0.7142857142857143,0.5357142857142858,0.14705882352941177,0.25,0.18518518518518517,0.22857142857142856,0.38095238095238093,0.2857142857142857,0.06369901740315533,0.6521886587142944,Yes,0.0,1.0,1.0
q3,How big is the Japanese data?,"7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus","About 100 million sentences.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3323495388031006,No,0.0,1.0,1.0
q4,How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?,3%,"The context states that the proposed method was effective when labeled data was small, but doesn't provide a quantitative comparison of improvement sizes.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12204284220933914,No,0.0,0.0,0.0
q5,How does their model learn using mostly raw data?,by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity,"The model automatically learns complex phenomena through label propagation.
",0.1111111111111111,0.07142857142857142,0.08695652173913043,0.0,0.0,0.0,0.1111111111111111,0.07142857142857142,0.08695652173913043,0.0,0.2653263211250305,Yes,0.0,0.0,0.0
q6,How big is seed lexicon used for training?,30 words,"15 positive words and 15 negative words
",0.14285714285714285,0.5,0.22222222222222224,0.0,0.0,0.0,0.14285714285714285,0.5,0.22222222222222224,0.06567274736060395,0.5033256411552429,Yes,0.0,0.0,1.0
q7,How do the various social phenomena examined manifest in different types of communities?,"Dynamic communities have substantially higher rates of monthly user retention than more stable communities. More distinctive communities exhibit moderately higher monthly retention rates than more generic communities. There is also a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community - a short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content.
","Radically contrasting engagement patterns emerge in different parts of the multi-community space,  with, for example, established members in niche communities more likely to engage with volatile content than outsiders, while the opposite is true in generic communities.
",0.39473684210526316,0.17045454545454544,0.2380952380952381,0.05405405405405406,0.022988505747126436,0.03225806451612903,0.18421052631578946,0.07954545454545454,0.11111111111111109,0.016258389259994222,0.4992992579936981,No,0.0,0.0,0.0
q8,How did the select the 300 Reddit communities for comparison?,They selected all the subreddits from January 2013 to December 2014 with at least 500 words in the vocabulary and at least 4 months of the subreddit's history. They also removed communities with the bulk of the contributions are in foreign language.,"The text states that almost 300 Reddit communities were mapped onto a typology, but does not describe the selection process.
",0.2,0.09302325581395349,0.12698412698412698,0.0,0.0,0.0,0.15,0.06976744186046512,0.09523809523809525,0.008362467544400669,0.540784478187561,No,0.0,0.0,0.0
q9,How is the clinical text structuring task defined?,CTS is extracting structural data from medical research data (unstructured). Authors define QA-CTS task that aims to discover most related text from original text.,"The QA-CTS task aims to discover the most related text from the input clinical text, unifying different specific tasks and making datasets shareable.
",0.4583333333333333,0.44,0.4489795918367347,0.30434782608695654,0.2916666666666667,0.2978723404255319,0.4583333333333333,0.44,0.4489795918367347,0.14173448105627176,0.8167037963867188,Yes,0.0,0.0,1.0
q10,"Is all text in this dataset a question, or are there unrelated sentences in between questions?",the dataset consists of pathology reports including sentences and questions and answers about tumor size and resection margins so it does include additional sentences ,"The context states that the dataset is a collection of affective events, but does not specify if all sentences are questions.  The answer is not found in the context.
",0.2413793103448276,0.2916666666666667,0.26415094339622647,0.03571428571428571,0.043478260869565216,0.0392156862745098,0.20689655172413793,0.25,0.22641509433962265,0.03053216387177772,0.46736371517181396,No,0.0,0.0,0.0
q11,How many questions are in the dataset?,"2,714 ","2,714
",1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,Yes,1.0,1.0,1.0
q12,What aspects have been compared between various language models?,"Quality measures using perplexity and recall, and performance measured using latency and energy usage. ","Model quality (e.g., lower perplexity) and performance (e.g., energy consumption, query latency).
",0.42857142857142855,0.42857142857142855,0.42857142857142855,0.15384615384615385,0.15384615384615385,0.15384615384615385,0.35714285714285715,0.35714285714285715,0.35714285714285715,0.03951744928978941,0.45016416907310486,Yes,0.0,0.0,1.0
q13,How many attention layers are there in their model?,one,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0340510718524456,No,0.0,0.0,0.0
q14,What are the three measures of bias which are reduced in experiments?,"RIPA, Neighborhood Metric, WEAT","Geometric bias mitigation, WEAT statistic, and a probabilistic approach.
",0.1111111111111111,0.25,0.15384615384615383,0.0,0.0,0.0,0.1111111111111111,0.25,0.15384615384615383,0.07495553473355841,0.22806091606616974,No,0.0,0.0,0.0
q15,How big is the dataset?,903019 references,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.17997866868972778,No,0.0,0.0,0.0
q16,How is the intensity of the PTSD established?,"Given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively, the estimated intensity  is established as mean  squared error.","LAXARY calculates possible scores for each survey question using a PTSD Linguistic dictionary created from twitter-based sentiments.  The dictionary includes words/aspects representing instantaneous perturbations.  The model provides extremely accurate measures of intensity estimation.
",0.2,0.19444444444444445,0.19718309859154928,0.0,0.0,0.0,0.08571428571428572,0.08333333333333333,0.08450704225352113,0.01270470339292822,0.44266507029533386,No,0.0,0.0,1.0
q17,how is quality measured?,Accuracy and the macro-F1 (averaged F1 over positive and negative classes) are used as a measure of quality.,"EM-score (Exact Match) and (macro-averaged) F$_1$-score.  Reliability is also measured using Cronbach's alpha.
",0.2777777777777778,0.2631578947368421,0.27027027027027023,0.0,0.0,0.0,0.2222222222222222,0.21052631578947367,0.21621621621621623,0.02224972978585352,0.679244875907898,No,0.0,0.0,0.0
q18,What is the accuracy reported by state-of-the-art methods?,"Answer with content missing: (Table 1)
Previous state-of-the art on same dataset: ResNet50 89% (6 languages), SVM-HMM 70% (4 languages)","95.4%
",0.5,0.043478260869565216,0.08,0.0,0.0,0.0,0.5,0.043478260869565216,0.08,0.0,0.27951815724372864,No,0.0,0.0,0.0
q19,How do the authors define or exemplify 'incorrect words'?,typos in spellings or ungrammatical words,"The context exemplifies ""incorrect words"" as missing or incorrectly spelled words in sentences, and sentences with poor grammar.  It also mentions tweets written in informal language that ignore sentence correctness.
",0.13333333333333333,0.6666666666666666,0.2222222222222222,0.0,0.0,0.0,0.06666666666666667,0.3333333333333333,0.1111111111111111,0.013897369362318638,0.48524826765060425,Yes,0.0,0.0,0.0
q20,By how much do they outperform other models in the sentiment in intent classification tasks?,In the sentiment classification task by 6% to 8% and in the intent classification task by 0.94% on average,"6% to 8% in the Twitter Sentiment Classification task.
",0.8888888888888888,0.4,0.5517241379310346,0.625,0.2631578947368421,0.37037037037037035,0.7777777777777778,0.35,0.48275862068965514,0.1638462975414579,0.7410264015197754,No,0.0,0.0,0.0
q21,Which experiments are perfomed?,They used BERT-based models to detect subjective language in the WNC corpus,"Experiments were performed on the WNC dataset using variations of $RoBERTa_{large}$, $ALBERT_{xxlarge.v2}$, $DistilRoBERTa$ and $BERT$, and also using weighted-average ensembling technique with these models.  Further, experiments are suggested regarding feature extraction and neural network architectures.
",0.1282051282051282,0.38461538461538464,0.19230769230769232,0.02631578947368421,0.08333333333333333,0.04,0.07692307692307693,0.23076923076923078,0.11538461538461538,0.01361200484362806,0.5648849606513977,Yes,0.0,1.0,1.0
q22,Is ROUGE their only baseline?,"No, other baseline metrics they use besides ROUGE-L are n-gram overlap, negative cross-entropy, perplexity, and BLEU.","Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.006662591348749398,0.023244662210345268,No,0.0,0.0,0.0
q23,By how much does their system outperform the lexicon-based models?,"Under the retrieval evaluation setting, their proposed model + IR2 had better MRR than NVDM by 0.3769, better MR by 4.6, and better Recall@10 by  20 . 
Under the generative evaluation setting the proposed model + IR2 had better BLEU by 0.044 , better CIDEr by 0.033, better ROUGE by 0.032, and better METEOR by 0.029","The context states that the lexicon-based models had practically no impact on prediction, while the BERT-based models performed considerably better.  However, it does not provide a specific numerical comparison.
",0.22580645161290322,0.11864406779661017,0.15555555555555556,0.0,0.0,0.0,0.1935483870967742,0.1016949152542373,0.13333333333333336,0.007360030845611022,0.4072984457015991,No,0.0,0.0,0.0
q24,How are the main international development topics that states raise identified?," They focus on exclusivity and semantic coherence measures: Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive. They select select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence.","By modeling topic prevalence in the context of structural covariates, controlling for region fixed effects and time trend, and examining the key words linked to the topics.
",0.4074074074074074,0.1864406779661017,0.2558139534883721,0.038461538461538464,0.017241379310344827,0.02380952380952381,0.18518518518518517,0.0847457627118644,0.11627906976744184,0.014681948036900561,0.5517979264259338,No,0.0,0.0,0.0
q25,Is the semantic hierarchy representation used for any task?,"Yes, Open IE","Yes.
",1.0,0.3333333333333333,0.5,0.0,0.0,0.0,1.0,0.3333333333333333,0.5,0.0,0.3891817331314087,Yes,1.0,1.0,1.0
q26,Is the model evaluated?,the English version is evaluated. The German version evaluation is in progress ,"Yes.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09660522639751434,Yes,0.0,0.0,0.0
q27,How better is accuracy of new model compared to previously reported models?,"Average accuracy of proposed model vs best prevous result:
Single-task Training: 57.57 vs 55.06
Multi-task Training: 50.17 vs 50.59","The new model outperforms previous models (TF-IDF, S2S, IR, BERT-Base, QANet) in EM-score and F$_1$-score in several experiments.  Specific improvements vary, from 0.13% to 6.39% in EM-score and 3.69% to 93.75% in F$_1$-score.
",0.022222222222222223,0.04,0.028571428571428574,0.0,0.0,0.0,0.022222222222222223,0.04,0.028571428571428574,0.006488583831481832,0.40998175740242004,No,0.0,0.0,0.0
q28,How does the active learning model work?,"Active learning methods has a learning engine (mainly used for training of classification problems) and the selection engine (which chooses samples that need to be relabeled by annotators from unlabeled data). Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively.","Active learning uses a learning engine (a classifier) and a selection engine (sampling strategy) to choose samples from unlabeled data for annotators to relabel.  These relabeled samples are then added to the training set, improving the classifier's accuracy in an iterative process.
",0.813953488372093,0.49295774647887325,0.6140350877192983,0.3333333333333333,0.2,0.25,0.5581395348837209,0.3380281690140845,0.42105263157894735,0.08063246854574997,0.894385814666748,Yes,0.0,1.0,1.0
q29,Did the annotators agreed and how much?,"For event types and participant types, there was a moderate to substantial level of agreement using the Fleiss' Kappa. For coreference chain annotation, there was average agreement of 90.5%.","Inter-annotator agreement was found to be sufficiently high.  A specific Kappa value (90.5%) is mentioned as good agreement.
",0.45,0.3,0.36000000000000004,0.05263157894736842,0.034482758620689655,0.041666666666666664,0.25,0.16666666666666666,0.2,0.029732547559268897,0.7549259662628174,Yes,0.0,1.0,1.0
q30,What datasets are used to evaluate this approach?," Kinship and Nations knowledge graphs, YAGO3-10 and WN18KGs knowledge graphs ","815 test instances (5,421 sentences).
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10898178070783615,No,0.0,0.0,0.0
q31,How was the dataset collected?,"They crawled travel information from the Web to build a database, created a multi-domain goal generator from the database, collected dialogue between workers an automatically annotated dialogue acts. ","The Chinese dataset was collected from Tencent News, a popular Chinese website for news and opinion articles.  It contains 198,112 news articles with titles, content, and user comments.  A Japanese dataset was extracted from Japanese websites using HTML layouts and linguistic patterns.  A dataset of twitter posts was used to develop PTSD Linguistic Dictionary.  A dataset of train transport connections was obtained with two intents.  An incomplete dataset was obtained through manipulation of original corpora.
",0.10526315789473684,0.27586206896551724,0.15238095238095237,0.0,0.0,0.0,0.06578947368421052,0.1724137931034483,0.09523809523809525,0.007207977807409391,0.2764446437358856,No,0.0,0.0,0.0
q32,What models other than standalone BERT is new model compared to?,Only Bert base and Bert large are compared to proposed approach.,"BERT, RoBERTa, ALBERT, and their base and large specifications.  Also, distilled BERT-based models.
",0.35714285714285715,0.45454545454545453,0.4,0.15384615384615385,0.2,0.17391304347826086,0.2857142857142857,0.36363636363636365,0.32,0.04814971807094068,0.5346024036407471,No,0.0,0.0,0.0
q33,How big is the performance difference between this method and the baseline?,"Comparing with the highest performing baseline: 1.3 points on ACE2004 dataset, 0.6 points on CWEB dataset, and 0.86 points in the average of all scores.","The context states that the two SLOR approaches outperform the best word-overlap metric baseline by 0.244 and 0.097 Pearson correlation, and 0.012 and 0.012 MSE, respectively, for SEQ2SEQ and T3.
",0.17142857142857143,0.21428571428571427,0.19047619047619047,0.029411764705882353,0.037037037037037035,0.032786885245901634,0.14285714285714285,0.17857142857142858,0.15873015873015875,0.029776653071789862,0.3915030360221863,No,0.0,0.0,0.0
q34,What approaches without reinforcement learning have been tried?,"classification, regression, neural methods","Classification-based and regression-based approaches.
",0.3333333333333333,0.5,0.4,0.0,0.0,0.0,0.3333333333333333,0.5,0.4,0.0,0.7665587663650513,Yes,0.0,0.0,0.0
q35,Which languages do they validate on?,"Ar, Bg, Ca, Cs, Da, De, En, Es, Eu, Fa, Fi, Fr, Ga, He, Hi, Hu, It, La, Lt, Lv, Nb, Nl, Nn, PL, Pt, Ro, Ru, Sl, Sv, Tr, Uk, Ur","Spanish (Es), French (Fr), Italian (It), English (En), German (De), and Russian (Ru).
",0.46153846153846156,0.1875,0.26666666666666666,0.0,0.0,0.0,0.3076923076923077,0.125,0.17777777777777778,0.007780176546215402,0.7302888035774231,No,0.0,0.0,0.0
q36,What is the baseline method for the task?,"For the emotion recognition from text they use described neural network as baseline.
For audio and face there is no baseline.","TF-IDF (Lexical, Non-Neural)
",0.2,0.047619047619047616,0.07692307692307693,0.0,0.0,0.0,0.2,0.047619047619047616,0.07692307692307693,0.0,0.2590518295764923,No,0.0,0.0,0.0
q37,what amounts of size were used on german-english?,"Training data with 159000, 80000, 40000, 20000, 10000 and 5000 sentences, and 7584 sentences for development","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08191808313131332,No,0.0,0.0,0.0
q38,How big is the dataset?,Resulting dataset was 7934 messages for train and 700 messages for test.,"198,112 news articles
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1897267997264862,No,0.0,0.0,0.0
q39,What MC abbreviate for?,machine comprehension,"The provided context does not contain an abbreviation for MC.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11777286231517792,No,0.0,0.0,0.0
q40,What are their correlation results?,High correlation results range from 0.472 to 0.936,"ROUGE-2 and ROUGE-SU4 are similar; ROUGE-SU4 is marginally better.  Precision and F1 are similar, but precision correlates better. Recall correlates poorly, and virtually not at all with the revised Kendall measure.  Pearson correlation for OR increases to 0.048.  WordSLOR and WPSLOR have the lowest MSE, with WordSLOR slightly better.  GE-LM performs better than ROUGE-L-mult and WPSLOR.  Pearson and MSE are both lower for results per compression system.
",0.05405405405405406,0.4,0.09523809523809523,0.0136986301369863,0.1111111111111111,0.024390243902439025,0.04054054054054054,0.3,0.07142857142857144,0.0060830344261010826,0.3682648539543152,No,0.0,0.0,0.0
q41,What simpler models do they look at?,"BiGRUs with attention, ROUGE, Language model, and next sentence prediction ","Distilled BERT-based models, smaller general-purpose language representation models pre-trained by leveraging distillation knowledge.
",0.125,0.2,0.15384615384615385,0.0,0.0,0.0,0.125,0.2,0.15384615384615385,0.02627961710408444,0.2911052703857422,No,0.0,0.0,0.0
q42,What linguistic quality aspects are addressed?,"Grammaticality, non-redundancy, referential clarity, focus, structure & coherence","Fluency, and five criteria given in Figure FIGREF2.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03435488317233919,0.43062517046928406,No,0.0,1.0,1.0
q43,What dataset do they use?,A parallel corpus where the source is an English expression of code and the target is Python code.,"AIDA-Train, Wikipedia, FigureEight, ISEAR, Twitter Emotion Corpus, MCTest, CNN/Daily Mail, CBT, PubMed.
",0.07142857142857142,0.05555555555555555,0.0625,0.0,0.0,0.0,0.07142857142857142,0.05555555555555555,0.0625,0.016466642419110007,0.1829909384250641,No,0.0,0.0,0.0
q44,What is typical GAN architecture for each text-to-image synhesis group?,"Semantic Enhancement GANs: DC-GANs, MC-GAN
Resolution Enhancement GANs: StackGANs, AttnGAN, HDGAN
Diversity Enhancement GANs: AC-GAN, TAC-GAN etc.
Motion Enhancement GAGs: T2S, T2V, StoryGAN","The provided text describes *types* of GAN-based text-to-image synthesis frameworks, but does not detail *typical architectures* for each group.  The text mentions *designs and architectures* such as using multiple discriminators, progressively trained discriminators, and hierarchical discriminators, but does not specify architectures for each framework type.
",0.020833333333333332,0.037037037037037035,0.02666666666666667,0.0,0.0,0.0,0.020833333333333332,0.037037037037037035,0.02666666666666667,0.009527800929822878,0.5133587718009949,No,0.0,0.0,0.0
q45,What language do the agents talk in?,English,"Natural language.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4631931781768799,No,0.0,0.0,0.0
q46,How much better is performance of proposed method than state-of-the-art methods in experiments?,"Accuracy of best proposed method KANE (LSTM+Concatenation) are 0.8011, 0.8592, 0.8605 compared to best state-of-the art method R-GCN + LR 0.7721, 0.8193, 0.8229 on three datasets respectively.","The context shows the proposed method outperforming Coppersmith et al.'s method in some experiments, and the three fine-tuned BERT versions in others.  It also states that multi-task versions of the proposed method often perform better than single-task versions.  However, a precise comparison to *all* state-of-the-art methods is not provided.
",0.17857142857142858,0.2777777777777778,0.21739130434782608,0.09090909090909091,0.14285714285714285,0.1111111111111111,0.16071428571428573,0.25,0.1956521739130435,0.018231598123120717,0.3751208186149597,No,0.0,0.0,0.0
q47,What baseline model is used?,Human evaluators,"TF-IDF
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05249981954693794,No,0.0,0.0,0.0
q48,What stylistic features are used to detect drunk texts?,"LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors,  Spelling errors, Repeated characters, Capitalisation, Length,  Emoticon (Presence/Count ) 
 and Sentiment Ratio","Capitalization, spelling errors, etc.
",0.5,0.08695652173913043,0.14814814814814817,0.3333333333333333,0.045454545454545456,0.08,0.5,0.08695652173913043,0.14814814814814817,0.0015034597035749203,0.4036506712436676,Yes,0.0,0.0,0.0
q49,What is the accuracy of the proposed technique?,51.7 and 51.6 on 4th and 8th grade question sets with no curated knowledge. 47.5 and 48.0 on 4th and 8th grade question sets when both solvers are given the same knowledge,"95.4%
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.39789265394210815,No,0.0,0.0,0.0
q50,Which retrieval system was used for baselines?,The dataset comes with a ranked set of relevant documents. Hence the baselines do not use a retrieval system.,"TF-IDF, S2S, IR, and Proposed+IR
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12677571177482605,No,1.0,1.0,1.0
q51,How much better was the BLSTM-CNN-CRF than the BLSTM-CRF?,Best BLSTM-CNN-CRF had F1 score 86.87 vs 86.69 of best BLSTM-CRF ,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04710185527801514,No,0.0,0.0,0.0
q52,What supplemental tasks are used for multitask learning?,"Multitask learning is used for the task of predicting relevance of a comment on a different question to a given question, where the supplemental tasks are predicting relevance between the questions, and between the comment and the corresponding question","Task A, task B, and task C.
",0.5714285714285714,0.10256410256410256,0.17391304347826086,0.0,0.0,0.0,0.5714285714285714,0.10256410256410256,0.17391304347826086,0.004495955179529304,0.380179226398468,No,0.0,0.0,0.0
q53,How much performance gap between their approach and the strong handcrafted method?,"0.007 MAP on Task A, 0.032 MAP on Task B, 0.055 MAP on Task C","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18740521371364594,No,0.0,0.0,0.0
q54,How big is their model?,Proposed model has 1.16 million parameters and 11.04 MB.,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04167251645138561,0.01991630531847477,No,0.0,0.0,0.0
q55,How many emotions do they look at?,9,"24
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.265918493270874,No,0.0,0.0,0.0
q56,What approach did previous models use for multi-span questions?,Only MTMSM specifically tried to tackle the multi-span questions. Their approach consisted of two parts: first train a dedicated categorical variable to predict the number of spans to extract and the second was to generalize the single-span head method of extracting a span,"Only one model, MTMSN, directly targeted multi-span questions.  Their approach consisted of training a dedicated categorical variable to predict the number of spans and generalizing the single-span head method using non-maximum suppression.
",0.7428571428571429,0.5777777777777777,0.65,0.6176470588235294,0.4772727272727273,0.5384615384615384,0.7428571428571429,0.5777777777777777,0.65,0.3731674377355062,0.8461543917655945,Yes,0.0,0.0,1.0
q57,What is difference in peformance between proposed model and state-of-the art on other question types?,"For single-span questions, the proposed LARGE-SQUAD improve performance of the MTMSNlarge baseline for 2.1 EM and 1.55 F1.
For number type question,  MTMSNlarge baseline  have improvement over LARGE-SQUAD for 3,11  EM and  2,98 F1. 
For date question,  LARGE-SQUAD have improvements in 2,02 EM but MTMSNlarge have improvement of 4,39 F1.","The proposed model achieves the best performance and better accuracy in discriminating plausible comments from correct comments.
",0.35294117647058826,0.1,0.15584415584415584,0.0625,0.01694915254237288,0.02666666666666667,0.29411764705882354,0.08333333333333333,0.12987012987012986,0.004070668587248388,0.27064964175224304,No,0.0,0.0,0.0
q58,What is the performance of proposed model on entire DROP dataset?,"The proposed model achieves  EM 77,63 and F1 80,73  on the test and EM  76,95 and  F1 80,25 on the dev","The proposed model slightly eclipses the current state-of-the-art results on the entire DROP dataset.
",0.35294117647058826,0.24,0.28571428571428564,0.1875,0.125,0.15,0.35294117647058826,0.24,0.28571428571428564,0.08629921818897687,0.44843533635139465,No,0.0,0.0,0.0
q59,How accurate is the aspect based sentiment classifier trained only using the XR loss?,"BiLSTM-XR-Dev Estimation accuracy is 83.31 for SemEval-15 and 87.68 for SemEval-16.
BiLSTM-XR accuracy is 83.31 for SemEval-15 and 88.12 for SemEval-16.
","The context states that training a BiLSTM with XR surpasses pre-trained BERT models, and using XR training on top of pre-trained BERT models further increases results.  However, it doesn't state the *absolute* accuracy of an aspect-based classifier trained only using XR.
",0.1111111111111111,0.15625,0.12987012987012989,0.0,0.0,0.0,0.1111111111111111,0.15625,0.12987012987012989,0.010992330671486831,0.5192539095878601,No,0.0,1.0,1.0
q60,What were the non-neural baselines used for the task?,The Lemming model in BIBREF17,"""first $n$"" baseline
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11984112858772278,No,0.0,0.0,0.0
q61,What are the models evaluated on?,They evaluate F1 score and agent's test performance on their own built interactive datasets (iSQuAD and iNewsQA),"200 unique comments, ranked by TF-IDF, S2S, IR, and Proposed+IR models.
",0.07692307692307693,0.05555555555555555,0.06451612903225808,0.0,0.0,0.0,0.07692307692307693,0.05555555555555555,0.06451612903225808,0.02031628835361819,0.24561698734760284,No,0.0,0.0,0.0
q62,What is the results of multimodal compared to unimodal models?,"Unimodal LSTM vs Best Multimodal (FCM)
- F score: 0.703 vs 0.704
- AUC: 0.732 vs 0.734 
- Mean Accuracy: 68.3 vs 68.4 ","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.014565104618668556,No,0.0,0.0,0.0
q63,What were their performance results?,"On SearchSnippets dataset ACC 77.01%, NMI 62.94%, on StackOverflow dataset ACC 51.14%, NMI 49.08%, on Biomedical dataset ACC 43.00%, NMI 38.18%","Recall rate, F1 score, and Overall accuracy were used as metrics.  A model trained on the AMMER corpus achieved an F1 score of 57%.  Results on the DROP's development set showed substantial improvement across all metrics compared to base models.  A base model surpassed the NABERT+ baseline in every metric.
",0.0392156862745098,0.07407407407407407,0.05128205128205129,0.0,0.0,0.0,0.0392156862745098,0.07407407407407407,0.05128205128205129,0.009359100614362485,0.3699129819869995,No,0.0,0.0,0.0
q64,By how much did they outperform the other methods?,"on SearchSnippets dataset by 6.72% in ACC, by 6.94% in NMI; on Biomedical dataset by 5.77% in ACC, 3.91% in NMI","The context states that the fine-tuned BERT versions ""clearly outperform all other methods,"" but does not quantify the difference.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.013249308450474928,0.36634379625320435,No,0.0,0.0,0.0
q65,What is the state of the art?,"POS and DP task: CONLL 2018
NER task: (no extensive work) Strong baselines CRF and BiLSTM-CRF
NLI task: mBERT or XLM (not clear from text)","State-of-the-art systems for many other languages have been significantly boosted by large-scale reading comprehension datasets like CNN/DailyMail and Squad.  DNN methods are also achieving great results in NER tasks due to increased data.
",0.07894736842105263,0.11538461538461539,0.09374999999999999,0.02702702702702703,0.04,0.03225806451612903,0.07894736842105263,0.11538461538461539,0.09374999999999999,0.01184949136625139,0.3788510859012604,No,0.0,0.0,0.0
q66,"What difficulties does sentiment analysis on Twitter have, compared to sentiment analysis in other domains?","Tweets noisy nature, use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, short (length limited) text","Initially, a lack of suitable datasets and lexicons for system training, development, and testing hindered research.  Some Twitter-specific resources were small and proprietary.
",0.20833333333333334,0.20833333333333334,0.20833333333333334,0.0,0.0,0.0,0.20833333333333334,0.20833333333333334,0.20833333333333334,0.031666579591975455,0.5244781374931335,No,0.0,0.0,0.0
q67,How many sentence transformations on average are available per unique sentence in dataset?,27.41 transformation on average of single seed sentence is available in dataset.,"15
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09500506520271301,No,0.0,0.0,1.0
q68,What annotations are available in the dataset?,"For each source sentence, transformation sentences that are transformed according to some criteria (paraphrase, minimal change etc.)","Participant labels, BIO format (Inside, Outside, Beginning) labels for NP heads, user states, system states, and dialogue acts.
",0.05555555555555555,0.058823529411764705,0.05714285714285714,0.0,0.0,0.0,0.05555555555555555,0.058823529411764705,0.05714285714285714,0.020752738657056444,0.1982969492673874,No,0.0,0.0,0.0
q69,"How are possible sentence transformations represented in dataset, as new sentences?","Yes, as new sentences.","The dataset contains multiple variations of each sentence, assigned to several annotators.  These variations represent possible transformations.
",0.058823529411764705,0.25,0.09523809523809523,0.0,0.0,0.0,0.058823529411764705,0.25,0.09523809523809523,0.02276859592073037,0.23477445542812347,Yes,0.0,0.0,0.0
q70,What are all 15 types of modifications ilustrated in the dataset?,"- paraphrase 1
- paraphrase 2
- different meaning
- opposite meaning
- nonsense
- minimal change
- generalization
- gossip
- formal sentence
- non-standard sentence
- simple sentence
- possibility
- ban
- future
- past","The answer is not found in the context.  While the text discusses *types* of modifications, it does not list 15 specific types.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2204117327928543,No,0.0,0.0,0.0
q71,What were their results on the three datasets?,"accuracy of 86.63 on STS, 85.14 on Sanders and 80.9 on HCR","The context provides results for various models on different datasets, but not a single set of results for three specific datasets.  It describes results on TEC, AMMER, Figure8, ISEAR, and DROP.
",0.12903225806451613,0.26666666666666666,0.17391304347826086,0.0,0.0,0.0,0.0967741935483871,0.2,0.13043478260869568,0.014491339833989647,0.196230947971344,No,0.0,0.0,0.0
q72,What semantic rules are proposed?,rules that compute polarity of words after POS tagging or parsing steps,"Ten semantic rules are proposed, based on the presentation of BIBREF16.  Five are used in the proposed method; the other five are used to compute polarity.
",0.15384615384615385,0.3333333333333333,0.21052631578947367,0.04,0.09090909090909091,0.05555555555555555,0.11538461538461539,0.25,0.15789473684210525,0.029527878083787772,0.6804519295692444,No,1.0,1.0,1.0
q73,What is the performance of the model?,"Experiment 1: ACC around 0.5 with 50% noise rate in worst case - clearly higher than baselines for all noise rates
Experiment 2: ACC on real noisy datasets: 0.7 on Movie, 0.79 on Laptop, 0.86 on Restaurant (clearly higher than baselines in almost all cases)","The model trained on the AMMER corpus achieves an F1 score of 57%.  Other models show substantial improvement on DROP's development set.
",0.08695652173913043,0.041666666666666664,0.056338028169014086,0.0,0.0,0.0,0.08695652173913043,0.041666666666666664,0.056338028169014086,0.0064677028505321,0.2476632297039032,No,0.0,0.0,0.0
q74,What was the performance of both approaches on their dataset?,ERR of 19.05 with i-vectors and 15.52 with x-vectors,"The context states that a single system performed better than a feature-rich system on Task A with sufficient training data.  On Task B, the performance is described as ""limited"" without specifics.  The performance of the other approaches mentioned (TRADE, etc.) is also described in terms of specific metrics (e.g., joint state accuracy), but not as overall performance on their datasets.
",0.03225806451612903,0.15384615384615385,0.05333333333333333,0.0,0.0,0.0,0.016129032258064516,0.07692307692307693,0.026666666666666665,0.0055694611682389795,0.057807933539152145,No,0.0,0.0,0.0
q75,What genres are covered?,"genre, entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement","mechanics, thermodynamics, poetry, art, Shakespeare, dance, literature, novel, symphony, drama, orchestra, music, ballet, arts, creative, sculpture, coming-of-age, strong-female-lead, LGBTQ+.
",0.043478260869565216,0.07142857142857142,0.05405405405405405,0.0,0.0,0.0,0.043478260869565216,0.07142857142857142,0.05405405405405405,0.04858587899082647,0.6411899328231812,No,0.0,0.0,0.0
q76,Which of the two speech recognition models works better overall on CN-Celeb?,x-vector,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10085505992174149,No,0.0,0.0,0.0
q77,By how much is performance on CN-Celeb inferior to performance on VoxCeleb?,"For i-vector system, performances are 11.75% inferior to voxceleb. For x-vector system, performances are 10.74% inferior to voxceleb","14.24% (CN-Celeb) vs. 11.34% (VoxCeleb(L) on SITW)
",0.18181818181818182,0.09090909090909091,0.12121212121212123,0.0,0.0,0.0,0.18181818181818182,0.09090909090909091,0.12121212121212123,0.021147443100385906,0.4844631254673004,No,0.0,0.0,0.0
q78,How do the authors measure performance?,Accuracy across six datasets,"The authors use EM-score, F₁-score, F₁ score (for question answering), Spearman's ρ, Kendall's τ, and Pearson's r.  They also use metrics for search and reading accuracy, WPSLOR and ROUGE.
",0.03125,0.25,0.05555555555555555,0.0,0.0,0.0,0.03125,0.25,0.05555555555555555,0.0,0.2523023188114166,No,0.0,0.0,0.0
q79,What is the latest paper covered by this survey?,Kim et al. (2019),"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04767707020457095,0.22909773886203766,No,0.0,0.0,0.0
q80,What learning paradigms do they cover in this survey?,"Considering ""What"" and ""How"" separately versus jointly optimizing for both.","(1) in-domain, (2) out-of-domain, and (3) transfer learning.
",0.09090909090909091,0.1,0.09523809523809525,0.0,0.0,0.0,0.09090909090909091,0.1,0.09523809523809525,0.02719665272174911,0.15150688588619232,No,0.0,0.0,0.0
q81,What are all the input modalities considered in prior work in question generation?,"Textual inputs, knowledge bases, and images.","Sentence or paragraph, image, and table.
",0.3333333333333333,0.3333333333333333,0.3333333333333333,0.0,0.0,0.0,0.16666666666666666,0.16666666666666666,0.16666666666666666,0.11339582221952005,0.4519147276878357,No,0.0,0.0,0.0
q82,How do this framework facilitate demographic inference from social media?,Demographic information is predicted using weighted lexicon of terms.,"The framework leverages weighted lexicons of terms (e.g., BIBREF73), profile descriptions (with rules and patterns), homophily interaction information, and content to predict age and gender.  It also uses visual features, emotional features, and domain-specific features.
",0.1891891891891892,0.7777777777777778,0.30434782608695654,0.08333333333333333,0.375,0.13636363636363638,0.10810810810810811,0.4444444444444444,0.17391304347826086,0.01833765126963985,0.6402227878570557,Yes,1.0,1.0,1.0
q83,How is the data annotated?,The data are self-reported by Twitter users and then verified by two human experts.,"The data is annotated using a final annotation schema, an iterative process, and scenario-specific annotation templates.  NP heads are labeled with participant labels.  Dialogue acts and states are annotated using rules and by human annotators.  In-house experts annotate narrative content and passages are accepted if the label matches any MTurk annotator.  The BIO format (Inside, Outside, Beginning) is used at a high level (CRF).
",0.10606060606060606,0.4666666666666667,0.1728395061728395,0.015384615384615385,0.07142857142857142,0.02531645569620253,0.10606060606060606,0.4666666666666667,0.1728395061728395,0.01324648569600232,0.28322333097457886,No,0.0,0.0,0.0
q84,Where does the information on individual-level demographics come from?,From Twitter profile descriptions of the users.,"Electronic health records, posted images (aesthetic features), profile pictures, screen names, language features from textual content, and external resources (personal blogs).
",0.09523809523809523,0.2857142857142857,0.14285714285714285,0.0,0.0,0.0,0.047619047619047616,0.14285714285714285,0.07142857142857142,0.014262733286728255,0.2757856845855713,No,0.0,0.0,0.0
q85,What is the source of the user interaction data? ,Sociability from ego-network on Twitter,"Reddit subreddits from January 2013 to December 2014.
",0.125,0.16666666666666666,0.14285714285714288,0.0,0.0,0.0,0.125,0.16666666666666666,0.14285714285714288,0.04767707020457095,0.08311280608177185,No,0.0,0.0,0.0
q86,What is the source of the textual data? ,Users' tweets,"Biomedical dataset and InScript corpus.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.054485294967889786,No,0.0,0.0,0.0
q87,What is the source of the visual data? ,Profile pictures from the Twitter users' profiles.,"Image data captured from photo-ready devices (like smartphones) and online social media services.
",0.07142857142857142,0.14285714285714285,0.09523809523809523,0.0,0.0,0.0,0.07142857142857142,0.14285714285714285,0.09523809523809523,0.02908317710573758,0.5360140204429626,Yes,0.0,0.0,0.0
q88,What result from experiments suggest that natural language based agents are more robust?,Average reward across 5 seeds show that NLP representations are robust to changes in the environment as well task-nuisances,"The context states that experiments were conducted under similar conditions for all representation methods, but does not state that natural language representations were more robust.
",0.16,0.2,0.17777777777777778,0.0,0.0,0.0,0.12,0.15,0.1333333333333333,0.018264728862681344,0.4288210868835449,No,0.0,0.0,0.0
q89,Which datasets are used in the paper?,"Google N-grams
COHA
Moral Foundations Dictionary (MFD)
","Biomedical, StackOverflow, CNN/Daily Mail
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12889617681503296,No,0.0,0.0,0.0
q90,How do they quantify moral relevance?,By complementing morally relevant seed words with a set of morally irrelevant seed words based on the notion of valence,"They compute diachronic moral relevance scores as $p(c_1\,|\,\mathbf {q})$ for each word $\mathbf{q}$ across 20 decades.  A linear regression of these scores on time ($T$) is used to measure change.  Words with average relevance scores below 0.5 are removed.
",0.18181818181818182,0.4,0.25000000000000006,0.046511627906976744,0.10526315789473684,0.06451612903225806,0.13636363636363635,0.3,0.18749999999999997,0.006998018210509476,0.4055580496788025,No,0.0,0.0,0.0
q91,How much is proposed model better in perplexity and BLEU score than typical UMT models?,"Perplexity of the best model is 65.58 compared to best baseline 105.79.
Bleu of the best model is 6.57 compared to best baseline 5.50.","The context shows improvements in perplexity and BLEU score for the proposed model compared to other models (e.g., SC-LSTM), but does not quantify the difference against ""typical UMT models.""  The answer is not found in the context.
",0.23076923076923078,0.32142857142857145,0.2686567164179105,0.02631578947368421,0.037037037037037035,0.03076923076923077,0.20512820512820512,0.2857142857142857,0.23880597014925373,0.021511596337479868,0.6767594814300537,No,0.0,0.0,0.0
q92,Do they train a different training method except from scheduled sampling?,"Answer with content missing: (list missing) 
Scheduled sampling: In our experiments, we found that models trained with scheduled sampling performed better (about 0.004 BLEU-4 on validation set) than the ones trained using teacher-forcing for the AVSD dataset. Hence, we use scheduled sampling for all the results we report in this paper.

Yes.","Not found in the context.
",0.6,0.05454545454545454,0.09999999999999999,0.0,0.0,0.0,0.4,0.03636363636363636,0.06666666666666667,8.592050106759325e-06,0.10047426074743271,No,0.0,0.0,0.0
q93,How do they define upward and downward reasoning?,"Upward reasoning is defined as going from one specific concept to a more general one. Downward reasoning is defined as the opposite, going from a general concept to one that is more specific.","The text describes upward and downward inferences based on the polarity of expressions (e.g., ""no,"" ""a few"") in premises and conclusions.  It does not explicitly define them.
",0.17857142857142858,0.15151515151515152,0.16393442622950818,0.0,0.0,0.0,0.14285714285714285,0.12121212121212122,0.13114754098360656,0.013737889784665155,0.5273131132125854,No,0.0,0.0,0.0
q94,Do they annotate their own dataset or use an existing one?,Use an existing one,"The context describes annotating a dataset.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.06202657148241997,No,0.0,0.0,0.0
q95,What kind of neural network architecture do they use?,CNN,"Neural regressor (NNR) and neural classifier (NNC) with an explicit reasoning module.  A two-layer BiLSTM architecture is also mentioned.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.17331288754940033,No,1.0,1.0,1.0
q96,What web and user-generated NER datasets are used for the analysis?,"MUC, CoNLL, ACE, OntoNotes, MSM, Ritter, UMBC","UMBC corpus for Twitter NER, and a further Twitter NER corpus created by BIBREF21.  WEB and TWI are also mentioned as datasets.
",0.045454545454545456,0.14285714285714285,0.06896551724137931,0.0,0.0,0.0,0.045454545454545456,0.14285714285714285,0.06896551724137931,0.017911710595643587,0.3247252106666565,No,0.0,1.0,1.0
q97,Which unlabeled data do they pretrain with?,1000 hours of WSJ audio data,"The context does not specify the unlabeled data used for pre-training.
",0.08333333333333333,0.16666666666666666,0.1111111111111111,0.0,0.0,0.0,0.08333333333333333,0.16666666666666666,0.1111111111111111,0.033864985683445356,0.08530040085315704,No,0.0,0.0,0.0
q98,How many convolutional layers does their model have?,wav2vec has 12 convolutional layers,"Two
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1546301394701004,No,0.0,0.0,0.0
q99,How big are the datasets?,"In-house dataset consists of  3716 documents 
ACE05 dataset consists of  1635 documents","000/1,610 pieces of news, respectively.  Vocabulary size of 1,858,452.
",0.15384615384615385,0.15384615384615385,0.15384615384615385,0.0,0.0,0.0,0.15384615384615385,0.15384615384615385,0.15384615384615385,0.03377156414337854,0.3096472918987274,No,0.0,0.0,0.0
q100,What baselines did they compare their model with?,the baseline where path generation uses a standard sequence-to-sequence model augmented with attention mechanism and path verification uses depth-first search,"Baseline, Ablation models, content, content-network, and image-based models, NABERT+ baseline
",0.25,0.13043478260869565,0.1714285714285714,0.0,0.0,0.0,0.25,0.13043478260869565,0.1714285714285714,0.02265000110415278,0.43913811445236206,No,0.0,0.0,0.0
q101,What was the performance of their model?,"For test-repeated set, EM score of 61.17, F1 of 93.54, ED of 0.75 and GM of 61.36. For test-new set, EM score of 41.71, F1 of 91.02, ED of 1.22 and GM of 41.81","The model trained on the AMMER corpus achieved an F1 score of 57%.
",0.23076923076923078,0.06818181818181818,0.10526315789473682,0.08333333333333333,0.023255813953488372,0.03636363636363636,0.23076923076923078,0.06818181818181818,0.10526315789473682,0.011044076998474914,0.5034802556037903,No,0.0,0.0,0.0
q102,What evaluation metrics are used?,"exact match, f1 score, edit distance and goal match","BLEU, BIBREF4, METEOR, ROUGE, ROUGE-LM, WPSLOR, F<sub>1</sub> score.
",0.08333333333333333,0.1111111111111111,0.09523809523809525,0.0,0.0,0.0,0.08333333333333333,0.1111111111111111,0.09523809523809525,0.021671320168371845,0.3033326268196106,No,0.0,0.0,0.0
q103,How were the navigation instructions collected?,using Amazon Mechanical Turk using simulated environments with topological maps,"Collected through Mechanical Turk using 100 simulated environments.
",0.625,0.5,0.5555555555555556,0.42857142857142855,0.3333333333333333,0.375,0.625,0.5,0.5555555555555556,0.1996939588188939,0.47383055090904236,Yes,1.0,1.0,1.0
q104,What language is the experiment done in?,english language,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18705984950065613,No,0.0,1.0,1.0
q105,What additional features are proposed for future work?,distinguishing between clinically positive and negative phenomena within each risk factor domain and accounting for structured data collected on the target cohort,"A deeper analysis of clinical narratives in EHRs, structural properties of online communities, topical information, identity-based measures, and analyses of more diverse communities supported by different platforms (e.g., GitHub, StackExchange, Wikipedia).  Also, additional features beyond paragraph domain scores, bag-of-words, length of stay, and number of previous admissions.
",0.09803921568627451,0.22727272727272727,0.136986301369863,0.0,0.0,0.0,0.0784313725490196,0.18181818181818182,0.10958904109589042,0.007328733472514606,0.32866349816322327,No,0.0,0.0,0.0
q106,What are their initial results on this task?,"Achieved the highest per-domain scores on Substance (F1 ≈ 0.8) and the lowest scores on Interpersonal and Mood (F1 ≈ 0.5), and show consistency in per-domain performance rankings between MLP and RBF models.","The initial results using the RNN encoder are shown in Table 2.
",0.25,0.08571428571428572,0.1276595744680851,0.0,0.0,0.0,0.25,0.08571428571428572,0.1276595744680851,0.0055019550188380294,0.17012353241443634,No,0.0,0.0,0.0
q107,How is morphology knowledge implemented in the method?,A BPE model is applied to the stem after morpheme segmentation.,"A morphological word segmentation method is proposed, generating a vocabulary with effective morpheme units and moderate subword units, improving generalization over morphologically-rich words.  The method combines morpheme segmentation and BPE.  Modified Morfessor is also used to incorporate morphology knowledge into word segmentation.
",0.16279069767441862,0.6363636363636364,0.2592592592592593,0.023809523809523808,0.1,0.038461538461538464,0.11627906976744186,0.45454545454545453,0.18518518518518515,0.02528595101106113,0.46462878584861755,Yes,0.0,0.0,1.0
q108,How is the performance on the task evaluated?,Comparison of test accuracies of neural network models on an inflection task and qualitative analysis of the errors,"Mean average precision (MAP) and F1 score; F1 score for question answering; evaluation during training and testing phases; per-turn evaluation metrics (BLEU, METEOR, ROUGE).
",0.08,0.1111111111111111,0.09302325581395349,0.0,0.0,0.0,0.04,0.05555555555555555,0.046511627906976744,0.010885011049519644,0.23058103024959564,No,0.0,0.0,0.0
q109,what datasets were used?,"IWSLT14 German-English, IWSLT14 Turkish-English, WMT14 English-German","Amazon Mechanical Turk dataset (100 simulated indoor environments, 8066 instruction/plan pairs), MCTest, CNN/Daily Mail, Children's Book Test (CBT), UltraSuite (UXTD, UXSSD, UPX), MSCOCO, Oxford-102, CUB, Oxford, COCO, and CIFAR-10.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.008165967685677476,0.2789919078350067,No,0.0,0.0,0.0
q110,How do they obtain human judgements?,Using crowdsourcing ,"The provided text describes using human judgments for evaluating selected concepts in historical time courses and predicting human judgments, but does not specify how those judgments are obtained.
",0.03571428571428571,0.5,0.06666666666666667,0.0,0.0,0.0,0.03571428571428571,0.5,0.06666666666666667,0.0,0.14016319811344147,No,0.0,0.0,0.0
q111,Does this approach perform better in the multi-domain or single-domain setting?,single-domain setting,"Better performance in multi-span questions.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07029405981302261,No,0.0,0.0,1.0
q112,How many samples did they generate for the artificial language?,"70,000","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08871352672576904,No,0.0,0.0,0.0
q113,Why does not the approach from English work on other languages?,"Because, unlike other languages, English does not mark grammatical genders","The approach, while successful for some languages, is less robust to new speakers in the test dataset, and struggles when source and target languages have differing morphological structures (e.g., suffixing vs. prefixing).  Also, annotation discrepancies and low overlap in forms between languages pose challenges.
",0.022222222222222223,0.1,0.03636363636363636,0.0,0.0,0.0,0.022222222222222223,0.1,0.03636363636363636,0.014102723265575037,0.3039693534374237,No,0.0,0.0,0.0
q114,How do they measure grammaticality?,by calculating log ratio of grammatical phrase over ungrammatical phrase,"Speakers' intuitions about sentence well-formedness, often using ""acceptability"" or ""grammaticality"" interchangeably, are used to draw conclusions about language rules.  A log ratio of grammatical phrases to ungrammatical phrases is also considered.
",0.21875,0.7,0.3333333333333333,0.16129032258064516,0.5555555555555556,0.25,0.21875,0.7,0.3333333333333333,0.06243702571093029,0.5747295022010803,Yes,0.0,0.0,0.0
q115,What is the difference in recall score between the systems?,"Between the model and Stanford, Spacy and Flair the differences are 42.91, 25.03, 69.8 with Traditional NERs as reference and  49.88, 43.36, 62.43 with Wikipedia titles as reference.","The context states that recall is consistently significantly lower than precision, and that Stanford NER and CRFSuite have almost 50% lower recall on unseen NEs compared to seen NEs.  However, a direct difference in recall scores between *all* systems is not provided.
",0.16666666666666666,0.20588235294117646,0.18421052631578944,0.0,0.0,0.0,0.11904761904761904,0.14705882352941177,0.13157894736842105,0.01165251222790752,0.44102537631988525,No,0.0,0.0,0.0
q116,What is their f1 score and recall?,"F1 score and Recall are 68.66, 80.08 with Traditional NERs as reference and 59.56, 69.76 with Wikipedia titles as reference.","The provided text gives an F1 score of 78.38% on the development set, but does not give a recall score.
",0.14285714285714285,0.125,0.13333333333333333,0.05,0.043478260869565216,0.046511627906976744,0.14285714285714285,0.125,0.13333333333333333,0.03916449665002247,0.7794774174690247,No,0.0,0.0,0.0
q117,How many layers does their system have?,4 layers,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.008996889926493168,No,0.0,0.0,0.0
q118,What context modelling methods are evaluated?,"Concat
Turn
Gate
Action Copy
Tree Copy
SQL Attn
Concat + Action Copy
Concat + Tree Copy
Concat + SQL Attn
Turn + Action Copy
Turn + Tree Copy
Turn + SQL Attn
Turn + SQL Attn + Action Copy","Different context modeling methods are evaluated.  The text mentions some representative models but doesn't explicitly list all methods.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06334307044744492,No,1.0,1.0,1.0
q119,Is the baseline a non-heirarchical model like BERT?,There were hierarchical and non-hierarchical baselines; BERT was one of those baselines,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13072708249092102,No,0.0,0.0,0.0
q120,How better are results compared to baseline models?,F1 score of best authors' model is 55.98 compared to BiLSTM and FastText that have F1 score slighlty higher than 46.61.,"The model outperforms baselines by a clear margin in new environments, with improvements in Exact Match and Goal Match metrics by 35% and 25%, respectively, in previously seen environments.  In task A, the model performs better than a strong feature-rich system with sufficient training data.  In task B, the model exhibits substantial improvement across all metrics compared to base models.
",0.08196721311475409,0.21739130434782608,0.11904761904761903,0.016666666666666666,0.045454545454545456,0.024390243902439025,0.04918032786885246,0.13043478260869565,0.07142857142857142,0.013587282474269302,0.39164280891418457,No,0.0,0.0,0.0
q121,How big is dataset used for training/testing?,"4,261  days for France and 4,748 for the UK","The context describes various datasets and their sizes for training and testing, but does not provide a single overall answer to the question of the size of the datasets used for training/testing.
",0.12121212121212122,0.36363636363636365,0.18181818181818182,0.0,0.0,0.0,0.09090909090909091,0.2727272727272727,0.13636363636363635,0.014500372328365223,0.09680502116680145,No,0.0,0.0,0.0
q122,What geometric properties do embeddings display?,Winter and summer words formed two separate clusters. Week day and week-end day words also formed separate clusters.,"Distances between word vectors, specifically cosine distance.
",0.14285714285714285,0.05263157894736842,0.07692307692307693,0.0,0.0,0.0,0.14285714285714285,0.05263157894736842,0.07692307692307693,0.014044464769840424,0.20036640763282776,No,0.0,0.0,0.0
q123,How accurate is model trained on text exclusively?,Relative error is less than 5%,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06882864236831665,No,0.0,0.0,0.0
q124,What was their result on Stance Sentiment Emotion Corpus?,F1 score of 66.66%,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.033772941678762436,No,0.0,0.0,0.0
q125,What performance did they obtain on the SemEval dataset?,F1 score of 82.10%,"Improved the existing best SemEval 2016 task 6 system by 3.2 F-score points for sentiment analysis.
",0.05555555555555555,0.2,0.08695652173913045,0.0,0.0,0.0,0.05555555555555555,0.2,0.08695652173913045,0.0,0.45013564825057983,No,0.0,0.0,0.0
q126,What are the state-of-the-art systems?,"For sentiment analysis UWB, INF-UFRGS-OPINION-MINING, LitisMind, pkudblab and SVM + n-grams + sentiment and for emotion analysis MaxEnt, SVM, LSTM, BiLSTM and CNN","Deep convolutional networks, maximum entropy, SVM, LSTM, Bi-LSTM, and CNN.
",0.36363636363636365,0.16,0.2222222222222222,0.2,0.08333333333333333,0.11764705882352941,0.36363636363636365,0.16,0.2222222222222222,0.12442453233488605,0.4871170222759247,Yes,0.0,0.0,0.0
q127,What is the size of their collected dataset?,3347 unique utterances ,"287,226 documents for training, 13,368 for validation, and 11,490 for test.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2907228469848633,No,0.0,0.0,0.0
q128,"What kind of features are used by the HMM models, and how interpretable are those?","A continuous emission HMM uses the hidden states of a 2-layer LSTM as features and a discrete emission HMM uses data as features. 
The interpretability of the model is shown in Figure 2. ","HMM models use spaces, indentation, and special characters (like comment symbols in Linux data).  The interpretability is described as ""some examples where the HMM and LSTM complement each other, such as learning different things about spaces and comments on Linux data, or punctuation on the Shakespeare data.""
",0.3191489361702128,0.4411764705882353,0.37037037037037035,0.021739130434782608,0.030303030303030304,0.02531645569620253,0.1702127659574468,0.23529411764705882,0.19753086419753088,0.03546647136740269,0.5432449579238892,No,0.0,1.0,1.0
q129,What kind of information do the HMMs learn that the LSTMs don't?,The HMM can identify punctuation or pick up on vowels.,"HMMs learn information about spaces, comments, and punctuation.
",0.25,0.2,0.22222222222222224,0.0,0.0,0.0,0.25,0.2,0.22222222222222224,0.04456882760699064,0.6856245994567871,Yes,1.0,1.0,1.0
q130,How large is the gap in performance between the HMMs and the LSTMs?,"With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower.","The hybrid algorithm (combining HMM and LSTM) performs a bit better than a standalone LSTM with the same LSTM state dimension, but this difference diminishes as the LSTM size increases.  The HMM can still make a difference.
",0.16216216216216217,0.17142857142857143,0.16666666666666669,0.0,0.0,0.0,0.16216216216216217,0.17142857142857143,0.16666666666666669,0.013058944351862712,0.4494694471359253,No,0.0,0.0,0.0
q131,what was their system's f1 performance?,"Proposed model achieves 0.86, 0.924, 0.71 F1 score on SR, HATE, HAR datasets respectively.","78.38% on the whole development set.
",0.14285714285714285,0.058823529411764705,0.08333333333333333,0.0,0.0,0.0,0.14285714285714285,0.058823529411764705,0.08333333333333333,0.016604670898042337,0.3611961305141449,No,0.0,0.0,0.0
q132,How much more coverage is in the new dataset?,278 more annotations,"Answer not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03185254707932472,No,0.0,0.0,0.0
q133,How was quality measured?,"Inter-annotator agreement, comparison against expert annotation, agreement with PropBank Data annotations.","Human evaluation, often by randomly sampling generated questions and asking annotators to rate them on a 5-point Likert scale, averaging ranks or percentages of best-ranked questions.  Expert and amateur human evaluators graded generated poems on fluency and semantic coherence.  Macro-averaging of precision, recall, and F1 scores was used for multilabel classification.
",0.037037037037037035,0.16666666666666666,0.06060606060606061,0.0,0.0,0.0,0.037037037037037035,0.16666666666666666,0.06060606060606061,0.007955340858308131,0.3126544952392578,No,0.0,0.0,0.0
q134,What is different in the improved annotation protocol?,a trained worker consolidates existing annotations ,"The improved annotation protocol includes a refined annotation schema, scenario-specific annotation templates, and single annotation mode for annotators.  Iterative refinement was necessary due to the complexity of the task.
",0.06666666666666667,0.3333333333333333,0.1111111111111111,0.0,0.0,0.0,0.06666666666666667,0.3333333333333333,0.1111111111111111,0.01123099644603982,0.4882583022117615,No,0.0,0.0,0.0
q135,What data were they used to train the multilingual encoder?,WMT 2014 En-Fr parallel corpus,"The context states the multilingual encoder was trained on data from language pair BIBREF30.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.20706374943256378,No,0.0,0.0,0.0
q136,From when are many VQA datasets collected?,late 2014,"2014–2016
",0.5,0.5,0.5,0.0,0.0,0.0,0.5,0.5,0.5,0.0,0.7098172307014465,No,1.0,1.0,1.0
q137,What is task success rate achieved? ,96-97.6% using the objects color or shape and 79% using shape alone,"Not found in the context.
",0.2,0.07142857142857142,0.10526315789473682,0.0,0.0,0.0,0.2,0.07142857142857142,0.10526315789473682,0.015330462064343475,-0.03624602407217026,No,0.0,0.0,0.0
q138,Does proposed end-to-end approach learn in reinforcement or supervised learning manner?,supervised learning,"Reinforcement learning.
",0.5,0.5,0.5,0.0,0.0,0.0,0.5,0.5,0.5,0.0,0.5885686278343201,No,0.0,0.0,1.0
q139,How is performance of this system measured?,using the BLEU score as a quantitative metric and human evaluation for quality,"Performance is measured using EM-score, macro-averaged F$_1$-score, normalized accuracy, and human evaluation (average rank or percentage of best-ranked questions).  Recall, F1 score, and overall accuracy are also used.
",0.15151515151515152,0.38461538461538464,0.21739130434782608,0.0625,0.16666666666666666,0.09090909090909091,0.15151515151515152,0.38461538461538464,0.21739130434782608,0.03693627242535955,0.4597885310649872,No,0.0,0.0,1.0
q140,How big dataset is used for training this system?,"For the question generation model 15,000 images with 75,000 questions. For the chatbot model, around 460k utterances over 230k dialogues.","The context mentions various datasets with different sizes.  No single ""training dataset"" size is given for the system as a whole.
",0.19047619047619047,0.18181818181818182,0.18604651162790697,0.05,0.047619047619047616,0.04878048780487805,0.19047619047619047,0.18181818181818182,0.18604651162790697,0.021300733682208986,0.2993650734424591,No,0.0,0.0,0.0
q141,How do they obtain word lattices from words?,By considering words as vertices and generating directed edges between neighboring words within a sentence,"For a sentence in Chinese, all possible substrings that can be considered as words are treated as vertices.  Neighboring words are connected by directed edges.  Out-of-vocabulary characters are replaced with labels to maintain graph connectivity.
",0.2972972972972973,0.7333333333333333,0.42307692307692313,0.1111111111111111,0.2857142857142857,0.16,0.16216216216216217,0.4,0.23076923076923078,0.03511091977922844,0.6649329662322998,Yes,0.0,0.0,1.0
q142,How better is proposed method than baselines perpexity wise?,Perplexity of proposed MEED model is 19.795 vs 19.913 of next best result on test set.,"Answer not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.015738541970947637,0.12303528934717178,No,0.0,0.0,0.0
q143,Who manually annotated the semantic roles for the set of learner texts?,Authors,"Two senior students.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2612665295600891,No,0.0,0.0,0.0
q144,How do they obtain region descriptions and object annotations?,they are available in the Visual Genome dataset,"The context states they use the Visual Genome dataset.
",0.5555555555555556,0.625,0.5882352941176471,0.375,0.42857142857142855,0.39999999999999997,0.5555555555555556,0.625,0.5882352941176471,0.2777619034011791,0.6331122517585754,Yes,0.0,0.0,1.0
q145,Which training dataset allowed for the best generalization to benchmark sets?,MultiNLI,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06096215546131134,No,0.0,0.0,0.0
q146,Which models were compared?,"BiLSTM-max, HBMP, ESIM, KIM, ESIM + ELMo, and BERT","""Ours with Mask and Ordered Triplets,"" ""Baseline,"" and ""Ablation"" models.  Also, ""MEED,"" and other unspecified models are mentioned in relation to perplexity scores.  Finally, various RF models, an aggregated forecaster (Agg), and a strong feature-rich system are mentioned.
",0.02564102564102564,0.1111111111111111,0.041666666666666664,0.0,0.0,0.0,0.02564102564102564,0.1111111111111111,0.041666666666666664,0.016474108593061365,0.2629825472831726,No,0.0,0.0,0.0
q147,What is private dashboard?,Private dashboard is leaderboard where competitors can see results after competition is finished - on hidden part of test set (private test set).,"Answer not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.004350967877375741,0.022878330200910568,No,0.0,0.0,0.0
q148,What is public dashboard?,"Public dashboard where competitors can see their results during competition, on part of the test set (public test set).","Answer not found in the context.
",0.16666666666666666,0.05263157894736842,0.08,0.0,0.0,0.0,0.16666666666666666,0.05263157894736842,0.08,0.007942726026489216,0.06462510675191879,No,0.0,0.0,0.0
q149,What dataset do they use?,They used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings and dataset provided in HSD task to train model (details not mentioned in paper).,"BIBREF3, FM-IQA BIBREF4, The VQA Dataset BIBREF1, Visual7W BIBREF5, Visual Genome BIBREF6, MSCOCO, Oxford-102, CUB, Oxford, COCO, CIFAR-10, StackOverflow, and Biomedical.
",0.08333333333333333,0.08,0.08163265306122448,0.0,0.0,0.0,0.041666666666666664,0.04,0.04081632653061224,0.012944532586971997,0.1501006782054901,No,0.0,0.0,0.0
q150,Do the use word embeddings alone or they replace some previous features of the model with word embeddings?,They use it as addition to previous model - they add new edge between words if word embeddings are similar.,"Word embeddings are used *in addition* to other features.
",0.6666666666666666,0.3157894736842105,0.42857142857142855,0.375,0.16666666666666666,0.23076923076923078,0.3333333333333333,0.15789473684210525,0.21428571428571427,0.04022953685398025,0.6917035579681396,Yes,0.0,0.0,0.0
q151,How many natural language explanations are human-written?,Totally 6980 validation and test image-sentence pairs have been corrected.,"100
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12353888899087906,No,0.0,0.0,0.0
q152,What is the dataset used as input to the Word2Vec algorithm?,Italian Wikipedia and Google News extraction producing final vocabulary of 618224 words,"100 billion words from Google and Twitter.  Wikipedia dumps for SearchSnippets; the whole StackOverflow dataset (including question titles and post contents) for StackOverflow; all titles and abstracts for Biomedical.
",0.13793103448275862,0.3333333333333333,0.1951219512195122,0.0,0.0,0.0,0.06896551724137931,0.16666666666666666,0.0975609756097561,0.014933695248193319,0.46399545669555664,No,0.0,0.0,0.0
q153,What methodology is used to compensate for limited labelled data?,Influential tweeters ( who they define as individuals certain to have a classifiable sentiment regarding the topic at hand) is used to label tweets in bulk in the absence of manually-labeled tweets.,"Lightly supervised learning, self training, and distantly-supervised learning.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.004631305510350447,0.09604465961456299,Yes,0.0,0.0,0.0
q154,What are the baseline state of the art models?,"Stanford NER, BiLSTM+CRF, LSTM+CNN+CRF, T-NER and BiLSTM+CNN+Co-Attention","BiMPM, ESIM, Decomposable Attention Model, KIM, and BERT.
",0.25,0.14285714285714285,0.18181818181818182,0.0,0.0,0.0,0.125,0.07142857142857142,0.09090909090909091,0.02549704254217688,0.42062318325042725,No,0.0,0.0,0.0
q155,How do they extract causality from text?,"They identify documents that contain the unigrams 'caused', 'causing', or 'causes'","Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'.  The word `cause' was not included.
",0.34782608695652173,0.7272727272727273,0.4705882352941176,0.18181818181818182,0.4,0.25000000000000006,0.34782608695652173,0.7272727272727273,0.4705882352941176,0.031731768853938885,0.7113483548164368,Yes,1.0,1.0,1.0
q156,"What is the source of the ""control"" corpus?","Randomly selected from a Twitter dump, temporally matched to causal documents","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.031560268253088,No,0.0,0.0,0.0
q157,"What are the selection criteria for ""causal statements""?","Presence of only the exact unigrams 'caused', 'causing', or 'causes'","Statements with high certainty of being causal statements.
",0.125,0.1,0.11111111111111112,0.0,0.0,0.0,0.125,0.1,0.11111111111111112,0.03416211359799978,0.37696373462677,No,0.0,0.0,0.0
q158,"Do they use expert annotations, crowdsourcing, or only automatic methods to analyze the corpora?",Only automatic methods,"Expert annotations and crowdsourcing.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10446573048830032,No,0.0,0.0,0.0
q159,how do they collect the comparable corpus?,Randomly from a Twitter dump,"A corpus of poems and a corpus of vernacular literature were collected from online resources.  The poem corpus contains 163K quatrain poems, and the vernacular literature corpus contains 337K short paragraphs from 281 famous books.
",0.05714285714285714,0.4,0.09999999999999999,0.0,0.0,0.0,0.02857142857142857,0.2,0.049999999999999996,0.011524538390587938,0.06174793839454651,No,0.0,0.0,0.0
q160,How do they collect the control corpus?,Randomly from Twitter,"The provided text does not describe how the control corpus is collected.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06355247646570206,No,0.0,0.0,0.0
q161,What are resolution model variables?,"Variables in the set {str, prec, attr} indicating in which mode the mention should be resolved.","Resolution mode variables $\Pi = \lbrace \pi _1, \ldots , \pi _n\rbrace $, where for each mention $j$ the variable $\pi _j \in \lbrace str, prec, attr\rbrace $ indicates in which mode the mention should be resolved.
",0.42857142857142855,0.9375,0.588235294117647,0.29411764705882354,0.6666666666666666,0.40816326530612246,0.4,0.875,0.5490196078431373,0.18713608144131694,0.7502115964889526,Yes,1.0,1.0,1.0
q162,Is the model presented in the paper state of the art?,"No, supervised models perform better for this task.","Yes.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4601079821586609,No,0.0,0.0,0.0
q163,What was the result of the highest performing system?,"For task 1 best F1 score was 0.9391 on closed and 0.9414 on open test.
For task2 best result had: Ratio 0.3175 , Satisfaction 64.53, Fluency 0, Turns -1 and Guide 2","Human.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.017148612067103386,No,0.0,0.0,0.0
q164,What do they mean by answer styles?,well-formed sentences vs concise answers,"Answer styles are related to the context of the question in the answer sentence, and can also control other styles such as length and speaking styles.  The system uses an artificial token corresponding to the target style at the beginning of the answer sentence.
",0.045454545454545456,0.3333333333333333,0.08,0.0,0.0,0.0,0.045454545454545456,0.3333333333333333,0.08,0.0,0.35591888427734375,No,0.0,0.0,0.0
q165,What are the baselines that Masque is compared against?,"BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D","Baseline and Ablation models.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12402989715337753,No,0.0,0.0,0.0
q166,What is the performance achieved on NarrativeQA?,"Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1005011796951294,No,0.0,0.0,0.0
q167,"What is an ""answer style""?",well-formed sentences vs concise answers,"An answer style is a style, like BIBREF14, corresponding to the target style at the beginning of the answer sentence (y1).  It is related to the context of the question in the answer sentence.  Other styles, like length and speaking styles, can also be controlled.
",0.044444444444444446,0.3333333333333333,0.0784313725490196,0.0,0.0,0.0,0.044444444444444446,0.3333333333333333,0.0784313725490196,0.0,0.31616100668907166,No,0.0,0.0,0.0
q168,How are the EAU text spans annotated?,Answer with content missing: (Data and pre-processing section) The data is suited for our experiments because the annotators were explicitly asked to provide annotations on a clausal level.,"Workers choose a label, highlight words in the hypothesis, and use at least half of the highlighted words to write an explanation for their decision.
",0.24,0.20689655172413793,0.22222222222222224,0.0,0.0,0.0,0.12,0.10344827586206896,0.11111111111111112,0.018117236860142973,0.3890286386013031,No,0.0,0.0,0.0
q169,Which Twitter corpus was used to train the word vectors?,They collected tweets in Russian language using a heuristic query specific to Russian,"1, 7, and 15 days of Twitter data.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.360611230134964,No,0.0,0.0,0.0
q170,How does proposed word embeddings compare to Sindhi fastText word representations?,"Proposed SG model vs SINDHI FASTTEXT:
Average cosine similarity score: 0.650 vs 0.388
Average semantic relatedness similarity score between countries and their capitals: 0.663 vs 0.391","The proposed Sindhi word embeddings have surpassed SdfastText in intrinsic evaluation matrices.
",0.16666666666666666,0.06666666666666667,0.09523809523809522,0.0,0.0,0.0,0.16666666666666666,0.06666666666666667,0.09523809523809522,0.0,0.460237979888916,Yes,1.0,1.0,1.0
q171,How many uniue words are in the dataset?,908456 unique words are available in collected corpus.,"618,224
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2953603267669678,No,0.0,0.0,0.0
q172,Which baseline methods are used?,standard parametrized attention and a non-attention baseline,"K-means (TF), K-means (TF-IDF), SkipVec, RecNN (Ave.), RecNN (SNLI + MultiNLI), and Paragraph Vector based clustering methods.  TF-IDF, S2S, IR, and Proposed+IR are also used as baseline models for ranking comments.
",0.05714285714285714,0.25,0.09302325581395347,0.0,0.0,0.0,0.05714285714285714,0.25,0.09302325581395347,0.008327748566971627,0.2859548032283783,No,0.0,0.0,0.0
q173,How much is the BLEU score?,Ranges from 44.22 to 100.00 depending on K and the sequence length.,"The context provides many BLEU scores, but no single ""the"" BLEU score.
",0.08333333333333333,0.07142857142857142,0.07692307692307691,0.0,0.0,0.0,0.08333333333333333,0.07142857142857142,0.07692307692307691,0.02908317710573758,0.1386643946170807,Yes,0.0,0.0,0.0
q174,Which datasets are used in experiments?,Sequence Copy Task and WMT'17,"MCTest, CNN/Daily Mail, CBT, MSCOCO, Oxford-102, MNIST, CUB, Oxford, COCO, CIFAR-10, Biomedical.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.033191196620464325,No,0.0,0.0,0.0
q175,What are new best results on standard benchmark?,"New best results of accuracy (P@1) on Vecmap:
Ours-GeoMMsemi: EN-IT 50.00 IT-EN 42.67 EN-DE 51.60 DE-EN 47.22 FI-EN 39.62 EN-ES 39.47 ES-EN 36.43","The final model “Ours with Mask and Ordered Triplets” outperforms the Baseline and Ablation models on all metrics in previously seen environments.
",0.09090909090909091,0.05128205128205128,0.06557377049180327,0.0,0.0,0.0,0.045454545454545456,0.02564102564102564,0.032786885245901634,0.01268559500048408,0.2523117661476135,No,0.0,0.0,0.0
q176,How better is performance compared to competitive baselines?,"Proposed method vs best baseline result on Vecmap (Accuracy P@1):
EN-IT: 50 vs 50
IT-EN: 42.67 vs 42.67
EN-DE: 51.6 vs 51.47
DE-EN: 47.22 vs 46.96
EN-FI: 35.88 vs 36.24
FI-EN: 39.62 vs 39.57
EN-ES: 39.47 vs 39.30
ES-EN: 36.43 vs 36.06","The final model ""Ours with Mask and Ordered Triplets"" outperforms baselines on all metrics in previously seen environments, increasing accuracy by 35% and 25% for Exact Match and Goal Match, respectively.  Other models outperform heuristic models, and the described model outperforms baselines by a clear margin in new environments.
",0.08163265306122448,0.06153846153846154,0.07017543859649124,0.0,0.0,0.0,0.08163265306122448,0.06153846153846154,0.07017543859649124,0.006258807799153787,0.4546942412853241,No,0.0,0.0,0.0
q177,What 6 language pairs is experimented on?,"EN<->ES
EN<->DE
EN<->IT
EN<->EO
EN<->MS
EN<->FI","English, Spanish, French, German, Russian, Italian.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.48453065752983093,No,0.0,0.0,0.0
q178,How do they enrich the positional embedding with length information,They introduce new trigonometric encoding which besides information about position uses additional length information (abs or relative).,"The context describes two methods:  one sums positional vectors with the difference between sentence length and position, and the other with the proportion of sentence covered.
",0.07692307692307693,0.11764705882352941,0.0930232558139535,0.0,0.0,0.0,0.07692307692307693,0.11764705882352941,0.0930232558139535,0.01693409667719809,0.2943528890609741,No,0.0,0.0,1.0
q179,How do they condition the output to a given target-source class?,"They use three groups short/normal/long translation classes to learn length token, which is in inference used to bias network to generate desired length group.","The condition vector, which is the class label (text string), is fed to both the generator and discriminator.  The class label must be related to the real data.
",0.21428571428571427,0.23076923076923078,0.22222222222222224,0.037037037037037035,0.04,0.03846153846153846,0.14285714285714285,0.15384615384615385,0.14814814814814817,0.04818497969696645,0.27573540806770325,No,0.0,0.0,0.0
q180,Is this library implemented into Torch or is framework agnostic?,It uses deep learning framework (pytorch),"PyTorch.
",1.0,0.16666666666666666,0.2857142857142857,0.0,0.0,0.0,1.0,0.16666666666666666,0.2857142857142857,0.0,0.5114911198616028,Yes,1.0,1.0,1.0
q181,How does this compare to traditional calibration methods like Platt Scaling?,No reliability diagrams are provided and no explicit comparison is made between confidence scores or methods.,"The answer is not found in the context.
",0.125,0.0625,0.08333333333333333,0.0,0.0,0.0,0.125,0.0625,0.08333333333333333,0.023309208103306348,0.061772335320711136,No,0.0,0.0,0.0
q182,What's the input representation of OpenIE tuples into the model?,word embeddings,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.17882280051708221,No,0.0,0.0,0.0
q183,"Is CRWIZ already used for data collection, what are the results?","Yes, CRWIZ has been used for data collection and its initial use resulted in 145 dialogues. The average time taken for the task was close to the estimate of 10 minutes, 14 dialogues (9.66%) resolved the emergency in the scenario, and these dialogues rated consistently higher in subjective and objective ratings than those which did not resolve the emergency. Qualitative results showed that participants believed that they were interacting with an automated assistant.","Yes, CRWIZ is used for data collection, and a dataset was collected and compared to a dataset from a lab setting with a single Wizard.  The results are not explicitly stated.
",0.45161290322580644,0.1891891891891892,0.26666666666666666,0.16666666666666666,0.0684931506849315,0.0970873786407767,0.3548387096774194,0.14864864864864866,0.20952380952380953,0.034385455592754265,0.7145479917526245,No,0.0,0.0,0.0
q184,What contextual features are used?,The words that can indicate the characteristics of the neighbor words as contextual keywords and generate it from the automatically extracted contextual keywords.,"Contextual keywords, ratio of quotation marks, exclamation marks, modal verbs, stop words, type-token ratio, hedging, named entity types, POS n-grams, sentiment, subjectivity scores, spell-checking, readability features (Coleman-Liau, Flesch), argument lexicon features, and surface features (word lengths, sentence lengths).
",0.14285714285714285,0.2608695652173913,0.1846153846153846,0.024390243902439025,0.045454545454545456,0.031746031746031744,0.07142857142857142,0.13043478260869565,0.0923076923076923,0.009196271525972168,0.5527533292770386,No,0.0,0.0,0.0
q185,How is the data in RAFAEL labelled?,"Using a set of annotation tools such as Morfeusz, PANTERA, Spejd, NERF and Liner","The authors projected word-sense annotations of publicly available corpora to supersenses and applied perceptron-trained Hidden Markov Model for sequence classification.  The system also analyses labels such as plant, animal or shape.
",0.15151515151515152,0.35714285714285715,0.21276595744680848,0.03125,0.07692307692307693,0.044444444444444446,0.09090909090909091,0.21428571428571427,0.1276595744680851,0.027374353552268266,0.35424795746803284,No,0.0,1.0,1.0
q186,How is the fluctuation in the sense of the word and its neighbors measured?,"Our method performs a statistical test to determine whether a given word is used polysemously in the text, according to the following steps:
1) Setting N, the size of the neighbor.
2) Choosing N neighboring words ai in the order whose angle with the vector of the given word w is the smallest.
3) Computing the surrounding uniformity for ai(0 < i ≤ N) and w.
4) Computing the mean m and the sample variance σ for the uniformities of ai .
5) Checking whether the uniformity of w is less than m − 3σ. If the value is less than m − 3σ, we may regard w as a polysemic word.","Surrounding uniformity, which roughly corresponds to statistical fluctuation in the vectors of neighbor words.
",0.7142857142857143,0.09345794392523364,0.16528925619834708,0.38461538461538464,0.04716981132075472,0.08403361344537816,0.42857142857142855,0.056074766355140186,0.09917355371900825,6.265447097918195e-05,0.5868595838546753,No,0.0,0.0,0.0
q187,By how much does SPNet outperforms state-of-the-art abstractive summarization methods on evaluation metrics?,"SPNet vs best baseline:
ROUGE-1: 90.97 vs 90.68
CIC: 70.45 vs 70.25","SPNet outperforms Pointer-Generator, Transformer, and other baseline methods by a large margin.
",0.15384615384615385,0.11764705882352941,0.13333333333333333,0.0,0.0,0.0,0.15384615384615385,0.11764705882352941,0.13333333333333333,0.03125190763972442,0.5002650618553162,No,0.0,1.0,1.0
q188,"Is it expected to have speaker role, semantic slot and dialog domain annotations in real world datasets?","Not at the moment, but summaries can be additionaly extended with this annotations.","Yes.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3343122601509094,No,1.0,1.0,1.0
q189,How does new evaluation metric considers critical informative entities?,Answer with content missing: (formula for CIC) it accounts for the most important information within each dialog domain. CIC can be applied to any summarization task with predefined essential entities,"The new metric, Critical Information Completeness (CIC), is a recall of semantic slot information between a candidate summary and a reference summary, considering a set of delexicalized values in the reference summary.
",0.09375,0.1,0.09677419354838711,0.0,0.0,0.0,0.09375,0.1,0.09677419354838711,0.025507996539511826,0.5522542595863342,Yes,0.0,0.0,0.0
q190,What are state of the art methods MMM is compared to?,"FTLM++, BERT-large, XLNet","FCNN classifier, MTMSNlarge, MAN classifier.  Also, simple baseline methods, MLP, MCB, and NMN.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.021597011339333432,0.21685338020324707,No,0.0,0.0,0.0
q191,What are the problems related to ambiguity in PICO sentence prediction tasks?,Some sentences are associated to ambiguous dimensions in the hidden state output,"The assignment of a single label to a sentence limits the quality of predictions for real-world reviewing tasks, because sentences can contain additional information related to other PICO classes.
",0.1,0.25,0.14285714285714288,0.0,0.0,0.0,0.06666666666666667,0.16666666666666666,0.09523809523809522,0.015784309053930103,0.23725776374340057,No,1.0,1.0,1.0
q192,How is knowledge stored in the memory?,entity memory and relational memory.,"Entities and relations in the text are modeled as memory slots.  Existing knowledge is not explicitly mentioned as a method of storage.  Information from curated knowledge bases is considered for question-answering, but the text focuses on the simpler case where all information is contained within the text.
",0.08333333333333333,0.8,0.1509433962264151,0.02127659574468085,0.25,0.0392156862745098,0.08333333333333333,0.8,0.1509433962264151,0.009402448118597483,0.6014208793640137,No,0.0,0.0,0.0
q193,How do they measure the diversity of inferences?,by number of distinct n-grams,"The context describes measuring NE and token/type diversity, but does not discuss measuring the diversity of inferences.  The answer is not found in the context.
",0.038461538461538464,0.16666666666666666,0.0625,0.0,0.0,0.0,0.038461538461538464,0.16666666666666666,0.0625,0.012414943415352929,0.21633675694465637,No,0.0,0.0,0.0
q194,By how much do they improve the accuracy of inferences over state-of-the-art methods?,"ON Event2Mind, the accuracy of proposed method is improved by  absolute BLUE  2.9,  10.87, 1.79 for xIntent, xReact and oReact respectively.
On Atomic dataset, the accuracy of proposed method is improved by  absolute BLUE 3.95.   4.11, 4.49 for xIntent, xReact and oReact.respectively.","The answer is not found in the context.
",0.375,0.061224489795918366,0.10526315789473682,0.0,0.0,0.0,0.375,0.061224489795918366,0.10526315789473682,0.000422782949230127,0.03131083771586418,No,0.0,0.0,0.0
q195,How does the context-aware variational autoencoder learn event background information?," CWVAE is trained on an auxiliary dataset to learn the event background information by using the context-aware latent variable.  Then, in finetute stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target.","In the pretrain stage, CWVAE is trained on an auxiliary dataset with rich event context information.  The context-acquiring latent variable $z_c$ is directly conditioned on the context, allowing it to acquire background knowledge.  Then, a context-aware latent variable $z_{c'}$ learns background knowledge from $z_c$.
",0.5102040816326531,0.5319148936170213,0.5208333333333334,0.25,0.2608695652173913,0.2553191489361702,0.3877551020408163,0.40425531914893614,0.39583333333333326,0.18418691262061235,0.88116854429245,Yes,0.0,1.0,1.0
q196,How much improvement does their method get over the fine tuning baseline?,"0.08 points on the 2011 test set, 0.44 points on the 2012 test set, 0.42 points on the 2013 test set for IWSLT-CE.","The context states that fine-tuning improves performance in several NMT scenarios, but does not specify the magnitude of improvement over a fine-tuning baseline.
",0.08,0.07407407407407407,0.07692307692307691,0.0,0.0,0.0,0.08,0.07407407407407407,0.07692307692307691,0.019045312326330072,0.2808067500591278,No,0.0,0.0,0.0
q197,By how much do they outpeform previous results on the word discrimination task?,Their best average precision tops previous best result by 0.202,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.10135649889707565,No,0.0,0.0,0.0
q198,How many paraphrases are generated per question?,"10*n paraphrases, where n depends on the number of paraphrases that contain the entity mention spans","10
",1.0,0.058823529411764705,0.1111111111111111,0.0,0.0,0.0,1.0,0.058823529411764705,0.1111111111111111,0.0,0.24006445705890656,No,0.0,0.0,0.0
q199,How strong was the correlation between exercise and diabetes?,weak correlation with p-value of 0.08,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1120087206363678,No,0.0,0.0,0.0
q200,How were topics of interest about DDEO identified?,using topic modeling model Latent Dirichlet Allocation (LDA),"LDA (Latent Dirichlet Allocation) topic modeling was used, analyzing unigrams (single words) associated with topics.  Inspection of the top unigrams for each topic revealed meaningful topics.
",0.2692307692307692,0.875,0.411764705882353,0.12,0.42857142857142855,0.1875,0.11538461538461539,0.375,0.1764705882352941,0.05912329024877923,0.7709238529205322,Yes,1.0,1.0,1.0
q201,How do their train their embeddings?,"The embeddings are learned several times using the training set, then the average is taken.","They use two main training approaches: one based on classifying the word using word labels of training segments, and another based on training ""Siamese"" networks.  In both cases, RNN-based embedding models are used.
",0.14705882352941177,0.3333333333333333,0.20408163265306123,0.0,0.0,0.0,0.11764705882352941,0.26666666666666666,0.163265306122449,0.014378558189726397,0.4513229727745056,No,0.0,0.0,0.0
q202,How do they model travel behavior?,The data from collected travel surveys is used to model travel behavior.,"They use random utility maximization (RUM) models, which are predominantly numeric, linear, and often flat.  They also use Travel Behavior embeddings, a method that borrows from NLP to encode categorical variables.
",0.1935483870967742,0.5,0.27906976744186046,0.03333333333333333,0.09090909090909091,0.04878048780487805,0.12903225806451613,0.3333333333333333,0.18604651162790697,0.012413584734794654,0.5108633041381836,Yes,0.0,0.0,0.0
q203,How do their interpret the coefficients?,The coefficients are projected back to the dummy variable space.,"Coefficients reflect the impact on load demand (e.g., positive for winter words increasing demand, negative for summer words decreasing it).  A word's presence causes a rise in the time series if the coefficient is positive, a decline if negative.  The magnitude of the coefficient indicates the strength of the impact.
",0.057692307692307696,0.3,0.0967741935483871,0.0196078431372549,0.1111111111111111,0.03333333333333333,0.057692307692307696,0.3,0.0967741935483871,0.007821590010947429,0.33244386315345764,Yes,0.0,0.0,0.0
q204,By how much do they outperform previous state-of-the-art models?,"Proposed ORNN has 0.769, 1.238, 0.818, 0.772 compared to 0.778, 1.244, 0.813, 0.781 of best state of the art result on Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.)","The answer is not found in the context.  While the context discusses improvements over baselines and previous models, it does not quantify the improvements against *state-of-the-art* models in a direct way.
",0.14705882352941177,0.1,0.11904761904761905,0.09090909090909091,0.061224489795918366,0.07317073170731708,0.11764705882352941,0.08,0.09523809523809526,0.007871853490274205,0.3871995806694031,No,0.0,0.0,0.0
q205,What is the performance difference between proposed method and state-of-the-arts on these datasets?,Difference is around 1 BLEU score lower on average than state of the art methods.,"TRADE, the state-of-the-art model, performs poorly in this setting.  The proposed ""Ours with Mask and Ordered Triplets"" model outperforms the Baseline and Ablation models on all metrics in previously seen environments.
",0.14705882352941177,0.3333333333333333,0.20408163265306123,0.09090909090909091,0.21428571428571427,0.1276595744680851,0.11764705882352941,0.26666666666666666,0.163265306122449,0.013113617852675475,0.2489447146654129,No,0.0,0.0,0.0
q206,What benchmarks are created?,Answer with content missing: (formula) The accuracy is defined as the ratio # of correct chains predicted to # of evaluation samples,"UMBC corpus for Twitter NER, a Twitter NER corpus created by BIBREF21.  A random forest trained on four features (time of year, day of the week, national average temperature, and wind speed).
",0.09375,0.15,0.11538461538461538,0.0,0.0,0.0,0.0625,0.1,0.07692307692307693,0.013737889784665155,0.04860851541161537,No,0.0,0.0,0.0
q207,What percentage fewer errors did professional translations make?,36%,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.17132845520973206,No,0.0,0.0,0.0
q208,What was the weakness in Hassan et al's evaluation design?,"MT developers to which crowd workers were compared are usually not professional translators, evaluation of sentences in isolation prevents raters from detecting translation errors, used not originally written Chinese test set
","The weakness was that the design did not consider the human judges and lacked objectivity, potentially leading to mistakes due to out-of-context dialogs.
",0.12,0.0967741935483871,0.10714285714285714,0.0,0.0,0.0,0.08,0.06451612903225806,0.07142857142857142,0.01439413740718659,0.3987670838832855,No,0.0,0.0,0.0
q209,What evidence do they present that the model attends to shallow context clues?,Using model gradients with respect to input features they presented that the most important model inputs are verbs associated with entities which shows that the model attends to shallow context clues,"The provided context shows examples of questions and their corresponding comments (e.g., ""snorkeling two days ago off the coast of dukhan..."").  A darker blue patch in a figure (Figure 5) indicates larger weight given to certain words, suggesting the model attends to these shallow context clues.
",0.2127659574468085,0.3225806451612903,0.2564102564102564,0.10869565217391304,0.16666666666666666,0.13157894736842102,0.19148936170212766,0.2903225806451613,0.23076923076923078,0.057855095943418026,0.550015926361084,No,0.0,0.0,0.0
q210,In what way is the input restructured?,"In four entity-centric ways - entity-first, entity-last, document-level and sentence-level","Input Module extracts vector representations of inputs at different levels of granularity by using several different encoders.  The final input embedding is a sum of token, segmentation, and position embeddings.
",0.06666666666666667,0.14285714285714285,0.09090909090909091,0.0,0.0,0.0,0.06666666666666667,0.14285714285714285,0.09090909090909091,0.014325480172973542,0.20680855214595795,No,0.0,0.0,0.0
q211,What language is the Twitter content in?,English,"English
",1.0,1.0,1.0,0.0,0.0,0.0,1.0,1.0,1.0,0.0,1.0,Yes,0.0,1.0,1.0
q212,What evaluations did the authors use on their system?,"BLEU scores, exact matches of words in both translations and topic cache, and cosine similarities of adjacent sentences for coherence.","Human evaluation of grammatical correctness, contextual coherence, emotional appropriateness; relative ranking; direct assessment; head-to-head evaluation.
",0.11764705882352941,0.1,0.1081081081081081,0.0,0.0,0.0,0.11764705882352941,0.1,0.1081081081081081,0.024689783360287473,0.5816641449928284,No,0.0,0.0,0.0
q213,What accuracy does CNN model achieve?,Combined per-pixel accuracy for character line segments is 74.79,"The CNN model's accuracy drops from around 0.8 to 0.5 when the noise rate increases from 0 to 0.5.
",0.043478260869565216,0.09090909090909091,0.0588235294117647,0.0,0.0,0.0,0.043478260869565216,0.09090909090909091,0.0588235294117647,0.019146030690102515,0.3612893521785736,No,0.0,0.0,0.0
q214,How many documents are in the Indiscapes dataset?,508,"508
",1.0,1.0,1.0,0.0,0.0,0.0,1.0,1.0,1.0,0.0,1.0,Yes,0.0,1.0,1.0
q215,What are simulated datasets collected?,There are 6 simulated datasets collected which is initialised with a corpus of size 550 and simulated by generating new documents from Wikipedia extracts and replacing existing documents,"A dataset of 8066 pairs of free-form natural language instructions and navigation plans for training, collected from 88 unique simulated environments.  A prototype dataset corresponding to videos from the Charades dataset, and an official dataset using the Charades sets.
",0.2,0.2857142857142857,0.23529411764705882,0.0,0.0,0.0,0.15,0.21428571428571427,0.1764705882352941,0.013338359564519316,0.41652175784111023,No,0.0,0.0,0.0
q216,What human evaluation metrics were used in the paper?,rating questions on a scale of 1-5 based on fluency of language used and relevance of the question to the context,"Relevance, conciseness, and readability, scored on a 1 to 5 scale.  Pearson correlation with human judgments.
",0.5,0.36363636363636365,0.4210526315789474,0.06666666666666667,0.047619047619047616,0.05555555555555555,0.25,0.18181818181818182,0.2105263157894737,0.04516705608862737,0.599757730960846,No,0.0,0.0,0.0
q217,"For the purposes of this paper, how is something determined to be domain specific knowledge?",reviews under distinct product categories are considered specific domain knowledge,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.061315350234508514,No,0.0,0.0,0.0
q218,What type of model are the ELMo representations used in?,A bi-LSTM with max-pooling on top of it,"CNNs and bi-directional LSTMs.
",0.4,0.2,0.26666666666666666,0.0,0.0,0.0,0.4,0.2,0.26666666666666666,0.0,0.4516381025314331,No,0.0,0.0,0.0
q219,By how much does using phonetic feedback improve state-of-the-art systems?,Improved AECNN-T by 2.1 and AECNN-T-SM BY 0.9,"The answer is not found in the context.  While the text highlights improvements from using phonetic feedback, it does not quantify the improvement relative to ""state-of-the-art"" systems.
",0.03333333333333333,0.07692307692307693,0.046511627906976744,0.0,0.0,0.0,0.03333333333333333,0.07692307692307693,0.046511627906976744,0.0,0.3431175947189331,No,0.0,0.0,0.0
q220,what are the baselines?,"AS Reader, GA Reader, CAS Reader","content, content-network, image-based models;  BiRNN language model; GA model; LSTM baseline;  TF-IDF, S2S, IR, Proposed+IR
",0.05263157894736842,0.16666666666666666,0.08,0.0,0.0,0.0,0.05263157894736842,0.16666666666666666,0.08,0.019011755113058346,0.24388350546360016,No,0.0,0.0,0.0
q221,What are the results of the experiment?,"They were able to create a language model from the dataset, but did not test.","Table TABREF19 shows results for in-domain, out-of-domain, and transfer learning experiments.  A baseline model was trained and validated on corpora.
",0.08695652173913043,0.13333333333333333,0.10526315789473685,0.0,0.0,0.0,0.08695652173913043,0.13333333333333333,0.10526315789473685,0.020705706652424008,0.411367267370224,No,0.0,0.0,0.0
q222,How was the dataset collected?,extracted text from Sorani Kurdish books of primary school and randomly created sentences,"The StackOverflow dataset is from Kaggle, randomly selecting 20,000 question titles from 20 tags.  The Biomedical dataset is from BioASQ, randomly selecting 20,000 paper titles from 20 different ones.  Other datasets are collected via crowdsourcing or from examinations.
",0.05,0.15384615384615385,0.07547169811320756,0.0,0.0,0.0,0.05,0.15384615384615385,0.07547169811320756,0.010134961585174011,0.16552677750587463,No,0.0,0.0,0.0
q223,How many annotators participated?,1,"Four
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.17445963621139526,No,0.0,0.0,1.0
q224,How do they show their model discovers underlying syntactic structure?,By visualizing syntactic distance estimated by the parsing network,"The model autonomously discovered to avoid inter-word attention connections, and use the hidden states of space (separator) tokens to summarize previous information.  This is presented as strong proof of the model's understanding of latent data structure.  Unsupervised parsing results also outperform some baseline models, demonstrating a structure similar to human expert-provided structure.
",0.03636363636363636,0.2222222222222222,0.0625,0.0,0.0,0.0,0.03636363636363636,0.2222222222222222,0.0625,0.007313525558399564,0.4638795256614685,No,0.0,0.0,0.0
q225,How do they measure performance of language model tasks?,"BPC, Perplexity","Accuracy is used to evaluate NLI performance.  For languages other than English, the test set is translated into English and used with an English classification model.  Other metrics, like TrueSkill, are used to evaluate translations.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.008954307276600085,-0.020796246826648712,No,0.0,0.0,0.0
q226,How are content clusters used to improve the prediction of incident severity?,they are used as additional features in a supervised classification task,"Content clusters are used to train a supervised classifier that predicts the degree of harm of an incident based on features of the record (location, external category, medical specialty), and the text itself.
",0.15151515151515152,0.45454545454545453,0.22727272727272727,0.0625,0.2,0.09523809523809523,0.12121212121212122,0.36363636363636365,0.18181818181818182,0.028209110765918005,0.3609272539615631,Yes,0.0,1.0,1.0
q227,What cluster identification method is used in this paper?,"A combination of Minimum spanning trees, K-Nearest Neighbors and Markov Stability BIBREF15, BIBREF16, BIBREF17, BIBREF18","A simple clustering algorithm that iteratively proposes random exchanges of memberships.
",0.18181818181818182,0.125,0.14814814814814814,0.0,0.0,0.0,0.18181818181818182,0.125,0.14814814814814814,0.022473460321102857,0.26682865619659424,No,0.0,0.0,0.0
q228,Why did they think this was a good idea?,They think it will help human TCM practitioners make prescriptions.,"They believed the superior visual grounding of their method would produce natural language explanations justifying visual question answers, thus increasing the capabilities of AI.
",0.041666666666666664,0.1,0.058823529411764705,0.0,0.0,0.0,0.041666666666666664,0.1,0.058823529411764705,0.01717906001229923,0.37893542647361755,No,0.0,0.0,0.0
q229,What QA models were used?,"A pointer network decodes the answer from a bidirectional LSTM with attention flow layer and self-matching layer, whose inputs come from word and character embeddings of the query and input text fed through a highway layer.","QA-BERT, XLM, XLNet, DistilBERT, and ALBERT.
",0.14285714285714285,0.02702702702702703,0.045454545454545456,0.0,0.0,0.0,0.14285714285714285,0.02702702702702703,0.045454545454545456,0.004237000139470399,0.21394199132919312,No,0.0,0.0,0.0
q230,How do their results compare against other competitors in the PAN 2017 shared task on Author Profiling?,"They achieved best result in the PAN 2017 shared task with accuracy for Variety prediction task 0.0013 more than the 2nd best baseline, accuracy for Gender prediction task 0.0029 more than 2nd best baseline and accuracy for Joint prediction task 0.0101 more than the 2nd best baseline","Their system, N-GrAM, was the best-performing out of the 22 systems submitted.
",0.21428571428571427,0.06,0.09375,0.0,0.0,0.0,0.21428571428571427,0.06,0.09375,0.003832229297296446,0.4128091037273407,Yes,0.0,0.0,0.0
q231,On which task does do model do worst?,Gender prediction task,"Arithmetic problems.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14608773589134216,No,0.0,0.0,0.0
q232,On which task does do model do best?,Variety prediction task,"PRN and BIDAF w/ static memory.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0444636307656765,No,0.0,0.0,0.0
q233,How does counterfactual data augmentation aim to tackle bias?,The training dataset is augmented by swapping all gendered words by their other gender counterparts,"By swapping gendered words.
",1.0,0.26666666666666666,0.4210526315789474,0.6666666666666666,0.14285714285714285,0.23529411764705882,1.0,0.26666666666666666,0.4210526315789474,0.031998052130773655,0.6494800448417664,Yes,1.0,1.0,1.0
q234,"In the targeted data collection approach, what type of data is targetted?",Gendered characters in the dataset,"Biomedical questions.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09897153824567795,No,0.0,0.0,0.0
q235,How do they determine which words are informative?,Informative are those that will not be suppressed by regularization performed.,"Informative words are nouns and verbs.  Two are matched if their raw text, lemmatizations, WordNet synsets, or aliases are the same.
",0.09523809523809523,0.18181818181818182,0.125,0.0,0.0,0.0,0.09523809523809523,0.18181818181818182,0.125,0.019011755113058346,0.5115741491317749,No,0.0,0.0,0.0
q236,What improvement does the MOE model make over the SOTA on language modelling?,Perpexity is improved from 34.7 to 28.0.,"The context states that the larger MoE model (4096 experts) achieved a 24% lower perplexity on the test set, and that a model with similar computational budget to the best published model has a lower test perplexity after 10 epochs.
",0.025,0.1111111111111111,0.04081632653061225,0.0,0.0,0.0,0.025,0.1111111111111111,0.04081632653061225,0.009668670750672485,0.42803922295570374,No,0.0,0.0,0.0
q237,What is the difference in performance between the interpretable system (e.g. vectors and cosine distance) and LSTM with ELMo system?,Accuracy of best interpretible system was 0.3945 while accuracy of LSTM-ELMo net was 0.6818.,"The context states that the ELMo-based model under-performs a first-occurrence baseline, suggesting the LSTM model with ELMo embeddings isn't learning complex entity semantics from long-term contexts as effectively as the baseline.  It also notes that the document-level, entity-first model performs better.  No direct comparison of interpretable systems to the LSTM-ELMo system is given.
",0.08196721311475409,0.29411764705882354,0.12820512820512822,0.03333333333333333,0.125,0.052631578947368425,0.08196721311475409,0.29411764705882354,0.12820512820512822,0.008851290873849787,0.5520352721214294,No,0.0,0.0,0.0
q238,Which language-pair had the better performance?,French-English,"English-French
",1.0,1.0,1.0,0.0,0.0,0.0,0.5,0.5,0.5,0.0,0.9834061861038208,No,0.0,0.0,0.0
q239,Which psycholinguistic and basic linguistic features are used?,"Emotion Sensor Feature, Part of Speech, Punctuation, Sentiment Analysis, Empath, TF-IDF Emoticon features","Ratio of quotation marks, exclamation marks, modal verbs, stop words, type-token ratio, hedging, named entity types, POS n-grams, sentiment and subjectivity scores, spell-checking, readability features (Coleman-Liau, Flesch), argument lexicon features, word lengths, sentence lengths, word /usage/linguistic-features.  Emoticons and punctuation features are also included.
",0.12244897959183673,0.42857142857142855,0.19047619047619047,0.0,0.0,0.0,0.08163265306122448,0.2857142857142857,0.12698412698412698,0.009057808281298079,0.7269341349601746,No,0.0,0.0,0.0
q240,How have the differences in communication styles between Twitter and Facebook increase the complexity of the problem?,Systems do not perform well both in Facebook and Twitter texts,"The answer is not found in the context.
",0.25,0.18181818181818182,0.2105263157894737,0.0,0.0,0.0,0.25,0.18181818181818182,0.2105263157894737,0.045400138092837265,0.035914067178964615,No,0.0,0.0,0.0
q241,What data/studies do the authors provide to support the assertion that the majority of aggressive conversations contain code-mixed languages?,None,"The authors cite the TRAC-2018 dataset as providing code-mixed data, but don't state that a majority of aggressive conversations are code-mixed.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0030731880106031895,Yes,0.0,0.0,0.0
q242,What datasets are used to evaluate the model?,"WN18, FB15k","Train, development, and test sets.  Swissmetro dataset.  Other corpora (Table TABREF12).
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.021476912089159056,0.10739302635192871,No,0.0,0.0,0.0
q243,What baseline models do they compare against?,"SLQA, Rusalka, HMA Model (single), TriAN (single), jiangnan (ensemble), MITRE (ensemble), TriAN (ensemble), HMA Model (ensemble)","TF-IDF, S2S, IR,  NABERT+, Stanford, Multigraph, MIR
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.009917215863997018,0.42644038796424866,No,0.0,0.0,0.0
q244,What are the differences with previous applications of neural networks for this task?,This approach considers related images,"The context mentions further exploration of feature extraction techniques (Constant-Q transform, FFT), improvements in neural network architectures (concatenating 1D and 2D ConvNets, deeper networks with skip connections and Inception modules), and experiments using various CNN variants (DCNN, grConv, AdaSent).  It also highlights the use of LSTMs and GRNNs for capturing contextual information, which are different from prior methods.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0883176252245903,Yes,0.0,0.0,0.0
q245,How much improvement is gained from the proposed approaches?,It eliminates non-termination in some models fixing for some models up to 6% of non-termination ratio.,"The provided text shows improvements in comparison to baseline methods and prior work, but does not quantify the *amount* of improvement gained.  It states that the proposed method outperforms some baselines, but lacks precise numerical values for the improvement.
",0.15384615384615385,0.3333333333333333,0.21052631578947367,0.0,0.0,0.0,0.07692307692307693,0.16666666666666666,0.10526315789473684,0.013224637682882356,0.3138573467731476,No,0.0,0.0,0.0
q246,Is infinite-length sequence generation a result of training with maximum likelihood?,There are is a strong conjecture that it might be the reason but it is not proven.,"Not found in the context.
",0.4,0.11764705882352941,0.1818181818181818,0.0,0.0,0.0,0.2,0.058823529411764705,0.0909090909090909,0.013063150080561919,0.1049891859292984,No,1.0,1.0,1.0
q247,How big is dataset for this challenge?,"133,287 images","The answer is not found in the context.  While various datasets are mentioned, there's no single overall dataset size given for the *challenge*.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.26081034541130066,No,0.0,0.0,0.0
q248,How better is performance of proposed model compared to baselines?,"Proposed model achive 66+-22 win rate, baseline CNN 13+-1  and baseline FiLM 32+-3 .","The proposed model outperforms baselines by a clear margin in new environments, and by a substantial amount in environments with sufficient training data.  It also outperforms baselines on previously seen environments, with particularly large improvements in Exact Match and Goal Match metrics.
",0.11904761904761904,0.3125,0.17241379310344826,0.024390243902439025,0.06666666666666667,0.03571428571428571,0.11904761904761904,0.3125,0.17241379310344826,0.011239503708631486,0.44861099123954773,No,0.0,0.0,0.0
q249,What DCGs are used?,Author's own DCG rules are defined from scratch.,"DGC-max, DGC-ave, and DGC-gated.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.049351578415363785,0.3797391951084137,No,0.0,0.0,0.0
q250,What is the performance difference of using a generated summary vs. a user-written one?,2.7 accuracy points,"The models consistently perform better with a user-written gold summary compared to a system-generated summary.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15412507951259613,No,0.0,0.0,0.0
q251,What evaluation metrics did they look at?,accuracy with standard deviation,"Automatic evaluation metrics, human perceived quality scores (relevance, conciseness, readability), task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, task ease, user expertise, F1-score, Recall rate, F1 score, Overall accuracy.
",0.029411764705882353,0.25,0.05263157894736842,0.0,0.0,0.0,0.029411764705882353,0.25,0.05263157894736842,0.007144315612998046,0.23387157917022705,No,0.0,0.0,0.0
q252,What are the datasets used for the task?,"Datasets used are Celex (English, Dutch), Festival (Italian), OpenLexuque (French), IIT-Guwahati (Manipuri), E-Hitz (Basque)","Stanford Sentiment Treebank (SST-2), Yelp dataset, Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), Quora Question Pairs, MCTest, CNN/Daily Mail, Children's Book Test, and a dataset for simulated indoor environments (using Amazon Mechanical Turk).
",0.05,0.125,0.07142857142857144,0.0,0.0,0.0,0.05,0.125,0.07142857142857144,0.027780159785069564,0.5235404372215271,No,0.0,0.0,0.0
q253,What is the accuracy of the model for the six languages tested?,"Authors report their best models have following accuracy: English CELEX (98.5%), Dutch CELEX (99.47%), Festival (99.990%), OpenLexique (100%), IIT-Guwahat (95.4%), E-Hitz (99.83%)","The accuracy of the model for the six languages is not found in the context.
",0.13333333333333333,0.06896551724137931,0.09090909090909091,0.0,0.0,0.0,0.06666666666666667,0.034482758620689655,0.045454545454545456,0.003750439771616539,0.40557417273521423,No,0.0,0.0,0.0
q254,Which models achieve state-of-the-art performances?,"CELEX (Dutch and English) - SVM-HMM
Festival, E-Hitz and OpenLexique - Liang hyphenation
IIT-Guwahat - Entropy CRF","BERT-based models, specifically $RoBERTa_{large}$ and DistillRoBERTa.
",0.125,0.058823529411764705,0.07999999999999999,0.0,0.0,0.0,0.125,0.058823529411764705,0.07999999999999999,0.022000115916912106,0.23598796129226685,No,0.0,0.0,0.0
q255,Which competitive relational classification models do they test?,For relation prediction they test TransE and for relation extraction they test position aware neural sequence model,"Spacy, Stanford NER, Flair, and a model by E. Tong et al. (BIBREF9).
",0.15384615384615385,0.11764705882352941,0.13333333333333333,0.0,0.0,0.0,0.15384615384615385,0.11764705882352941,0.13333333333333333,0.021597011339333432,0.28078505396842957,No,0.0,0.0,0.0
q256,How do they gather human judgements for similarity between relations?,By assessing similarity of 360 pairs of relations from a subset of Wikidata using an integer similarity score from 0 to 4,"Nine undergraduate subjects were asked to rate the similarity of 360 pairs of relations from a subset of Wikidata, on a scale of 0 (no similarity) to 40 (high similarity).
",0.4666666666666667,0.6363636363636364,0.5384615384615385,0.3448275862068966,0.47619047619047616,0.39999999999999997,0.43333333333333335,0.5909090909090909,0.5,0.29058572949157324,0.8039224743843079,No,0.0,0.0,1.0
q257,What text classification task is considered?,To classify a text as belonging to one of the ten possible classes.,"Question classification; predicting the degree of harm (DoH) among five possible values (1-5).
",0.21428571428571427,0.23076923076923078,0.22222222222222224,0.0,0.0,0.0,0.14285714285714285,0.15384615384615385,0.14814814814814817,0.025683319547529764,0.2946854531764984,No,0.0,0.0,0.0
q258,What novel class of recurrent-like networks is proposed?,"A network, whose learned functions satisfy a certain equation. The  network contains RNN cells with either nested internal memories or dependencies that extend temporally beyond the immediately previous hidden state.","A neural network architecture that leverages both recurrence and one-dimensional convolutions.
",0.25,0.1,0.14285714285714288,0.0,0.0,0.0,0.25,0.1,0.14285714285714288,0.008322437662428014,0.4197227358818054,No,0.0,0.0,1.0
q259,How much bigger is Switchboard-2000 than Switchboard-300 database?,Switchboard-2000 contains 1700 more hours of speech data.,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04923026124015933,0.029344912618398666,No,0.0,0.0,0.0
q260,What domains are detected in this paper?,"Answer with content missing: (Experimental setup not properly rendered) In our experiments we used seven target domains: “Business and Commerce” (BUS), “Government and Politics” (GOV), “Physical and Mental Health” (HEA), “Law and Order” (LAW),
“Lifestyle” (LIF), “Military” (MIL), and “General Purpose” (GEN). Exceptionally, GEN does
not have a natural root category.","The provided text mentions ""Lifestyle"" and ""Scientific"" domains, and implies the detection of other domains from Wikipedia.
",0.23529411764705882,0.0784313725490196,0.11764705882352941,0.0,0.0,0.0,0.11764705882352941,0.0392156862745098,0.058823529411764705,0.003789568569475918,0.581296443939209,No,0.0,0.0,1.0
q261,Why do they think this task is hard?  What is the baseline performance?,"1. there may be situations where more than one action is reasonable, and also because writers tell a story playing with elements such as surprise or uncertainty.
2. Macro F1 = 14.6 (MLR, length 96 snippet)
Weighted F1 = 31.1 (LSTM, length 128 snippet)","The task is hard because baselines lag behind human performance by 16.4% and 32.1% for Quasar-S and Quasar-T respectively.
",0.17391304347826086,0.09090909090909091,0.11940298507462685,0.0,0.0,0.0,0.13043478260869565,0.06818181818181818,0.08955223880597014,0.005703777119494772,0.12726454436779022,No,0.0,0.0,0.0
q262,How do they generate the synthetic dataset?,using generative process,"Users open existing documents, make changes, and save them as separate documents.  The generation process mimics real-world actions.
",0.10526315789473684,0.6666666666666666,0.18181818181818182,0.05555555555555555,0.5,0.09999999999999999,0.10526315789473684,0.6666666666666666,0.18181818181818182,0.017272237992167868,0.3258083760738373,No,0.0,0.0,1.0
q263,What is the average length of the claims?,Average claim length is 8.9 tokens.,"The answer is not found in the context.
",0.125,0.14285714285714285,0.13333333333333333,0.0,0.0,0.0,0.125,0.14285714285714285,0.13333333333333333,0.05669791110976001,0.0089884577319026,No,0.0,0.0,0.0
q264,What debate topics are included in the dataset?,"Ethics, Gender, Human rights, Sports, Freedom of Speech, Society, Religion, Philosophy, Health, Culture, World, Politics, Environment, Education, Digital Freedom, Economy, Science and Law","Politics, World, Society, Law, Ethics, War, Conflict in the Middle East, Latin America, Commonwealth, and others.  (e.g., iPhone6, a political candidate, a policy, an event).
",0.23076923076923078,0.2608695652173913,0.24489795918367346,0.0,0.0,0.0,0.07692307692307693,0.08695652173913043,0.08163265306122448,0.07514160700018863,0.699445903301239,No,0.0,0.0,0.0
q265,"By how much, the proposed method improves BiDAF and DCN on SQuAD dataset?","In terms of F1 score, the Hybrid approach improved by 23.47% and 1.39% on BiDAF and DCN respectively. The DCA approach improved by 23.2% and 1.12% on BiDAF and DCN respectively.","The context does not contain the answer to how much the proposed method improves BiDAF and DCN on the SQuAD dataset.  It discusses improvements to BiDAF and other models, but not in comparison to the baseline or each other in terms of a quantifiable improvement.
",0.28888888888888886,0.37142857142857144,0.325,0.11363636363636363,0.14705882352941177,0.12820512820512822,0.2,0.2571428571428571,0.225,0.04857535428328097,0.572501003742218,No,0.0,0.0,0.0
q266,What are the linguistic differences between each class?,"Each class has different patterns in adjectives, adverbs and verbs for sarcastic and non-sarcastic classes","The languages are divided into Indo-European and Sino-Tibetan families, a language isolate (Basque), and subfamilies within Indo-European (West Germanic, Romance).  There are differences in syllable structures, verbal morphology, plural morpheme acquisition, and past tense acquisition across languages.  Language varieties also differ within some languages (e.g., Portuguese, Spanish).
",0.0784313725490196,0.25,0.11940298507462686,0.0,0.0,0.0,0.058823529411764705,0.1875,0.08955223880597013,0.0076347001877086085,0.2511540949344635,No,0.0,0.0,0.0
q267,what genres do they songs fall under?,"Gospel, Sertanejo, MPB, Forró, Pagode, Rock, Samba, Pop, Axé, Funk-carioca, Infantil, Velha-guarda, Bossa-nova and Jovem-guarda","Blues, Rap, Metal, Folk, R&B, Reggae, Country, and Religious.
",0.1,0.05263157894736842,0.06896551724137931,0.0,0.0,0.0,0.1,0.05263157894736842,0.06896551724137931,0.022346932451407998,0.7194342613220215,No,0.0,0.0,0.0
q268,To what other competitive baselines is this approach compared?,"LSTMs with and without attention, HRED, VHRED with and without attention, MMI and Reranking-RL","Span models.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.17239812016487122,No,0.0,0.0,0.0
q269,"How is human evaluation performed, what was the criteria?",Through Amazon MTurk annotators to determine plausibility and content richness of the response,"Human evaluators, divided into expert (15 Chinese literature bachelors) and amateur (15 other field bachelors) groups, graded generated poems from four perspectives: Fluency (grammatical and rhythmic well-formedness), Semantic coherence, Accuracy (fidelity to source), and Appropriateness (suitability to the context).  Evaluators were native Chinese speakers.
",0.06666666666666667,0.23076923076923078,0.10344827586206898,0.0,0.0,0.0,0.06666666666666667,0.23076923076923078,0.10344827586206898,0.00769226298433097,0.2258082926273346,No,0.0,0.0,0.0
q270,How much better were results of the proposed models than base LSTM-RNN model?,on diversity 6.87 and on relevance 4.6 points higher,"The context states that LSTM models achieve similar test perplexity to previous work, but does not directly compare proposed models to a base LSTM-RNN model.  Therefore, the answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09855867177248001,No,0.0,0.0,0.0
q271,Which one of the four proposed models performed best?,the hybrid model MinAvgOut + RL,"""Ours with Mask and Ordered Triplets""
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09482955187559128,No,0.0,0.0,0.0
q272,How much is proposed model better than baselines in performed experiments?,"most of the models have similar performance on BPRA: DSTC2 (+0.0015), Maluuba (+0.0729)
GDP achieves the best performance in APRA: DSTC2 (+0.2893), Maluuba (+0.2896)
GDP significantly outperforms the baselines on BLEU: DSTC2 (+0.0791), Maluuba (+0.0492)","The proposed model outperforms baselines by a clear margin, with increases in EM and GM scores.  For example, the difference between the proposed model and the second best model is INLINEFORM0 EM and GM.  There is also an over INLINEFORM0 increase in EM and GM between the proposed model and the next best two models.
",0.14545454545454545,0.1951219512195122,0.16666666666666666,0.0,0.0,0.0,0.10909090909090909,0.14634146341463414,0.125,0.010749257799288672,0.504580020904541,No,0.0,0.0,0.0
q273,By how much is precission increased?,"ROUGE-1 increases by 0.05, ROUGE-2 by 0.06 and ROUGE-L by 0.09","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15129078924655914,No,0.0,0.0,0.0
q274,What labels are in the dataset?,binary label of stress or not stress,"Location (LOC), Person (PER), Organization (ORG), Miscellaneous (TABREF15); Non-Aggressive (NAG), Overtly-Aggressive (OAG), Covertly-Aggressive (CAG); seven ordered levels of likelihood of human trafficking; seven classes based on structured headings (PubMed data).
",0.030303030303030304,0.14285714285714285,0.05,0.0,0.0,0.0,0.030303030303030304,0.14285714285714285,0.05,0.006609918289641935,0.2092476189136505,No,0.0,0.0,0.0
q275,"How are customer satisfaction, customer frustration and overall problem resolution data collected?",By annotators on Amazon Mechanical Turk.,"The data consists of 216 satisfied and 500 unsatisfied customer conversations, 271 resolved and 425 unresolved problem conversations, and 534 frustrated and 229 not frustrated customer conversations.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.012414943415352929,0.06818710267543793,No,0.0,0.0,0.0
q276,How many improvements on the French-German translation benchmark?,one,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06584003567695618,No,0.0,0.0,0.0
q277,How do they prevent the model complexity increasing with the increased number of slots?,They exclude slot-specific parameters and incorporate better feature representation of user utterance and dialogue states using syntactic information and convolutional neural networks (CNN).,"They propose the Slot-Independent Model (SIM) which does not increase in complexity as the number of slots increases.
",0.15789473684210525,0.125,0.13953488372093023,0.0,0.0,0.0,0.15789473684210525,0.125,0.13953488372093023,0.021403027544652987,0.44947898387908936,Yes,0.0,1.0,1.0
q278,How do they measure model size?,By the number of parameters.,"The context does not contain information about how model size is measured.
",0.08333333333333333,0.2,0.11764705882352941,0.0,0.0,0.0,0.08333333333333333,0.2,0.11764705882352941,0.030890553181566974,0.31517478823661804,No,0.0,0.0,0.0
q279,Which model architecture do they for sentence encoding?,"Answer with content missing: (Skip-thought vectors-Natural Language Inference paragraphs) The encoder for the current sentence and the decoders for the previous (STP) and next sentence (STN) are typically parameterized as separate RNNs
- RNN","BiLSTM-max with a hidden size of 600D per direction.
",0.1,0.02857142857142857,0.04444444444444444,0.0,0.0,0.0,0.1,0.02857142857142857,0.04444444444444444,0.0018903162968037812,0.012059446424245834,No,0.0,0.0,0.0
q280,Which data sources do they use?,"- En-Fr (WMT14)
- En-De (WMT15)
- Skipthought (BookCorpus)
- AllNLI (SNLI + MultiNLI)
- Parsing (PTB + 1-billion word)","Online debate websites, source documents/summaries (free text), crowd sourcing, examinations designed by educational experts,  French meteorological office (Météo France), National Oceanic and Atmospheric Administration (NOAA) stations, Stanford Sentiment Treebank (SST-2), Yelp dataset, Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), and Quora Question Pairs.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.009047454515789121,0.34803828597068787,No,0.0,0.0,0.0
q281,How were breast cancer related posts compiled from the Twitter streaming API?,"By using  keywords `breast' AND `cancer' in tweet collecting process. 
","A primary feed searched for ""breast"" AND ""cancer"", and a secondary feed searched for ""cancer"".  A total of 48,113 tweets from these accounts were compiled.
",0.15384615384615385,0.4,0.2222222222222222,0.08,0.2222222222222222,0.11764705882352941,0.15384615384615385,0.4,0.2222222222222222,0.012944532586971997,0.7368656992912292,No,1.0,1.0,1.0
q282,What machine learning and NLP methods were used to sift tweets relevant to breast cancer experiences?,"ML  logistic regression classifier combined with a Convolutional Neural Network (CNN) to identify self-reported diagnostic tweets.
NLP methods:  tweet conversion to numeric word vector,  removing tweets containing hyperlinks, removing ""retweets"", removing all tweets containing horoscope indicators,  lowercasing and  removing punctuation.","Convolutional Neural Network (CNN) with Google's Tensorflow interface.
",0.5555555555555556,0.12195121951219512,0.2,0.375,0.075,0.12499999999999999,0.4444444444444444,0.0975609756097561,0.16,0.013073032850034427,0.3149792551994324,No,0.0,0.0,1.0
q283,What approach performs better in experiments global latent or sequence of fine-grained latent variables?,"PPL: SVT
Diversity: GVT
Embeddings Similarity: SVT
Human Evaluation: SVT","The GVT (global latent variable) achieves relatively lower reconstruction PPL, suggesting better performance.  The sequential latent variables (SVT) further improve reconstruction PPL.
",0.13636363636363635,0.3,0.18749999999999997,0.0,0.0,0.0,0.09090909090909091,0.2,0.12500000000000003,0.01693409667719809,0.4747178554534912,No,0.0,1.0,1.0
q284,Which translation system do they use to translate to English?,Attention-based translation model with convolution sequence to sequence model,"Google Translate
",0.5,0.1,0.16666666666666669,0.0,0.0,0.0,0.5,0.1,0.16666666666666669,0.0,0.22756540775299072,No,0.0,0.0,0.0
q285,Which pre-trained English NER model do they use?,Bidirectional LSTM based NER model of Flair,"Flair's default NER model.
",0.6,0.42857142857142855,0.5,0.25,0.16666666666666666,0.2,0.4,0.2857142857142857,0.3333333333333333,0.14320952289897704,0.6513205170631409,Yes,1.0,1.0,1.0
q286,How much of the ASR grapheme set is shared between languages?,Little overlap except common basic Latin alphabet and that Hindi and Marathi languages use same script.,"The context states that the languages share the common basic Latin alphabet and that Hindi and Marathi use the Devanagari script.  It does not quantify the degree of shared characters beyond this.
",0.375,0.75,0.5,0.25806451612903225,0.5333333333333333,0.34782608695652173,0.34375,0.6875,0.4583333333333333,0.2577701686990218,0.8090642094612122,Yes,0.0,1.0,1.0
q287,What are the languages used to test the model?,"Hindi, English and German (German task won)","English, French, German, Spanish, Russian, Italian.
",0.3333333333333333,0.2857142857142857,0.30769230769230765,0.0,0.0,0.0,0.3333333333333333,0.2857142857142857,0.30769230769230765,0.04456882760699064,0.6182389259338379,No,0.0,0.0,0.0
q288,Which language has the lowest error rate reduction?,thai,"Sinhala
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.37077653408050537,No,0.0,0.0,0.0
q289,How is moral bias measured?,"Answer with content missing: (formula 1) bias(q, a, b) = cos(a, q) − cos(b, q)
Bias is calculated as substraction of cosine similarities of question and some answer for two opposite answers.","Moral bias is measured via cosine-similarity, clustering, or semantic search, and by computing the moral subspace of the text embedding, using PCA on selected atomic actions.  The correlation between the WEAT value and the moral bias is calculated using Pearson's Correlation Coefficient.
",0.18181818181818182,0.24242424242424243,0.2077922077922078,0.06976744186046512,0.09375,0.07999999999999999,0.13636363636363635,0.18181818181818182,0.15584415584415584,0.022276776176871585,0.4408699572086334,No,0.0,0.0,0.0
q290,What metrics are used for evaluation?,word error rate,"Task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, F1-score, BLEU, ROUGE, METEOR, WPSLOR, ROUGE-LM.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2342829704284668,No,0.0,0.0,0.0
q291,How much training data is used?,"163,110,000 utterances","2,838 (or 7934)
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5640202760696411,No,0.0,0.0,0.0
q292,How does their model differ from BERT?,Their model does not differ from BERT.,"The input embeddings are the sum of token embeddings, segmentation embeddings, and position embeddings.  Segmentation embeddings use learned sentence A and sentence B embeddings for the first and second sentences, respectively.  ALBERT uses factorized embedding parameterization and cross-layer parameter sharing for parameter reduction.  Distilled BERT-based models are also described.
",0.0392156862745098,0.2857142857142857,0.06896551724137931,0.0,0.0,0.0,0.0196078431372549,0.14285714285714285,0.034482758620689655,0.006488583831481832,0.3829965889453888,No,0.0,0.0,0.0
q293,How does explicit constraint on the KL divergence term that authors propose looks like?,"Answer with content missing: (Formula 2) Formula 2 is an answer: 
\big \langle\! \log p_\theta({x}|{z}) \big \rangle_{q_\phi({z}|{x})}  -  \beta |D_{KL}\big(q_\phi({z}|{x}) || p({z})\big)-C|","$|D_{KL}\big (q_\phi ({z}|{x}) || p({z})\big )-C|$ where $C \ge 0$.
",0.7333333333333333,0.3055555555555556,0.43137254901960786,0.7142857142857143,0.2857142857142857,0.40816326530612246,0.7333333333333333,0.3055555555555556,0.43137254901960786,0.260985971676289,0.6525278687477112,Yes,0.0,0.0,1.0
q294,what was the baseline?,There is no baseline.,"Content, content-network, and image-based models.  Also, a LSTM baseline and a hierarchical transformer summarization model (HeriTransfomer).  Model 1 is also a baseline.
",0.08333333333333333,0.5,0.14285714285714285,0.0,0.0,0.0,0.08333333333333333,0.5,0.14285714285714285,0.02747878682374729,0.392927885055542,No,1.0,1.0,1.0
q295,How big is dataset used?,"553,451 documents","The sizes of various datasets vary.  Some are in the hundreds of thousands (e.g., 287,226 documents), others are in millions (e.g., 17M training tokens).  Information on total dataset size isn't given for all datasets.
",0.02631578947368421,0.3333333333333333,0.048780487804878044,0.0,0.0,0.0,0.02631578947368421,0.3333333333333333,0.048780487804878044,0.0072916986445412555,0.5551185607910156,No,0.0,1.0,1.0
q296,How much better does this baseline neural model do?,"The model outperforms at every point in the
implicit-tuples PR curve reaching almost 0.8 in recall","The baseline neural model outperforms the EntNet model.
",0.5,0.2222222222222222,0.30769230769230765,0.14285714285714285,0.058823529411764705,0.08333333333333333,0.5,0.2222222222222222,0.30769230769230765,0.05209696906543444,0.3537353575229645,No,0.0,0.0,0.0
q297,What are the baseline models?,"MC-CNN
MVCNN
CNN","TF-IDF, S2S, IR, and Proposed+IR
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10553345829248428,No,0.0,0.0,0.0
q298,By how much of MGNC-CNN out perform the baselines?,"In terms of Subj the Average MGNC-CNN is better than the average score of baselines by 0.5.  Similarly, Scores of SST-1, SST-2, and TREC where MGNC-CNN has similar improvements. 
In case of Irony the difference is about 2.0. 
","The context states that MGNC-CNN and MG-CNN outperform baseline methods, but does not give a specific performance difference.
",0.35,0.1590909090909091,0.21875,0.05263157894736842,0.023255813953488372,0.03225806451612903,0.3,0.13636363636363635,0.18749999999999997,0.008623188376772919,0.7534796595573425,No,0.0,0.0,0.0
q299,What is increase in percentage of humor contained in headlines generated with TitleStylist method (w.r.t. baselines)?,"Humor in headlines (TitleStylist vs Multitask baseline):
Relevance: +6.53% (5.87 vs 5.51)
Attraction: +3.72% (8.93 vs 8.61)
Fluency: 1,98% (9.29 vs 9.11)","Answer not found in the context.
",0.16666666666666666,0.03225806451612903,0.05405405405405405,0.0,0.0,0.0,0.16666666666666666,0.03225806451612903,0.05405405405405405,0.0006792646002484808,0.0888647586107254,No,0.0,0.0,0.0
q300,What are the languages they consider in this paper?,"The languages considered were English, Chinese, German, Russian, Arabic, Spanish, French","English, French, German, Spanish, Russian, Italian, Basque, Hungarian, Navajo, Turkish, Quechua, Dutch, Arabic, and Zulu,  Manipuri.
",0.375,0.5454545454545454,0.4444444444444444,0.0,0.0,0.0,0.25,0.36363636363636365,0.2962962962962963,0.13351899845512327,0.6622099876403809,No,0.0,0.0,0.0
q301,Did they experiment with tasks other than word problems in math?,They experimented with sentiment analysis and natural language inference task,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.26264309883117676,No,0.0,0.0,0.0
q302,What evaluation metrics are used?,Accuracy on each dataset and the average accuracy on all datasets.,"Task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, F1-score, BPRA,  belief per-response accuracy.
",0.05263157894736842,0.09090909090909091,0.06666666666666667,0.0,0.0,0.0,0.05263157894736842,0.09090909090909091,0.06666666666666667,0.01717906001229923,0.31599900126457214,No,0.0,0.0,0.0
q303,What kind of Youtube video transcripts did they use?,"youtube video transcripts on news covering different topics like technology, human rights, terrorism and politics","Anonymized, transcribed Voice Search data.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.015738541970947637,0.27944356203079224,No,0.0,0.0,0.0
q304,What makes it a more reliable metric?,It takes into account the agreement between different systems,"It has an interpretable meaning: the expected error made by a given metric as compared to the human rating.
",0.10526315789473684,0.2222222222222222,0.14285714285714285,0.0,0.0,0.0,0.10526315789473684,0.2222222222222222,0.14285714285714285,0.021597011339333432,0.25112590193748474,No,0.0,0.0,0.0
q305,How much in experiments is performance improved for models trained with generated adversarial examples?,"Answer with content missing: (Table 1) The performance of all the target models raises significantly, while that on the original
examples remain comparable (e.g. the overall accuracy of BERT on modified examples raises from 24.1% to 66.0% on Quora)","The performance of all target models raises significantly after adversarial training.
",0.7272727272727273,0.19047619047619047,0.3018867924528302,0.6,0.14634146341463414,0.23529411764705882,0.7272727272727273,0.19047619047619047,0.3018867924528302,0.018403522953413978,0.5331608057022095,Yes,0.0,1.0,1.0
q306,How is the delta-softmax calculated?,Answer with content missing: (Formula) Formula is the answer.,"$\Delta_s$ (delta-softmax) is the softmax probability of the correct relation for the EDU pair.
",0.125,0.2222222222222222,0.16,0.06666666666666667,0.125,0.08695652173913045,0.125,0.2222222222222222,0.16,0.04343737891358309,0.1653551310300827,No,0.0,0.0,0.0
q307,Which two datasets does the resource come from?,two surveys by two groups - school students and meteorologists to draw on a map a polygon representing a given geographical descriptor,"REF0 and ASQ
",0.3333333333333333,0.047619047619047616,0.08333333333333333,0.0,0.0,0.0,0.3333333333333333,0.047619047619047616,0.08333333333333333,0.0,-0.011466060765087605,No,0.0,0.0,0.0
q308,What is the size of the second dataset?,1 000 labeled dialogues for training and 240 unlabeled dialogues for evaluation,"141 symptom-prescription pairs
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16394396126270294,No,0.0,0.0,0.0
q309,How large is the first dataset?,1 000 labeled dialogues for training and 240 unlabeled dialogues for evaluation,"287,226 documents
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14622820913791656,No,0.0,0.0,0.0
q310,Who was the top-scoring team?,IDEA,"AM
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13247697055339813,No,0.0,0.0,0.0
q311,Why is big data not appropriate for this task?,Training embeddings from small-corpora can increase the performance of some tasks,"Big datasets are not always available, particularly in computational social science NLP applications where the data of interest are often not obtained from large-scale sources.  Also, the size of a dataset does not correspond to its degree of relevance for a particular analysis; even very large corpora can have idiosyncrasies.
",0.09803921568627451,0.4166666666666667,0.15873015873015872,0.02,0.09090909090909091,0.032786885245901634,0.058823529411764705,0.25,0.09523809523809523,0.009347836091389596,0.2696937322616577,Yes,0.0,0.0,0.0
q312,What is an example of a computational social science NLP task?,Visualization of State of the union addresses,"Answer not found in the context.
",0.16666666666666666,0.14285714285714285,0.15384615384615383,0.0,0.0,0.0,0.16666666666666666,0.14285714285714285,0.15384615384615383,0.06567274736060395,0.07012403011322021,No,0.0,0.0,0.0
q313,Which major geographical regions are studied?,"Northeast U.S, South U.S., West U.S. and Midwest U.S.","Latin America and the Caribbean (LCN), South Asia (SAS), Sub-Saharan Africa (SSA), Europe and Central Asia (ECS), Middle East and North Africa (MEA), East Asia and the Pacific (EAS), North America (NAC).
",0.06060606060606061,0.15384615384615385,0.08695652173913043,0.0,0.0,0.0,0.06060606060606061,0.15384615384615385,0.08695652173913043,0.017268919138039227,0.6425136923789978,No,0.0,0.0,0.0
q314,How strong is the correlation between the prevalence of the #MeToo movement and official reports [of sexual harassment]?,0.9098 correlation,"The change in the number of reported cases constitutes a considerable change in the number of #MeToo users on Twitter.  (p-value is extremely small at 5.7e-13).
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.22760702669620514,No,0.0,0.0,0.0
q315,How are the topics embedded in the #MeToo tweets extracted?,Using Latent Dirichlet Allocation on TF-IDF transformed from the corpus,"Latent Dirichlet Allocation (LDA) is used to label universal topics.  The optimal topic number is chosen based on the highest coherence score.
",0.2727272727272727,0.5454545454545454,0.3636363636363636,0.09523809523809523,0.2,0.12903225806451613,0.22727272727272727,0.45454545454545453,0.30303030303030304,0.06109833740738946,0.38997986912727356,Yes,1.0,1.0,1.0
q316,Which geographical regions correlate to the trend?,"Northeast U.S., West U.S. and South U.S.","South Asia, Latin America and the Caribbean, East Asia and the Pacific.  (and the UK, exhibiting a decreasing trend)
",0.10526315789473684,0.2,0.13793103448275862,0.0,0.0,0.0,0.05263157894736842,0.1,0.06896551724137931,0.021300733682208986,0.612308919429779,No,0.0,0.0,0.0
q317,What two components are included in their proposed framework?,evidence extraction and answer synthesis,"A deep learning classifier and a tweet processor.
",0.125,0.2,0.15384615384615385,0.0,0.0,0.0,0.125,0.2,0.15384615384615385,0.04767707020457095,0.195113867521286,No,0.0,0.0,0.0
q318,Which modifications do they make to well-established Seq2seq architectures?,"Replacing attention mechanism to query-key attention, and adding a loss to make the attention mask as diagonal as possible","They use various regularization techniques, a single-head LSTM attention mechanism, and consider the specific problem of TTS to modify the Seq2seq model for better performance.  They also employ variational-based methods to increase generation diversity.
",0.19444444444444445,0.35,0.25,0.02857142857142857,0.05263157894736842,0.037037037037037035,0.1388888888888889,0.25,0.17857142857142858,0.03259631698411772,0.1164146140217781,No,0.0,0.0,0.0
q319,How was speed measured?,how long it takes the system to lemmatize a set number of words,"The answer is not found in the context.
",0.125,0.07692307692307693,0.09523809523809525,0.0,0.0,0.0,0.125,0.07692307692307693,0.09523809523809525,0.03056960239296902,0.1064307764172554,No,0.0,0.0,0.0
q320,What were their accuracy results on the task?,97.32%,"Accuracy results vary: 100% on non-monotone problems, 95.4% with a specific approach, and 95.0% with another approach.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.015732934811145335,0.45893776416778564,No,0.0,0.0,0.0
q321,What two types the Chinese reading comprehension dataset consists of?,cloze-style reading comprehension and user query reading comprehension questions,"People Daily news datasets and Children's Fairy Tale datasets.
",0.1,0.1,0.10000000000000002,0.0,0.0,0.0,0.1,0.1,0.10000000000000002,0.04196114906296548,0.20782171189785004,No,0.0,1.0,1.0
q322,For which languages most of the existing MRC datasets are created?,English,"English, German, Spanish, Italian, Japanese, and Portuguese.
",0.14285714285714285,1.0,0.25,0.0,0.0,0.0,0.14285714285714285,1.0,0.25,0.030890553181566974,0.444061815738678,No,0.0,1.0,1.0
q323,Which sentiment analysis tasks are addressed?,12 binary-class classification and multi-class classification of reviews based on rating,"Tweet-level sentiment, clinical narrative sentiment (positive/negative phenomena within risk factor domains).
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1687026470899582,No,0.0,0.0,0.0
q324,Which 3 NLP areas are cited the most?,"machine translation, statistical machine, sentiment analysis","Language, machine translation, and lexical semantics.
",0.3333333333333333,0.3333333333333333,0.3333333333333333,0.2,0.2,0.20000000000000004,0.3333333333333333,0.3333333333333333,0.3333333333333333,0.19070828081828378,0.6009442806243896,No,0.0,0.0,0.0
q325,Which journal and conference are cited the most in recent years?,CL Journal and EMNLP conference,"The context states that CL Journal, EMNLP, and ACL have the best track record for accepting papers that have gone on to receive 1000 or more citations.  It also notes that CL Journal has a particularly high percentage of papers with 100 or more citations.  However, it does not state which is cited *most* overall in recent years.
",0.06896551724137931,0.8,0.12698412698412698,0.017543859649122806,0.25,0.03278688524590164,0.05172413793103448,0.6,0.09523809523809525,0.013039112944078961,0.41237396001815796,No,0.0,1.0,1.0
q326,Which 5 languages appear most frequently in AA paper titles?,"English, Chinese, French, Japanese and Arabic","Chinese, Arabic, Korean, Japanese, and Hindi
",0.6666666666666666,0.6666666666666666,0.6666666666666666,0.2,0.2,0.20000000000000004,0.5,0.5,0.5,0.13650604313545334,0.9206621646881104,No,0.0,0.0,0.0
q327,How much F1 was improved after adding skip connections?,"Simple Skip improves F1 from 74.34 to 74.81
Transformer Skip improes F1 from 74.34 to 74.95 ","The skip connections improved scores further.  However, the amount of improvement is not specified.
",0.14285714285714285,0.1,0.11764705882352941,0.0,0.0,0.0,0.14285714285714285,0.1,0.11764705882352941,0.0,0.36544904112815857,No,1.0,1.0,1.0
q328,How much gain does the model achieve with pretraining MVCNN?,0.8 points on Binary; 0.7 points on Fine-Grained; 0.6 points on Senti140; 0.7 points on Subj,"+1.25 ROUGE improvement.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.339648574590683,No,0.0,0.0,0.0
q329,What is the highest accuracy score achieved?,82.0%,"93.06% (unnormalized) for scene recognition.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04767707020457095,0.5278199911117554,No,0.0,0.0,0.0
q330,What are the three datasets used in the paper?,Data released for APDA shared task contains 3 datasets.,"France, UK, Biomedical
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09773903340101242,No,0.0,0.0,0.0
q331,What is improvement in accuracy for short Jokes in relation other types of jokes?,It had the highest accuracy comparing to all datasets 0.986% and It had the highest improvement comparing to previous methods on the same dataset by 8%,"A jump of 8 percent from the most recent work done with CNNs (Table 4).
",0.13333333333333333,0.07407407407407407,0.09523809523809523,0.0,0.0,0.0,0.06666666666666667,0.037037037037037035,0.047619047619047616,0.014653947328713739,0.47694018483161926,Yes,0.0,0.0,1.0
q332,What baseline did they compare Entity-GCN to?,"Human, FastQA, BiDAF, Coref-GRU, MHPGM, Weaver / Jenga, MHQA-GRN","Many techniques that use expensive question-aware recurrent document encoders.  BiDAF.
",0.09090909090909091,0.1,0.09523809523809525,0.0,0.0,0.0,0.09090909090909091,0.1,0.09523809523809525,0.026374077368969157,0.1260341852903366,Yes,1.0,1.0,1.0
q333,How did they get relations between mentions?,"Assign a value to the relation based on whether mentions occur in the same document, if mentions are identical, or if mentions are in the same coreference chain.","The provided text describes using predictions from a coreference resolution system to add mentions of elements.  It also describes associating mentions by encoding selected entity information.  The exact method for getting relations between mentions isn't fully explained.
",0.23684210526315788,0.32142857142857145,0.2727272727272727,0.0,0.0,0.0,0.13157894736842105,0.17857142857142858,0.15151515151515152,0.014943489981430611,0.5816146731376648,No,0.0,0.0,0.0
q334,How did they detect entity mentions?,Exact matches to the entity string and predictions from a coreference resolution system,"The basic rules used to detect mentions are similar to those of Lee:2013:CL, except that a set of filtering rules is used to discard extracted entities.
",0.14285714285714285,0.3076923076923077,0.1951219512195122,0.0,0.0,0.0,0.07142857142857142,0.15384615384615385,0.0975609756097561,0.013794462224541233,0.2941228151321411,No,0.0,0.0,0.0
q335,What performance does the Entity-GCN get on WIKIHOP?,"During testing: 67.6 for single model without coreference, 66.4 for single model with coreference, 71.2 for ensemble of 5 models","Entity-GCN achieves over 2% improvement over the best previously-published results, and an ensemble brings a further 3.6% improvement, only 3% below human performance.
",0.11538461538461539,0.13043478260869565,0.12244897959183673,0.0,0.0,0.0,0.07692307692307693,0.08695652173913043,0.08163265306122448,0.01693409667719809,0.45588573813438416,No,0.0,1.0,1.0
q336,What document context was added?,Preceding and following sentence of each metaphor and paraphrase are added as document context,"A list of ranked and scored pseudodocuments relevant to the question.
",0.18181818181818182,0.14285714285714285,0.16,0.0,0.0,0.0,0.18181818181818182,0.14285714285714285,0.16,0.03408991996483855,0.14869819581508636,No,0.0,0.0,0.0
q337,What were the results of the first experiment?,Best performance achieved is 0.72 F1 score,"The accuracies of classifiers trained on the filtered datasets were very similar to the accuracy of the classifier on the unfiltered dataset.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3211016356945038,No,0.0,0.0,0.0
q338,By how much does their model outperform existing methods?,Answer with content missing: (Table II) Proposed model has F1 score of  0.7220 compared to 0.7148 best state-state-of-the-art result.,"Their model surpasses the current state-of-the-art model with an accuracy of 84.84%.
",0.4375,0.28,0.34146341463414637,0.2,0.125,0.15384615384615385,0.3125,0.2,0.24390243902439027,0.021116374942992202,0.5631896257400513,No,0.0,0.0,0.0
q339,What is the performance of their model?,Answer with content missing: (Table II) Proposed model has F1 score of  0.7220.,"The models surpass previously published results, even with significantly less computation.  Base models outperform the NABERT+ baseline.  Large models show substantial improvement over base models, especially in multi-span questions.  Results vary depending on the dataset (e.g., 73% on TEC, 57% on AMMER).  Comparing to MTMSNlarge, performance is similar, but better for multi-span questions.  Models have similar computational budget and training times to the best published models.
",0.028985507246376812,0.14285714285714285,0.048192771084337345,0.0,0.0,0.0,0.028985507246376812,0.14285714285714285,0.048192771084337345,0.005917677800883594,0.29556310176849365,No,0.0,0.0,0.0
q340,How do they damage different neural modules?,"Damage to neural modules is done by randomly initializing their weights, causing the loss of all learned information.","Damage to the analog of Broca's area (the LSTMs) is described.  The context does not detail damage to other neural modules.
",0.3181818181818182,0.3888888888888889,0.35000000000000003,0.09523809523809523,0.11764705882352941,0.10526315789473684,0.18181818181818182,0.2222222222222222,0.19999999999999998,0.050661968099322066,0.6003646850585938,No,0.0,0.0,0.0
q341,How long are the datasets?,"Travel dataset contains 4100 raw samples, 11291 clauses, Hotel dataset contains 3825 raw samples, 11264 clauses, and the Mobile dataset contains 3483 raw samples and 8118 clauses","The context describes various datasets with varying sizes.  No single answer for ""how long"" is provided.
",0.125,0.07407407407407407,0.09302325581395349,0.0,0.0,0.0,0.125,0.07407407407407407,0.09302325581395349,0.0,0.27477729320526123,No,0.0,0.0,0.0
q342,What are the sources of the data?,"User reviews written in Chinese collected online for hotel, mobile phone, and travel domains","Online debate websites, Twitter data, and Arabic WikiNews.
",0.25,0.14285714285714285,0.18181818181818182,0.0,0.0,0.0,0.25,0.14285714285714285,0.18181818181818182,0.052652614211195894,0.25523820519447327,No,0.0,0.0,0.0
q343,What is the new labeling strategy?,They use a two-stage labeling strategy where in the first stage single annotators label a large number of short texts with relatively pure sentiment orientations and in the second stage multiple annotators label few text samples with mixed sentiment orientations,"A two-stage labeling strategy for sentiment texts.  In the first stage, annotators label short texts with pure sentiment; in the second stage, a smaller number of texts with mixed sentiment are labeled by multiple annotators.
",0.8333333333333334,0.7317073170731707,0.7792207792207793,0.5142857142857142,0.45,0.48,0.6666666666666666,0.5853658536585366,0.6233766233766234,0.17896501854656127,0.808616578578949,Yes,1.0,1.0,1.0
q344,How are their changes evaluated?,The changes are evaluated based on accuracy of intent and entity recognition on SNIPS dataset,"Human translations are judged more favorably than MT when evaluation design changes are made.
",0.21428571428571427,0.2,0.20689655172413796,0.07692307692307693,0.07142857142857142,0.07407407407407408,0.14285714285714285,0.13333333333333333,0.1379310344827586,0.052559234208168874,0.27410823106765747,No,0.0,0.0,0.0
q345,What are the six target languages?,"Answer with content missing: (3 Experimental Setup) We experiment with six target languages: French (FR), Brazilian Portuguese (PT), Italian (IT), Polish (PL), Croatian (HR), and Finnish (FI).","English, Spanish, and Zulu.
",0.25,0.037037037037037035,0.06451612903225806,0.0,0.0,0.0,0.25,0.037037037037037035,0.06451612903225806,0.0003871736459208026,0.5532875657081604,No,0.0,0.0,0.0
q346,What is the size of the released dataset?,"440 sentences, 2247 triples extracted from those sentences, and 11262 judgements on those triples.","287,226 documents for training, 13,368 for validation, and 11,490 for test.
",0.07142857142857142,0.07142857142857142,0.07142857142857142,0.0,0.0,0.0,0.07142857142857142,0.07142857142857142,0.07142857142857142,0.05451525752850337,0.3706270456314087,No,0.0,0.0,0.0
q347,Which OpenIE systems were used?,OpenIE4 and MiniIE,"OpenIE 4, MinIE, OLLIE, and Reverb.
",0.16666666666666666,0.3333333333333333,0.2222222222222222,0.0,0.0,0.0,0.16666666666666666,0.3333333333333333,0.2222222222222222,0.04196114906296548,0.629252552986145,No,1.0,1.0,1.0
q348,how are the bidirectional lms obtained?,"They pre-train forward and backward LMs separately, remove top layer softmax, and concatenate to obtain the bidirectional LMs.","A bidirectional LSTM (BiLSTM) is formed by concatenating an LSTM moving forward through time with an LSTM moving backward through time.
",0.19047619047619047,0.21052631578947367,0.2,0.0,0.0,0.0,0.09523809523809523,0.10526315789473684,0.1,0.02224972978585352,0.5639222264289856,No,0.0,0.0,0.0
q349,what metrics are used in evaluation?,micro-averaged F1,"Manual evaluation, ROUGE, BLEU, METEOR, task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, F1-score, BPRA.
",0.05,0.3333333333333333,0.08695652173913045,0.0,0.0,0.0,0.05,0.3333333333333333,0.08695652173913045,0.011993481292529619,0.12845225632190704,No,0.0,0.0,0.0
q350,what results do they achieve?,91.93% F1 score on CoNLL 2003 NER task and 96.37% F1 score on CoNLL 2000 Chunking task,"Classification produces better results than regression, but the quality depends on data annotation.  Precision and F1 correlate better with human evaluation than Recall.  Reinforcement learning gives promising results, especially in BioASQ human evaluations.
",0.09090909090909091,0.15789473684210525,0.11538461538461539,0.0,0.0,0.0,0.09090909090909091,0.15789473684210525,0.11538461538461539,0.012753998269755913,0.14631766080856323,No,0.0,0.0,0.0
q351,what previous systems were compared to?,"Chiu and Nichols (2016), Lample et al. (2016), Ma and Hovy (2016), Yang et al. (2017), Hashimoto et al. (2016), Søgaard and Goldberg (2016) ","Four baseline systems, including two simple ones and two neural network models.  Also, participant systems in a competition.
",0.05555555555555555,0.04,0.046511627906976744,0.0,0.0,0.0,0.05555555555555555,0.04,0.046511627906976744,0.009501597575403952,0.18913820385932922,No,0.0,0.0,0.0
q352,Are this models usually semi/supervised or unsupervised?,"Both supervised and unsupervised, depending on the task that needs to be solved.","Supervised.
",1.0,0.07692307692307693,0.14285714285714288,0.0,0.0,0.0,1.0,0.07692307692307693,0.14285714285714288,0.0,0.6006678938865662,No,0.0,1.0,1.0
q353,"When they say ""comparable performance"", how much of a performance drop do these new embeddings result in?","Performance was comparable, with the proposed method quite close and sometimes exceeding performance of baseline method.","The answer is not found in the context.  While the text discusses performance improvements and comparisons, it does not quantify the *drop* in performance relative to a baseline or other models in terms of a numerical value.
",0.16216216216216217,0.375,0.22641509433962265,0.0,0.0,0.0,0.10810810810810811,0.25,0.1509433962264151,0.014204962126567007,0.4798614978790283,No,0.0,1.0,1.0
q354,What types of word representations are they evaluating?,GloVE; SGNS,"GloVe word embedding, character-CNN embedding, Part-Of-Speech (POS) embedding, Named-Entity-Recognition (NER) embedding, and exact match feature.
",0.05,0.5,0.09090909090909091,0.0,0.0,0.0,0.05,0.5,0.09090909090909091,0.0,0.18938474357128143,No,0.0,0.0,0.0
q355,What type of recurrent layers does the model use?,GRU,"Deep RNN, LSTM, and quasi-recurrent layers.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.003201768733561039,No,0.0,0.0,0.0
q356,What is a word confusion network?,It is a network used to encode speech lattices to maintain a rich hypothesis space.,"The answer is not found in the context.
",0.125,0.06666666666666667,0.08695652173913045,0.0,0.0,0.0,0.125,0.06666666666666667,0.08695652173913045,0.02604848453271721,0.2145993411540985,No,0.0,0.0,0.0
q357,What evaluation metrics were used in the experiment?,"For sentence-level prediction they used tolerance accuracy, for segment retrieval accuracy and MRR and for the pipeline approach they used overall accuracy","micro-F1, EM-score, F$_1$-score, relevance, attraction, fluency, style strength
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.016401208014088856,0.3821394443511963,No,0.0,0.0,0.0
q358,What kind of instructional videos are in the dataset?,tutorial videos for a photo-editing software,"Screencast tutorial videos with spoken narratives for a photo-editing software.
",0.6363636363636364,1.0,0.7777777777777778,0.5,0.8333333333333334,0.625,0.6363636363636364,1.0,0.7777777777777778,0.27901593935858265,0.6981295347213745,Yes,0.0,1.0,1.0
q359,What baseline algorithms were presented?,"a sentence-level prediction algorithm, a segment retrieval algorithm and a pipeline segment retrieval algorithm","Two simple ones and two neural network models.
",0.125,0.06666666666666667,0.08695652173913045,0.0,0.0,0.0,0.125,0.06666666666666667,0.08695652173913045,0.02447822402834545,0.28031647205352783,No,0.0,0.0,0.0
q360,How does TP-N2F compare to LSTM-based Seq2Seq in terms of training and inference speed?,"Full Testing Set accuracy: 84.02
Cleaned Testing Set accuracy: 93.48","Answer not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.039327241480350494,No,0.0,0.0,0.0
q361,What is the performance proposed model achieved on AlgoList benchmark?,"Full Testing Set Accuracy: 84.02
Cleaned Testing Set Accuracy: 93.48","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04697878658771515,No,0.0,0.0,0.0
q362,What is the performance proposed model achieved on MathQA?,"Operation accuracy: 71.89
Execution accuracy: 55.95","The provided text states the model has a lower test perplexity after 10 epochs compared to other models, but does not give specific performance numbers.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.36189213395118713,No,0.0,0.0,0.0
q363,What previous methods is the proposed method compared against?,"BLSTM+Attention+BLSTM
Hierarchical BLSTM-CRF
CRF-ASN
Hierarchical CNN (window 4)
mLSTM-RNN
DRLM-Conditional
LSTM-Softmax
RCNN
CNN
CRF
LSTM
BERT","Coppersmith et al.'s method, K-means, SkipVec, RecNN, and Para2vec.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.23733176290988922,No,0.0,0.0,0.0
q364,What is the baseline model used?,"The baseline models used are DrQA modified to support answering no answer questions, DrQA+CoQA which is pre-tuned on CoQA dataset, vanilla BERT, BERT+review tuned on domain reviews, BERT+CoQA tuned on the supervised CoQA data","Modelo 1
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.20534294843673706,No,0.0,0.0,0.0
q365,What domains are present in the data?,"Alarm, Banks, Buses, Calendar, Events, Flights, Homes, Hotels, Media, Messaging, Movies, Music, Payment, Rental Cars, Restaurants, Ride Sharing, Services, Train, Travel, Weather","20 domains are listed in Table TABREF10.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.168079674243927,No,0.0,1.0,1.0
q366,"How many texts/datapoints are in the SemEval, TASS and SENTIPOLC datasets?","Total number of annotated data:
Semeval'15: 10712
Semeval'16: 28632
Tass'15: 69000
Sentipol'14: 6428","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.20116351544857025,No,0.0,0.0,0.0
q367,In which languages did the approach outperform the reported results?,"Arabic, German, Portuguese, Russian, Swedish","Arabic, Portuguese, Russian, and Swedish.
",0.8,0.8,0.8000000000000002,0.25,0.25,0.25,0.8,0.8,0.8000000000000002,0.5133450480401706,0.939581036567688,Yes,1.0,1.0,1.0
q368,Which is the baseline model?,"The three baseline models are the i-vector model, a standard RNN LID system and a multi-task RNN LID system. ","Model 1
",0.5,0.047619047619047616,0.08695652173913042,0.0,0.0,0.0,0.5,0.047619047619047616,0.08695652173913042,0.0,0.3937358558177948,No,0.0,0.0,0.0
q369,What is the main contribution of the paper? ,"Proposing an improved RNN model, the phonetic temporal neural LID approach, based on phonetic features that results in better performance","A novel approach applying NLP methods to a new dataset of UN speeches to uncover main development topics, structural factors influencing discussion, and state preferences regarding the international development agenda.  The paper demonstrates how text analytic approaches can help understand global governance.
",0.047619047619047616,0.1,0.06451612903225806,0.0,0.0,0.0,0.047619047619047616,0.1,0.06451612903225806,0.011239503708631486,0.27979984879493713,No,0.0,0.0,0.0
q370,How do they get the formal languages?,These are well-known formal languages some of which was used in the literature to evaluate the learning capabilities of RNNs.,"The context states the formal languages are $a^n b^n$, $a^n b^n c^n$, and $a^n b^n c^n d^n$.
",0.19230769230769232,0.23809523809523808,0.21276595744680848,0.04,0.05,0.044444444444444446,0.07692307692307693,0.09523809523809523,0.0851063829787234,0.02092817867871188,0.3999001979827881,No,0.0,0.0,0.0
q371,What is a confusion network or lattice?,"graph-like structures where arcs connect nodes representing multiple hypothesized words, thus allowing multiple incoming arcs unlike 1-best sequences","A confusion network and a lattice are graph-like structures where arcs connected by nodes represent hypothesized words.  In a lattice, each node marks a timestamp in an utterance, and each arc represents a hypothesized word with its corresponding acoustic and language model scores.
",0.25,0.55,0.34375,0.16279069767441862,0.3684210526315789,0.22580645161290322,0.25,0.55,0.34375,0.06218201247082572,0.468644917011261,Yes,0.0,0.0,1.0
q372,How close do clusters match to ground truth tone categories?,"NMI between cluster assignments and ground truth tones for all sylables is:
Mandarin: 0.641
Cantonese: 0.464","In Mandarin, the 4 clusters correspond very well with the 4 phonemic tone categories.  In Cantonese, the 5 clusters correspond to low-falling, mid-level, high-level, mid-rising, and low-rising tones.
",0.15151515151515152,0.2777777777777778,0.19607843137254904,0.0,0.0,0.0,0.09090909090909091,0.16666666666666666,0.11764705882352942,0.014500372328365223,0.5247488617897034,No,0.0,0.0,1.0
q373,what are the evaluation metrics?,"Precision, Recall, F1","Task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, ROUGE, BLEU, METEOR, micro-F1, BPRA, F1-score, user satisfaction degree, dialogue fluency.
",0.04,0.3333333333333333,0.07142857142857142,0.0,0.0,0.0,0.04,0.3333333333333333,0.07142857142857142,0.02205438848664384,0.27809759974479675,No,0.0,0.0,0.0
q374,which datasets were used in evaluation?,"CoNLL 2003, GermEval 2014, CoNLL 2002, Egunkaria, MUC7, Wikigold, MEANTIME, SONAR-1, Ancora 2.0","An open dataset for Polish QA systems, published by BIBREF56, gathered from Did you know... column of Polish Wikipedia.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.017911710595643587,0.384633332490921,No,0.0,0.0,0.0
q375,what are the baselines?,Perceptron model using the local features.,"ANRES_Sing_R, ANRES_Sing_Z, ANRES_Seq_R, ANRES_Seq_Z.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.015732934811145335,0.12773558497428894,No,0.0,0.0,0.0
q376,What monolingual word representations are used?,"AraVec for Arabic, FastText for French, and Word2vec Google News for English.","Monolingual word embeddings.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010211566521809648,0.3214159607887268,No,0.0,0.0,0.0
q377,Do they build one model per topic or on all topics?,One model per topic.,"The context describes models that explicitly model different types of questions, but it does not specify whether a model is built per topic or across all topics.
",0.1111111111111111,0.75,0.19354838709677416,0.038461538461538464,0.3333333333333333,0.06896551724137931,0.1111111111111111,0.75,0.19354838709677416,0.030603367373678015,0.6477341055870056,No,0.0,0.0,0.0
q378,Do they quantitavely or qualitatively evalute the output of their low-rank approximation to verify the grouping of lexical items?,They evaluate quantitatively.,"Quantitatively.
",1.0,0.3333333333333333,0.5,0.0,0.0,0.0,1.0,0.3333333333333333,0.5,0.0,0.5337651968002319,Yes,1.0,1.0,1.0
q379,How well does their system perform on the development set of SRE?,"EER 16.04, Cmindet 0.6012, Cdet 0.6107","The context states that the system's performance on the SRE 2016 development set is described in a later section (SECREF79).
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.25407660007476807,No,0.0,0.0,0.0
q380,Which of the classifiers showed the best performance?,Logistic regression,"Support Vector Machine (SVM)
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.049438003450632095,No,0.0,0.0,0.0
q381,How are the keywords associated with events such as protests selected?,"By using a Bayesian approach  and by using word-pairs, where they extract all the pairs of co-occurring words within each tweet.  They search for the words that achieve the highest number of spikes matching the days of events.","Keywords associated with events are selected by searching for words that achieve the highest number of spikes matching the days of events, using Jaccard similarity to calculate scores.  The method looks for word-pairs that spike in frequency on the specific day of an event.
",0.4888888888888889,0.55,0.5176470588235293,0.3181818181818182,0.358974358974359,0.3373493975903615,0.35555555555555557,0.4,0.3764705882352941,0.2869914331736318,0.6876062750816345,No,1.0,1.0,1.0
q382,How many speeches are in the dataset?,5575 speeches,"1201
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10023020952939987,No,0.0,0.0,0.0
q383,What is different in BERT-gen from standard BERT?,"They use a left-to-right attention mask so that the input tokens can only attend to other input tokens, and the target tokens can only attend to the input tokens and already generated target tokens.","BERT-gen allows for the direct generation of text from out-of-the-box data.
",0.2,0.08333333333333333,0.11764705882352941,0.0,0.0,0.0,0.13333333333333333,0.05555555555555555,0.0784313725490196,0.005450287747855477,0.16119153797626495,No,0.0,0.0,0.0
q384,How are multimodal representations combined?,The image feature vectors are mapped into BERT embedding dimensions and treated like a text sequence afterwards.,"Concatenated and then projected to a fixed size representation.
",0.2222222222222222,0.11764705882352941,0.15384615384615383,0.0,0.0,0.0,0.2222222222222222,0.11764705882352941,0.15384615384615383,0.02481373275521031,0.3740044832229614,Yes,0.0,0.0,0.0
q385,What is the problem with existing metrics that they are trying to address?,"Answer with content missing: (whole introduction) However, recent
studies observe the limits of ROUGE and find in
some cases, it fails to reach consensus with human.
judgment (Paulus et al., 2017; Schluter, 2017).","Existing metrics penalize diversity and reward hypotheses similar to the reference hypothesis.  They also correlate weakly or moderately with human judgments, and a single reference doesn't perform well.
",0.20689655172413793,0.18181818181818182,0.1935483870967742,0.07142857142857142,0.0625,0.06666666666666667,0.1724137931034483,0.15151515151515152,0.16129032258064518,0.02090969819188881,0.24309386312961578,Yes,0.0,0.0,0.0
q386,How are discourse embeddings analyzed?,They perform t-SNE clustering to analyze discourse embeddings,"t-SNE clustering is performed on them, using the best performing model.  Closest neighbors are examined to see if similar discourse relations tend to group together.
",0.23076923076923078,0.6666666666666666,0.3428571428571429,0.08,0.25,0.12121212121212122,0.19230769230769232,0.5555555555555556,0.28571428571428575,0.03176023542809473,0.7268531918525696,Yes,1.0,1.0,1.0
q387,How are discourse features incorporated into the model?,They derive entity grid with grammatical relations and RST discourse relations and concatenate them with pooling vector for the char-bigrams before feeding to the resulting vector to the softmax layer.,"Two extensions utilize discourse information: (i) CNN2 enhanced with relation probability vectors (CNN2-PV), and (ii) CNN2 enhanced with discourse embeddings (CNN2-DE).
",0.2608695652173913,0.1935483870967742,0.2222222222222222,0.0,0.0,0.0,0.17391304347826086,0.12903225806451613,0.14814814814814814,0.01533227700920213,0.39501482248306274,No,0.0,0.0,0.0
q388,What discourse features are used?,Entity grid with grammatical relations and RST discourse relations.,"Relation probability vectors, discourse embeddings.
",0.4,0.2222222222222222,0.2857142857142857,0.0,0.0,0.0,0.4,0.2222222222222222,0.2857142857142857,0.05087641220727392,0.3963501453399658,No,0.0,0.0,0.0
q389,What are proof paths?,A sequence of logical statements represented in a computational graph,"Paths from the root to leaf nodes in an argument tree.
",0.09090909090909091,0.1,0.09523809523809525,0.0,0.0,0.0,0.09090909090909091,0.1,0.09523809523809525,0.033864985683445356,0.2242867350578308,No,0.0,0.0,0.0
q390,What external sources are used?,"Raw data from Gigaword, Automatically segmented text from Gigaword, Heterogenous training data from People's Daily, POS data from People's Daily","Press releases, academic journals, books, and transcripts of recorded speech.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.019651001128570364,0.22635327279567719,No,0.0,0.0,0.0
q391,How much better peformance is achieved in human evaluation when model is trained considering proposed metric?,"Pearson correlation to human judgement - proposed vs next best metric
Sample level comparison:
- Story generation: 0.387 vs 0.148
- Dialogue: 0.472 vs 0.341
Model level comparison:
- Story generation:  0.631 vs 0.302
- Dialogue: 0.783 vs 0.553","The context states the proposed metric correlates *far better* with human judgment than BLEU and perplexity, and consistently outperforms other metrics by a large margin.  It also correlates better with human evaluation on both datasets used in the experiments.
",0.10256410256410256,0.09523809523809523,0.09876543209876541,0.0,0.0,0.0,0.07692307692307693,0.07142857142857142,0.07407407407407408,0.010705979840202478,0.5226165652275085,Yes,0.0,0.0,0.0
q392,How much transcribed data is available for for Ainu language?,Transcribed data is available for duration of 38h 54m 38s for 8 speakers.,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.021395383339308693,0.07647649943828583,No,0.0,0.0,0.0
q393,What baseline approaches do they compare against?,"HotspotQA: Yang, Ding, Muppet
Fever: Hanselowski, Yoneda, Nie","K-means, SkipVec, RecNN, and Para2vec.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03600104948631532,0.06820996105670929,No,0.0,0.0,0.0
q394,how many domains did they experiment with?,2,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2223912626504898,No,1.0,1.0,1.0
q395,How long is the dataset?,8000,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04955971613526344,No,0.0,0.0,0.0
q396,What is a string kernel?,String kernel is a technique that uses character n-grams to measure the similarity of strings,"String kernels measure the similarity of strings through character n-grams.
",0.9090909090909091,0.625,0.7407407407407406,0.7,0.4666666666666667,0.56,0.6363636363636364,0.4375,0.5185185185185185,0.29003222136949985,0.9640392661094666,Yes,1.0,1.0,1.0
q397,How do they correlate NED with emotional bond levels?,They compute Pearson’s correlation between NED measure for patient-to-therapist and patient-perceived emotional bond rating and NED measure for therapist-to-patient and patient-perceived emotional bond rating,"The patient-to-therapist NED is negatively correlated with emotional bond, while therapist-to-patient NED does not have a significant correlation.  A t-SNE transformation of the difference of all patient-to-therapist turn embedding pairs is also used to visualize sessions with different emotional bond levels.
",0.2916666666666667,0.45161290322580644,0.35443037974683544,0.1276595744680851,0.2,0.1558441558441558,0.22916666666666666,0.3548387096774194,0.27848101265822783,0.028827505238626313,0.708590567111969,No,0.0,1.0,1.0
q398,What was their F1 score on the Bengali NER corpus?,52.0%,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1726384460926056,No,0.0,0.0,0.0
q399,What is the size of the dataset?,"300,000 sentences with 1.5 million single-quiz questions","The CNNDM dataset has 287,226 training documents, 13,368 validation documents, and 11,490 test documents.  The NYT50 dataset is derived from the CNNDM dataset.  The SCI dataset contains 220 sentences.  The STS Corpus dataset contains 17M training tokens.
",0.025,0.1,0.04000000000000001,0.0,0.0,0.0,0.025,0.1,0.04000000000000001,0.008522452865706931,0.3897760808467865,No,0.0,0.0,0.0
q400,How many examples do they have in the target domain?,"Around 388k examples, 194k from tst2013 (in-domain) and 194k from newstest2014 (out-of-domain)","Not found in the context.
",0.2,0.06666666666666667,0.1,0.0,0.0,0.0,0.2,0.06666666666666667,0.1,0.0,0.10888678580522537,No,0.0,0.0,0.0
q401,What is the baseline model?,a RNN-based seq2seq VC model called ATTS2S based on the Tacotron model,"Model 1
",0.5,0.07692307692307693,0.13333333333333336,0.0,0.0,0.0,0.5,0.07692307692307693,0.13333333333333336,0.0,0.2822732627391815,No,0.0,0.0,0.0
q402,Where did they get training data?,AmazonQA and ConciergeQA datasets,"From a corpus of approximately 300,000 sentences (yielding 1.5 million single-quiz question training examples), and from the organizers (2,250 users, 225,000 tweets).  Also, from anonymized user interaction data, real quizzes generated for a collection of several input video sources.  And from commercially available data, artificially inflated data, and labeled/self-labeled anonymized recognition requests.
",0.01694915254237288,0.25,0.031746031746031744,0.0,0.0,0.0,0.01694915254237288,0.25,0.031746031746031744,0.0053985482750549645,0.10374784469604492,No,0.0,0.0,0.0
q403,What extraction model did they use?,"Multi-Encoder, Constrained-Decoder model","Sequence-to-sequence model.
",0.25,0.2,0.22222222222222224,0.0,0.0,0.0,0.25,0.2,0.22222222222222224,0.0,0.2818068265914917,No,0.0,0.0,0.0
q404,Which datasets did they experiment on?,ConciergeQA and AmazonQA,"Action Adult Adventure Animation Biography Comedy Crime Documentary Drama Family Fantasy Film-Noir Game-Show History Horror Music Musical Mystery Reality-TV Romance Sci-Fi Short Sport Talk-Show Thriller War Western, Biomedical, novel-9
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08400735259056091,No,0.0,0.0,0.0
q405,How do slot binary classifiers improve performance?,by adding extra supervision to generate the slots that will be present in the response,"Slot binary classifiers resolve issues of TSCP by performing binary classification for each slot, eliminating undesired dependencies among slots.
",0.10526315789473684,0.13333333333333333,0.11764705882352941,0.0,0.0,0.0,0.10526315789473684,0.13333333333333333,0.11764705882352941,0.021597011339333432,0.31824803352355957,No,0.0,0.0,0.0
q406,What baselines have been used in this work?,"NDM, LIDM, KVRN, and TSCP/RL","Baseline 1, Baseline 2, Baseline 3, ANRES_Sing_R, ANRES_Sing_Z, ANRES_Seq_R, ANRES_Seq_Z, and a LSTM baseline.
",0.045454545454545456,0.16666666666666666,0.07142857142857144,0.0,0.0,0.0,0.045454545454545456,0.16666666666666666,0.07142857142857144,0.02304907678117588,0.33083298802375793,No,0.0,0.0,0.0
q407,what bottlenecks were identified?,Confusion in recognizing the words that are active at a given node by a speech recognition solution developed for Indian Railway Inquiry System.,"Recognition bottlenecks in a menu-based speech solution.
",0.625,0.21739130434782608,0.3225806451612903,0.0,0.0,0.0,0.5,0.17391304347826086,0.25806451612903225,0.011175866963886784,0.48385733366012573,Yes,1.0,1.0,1.0
q408,By how much do they outperform BiLSTMs in Sentiment Analysis?,Proposed RCRN outperforms ablative baselines BiLSTM by +2.9% and 3L-BiLSTM by +1.1% on average across 16 datasets.,"The context states the model outperforms *all* strong baselines, including BiLSTMs, but does not give a specific percentage improvement over BiLSTMs.  The answer is not found in the context.
",0.13793103448275862,0.2,0.16326530612244897,0.0,0.0,0.0,0.13793103448275862,0.2,0.16326530612244897,0.013897369362318638,0.604133665561676,No,0.0,0.0,0.0
q409,what state of the accuracy did they obtain?,51.5,"95.4%, 95.0%, 91%, 99%, 82.9%, 88%, 14.6%
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3325459659099579,No,0.0,0.0,0.0
q410,what models did they compare to?,"High-order CNN, Tree-LSTM, DRNN, DCNN, CNN-MC, NBoW and SVM ","NABERT+ baseline, Coref-GRU, MHPGM, Weaver, MHQA-GRN,  other models, Spacy, Stanford NER, Flair.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.025828020030551086,0.5289349555969238,No,0.0,0.0,0.0
q411,which benchmark tasks did they experiment on?, They used Stanford Sentiment Treebank benchmark for sentiment classification task and   AG English news corpus for the text classification task.,"Molecule generation, NER (Named Entity Recognition).
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.013967653161805977,0.22919988632202148,No,0.0,0.0,0.0
q412,How is the proficiency score calculated?,"They used 6 indicators for proficiency (same for written and spoken) each marked by bad, medium or good by one expert.","Each indicator is scored 0, 1, or 2 (bad, medium, good).  The total score is the sum of the scores for the 6 indicators.
",0.3333333333333333,0.38095238095238093,0.35555555555555557,0.08695652173913043,0.1,0.09302325581395349,0.16666666666666666,0.19047619047619047,0.17777777777777778,0.06612462998516985,0.5268851518630981,Yes,0.0,0.0,1.0
q413,What proficiency indicators are used to the score the utterances?,"6 indicators:
- lexical richness
- pronunciation and fluency
- syntactical correctness
- fulfillment of delivery
- coherence and cohesion
- communicative, descriptive, narrative skills","6 indicators.
",1.0,0.10526315789473684,0.1904761904761905,1.0,0.05555555555555555,0.10526315789473684,1.0,0.10526315789473684,0.1904761904761905,0.0,0.5036720037460327,No,0.0,1.0,1.0
q414,What accuracy is achieved by the speech recognition system?,"Accuracy not available: WER results are reported 42.6 German, 35.9 English","91.2% accuracy on utterances containing words, sentences, and conversations.  82.9% accuracy with a discrepancy measure.  80% accuracy in speaker-closed setting, and 60% in speaker-open setting.
",0.06896551724137931,0.15384615384615385,0.09523809523809525,0.0,0.0,0.0,0.06896551724137931,0.15384615384615385,0.09523809523809525,0.010559717178170818,0.43430155515670776,No,0.0,0.0,0.0
q415,How is the speech recognition system evaluated?,Speech recognition system is evaluated using WER metric.,"Generally, by putting it to actual use by intended users and analyzing logs of successful and unsuccessful transactions.  A method for evaluation without requiring human testing is also described.
",0.10344827586206896,0.375,0.16216216216216217,0.0,0.0,0.0,0.034482758620689655,0.125,0.05405405405405405,0.013794462224541233,0.2844931483268738,No,0.0,0.0,0.0
q416,How many of the utterances are transcribed?,Total number of transcribed utterances including Train and Test for both Eng and Ger language is 5562 (2188 cleaned),"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10197373479604721,No,0.0,0.0,0.0
q417,How many utterances are in the corpus?,Total number of utterances available is: 70607 (37344 ENG + 33263 GER),"The answer is not found in the context.
",0.125,0.09090909090909091,0.10526315789473685,0.0,0.0,0.0,0.125,0.09090909090909091,0.10526315789473685,0.02447822402834545,0.28772735595703125,No,1.0,1.0,1.0
q418,By how much does their model outperform both the state-of-the-art systems?,w.r.t Rouge-1 their model outperforms by 0.98% and w.r.t Rouge-L their model outperforms by 0.45%,"Our model outperforms the three baseline systems on all evaluation metrics.  Specifically, our model achieves improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford system, and 2.62% and 3.02% on CoNLL F1 score over the MIR model.
",0.09090909090909091,0.17391304347826086,0.11940298507462685,0.023255813953488372,0.045454545454545456,0.03076923076923077,0.09090909090909091,0.17391304347826086,0.11940298507462685,0.025581743419597534,0.5064241886138916,No,0.0,0.0,0.0
q419,What is the state-of-the art?,neural attention model with a convolutional encoder with an RNN decoder and RNN encoder-decoder,"The best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks.
",0.14285714285714285,0.13333333333333333,0.1379310344827586,0.0,0.0,0.0,0.07142857142857142,0.06666666666666667,0.0689655172413793,0.03408991996483855,0.14022524654865265,No,0.0,0.0,0.0
q420,What was the performance on the self-collected corpus?,F1 scores of 86.16 on slot filling and 94.56 on intent detection,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12718303501605988,No,0.0,0.0,0.0
q421,What is the size of their dataset?,"10,001 utterances","17M training tokens, 44k vocabulary size.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.44909903407096863,No,0.0,0.0,0.0
q422,how was the dataset built?,"Questions are gathered from anonymized, aggregated queries to the Google search engine. Queries that are likely to be yes/no questions are heuristically identified: we found selecting queries where the first word is in a manually constructed set of indicator words and are of sufficient length, to be effective. Questions are only kept if a Wikipedia page is returned as one of the first five results, in which case the question and Wikipedia page are given to a human annotator for further processing. Annotators label question/article pairs in a three-step process. First, they decide if the question is good, meaning it is comprehensible, unambiguous, and requesting factual information. This judgment is made before the annotator sees the Wikipedia page. Next, for good questions, annotators find a passage within the document that contains enough information to answer the question. Annotators can mark questions as “not answerable"" if the Wikipedia article does not contain the requested information. Finally, annotators mark whether the question's answer is “yes"" or “no""","The methods for dataset creation vary.  Some datasets were constructed by filtering generated datasets, trimming to the original size, and ensuring equal representation of labels.  Other datasets were scraped from websites (PDF files), converted to text, and then parsed for relevant data using pattern matching.  Still other datasets were sourced from existing corpora.
",0.22641509433962265,0.07100591715976332,0.10810810810810811,0.019230769230769232,0.005952380952380952,0.00909090909090909,0.1509433962264151,0.047337278106508875,0.07207207207207207,0.003105444036148844,0.311916708946228,No,0.0,1.0,1.0
q423,what processing was done on the speeches before being parsed?,Remove numbers and interjections,"Punctuation, XML characters, hyperlinks, Twitter-specific ""at-mentions"" and ""hashtags"" were removed.  Speeches were broken into blocks of about 5 minutes each, and concatenated utterances were transcribed.
",0.07407407407407407,0.5,0.12903225806451613,0.0,0.0,0.0,0.07407407407407407,0.5,0.12903225806451613,0.010559717178170818,0.2682895362377167,No,0.0,0.0,0.0
q424,What is the performance change of the textual semantic similarity task when no error and maximum errors (noise) are present?,"10 Epochs: pearson-Spearman correlation  drops  60 points when error increase by 20%
50 Epochs: pearson-Spearman correlation  drops  55 points when error increase by 20%","The context states that as the percentage of error increases, there is a significant drop in performance for the textual semantic similarity task.  However, it does not specify the exact performance levels for zero error or maximum error.
",0.10526315789473684,0.15384615384615385,0.125,0.02702702702702703,0.04,0.03225806451612903,0.10526315789473684,0.15384615384615385,0.125,0.01038539619163422,0.33642715215682983,No,0.0,0.0,1.0
q425,Which sentiment analysis data set has a larger performance drop when a 10% error is introduced?,SST-2 dataset,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.053742747753858566,No,0.0,0.0,0.0
q426,How much is pre-training loss increased in Low/Medium/Hard level of pruning?,"The increase is linearly from lowest on average 2.0 , medium around 3.5, and the largest is 6.0","Low levels (30-40%): no increase.
Medium levels: increases.
Hard levels: not specified.
",0.15384615384615385,0.1,0.12121212121212123,0.0,0.0,0.0,0.15384615384615385,0.1,0.12121212121212123,0.016466642419110007,0.679631769657135,No,0.0,0.0,0.0
q427,What is the average length of the recordings?,40 minutes,"10.4 seconds (with a standard deviation of 2.3 seconds)
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5150466561317444,No,0.0,0.0,0.0
q428,What were their results?,Best model achieved F-score 74.7 on NALCS and F-score of 70.0 on LMS on test set,"The method achieved good rankings in three international contests (TASS'15, SemEval'15-16, and SENTIPOLC'14).  Excellent results were obtained in five other languages.  On newstest2019, results were up to 0.85 BLEU points better than the Baseline;  newstest2017 saw an improvement of 0.18 BLEU points with statistical significance; newstest2018, an improvement of 0.15 BLEU points without statistical significance.  Emergency Resolved Dialogues scored significantly higher than Emergency Not Resolved Dialogues in a study.  Results for English-French translation are also positive (Document system obtained the...).
",0.06818181818181818,0.3,0.11111111111111109,0.0,0.0,0.0,0.045454545454545456,0.2,0.07407407407407407,0.004724186878758777,0.4542448818683624,No,0.0,0.0,0.0
q429,What is the prediction accuracy of the model?,"mean prediction accuracy 0.99582651
S&P 500 Accuracy 0.99582651","Training: 69.9%
Validation: 64.7%
Test: 65.3% (binary classification)

82.9% (synchronisation offset)
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.38733941316604614,No,1.0,1.0,1.0
q430,What is the dataset used in the paper?,"historical S&P 500 component stocks
 306242 news articles","Biomedical, SCI, novel-9, CNNDM, NYT50
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3132098317146301,No,0.0,0.0,0.0
q431,How does the SCAN dataset evaluate compositional generalization?,"it systematically holds out inputs in the training set containing basic primitive verb, ""jump"", and tests on sequences containing that verb.","The SCAN dataset tests the ability to generalize known primitive verbs to valid unseen constructions, and  later augmented tests included compositional generalization based on known templates.  It uses splits with more than one example of the held-out primitive verb, where training sets include varying numbers of random samples of command sequences.
",0.23076923076923078,0.5714285714285714,0.3287671232876712,0.0392156862745098,0.1,0.05633802816901408,0.1346153846153846,0.3333333333333333,0.19178082191780818,0.037680102057097764,0.5935527682304382,Yes,0.0,0.0,0.0
q432,How much does this system outperform prior work?,"The system outperforms by 27.7% the LSTM model, 38.5% the RL-SPINN model and 41.6% the Gumbel Tree-LSTM","The system outperforms the three baseline systems on all evaluation metrics, with improvements of 2.93% and 3.01% on CoNLL F1 score.  Additional improvements, relative to 300h setups, are also mentioned.
",0.15625,0.22727272727272727,0.18518518518518515,0.06451612903225806,0.09523809523809523,0.07692307692307691,0.15625,0.22727272727272727,0.18518518518518515,0.04739796546500555,0.6113978028297424,No,0.0,0.0,0.0
q433,What are the baseline systems that are compared against?,"The system is compared to baseline models: LSTM, RL-SPINN and Gumbel Tree-LSTM","Baseline 1: smooth L1 distance; Baseline 2: PCA-based symmetric acoustic similarity; Baseline 3: Nonlinear dynamical systems-based complexity measure; Stanford system; Multigraph system; MIR system; Tactron 2.
",0.07142857142857142,0.14285714285714285,0.09523809523809523,0.0,0.0,0.0,0.03571428571428571,0.07142857142857142,0.047619047619047616,0.012193311110766474,0.42640599608421326,No,0.0,0.0,0.0
q434,What systems are tested?,"BULATS i-vector/PLDA
BULATS x-vector/PLDA
VoxCeleb x-vector/PLDA
PLDA adaptation (X1)
 Extractor fine-tuning (X2) ","Support Vector Machines (SVMs), logistic regression, Naïve Bayes, Perceptron, and decision trees;  neural network models.
",0.0625,0.05263157894736842,0.05714285714285714,0.0,0.0,0.0,0.0625,0.05263157894736842,0.05714285714285714,0.019582248325011244,0.23699520528316498,No,0.0,0.0,0.0
q435,What benchmark datasets they use?,VQA and GeoQA,"Stanford Sentiment Treebank (SST-2), Yelp dataset, Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), Quora Question Pairs (QQP), SQuAD 1.1, SQuAD 2.0, MRQA, IMDb, QA4IE-SPAN-S/M/L, QA4IE-SEQ-S/M/L.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11570996046066284,No,0.0,0.0,0.0
q436,How do they combine MonaLog with BERT?,They use Monalog for data-augmentation to fine-tune BERT on this task,"MonaLog is followed by the re-trained BERT.
",0.25,0.15384615384615385,0.1904761904761905,0.0,0.0,0.0,0.25,0.15384615384615385,0.1904761904761905,0.037954847898457066,0.702785074710846,Yes,0.0,0.0,0.0
q437,How do they select monotonicity facts?,They derive it from Wordnet,"They use automatically generated datasets (HELP), crowdsourced datasets, and examples from previous manually curated datasets and linguistics publications.  They also consider examples from previous linguistics publications and manually curated datasets like FraCaS and GLUE.
",0.058823529411764705,0.4,0.10256410256410256,0.0,0.0,0.0,0.058823529411764705,0.4,0.10256410256410256,0.01092534162077208,0.40507790446281433,No,0.0,0.0,0.0
q438,What are the 12 categories devised?,"Economics, Genocide, Geography, History, Human Rights, Kurdish, Kurdology, Philosophy, Physics, Theology, Sociology, Social Study","The provided text mentions 15 hand-coded categories, and a 44-community partition, but does not explicitly state 12 categories.  The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.013794462224541233,0.16344131529331207,No,0.0,0.0,0.0
q439,what are the off-the-shelf systems discussed in the paper?,"Answer with content missing: (Names of many identifiers missing) TextCat, ChromeCLD, LangDetect, langid.py, whatlang, whatthelang, YALI, LDIG, Polyglot 3000, Lextek Language Identifier and Open Xerox Language Identifier.","IBREF28, Latent-Tree, Berkeley, LaSO, Latent-Strc, Model-Stack, Non-Linear, and the first CRF.
",0.06666666666666667,0.03571428571428571,0.046511627906976744,0.0,0.0,0.0,0.06666666666666667,0.03571428571428571,0.046511627906976744,0.010450618213495425,0.37557724118232727,No,0.0,0.0,0.0
q440,How many rules had to be defined?,"WikiSQL - 2 rules (SELECT, WHERE)
SimpleQuestions - 1 rule
SequentialQA - 3 rules (SELECT, WHERE, COPY)","Ten
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.033411670476198196,No,0.0,0.0,0.0
q441,What was performance of classifiers before/after using distant supervision?,"Bi-LSTM: For low resource <17k clean data: Using distant supervision resulted in huge boost of F1 score (1k eg. ~9 to ~36 wit distant supervision)
BERT: <5k clean data boost of F1 (1k eg. ~32 to ~47 with distant supervision)","Before distant supervision, the performance is not specified.  After, the model's performance improves, often doubling a class's F1-score, and distant supervision sometimes improves a model's performance in the 1k and 2k data sets.  However, if the model trained on clean data performs better than one trained on clean data + distant supervision, then the distant supervision causes overfitting.
",0.22950819672131148,0.34146341463414637,0.27450980392156865,0.1,0.15,0.12,0.16393442622950818,0.24390243902439024,0.19607843137254902,0.026348495048119727,0.47808638215065,No,0.0,1.0,1.0
q442,How big are the datasets used?,"Evaluation datasets used:
CMRC 2018 - 18939 questions, 10 answers
DRCD - 33953 questions, 5 answers
NIST MT02/03/04/05/06/08 Chinese-English - Not specified

Source language train data:
SQuAD - Not specified","Not found in the context.
",0.2,0.03125,0.05405405405405406,0.0,0.0,0.0,0.2,0.03125,0.05405405405405406,0.00014416095896537825,0.23379558324813843,No,0.0,0.0,0.0
q443,What datasets are used for training/testing models? ,"Microsoft Research dataset containing movie, taxi and restaurant domains.","The context mentions datasets like MSCOCO, Oxford-102, MNIST, CUB, Oxford, COCO, CIFAR-10, and Swissmetro, but does not specify which are used for training and testing in *all* cases.  It also references Tables TABREF12, TABREF19, TABREF83, and TABREF10, which likely contain the needed specifics.
",0.06666666666666667,0.3333333333333333,0.1111111111111111,0.0,0.0,0.0,0.044444444444444446,0.2222222222222222,0.07407407407407407,0.008093746504746997,0.29538944363594055,No,0.0,0.0,0.0
q444,How better is gCAS approach compared to other approaches?,"For entity  F1 in the movie, taxi and restaurant domain it results in scores of 50.86, 64, and 60.35. For success, it results it outperforms in the movie and restaurant domain with scores of 77.95 and 71.52","gCAS outperforms all other methods, especially in predicting slots consistent with the act.  Its performance is particularly strong on Entity F$_1$ and Success F$_1$, though act-slot pair predictions aren't perfect.
",0.20588235294117646,0.17073170731707318,0.18666666666666668,0.0,0.0,0.0,0.11764705882352941,0.0975609756097561,0.10666666666666667,0.014687168144827003,0.33319416642189026,Yes,0.0,0.0,0.0
q445,What is specific to gCAS cell?,"It has three sequentially connected units to output continue, act and slots generating multi-acts in a doble recurrent manner.","gCAS uses a recurrent cell with separate ""continue,"" ""act,"" and ""slot"" units connected sequentially.  It allows the decoder to output a tuple at each step.
",0.48,0.6,0.5333333333333332,0.16666666666666666,0.21052631578947367,0.186046511627907,0.2,0.25,0.22222222222222224,0.040102192774550585,0.5186738967895508,Yes,1.0,1.0,1.0
q446,What is the source of external knowledge?,counts of predicate-argument tuples from English Wikipedia,"The context states that external knowledge comes from ""different knowledge sources.""  It gives examples like parsing errors and incorrectly identified relations, but doesn't specify the *type* of sources.
",0.06896551724137931,0.25,0.1081081081081081,0.0,0.0,0.0,0.034482758620689655,0.125,0.05405405405405405,0.012557690800697192,0.08937834948301315,No,0.0,0.0,0.0
q447,What were the sizes of the test sets?,Test set 1 contained 57 drug labels and 8208 sentences and test set 2 contained 66 drug labels and 4224 sentences,"1585 IPUs for one test set, 1841 IPUs for another.  A test set of 100 dialogs with four turns is also mentioned.
",0.18181818181818182,0.19047619047619047,0.18604651162790697,0.09523809523809523,0.1,0.0975609756097561,0.18181818181818182,0.19047619047619047,0.18604651162790697,0.033337449830272974,0.3235176205635071,No,0.0,0.0,0.0
q448,Which datasets are used?,"ABSA SemEval 2014-2016 datasets
Yelp Academic Dataset
Wikipedia dumps","Stanford Sentiment Treebank (SST-2), Yelp, Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), Quora Question Pairs, SCI, VQA Dataset, Visual7W, Visual Genome, and INLINEFORM0.
",0.07142857142857142,0.2,0.10526315789473682,0.0,0.0,0.0,0.07142857142857142,0.2,0.10526315789473682,0.01038539619163422,0.5204460620880127,No,0.0,0.0,0.0
q449,How much does it minimally cost to fine-tune some model according to benchmarking framework?,"$1,728","A few minutes to hours.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0801490843296051,No,0.0,0.0,0.0
q450,What models are included in baseline benchmarking results?,"BERT, XLNET RoBERTa, ALBERT, DistilBERT","Baseline 1, Baseline 2, Baseline 3.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05669791110976001,0.10246112942695618,No,0.0,0.0,0.0
q451,"It looks like learning to paraphrase questions, a neural scoring model and a answer selection model cannot be trained end-to-end. How are they trained?",using multiple pivot sentences,"The entire system, including the paraphrase generation, scoring model, and QA model, is trained end-to-end using question-answer pairs.
",0.047619047619047616,0.25,0.08,0.0,0.0,0.0,0.047619047619047616,0.25,0.08,0.016466642419110007,0.11957179009914398,No,0.0,0.0,0.0
q452,How much more accurate is the model than the baseline?,"For the Oshiete-goo dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, Trans, by 0.021, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.037.  For the nfL6 dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, CLSTM, by 0.028, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.040. Human evaluation of the NAGM's generated outputs for the Oshiete-goo dataset had 47% ratings of (1), the highest rating, while CLSTM only received 21% ratings of (1). For the nfL6 dataset, the comparison of (1)'s was NAGM's 50% to CLSTM's 30%. ","The model outperforms the baseline by 35% in Exact Match and 25% in Goal Match in previously seen environments.  It also performs better on ""What"" questions, with 30 fewer incorrect outputs.  In new environments, the model significantly outperforms the second-best model.
",0.2857142857142857,0.096,0.1437125748502994,0.0,0.0,0.0,0.23809523809523808,0.08,0.11976047904191615,0.004548967347164031,0.5512735247612,No,0.0,0.0,0.0
q453,What is new state-of-the-art performance on CoNLL-2009 dataset?,In closed setting 84.22 F1 and in open 87.35 F1.,"The answer is not found in the context.
",0.125,0.08333333333333333,0.1,0.0,0.0,0.0,0.125,0.08333333333333333,0.1,0.045400138092837265,0.3038182556629181,No,0.0,0.0,0.0
q454,What are two strong baseline methods authors refer to?,Marcheggiani and Titov (2017) and Cai et al. (2018),"Baseline 1 and Baseline 2.
",0.2,0.1111111111111111,0.14285714285714285,0.0,0.0,0.0,0.2,0.1111111111111111,0.14285714285714285,0.02544354209531657,0.09049217402935028,No,0.0,0.0,0.0
q455,How many category tags are considered?,14 categories,"100
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05683956667780876,No,0.0,0.0,0.0
q456,What domain does the dataset fall into?,YouTube videos,"The provided texts describe multiple domains, including Lifestyle, patent, scientific, and Wikipedia.  No single dataset is definitively identified.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.060621228069067,No,0.0,0.0,0.0
q457,How are EAC evaluated?,"Qualitatively through efficiency, effectiveness and satisfaction aspects and quantitatively through metrics such as precision, recall, accuracy, BLEU score and even human judgement.","EAC performance is evaluated using qualitative and quantitative assessment techniques.  Qualitative assessment uses ISO 9241, while quantitative analysis uses unspecified techniques.
",0.14285714285714285,0.13636363636363635,0.1395348837209302,0.05,0.047619047619047616,0.04878048780487805,0.14285714285714285,0.13636363636363635,0.1395348837209302,0.01827272198658944,0.5087496638298035,No,1.0,1.0,1.0
q458,What is triangulation?,"Answer with content missing: (Chapter 3) The concept can be easily explained with an example, visualized in Figure 1. Consider the Portuguese (Pt) word trabalho which, according to the MUSE Pt–En dictionary, has the words job and work as possible En translations. In turn, these two En words can be translated to 4 and 5 Czech (Cs) words respectively. By utilizing the transitive property (which translation should exhibit) we can identify the set of 7 possible Cs translations for the Pt word trabalho.","A method to extend MUSE dictionaries to an additional 2352 lexicons covering 49 languages.
",0.35714285714285715,0.05952380952380952,0.1020408163265306,0.0,0.0,0.0,0.2857142857142857,0.047619047619047616,0.08163265306122448,0.00013594382026076122,0.3792802691459656,No,0.0,0.0,0.0
q459,What languages do they use?,"Train languages are: Cantonese, Bengali, Pashto, Turkish, Vietnamese, Haitian, Tamil, Kurdish, Tokpisin and Georgian, while Assamese, Tagalog, Swahili, Lao are used as target languages.","Amharic, Arabic, Hindi, Somali, English, Spanish, French, Italian, German, Russian, Basque, French, German, Hungarian, Italian, Navajo, Turkish, Quechua, English, Spanish, Zulu, Russian, Polish, Norwegian, Romanian, Dutch, Czech, Turkish, Thai, Urdu,  English, French, German.
",0.030303030303030304,0.041666666666666664,0.03508771929824561,0.0,0.0,0.0,0.030303030303030304,0.041666666666666664,0.03508771929824561,0.029994218786276502,0.6903309226036072,No,0.0,0.0,0.0
q460,How they evaluate their approach?,"They  evaluate newly proposed models in several low-resource settings across different languages with real, distantly supervised data with non-synthetic noise","Evaluation using cross-domain scenarios, comparing estimates with actual attack effects, studying adversarial attack effects on evaluation metrics, exploring application to KG representation interpretability, detecting incorrect triples, and leave-one evaluation.  Human evaluation for NLG, and examining the impact of imperfect metrics on training NLG models.  Also, evaluation of the quality of approximations and comparison with influence function (IF).
",0.06779661016949153,0.18181818181818182,0.09876543209876544,0.0,0.0,0.0,0.05084745762711865,0.13636363636363635,0.07407407407407408,0.0074084238110654985,0.38456451892852783,No,0.0,0.0,0.0
q461,How large is the corpus?,"It contains 106,350 documents","13 million NRLS records.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.24316242337226868,No,0.0,0.0,0.0
q462,How large is the dataset?,over 104k documents,"The answer is not found in the context.  While various datasets and their sizes are mentioned, there is no single answer to ""how large is the dataset.""
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.202107772231102,No,0.0,0.0,0.0
q463,What was their perplexity score?,Perplexity score 142.84 on dev and 138.91 on test,"The answer is not found in the context.  While perplexity scores are discussed, no specific score is given.
",0.1111111111111111,0.18181818181818182,0.13793103448275862,0.058823529411764705,0.1,0.07407407407407408,0.1111111111111111,0.18181818181818182,0.13793103448275862,0.018160849415439308,0.6749700307846069,No,0.0,0.0,0.0
q464,What parallel corpus did they use?,Parallel monolingual corpus in English and Mandarin,"The parallel corpora from the shared translation task of WMT'15 and WMT'16 were used.  Also, Fr*-De and Fr-De* corpora.
",0.08695652173913043,0.2857142857142857,0.13333333333333333,0.0,0.0,0.0,0.08695652173913043,0.2857142857142857,0.13333333333333333,0.015061893230938673,0.35733476281166077,No,0.0,0.0,0.0
q465,In which setting they achieve the state of the art?,in open-ended task esp. for counting-type questions ,"SemEval-2015 and SemEval-2016.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.058763508032616325,0.04884079843759537,No,0.0,0.0,0.0
q466,What they formulate the question generation as?,LASSO optimization problem,"The context does not state what they formulate question generation *as*.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.04328599572181702,No,0.0,0.0,0.0
q467,"Was the degree of offensiveness taken as how generally offensive the text was, or how personally offensive it was to the annotator?",Personal thought of the annotator.,"The offensiveness was rated on a 6-point Likert scale.
",0.1,0.2,0.13333333333333333,0.0,0.0,0.0,0.1,0.2,0.13333333333333333,0.033864985683445356,0.08967381715774536,No,0.0,0.0,0.0
q468,Which embeddings do they detect biases in?,Word embeddings trained on GoogleNews and Word embeddings trained on Reddit dataset,"Mainstream, striking examples from the literature on embedding bias.
",0.2222222222222222,0.16666666666666666,0.1904761904761905,0.0,0.0,0.0,0.2222222222222222,0.16666666666666666,0.1904761904761905,0.034220976227266095,0.3054564595222473,No,0.0,0.0,0.0
q469,What are the two PharmaCoNER subtasks?,Entity identification with offset mapping and concept indexing,"The context describes two PharmaCoNER subtasks, but does not name them.  It only mentions that one is dealing with the annotation of entities and the other is about evaluating dataset quality and the role of resource-based systems in validation.
",0.075,0.375,0.12499999999999999,0.0,0.0,0.0,0.05,0.25,0.08333333333333334,0.01038539619163422,0.12173408269882202,No,0.0,0.0,0.0
q470,How do they perform data augmentation?,They randomly sample sentences from Wikipedia that contains an object RC and add them to training data,"Mirroring, random cropping, and replacement-based methods (replacing words with synonyms) are used.  Frequency masking, speed perturbation, and volume perturbation are also employed.
",0.043478260869565216,0.058823529411764705,0.049999999999999996,0.0,0.0,0.0,0.043478260869565216,0.058823529411764705,0.049999999999999996,0.012414943415352929,0.3174320161342621,No,0.0,0.0,0.0
q471,What are the characteristics of the city dialect?,Lexicon of the cities tend to use most forms of a particular concept,"The city descriptions are relatively short, contain rare words, complex syntactic structures, and figurative language.
",0.13333333333333333,0.15384615384615385,0.14285714285714288,0.07142857142857142,0.08333333333333333,0.07692307692307691,0.13333333333333333,0.15384615384615385,0.14285714285714288,0.0,0.5497179627418518,No,0.0,0.0,0.0
q472,What are the characteristics of the rural dialect?,It uses particular forms of a concept rather than all of them uniformly,"Rural regions, represented by a purple background on the maps, have more specific words in their lexicon.
",0.058823529411764705,0.07692307692307693,0.06666666666666667,0.0,0.0,0.0,0.058823529411764705,0.07692307692307693,0.06666666666666667,0.019146030690102515,0.22001412510871887,No,0.0,1.0,1.0
q473,What is the performance of the models on the tasks?,"Overall accuracy per model is: 5-gram (60.5), LSTM (68.9), TXL (68.7), GPT-2 (80.1)","The best model obtained 42.48% accuracy for age, 37.50% for dialect, and 57.81% for gender.  Other results are mentioned, but not in a summarized form.
",0.07142857142857142,0.10526315789473684,0.0851063829787234,0.0,0.0,0.0,0.03571428571428571,0.05263157894736842,0.0425531914893617,0.016794256847485506,0.6353577375411987,No,0.0,0.0,0.0
q474,What other non-neural baselines do the authors compare to? ,"bag of words, tf-idf, bag-of-means","PCA-based acoustic similarity measure (Lee et al. BIBREF10) and a nonlinear dynamical systems-based complexity measure (BIBREF6).
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10689083486795425,No,0.0,0.0,0.0
q475,On what dataset is Aristo system trained?,"Aristo Corpus
Regents 4th
Regents 8th
Regents `12th
ARC-Easy
ARC-challenge ","Several datasets of independently-authored science questions taken from standardized tests.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.20727182924747467,No,0.0,1.0,1.0
q476,How many roles are proposed?,12,"The context does not specify a precise number of proposed roles.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1698548048734665,No,0.0,0.0,0.0
q477,What language technologies have been introduced in the past?,"- Font & Keyboard
- Speech-to-Text
- Text-to-Speech
- Text Prediction
- Spell Checker
- Grammar Checker
- Text Search
- Machine Translation
- Voice to Text Search
- Voice to Speech Search","Statistical modeling approaches, linguistic theory based approaches, deep neural networks, and Transformer-based architectures.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4438280463218689,No,0.0,0.0,0.0
q478,How do they define local variance?,The reciprocal of the variance of the attention distribution,"Local variance loss is defined as the reciprocal of the variance of the attention distribution, calculated using the median value instead of the mean, to encourage the model to focus on more salient parts of the input.
",0.24324324324324326,1.0,0.391304347826087,0.2222222222222222,1.0,0.3636363636363636,0.24324324324324326,1.0,0.391304347826087,0.1663603558532716,0.5547320246696472,Yes,1.0,1.0,1.0
q479,how do they measure discussion quality?,"Measuring three aspects: argumentation, specificity and knowledge domain.","They measure discussion quality using students' argument moves (claims and textual evidence), argumentation, specificity, and knowledge domain.  Inter-rater reliability is assessed using Cohen's kappa.  Expert evaluations are also used for validity comparison (quadratic-weighted kappa).
",0.16216216216216217,0.75,0.26666666666666666,0.1111111111111111,0.5714285714285714,0.18604651162790697,0.16216216216216217,0.75,0.26666666666666666,0.0753457490838286,0.5702138543128967,Yes,1.0,1.0,1.0
q480,what were the baselines?,"2008 Punyakanok et al. 
2009 Zhao et al. + ME 
2008 Toutanova et al. 
2010 Bjorkelund et al.  
2015 FitzGerald et al. 
2015 Zhou and Xu 
2016 Roth and Lapata 
2017 He et al. 
2017 Marcheggiani et al.
2017 Marcheggiani and Titov 
2018 Tan et al. 
2018 He et al. 
2018 Strubell et al. 
2018 Cai et al. 
2018 He et al. 
2018 Li et al. 
","Baseline 1: smooth L1 distance; Baseline 2: PCA-based acoustic similarity; Baseline 3: Nonlinear dynamical systems-based complexity; ANRES_Sing_R; ANRES_Sing_Z; ANRES_Seq_R; ANRES_Seq_Z
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.23547521233558655,No,0.0,0.0,0.0
q481,Which soft-selection approaches are evaluated?,LSTM and BERT ,"Traditional attention-based soft-selection methods.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09196469187736511,No,0.0,0.0,0.0
q482,How big is slot filing dataset?,"Dataset has 1737 train, 497 dev and 559 test sentences.","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03416211359799978,0.20938529074192047,No,0.0,0.0,0.0
q483,How large is the dataset they generate?,4.756 million sentences,"287,226 training documents, 13,368 validation documents, and 11,490 test documents.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3228995203971863,No,0.0,0.0,0.0
q484,What are the weaknesses of their proposed interpretability quantification method?,can be biased by dataset used and may generate categories which are suboptimal compared to human designed categories,"The provided context does not describe a proposed interpretability quantification method.  Therefore, weaknesses cannot be determined.
",0.0625,0.05555555555555555,0.058823529411764705,0.0,0.0,0.0,0.0625,0.05555555555555555,0.058823529411764705,0.020244462660665506,0.3258708417415619,No,0.0,0.0,0.0
q485,What advantages does their proposed method of quantifying interpretability have over the human-in-the-loop evaluation they compare to?,it is less expensive and quantifies interpretability using continuous values rather than binary evaluations,"The provided text does not describe a method for quantifying interpretability.  It describes a comparison-based evaluation metric for machine translation that correlates better with human judgment than existing metrics.  There is no comparison between this method and a human-in-the-loop evaluation method in the context of interpretability.
",0.14,0.5,0.21875000000000003,0.02040816326530612,0.07692307692307693,0.03225806451612903,0.08,0.2857142857142857,0.125,0.01052923339622545,0.34679996967315674,No,0.0,0.0,0.0
q486,How was lexical diversity measured?,By computing number of unique responses and number of responses divided by the number of unique responses to that question for each of the questions,"Yule's I measure and the Measure of Textual Lexical Diversity (MTLD).
",0.25,0.12,0.16216216216216217,0.0,0.0,0.0,0.25,0.12,0.16216216216216217,0.017035018586323527,0.14360061287879944,No,0.0,0.0,0.0
q487,Which existing models does this approach outperform?,"RNN-context, SRB, CopyNet, RNN-distract, DRGD","Regression-based approaches, manually defined knowledge and features, and several state-of-the-art NLI models (BiMPM, ESIM, Decomposable Attention Model, KIM).
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02042946039568498,0.19157391786575317,No,0.0,0.0,0.0
q488,What human evaluation method is proposed?,comparing the summary with the text instead of the reference and labeling the candidate bad if it is incorrect or irrelevant,"Annotators evaluate the summary against the source content based on the goodness of the summary.
",0.4,0.2857142857142857,0.3333333333333333,0.14285714285714285,0.1,0.11764705882352941,0.3333333333333333,0.23809523809523808,0.2777777777777778,0.05600607423889073,0.5193849205970764,Yes,1.0,1.0,1.0
q489,What languages are represented in the dataset?,"EN, JA, ES, AR, PT, KO, TH, FR, TR, RU, IT, DE, PL, NL, EL, SV, FA, VI, FI, CS, UK, HI, DA, HU, NO, RO, SR, LV, BG, UR, TA, MR, BN, IN, KN, ET, SL, GU, CY, ZH, CKB, IS, LT, ML, SI, IW, NE, KM, MY, TL, KA, BO","English, French, German, Spanish, Russian, Italian.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.5768763301763e-05,0.5099040865898132,No,0.0,0.0,0.0
q490,How faster is training and decoding compared to former models?,"Proposed vs best baseline:
Decoding: 8541 vs 8532 tokens/sec
Training: 8h vs 8h","The provided text states that the model results in faster decoding time, but does not compare it to other models in a way that quantifies the speed difference.  The answer is not found in the context.
",0.027777777777777776,0.07142857142857142,0.04,0.0,0.0,0.0,0.027777777777777776,0.07142857142857142,0.04,0.0,0.4407627582550049,No,1.0,1.0,1.0
q491,What datasets are used to evaluate the model?,WN18 and FB15k,"Oshiete-goo dataset, nfL6 dataset, and corpora listed in Table TABREF12.
",0.09090909090909091,0.3333333333333333,0.14285714285714288,0.0,0.0,0.0,0.09090909090909091,0.3333333333333333,0.14285714285714288,0.030890553181566974,0.17078492045402527,No,0.0,0.0,0.0
q492,What is the source of the dataset?,"Online sites tagged as fake news site by Verafiles and NUJP and news website in the Philippines, including Pilipino Star Ngayon, Abante, and Bandera","The dataset sources vary.  Some come from online debate websites, the OA-STM corpus, the testing images of MS COCO dataset, and the testing questions of US DARPA.
",0.14285714285714285,0.16666666666666666,0.15384615384615383,0.0,0.0,0.0,0.14285714285714285,0.16666666666666666,0.15384615384615383,0.029171735901692596,0.2400885820388794,No,0.0,0.0,0.0
q493,What were the baselines?,"Siamese neural network consisting of an embedding layer, a LSTM layer and a feed-forward layer with ReLU activations","Baseline 1: smooth L1 distance; Baseline 2: PCA-based acoustic similarity; Baseline 3: Nonlinear dynamical systems-based complexity; ANRES_Sing_R; ANRES_Sing_Z; ANRES_Seq_R; ANRES_Seq_Z
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2363472729921341,No,0.0,0.0,0.0
q494,What metadata is included?,"besides claim, label and claim url, it also includes a claim ID, reason, category, speaker, checker, tags, claim entities, article title, publish data and claim date","Normalized representation of the service interface (name, tasks, entities, descriptions).
",0.1,0.038461538461538464,0.05555555555555555,0.0,0.0,0.0,0.1,0.038461538461538464,0.05555555555555555,0.016664939497456552,0.300778329372406,No,0.0,0.0,0.0
q495,Do the images have multilingual annotations or monolingual ones?,monolingual,"Monolingual.
",1.0,1.0,1.0,0.0,0.0,0.0,1.0,1.0,1.0,0.0,0.9461771249771118,Yes,0.0,0.0,1.0
q496,How much important is the visual grounding in the learning of the multilingual representations?,performance is significantly degraded without pixel data,"The provided text states that image representations leverage generic image features, but does not explicitly state the importance of visual grounding in learning multilingual representations.  It is implied that visual grounding is a factor but not the primary focus of the investigation.
",0.023809523809523808,0.14285714285714285,0.04081632653061224,0.0,0.0,0.0,0.023809523809523808,0.14285714285714285,0.04081632653061224,0.008130350574508933,0.1860819160938263,No,0.0,0.0,0.0
q497,How is the generative model evaluated?,Comparing BLEU score of model with and without attention,"A classifier is trained on the generated dataset, and its accuracy on the original test set is compared to the accuracy of a classifier trained on the original training set and tested on the original test set.
",0.08108108108108109,0.3333333333333333,0.13043478260869568,0.0,0.0,0.0,0.08108108108108109,0.3333333333333333,0.13043478260869568,0.011216940038341035,0.2001863718032837,No,0.0,0.0,0.0
q498,What is an example of a health-related tweet?,"The health benefits of alcohol consumption are more limited than previously thought, researchers say","The context provides ""diabetes, diet, exercise, and obesity"" as examples of health-related topics discussed in tweets.
",0.17647058823529413,0.21428571428571427,0.1935483870967742,0.0,0.0,0.0,0.11764705882352941,0.14285714285714285,0.12903225806451615,0.022731543567022873,0.2701731324195862,No,0.0,0.0,0.0
q499,What is the challenge for other language except English,not researched as much as English,"The provided text describes challenges for Spanish, Italian, and Zulu, as well as comparisons with results from other languages in international contests.  It also details a challenge focused on non-native children's speech recognition, with an English subset.  No single, overarching ""challenge for other languages"" is described.
",0.0625,0.5,0.1111111111111111,0.0,0.0,0.0,0.0625,0.5,0.1111111111111111,0.008539456561476954,0.27927663922309875,No,0.0,0.0,1.0
q500,How many categories of offensive language were there?,3,"Three
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.85391765832901,Yes,0.0,0.0,0.0
q501,Which matching features do they employ?,Matching features from matching sentences from various perspectives.,"INLINEFORM6, INLINEFORM7, INLINEFORM8, INLINEFORM9, INLINEFORM10, Brown BIBREF32 clusters, Clark BIBREF33 clusters, Word2vec BIBREF34 clusters, orthographic, word shape, n-gram features.
",0.05,0.125,0.07142857142857144,0.0,0.0,0.0,0.05,0.125,0.07142857142857144,0.014763939041893883,0.21849669516086578,No,0.0,0.0,0.0
q502,By how much does their method outperform the multi-head attention model?,Their average improvement in Character Error Rate over the best MHA model was 0.33 percent points.,"The context does not contain the performance figures of a *single* multi-head attention model for comparison.  It discusses the *impact* of multi-head attention, but not a direct comparison to another method.
",0.06060606060606061,0.11764705882352941,0.08,0.0,0.0,0.0,0.06060606060606061,0.11764705882352941,0.08,0.012753998269755913,0.40515121817588806,No,0.0,0.0,0.0
q503,How large is the corpus they use?,449050,"13 million NRLS records; 1 million and 2 million records; 5 million articles from English Wikipedia.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11141545325517654,No,0.0,0.0,0.0
q504,How many shared layers are in the system?,1,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.17235848307609558,No,0.0,0.0,0.0
q505,How many additional task-specific layers are introduced?,2 for the ADE dataset and 3 for the CoNLL04 dataset,"Two sets of task-specific BiRNN layers.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14370405673980713,No,0.0,1.0,1.0
q506,How many layers of self-attention does the model have?,"1, 4, 8, 16, 32, 64","10
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.23165102303028107,No,0.0,1.0,1.0
q507,what are the state of the art methods?,"S2VT, RGB (VGG), RGB (VGG)+Flow (AlexNet), LSTM-E (VGG), LSTM-E (C3D) and Yao et al.","DNN methods,  Maximum Entropy Markov Model (MEMM), Support Vector Machine (SVM), and Conditional Random Field (CRF).
",0.0625,0.058823529411764705,0.06060606060606061,0.0,0.0,0.0,0.0625,0.058823529411764705,0.06060606060606061,0.04341121041750853,0.45059219002723694,No,0.0,0.0,0.0
q508,Which four languages do they experiment with?,"German, English, Italian, Chinese","Spanish, English, Italian, and Russian.
",0.4,0.5,0.4444444444444445,0.25,0.3333333333333333,0.28571428571428575,0.4,0.5,0.4444444444444445,0.4463236137853328,0.8547719717025757,No,0.0,0.0,0.0
q509,Does DCA or GMM-based attention perform better in experiments?,About the same performance,"DCA performs better than GMMv2b in some experiments.
",0.125,0.25,0.16666666666666666,0.0,0.0,0.0,0.125,0.25,0.16666666666666666,0.0,0.42241644859313965,No,0.0,1.0,1.0
q510,What evaluation metric is used?,F1 and Weighted-F1,"Manual evaluation, automatic evaluation metrics (ROUGE, BLEU, METEOR), and task-based evaluation.
",0.08333333333333333,0.25,0.125,0.0,0.0,0.0,0.08333333333333333,0.25,0.125,0.021476912089159056,0.0907435193657875,No,0.0,0.0,0.0
q511,"Is any data-to-text generation model trained on this new corpus, what are the results?","Yes, Transformer based seq2seq is evaluated with average BLEU 0.519, METEOR 0.388, ROUGE 0.631 CIDEr 2.531 and SER 2.55%.","No data-to-text generation model is mentioned as being trained on the new corpus.
",0.06666666666666667,0.041666666666666664,0.05128205128205127,0.0,0.0,0.0,0.06666666666666667,0.041666666666666664,0.05128205128205127,0.016532587556583624,0.12993527948856354,No,0.0,0.0,0.0
q512,How are the potentially relevant text fragments identified?," Generate a query out of the claim and querying a search engine, rank the words by means of TF-IDF, use IBM's AlchemyAPI to identify named entities, generate queries of 5–10 tokens, which execute against a search engine, and collect the snippets and the URLs in the results, skipping any result that points to a domain that is considered unreliable.","Entities are extracted, contexts are generated, and a distance between a question and the entity context is computed.  Candidate chunks come from lemmata, surface forms, and base forms.  A PATRICIA trie is used to search for matching strings.
",0.34210526315789475,0.20967741935483872,0.26000000000000006,0.02702702702702703,0.01639344262295082,0.02040816326530612,0.23684210526315788,0.14516129032258066,0.18,0.019724250923638723,0.43305543065071106,No,0.0,0.0,0.0
q513,What dataset did they use?,"weibo-100k, Ontonotes, LCQMC and XNLI","A dataset of 3 billion (query, image, weight) triples, with 900 million unique images and 220 million unique queries; and a dataset (SCI) of 220 sentences from the scientific literature.
",0.03333333333333333,0.16666666666666666,0.05555555555555555,0.0,0.0,0.0,0.03333333333333333,0.16666666666666666,0.05555555555555555,0.012413584734794654,0.12773996591567993,No,0.0,0.0,0.0
q514,What is the size of the dataset?,3029,"The answer is not found in the context.  While sizes of various *parts* of different datasets are given, there is no single ""size of the dataset"" figure.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0074570029973983765,No,0.0,0.0,0.0
q515,What are the 12 AV approaches which are examined?,"MOCC, OCCAV, COAV, AVeer, GLAD, DistAV, Unmasking, Caravel, GenIM, ImpGI, SPATIUM and NNCD","The provided text lists 12 existing AV approaches, but does not name them.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.015416838024282304,0.444833904504776,No,1.0,1.0,1.0
q516,how was annotation done?,Annotation was done with the help of annotators from Amazon Mechanical Turk on snippets of conversations,"Annotations were done by undergraduate students of computational linguistics, in single annotation mode.  A small sample was annotated by all four annotators to assess inter-annotator agreement.  Annotators discussed emerging issues with the authors throughout the process.
",0.1891891891891892,0.4375,0.2641509433962264,0.027777777777777776,0.06666666666666667,0.0392156862745098,0.10810810810810811,0.25,0.1509433962264151,0.024181756076515733,0.5392797589302063,No,0.0,0.0,0.0
q517,How do they measure correlation between the prediction and explanation quality?,They look at the performance accuracy of explanation and the prediction performance,"The provided text describes experiments to investigate how prediction accuracy is affected by jointly training the network to produce explanations and how well the model learns to generate explanations, but does not detail a method for measuring correlation between prediction and explanation quality.
",0.13953488372093023,0.5,0.21818181818181817,0.023809523809523808,0.09090909090909091,0.03773584905660377,0.13953488372093023,0.5,0.21818181818181817,0.012724686314753923,0.542381227016449,No,0.0,0.0,0.0
q518,What datasets are used to evaluate the introduced method?,"They used a dataset from Taobao which contained a collection of conversation records between customers and customer service staffs. It contains over five kinds of conversations,
including chit-chat, product and discount consultation, querying delivery progress and after-sales feedback. ","Swissmetro, Oshiete-goo, and nfL6 datasets.
",0.3333333333333333,0.05,0.08695652173913045,0.0,0.0,0.0,0.16666666666666666,0.025,0.04347826086956522,0.0009831150070693118,0.24793866276741028,No,0.0,0.0,0.0
q519,What are the results achieved from the introduced method?,"Their model resulted in values of 0.476, 0.672 and 0.893 for recall at position 1,2 and 5 respectively in 10 candidates.","The method achieves significant improvement in ROUGE-L compared to the baseline.  It also outperforms the baseline on the development set in terms of ROUGE-L.  All three methods (GLAD, Caravel, and COAV) perform similarly in terms of AUC, but show different results regarding c@1.
",0.13043478260869565,0.24,0.16901408450704222,0.0,0.0,0.0,0.08695652173913043,0.16,0.11267605633802817,0.010747017861729933,0.20413239300251007,No,0.0,0.0,0.0
q520,How do they incorporate human advice?,by converting human advice to first-order logic format and use as an input to calculate gradient,"They explicitly represent advice in calculating gradients, allowing the system to trade off between data and advice throughout the learning phase.  Advice is converted to first-order logic and given as input to the algorithm.
",0.34285714285714286,0.7058823529411765,0.46153846153846156,0.14705882352941177,0.3125,0.2,0.2571428571428571,0.5294117647058824,0.34615384615384615,0.0524544714107019,0.5517171621322632,Yes,0.0,1.0,1.0
q521,What affective-based features are used?,"affective features provided by different emotion models such as Emolex, EmoSenticNet, Dictionary of Affect in Language, Affective Norms for English Words and Linguistics Inquiry and Word Count","DAL Activation, ANEW Dominance, Emolex Negative, Emolex Fear, LIWC Assent, LIWC Cause, LIWC Certain, LIWC.  Also, features based on positive/negative sentiment polarity lexicons (number of positive terms, number of negative terms, etc.).
",0.09090909090909091,0.1111111111111111,0.09999999999999999,0.0,0.0,0.0,0.06060606060606061,0.07407407407407407,0.06666666666666667,0.01165251222790752,0.5960659384727478,No,0.0,1.0,1.0
q522,How big is performance improvement proposed methods are used?,"Data augmentation (es)  improved Adv es by 20% comparing to baseline 
Data augmentation (cs) improved Adv cs by 16.5% comparing to baseline
Data augmentation (cs+es) improved both Adv cs and Adv es by at least 10% comparing to baseline 
All models show improvements over adversarial sets  
","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.014365974813699722,No,0.0,0.0,0.0
q523,By how much does transfer learning improve performance on this task?,"In task 1 best transfer learning strategy improves F1 score by 4.4% and accuracy score by 3.3%, in task 2 best transfer learning strategy improves F1 score by 2.9% and accuracy score by 1.7%","Transfer learning achieves an average F₁ = 75%, better than the in-domain Experiment 1 results.  The best performance is F₁ = 76% in transfer learning models.
",0.32,0.21052631578947367,0.25396825396825395,0.08333333333333333,0.05405405405405406,0.06557377049180328,0.24,0.15789473684210525,0.19047619047619047,0.03209666354513308,0.6414295434951782,No,0.0,0.0,0.0
q524,What baseline is used?,SVM,"Baseline 1, Baseline 2, Baseline 3, ANRES_Sing_R, ANRES_Sing_Z, ANRES_Seq_R, ANRES_Seq_Z, LSTM.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19334439933300018,No,0.0,0.0,0.0
q525,What topic clusters are identified by LDA?,"Clusters of Twitter user ids from accounts of American or German political actors, musicians, media websites or sports club","Substance use, suicidality, and anxiety disorders.  Also, topics in a causal corpus tend to fall into three main categories, but those categories are not specified.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.014262733286728255,0.10694704949855804,No,0.0,0.0,0.0
q526,How much do they outperform previous state-of-the-art?,"On subtask 3 best proposed model has F1 score of 92.18 compared to best previous F1 score of 88.58.
On subtask 4 best proposed model has 85.9, 89.9 and 95.6 compared to best previous results of 82.9, 84.0 and 89.9 on 4-way, 3-way and binary aspect polarity.","Even the fastest of the new models beats the best published result, requiring only 6% of the computation.
",0.3333333333333333,0.10526315789473684,0.16,0.0,0.0,0.0,0.2777777777777778,0.08771929824561403,0.13333333333333333,0.005129257734067966,0.4639439880847931,No,0.0,0.0,0.0
q527,How big is the provided treebank?,"1448 sentences more than the dataset from Bhat et al., 2017","The context mentions treebanks with sizes ranging from more than 3000 sentences to 2,557 and 2,922 days.  It does not provide a single, definitive size for a single treebank.
",0.16129032258064516,0.45454545454545453,0.23809523809523805,0.03333333333333333,0.1,0.05,0.06451612903225806,0.18181818181818182,0.09523809523809523,0.03053216387177772,0.5697529911994934,No,0.0,0.0,0.0
q528,what dataset was used?,The dataset from a joint ADAPT-Microsoft project,"Basic Question Dataset (BQD), MS COCO dataset, SCI dataset.
",0.1111111111111111,0.125,0.11764705882352941,0.0,0.0,0.0,0.1111111111111111,0.125,0.11764705882352941,0.028398387225677894,0.2655051052570343,No,0.0,0.0,0.0
q529,What are the citation intent labels in the datasets?,"Background, extends, uses, motivation, compare/contrast, and future work for the ACL-ARC dataset. Background, method, result comparison for the SciCite dataset.","Background, Method, ResultComparison.  (Table 1) and six intent categories outlined in Table 2.
",0.23076923076923078,0.13636363636363635,0.1714285714285714,0.08333333333333333,0.047619047619047616,0.060606060606060615,0.15384615384615385,0.09090909090909091,0.1142857142857143,0.028335136661154864,0.43605124950408936,No,0.0,1.0,1.0
q530,How is quality of annotation measured?,Annotators went through various phases to make sure their annotations did not deviate from the mean.,"By comparing independent annotations, adjudicating disagreements, measuring accuracy against AMT annotations,  and/or measuring inter-annotator agreement.  Gold-standard labels are also created for comparison.
",0.08,0.125,0.09756097560975609,0.0,0.0,0.0,0.08,0.125,0.09756097560975609,0.015301683686839006,0.47957104444503784,Yes,0.0,0.0,0.0
q531,What accuracy score do they obtain?,the best performing model obtained an accuracy of 0.86,"Team 9 achieved a joint goal accuracy of 86.53%.
",0.3,0.3,0.3,0.1111111111111111,0.1111111111111111,0.1111111111111111,0.3,0.3,0.3,0.07495553473355841,0.4572869539260864,No,0.0,0.0,0.0
q532,What is the 12 class bilingual text?,"Appreciation, Satisfied, Peripheral complaint, Demanded inquiry, Corruption, Lagged response, Unresponsive, Medicine payment, Adverse behavior, Grievance ascribed and Obnoxious/irrelevant","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13851246237754822,No,0.0,0.0,0.0
q533,Which are the sequence model architectures this method can be transferred across?,The sequence model architectures which this method is transferred to are: LSTM and Transformer-based models,"LSTM seq2seq, Convolutional seq2seq, and transformer.
",0.5,0.1875,0.2727272727272727,0.2,0.06666666666666667,0.1,0.5,0.1875,0.2727272727272727,0.02604848453271721,0.4212134778499603,Yes,0.0,1.0,1.0
q534, What percentage of improvement in inference speed is obtained by the proposed method over the newest state-of-the-art methods?,"Across 4 datasets, the best performing proposed model (CNN) achieved an average of 363% improvement over the state of the art method (LR-CNN)","The answer is not found in the context.  While the text discusses faster inference speed than some previous methods, it does not quantify the improvement relative to the *newest* state-of-the-art methods.
",0.23529411764705882,0.3333333333333333,0.27586206896551724,0.12121212121212122,0.17391304347826086,0.14285714285714288,0.23529411764705882,0.3333333333333333,0.27586206896551724,0.01533227700920213,0.38082098960876465,No,0.0,0.0,0.0
q535,What is the metric that is measures in this paper?,error rate in a minimal pair ABX discrimination task,"Premise-Hypothesis Distance, METEOR, tIoU, BIBREF16, WMD, VSM, PV-DTW, and WordSLOR/WPSLOR.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11308444291353226,No,0.0,0.0,0.0
q536,Was evaluation metrics and criteria were used to evaluate the output of the cascaded multimodal speech translation?,BLEU scores,"Automatic metrics, subjective human evaluation (with a native German/fluent English speaker), and sentence-level ranking of outputs (head-to-head) were used.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19656215608119965,No,0.0,0.0,0.0
q537,What are the two decoding functions?,"a linear projection and a bijective function with continuous transformation though  ‘affine coupling layer’ of (Dinh et al.,2016). ","Linear projection and a family of bijective transformations with non-linear functions.
",0.75,0.47368421052631576,0.5806451612903226,0.2727272727272727,0.16666666666666666,0.20689655172413793,0.5,0.3157894736842105,0.3870967741935484,0.06229699880447546,0.7118852138519287,Yes,0.0,0.0,1.0
q538,What are the domains covered in the dataset?,"Alarm
Bank
Bus
Calendar
Event
Flight
Home
Hotel
Media
Movie
Music
RentalCar
Restaurant
RideShare
Service
Travel
Weather","17 domains (excluding the ""Alarm"" domain), and 20 domains across the training, development, and test datasets.
",0.0625,0.058823529411764705,0.06060606060606061,0.0,0.0,0.0,0.0625,0.058823529411764705,0.06060606060606061,0.015732934811145335,0.08713909238576889,No,0.0,1.0,1.0
q539,How are the two different models trained?,They pre-train the models using 600000 articles as an unsupervised dataset and then fine-tune the models on small training set.,"One model is trained first for coarse-grained features, then the other is unfrozen and trained in a low learning rate for fine-grained features.  Another method fine-tunes a BERT model with a new prediction layer first, then deploys the model and loads the fine-tuned model.  Both models described use Stochastic Gradient Descent with ADAM optimization and a learning rate of 1e-4.  Dropout regularization is also used.
",0.15714285714285714,0.5,0.2391304347826087,0.028985507246376812,0.09523809523809523,0.044444444444444446,0.1,0.3181818181818182,0.15217391304347827,0.007962770907450394,0.3554117679595947,No,0.0,0.0,0.0
q540,How long is the dataset?,"645, 600000","Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07716593146324158,No,0.0,0.0,0.0
q541,How big are negative effects of proposed techniques on high-resource tasks?,The negative effects were insignificant.,"Comparable on high-resource En-Fr task.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08116697886877475,0.10977362841367722,No,0.0,0.0,0.0
q542,"Are this techniques used in training multilingual models, on what languages?",English to French and English to German,"English, Russian, Arabic, Chinese, German, Spanish, and French.  Also, Asturian (target) and Spanish (source).
",0.2857142857142857,0.5714285714285714,0.38095238095238093,0.0,0.0,0.0,0.21428571428571427,0.42857142857142855,0.2857142857142857,0.01962672844874697,0.5197693109512329,No,0.0,0.0,0.0
q543,What metric is used to measure performance?,"Accuracy and F1 score for supervised tasks, Pearson's and Spearman's correlation for unsupervised tasks","Accuracy, precision, recall, F1-score, BIBREF16, ROUGE-LM, WPSLOR, ROUGE.
",0.3,0.1875,0.23076923076923075,0.1111111111111111,0.06666666666666667,0.08333333333333334,0.3,0.1875,0.23076923076923075,0.030372940354383413,0.3661312460899353,No,0.0,0.0,0.0
q544,How do Zipf and Herdan-Heap's laws differ?,"Zipf's law describes change of word frequency rate, while Heaps-Herdan describes different word number in large texts (assumed that Hepas-Herdan is consequence of Zipf's)","The context states that Heaps-Herdan law and the degree distribution of the adjacency network are consequences of Zipf's law.  It further states that Zipf's law reflects only word frequency, not word order.  The provided text does not describe a difference between Zipf's law and Heaps-Herdan's law beyond the fact that the latter is a consequence of the former.
",0.296875,0.6785714285714286,0.41304347826086957,0.1111111111111111,0.25925925925925924,0.15555555555555556,0.171875,0.39285714285714285,0.2391304347826087,0.026396548405028194,0.764000415802002,Yes,0.0,1.0,1.0
q545,How are the synthetic examples generated?,"Random perturbation of Wikipedia sentences using mask-filling with BERT, backtranslation and randomly drop out","The generative model is trained on the original training dataset.  Then, the premise and label from an example in the original dataset are taken as input to generate a new hypothesis.  The generated hypothesis is combined with the premise and the label to form a new unseen example.  This is done for every example in the original dataset.  Also, synthetic data is generated using simple rules such as dropping non-initial vowels and replacing consonants.
",0.05333333333333334,0.26666666666666666,0.08888888888888889,0.0,0.0,0.0,0.04,0.2,0.06666666666666667,0.006289930247960802,0.20882263779640198,No,0.0,0.0,0.0
q546,By how much does the new parser outperform the current state-of-the-art?,Proposed method achieves 94.5 UAS and 92.4 LAS  compared to 94.3 and 92.2 of best state-of-the -art greedy based parser. Best state-of-the art parser overall achieves 95.8 UAS and 94.6 LAS.,"The context states the new parser is ""competitive with the state-of-the-art on PTB when considering the UAS metric,"" and ""gives better or comparable results"" to BIAF and STACKPTR, but does *not* specify by how much.
",0.3157894736842105,0.2926829268292683,0.3037974683544304,0.08108108108108109,0.075,0.07792207792207792,0.23684210526315788,0.21951219512195122,0.2278481012658228,0.013024295746348018,0.6139529943466187,No,0.0,0.0,0.0
q547,What experimental evaluation is used?,root mean square error between the actual and the predicted price of Bitcoin for every minute,"Cross-domain evaluation, human evaluation (relevance, attraction, fluency, style strength), qualitative and quantitative assessment (using ISO 9241 and other techniques).
",0.05,0.0625,0.05555555555555556,0.0,0.0,0.0,0.05,0.0625,0.05555555555555556,0.01286713095960916,-0.12521924078464508,No,0.0,0.0,0.0
q548,How is the architecture fault-tolerant?,By using Apache Spark which stores all executions in a lineage graph and recovers to the previous steady state from any fault,"The architecture uses Apache Spark, which acts as an in-memory data store and allows for fault tolerant processing of large volumes of data, and is persistent in real time.
",0.3,0.4090909090909091,0.34615384615384615,0.10344827586206896,0.14285714285714285,0.12000000000000001,0.23333333333333334,0.3181818181818182,0.2692307692307693,0.03053216387177772,0.580661952495575,Yes,0.0,1.0,1.0
q549,Which elements of the platform are modular?,"handling large volume incoming data, sentiment analysis on tweets and predictive online learning","The question answering platform, a bitcoin price prediction platform, and a modular architecture for a spoken dialog system are all described as modular.
",0.08695652173913043,0.15384615384615385,0.1111111111111111,0.0,0.0,0.0,0.043478260869565216,0.07692307692307693,0.05555555555555555,0.01717906001229923,0.06888771057128906,No,0.0,0.0,0.0
q550,Could you tell me more about the metrics used for performance evaluation?,"BLUE utilizes different metrics for each of the tasks: Pearson correlation coefficient, F-1 scores, micro-averaging, and accuracy","The context describes various evaluation metrics, including:

* **Hyperparameter search and early stopping:** Success rate in selecting the best combination (out of 10 runs) and average human-annotated score for selected models.
* **NLG evaluation:** Recall rate, F1 score, and overall accuracy, averaged across perspectives.  Overall performance is approximated by multiplying disjoint measures.
* **Task 2:** Task completion ratio, user satisfaction degree (scores from -2 to 2), response fluency (scores from -1 to 1), and number of dialogue turns.
",0.13157894736842105,0.5263157894736842,0.21052631578947367,0.0,0.0,0.0,0.06578947368421052,0.2631578947368421,0.10526315789473684,0.009889724537791068,0.4784938097000122,No,0.0,0.0,0.0
q551,What are the tasks that this method has shown improvements?,"bilingual dictionary induction, monolingual and cross-lingual word similarity, and cross-lingual hypernym discovery","The context states that the method shows improvement in WER (word error rate) on Hub5'00.  It also mentions improvements when supervised data is reduced by half.
",0.037037037037037035,0.07142857142857142,0.04878048780487805,0.0,0.0,0.0,0.037037037037037035,0.07142857142857142,0.04878048780487805,0.012414943415352929,0.19726809859275818,No,0.0,0.0,0.0
q552,Why does the model improve in monolingual spaces as well? ,because word pair similarity increases if the two words translate to similar parts of the cross-lingual embedding space,"The context states that multilingual models, when trained on a combination of languages, improve recognition performance in languages with limited data.  This, in turn, benefits monolingual performance as well.
",0.06896551724137931,0.10526315789473684,0.08333333333333333,0.0,0.0,0.0,0.06896551724137931,0.10526315789473684,0.08333333333333333,0.010559717178170818,0.4883720874786377,No,0.0,0.0,0.0
q553,How is annotation projection done when languages have different word order?,"Word alignments are generated for parallel text, and aligned words are assumed to also share AMR node alignments.","The context states that the approach projects target-language word embeddings to the source-language embedding space preserving word order.  It is expected to work better for languages with similar word order to the source language (English).
",0.10810810810810811,0.2222222222222222,0.14545454545454545,0.0,0.0,0.0,0.10810810810810811,0.2222222222222222,0.14545454545454545,0.012413584734794654,0.3930950462818146,No,0.0,0.0,0.0
q554,What's the precision of the system?,"0.8320 on semantic typing, 0.7194 on entity matching","The provided context shows precision scores ranging from 0.92 to 0.97, and an average precision of 88% (and 91% estimated).  No single ""system"" precision is definitively stated.
",0.06896551724137931,0.2,0.10256410256410257,0.0,0.0,0.0,0.06896551724137931,0.2,0.10256410256410257,0.010253311603116816,0.34084323048591614,No,0.0,0.0,0.0
q555,Which of the two ensembles yields the best performance?,Answer with content missing: (Table 2) CONCAT ensemble,"Mean probability ensemble.
",0.3333333333333333,0.125,0.18181818181818182,0.0,0.0,0.0,0.3333333333333333,0.125,0.18181818181818182,0.027757915716335905,0.2625685930252075,No,0.0,0.0,0.0
q556,What is the new initialization method proposed in this paper?,They initialize their word and entity embeddings with vectors pre-trained over a large corpus of unlabeled data.,"Multichannel initialization.
",0.5,0.05555555555555555,0.09999999999999999,0.0,0.0,0.0,0.5,0.05555555555555555,0.09999999999999999,0.0,0.14951671659946442,No,0.0,0.0,0.0
q557,How was a quality control performed so that the text is noisy but the annotations are accurate?,The authors believe that the Wikilinks corpus  contains ground truth annotations while being noisy. They discard mentions that cannot have ground-truth verified by comparison with Wikipedia.,"A small sample of texts was annotated by all annotators, and inter-annotator agreement was measured.  For each annotator and question, the probability of agreement with the majority response was computed; annotators with probabilities more than two standard deviations from the mean were discarded.
",0.13636363636363635,0.2222222222222222,0.16901408450704225,0.0,0.0,0.0,0.09090909090909091,0.14814814814814814,0.11267605633802817,0.01137287189899225,0.31836989521980286,No,0.0,0.0,0.0
q558,Is it a neural model? How is it trained?,"No, it is a probabilistic model trained by finding feature weights through gradient ascent","Yes.  Trained using cross-entropy as the loss function, with variations including neural regressors and classifiers.
",0.0625,0.07142857142857142,0.06666666666666667,0.0,0.0,0.0,0.0625,0.07142857142857142,0.06666666666666667,0.021476912089159056,0.607145369052887,No,0.0,0.0,0.0
q559,How they evaluate quality of generated output?,Through human evaluation where they are asked to evaluate the generated output on a likert scale.,"They evaluate quality through summarization quality (BLEU, METEOR, ROUGE) and language fluency, along with a human evaluation of style strength.
",0.3,0.375,0.33333333333333326,0.05263157894736842,0.06666666666666667,0.058823529411764705,0.15,0.1875,0.16666666666666663,0.036329288779852226,0.5624240636825562,No,0.0,0.0,1.0
q560,What are the four forums the data comes from?,"Darkode,  Hack Forums, Blackhat and Nulled.","BIBREF, Reddit, Twitter, Facebook
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05868924818816534,0.3413001298904419,No,0.0,0.0,0.0
q561,How are sentence embeddings incorporated into the speech recognition system?,BERT generates sentence embeddings that represent words in context. These sentence embeddings are merged into a single conversational-context vector that is used to calculate a gated embedding and is later combined with the output of the decoder h to provide the gated activations for the next hidden layer.,"Pre-trained sentence embeddings, such as those from BERT, are encoded to learn conversational-context embeddings.  These embeddings are used within the end-to-end speech recognition framework.
",0.4642857142857143,0.2653061224489796,0.3376623376623377,0.1111111111111111,0.0625,0.08,0.32142857142857145,0.1836734693877551,0.23376623376623376,0.02507338456624256,0.7720527052879333,Yes,0.0,0.0,0.0
q562,How different is the dataset size of source and target?,the training dataset is large while the target dataset is usually much smaller,"The source dataset is significantly larger than the target dataset.  For example, 25% of MovieQA (2462 examples) is larger than the MC160 training set (280 examples).
",0.3076923076923077,0.6153846153846154,0.4102564102564103,0.12,0.25,0.16216216216216217,0.2692307692307692,0.5384615384615384,0.3589743589743589,0.057301920691894166,0.6174017786979675,Yes,0.0,0.0,1.0
q563,What type of documents are supported by the annotation platform?,"Variety of formats supported (PDF, Word...), user can define content elements of document","PDFs and Microsoft Word
",0.5,0.15384615384615385,0.23529411764705882,0.0,0.0,0.0,0.5,0.15384615384615385,0.23529411764705882,0.002925662791955391,0.5114766955375671,Yes,1.0,1.0,1.0
q564,What are the strong baselines you have?,optimize single task with no synthetic data,"The context does not specify which baselines are ""strong"".  It describes multiple baselines, but does not evaluate their relative strengths.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.03516089543700218,No,0.0,0.0,0.0
q565,What are causal attribution networks?,"networks where nodes represent causes and effects, and directed edges represent cause-effect relationships proposed by humans","A causal attribution dataset is a collection of text pairs reflecting cause-effect relationships proposed by humans.  These networks map out the underlying or latent causal attribution network held collectively by humans.  Nodes are defined by written descriptions, and edges represent cause-effect relationships.
",0.29545454545454547,0.7647058823529411,0.42622950819672134,0.16279069767441862,0.4375,0.23728813559322032,0.18181818181818182,0.47058823529411764,0.26229508196721313,0.1279600659916637,0.5146392583847046,Yes,1.0,1.0,1.0
q566,how did they ask if a tweet was racist?,"if it includes  negative utterances, negative generalizations and insults concerning ethnicity, nationality, religion and culture.","The context does not describe how participants were asked if a tweet was racist.  It only mentions assessing offensiveness on a scale and asking about hate speech in general.
",0.13793103448275862,0.26666666666666666,0.18181818181818182,0.0,0.0,0.0,0.10344827586206896,0.2,0.13636363636363635,0.015784309053930103,0.4865822196006775,No,0.0,0.0,0.0
q567,How does the model compute the likelihood of executing to the correction semantic denotation?,By treating logical forms as a latent variable and training a discriminative log-linear model over logical form y given x.,"The parser generates candidate logical forms via beam search, executes them, and marks those yielding the correct denotation as consistent.  The parser is trained to maximize the total likelihood of these consistent logical forms.
",0.20588235294117646,0.3333333333333333,0.2545454545454545,0.06060606060606061,0.1,0.07547169811320756,0.17647058823529413,0.2857142857142857,0.21818181818181817,0.025507996539511826,0.42232030630111694,Yes,0.0,0.0,0.0
q568,What are state of the art methods authors compare their work with? ,"ISOT dataset: LLVM
Liar dataset: Hybrid CNN and LSTM with attention","Potthast et al. BIBREF10 evaluated three AO approaches against 44 AV methods.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13062220811843872,No,0.0,0.0,0.0
q569,How much improvement do they get?,Their GTRS approach got an improvement of 3.89% compared to SVM and 27.91% compared to Pawlak.,"Various improvements are mentioned, ranging from negligible (e.g., 2% increase in accuracy) to significant (e.g., 5.4 EM improvement in multi-span questions).  A 2.1 point gain is also noted.
",0.06060606060606061,0.1111111111111111,0.0784313725490196,0.0,0.0,0.0,0.06060606060606061,0.1111111111111111,0.0784313725490196,0.01176877973200725,0.48914921283721924,No,0.0,0.0,0.0
q570,Which languages do they test on?,"Answer with content missing: (Applications section) We use Wikipedia articles
in five languages
(Kinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English) as well as the Na dataset of Adams
et al. (2017).
Select:
Kinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English","Dutch, Spanish, English, German, Basque, French, Italian, Finnish, Hindi, Portuguese, Japanese, Thai, Vietnamese, Arabic, Russian.
",0.06666666666666667,0.022222222222222223,0.03333333333333333,0.0,0.0,0.0,0.06666666666666667,0.022222222222222223,0.03333333333333333,0.007348874222110592,0.5878071784973145,No,0.0,0.0,0.0
q571,What limitations are mentioned?,"deciding publisher partisanship, risk annotator bias because of short description text provided to annotators","The limitations mentioned are:  sequences in the training set are usually assumed to be uniformly or geometrically distributed, with little regard to the nature and complexity of the language.
",0.06896551724137931,0.14285714285714285,0.09302325581395349,0.0,0.0,0.0,0.034482758620689655,0.07142857142857142,0.046511627906976744,0.015266081935888859,0.11412461847066879,No,0.0,0.0,0.0
q572,What are the baselines?,"CNN, LSTM, BERT","Baseline 1: smooth L1 distance; Baseline 2: PCA-based acoustic similarity; Baseline 3: Nonlinear dynamical systems-based complexity; ANRES_Sing_R; ANRES_Sing_Z; ANRES_Seq_R; ANRES_Seq_Z.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1210097149014473,No,0.0,0.0,0.0
q573,What semantic features help in detecting whether a piece of text is genuine or generated? of ,"No feature is given, only discussion that semantic features are use in practice and yet to be discovered how to embed that knowledge into statistical decision theory framework.","Dense representations of the claim, snippets, and related sentences, automatically trained using Long Short-Term Memory networks.
",0.11764705882352941,0.07142857142857142,0.08888888888888889,0.0,0.0,0.0,0.058823529411764705,0.03571428571428571,0.044444444444444446,0.015283112973810702,0.3722571134567261,No,0.0,0.0,0.0
q574,Is the assumption that natural language is stationary and ergodic valid?,It is not completely valid for natural languages because of diversity of language - this is called smoothing requirement.,"Manning and Schütze argue that, even though not quite correct, language text can be modeled as stationary, ergodic random processes.
",0.09523809523809523,0.1111111111111111,0.10256410256410256,0.0,0.0,0.0,0.09523809523809523,0.1111111111111111,0.10256410256410256,0.020705706652424008,0.28625616431236267,Yes,1.0,1.0,1.0
q575,Which models do they try out?,"DocQA, SAN, QANet, ASReader, LM, Random Guess","Support Vector Machines (SVMs), logistic regression, Naïve Bayes, Perceptron, and decision trees.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.028629993657668874,0.23473964631557465,No,0.0,0.0,0.0
q576,What are the competing models?,"TEACHER FORCING (TF), SCHEDULED SAMPLING (SS),  SEQGAN, RANKGAN, LEAKGAN.","BiMPM, ESIM, Decomposable Attention Model, KIM, BERT, Latent-Tree, Berkeley, LaSO, Latent-Strc, Model-Stack, Non-Linear, Coref-GRU, MHPGM, Weaver, MHQA-GRN.  Also, RF (with and without reduced features), Aggregated forecaster, MLP, Random Forest, and Linear Support Vector Machine.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.017429991698135954,0.4932931363582611,No,0.0,0.0,0.0
q577,How is the input triple translated to a slot-filling task?,"The relation R(x,y) is mapped onto a question q whose answer is y","The context describes a slot-filling task as a sequence labeling problem where a parameterized mapping function $f_{\theta}$ maps input words to slot tags.  No specific input triple translation is described.
",0.125,0.26666666666666666,0.1702127659574468,0.0,0.0,0.0,0.09375,0.2,0.12765957446808512,0.012413584734794654,0.10494636744260788,No,0.0,0.0,0.0
q578,How is module that analyzes behavioral state trained?,pre-trained to identify the presence of behavior from a sequence of word using the Couples Therapy Corpus,"The behavior model is pre-trained to identify the presence of each behavior from a sequence of words using a multi-label classification scheme.  The pre-trained portion was implemented using a single layer RNN with LSTM units with dimension size 50.
",0.35714285714285715,0.8333333333333334,0.5,0.2926829268292683,0.7058823529411765,0.41379310344827586,0.35714285714285715,0.8333333333333334,0.5,0.19858659849836868,0.5865997672080994,Yes,0.0,0.0,0.0
q579,"Can the model add new relations to the knowledge graph, or just new entities?",The model does not add new relations to the knowledge graph.,"The answer is not found in the context.
",0.375,0.2727272727272727,0.3157894736842105,0.0,0.0,0.0,0.375,0.2727272727272727,0.3157894736842105,0.04831252436962163,0.10091260820627213,No,0.0,0.0,0.0
q580,Do twitter users tend to tweet about the DOS attack when it occurs? How much data supports this assumption?,The dataset contains about 590 tweets about DDos attacks.,"Yes, the context states that legitimate users are willing to reveal information about service outages on social media platforms, suggesting that Twitter users do tweet about DoS attacks when they occur.  The text references a dataset of annotated tweets extracted from Twitter during DoS attacks, but doesn't specify the volume of data.
",0.11320754716981132,0.6666666666666666,0.1935483870967742,0.019230769230769232,0.125,0.03333333333333334,0.09433962264150944,0.5555555555555556,0.16129032258064518,0.010363935070930818,0.65029376745224,Yes,0.0,1.0,1.0
q581,What is the training and test data used?,"Tweets related to a Bank of America DDos attack were used as training data. The test datasets contain tweets related to attacks to Bank of America, PNC and Wells Fargo.","The training data consists of all previous rounds of data, Wikipedia Toxic Comments data, and the newly collected adversarial data from the ""break it"" round.  Training-22 (with gold standard annotations) and an external dataset (NLM-180 BIBREF5) are also used.  Test sets include Test Set 1 (57 drug labels), Test Set 2 (66 drug labels), and the test sets from the APDA shared task (without labels).
",0.13432835820895522,0.3,0.18556701030927836,0.030303030303030304,0.06896551724137931,0.04210526315789474,0.07462686567164178,0.16666666666666666,0.10309278350515463,0.013126882931262702,0.4744030833244324,No,0.0,0.0,0.0
q582,What writing styles are present in the corpus?,"current news, historical news, free time, sports, juridical news pieces, personal adverts, editorials.","Humor, newswire, letters, journal, and non-fiction.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.024727736451295213,0.7706505656242371,No,0.0,0.0,0.0
q583,What meta-information is being transferred?,"high-order representation of a relation, loss gradient of relation meta","Relation-specific meta information, specifically relation meta and gradient meta.
",0.4,0.36363636363636365,0.380952380952381,0.1111111111111111,0.1,0.10526315789473685,0.3,0.2727272727272727,0.28571428571428564,0.08913765521398126,0.5275313854217529,Yes,1.0,1.0,1.0
q584,What datasets are used to evaluate the approach?,"NELL-One, Wiki-One","Swissmetro dataset.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.006415737792849541,No,0.0,0.0,0.0
q585,How much is performance hurt when using too small amount of layers in encoder?,"comparing to the results from reducing the number of layers in the decoder, the BLEU score was 69.93 which is less than 1% in case of test2016 and in case of test2017 it was less by 0.2 %. In terms of TER it had higher score by 0.7 in case of test2016 and 0.1 in case of test2017. ","Reducing the $enc_{src \rightarrow mt}$ encoder block's depth reduces performance for all four scores.
",0.1875,0.04918032786885246,0.07792207792207792,0.06666666666666667,0.016666666666666666,0.02666666666666667,0.1875,0.04918032786885246,0.07792207792207792,0.003334108906292714,0.5094732046127319,No,1.0,1.0,1.0
q586,What neural machine translation models can learn in terms of transfer learning?,Multilingual Neural Machine Translation Models,"Neural machine translation (NMT) models can learn from a corpus of examples to build a statistical model that aims to find the most probable target sequence for an input sequence.
",0.13333333333333333,0.8,0.2285714285714286,0.06896551724137931,0.5,0.1212121212121212,0.13333333333333333,0.8,0.2285714285714286,0.01123099644603982,0.49535030126571655,No,0.0,0.0,0.0
q587,How does the semi-automatic construction process work?,Automatic transcription of 5000 tokens through sequential neural models trained on the annotated part of the corpus,"Researchers use crowdworkers to create sentiment and semantic role-labeling systems, then construct examples to find their faults.  Models are retrained to improve them, and the build, break, and fix sequence is repeated.
",0.06060606060606061,0.11764705882352941,0.08,0.0,0.0,0.0,0.06060606060606061,0.11764705882352941,0.08,0.00969094302009921,0.22748255729675293,No,0.0,0.0,0.0
q588,"What does ""explicitly leverages their probabilistic correlation to guide the training process of both models"" mean?","The framework jointly learns parametrized QA and QG models subject to the constraint in equation 2. In more detail, they minimize QA and QG loss functions, with a third dual loss for regularization.","The framework uses the probabilistic relationship between QA and QG as a regularizer to influence how both models are trained.
",0.5,0.30303030303030304,0.37735849056603776,0.15789473684210525,0.09375,0.11764705882352941,0.4,0.24242424242424243,0.30188679245283023,0.04718597484247946,0.7433478832244873,Yes,0.0,0.0,0.0
q589,What is WNGT 2019 shared task?,efficiency task aimed  at reducing the number of parameters while minimizing drop in performance,"NDNLP's submission to the 2019 WNGT Efficiency shared task uses a standard, recommended baseline Transformer network.
",0.17647058823529413,0.21428571428571427,0.1935483870967742,0.0,0.0,0.0,0.11764705882352941,0.14285714285714285,0.12903225806451615,0.0255404966647159,0.059212617576122284,No,1.0,1.0,1.0
q590,Was any variation in results observed based on language typology?,It is observed some variability - but not significant. Bert does not seem to gain much more syntax information than with type level information.,"Yes.  Different languages showed varying results in clustering coefficient distributions, with notable differences between Spanish and Turkish, and similarities between Spanish and French.
",0.043478260869565216,0.043478260869565216,0.043478260869565216,0.0,0.0,0.0,0.043478260869565216,0.043478260869565216,0.043478260869565216,0.018264728862681344,0.337597131729126,No,0.0,0.0,0.0
q591,Can the approach be generalized to other technical domains as well? ,"There is no reason to think that this approach wouldn't also be successful for other technical domains. Technical terms are replaced with tokens, therefore so as long as there is a corresponding process for identifying and replacing technical terms in the new domain this approach could be viable.","Models should generalize well for all domains.
",0.2857142857142857,0.04081632653061224,0.07142857142857142,0.0,0.0,0.0,0.2857142857142857,0.04081632653061224,0.07142857142857142,0.0005660406959623696,0.4159547984600067,Yes,0.0,1.0,1.0
q592,What dataset they use for evaluation?,The same 2K set from Gigaword used in BIBREF7,"16 Amazon review datasets, Stanford Sentiment TreeBank (SST-5/SST-2), and the IMDb Sentiment dataset.
",0.0625,0.1111111111111111,0.08,0.0,0.0,0.0,0.0625,0.1111111111111111,0.08,0.0,0.12213753908872604,No,0.0,0.0,0.0
q593,Which regions of the United States do they consider?,all regions except those that are colored black,"Northeast, Southeast, Midwest, and West Coast.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5048938393592834,No,0.0,0.0,0.0
q594,How is performance measured?,they use ROC curves and cross-validation,"Performance is measured by the number of times a metric selects the best hyperparameter combination or checkpoint (out of 10 trials), and the average human-annotated score for the selected models.  For speech recognition, performance is evaluated by analyzing successful and unsuccessful transactions in logs.  Classifier performance depends on the competition, using metrics like average score INLINEFORM0, accuracy, or a custom metric.
",0.03225806451612903,0.2857142857142857,0.05797101449275363,0.0,0.0,0.0,0.016129032258064516,0.14285714285714285,0.028985507246376815,0.005087757562008263,0.3265922963619232,No,0.0,0.0,0.0
q595,What is novel in author's approach?,"They use self-play learning , optimize the model for specific metrics, train separate models per user, use model  and response classification predictors, and filter the dataset to obtain higher quality training data.","The authors propose an AA approach based on time-aware language models that incorporate temporal changes in writing style.  Their experiments investigate whether author writing style can be recognized despite large time spans between documents.
",0.05714285714285714,0.0625,0.05970149253731344,0.0,0.0,0.0,0.05714285714285714,0.0625,0.05970149253731344,0.012193311110766474,0.22854718565940857,No,0.0,0.0,0.0
q596,How large is the Dialog State Tracking Dataset?,"1,618 training dialogs, 500 validation dialogs, and 1,117 test dialogs","Over 16,000 dialogues.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4823288917541504,No,0.0,1.0,1.0
q597,What dataset is used for train/test of this method?,Training datasets: TTS System dataset and embedding selection dataset. Evaluation datasets: Common Prosody Errors dataset and LFR dataset.,"The training data consists of all previous rounds of data, plus the Wikipedia Toxic Comments data.  Test sets are used to measure robustness.
",0.043478260869565216,0.05555555555555555,0.04878048780487805,0.0,0.0,0.0,0.043478260869565216,0.05555555555555555,0.04878048780487805,0.01717906001229923,0.5700893998146057,No,0.0,0.0,0.0
q598,How much is the gap between using the proposed objective and using only cross-entropy objective?,The mixed objective improves EM by 2.5% and F1 by 2.2%,"The context states that the proposed mixed objective converges quickly and outperforms the purely cross entropy model in practice.  However, it does not quantify the gap.
",0.15384615384615385,0.3076923076923077,0.20512820512820515,0.04,0.08333333333333333,0.05405405405405406,0.15384615384615385,0.3076923076923077,0.20512820512820515,0.030603367373678015,0.48289382457733154,No,0.0,0.0,1.0
q599,How many domains of ontologies do they gather data from?,"5 domains: software, stuff, african wildlife, healthcare, datatypes","17
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07417096197605133,No,0.0,0.0,0.0
q600,what is the practical application for this paper?,Improve existing NLP methods. Improve linguistic analysis. Measure impact of word normalization tools.,"The paper demonstrates how a corpus can be used for social science research by analyzing budget speeches, spoken corpus features, and language representations for decision making.  It also discusses applications in image synthesis, image and video editing, and text-to-image synthesis.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.009451258377819046,0.4574417769908905,No,0.0,0.0,0.0
q601,What's the method used here?,Two neural networks: an extractor based on an encoder (BERT) and a decoder (LSTM Pointer Network BIBREF22) and an abstractor identical to the one proposed in BIBREF8.,"A method using Bayes' rule for end-to-end training, with a shared ratio of posteriors.
",0.125,0.07407407407407407,0.09302325581395349,0.0,0.0,0.0,0.125,0.07407407407407407,0.09302325581395349,0.010050877063301288,0.24900935590267181,No,0.0,0.0,0.0
q602,By how much does their method outperform state-of-the-art OOD detection?,"AE-HCN outperforms by 17%, AE-HCN-CNN outperforms by 20% on average","More than 17 points in Precision@K on average.
",0.3333333333333333,0.23076923076923078,0.27272727272727276,0.125,0.08333333333333333,0.1,0.3333333333333333,0.23076923076923078,0.27272727272727276,0.06916141792551135,0.2601360082626343,No,0.0,1.0,1.0
q603,What are dilated convolutions?,Similar to standard convolutional networks but instead they skip some input values effectively operating on a broader scale.,"Dilated convolutions skip some input values so that the convolution kernel is applied over a larger area than its own, increasing the receptive field without increasing the number of parameters.
",0.2,0.3333333333333333,0.25,0.10344827586206896,0.17647058823529413,0.13043478260869565,0.2,0.3333333333333333,0.25,0.08036517486170723,0.3234619200229645,Yes,1.0,1.0,1.0
q604,what are the three methods presented in the paper?,"Optimized TF-IDF, iterated TF-IDF, BERT re-ranking.","The provided texts describe multiple methods, but not three *specific* methods are presented in all the contexts.  The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.014763939041893883,0.16407324373722076,No,0.0,0.0,0.0
q605,what datasets did the authors use?,"Kaggle
Subversive Kaggle
Wikipedia
Subversive Wikipedia
Reddit
Subversive Reddit ","The authors used two datasets: one of 220 sentences from the scientific literature (SCI) and a dataset of 19 novels (novel-9).
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06300348043441772,No,0.0,1.0,1.0
q606,How much performance improvements they achieve on SQuAD?,Compared to baselines SAN (Table 1) shows  improvement of 1.096% on EM and 0.689% F1. Compared to other published SQuAD results (Table 2) SAN is ranked second. ,"F1 score improved by 11.8 points in Japanese SQuAD.
",0.3,0.10344827586206896,0.15384615384615385,0.0,0.0,0.0,0.2,0.06896551724137931,0.10256410256410257,0.004533060144624593,0.5429458618164062,No,0.0,0.0,0.0
q607,What is the baseline?,The baseline is a multi-task architecture inspired by another paper.,"There are multiple baselines described.  Baseline 1 uses smooth L1 distance, Baseline 2 uses a PCA-based acoustic similarity measure, Baseline 3 uses a nonlinear dynamical systems-based complexity measure, and others.  More baselines are also mentioned but not fully described.
",0.04878048780487805,0.18181818181818182,0.07692307692307693,0.0,0.0,0.0,0.04878048780487805,0.18181818181818182,0.07692307692307693,0.009668670750672485,0.4621390402317047,No,0.0,0.0,0.0
q608,What is the network architecture?,"The network architecture has a multi-task Bi-Directional Recurrent Neural Network, with an unsupervised sequence labeling task and a low-dimensional embedding layer between tasks. There is a hidden layer after each successive task with skip connections to the senior supervised layers.","The network architecture consists of five main modules: an input module, an attention module, a reasoning module, a modeling module, and an output module.
",0.2916666666666667,0.16279069767441862,0.20895522388059704,0.08695652173913043,0.047619047619047616,0.061538461538461535,0.25,0.13953488372093023,0.1791044776119403,0.03943266142303915,0.4585614800453186,No,0.0,0.0,0.0
q609,What does recurrent deep stacking network do?,Stacks and joins outputs of previous frames with inputs of the current frame,"RDSN stacks and concatenates the outputs of previous frames into the input features of the current frame, utilizing phoneme information in previous frames as additional inputs to the raw features.
",0.36666666666666664,0.8461538461538461,0.5116279069767441,0.2413793103448276,0.5833333333333334,0.34146341463414637,0.36666666666666664,0.8461538461538461,0.5116279069767441,0.1535686541317235,0.46390923857688904,Yes,1.0,1.0,1.0
q610,What is the reward model for the reinforcement learning appraoch?,"reward 1 for successfully completing the task, with a discount by the number of turns, and reward 0 when fail","A balanced reward maximizing the probability of finding an answer to the generated question within the input paragraph but not inside the source sentence.  Also, a QA-based metric reward, computing the number of overlapping words between the ground truth answer and the predicted answer.
",0.17777777777777778,0.4,0.24615384615384614,0.045454545454545456,0.10526315789473684,0.06349206349206349,0.15555555555555556,0.35,0.21538461538461537,0.03825362884400915,0.37071311473846436,No,0.0,0.0,0.0
q611,Does this paper propose a new task that others can try to improve performance on?,"No, there has been previous work on recognizing social norm violation.","Yes.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3446711003780365,No,0.0,0.0,0.0
q612,How big is their dataset?,"3 million webpages processed with a CCG parser for training, 220 queries for development, and 307 queries for testing","Not found in the context.  There are several dataset sizes mentioned, but no single ""their"" dataset size is given.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.015732934811145335,0.1648988425731659,No,0.0,0.0,0.0
q613,What task do they evaluate on?,Fill-in-the-blank natural language questions,"Task 1 and Task 2.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07148841768503189,No,0.0,0.0,0.0
q614,How many feature maps are generated for a given triple?,3 feature maps for a given tuple,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07794737815856934,No,0.0,0.0,0.0
