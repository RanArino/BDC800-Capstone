qa_id,query,ground_truth,llm_answer,rouge1_p,rouge1_r,rouge1_f,rouge2_p,rouge2_r,rouge2_f,rougeL_p,rougeL_r,rougeL_f,bleu,cosine_sim,self_checker,llm_eval@1,llm_eval@3,llm_eval@5
q0,What is the seed lexicon?,a vocabulary of positive and negative predicates that helps determine the polarity score of an event,"A seed lexicon consisting of 15 positive words and 15 negative words.
",0.4166666666666667,0.3125,0.35714285714285715,0.0,0.0,0.0,0.4166666666666667,0.3125,0.35714285714285715,0.03468317761317914,0.4117070138454437,No,0.0,1.0,1.0
q1,What are the results?,"Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. 
Using a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.","Results are shown in Table TABREF16 and Table TABREF23.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.1584518942419951e-07,0.06696493178606033,No,0.0,0.0,0.0
q2,How are relations used to propagate polarity?,"based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event ","Discourse relations are exploited to efficiently propagate polarity from seed predicate structures, in addition to linguistic patterns.
",0.11764705882352941,0.09523809523809523,0.10526315789473684,0.0,0.0,0.0,0.11764705882352941,0.09523809523809523,0.10526315789473684,0.02055847707468329,0.4205455780029297,Yes,0.0,1.0,1.0
q3,How big is the Japanese data?,"7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus","About 100 million sentences.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3323495388031006,No,0.0,0.0,1.0
q4,How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?,3%,"The context states that the method is effective when labeled data are small, but does not provide a quantitative comparison of improvements.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11433730274438858,No,0.0,0.0,0.0
q5,How does their model learn using mostly raw data?,by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity,"Their method requires a very small seed lexicon and a large raw corpus.  They assume they can automatically extract features from the raw corpus.
",0.08333333333333333,0.14285714285714285,0.10526315789473685,0.0,0.0,0.0,0.041666666666666664,0.07142857142857142,0.052631578947368425,0.01717906001229923,0.22932671010494232,No,0.0,0.0,0.0
q6,How big is seed lexicon used for training?,30 words,"15 positive words and 15 negative words
",0.14285714285714285,0.5,0.22222222222222224,0.0,0.0,0.0,0.14285714285714285,0.5,0.22222222222222224,0.06567274736060395,0.5033256411552429,Yes,0.0,0.0,0.0
q7,How do the various social phenomena examined manifest in different types of communities?,"Dynamic communities have substantially higher rates of monthly user retention than more stable communities. More distinctive communities exhibit moderately higher monthly retention rates than more generic communities. There is also a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community - a short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content.
","Radically contrasting engagement patterns emerge in different parts of the multi-community space.  For example, in niche communities, established members engage more with volatile content than outsiders, while the opposite is true in generic communities.
",0.4,0.1590909090909091,0.22764227642276424,0.029411764705882353,0.011494252873563218,0.01652892561983471,0.2,0.07954545454545454,0.11382113821138212,0.01469067870176959,0.48742973804473877,No,0.0,0.0,0.0
q8,How did the select the 300 Reddit communities for comparison?,They selected all the subreddits from January 2013 to December 2014 with at least 500 words in the vocabulary and at least 4 months of the subreddit's history. They also removed communities with the bulk of the contributions are in foreign language.,"The text states that the dataset consists of all subreddits on Reddit from January 2013 to December 2014, with at least 500 words in the vocabulary used to estimate measures in at least 4 months of the subreddit's history.  The text does not specify *how* the 300 were selected from this larger dataset.
",0.5740740740740741,0.7209302325581395,0.6391752577319588,0.39622641509433965,0.5,0.4421052631578948,0.5,0.627906976744186,0.5567010309278351,0.3469594895512732,0.6794081330299377,Yes,0.0,0.0,1.0
q9,How is the clinical text structuring task defined?,CTS is extracting structural data from medical research data (unstructured). Authors define QA-CTS task that aims to discover most related text from original text.,"The clinical text structuring task aims to discover structured patient medical data, such as symptoms, diseases, tumor size, surgical details, and lab results, from clinical text.
",0.38461538461538464,0.4,0.39215686274509803,0.08,0.08333333333333333,0.08163265306122448,0.2692307692307692,0.28,0.27450980392156865,0.06251526810911487,0.5678231120109558,No,0.0,0.0,1.0
q10,"Is all text in this dataset a question, or are there unrelated sentences in between questions?",the dataset consists of pathology reports including sentences and questions and answers about tumor size and resection margins so it does include additional sentences ,"Not found in the context.
",0.2,0.041666666666666664,0.06896551724137931,0.0,0.0,0.0,0.2,0.041666666666666664,0.06896551724137931,0.004041065926152656,0.0966578871011734,No,0.0,0.0,0.0
q11,How many questions are in the dataset?,"2,714 ","2,714
",1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,Yes,1.0,1.0,1.0
q12,What aspects have been compared between various language models?,"Quality measures using perplexity and recall, and performance measured using latency and energy usage. ","Model quality (e.g., lower perplexity) and performance (energy consumption, query latency, etc.).
",0.46153846153846156,0.42857142857142855,0.4444444444444445,0.16666666666666666,0.15384615384615385,0.16,0.38461538461538464,0.35714285714285715,0.3703703703703704,0.04303846266589084,0.4486493468284607,Yes,0.0,0.0,1.0
q13,How many attention layers are there in their model?,one,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0340510718524456,No,0.0,0.0,0.0
q14,What are the three measures of bias which are reduced in experiments?,"RIPA, Neighborhood Metric, WEAT","Geometric bias mitigation,  measures of geometric bias mitigation, and measures of  probabilistic bias mitigation.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02719665272174911,0.10801170021295547,No,0.0,0.0,0.0
q15,How big is the dataset?,903019 references,"The answer is not found in the context.  While dataset sizes are mentioned in several examples, there is no single, overall dataset size provided.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10172918438911438,No,0.0,0.0,0.0
q16,How is the intensity of the PTSD established?,"Given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively, the estimated intensity  is established as mean  squared error.","The intensity is estimated using a mean squared error of 1.2, with four intensity levels (No PTSD, Low Risk PTSD, Moderate Risk PTSD, and High Risk PTSD) assigned scores of 0, 1, 2, and 3 respectively.
",0.8378378378378378,0.8611111111111112,0.8493150684931507,0.6111111111111112,0.6285714285714286,0.619718309859155,0.5945945945945946,0.6111111111111112,0.6027397260273972,0.43920055654355106,0.8931287527084351,Yes,0.0,1.0,1.0
q17,how is quality measured?,Accuracy and the macro-F1 (averaged F1 over positive and negative classes) are used as a measure of quality.,"EM-score (Exact Match) and (macro-averaged) F$_1$-score.  Reliability is also assessed by Cronbach's alpha.
",0.16666666666666666,0.15789473684210525,0.16216216216216214,0.0,0.0,0.0,0.16666666666666666,0.15789473684210525,0.16216216216216214,0.02224972978585352,0.679426908493042,No,0.0,0.0,0.0
q18,What is the accuracy reported by state-of-the-art methods?,"Answer with content missing: (Table 1)
Previous state-of-the art on same dataset: ResNet50 89% (6 languages), SVM-HMM 70% (4 languages)","95.4%
",0.5,0.043478260869565216,0.08,0.0,0.0,0.0,0.5,0.043478260869565216,0.08,0.0,0.27951815724372864,No,0.0,0.0,1.0
q19,How do the authors define or exemplify 'incorrect words'?,typos in spellings or ungrammatical words,"Missing or incorrect words, and sentences with grammatical errors.  Examples include tweets written in informal language, with disregard for sentence correctness.
",0.14285714285714285,0.5,0.22222222222222224,0.0,0.0,0.0,0.09523809523809523,0.3333333333333333,0.14814814814814814,0.01982256626710344,0.5404530763626099,Yes,0.0,0.0,0.0
q20,By how much do they outperform other models in the sentiment in intent classification tasks?,In the sentiment classification task by 6% to 8% and in the intent classification task by 0.94% on average,"6% to 8% better F1-micro scores.
",0.42857142857142855,0.15,0.2222222222222222,0.3333333333333333,0.10526315789473684,0.16,0.42857142857142855,0.15,0.2222222222222222,0.12906828346152652,0.4082324802875519,Yes,0.0,0.0,0.0
q21,Which experiments are perfomed?,They used BERT-based models to detect subjective language in the WNC corpus,"Experiments on the WNC dataset with variations of $RoBERTa_{large}$, $ALBERT_{xxlarge.v2}$, $DistilRoBERTa$ and $BERT$,  and weighted-average ensembling.
",0.15,0.23076923076923078,0.18181818181818185,0.05263157894736842,0.08333333333333333,0.06451612903225808,0.1,0.15384615384615385,0.12121212121212123,0.01938188604019842,0.46995750069618225,No,0.0,0.0,1.0
q22,Is ROUGE their only baseline?,"No, other baseline metrics they use besides ROUGE-L are n-gram overlap, negative cross-entropy, perplexity, and BLEU.","Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.006662591348749398,0.023244662210345268,No,0.0,0.0,0.0
q23,By how much does their system outperform the lexicon-based models?,"Under the retrieval evaluation setting, their proposed model + IR2 had better MRR than NVDM by 0.3769, better MR by 4.6, and better Recall@10 by  20 . 
Under the generative evaluation setting the proposed model + IR2 had better BLEU by 0.044 , better CIDEr by 0.033, better ROUGE by 0.032, and better METEOR by 0.029","The answer is not found in the context.
",0.25,0.03389830508474576,0.05970149253731343,0.0,0.0,0.0,0.25,0.03389830508474576,0.05970149253731343,0.00014054007057575334,0.1405528038740158,No,0.0,0.0,0.0
q24,How are the main international development topics that states raise identified?," They focus on exclusivity and semantic coherence measures: Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive. They select select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence.","By examining the key words associated with topics discussed in UN General Debate statements, identifying topics like economic development and sustainable development as the principal ones.
",0.3076923076923077,0.13559322033898305,0.18823529411764706,0.0,0.0,0.0,0.23076923076923078,0.1016949152542373,0.14117647058823532,0.006169752022583682,0.28981003165245056,No,0.0,0.0,0.0
q25,Is the semantic hierarchy representation used for any task?,"Yes, Open IE","Yes.
",1.0,0.3333333333333333,0.5,0.0,0.0,0.0,1.0,0.3333333333333333,0.5,0.0,0.3891817331314087,Yes,1.0,1.0,1.0
q26,Is the model evaluated?,the English version is evaluated. The German version evaluation is in progress ,"Yes.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09660522639751434,Yes,0.0,0.0,0.0
q27,How better is accuracy of new model compared to previously reported models?,"Average accuracy of proposed model vs best prevous result:
Single-task Training: 57.57 vs 55.06
Multi-task Training: 50.17 vs 50.59","The new model achieved improvements in EM-score and F$_1$-score compared to BERT-Base, QANet, and other baseline models (TF-IDF, S2S, IR).  Specific improvements vary, but examples include 5.64% better EM-score and 3.69% better F$_1$-score than BERT-Base.
",0.021739130434782608,0.04,0.028169014084507043,0.0,0.0,0.0,0.021739130434782608,0.04,0.028169014084507043,0.006488583831481832,0.3383903205394745,No,0.0,0.0,0.0
q28,How does the active learning model work?,"Active learning methods has a learning engine (mainly used for training of classification problems) and the selection engine (which chooses samples that need to be relabeled by annotators from unlabeled data). Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively.","Active learning uses a learning engine (a classifier) and a selection engine (sampling strategy) to select and label a small subset of unlabeled data.  The relabeled samples are added to the training set, and the classifier is retrained, improving accuracy in iterative steps.
",0.7674418604651163,0.4647887323943662,0.5789473684210527,0.30952380952380953,0.18571428571428572,0.23214285714285715,0.5348837209302325,0.323943661971831,0.4035087719298246,0.11547224474629197,0.8988533616065979,Yes,0.0,1.0,1.0
q29,Did the annotators agreed and how much?,"For event types and participant types, there was a moderate to substantial level of agreement using the Fleiss' Kappa. For coreference chain annotation, there was average agreement of 90.5%.","Inter-annotator agreement was found to be sufficiently high, and moderate to substantial agreement was observed on a subset of cases.  A 90.5% agreement was considered good.
",0.4642857142857143,0.43333333333333335,0.4482758620689655,0.1111111111111111,0.10344827586206896,0.10714285714285715,0.32142857142857145,0.3,0.3103448275862069,0.06693132716021727,0.7054618000984192,Yes,0.0,1.0,1.0
q30,What datasets are used to evaluate this approach?," Kinship and Nations knowledge graphs, YAGO3-10 and WN18KGs knowledge graphs ","A large-scale Chinese dataset with millions of real comments and a human-annotated test set collected from Tencent News, consisting of 198,112 news articles.  Also, 815 test instances (5,421 sentences) are used.
",0.02857142857142857,0.09090909090909091,0.043478260869565216,0.0,0.0,0.0,0.02857142857142857,0.09090909090909091,0.043478260869565216,0.01184949136625139,0.2378021627664566,No,0.0,0.0,0.0
q31,How was the dataset collected?,"They crawled travel information from the Web to build a database, created a multi-domain goal generator from the database, collected dialogue between workers an automatically annotated dialogue acts. ","Extracted from Japanese websites using HTML layouts and linguistic patterns, covering various genres.  For the paired worker dataset, 90 well-trained workers participated in data collection online, choosing tourist or system roles and completing dialogues.
",0.14285714285714285,0.1724137931034483,0.15625,0.0,0.0,0.0,0.11428571428571428,0.13793103448275862,0.125,0.014762323289306968,0.5239506959915161,No,0.0,0.0,0.0
q32,What models other than standalone BERT is new model compared to?,Only Bert base and Bert large are compared to proposed approach.,"RoBERTa<sub>large</sub>, ALBERT, BERT, and their ensemble models.
",0.3,0.2727272727272727,0.28571428571428564,0.0,0.0,0.0,0.2,0.18181818181818182,0.1904761904761905,0.02664321121388895,0.49386560916900635,No,0.0,0.0,0.0
q33,How big is the performance difference between this method and the baseline?,"Comparing with the highest performing baseline: 1.3 points on ACE2004 dataset, 0.6 points on CWEB dataset, and 0.86 points in the average of all scores.","0.244 and 0.097 Pearson correlation, and 0.012 and 0.012 MSE.
",0.21428571428571427,0.10714285714285714,0.14285714285714285,0.07692307692307693,0.037037037037037035,0.05,0.21428571428571427,0.10714285714285714,0.14285714285714285,0.018178296833472284,0.4656331539154053,No,0.0,0.0,0.0
q34,What approaches without reinforcement learning have been tried?,"classification, regression, neural methods","Classification-based and regression-based approaches.  Deep learning architectures.
",0.2222222222222222,0.5,0.30769230769230765,0.0,0.0,0.0,0.2222222222222222,0.5,0.30769230769230765,0.0,0.7469569444656372,Yes,0.0,0.0,0.0
q35,Which languages do they validate on?,"Ar, Bg, Ca, Cs, Da, De, En, Es, Eu, Fa, Fi, Fr, Ga, He, Hi, Hu, It, La, Lt, Lv, Nb, Nl, Nn, PL, Pt, Ro, Ru, Sl, Sv, Tr, Uk, Ur","Spanish (Es), French (Fr), Italian (It), English, German, and Russian.
",0.3,0.09375,0.14285714285714285,0.0,0.0,0.0,0.3,0.09375,0.14285714285714285,0.004505615438166308,0.6767638921737671,No,0.0,0.0,0.0
q36,What is the baseline method for the task?,"For the emotion recognition from text they use described neural network as baseline.
For audio and face there is no baseline.","A neural network model trained on the train set, using dropout rate of 0.3, Adam optimizer with a learning rate of $10^{-5}$, batch normalization, and gradually increasing the KL term from 0 to 1 after each epoch.
",0.15384615384615385,0.2857142857142857,0.2,0.02631578947368421,0.05,0.034482758620689655,0.07692307692307693,0.14285714285714285,0.1,0.020018796078701696,0.27099546790122986,No,0.0,0.0,0.0
q37,what amounts of size were used on german-english?,"Training data with 159000, 80000, 40000, 20000, 10000 and 5000 sentences, and 7584 sentences for development","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08191808313131332,No,0.0,0.0,0.0
q38,How big is the dataset?,Resulting dataset was 7934 messages for train and 700 messages for test.,"198,112 news articles
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1897267997264862,No,0.0,0.0,0.0
q39,What MC abbreviate for?,machine comprehension,"The context does not contain an abbreviation for MC.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11897516995668411,No,0.0,0.0,0.0
q40,What are their correlation results?,High correlation results range from 0.472 to 0.936,"ROUGE-2 and ROUGE-SU4 have similar correlation values, with ROUGE-SU4 being slightly better.  Precision and F1 are similar, but precision has better correlation. Recall shows poor correlation, and virtually no correlation using the revised Kendall measure.  Pearson correlation for a single reference increases to 0.048.  WordSLOR and WPSLOR have the lowest MSE, with WordSLOR having a slightly lower error than WPSLOR.
",0.046875,0.3,0.08108108108108107,0.015873015873015872,0.1111111111111111,0.027777777777777776,0.046875,0.3,0.08108108108108107,0.0062297170254052405,0.49090632796287537,No,0.0,0.0,0.0
q41,What simpler models do they look at?,"BiGRUs with attention, ROUGE, Language model, and next sentence prediction ","Distilled BERT-based models, smaller general-purpose language representation models pre-trained by leveraging distillation knowledge.
",0.125,0.2,0.15384615384615385,0.0,0.0,0.0,0.125,0.2,0.15384615384615385,0.02627961710408444,0.2911052703857422,No,0.0,0.0,0.0
q42,What linguistic quality aspects are addressed?,"Grammaticality, non-redundancy, referential clarity, focus, structure & coherence","Fluency.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3204004466533661,No,0.0,1.0,1.0
q43,What dataset do they use?,A parallel corpus where the source is an English expression of code and the target is Python code.,"AIDA-Train, Wikipedia, FigureEight, ISEAR, Twitter Emotion Corpus, MCTest, CNN/Daily Mail, CBT.
",0.07692307692307693,0.05555555555555555,0.06451612903225808,0.0,0.0,0.0,0.07692307692307693,0.05555555555555555,0.06451612903225808,0.018160849415439308,0.175665482878685,No,0.0,0.0,0.0
q44,What is typical GAN architecture for each text-to-image synhesis group?,"Semantic Enhancement GANs: DC-GANs, MC-GAN
Resolution Enhancement GANs: StackGANs, AttnGAN, HDGAN
Diversity Enhancement GANs: AC-GAN, TAC-GAN etc.
Motion Enhancement GAGs: T2S, T2V, StoryGAN","Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0007632587629084129,0.10400170832872391,No,0.0,0.0,0.0
q45,What language do the agents talk in?,English,"Natural language.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4631931781768799,No,0.0,0.0,0.0
q46,How much better is performance of proposed method than state-of-the-art methods in experiments?,"Accuracy of best proposed method KANE (LSTM+Concatenation) are 0.8011, 0.8592, 0.8605 compared to best state-of-the art method R-GCN + LR 0.7721, 0.8193, 0.8229 on three datasets respectively.","The context states that the proposed method outperforms Coppersmith et al.'s method, and that BERT versions outperform other methods.  It also shows that multi-task versions of BERT often perform better than single-task versions. However, it does not provide a specific quantitative comparison of the proposed method's performance against all state-of-the-art methods.  Thus, the answer is not found in the context.
",0.11940298507462686,0.2222222222222222,0.1553398058252427,0.07575757575757576,0.14285714285714285,0.09900990099009901,0.11940298507462686,0.2222222222222222,0.1553398058252427,0.014816847622130997,0.3377261459827423,No,0.0,0.0,0.0
q47,What baseline model is used?,Human evaluators,"A baseline model composed of word embedding, input encoder, alignment, aggregation, and prediction, with dropout rate of 0.3, Adam optimizer with a learning rate of 10^-5, and batch normalization.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.17277659475803375,No,0.0,0.0,0.0
q48,What stylistic features are used to detect drunk texts?,"LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors,  Spelling errors, Repeated characters, Capitalisation, Length,  Emoticon (Presence/Count ) 
 and Sentiment Ratio","Capitalization, spelling errors, etc.
",0.5,0.08695652173913043,0.14814814814814817,0.3333333333333333,0.045454545454545456,0.08,0.5,0.08695652173913043,0.14814814814814817,0.0015034597035749203,0.4036506712436676,Yes,0.0,0.0,0.0
q49,What is the accuracy of the proposed technique?,51.7 and 51.6 on 4th and 8th grade question sets with no curated knowledge. 47.5 and 48.0 on 4th and 8th grade question sets when both solvers are given the same knowledge,"95.4%
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.39789265394210815,No,0.0,0.0,0.0
q50,Which retrieval system was used for baselines?,The dataset comes with a ranked set of relevant documents. Hence the baselines do not use a retrieval system.,"IR (information retrieval)
",0.3333333333333333,0.05263157894736842,0.09090909090909091,0.0,0.0,0.0,0.3333333333333333,0.05263157894736842,0.09090909090909091,0.0043542900280071906,0.2975488305091858,No,0.0,0.0,1.0
q51,How much better was the BLSTM-CNN-CRF than the BLSTM-CRF?,Best BLSTM-CNN-CRF had F1 score 86.87 vs 86.69 of best BLSTM-CRF ,"Answer not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.00016737543046474457,No,0.0,0.0,0.0
q52,What supplemental tasks are used for multitask learning?,"Multitask learning is used for the task of predicting relevance of a comment on a different question to a given question, where the supplemental tasks are predicting relevance between the questions, and between the comment and the corresponding question","The supplemental tasks for multitask learning are predicting the relationships of tasks other than the main task (task C).  Specifically, the context mentions predicting the relationships of tasks not specified further.
",0.5161290322580645,0.41025641025641024,0.4571428571428572,0.13333333333333333,0.10526315789473684,0.11764705882352941,0.25806451612903225,0.20512820512820512,0.2285714285714286,0.03512354061915986,0.645017147064209,Yes,0.0,0.0,0.0
q53,How much performance gap between their approach and the strong handcrafted method?,"0.007 MAP on Task A, 0.032 MAP on Task B, 0.055 MAP on Task C","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18740521371364594,No,0.0,0.0,0.0
q54,How big is their model?,Proposed model has 1.16 million parameters and 11.04 MB.,"The context describes various models with different sizes, but does not give a single overall answer to ""how big is their model?"".
",0.045454545454545456,0.09090909090909091,0.060606060606060615,0.0,0.0,0.0,0.045454545454545456,0.09090909090909091,0.060606060606060615,0.01650404559570942,0.4514361619949341,No,0.0,0.0,0.0
q55,How many emotions do they look at?,9,"24
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.265918493270874,No,0.0,0.0,0.0
q56,What approach did previous models use for multi-span questions?,Only MTMSM specifically tried to tackle the multi-span questions. Their approach consisted of two parts: first train a dedicated categorical variable to predict the number of spans to extract and the second was to generalize the single-span head method of extracting a span,"The previous model (MTMSN) trained a dedicated categorical variable to predict the number of spans to extract, and generalized the single-span head method by utilizing non-maximum suppression (NMS) to extract multi-span answers.
",0.7142857142857143,0.5555555555555556,0.6250000000000001,0.5588235294117647,0.4318181818181818,0.48717948717948717,0.6571428571428571,0.5111111111111111,0.575,0.3086698691299079,0.7826266884803772,Yes,0.0,0.0,1.0
q57,What is difference in peformance between proposed model and state-of-the art on other question types?,"For single-span questions, the proposed LARGE-SQUAD improve performance of the MTMSNlarge baseline for 2.1 EM and 1.55 F1.
For number type question,  MTMSNlarge baseline  have improvement over LARGE-SQUAD for 3,11  EM and  2,98 F1. 
For date question,  LARGE-SQUAD have improvements in 2,02 EM but MTMSNlarge have improvement of 4,39 F1.","The provided text states the proposed model achieves the best performance and better accuracy in discriminating plausible comments from correct comments.  It does not offer a comparison of performance on other question types with state-of-the-art models.
",0.2564102564102564,0.16666666666666666,0.20202020202020202,0.05263157894736842,0.03389830508474576,0.041237113402061855,0.15384615384615385,0.1,0.12121212121212123,0.01804762664289501,0.3934195041656494,No,0.0,0.0,0.0
q58,What is the performance of proposed model on entire DROP dataset?,"The proposed model achieves  EM 77,63 and F1 80,73  on the test and EM  76,95 and  F1 80,25 on the dev","The context states that the model achieved 68.73% EM score and 77.39% F1 score on the SQuAD test set, but does not provide the performance on the entire DROP dataset.  Answer not found in the context.
",0.34210526315789475,0.52,0.41269841269841273,0.08108108108108109,0.125,0.09836065573770492,0.2894736842105263,0.44,0.34920634920634924,0.03184652052227926,0.5935584306716919,No,0.0,0.0,0.0
q59,How accurate is the aspect based sentiment classifier trained only using the XR loss?,"BiLSTM-XR-Dev Estimation accuracy is 83.31 for SemEval-15 and 87.68 for SemEval-16.
BiLSTM-XR accuracy is 83.31 for SemEval-15 and 88.12 for SemEval-16.
","The context states that an unannotated corpus of sentences is sufficient to surpass the results of a sentence-level trained classifier, and more unannotated data further improves results.  It does *not* state the *accuracy* of the classifier trained only using the XR loss.
",0.09302325581395349,0.125,0.10666666666666666,0.0,0.0,0.0,0.06976744186046512,0.09375,0.07999999999999999,0.011133308819227488,0.2732035219669342,No,0.0,1.0,1.0
q60,What were the non-neural baselines used for the task?,The Lemming model in BIBREF17,"The ""first $n$"" baseline.
",0.25,0.2,0.22222222222222224,0.0,0.0,0.0,0.25,0.2,0.22222222222222224,0.04767707020457095,0.13847222924232483,No,0.0,0.0,0.0
q61,What are the models evaluated on?,They evaluate F1 score and agent's test performance on their own built interactive datasets (iSQuAD and iNewsQA),"200 unique comments, ranked with four models (TF-IDF, S2S, IR, and Proposed+IR).
",0.07142857142857142,0.05555555555555555,0.0625,0.0,0.0,0.0,0.07142857142857142,0.05555555555555555,0.0625,0.023901021968803136,0.2462560534477234,No,0.0,0.0,0.0
q62,What is the results of multimodal compared to unimodal models?,"Unimodal LSTM vs Best Multimodal (FCM)
- F score: 0.703 vs 0.704
- AUC: 0.732 vs 0.734 
- Mean Accuracy: 68.3 vs 68.4 ","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.014565104618668556,No,0.0,0.0,0.0
q63,What were their performance results?,"On SearchSnippets dataset ACC 77.01%, NMI 62.94%, on StackOverflow dataset ACC 51.14%, NMI 49.08%, on Biomedical dataset ACC 43.00%, NMI 38.18%","F1 score of 57% on AMMER corpus; 73% on TEC;  performance results on DROP's development set are shown in Table TABREF24;  model surpasses NABERT+ baseline on all metrics for DROP.
",0.0967741935483871,0.1111111111111111,0.10344827586206896,0.0,0.0,0.0,0.0967741935483871,0.1111111111111111,0.10344827586206896,0.014091499441909466,0.3706388771533966,No,0.0,0.0,0.0
q64,By how much did they outperform the other methods?,"on SearchSnippets dataset by 6.72% in ACC, by 6.94% in NMI; on Biomedical dataset by 5.77% in ACC, 3.91% in NMI","The context states that fine-tuned BERT versions ""clearly outperform all other methods"", but does not give specific performance differences.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.013249308450474928,0.3488330841064453,No,0.0,0.0,0.0
q65,What is the state of the art?,"POS and DP task: CONLL 2018
NER task: (no extensive work) Strong baselines CRF and BiLSTM-CRF
NLI task: mBERT or XLM (not clear from text)","The context discusses various machine learning methods, including deep learning, for tasks like named entity recognition (NER) and acoustic speech processing, but does not explicitly define a ""state of the art.""
",0.12903225806451613,0.15384615384615385,0.14035087719298245,0.0,0.0,0.0,0.12903225806451613,0.15384615384615385,0.14035087719298245,0.014104555382959006,0.20691949129104614,No,0.0,0.0,0.0
q66,"What difficulties does sentiment analysis on Twitter have, compared to sentiment analysis in other domains?","Tweets noisy nature, use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, short (length limited) text","The unavailability of suitable datasets and lexicons for system training, development, and testing, and initially small and proprietary Twitter-specific resources.
",0.23809523809523808,0.20833333333333334,0.22222222222222224,0.0,0.0,0.0,0.19047619047619047,0.16666666666666666,0.17777777777777778,0.030838406059071864,0.6266288757324219,No,0.0,0.0,0.0
q67,How many sentence transformations on average are available per unique sentence in dataset?,27.41 transformation on average of single seed sentence is available in dataset.,"15
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09500506520271301,No,0.0,0.0,0.0
q68,What annotations are available in the dataset?,"For each source sentence, transformation sentences that are transformed according to some criteria (paraphrase, minimal change etc.)","Participant labels, BIO format (Inside, Outside, Beginning) labels, user states, system states, and dialogue acts.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02352622489487909,0.19687685370445251,No,0.0,0.0,0.0
q69,"How are possible sentence transformations represented in dataset, as new sentences?","Yes, as new sentences.","The context states that sentence variations, likely paraphrases, are collected and, after spell/grammar checking, are added to the dataset as new sentences.
",0.13043478260869565,0.75,0.22222222222222218,0.09090909090909091,0.6666666666666666,0.16,0.13043478260869565,0.75,0.22222222222222218,0.08517399705356635,0.3762667179107666,Yes,0.0,0.0,0.0
q70,What are all 15 types of modifications ilustrated in the dataset?,"- paraphrase 1
- paraphrase 2
- different meaning
- opposite meaning
- nonsense
- minimal change
- generalization
- gossip
- formal sentence
- non-standard sentence
- simple sentence
- possibility
- ban
- future
- past","The text lists several modifications, but not a complete, numbered list of 15 types.  It mentions:  first round examples, change of word order, interrogative/imperative transformations, fairy-tale style, excessive use of diminutives/vulgarisms, dada writing.  The answer is not found in the context.
",0.022727272727272728,0.041666666666666664,0.029411764705882356,0.0,0.0,0.0,0.022727272727272728,0.041666666666666664,0.029411764705882356,0.006488583831481832,0.46072787046432495,No,0.0,0.0,0.0
q71,What were their results on the three datasets?,"accuracy of 86.63 on STS, 85.14 on Sanders and 80.9 on HCR","The provided text describes results on various datasets but does not explicitly list results on three specific datasets.  Results are given for TEC, AMMER, Figure8, ISEAR, and DROP.
",0.10714285714285714,0.2,0.13953488372093023,0.0,0.0,0.0,0.10714285714285714,0.2,0.13953488372093023,0.01539373025281168,0.3161521852016449,No,0.0,0.0,0.0
q72,What semantic rules are proposed?,rules that compute polarity of words after POS tagging or parsing steps,"Ten semantic rules are proposed, but only five are used in the presented method.
",0.07142857142857142,0.08333333333333333,0.07692307692307691,0.0,0.0,0.0,0.07142857142857142,0.08333333333333333,0.07692307692307691,0.024455939372403628,0.36494219303131104,No,1.0,1.0,1.0
q73,What is the performance of the model?,"Experiment 1: ACC around 0.5 with 50% noise rate in worst case - clearly higher than baselines for all noise rates
Experiment 2: ACC on real noisy datasets: 0.7 on Movie, 0.79 on Laptop, 0.86 on Restaurant (clearly higher than baselines in almost all cases)","The model trained on the AMMER corpus achieves an F1 score of 57%.  A different model achieves 68.73%/77.74% on EM and F1, respectively, and 69.10%/78.38% on EM and F1 for a larger value of K.
",0.0975609756097561,0.08333333333333333,0.08988764044943819,0.0,0.0,0.0,0.07317073170731707,0.0625,0.06741573033707865,0.011147132099560858,0.2825795114040375,No,0.0,0.0,0.0
q74,What was the performance of both approaches on their dataset?,ERR of 19.05 with i-vectors and 15.52 with x-vectors,"The context describes experiments with different models and datasets, but does not give overall performance results for all approaches on their datasets.  It discusses specific results for different model components and tasks, but does not provide a summary of how well each approach performed overall.
",0.06666666666666667,0.23076923076923078,0.10344827586206898,0.0,0.0,0.0,0.044444444444444446,0.15384615384615385,0.06896551724137931,0.009798555761147508,0.09040221571922302,No,0.0,0.0,0.0
q75,What genres are covered?,"genre, entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement","mechanics, thermodynamics, poetry, art, Shakespeare, dance, literature, novel, symphony, drama, orchestra, music, ballet, arts, creative, sculpture,  coming-of-age, strong-female-lead, LGBTQ+
",0.043478260869565216,0.07142857142857142,0.05405405405405405,0.0,0.0,0.0,0.043478260869565216,0.07142857142857142,0.05405405405405405,0.049918233114942465,0.6348605751991272,No,0.0,0.0,0.0
q76,Which of the two speech recognition models works better overall on CN-Celeb?,x-vector,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10085505992174149,No,0.0,0.0,0.0
q77,By how much is performance on CN-Celeb inferior to performance on VoxCeleb?,"For i-vector system, performances are 11.75% inferior to voxceleb. For x-vector system, performances are 10.74% inferior to voxceleb","14.24% (CN-Celeb) vs 11.34% (VoxCeleb)
",0.25,0.09090909090909091,0.13333333333333333,0.0,0.0,0.0,0.25,0.09090909090909091,0.13333333333333333,0.014971158614143297,0.4812391996383667,No,0.0,0.0,0.0
q78,How do the authors measure performance?,Accuracy across six datasets,"EM-score, F₁-score, F₁-score (question answering), Spearman's ρ, Kendall's τ, Pearson's r, average of predicted and manual scores.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.26228269934654236,No,0.0,0.0,0.0
q79,What is the latest paper covered by this survey?,Kim et al. (2019),"The answer is not found in the context.  While dates are mentioned for some datasets, there's no indication of the *publication* dates of the referenced papers.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.011993481292529619,0.3794443607330322,No,0.0,0.0,0.0
q80,What learning paradigms do they cover in this survey?,"Considering ""What"" and ""How"" separately versus jointly optimizing for both.","in-domain, out-of-domain, and transfer learning
",0.125,0.1,0.11111111111111112,0.0,0.0,0.0,0.125,0.1,0.11111111111111112,0.020943469770777073,0.11415285617113113,No,0.0,0.0,0.0
q81,What are all the input modalities considered in prior work in question generation?,"Textual inputs, knowledge bases, and images.","The provided text mentions adapting NQG models to other input modalities, but does not explicitly list all input modalities considered in prior work.
",0.043478260869565216,0.16666666666666666,0.06896551724137931,0.0,0.0,0.0,0.043478260869565216,0.16666666666666666,0.06896551724137931,0.017911710595643587,0.2390565276145935,No,0.0,0.0,0.0
q82,How do this framework facilitate demographic inference from social media?,Demographic information is predicted using weighted lexicon of terms.,"The framework leverages weighted lexicons of terms, profile descriptions with rules and patterns, homophily interaction information, and visual features (images) to predict age and gender from textual and visual content.
",0.2,0.6666666666666666,0.30769230769230765,0.10344827586206896,0.375,0.16216216216216217,0.13333333333333333,0.4444444444444444,0.20512820512820512,0.02578571354943003,0.667297899723053,Yes,1.0,1.0,1.0
q83,How is the data annotated?,The data are self-reported by Twitter users and then verified by two human experts.,"The data was annotated using a final annotation schema, with iterative refinement due to complexity.  Scenario-specific annotation templates were designed, containing event and participant labels.  NP heads were labeled with participant labels.  Workers were trained and paired to converse, annotating user and system states.  Dialogue acts were automatically annotated using rules based on user states, system states, and dialogue histories.  In-house annotators verified the labels of MTurk annotators.  BIO format (Inside, Outside, Beginning) was used at a top layer.
",0.06172839506172839,0.3333333333333333,0.10416666666666666,0.025,0.14285714285714285,0.0425531914893617,0.06172839506172839,0.3333333333333333,0.10416666666666666,0.009510195519005135,0.27346017956733704,No,0.0,0.0,0.0
q84,Where does the information on individual-level demographics come from?,From Twitter profile descriptions of the users.,"Facebook users who shared their status, age, and gender.  A subset of 1061 users from INLINEFORM0.
",0.1875,0.42857142857142855,0.26086956521739124,0.0,0.0,0.0,0.125,0.2857142857142857,0.17391304347826086,0.025197593442434794,0.4435288906097412,No,0.0,0.0,0.0
q85,What is the source of the user interaction data? ,Sociability from ego-network on Twitter,"Reddit and Twitter.
",0.3333333333333333,0.16666666666666666,0.2222222222222222,0.0,0.0,0.0,0.3333333333333333,0.16666666666666666,0.2222222222222222,0.1244023474812678,0.27047571539878845,No,0.0,0.0,0.0
q86,What is the source of the textual data? ,Users' tweets,"Biomedical dataset, InScript corpus.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.051319532096385956,No,0.0,0.0,0.0
q87,What is the source of the visual data? ,Profile pictures from the Twitter users' profiles.,"Image data captured from photo-ready devices (like smartphones) and online social media services.  Video, speech, and biosignals (from Empatica E4).  Profile/shared images.
",0.08333333333333333,0.2857142857142857,0.12903225806451613,0.0,0.0,0.0,0.08333333333333333,0.2857142857142857,0.12903225806451613,0.014780822562194807,0.4124325215816498,No,0.0,0.0,0.0
q88,What result from experiments suggest that natural language based agents are more robust?,Average reward across 5 seeds show that NLP representations are robust to changes in the environment as well task-nuisances,"The context states that experiments comparing natural language representations to other methods were conducted under similar conditions, but does not state that natural language representations were found to be more robust.
",0.16129032258064516,0.25,0.19607843137254902,0.0,0.0,0.0,0.0967741935483871,0.15,0.11764705882352941,0.015883027492953544,0.4072542190551758,No,0.0,1.0,1.0
q89,Which datasets are used in the paper?,"Google N-grams
COHA
Moral Foundations Dictionary (MFD)
","Biomedical, StackOverflow, CNN/Daily Mail
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12889617681503296,No,0.0,0.0,0.0
q90,How do they quantify moral relevance?,By complementing morally relevant seed words with a set of morally irrelevant seed words based on the notion of valence,"They use the proportion of ""not a moral issue"" human responses paired with irrelevance predictions and the proportion of ""acceptable"" responses paired with positive predictions, performing linear regressions on these values.  Diachronic moral relevance scores (p(c₁|q)) are also computed for 20 decades.
",0.22727272727272727,0.5,0.3125,0.023255813953488372,0.05263157894736842,0.03225806451612903,0.11363636363636363,0.25,0.15625,0.010155182501165065,0.32366859912872314,No,0.0,0.0,0.0
q91,How much is proposed model better in perplexity and BLEU score than typical UMT models?,"Perplexity of the best model is 65.58 compared to best baseline 105.79.
Bleu of the best model is 6.57 compared to best baseline 5.50.","The answer is not found in the context.
",0.375,0.10714285714285714,0.16666666666666666,0.0,0.0,0.0,0.375,0.10714285714285714,0.16666666666666666,0.009489773613755376,0.12452219426631927,No,0.0,0.0,0.0
q92,Do they train a different training method except from scheduled sampling?,"Answer with content missing: (list missing) 
Scheduled sampling: In our experiments, we found that models trained with scheduled sampling performed better (about 0.004 BLEU-4 on validation set) than the ones trained using teacher-forcing for the AVSD dataset. Hence, we use scheduled sampling for all the results we report in this paper.

Yes.","Not found in the context.
",0.6,0.05454545454545454,0.09999999999999999,0.0,0.0,0.0,0.4,0.03636363636363636,0.06666666666666667,8.592050106759325e-06,0.10047426074743271,No,0.0,0.0,0.0
q93,How do they define upward and downward reasoning?,"Upward reasoning is defined as going from one specific concept to a more general one. Downward reasoning is defined as the opposite, going from a general concept to one that is more specific.","The text describes upward/downward expressions related to arguments within sentences, but does not explicitly define upward and downward reasoning.
",0.3,0.18181818181818182,0.22641509433962265,0.05263157894736842,0.03125,0.0392156862745098,0.2,0.12121212121212122,0.15094339622641512,0.01323272412374924,0.800455629825592,No,0.0,0.0,0.0
q94,Do they annotate their own dataset or use an existing one?,Use an existing one,"The context describes annotating a dataset.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.06202657148241997,No,0.0,0.0,0.0
q95,What kind of neural network architecture do they use?,CNN,"Neural regressor (NNR) and neural classifier (NNC).  NNC is used in two setups: ""NNC top 5"" and ""NNC with an explicit reasoning module"".  BiLSTM and BERT are also used as baselines.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1573392152786255,No,0.0,0.0,1.0
q96,What web and user-generated NER datasets are used for the analysis?,"MUC, CoNLL, ACE, OntoNotes, MSM, Ritter, UMBC","UMBC corpus for Twitter NER, and a further Twitter NER corpus.  WEB and TWI.
",0.07142857142857142,0.14285714285714285,0.09523809523809523,0.0,0.0,0.0,0.07142857142857142,0.14285714285714285,0.09523809523809523,0.02719665272174911,0.3981857895851135,No,0.0,1.0,1.0
q97,Which unlabeled data do they pretrain with?,1000 hours of WSJ audio data,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.020082570612430573,No,0.0,0.0,0.0
q98,How many convolutional layers does their model have?,wav2vec has 12 convolutional layers,"Two
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1546301394701004,No,0.0,0.0,0.0
q99,How big are the datasets?,"In-house dataset consists of  3716 documents 
ACE05 dataset consists of  1635 documents","000/1,610 pieces of news, vocabulary size of more than 100k.
",0.16666666666666666,0.15384615384615385,0.16,0.0,0.0,0.0,0.16666666666666666,0.15384615384615385,0.16,0.03377156414337854,0.29687631130218506,No,0.0,0.0,1.0
q100,What baselines did they compare their model with?,the baseline where path generation uses a standard sequence-to-sequence model augmented with attention mechanism and path verification uses depth-first search,"Baseline and Ablation models;  content, content-network, and image-based models; NABERT+ baseline.
",0.23076923076923078,0.13043478260869565,0.16666666666666669,0.0,0.0,0.0,0.23076923076923078,0.13043478260869565,0.16666666666666669,0.022796871594840864,0.4473932981491089,No,0.0,0.0,0.0
q101,What was the performance of their model?,"For test-repeated set, EM score of 61.17, F1 of 93.54, ED of 0.75 and GM of 61.36. For test-new set, EM score of 41.71, F1 of 91.02, ED of 1.22 and GM of 41.81","The model achieved an F1 score of 57% on the AMMER corpus.  Results on other datasets and metrics are also described.
",0.19047619047619047,0.09090909090909091,0.12307692307692307,0.05,0.023255813953488372,0.031746031746031744,0.19047619047619047,0.09090909090909091,0.12307692307692307,0.019484958403602582,0.49810484051704407,No,0.0,0.0,0.0
q102,What evaluation metrics are used?,"exact match, f1 score, edit distance and goal match","BLEU, BIBREF4, METEOR, ROUGE B, F1 score, Recall rate, Overall accuracy.
",0.18181818181818182,0.2222222222222222,0.19999999999999998,0.1,0.125,0.11111111111111112,0.18181818181818182,0.2222222222222222,0.19999999999999998,0.04753622060013117,0.3355097770690918,No,0.0,0.0,0.0
q103,How were the navigation instructions collected?,using Amazon Mechanical Turk using simulated environments with topological maps,"Collected through Mechanical Turk using 100 simulated environments with corresponding topological maps.
",0.6666666666666666,0.8,0.7272727272727272,0.45454545454545453,0.5555555555555556,0.5,0.6666666666666666,0.8,0.7272727272727272,0.2197281387499716,0.5839466452598572,Yes,1.0,1.0,1.0
q104,What language is the experiment done in?,english language,"English.
",1.0,0.5,0.6666666666666666,0.0,0.0,0.0,1.0,0.5,0.6666666666666666,0.0,0.6918691396713257,Yes,0.0,0.0,1.0
q105,What additional features are proposed for future work?,distinguishing between clinically positive and negative phenomena within each risk factor domain and accounting for structured data collected on the target cohort,"Combining structural, topical, and identity-based measures to characterize user engagement; extending analyses to diverse communities on different platforms; exploring more interesting sentiment analysis formulations; improvements in neural network architectures (e.g., concatenating high-level features, deeper networks with skip connections and Inception modules); exploring various feature extraction techniques (e.g., Constant-Q transform, Fast Fourier Transform); and training models for longer, with additional downstream tasks, baselines and analysis on cleaner corpora.
",0.06944444444444445,0.22727272727272727,0.10638297872340427,0.0,0.0,0.0,0.05555555555555555,0.18181818181818182,0.0851063829787234,0.005587028145593606,0.25764787197113037,No,0.0,0.0,0.0
q106,What are their initial results on this task?,"Achieved the highest per-domain scores on Substance (F1 ≈ 0.8) and the lowest scores on Interpersonal and Mood (F1 ≈ 0.5), and show consistency in per-domain performance rankings between MLP and RBF models.","The final model ""Ours with Mask and Ordered Triplets"" outperforms the Baseline and Ablation models on all metrics in previously seen environments.  The difference is particularly evident for Exact Match and Goal Match metrics.
",0.23529411764705882,0.22857142857142856,0.23188405797101447,0.0,0.0,0.0,0.17647058823529413,0.17142857142857143,0.1739130434782609,0.01587485381146055,0.4587046205997467,No,0.0,0.0,0.0
q107,How is morphology knowledge implemented in the method?,A BPE model is applied to the stem after morpheme segmentation.,"A morphological word segmentation method is used on the source (lemmatization and morphological feature analysis).  A morphologically motivated segmentation strategy combining morpheme segmentation and BPE is also proposed.  Other methods, like using Morfessor and rule-based segmentation, are also mentioned.
",0.15,0.5454545454545454,0.2352941176470588,0.02564102564102564,0.1,0.04081632653061225,0.125,0.45454545454545453,0.196078431372549,0.021740981369339423,0.47228917479515076,Yes,0.0,1.0,1.0
q108,How is the performance on the task evaluated?,Comparison of test accuracies of neural network models on an inflection task and qualitative analysis of the errors,"For question answering tasks, F1 score is used.  For RL tasks, agent performance is evaluated during training and testing.  For cross-multi-domain tasks, task finish rate is calculated over 1000 simulations.
",0.09375,0.16666666666666666,0.12000000000000002,0.0,0.0,0.0,0.0625,0.1111111111111111,0.08,0.012193311110766474,0.2090054601430893,No,0.0,0.0,0.0
q109,what datasets were used?,"IWSLT14 German-English, IWSLT14 Turkish-English, WMT14 English-German","Amazon Mechanical Turk dataset of simulated indoor environments, MCTest, CNN/Daily Mail, Children's Book Test, StackOverflow, and BioASQ.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.017911710595643587,0.2569635212421417,No,0.0,0.0,0.0
q110,How do they obtain human judgements?,Using crowdsourcing ,"The provided text describes using human judgments for evaluating model performance in predicting time courses of moral relevance, moral polarity, and fine-grained moral categories.  However, it does not specify how those human judgments were obtained.
",0.027777777777777776,0.5,0.05263157894736842,0.0,0.0,0.0,0.027777777777777776,0.5,0.05263157894736842,0.0,0.032006196677684784,No,0.0,0.0,0.0
q111,Does this approach perform better in the multi-domain or single-domain setting?,single-domain setting,"The approach performs better in the multi-span question setting, with a significant improvement in both EM and F1 scores compared to other models.
",0.041666666666666664,0.3333333333333333,0.07407407407407407,0.0,0.0,0.0,0.041666666666666664,0.3333333333333333,0.07407407407407407,0.015061893230938673,0.1258254200220108,No,0.0,0.0,0.0
q112,How many samples did they generate for the artificial language?,"70,000","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08871352672576904,No,0.0,0.0,0.0
q113,Why does not the approach from English work on other languages?,"Because, unlike other languages, English does not mark grammatical genders","The approach, while successful with English, French, and German, is less robust to new speakers in the test dataset.  Also,  difficulties arise with perceptually similar languages (like German and English), and issues exist with annotation discrepancies and differences in the choice of attributes to annotate across languages.  Different morphological structures (e.g., suffixing vs. prefixing) between source and target languages affect learning.
",0.03225806451612903,0.2,0.05555555555555555,0.0,0.0,0.0,0.03225806451612903,0.2,0.05555555555555555,0.006452284396459631,0.3392857313156128,No,0.0,0.0,0.0
q114,How do they measure grammaticality?,by calculating log ratio of grammatical phrase over ungrammatical phrase,"Speakers' intuitions about sentence well-formedness, using ""acceptability"" or ""grammaticality"" judgments.  A log ratio of grammatical to ungrammatical phrases is also used.
",0.2727272727272727,0.6,0.37499999999999994,0.19047619047619047,0.4444444444444444,0.26666666666666666,0.2727272727272727,0.6,0.37499999999999994,0.08839374326825924,0.6219295859336853,Yes,0.0,0.0,1.0
q115,What is the difference in recall score between the systems?,"Between the model and Stanford, Spacy and Flair the differences are 42.91, 25.03, 69.8 with Traditional NERs as reference and  49.88, 43.36, 62.43 with Wikipedia titles as reference.","The context states that recall is consistently significantly lower than precision for the proposed method, but does not give specific recall scores for different systems.  Therefore, the difference is not found in the context.
",0.08823529411764706,0.08823529411764706,0.08823529411764706,0.030303030303030304,0.030303030303030304,0.030303030303030304,0.08823529411764706,0.08823529411764706,0.08823529411764706,0.014491339833989647,0.36351343989372253,No,0.0,0.0,0.0
q116,What is their f1 score and recall?,"F1 score and Recall are 68.66, 80.08 with Traditional NERs as reference and 59.56, 69.76 with Wikipedia titles as reference.","F1 score: 78.38%
Recall: Not found in the context.
",0.3,0.125,0.17647058823529413,0.1111111111111111,0.043478260869565216,0.0625,0.3,0.125,0.17647058823529413,0.034043993726923044,0.7712615132331848,No,0.0,0.0,0.0
q117,How many layers does their system have?,4 layers,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.008996889926493168,No,0.0,0.0,0.0
q118,What context modelling methods are evaluated?,"Concat
Turn
Gate
Action Copy
Tree Copy
SQL Attn
Concat + Action Copy
Concat + Tree Copy
Concat + SQL Attn
Turn + Action Copy
Turn + Tree Copy
Turn + SQL Attn
Turn + SQL Attn + Action Copy","Different context modeling methods are evaluated, as shown in Table TABREF39, with a focus on three representative models.  A simple concatenation method is also included for comparison.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2493593543767929,No,0.0,0.0,0.0
q119,Is the baseline a non-heirarchical model like BERT?,There were hierarchical and non-hierarchical baselines; BERT was one of those baselines,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13072708249092102,No,0.0,0.0,0.0
q120,How better are results compared to baseline models?,F1 score of best authors' model is 55.98 compared to BiLSTM and FastText that have F1 score slighlty higher than 46.61.,"Our model outperforms the baseline and ablation models on all metrics in previously seen environments, with increases in accuracy of 35% and 25% for Exact Match and Goal Match, respectively.  For task A, our system performs better than a strong feature-rich based system when enough training data is available.  Our base model surpasses the NABERT+ baseline in every metric for task B.
",0.07936507936507936,0.21739130434782608,0.11627906976744186,0.0,0.0,0.0,0.047619047619047616,0.13043478260869565,0.06976744186046512,0.007962770907450394,0.35887420177459717,No,0.0,0.0,0.0
q121,How big is dataset used for training/testing?,"4,261  days for France and 4,748 for the UK","The answer is not found in the context.  While various datasets and their splits are mentioned, there's no single, unified answer to the size of the training/testing datasets across all examples.
",0.06060606060606061,0.18181818181818182,0.09090909090909091,0.0,0.0,0.0,0.06060606060606061,0.18181818181818182,0.09090909090909091,0.01184949136625139,0.17526377737522125,No,0.0,0.0,0.0
q122,What geometric properties do embeddings display?,Winter and summer words formed two separate clusters. Week day and week-end day words also formed separate clusters.,"Distances between word vectors, specifically cosine distance.
",0.14285714285714285,0.05263157894736842,0.07692307692307693,0.0,0.0,0.0,0.14285714285714285,0.05263157894736842,0.07692307692307693,0.014044464769840424,0.20036640763282776,No,0.0,0.0,0.0
q123,How accurate is model trained on text exclusively?,Relative error is less than 5%,"The answer is not found in the context.
",0.125,0.16666666666666666,0.14285714285714288,0.0,0.0,0.0,0.125,0.16666666666666666,0.14285714285714288,0.04767707020457095,0.1734209656715393,No,0.0,0.0,0.0
q124,What was their result on Stance Sentiment Emotion Corpus?,F1 score of 66.66%,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1218726709485054,No,0.0,0.0,0.0
q125,What performance did they obtain on the SemEval dataset?,F1 score of 82.10%,"Improved the existing best system of SemEval 2016 task 6 by 3.2 F-score points.
",0.125,0.4,0.19047619047619047,0.0,0.0,0.0,0.0625,0.2,0.09523809523809523,0.02627961710408444,0.45907145738601685,No,0.0,0.0,0.0
q126,What are the state-of-the-art systems?,"For sentiment analysis UWB, INF-UFRGS-OPINION-MINING, LitisMind, pkudblab and SVM + n-grams + sentiment and for emotion analysis MaxEnt, SVM, LSTM, BiLSTM and CNN","Deep convolutional networks.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13859516382217407,No,0.0,0.0,0.0
q127,What is the size of their collected dataset?,3347 unique utterances ,"287,226 training documents, 13,368 validation, and 11,490 test documents for CNNDM; 137,778 training, 17,222 validation, and 17,223 test for NYT50.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2505072057247162,No,0.0,0.0,0.0
q128,"What kind of features are used by the HMM models, and how interpretable are those?","A continuous emission HMM uses the hidden states of a 2-layer LSTM as features and a discrete emission HMM uses data as features. 
The interpretability of the model is shown in Figure 2. ","HMM models pick up on spaces, indentation, and special characters (such as comment symbols in Linux data).  The provided text indicates that these features are interpretable.
",0.34615384615384615,0.2647058823529412,0.29999999999999993,0.0,0.0,0.0,0.19230769230769232,0.14705882352941177,0.16666666666666669,0.028901600036276242,0.38802433013916016,No,0.0,0.0,0.0
q129,What kind of information do the HMMs learn that the LSTMs don't?,The HMM can identify punctuation or pick up on vowels.,"HMMs learn information about spaces, comments, and punctuation.
",0.25,0.2,0.22222222222222224,0.0,0.0,0.0,0.25,0.2,0.22222222222222224,0.04456882760699064,0.6856245994567871,No,1.0,1.0,1.0
q130,How large is the gap in performance between the HMMs and the LSTMs?,"With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower.","The hybrid algorithm (combining HMM and LSTM) performs a bit better than the standalone LSTM with the same LSTM state dimension on all text datasets.  The difference diminishes as the LSTM size increases.
",0.21212121212121213,0.2,0.2058823529411765,0.0,0.0,0.0,0.21212121212121213,0.2,0.2058823529411765,0.025058623314122577,0.46104663610458374,No,0.0,0.0,0.0
q131,what was their system's f1 performance?,"Proposed model achieves 0.86, 0.924, 0.71 F1 score on SR, HATE, HAR datasets respectively.","The system achieved an F1 score of 78.38% on the whole development set.
",0.2857142857142857,0.23529411764705882,0.2580645161290323,0.07692307692307693,0.0625,0.06896551724137931,0.2857142857142857,0.23529411764705882,0.2580645161290323,0.047873443378439184,0.6584826707839966,No,0.0,0.0,0.0
q132,How much more coverage is in the new dataset?,278 more annotations,"Answer not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03185254707932472,No,0.0,0.0,0.0
q133,How was quality measured?,"Inter-annotator agreement, comparison against expert annotation, agreement with PropBank Data annotations.","Human evaluation, often by randomly sampling generated questions and asking annotators to rate them on a 5-point Likert scale; average rank or percentage of best-ranked questions are used.  For poems, evaluators graded fluency, semantic coherence, imagery, and originality.  For other tasks, macro-averaging of precision, recall, and F1 scores was used.
",0.018867924528301886,0.08333333333333333,0.03076923076923077,0.0,0.0,0.0,0.018867924528301886,0.08333333333333333,0.03076923076923077,0.007446030455068397,0.310083270072937,No,0.0,0.0,0.0
q134,What is different in the improved annotation protocol?,a trained worker consolidates existing annotations ,"The improved annotation protocol has a new gold standard for evaluations with higher coverage of core and implied roles than the previous Dense evaluation dataset.  It also has three layers of annotation: event types, participant types and coreference annotation.
",0.05128205128205128,0.3333333333333333,0.08888888888888889,0.0,0.0,0.0,0.05128205128205128,0.3333333333333333,0.08888888888888889,0.008522452865706931,0.4540102481842041,No,0.0,0.0,0.0
q135,What data were they used to train the multilingual encoder?,WMT 2014 En-Fr parallel corpus,"The context does not specify the data used to train the multilingual encoder.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19531194865703583,No,0.0,0.0,0.0
q136,From when are many VQA datasets collected?,late 2014,"2014–2016
",0.5,0.5,0.5,0.0,0.0,0.0,0.5,0.5,0.5,0.0,0.7098172307014465,No,1.0,1.0,1.0
q137,What is task success rate achieved? ,96-97.6% using the objects color or shape and 79% using shape alone,"The context discusses ""task finish rate"" calculated over 1000 simulations, but not a success rate.  The answer is not found in the context.
",0.043478260869565216,0.07142857142857142,0.05405405405405405,0.0,0.0,0.0,0.043478260869565216,0.07142857142857142,0.05405405405405405,0.013353534059549444,0.20560240745544434,No,0.0,0.0,0.0
q138,Does proposed end-to-end approach learn in reinforcement or supervised learning manner?,supervised learning,"Reinforcement learning.
",0.5,0.5,0.5,0.0,0.0,0.0,0.5,0.5,0.5,0.0,0.5885686278343201,No,0.0,0.0,0.0
q139,How is performance of this system measured?,using the BLEU score as a quantitative metric and human evaluation for quality,"Performance is measured using EM-score, macro-averaged F$_1$-score, recall rate, F1 score, and overall accuracy.  Normalized metrics are used to compensate for answer distribution imbalances.  Human evaluation using a Likert scale is also used for some systems.
",0.2,0.6153846153846154,0.30188679245283023,0.02564102564102564,0.08333333333333333,0.039215686274509796,0.15,0.46153846153846156,0.22641509433962265,0.011903247365290314,0.45607948303222656,No,0.0,0.0,1.0
q140,How big dataset is used for training this system?,"For the question generation model 15,000 images with 75,000 questions. For the chatbot model, around 460k utterances over 230k dialogues.","The context mentions various datasets, but doesn't specify a single, overall training dataset size.
",0.06666666666666667,0.045454545454545456,0.05405405405405406,0.0,0.0,0.0,0.06666666666666667,0.045454545454545456,0.05405405405405406,0.019108871956733893,0.29719164967536926,No,0.0,0.0,0.0
q141,How do they obtain word lattices from words?,By considering words as vertices and generating directed edges between neighboring words within a sentence,"For a sentence in Chinese, all possible substrings that can be considered as words are treated as vertices.  Neighboring words are connected by directed edges.  Out-of-vocabulary characters are replaced with labels to connect the graph.
",0.2972972972972973,0.7333333333333333,0.42307692307692313,0.1111111111111111,0.2857142857142857,0.16,0.16216216216216217,0.4,0.23076923076923078,0.03511091977922844,0.6797617673873901,Yes,0.0,0.0,1.0
q142,How better is proposed method than baselines perpexity wise?,Perplexity of proposed MEED model is 19.795 vs 19.913 of next best result on test set.,"Answer not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.015738541970947637,0.12303528934717178,No,0.0,0.0,0.0
q143,Who manually annotated the semantic roles for the set of learner texts?,Authors,"Undergraduate students of computational linguistics.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2045402079820633,No,0.0,0.0,0.0
q144,How do they obtain region descriptions and object annotations?,they are available in the Visual Genome dataset,"The text describes a method to mine visual groundings for region annotations, and algorithms to generate questions from COCO and Visual Genome annotations.  Human annotators were used for certain question types.
",0.0967741935483871,0.375,0.15384615384615383,0.03333333333333333,0.14285714285714285,0.05405405405405406,0.0967741935483871,0.375,0.15384615384615383,0.021770022099039287,0.3791799247264862,No,0.0,0.0,1.0
q145,Which training dataset allowed for the best generalization to benchmark sets?,MultiNLI,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06096215546131134,No,0.0,0.0,0.0
q146,Which models were compared?,"BiLSTM-max, HBMP, ESIM, KIM, ESIM + ELMo, and BERT","Final model ""Ours with Mask and Ordered Triplets,"" Baseline, and Ablation models;  MEED;  and various RF models (with and without reduced features) and an aggregated forecaster (Agg).
",0.037037037037037035,0.1111111111111111,0.05555555555555555,0.0,0.0,0.0,0.037037037037037035,0.1111111111111111,0.05555555555555555,0.02144958285037965,0.2831520140171051,No,0.0,0.0,0.0
q147,What is private dashboard?,Private dashboard is leaderboard where competitors can see results after competition is finished - on hidden part of test set (private test set).,"Answer not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.004350967877375741,0.022878330200910568,No,0.0,0.0,0.0
q148,What is public dashboard?,"Public dashboard where competitors can see their results during competition, on part of the test set (public test set).","Answer not found in the context.
",0.16666666666666666,0.05263157894736842,0.08,0.0,0.0,0.0,0.16666666666666666,0.05263157894736842,0.08,0.007942726026489216,0.06462510675191879,No,0.0,0.0,0.0
q149,What dataset do they use?,They used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings and dataset provided in HSD task to train model (details not mentioned in paper).,"StackOverflow, Biomedical, UltraSuite, AIDA-Train and Wikipedia.
",0.2857142857142857,0.08,0.125,0.0,0.0,0.0,0.14285714285714285,0.04,0.0625,0.008248496679006177,0.2247730940580368,No,0.0,0.0,0.0
q150,Do the use word embeddings alone or they replace some previous features of the model with word embeddings?,They use it as addition to previous model - they add new edge between words if word embeddings are similar.,"The context describes the use of word embeddings *in addition* to other features (character embeddings), not as a replacement.
",0.3157894736842105,0.3157894736842105,0.3157894736842105,0.1111111111111111,0.1111111111111111,0.1111111111111111,0.21052631578947367,0.21052631578947367,0.21052631578947367,0.04120278493991909,0.5604454278945923,Yes,0.0,0.0,0.0
q151,How many natural language explanations are human-written?,Totally 6980 validation and test image-sentence pairs have been corrected.,"100
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12353888899087906,No,0.0,0.0,0.0
q152,What is the dataset used as input to the Word2Vec algorithm?,Italian Wikipedia and Google News extraction producing final vocabulary of 618224 words,"100 billion words from Google and Twitter, aggregated global word-word co-occurrence statistics from a corpus, Wikipedia dumps, the whole corpus of the StackOverflow dataset (including question titles and post contents), all titles and abstracts of Biomedical datasets, and BookCorpus datasets.
",0.11904761904761904,0.4166666666666667,0.18518518518518517,0.0,0.0,0.0,0.07142857142857142,0.25,0.11111111111111112,0.01137287189899225,0.4984467625617981,No,0.0,0.0,0.0
q153,What methodology is used to compensate for limited labelled data?,Influential tweeters ( who they define as individuals certain to have a classifiable sentiment regarding the topic at hand) is used to label tweets in bulk in the absence of manually-labeled tweets.,"Lightly supervised learning, self-training, and distantly-supervised learning.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.003806629560664179,0.09852853417396545,No,0.0,0.0,0.0
q154,What are the baseline state of the art models?,"Stanford NER, BiLSTM+CRF, LSTM+CNN+CRF, T-NER and BiLSTM+CNN+Co-Attention","BiMPM, ESIM, Decomposable Attention Model, KIM, and BERT.
",0.25,0.14285714285714285,0.18181818181818182,0.0,0.0,0.0,0.125,0.07142857142857142,0.09090909090909091,0.02549704254217688,0.42062318325042725,No,0.0,0.0,0.0
q155,How do they extract causality from text?,"They identify documents that contain the unigrams 'caused', 'causing', or 'causes'","Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'.
",0.47058823529411764,0.7272727272727273,0.5714285714285714,0.25,0.4,0.3076923076923077,0.47058823529411764,0.7272727272727273,0.5714285714285714,0.04303846266589084,0.7502386569976807,Yes,1.0,1.0,1.0
q156,"What is the source of the ""control"" corpus?","Randomly selected from a Twitter dump, temporally matched to causal documents","Control documents were selected randomly, matched temporally to the causal documents, and did not contain specific words or bidirectional words.
",0.35,0.6363636363636364,0.45161290322580644,0.05263157894736842,0.1,0.06896551724137931,0.25,0.45454545454545453,0.3225806451612903,0.04504556737153658,0.42353057861328125,Yes,0.0,0.0,0.0
q157,"What are the selection criteria for ""causal statements""?","Presence of only the exact unigrams 'caused', 'causing', or 'causes'","Statements with high certainty of being causal statements.
",0.125,0.1,0.11111111111111112,0.0,0.0,0.0,0.125,0.1,0.11111111111111112,0.03416211359799978,0.37696373462677,No,0.0,0.0,0.0
q158,"Do they use expert annotations, crowdsourcing, or only automatic methods to analyze the corpora?",Only automatic methods,"Expert annotations, crowdsourcing, and automatic methods are used.
",0.25,0.6666666666666666,0.36363636363636365,0.14285714285714285,0.5,0.22222222222222224,0.25,0.6666666666666666,0.36363636363636365,0.07495553473355841,0.2978677451610565,No,0.0,0.0,0.0
q159,how do they collect the comparable corpus?,Randomly from a Twitter dump,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.02636869065463543,No,0.0,0.0,0.0
q160,How do they collect the control corpus?,Randomly from Twitter,"The context describes how a ""control"" corpus was created alongside a ""causal"" corpus, but it doesn't describe how the control corpus was specifically collected.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05041629821062088,No,0.0,0.0,0.0
q161,What are resolution model variables?,"Variables in the set {str, prec, attr} indicating in which mode the mention should be resolved.","Resolution mode variables $\Pi = \lbrace \pi _1, \ldots , \pi _n\rbrace $, where for each mention $j$ the variable $\pi _j \in \lbrace str, prec, attr\rbrace $ indicates in which mode the mention should be resolved.
",0.42857142857142855,0.9375,0.588235294117647,0.29411764705882354,0.6666666666666666,0.40816326530612246,0.4,0.875,0.5490196078431373,0.18713608144131694,0.7502115964889526,Yes,1.0,1.0,1.0
q162,Is the model presented in the paper state of the art?,"No, supervised models perform better for this task.","Yes.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4601079821586609,No,0.0,0.0,0.0
q163,What was the result of the highest performing system?,"For task 1 best F1 score was 0.9391 on closed and 0.9414 on open test.
For task2 best result had: Ratio 0.3175 , Satisfaction 64.53, Fluency 0, Turns -1 and Guide 2","Human
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.026075851172208786,No,0.0,0.0,0.0
q164,What do they mean by answer styles?,well-formed sentences vs concise answers,"Answer styles are related to the context of the question in the answer sentence, and can control other styles such as length and speaking styles.  An artificial token corresponding to the target style is introduced at the beginning of the answer sentence.
",0.047619047619047616,0.3333333333333333,0.08333333333333333,0.0,0.0,0.0,0.047619047619047616,0.3333333333333333,0.08333333333333333,0.0,0.3538374900817871,No,0.0,0.0,0.0
q165,What are the baselines that Masque is compared against?,"BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D","Baseline and Ablation models.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12402989715337753,No,0.0,0.0,0.0
q166,What is the performance achieved on NarrativeQA?,"Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1005011796951294,No,0.0,0.0,0.0
q167,"What is an ""answer style""?",well-formed sentences vs concise answers,"An ""answer style"" is related to the context of the question in the answer sentence, and can also control other styles such as length and speaking styles.  It also refers to the expected form of the answer,  typically having four settings: a text span in the passage, human-generated abstractive answer, etc.
",0.057692307692307696,0.5,0.10344827586206896,0.0,0.0,0.0,0.038461538461538464,0.3333333333333333,0.06896551724137931,0.0,0.3599873185157776,No,0.0,0.0,0.0
q168,How are the EAU text spans annotated?,Answer with content missing: (Data and pre-processing section) The data is suited for our experiments because the annotators were explicitly asked to provide annotations on a clausal level.,"The context describes calculating the element-wise sum of GloVe vectors for words within the EAU span and surrounding context, and computing element-wise subtraction of source and target EAU vectors.  It also mentions using a corrected dataset (SNLI-VE-2.0) created through Amazon Mechanical Turk annotations.  However, the *method* of annotating the EAU text spans themselves isn't described.
",0.11475409836065574,0.2413793103448276,0.15555555555555556,0.0,0.0,0.0,0.08196721311475409,0.1724137931034483,0.11111111111111112,0.009799527181568339,0.4090081453323364,No,0.0,0.0,0.0
q169,Which Twitter corpus was used to train the word vectors?,They collected tweets in Russian language using a heuristic query specific to Russian,"1, 7, and 15 days of Twitter data.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.360611230134964,No,0.0,0.0,0.0
q170,How does proposed word embeddings compare to Sindhi fastText word representations?,"Proposed SG model vs SINDHI FASTTEXT:
Average cosine similarity score: 0.650 vs 0.388
Average semantic relatedness similarity score between countries and their capitals: 0.663 vs 0.391","The proposed Sindhi word embeddings have surpassed SdfastText in intrinsic evaluation matrices.
",0.16666666666666666,0.06666666666666667,0.09523809523809522,0.0,0.0,0.0,0.16666666666666666,0.06666666666666667,0.09523809523809522,0.0,0.460237979888916,Yes,0.0,0.0,1.0
q171,How many uniue words are in the dataset?,908456 unique words are available in collected corpus.,"618,224
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2953603267669678,No,0.0,0.0,0.0
q172,Which baseline methods are used?,standard parametrized attention and a non-attention baseline,"K-means (TF), K-means (TF-IDF), SkipVec, RecNN (Ave.), RecNN (SNLI + MultiNLI), Para2vec.  BiRNN language model.  Linear or multi-layer perceptron (MLP) classifiers.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14808562397956848,No,0.0,0.0,0.0
q173,How much is the BLEU score?,Ranges from 44.22 to 100.00 depending on K and the sequence length.,"The context provides several BLEU scores, but no single ""the"" BLEU score.
",0.08333333333333333,0.07142857142857142,0.07692307692307691,0.0,0.0,0.0,0.08333333333333333,0.07142857142857142,0.07692307692307691,0.02908317710573758,0.13657350838184357,No,0.0,0.0,0.0
q174,Which datasets are used in experiments?,Sequence Copy Task and WMT'17,"MCTest, CNN/Daily Mail, Children's Book Test (CBT), and Biomedical.
",0.09090909090909091,0.16666666666666666,0.11764705882352942,0.0,0.0,0.0,0.09090909090909091,0.16666666666666666,0.11764705882352942,0.022869567780619007,0.07437118887901306,No,0.0,0.0,0.0
q175,What are new best results on standard benchmark?,"New best results of accuracy (P@1) on Vecmap:
Ours-GeoMMsemi: EN-IT 50.00 IT-EN 42.67 EN-DE 51.60 DE-EN 47.22 FI-EN 39.62 EN-ES 39.47 ES-EN 36.43","""Ours with Mask and Ordered Triplets"" outperforms Baseline and Ablation models on all metrics in previously seen environments.
",0.1111111111111111,0.05128205128205128,0.07017543859649122,0.0,0.0,0.0,0.05555555555555555,0.02564102564102564,0.03508771929824561,0.012407681605258377,0.25870639085769653,No,0.0,0.0,0.0
q176,How better is performance compared to competitive baselines?,"Proposed method vs best baseline result on Vecmap (Accuracy P@1):
EN-IT: 50 vs 50
IT-EN: 42.67 vs 42.67
EN-DE: 51.6 vs 51.47
DE-EN: 47.22 vs 46.96
EN-FI: 35.88 vs 36.24
FI-EN: 39.62 vs 39.57
EN-ES: 39.47 vs 39.30
ES-EN: 36.43 vs 36.06","The final model ""Ours with Mask and Ordered Triplets"" outperforms baselines on all metrics in previously seen environments, increasing accuracy by 35% and 25% for Exact Match and Goal Match respectively.  Our model also outperforms baselines by a clear margin in new environments.
",0.09302325581395349,0.06153846153846154,0.07407407407407407,0.0,0.0,0.0,0.09302325581395349,0.06153846153846154,0.07407407407407407,0.006597801774623015,0.4331713914871216,No,0.0,0.0,0.0
q177,What 6 language pairs is experimented on?,"EN<->ES
EN<->DE
EN<->IT
EN<->EO
EN<->MS
EN<->FI","English, French, German, Spanish, Italian, and Russian.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4605304002761841,No,0.0,0.0,0.0
q178,How do they enrich the positional embedding with length information,They introduce new trigonometric encoding which besides information about position uses additional length information (abs or relative).,"The provided text describes how a segment of 1 is assigned to a sentence until a [SEP] character appears, then it becomes 0.  A position embedding layer adds information related to the token's position in the sentence.  No specific enrichment of the positional embedding with length information is described.
",0.08,0.23529411764705882,0.11940298507462688,0.02040816326530612,0.0625,0.030769230769230767,0.08,0.23529411764705882,0.11940298507462688,0.01662306444973684,0.403242290019989,No,0.0,0.0,0.0
q179,How do they condition the output to a given target-source class?,"They use three groups short/normal/long translation classes to learn length token, which is in inference used to bias network to generate desired length group.","The condition vector, a text string representing the class label (e.g., ""Red bird""), is fed to both the generator and discriminator.  It is important that the condition vector is related to the real data.
",0.14285714285714285,0.19230769230769232,0.16393442622950818,0.0,0.0,0.0,0.11428571428571428,0.15384615384615385,0.13114754098360656,0.01188432823684058,0.2917085289955139,No,0.0,0.0,0.0
q180,Is this library implemented into Torch or is framework agnostic?,It uses deep learning framework (pytorch),"PyTorch.
",1.0,0.16666666666666666,0.2857142857142857,0.0,0.0,0.0,1.0,0.16666666666666666,0.2857142857142857,0.0,0.5114911198616028,Yes,1.0,1.0,1.0
q181,How does this compare to traditional calibration methods like Platt Scaling?,No reliability diagrams are provided and no explicit comparison is made between confidence scores or methods.,"The answer is not found in the context.
",0.125,0.0625,0.08333333333333333,0.0,0.0,0.0,0.125,0.0625,0.08333333333333333,0.023309208103306348,0.061772335320711136,No,0.0,0.0,0.0
q182,What's the input representation of OpenIE tuples into the model?,word embeddings,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.17882280051708221,No,0.0,0.0,0.0
q183,"Is CRWIZ already used for data collection, what are the results?","Yes, CRWIZ has been used for data collection and its initial use resulted in 145 dialogues. The average time taken for the task was close to the estimate of 10 minutes, 14 dialogues (9.66%) resolved the emergency in the scenario, and these dialogues rated consistently higher in subjective and objective ratings than those which did not resolve the emergency. Qualitative results showed that participants believed that they were interacting with an automated assistant.","Yes, CRWIZ is used for data collection.  The context describes a comparison of the CRWIZ dataset to one collected in a lab setting, but no results are presented.
",0.42857142857142855,0.16216216216216217,0.23529411764705885,0.14814814814814814,0.0547945205479452,0.08,0.39285714285714285,0.14864864864864866,0.21568627450980393,0.028330566759793466,0.7087149620056152,No,0.0,0.0,0.0
q184,What contextual features are used?,The words that can indicate the characteristics of the neighbor words as contextual keywords and generate it from the automatically extracted contextual keywords.,"Automatically extracted contextual keywords, jointly learned during training.
",0.5,0.17391304347826086,0.25806451612903225,0.42857142857142855,0.13636363636363635,0.20689655172413793,0.5,0.17391304347826086,0.25806451612903225,0.041389967849713896,0.6636877059936523,Yes,1.0,1.0,1.0
q185,How is the data in RAFAEL labelled?,"Using a set of annotation tools such as Morfeusz, PANTERA, Spejd, NERF and Liner","The authors projected word-sense annotations of publicly available corpora to supersenses and applied perceptron-trained Hidden Markov Model for sequence classification.  They also use labels such as plant, animal, or shape.
",0.1875,0.42857142857142855,0.26086956521739124,0.03225806451612903,0.07692307692307693,0.045454545454545456,0.09375,0.21428571428571427,0.13043478260869562,0.02865096034594708,0.3742852210998535,No,0.0,1.0,1.0
q186,How is the fluctuation in the sense of the word and its neighbors measured?,"Our method performs a statistical test to determine whether a given word is used polysemously in the text, according to the following steps:
1) Setting N, the size of the neighbor.
2) Choosing N neighboring words ai in the order whose angle with the vector of the given word w is the smallest.
3) Computing the surrounding uniformity for ai(0 < i ≤ N) and w.
4) Computing the mean m and the sample variance σ for the uniformities of ai .
5) Checking whether the uniformity of w is less than m − 3σ. If the value is less than m − 3σ, we may regard w as a polysemic word.","Using the concept of surrounding uniformity, which roughly corresponds to statistical fluctuation in the vectors of neighboring words.
",0.7222222222222222,0.12149532710280374,0.20800000000000002,0.29411764705882354,0.04716981132075472,0.0813008130081301,0.4444444444444444,0.07476635514018691,0.128,0.0003457104224869728,0.6514769792556763,No,0.0,0.0,0.0
q187,By how much does SPNet outperforms state-of-the-art abstractive summarization methods on evaluation metrics?,"SPNet vs best baseline:
ROUGE-1: 90.97 vs 90.68
CIC: 70.45 vs 70.25","SPNet outperforms Pointer-Generator, Transformer, and other baseline methods by a significant margin.
",0.15384615384615385,0.11764705882352941,0.13333333333333333,0.0,0.0,0.0,0.15384615384615385,0.11764705882352941,0.13333333333333333,0.03125190763972442,0.5005999803543091,No,1.0,1.0,1.0
q188,"Is it expected to have speaker role, semantic slot and dialog domain annotations in real world datasets?","Not at the moment, but summaries can be additionaly extended with this annotations.","Yes.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3343122601509094,No,0.0,1.0,1.0
q189,How does new evaluation metric considers critical informative entities?,Answer with content missing: (formula for CIC) it accounts for the most important information within each dialog domain. CIC can be applied to any summarization task with predefined essential entities,"The new metric, Critical Information Completeness (CIC), is a recall of semantic slot information between a candidate and reference summary.  It considers delexicalized values in the reference summary.
",0.14285714285714285,0.13333333333333333,0.1379310344827586,0.0,0.0,0.0,0.10714285714285714,0.1,0.10344827586206898,0.02865096034594708,0.5556213855743408,Yes,0.0,0.0,1.0
q190,What are state of the art methods MMM is compared to?,"FTLM++, BERT-large, XLNet","MAN classifier, FCNN classifier, MCB, NMN, MLP, MTMSNlarge, MTMSNbase, large-squad, MC160, MC500.
",0.07692307692307693,0.25,0.11764705882352941,0.0,0.0,0.0,0.07692307692307693,0.25,0.11764705882352941,0.020540268312306344,0.21771541237831116,No,0.0,0.0,0.0
q191,What are the problems related to ambiguity in PICO sentence prediction tasks?,Some sentences are associated to ambiguous dimensions in the hidden state output,"The assignment of a single label limits the quality of predictions for use in real-world reviewing tasks.
",0.1111111111111111,0.16666666666666666,0.13333333333333333,0.0,0.0,0.0,0.05555555555555555,0.08333333333333333,0.06666666666666667,0.0255404966647159,0.18694154918193817,No,1.0,1.0,1.0
q192,How is knowledge stored in the memory?,entity memory and relational memory.,"The context describes storing information in memory in a hierarchical structure, with recent memories prioritized, and using memory slots to model entities and relations in text.  It also mentions using LSTM networks to encode contextual word sequences and entity descriptions as fixed-size vectors.
",0.11363636363636363,1.0,0.20408163265306123,0.023255813953488372,0.25,0.0425531914893617,0.06818181818181818,0.6,0.1224489795918367,0.011622974570266495,0.520897626876831,No,0.0,0.0,0.0
q193,How do they measure the diversity of inferences?,by number of distinct n-grams,"The context describes how NE and token/type diversity are measured for corpora, but does not describe how diversity of inferences is measured.  The answer is not found in the context.
",0.03225806451612903,0.16666666666666666,0.05405405405405405,0.0,0.0,0.0,0.03225806451612903,0.16666666666666666,0.05405405405405405,0.010559717178170818,0.1511230617761612,No,0.0,0.0,0.0
q194,By how much do they improve the accuracy of inferences over state-of-the-art methods?,"ON Event2Mind, the accuracy of proposed method is improved by  absolute BLUE  2.9,  10.87, 1.79 for xIntent, xReact and oReact respectively.
On Atomic dataset, the accuracy of proposed method is improved by  absolute BLUE 3.95.   4.11, 4.49 for xIntent, xReact and oReact.respectively.","The context mentions accuracy improvements for specific methods (e.g., 95.4%, 95.0%), but does not quantify the improvement *over* state-of-the-art methods.  The answer is not found in the context.
",0.35294117647058826,0.24489795918367346,0.2891566265060241,0.030303030303030304,0.020833333333333332,0.024691358024691357,0.23529411764705882,0.16326530612244897,0.19277108433734938,0.012214529534378642,0.4739122986793518,No,0.0,0.0,0.0
q195,How does the context-aware variational autoencoder learn event background information?," CWVAE is trained on an auxiliary dataset to learn the event background information by using the context-aware latent variable.  Then, in finetute stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target.","In the pretrain stage, CWVAE is trained on an auxiliary dataset with rich event context information.  The context-acquiring latent variable $z_c$ is directly conditioned on the context, allowing it to acquire background knowledge.  Then, a context-aware latent variable $z_{c'}$ learns background knowledge from $z_c$.
",0.5102040816326531,0.5319148936170213,0.5208333333333334,0.25,0.2608695652173913,0.2553191489361702,0.3877551020408163,0.40425531914893614,0.39583333333333326,0.18418691262061235,0.88116854429245,Yes,0.0,1.0,1.0
q196,How much improvement does their method get over the fine tuning baseline?,"0.08 points on the 2011 test set, 0.44 points on the 2012 test set, 0.42 points on the 2013 test set for IWSLT-CE.","Not found in the context.
",0.2,0.037037037037037035,0.0625,0.0,0.0,0.0,0.2,0.037037037037037035,0.0625,0.0034434089760123476,0.05937383323907852,No,0.0,0.0,0.0
q197,By how much do they outpeform previous results on the word discrimination task?,Their best average precision tops previous best result by 0.202,"The provided text states that the approach ""establishes a new state of the art for classifying hate speech, outperforming previous results by as much as 12 F1 points"", but does not provide any information on the word discrimination task.  Therefore, the answer is not found in the context.
",0.0625,0.2727272727272727,0.10169491525423728,0.02127659574468085,0.1,0.03508771929824561,0.0625,0.2727272727272727,0.10169491525423728,0.007860561859628806,0.1581307351589203,No,0.0,0.0,0.0
q198,How many paraphrases are generated per question?,"10*n paraphrases, where n depends on the number of paraphrases that contain the entity mention spans","10
",1.0,0.058823529411764705,0.1111111111111111,0.0,0.0,0.0,1.0,0.058823529411764705,0.1111111111111111,0.0,0.24006445705890656,No,0.0,0.0,0.0
q199,How strong was the correlation between exercise and diabetes?,weak correlation with p-value of 0.08,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1120087206363678,No,0.0,0.0,0.0
q200,How were topics of interest about DDEO identified?,using topic modeling model Latent Dirichlet Allocation (LDA),"Semantic and linguistics analyses of tweets containing DDEO words, using Latent Dirichlet Allocation (LDA) topic modeling.
",0.4375,0.875,0.5833333333333334,0.26666666666666666,0.5714285714285714,0.36363636363636365,0.3125,0.625,0.4166666666666667,0.27322315104876443,0.5820923447608948,Yes,0.0,1.0,1.0
q201,How do their train their embeddings?,"The embeddings are learned several times using the training set, then the average is taken.","They train their own word embeddings using CBOW (Continuous Bag-of-Words) and convert texts to sequences of integers, assigning each word a unique number from 1 to V (vocabulary size).  They also use character composition and pre-trained GloVe vectors.  Fine-tuning of pre-trained GloVe embeddings on the training data was also explored.
",0.07272727272727272,0.26666666666666666,0.1142857142857143,0.018518518518518517,0.07142857142857142,0.029411764705882353,0.07272727272727272,0.26666666666666666,0.1142857142857143,0.016474108593061365,0.4760701656341553,No,0.0,0.0,0.0
q202,How do they model travel behavior?,The data from collected travel surveys is used to model travel behavior.,"They use random utility maximization (RUM) models, which are predominantly numeric, linear, and often flat.  They also introduce Travel Behavior embeddings, borrowing from NLP concepts, to encode categorical variables.
",0.20689655172413793,0.5,0.2926829268292683,0.03571428571428571,0.09090909090909091,0.05128205128205127,0.13793103448275862,0.3333333333333333,0.1951219512195122,0.012753998269755913,0.5268919467926025,Yes,0.0,0.0,0.0
q203,How do their interpret the coefficients?,The coefficients are projected back to the dummy variable space.,"The context states that concreteness is a significant negative predictor of change toward moral relevance, meaning abstract concepts are more strongly associated with increasing moral relevance over time.  For dummy variables, coefficients have a direct interpretation, but for encoded variables (with K>1), individual coefficients lack meaning.
",0.10638297872340426,0.5,0.17543859649122806,0.021739130434782608,0.1111111111111111,0.03636363636363637,0.0851063829787234,0.4,0.14035087719298245,0.009702695924258075,0.32057899236679077,No,0.0,0.0,1.0
q204,By how much do they outperform previous state-of-the-art models?,"Proposed ORNN has 0.769, 1.238, 0.818, 0.772 compared to 0.778, 1.244, 0.813, 0.781 of best state of the art result on Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.)","Not found in the context.
",0.2,0.02,0.03636363636363636,0.0,0.0,0.0,0.2,0.02,0.03636363636363636,1.1912050971796437e-05,0.022967610508203506,No,0.0,0.0,0.0
q205,What is the performance difference between proposed method and state-of-the-arts on these datasets?,Difference is around 1 BLEU score lower on average than state of the art methods.,"TRADE, the state-of-the-art model, performs poorly in this setting.  The proposed method (""Ours with Mask and Ordered Triplets"") outperforms the baseline and ablation models on all metrics in previously seen environments.
",0.17647058823529413,0.4,0.2448979591836735,0.09090909090909091,0.21428571428571427,0.1276595744680851,0.14705882352941177,0.3333333333333333,0.20408163265306123,0.012413584734794654,0.24413886666297913,No,0.0,0.0,0.0
q206,What benchmarks are created?,Answer with content missing: (formula) The accuracy is defined as the ratio # of correct chains predicted to # of evaluation samples,"SQuAD
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.025414910167455673,No,0.0,0.0,0.0
q207,What percentage fewer errors did professional translations make?,36%,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.17132845520973206,No,0.0,0.0,0.0
q208,What was the weakness in Hassan et al's evaluation design?,"MT developers to which crowd workers were compared are usually not professional translators, evaluation of sentences in isolation prevents raters from detecting translation errors, used not originally written Chinese test set
","The answer is not found in the context.
",0.25,0.06451612903225806,0.10256410256410256,0.0,0.0,0.0,0.25,0.06451612903225806,0.10256410256410256,0.003939566541029747,0.17549961805343628,No,0.0,0.0,0.0
q209,What evidence do they present that the model attends to shallow context clues?,Using model gradients with respect to input features they presented that the most important model inputs are verbs associated with entities which shows that the model attends to shallow context clues,"Figure FIGREF37 shows that the model learns to extract shallow clues of identifying actions exerted upon only the entity being tracked, regardless of other entities, by leveraging verb semantics.
",0.3448275862068966,0.3225806451612903,0.33333333333333337,0.10714285714285714,0.1,0.10344827586206898,0.2413793103448276,0.22580645161290322,0.23333333333333334,0.08893868599300615,0.6362320184707642,No,0.0,0.0,1.0
q210,In what way is the input restructured?,"In four entity-centric ways - entity-first, entity-last, document-level and sentence-level","The input is restructured by an embedding layer that combines token embeddings, segmentation embeddings, and position embeddings.
",0.058823529411764705,0.07142857142857142,0.06451612903225808,0.0,0.0,0.0,0.058823529411764705,0.07142857142857142,0.06451612903225808,0.025197593442434794,0.3425677418708801,No,0.0,0.0,0.0
q211,What language is the Twitter content in?,English,"English
",1.0,1.0,1.0,0.0,0.0,0.0,1.0,1.0,1.0,0.0,1.0,Yes,0.0,1.0,1.0
q212,What evaluations did the authors use on their system?,"BLEU scores, exact matches of words in both translations and topic cache, and cosine similarities of adjacent sentences for coherence.","Human evaluation of responses' grammatical correctness, contextual coherence, and emotional appropriateness; relative ranking; direct assessment; head-to-head evaluation.
",0.15789473684210525,0.15,0.15384615384615385,0.0,0.0,0.0,0.10526315789473684,0.1,0.10256410256410256,0.04334264033674369,0.49728924036026,No,0.0,0.0,0.0
q213,What accuracy does CNN model achieve?,Combined per-pixel accuracy for character line segments is 74.79,"The CNN accuracy drops from around 0.8 to 0.5 when noise rate increases from 0 to 0.5.
",0.05,0.09090909090909091,0.06451612903225806,0.0,0.0,0.0,0.05,0.09090909090909091,0.06451612903225806,0.021476912089159056,0.40770211815834045,No,0.0,0.0,0.0
q214,How many documents are in the Indiscapes dataset?,508,"508
",1.0,1.0,1.0,0.0,0.0,0.0,1.0,1.0,1.0,0.0,1.0,Yes,0.0,1.0,1.0
q215,What are simulated datasets collected?,There are 6 simulated datasets collected which is initialised with a corpus of size 550 and simulated by generating new documents from Wikipedia extracts and replacing existing documents,"A dataset of 8066 pairs of free-form natural language instructions and navigation plans for training behavioral robot navigation, and a prototype dataset based on videos from the Charades dataset.
",0.2,0.21428571428571427,0.20689655172413796,0.0,0.0,0.0,0.13333333333333333,0.14285714285714285,0.1379310344827586,0.017934437633383676,0.3897682726383209,No,0.0,0.0,0.0
q216,What human evaluation metrics were used in the paper?,rating questions on a scale of 1-5 based on fluency of language used and relevance of the question to the context,"Relevance, conciseness, and readability on a 1 to 5 scale.
",0.8,0.36363636363636365,0.5000000000000001,0.1111111111111111,0.047619047619047616,0.06666666666666667,0.4,0.18181818181818182,0.25000000000000006,0.03915615274533091,0.6892790794372559,No,0.0,0.0,0.0
q217,"For the purposes of this paper, how is something determined to be domain specific knowledge?",reviews under distinct product categories are considered specific domain knowledge,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.061315350234508514,No,0.0,0.0,0.0
q218,What type of model are the ELMo representations used in?,A bi-LSTM with max-pooling on top of it,"CNNs and bi-directional LSTMs.
",0.4,0.2,0.26666666666666666,0.0,0.0,0.0,0.4,0.2,0.26666666666666666,0.0,0.4516381025314331,No,0.0,0.0,0.0
q219,By how much does using phonetic feedback improve state-of-the-art systems?,Improved AECNN-T by 2.1 and AECNN-T-SM BY 0.9,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07014480978250504,No,0.0,1.0,1.0
q220,what are the baselines?,"AS Reader, GA Reader, CAS Reader","content, content-network, image-based models; BiRNN language model; GA model; LSTM baseline;  TF-IDF, S2S, IR, and Proposed+IR
",0.05,0.16666666666666666,0.07692307692307691,0.0,0.0,0.0,0.05,0.16666666666666666,0.07692307692307691,0.018264728862681344,0.2384779304265976,No,0.0,0.0,0.0
q221,What are the results of the experiment?,"They were able to create a language model from the dataset, but did not test.","The provided text describes experiments, but does not present results in a summary form.  The text describes *what* experiments were conducted and *how* they were performed, but not the *results* of those experiments.  The answer is not found in the context.
",0.14634146341463414,0.4,0.21428571428571425,0.025,0.07142857142857142,0.03703703703703704,0.0975609756097561,0.26666666666666666,0.14285714285714285,0.01915654182812138,0.3427348732948303,No,0.0,0.0,0.0
q222,How was the dataset collected?,extracted text from Sorani Kurdish books of primary school and randomly created sentences,"The datasets were collected from Kaggle.com (StackOverflow), BioASQ's official website (Biomedical), and crowd sourcing or from examinations designed by educational experts (multi-choice QA).  For some datasets, 20,000 question titles or paper titles from 20 different tags were randomly selected.
",0.06976744186046512,0.23076923076923078,0.10714285714285714,0.0,0.0,0.0,0.06976744186046512,0.23076923076923078,0.10714285714285714,0.009216168770877761,0.22320011258125305,No,0.0,0.0,0.0
q223,How many annotators participated?,1,"Four annotators.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12334174662828445,No,0.0,0.0,0.0
q224,How do they show their model discovers underlying syntactic structure?,By visualizing syntactic distance estimated by the parsing network,"The model implicitly models tree structure through attention.
",0.125,0.1111111111111111,0.11764705882352941,0.0,0.0,0.0,0.125,0.1111111111111111,0.11764705882352941,0.0,0.31617555022239685,No,0.0,0.0,0.0
q225,How do they measure performance of language model tasks?,"BPC, Perplexity","Accuracy, EM-score, and F$_1$-score.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.033864985683445356,0.0479465126991272,No,0.0,0.0,0.0
q226,How are content clusters used to improve the prediction of incident severity?,they are used as additional features in a supervised classification task,"The method uses machine learning to predict the degree of harm of an incident based on features of the record (location, external category, medical specialty) and the content clusters.
",0.06896551724137931,0.18181818181818182,0.1,0.0,0.0,0.0,0.06896551724137931,0.18181818181818182,0.1,0.010885011049519644,0.2328372597694397,Yes,0.0,0.0,1.0
q227,What cluster identification method is used in this paper?,"A combination of Minimum spanning trees, K-Nearest Neighbors and Markov Stability BIBREF15, BIBREF16, BIBREF17, BIBREF18","A simple clustering algorithm that initializes with random cluster assignments and iteratively proposes random exchanges of memberships is used.
",0.15789473684210525,0.1875,0.17142857142857143,0.0,0.0,0.0,0.10526315789473684,0.125,0.11428571428571428,0.025197593442434794,0.22369611263275146,No,0.0,0.0,0.0
q228,Why did they think this was a good idea?,They think it will help human TCM practitioners make prescriptions.,"They believed the superior visual grounding of their method would generate natural language explanations justifying answers to visual questions, thus increasing the capabilities of AI-based technologies to explain their decisions.
",0.03225806451612903,0.1,0.04878048780487805,0.0,0.0,0.0,0.03225806451612903,0.1,0.04878048780487805,0.013794462224541233,0.385833740234375,No,0.0,0.0,0.0
q229,What QA models were used?,"A pointer network decodes the answer from a bidirectional LSTM with attention flow layer and self-matching layer, whose inputs come from word and character embeddings of the query and input text fed through a highway layer.","QA-BERT, XLM, XLNet, DistilBERT, and ALBERT.
",0.14285714285714285,0.02702702702702703,0.045454545454545456,0.0,0.0,0.0,0.14285714285714285,0.02702702702702703,0.045454545454545456,0.004237000139470399,0.21394199132919312,No,0.0,0.0,0.0
q230,How do their results compare against other competitors in the PAN 2017 shared task on Author Profiling?,"They achieved best result in the PAN 2017 shared task with accuracy for Variety prediction task 0.0013 more than the 2nd best baseline, accuracy for Gender prediction task 0.0029 more than 2nd best baseline and accuracy for Joint prediction task 0.0101 more than the 2nd best baseline","N-GrAM was the best-performing system out of 22 submissions.
",0.18181818181818182,0.04,0.06557377049180327,0.0,0.0,0.0,0.18181818181818182,0.04,0.06557377049180327,0.000938703292510158,0.31993725895881653,Yes,0.0,0.0,0.0
q231,On which task does do model do worst?,Gender prediction task,"Arithmetic problems.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14608773589134216,No,0.0,0.0,0.0
q232,On which task does do model do best?,Variety prediction task,"Cooperative game.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14636975526809692,No,0.0,0.0,0.0
q233,How does counterfactual data augmentation aim to tackle bias?,The training dataset is augmented by swapping all gendered words by their other gender counterparts,"The provided context does not describe how counterfactual data augmentation aims to tackle bias.  It discusses counterfactuals in the context of evaluating model truthfulness, but not in relation to bias mitigation.
",0.06451612903225806,0.13333333333333333,0.08695652173913043,0.0,0.0,0.0,0.06451612903225806,0.13333333333333333,0.08695652173913043,0.010885011049519644,0.3165391683578491,No,0.0,0.0,0.0
q234,"In the targeted data collection approach, what type of data is targetted?",Gendered characters in the dataset,"Entity or entity aspect.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05290309339761734,No,0.0,0.0,0.0
q235,How do they determine which words are informative?,Informative are those that will not be suppressed by regularization performed.,"Informative words are nouns and verbs.  Two are matched if their raw text, lemmatizations, WordNet synsets, or aliases are the same.
",0.09523809523809523,0.18181818181818182,0.125,0.0,0.0,0.0,0.09523809523809523,0.18181818181818182,0.125,0.019011755113058346,0.5115741491317749,No,0.0,0.0,0.0
q236,What improvement does the MOE model make over the SOTA on language modelling?,Perpexity is improved from 34.7 to 28.0.,"The context states that the larger MoE model has a similar computational budget to the best published model from the literature, and a lower test perplexity after 10 epochs.
",0.06896551724137931,0.2222222222222222,0.10526315789473684,0.0,0.0,0.0,0.034482758620689655,0.1111111111111111,0.05263157894736842,0.015784309053930103,0.33820176124572754,No,0.0,0.0,0.0
q237,What is the difference in performance between the interpretable system (e.g. vectors and cosine distance) and LSTM with ELMo system?,Accuracy of best interpretible system was 0.3945 while accuracy of LSTM-ELMo net was 0.6818.,"The context states that the ELMo-based model under-performed the first-occurrence baseline, implying the LSTM with ELMo system performed worse than a simpler baseline.  No direct comparison to an interpretable system (vectors and cosine distance) is provided.
",0.10256410256410256,0.23529411764705882,0.14285714285714282,0.02631578947368421,0.0625,0.037037037037037035,0.05128205128205128,0.11764705882352941,0.07142857142857141,0.010648525923253459,0.5670023560523987,No,0.0,0.0,0.0
q238,Which language-pair had the better performance?,French-English,"English-French
",1.0,1.0,1.0,0.0,0.0,0.0,0.5,0.5,0.5,0.0,0.9834061861038208,No,0.0,0.0,0.0
q239,Which psycholinguistic and basic linguistic features are used?,"Emotion Sensor Feature, Part of Speech, Punctuation, Sentiment Analysis, Empath, TF-IDF Emoticon features","Ratio of quotation marks, exclamation marks, modal verbs, stop words, type-token ratio, hedging, named entity types, POS n-grams, sentiment and subjectivity scores, spell-checking, readability features (Coleman-Liau, Flesch), argument lexicon features, word lengths, sentence lengths, word/usage/linguistic-features, emoticons, and punctuation features.
",0.13043478260869565,0.42857142857142855,0.2,0.0,0.0,0.0,0.08695652173913043,0.2857142857142857,0.13333333333333333,0.00935246702041104,0.7301424145698547,No,0.0,1.0,1.0
q240,How have the differences in communication styles between Twitter and Facebook increase the complexity of the problem?,Systems do not perform well both in Facebook and Twitter texts,"The context does not contain information about Facebook communication styles.  Therefore, the answer is not found in the context.
",0.15789473684210525,0.2727272727272727,0.19999999999999998,0.0,0.0,0.0,0.10526315789473684,0.18181818181818182,0.13333333333333333,0.022731543567022873,0.385894238948822,No,0.0,0.0,0.0
q241,What data/studies do the authors provide to support the assertion that the majority of aggressive conversations contain code-mixed languages?,None,"The authors cite the TRAC-2018 dataset as providing code-mixed data.  They specifically mention a training dataset provided by TRAC.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.042724400758743286,No,0.0,0.0,0.0
q242,What datasets are used to evaluate the model?,"WN18, FB15k","Swissmetro, development, and test sets.  Also, news (bn and nw), and a portion of bc.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.016466642419110007,0.21038633584976196,No,0.0,0.0,0.0
q243,What baseline models do they compare against?,"SLQA, Rusalka, HMA Model (single), TriAN (single), jiangnan (ensemble), MITRE (ensemble), TriAN (ensemble), HMA Model (ensemble)","NABERT+ baseline, Stanford system, Multigraph system, MIR system.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.007484212446303174,0.538356602191925,No,0.0,0.0,0.0
q244,What are the differences with previous applications of neural networks for this task?,This approach considers related images,"The context mentions further exploration of feature extraction techniques (e.g., Constant-Q transform, FFT), improvements in neural network architectures (e.g., concatenating 1D/2D ConvNets, deeper networks with skip connections/Inception modules), and experiments using different CNN variants.  It also highlights the use of LSTMs, GRNNs, and deep layers of neural networks to learn character representations, contrasting with earlier methods not explicitly detailed.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06444872915744781,No,0.0,0.0,0.0
q245,How much improvement is gained from the proposed approaches?,It eliminates non-termination in some models fixing for some models up to 6% of non-termination ratio.,"The context does not state a specific improvement value for the proposed approaches.  It mentions outperformance in some comparisons but does not give numerical gains.
",0.16,0.2222222222222222,0.18604651162790697,0.041666666666666664,0.058823529411764705,0.048780487804878044,0.12,0.16666666666666666,0.13953488372093023,0.0349018070868319,0.33853697776794434,No,0.0,0.0,0.0
q246,Is infinite-length sequence generation a result of training with maximum likelihood?,There are is a strong conjecture that it might be the reason but it is not proven.,"Not found in the context.
",0.4,0.11764705882352941,0.1818181818181818,0.0,0.0,0.0,0.2,0.058823529411764705,0.0909090909090909,0.013063150080561919,0.1049891859292984,No,1.0,1.0,1.0
q247,How big is dataset for this challenge?,"133,287 images","The provided text describes various datasets with sizes ranging from 20,000 to 3,370,528 samples,  287,226 documents, and more.  No single ""challenge"" dataset size is explicitly stated.
",0.03333333333333333,0.3333333333333333,0.0606060606060606,0.0,0.0,0.0,0.03333333333333333,0.3333333333333333,0.0606060606060606,0.0,0.24108906090259552,No,0.0,0.0,0.0
q248,How better is performance of proposed model compared to baselines?,"Proposed model achive 66+-22 win rate, baseline CNN 13+-1  and baseline FiLM 32+-3 .","The proposed model outperforms baselines by a clear margin in new environments, with improvements of INLINEFORM0 EM and GM compared to the second-best model.  In previously seen environments, the model increases accuracy by 35% and 25% for Exact Match and Goal Match, respectively, compared to the baseline.  On task A, with sufficient training data, the single system performs better than a strong feature-rich system.  The model also substantially improves over base models on DROP's development set across all metrics, surpassing the NABERT+ baseline.
",0.05813953488372093,0.3125,0.09803921568627452,0.011764705882352941,0.06666666666666667,0.02,0.05813953488372093,0.3125,0.09803921568627452,0.006022210197881272,0.4304019808769226,No,0.0,0.0,0.0
q249,What DCGs are used?,Author's own DCG rules are defined from scratch.,"DGC-max, DGC-ave, and DGC-gated.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.049351578415363785,0.3797391951084137,No,0.0,0.0,0.0
q250,What is the performance difference of using a generated summary vs. a user-written one?,2.7 accuracy points,"The models work consistently better when a user-written gold summary is used compared to a system-generated summary.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13657426834106445,Yes,0.0,0.0,0.0
q251,What evaluation metrics did they look at?,accuracy with standard deviation,"Automatic evaluation metrics, relevance, conciseness, readability, task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, task ease, user expertise, F1-score, Recall rate, F1 score, Overall accuracy.
",0.03333333333333333,0.25,0.058823529411764705,0.0,0.0,0.0,0.03333333333333333,0.25,0.058823529411764705,0.008130350574508933,0.2350306510925293,No,0.0,0.0,0.0
q252,What are the datasets used for the task?,"Datasets used are Celex (English, Dutch), Festival (Italian), OpenLexuque (French), IIT-Guwahati (Manipuri), E-Hitz (Basque)","Stanford Sentiment Treebank (SST-2), Yelp dataset, Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), Quora Question Pairs, MCTest, CNN/Daily Mail, Children's Book Test, and a dataset of simulated indoor environments.
",0.027777777777777776,0.0625,0.03846153846153846,0.0,0.0,0.0,0.027777777777777776,0.0625,0.03846153846153846,0.030011156837936422,0.5038681626319885,No,0.0,0.0,0.0
q253,What is the accuracy of the model for the six languages tested?,"Authors report their best models have following accuracy: English CELEX (98.5%), Dutch CELEX (99.47%), Festival (99.990%), OpenLexique (100%), IIT-Guwahat (95.4%), E-Hitz (99.83%)","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.02340940572321415,No,0.0,0.0,0.0
q254,Which models achieve state-of-the-art performances?,"CELEX (Dutch and English) - SVM-HMM
Festival, E-Hitz and OpenLexique - Liang hyphenation
IIT-Guwahat - Entropy CRF","BERT-based ensemble models, and specifically RoBERTa<sub>large</sub>, achieve state-of-the-art performance.
",0.0625,0.058823529411764705,0.06060606060606061,0.0,0.0,0.0,0.0625,0.058823529411764705,0.06060606060606061,0.02276859592073037,0.22358335554599762,No,0.0,0.0,0.0
q255,Which competitive relational classification models do they test?,For relation prediction they test TransE and for relation extraction they test position aware neural sequence model,"Spacy, Stanford NER, and Flair.
",0.2,0.058823529411764705,0.0909090909090909,0.0,0.0,0.0,0.2,0.058823529411764705,0.0909090909090909,0.017928600661603712,0.254048615694046,No,0.0,0.0,0.0
q256,How do they gather human judgements for similarity between relations?,By assessing similarity of 360 pairs of relations from a subset of Wikidata using an integer similarity score from 0 to 4,"The provided text states experiments and analysis were aimed at answering how well the computed similarity score correlates with human judgment about the similarity between relations.  It does not describe *how* human judgments were gathered.
",0.11428571428571428,0.18181818181818182,0.14035087719298245,0.029411764705882353,0.047619047619047616,0.036363636363636355,0.05714285714285714,0.09090909090909091,0.07017543859649122,0.02243388007668207,0.4516100585460663,No,0.0,0.0,0.0
q257,What text classification task is considered?,To classify a text as belonging to one of the ten possible classes.,"Question classification; predicting the degree of harm (DoH) among five possible values (1-5).
",0.21428571428571427,0.23076923076923078,0.22222222222222224,0.0,0.0,0.0,0.14285714285714285,0.15384615384615385,0.14814814814814817,0.025683319547529764,0.2946854531764984,No,0.0,0.0,0.0
q258,What novel class of recurrent-like networks is proposed?,"A network, whose learned functions satisfy a certain equation. The  network contains RNN cells with either nested internal memories or dependencies that extend temporally beyond the immediately previous hidden state.","A neural network architecture that leverages both recurrence and one-dimensional convolutions.
",0.25,0.1,0.14285714285714288,0.0,0.0,0.0,0.25,0.1,0.14285714285714288,0.008322437662428014,0.4197227358818054,No,0.0,0.0,0.0
q259,How much bigger is Switchboard-2000 than Switchboard-300 database?,Switchboard-2000 contains 1700 more hours of speech data.,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04767707020457095,0.04205717891454697,No,0.0,0.0,0.0
q260,What domains are detected in this paper?,"Answer with content missing: (Experimental setup not properly rendered) In our experiments we used seven target domains: “Business and Commerce” (BUS), “Government and Politics” (GOV), “Physical and Mental Health” (HEA), “Law and Order” (LAW),
“Lifestyle” (LIF), “Military” (MIL), and “General Purpose” (GEN). Exceptionally, GEN does
not have a natural root category.","Lifestyle, scientific.
",0.5,0.0196078431372549,0.03773584905660377,0.0,0.0,0.0,0.5,0.0196078431372549,0.03773584905660377,1.7547359743938743e-09,0.2561590075492859,No,0.0,0.0,1.0
q261,Why do they think this task is hard?  What is the baseline performance?,"1. there may be situations where more than one action is reasonable, and also because writers tell a story playing with elements such as surprise or uncertainty.
2. Macro F1 = 14.6 (MLR, length 96 snippet)
Weighted F1 = 31.1 (LSTM, length 128 snippet)","The task is hard because baselines lag behind human performance by 16.4% and 32.1% for Quasar-S and Quasar-T respectively.
",0.17391304347826086,0.09090909090909091,0.11940298507462685,0.0,0.0,0.0,0.13043478260869565,0.06818181818181818,0.08955223880597014,0.005703777119494772,0.12726454436779022,No,0.0,0.0,1.0
q262,How do they generate the synthetic dataset?,using generative process,"Users open existing documents, make changes, and save them as separate documents.  Another method uses an artificial language defined by a grammar, with sentences generated according to the grammar and entailment relations established by a fixed background logic (using FOL instead of natural logic).
",0.045454545454545456,0.6666666666666666,0.08510638297872342,0.0,0.0,0.0,0.045454545454545456,0.6666666666666666,0.08510638297872342,0.007144315612998046,0.16688495874404907,No,0.0,0.0,1.0
q263,What is the average length of the claims?,Average claim length is 8.9 tokens.,"Answer not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06567274736060395,0.022032471373677254,No,0.0,0.0,0.0
q264,What debate topics are included in the dataset?,"Ethics, Gender, Human rights, Sports, Freedom of Speech, Society, Religion, Philosophy, Health, Culture, World, Politics, Environment, Education, Digital Freedom, Economy, Science and Law","Politics, World, Society, Law, Ethics, War, Conflict in the Middle East, Latin America, Commonwealth, and 10+ other topics.
",0.3333333333333333,0.2608695652173913,0.2926829268292683,0.0,0.0,0.0,0.1111111111111111,0.08695652173913043,0.0975609756097561,0.07940185052255694,0.7380563616752625,No,0.0,0.0,0.0
q265,"By how much, the proposed method improves BiDAF and DCN on SQuAD dataset?","In terms of F1 score, the Hybrid approach improved by 23.47% and 1.39% on BiDAF and DCN respectively. The DCA approach improved by 23.2% and 1.12% on BiDAF and DCN respectively.","The context states that the hybrid and DCA models perform better than BiDAF and Co-Attention, but does not provide specific improvement numbers.
",0.34782608695652173,0.22857142857142856,0.27586206896551724,0.09090909090909091,0.058823529411764705,0.07142857142857142,0.2608695652173913,0.17142857142857143,0.20689655172413796,0.02483226240878586,0.7081404328346252,No,0.0,0.0,0.0
q266,What are the linguistic differences between each class?,"Each class has different patterns in adjectives, adverbs and verbs for sarcastic and non-sarcastic classes","The languages are divided into Indo-European and Sino-Tibetan families, with Basque as an isolate.  Within Indo-European, there are West Germanic and Romance subfamilies.  Languages within the same phyla (e.g., Romance, Germanic) share similar pronounced words.  Language varieties also exist within some languages (e.g., Portuguese, Spanish).  The languages differ in their syllable structures, verbal morphology, plural morphemes, and past tense acquisition.
",0.06153846153846154,0.25,0.09876543209876544,0.0,0.0,0.0,0.046153846153846156,0.1875,0.07407407407407408,0.006061162699478887,0.21402323246002197,No,0.0,0.0,0.0
q267,what genres do they songs fall under?,"Gospel, Sertanejo, MPB, Forró, Pagode, Rock, Samba, Pop, Axé, Funk-carioca, Infantil, Velha-guarda, Bossa-nova and Jovem-guarda","Blues, Rap, Metal, Folk, R&B, Reggae, Country, and Religious.
",0.1,0.05263157894736842,0.06896551724137931,0.0,0.0,0.0,0.1,0.05263157894736842,0.06896551724137931,0.022346932451407998,0.7194342613220215,No,0.0,0.0,0.0
q268,To what other competitive baselines is this approach compared?,"LSTMs with and without attention, HRED, VHRED with and without attention, MMI and Reranking-RL","Baseline and Ablation models.
",0.25,0.06666666666666667,0.10526315789473685,0.0,0.0,0.0,0.25,0.06666666666666667,0.10526315789473685,0.009690650671856618,0.17700189352035522,No,0.0,0.0,0.0
q269,"How is human evaluation performed, what was the criteria?",Through Amazon MTurk annotators to determine plausibility and content richness of the response,"Human evaluators graded generated poems from four perspectives: Fluency (grammatical and rhythmic well-formedness), Semantic coherence, adequacy, and appropriateness.  Evaluators were native Chinese speakers, divided into expert (15 with bachelor's degrees in Chinese literature) and amateur (15 with bachelor's degrees in other fields) groups.
",0.021739130434782608,0.07692307692307693,0.03389830508474576,0.0,0.0,0.0,0.021739130434782608,0.07692307692307693,0.03389830508474576,0.006488583831481832,0.20536699891090393,No,0.0,0.0,0.0
q270,How much better were results of the proposed models than base LSTM-RNN model?,on diversity 6.87 and on relevance 4.6 points higher,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12061148881912231,No,0.0,0.0,0.0
q271,Which one of the four proposed models performed best?,the hybrid model MinAvgOut + RL,"""Ours with Mask and Ordered Triplets""
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09482955187559128,No,0.0,0.0,0.0
q272,How much is proposed model better than baselines in performed experiments?,"most of the models have similar performance on BPRA: DSTC2 (+0.0015), Maluuba (+0.0729)
GDP achieves the best performance in APRA: DSTC2 (+0.2893), Maluuba (+0.2896)
GDP significantly outperforms the baselines on BLEU: DSTC2 (+0.0791), Maluuba (+0.0492)","The proposed model outperforms baselines by a clear margin in new environments, showing increases in EM and GM.  For example, it outperforms the second best model by several points in the Test-New set and in comparison to the next best two models by over INLINEFORM0 increases in EM and GM.  In previously seen environments, it increases accuracy by 35% and 25% compared to baselines for EM and GM metrics, respectively.
",0.11267605633802817,0.1951219512195122,0.14285714285714285,0.014285714285714285,0.025,0.01818181818181818,0.09859154929577464,0.17073170731707318,0.125,0.013973949572219965,0.5116674304008484,No,0.0,0.0,0.0
q273,By how much is precission increased?,"ROUGE-1 increases by 0.05, ROUGE-2 by 0.06 and ROUGE-L by 0.09","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15129078924655914,No,0.0,0.0,0.0
q274,What labels are in the dataset?,binary label of stress or not stress,"LOC, PER, ORG, Miscellaneous, NAG, OAG, CAG, PIO,  (and possibly others depending on the source).
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18578079342842102,No,0.0,0.0,0.0
q275,"How are customer satisfaction, customer frustration and overall problem resolution data collected?",By annotators on Amazon Mechanical Turk.,"The analysis uses 216 satisfied and 500 unsatisfied customer conversations, 271 resolved and 425 unresolved problem conversations, and 534 frustrated and 229 not frustrated customer conversations.  The data is labeled based on consensus or majority vote.
",0.027777777777777776,0.16666666666666666,0.04761904761904762,0.0,0.0,0.0,0.027777777777777776,0.16666666666666666,0.04761904761904762,0.01092534162077208,0.07467608153820038,No,0.0,0.0,0.0
q276,How many improvements on the French-German translation benchmark?,one,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06584003567695618,No,0.0,0.0,0.0
q277,How do they prevent the model complexity increasing with the increased number of slots?,They exclude slot-specific parameters and incorporate better feature representation of user utterance and dialogue states using syntactic information and convolutional neural networks (CNN).,"The provided text discusses models that remove the model's dependency on slot-specific mechanisms.
",0.13333333333333333,0.08333333333333333,0.10256410256410255,0.07142857142857142,0.043478260869565216,0.05405405405405405,0.13333333333333333,0.08333333333333333,0.10256410256410255,0.014331734778487197,0.4313034415245056,Yes,0.0,0.0,0.0
q278,How do they measure model size?,By the number of parameters.,"Not found in the context.
",0.2,0.2,0.20000000000000004,0.0,0.0,0.0,0.2,0.2,0.20000000000000004,0.09652434877402244,0.052915118634700775,No,0.0,0.0,0.0
q279,Which model architecture do they for sentence encoding?,"Answer with content missing: (Skip-thought vectors-Natural Language Inference paragraphs) The encoder for the current sentence and the decoders for the previous (STP) and next sentence (STN) are typically parameterized as separate RNNs
- RNN","BiLSTM-max with a hidden size of 600D per direction.
",0.1,0.02857142857142857,0.04444444444444444,0.0,0.0,0.0,0.1,0.02857142857142857,0.04444444444444444,0.0018903162968037812,0.012059446424245834,No,0.0,1.0,1.0
q280,Which data sources do they use?,"- En-Fr (WMT14)
- En-De (WMT15)
- Skipthought (BookCorpus)
- AllNLI (SNLI + MultiNLI)
- Parsing (PTB + 1-billion word)","Online debate websites, source documents or summaries (in free text format), crowd sourcing, examinations designed by educational experts, French meteorological office (Météo France), stations of the National Oceanic and Atmospheric Administration (NOAA), Stanford Sentiment Treebank (SST-2), Yelp dataset, Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), and Quora Question Pairs.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.008554958784849806,0.3322068452835083,No,0.0,0.0,0.0
q281,How were breast cancer related posts compiled from the Twitter streaming API?,"By using  keywords `breast' AND `cancer' in tweet collecting process. 
","A total of 1,000 tweets were classified as relevant.  A secondary feed searched for the keyword ""cancer"" to collect additional tweets.  48,113 tweets from these accounts were compiled from both the ""cancer"" and ""breast cancer"" feeds.
",0.15789473684210525,0.6,0.25,0.0,0.0,0.0,0.07894736842105263,0.3,0.125,0.008130350574508933,0.7305070161819458,No,1.0,1.0,1.0
q282,What machine learning and NLP methods were used to sift tweets relevant to breast cancer experiences?,"ML  logistic regression classifier combined with a Convolutional Neural Network (CNN) to identify self-reported diagnostic tweets.
NLP methods:  tweet conversion to numeric word vector,  removing tweets containing hyperlinks, removing ""retweets"", removing all tweets containing horoscope indicators,  lowercasing and  removing punctuation.","Convolutional Neural Network (CNN) with Google's Tensorflow interface.
",0.5555555555555556,0.12195121951219512,0.2,0.375,0.075,0.12499999999999999,0.4444444444444444,0.0975609756097561,0.16,0.013073032850034427,0.3149792551994324,No,0.0,0.0,0.0
q283,What approach performs better in experiments global latent or sequence of fine-grained latent variables?,"PPL: SVT
Diversity: GVT
Embeddings Similarity: SVT
Human Evaluation: SVT","GVT (global latent variable) achieves relatively lower reconstruction PPL, suggesting it contains rich latent information.  Sequential latent variables (SVT) further improve reconstruction PPL.
",0.13043478260869565,0.3,0.18181818181818182,0.0,0.0,0.0,0.08695652173913043,0.2,0.12121212121212122,0.016338984401221654,0.5177446603775024,No,0.0,1.0,1.0
q284,Which translation system do they use to translate to English?,Attention-based translation model with convolution sequence to sequence model,"Google Translate.
",0.5,0.1,0.16666666666666669,0.0,0.0,0.0,0.5,0.1,0.16666666666666669,0.0,0.1381053477525711,No,0.0,0.0,0.0
q285,Which pre-trained English NER model do they use?,Bidirectional LSTM based NER model of Flair,"Flair's default NER model.
",0.6,0.42857142857142855,0.5,0.25,0.16666666666666666,0.2,0.4,0.2857142857142857,0.3333333333333333,0.14320952289897704,0.6513205170631409,Yes,1.0,1.0,1.0
q286,How much of the ASR grapheme set is shared between languages?,Little overlap except common basic Latin alphabet and that Hindi and Marathi languages use same script.,"The 7 languages share the common basic Latin alphabet and both Hindi and Marathi use Devanagari script.
",0.6470588235294118,0.6875,0.6666666666666667,0.375,0.4,0.38709677419354843,0.5882352941176471,0.625,0.6060606060606061,0.3092852090394752,0.779794454574585,Yes,0.0,1.0,1.0
q287,What are the languages used to test the model?,"Hindi, English and German (German task won)","English, French, German, and Spanish.
",0.6,0.42857142857142855,0.5,0.0,0.0,0.0,0.4,0.2857142857142857,0.3333333333333333,0.06033504141761816,0.6183279752731323,No,0.0,0.0,0.0
q288,Which language has the lowest error rate reduction?,thai,"Sinhala
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.37077653408050537,No,0.0,0.0,0.0
q289,How is moral bias measured?,"Answer with content missing: (formula 1) bias(q, a, b) = cos(a, q) − cos(b, q)
Bias is calculated as substraction of cosine similarities of question and some answer for two opposite answers.","Moral bias is measured by averaging the biases from several question/answer prompts with the element of interest inserted, using a template for moral biases with ten prompts (Tab. TABREF15).
",0.26666666666666666,0.24242424242424243,0.253968253968254,0.034482758620689655,0.03125,0.03278688524590164,0.16666666666666666,0.15151515151515152,0.15873015873015872,0.013299364976647101,0.5031707882881165,No,0.0,0.0,0.0
q290,What metrics are used for evaluation?,word error rate,"Task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, F1-score, BPRA, Belief Per-Response Accuracy, average rank, percentage of best-ranked questions, BLEU, ROUGE, METEOR.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.20707550644874573,No,0.0,0.0,0.0
q291,How much training data is used?,"163,110,000 utterances","2489 messages, then augmented to 7934 messages.  Also, 17M training tokens.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5054091811180115,No,0.0,0.0,0.0
q292,How does their model differ from BERT?,Their model does not differ from BERT.,"The input embeddings are the sum of token, segmentation, and position embeddings.  The segmentation embeddings use learned sentence A and B embeddings for the first and second sentences, respectively.  Also, different parameterizations and sharing are used for parameter reduction.
",0.02564102564102564,0.14285714285714285,0.043478260869565216,0.0,0.0,0.0,0.02564102564102564,0.14285714285714285,0.043478260869565216,0.007947529289544673,0.21953707933425903,No,0.0,0.0,0.0
q293,How does explicit constraint on the KL divergence term that authors propose looks like?,"Answer with content missing: (Formula 2) Formula 2 is an answer: 
\big \langle\! \log p_\theta({x}|{z}) \big \rangle_{q_\phi({z}|{x})}  -  \beta |D_{KL}\big(q_\phi({z}|{x}) || p({z})\big)-C|","$|D_{KL}\big (q_\phi ({z}|{x}) || p({z})\big )-C|$
",1.0,0.3055555555555556,0.46808510638297873,1.0,0.2857142857142857,0.4444444444444445,1.0,0.3055555555555556,0.46808510638297873,0.20218161785227923,0.5797313451766968,No,0.0,0.0,1.0
q294,what was the baseline?,There is no baseline.,"There are multiple baselines described.  Content, content-network, and image-based models; a LSTM baseline; a hierarchical transformer summarization model (HeriTransfomer); and a standard parallel corpus-trained baseline.
",0.07142857142857142,0.5,0.125,0.0,0.0,0.0,0.07142857142857142,0.5,0.125,0.024092489848483226,0.6145429015159607,No,1.0,1.0,1.0
q295,How big is dataset used?,"553,451 documents","The provided text describes multiple datasets with varying sizes.  No single ""the"" dataset size is explicitly stated.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.20743727684020996,No,0.0,0.0,0.0
q296,How much better does this baseline neural model do?,"The model outperforms at every point in the
implicit-tuples PR curve reaching almost 0.8 in recall","The baseline neural model outperforms the EntNet model in terms of mean error and achieves 0% test error on more tasks.
",0.2857142857142857,0.3333333333333333,0.30769230769230765,0.05,0.058823529411764705,0.05405405405405405,0.23809523809523808,0.2777777777777778,0.25641025641025644,0.04141141330484801,0.2958797514438629,No,0.0,0.0,0.0
q297,What are the baseline models?,"MC-CNN
MVCNN
CNN","Model 1, a setup inspired by BIBREF38, using a dropout rate of 0.3, optimized with Adam, and a learning rate of 10<sup>-5</sup>.  A baseline model composed of word embedding, input encoder, alignment, aggregation, and prediction.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13997794687747955,No,0.0,0.0,0.0
q298,By how much of MGNC-CNN out perform the baselines?,"In terms of Subj the Average MGNC-CNN is better than the average score of baselines by 0.5.  Similarly, Scores of SST-1, SST-2, and TREC where MGNC-CNN has similar improvements. 
In case of Irony the difference is about 2.0. 
","The provided text states that MGNC-CNN and MG-CNN outperform baseline methods, but does not give specific performance differences.
",0.35,0.1590909090909091,0.21875,0.05263157894736842,0.023255813953488372,0.03225806451612903,0.3,0.13636363636363635,0.18749999999999997,0.008155308050186705,0.7382693290710449,No,0.0,0.0,0.0
q299,What is increase in percentage of humor contained in headlines generated with TitleStylist method (w.r.t. baselines)?,"Humor in headlines (TitleStylist vs Multitask baseline):
Relevance: +6.53% (5.87 vs 5.51)
Attraction: +3.72% (8.93 vs 8.61)
Fluency: 1,98% (9.29 vs 9.11)","Answer not found in the context.
",0.16666666666666666,0.03225806451612903,0.05405405405405405,0.0,0.0,0.0,0.16666666666666666,0.03225806451612903,0.05405405405405405,0.0006792646002484808,0.0888647586107254,No,0.0,0.0,1.0
q300,What are the languages they consider in this paper?,"The languages considered were English, Chinese, German, Russian, Arabic, Spanish, French","English, French, German, Spanish, Russian, Italian, Basque, Hungarian, Navajo, Turkish, Quechua, Arabic, Dutch,  Manipuri, Zulu, and potentially others.
",0.3333333333333333,0.5454545454545454,0.41379310344827586,0.0,0.0,0.0,0.2222222222222222,0.36363636363636365,0.27586206896551724,0.12117880855911828,0.6456834077835083,No,0.0,0.0,0.0
q301,Did they experiment with tasks other than word problems in math?,They experimented with sentiment analysis and natural language inference task,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.26264309883117676,No,0.0,0.0,0.0
q302,What evaluation metrics are used?,Accuracy on each dataset and the average accuracy on all datasets.,"Task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, F1-score, BPRA, Belief Per-Response Accuracy,  average rank or percentage of best-ranked questions.
",0.07407407407407407,0.18181818181818182,0.10526315789473684,0.0,0.0,0.0,0.07407407407407407,0.18181818181818182,0.10526315789473684,0.014325480172973542,0.31951507925987244,No,0.0,0.0,0.0
q303,What kind of Youtube video transcripts did they use?,"youtube video transcripts on news covering different topics like technology, human rights, terrorism and politics","Anonymized, transcribed Voice Search data.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.015738541970947637,0.27944356203079224,No,0.0,0.0,0.0
q304,What makes it a more reliable metric?,It takes into account the agreement between different systems,"It has an interpretable meaning: the expected error made by a given metric as compared to the human rating.
",0.10526315789473684,0.2222222222222222,0.14285714285714285,0.0,0.0,0.0,0.10526315789473684,0.2222222222222222,0.14285714285714285,0.021597011339333432,0.25112590193748474,No,0.0,0.0,0.0
q305,How much in experiments is performance improved for models trained with generated adversarial examples?,"Answer with content missing: (Table 1) The performance of all the target models raises significantly, while that on the original
examples remain comparable (e.g. the overall accuracy of BERT on modified examples raises from 24.1% to 66.0% on Quora)","The performance of all target models raises significantly after adversarial training.
",0.7272727272727273,0.19047619047619047,0.3018867924528302,0.6,0.14634146341463414,0.23529411764705882,0.7272727272727273,0.19047619047619047,0.3018867924528302,0.018403522953413978,0.5331608057022095,Yes,0.0,0.0,0.0
q306,How is the delta-softmax calculated?,Answer with content missing: (Formula) Formula is the answer.,"The provided text does not describe how a delta-softmax is calculated.
",0.16666666666666666,0.2222222222222222,0.1904761904761905,0.0,0.0,0.0,0.08333333333333333,0.1111111111111111,0.09523809523809525,0.03705247205763761,0.1139208972454071,No,0.0,0.0,0.0
q307,Which two datasets does the resource come from?,two surveys by two groups - school students and meteorologists to draw on a map a polygon representing a given geographical descriptor,"REF0 and ASQ
",0.3333333333333333,0.047619047619047616,0.08333333333333333,0.0,0.0,0.0,0.3333333333333333,0.047619047619047616,0.08333333333333333,0.0,-0.011466060765087605,No,0.0,0.0,0.0
q308,What is the size of the second dataset?,1 000 labeled dialogues for training and 240 unlabeled dialogues for evaluation,"141 symptom-prescription pairs
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16394396126270294,No,0.0,0.0,0.0
q309,How large is the first dataset?,1 000 labeled dialogues for training and 240 unlabeled dialogues for evaluation,"The first dataset has 287,226 documents.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13922955095767975,No,0.0,0.0,0.0
q310,Who was the top-scoring team?,IDEA,"AM
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13247697055339813,No,0.0,0.0,0.0
q311,Why is big data not appropriate for this task?,Training embeddings from small-corpora can increase the performance of some tasks,"Big data is not always available, particularly in computational social science NLP applications where data of interest is often not from large-scale sources.  Even very large corpora have idiosyncrasies that can make their embeddings irrelevant for a particular analysis.
",0.125,0.4166666666666667,0.1923076923076923,0.0,0.0,0.0,0.075,0.25,0.11538461538461538,0.012350387043213578,0.5031976699829102,Yes,0.0,1.0,1.0
q312,What is an example of a computational social science NLP task?,Visualization of State of the union addresses,"The answer is not found in the context.
",0.125,0.14285714285714285,0.13333333333333333,0.0,0.0,0.0,0.125,0.14285714285714285,0.13333333333333333,0.04767707020457095,0.06527452915906906,No,0.0,0.0,0.0
q313,Which major geographical regions are studied?,"Northeast U.S, South U.S., West U.S. and Midwest U.S.","Latin America and the Caribbean (LCN), South Asia (SAS), Sub-Saharan Africa (SSA), Europe and Central Asia (ECS), Middle East and North Africa (MEA), East Asia and the Pacific (EAS), North America (NAC)
",0.06060606060606061,0.15384615384615385,0.08695652173913043,0.0,0.0,0.0,0.06060606060606061,0.15384615384615385,0.08695652173913043,0.016655497133943254,0.6446688175201416,No,0.0,0.0,0.0
q314,How strong is the correlation between the prevalence of the #MeToo movement and official reports [of sexual harassment]?,0.9098 correlation,"The change in the number of reported cases constitutes a considerable change in the number of #MeToo users on Twitter (p-value extremely small at 5.7e-13).
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.239493727684021,No,0.0,1.0,1.0
q315,How are the topics embedded in the #MeToo tweets extracted?,Using Latent Dirichlet Allocation on TF-IDF transformed from the corpus,"Latent Dirichlet Allocation (LDA) is used to label universal topics.  The optimal topic number is selected based on the highest coherence score.
",0.2727272727272727,0.5454545454545454,0.3636363636363636,0.09523809523809523,0.2,0.12903225806451613,0.22727272727272727,0.45454545454545453,0.30303030303030304,0.06109833740738946,0.3915863037109375,Yes,1.0,1.0,1.0
q316,Which geographical regions correlate to the trend?,"Northeast U.S., West U.S. and South U.S.","South Asia, Latin America and the Caribbean, East Asia and the Pacific, the Southeast and Midwest of the US, and coastal cities.
",0.09090909090909091,0.2,0.12500000000000003,0.0,0.0,0.0,0.045454545454545456,0.1,0.06250000000000001,0.01962672844874697,0.761216402053833,No,0.0,0.0,0.0
q317,What two components are included in their proposed framework?,evidence extraction and answer synthesis,"A deep learning classifier and a tweet processor.
",0.125,0.2,0.15384615384615385,0.0,0.0,0.0,0.125,0.2,0.15384615384615385,0.04767707020457095,0.195113867521286,No,0.0,0.0,0.0
q318,Which modifications do they make to well-established Seq2seq architectures?,"Replacing attention mechanism to query-key attention, and adding a loss to make the attention mask as diagonal as possible","Consistent application of regularization techniques, and consideration of the monotonic nature of attention in TTS tasks.  They also use variational-based methods and a flow-based sequence-to-sequence model.
",0.16666666666666666,0.25,0.2,0.0,0.0,0.0,0.13333333333333333,0.2,0.16,0.032359121225441656,0.25856658816337585,No,0.0,0.0,0.0
q319,How was speed measured?,how long it takes the system to lemmatize a set number of words,"The answer is not found in the context.
",0.125,0.07692307692307693,0.09523809523809525,0.0,0.0,0.0,0.125,0.07692307692307693,0.09523809523809525,0.03056960239296902,0.1064307764172554,No,0.0,0.0,0.0
q320,What were their accuracy results on the task?,97.32%,"The provided text mentions accuracy results for SNLI (90.4% and 90.6%) and an F1 score of 57% on TEC, but no accuracy results for tasks A, B, or C.  The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.007772751558853213,0.34607288241386414,No,0.0,0.0,0.0
q321,What two types the Chinese reading comprehension dataset consists of?,cloze-style reading comprehension and user query reading comprehension questions,"People Daily news datasets and Children's Fairy Tale datasets.
",0.1,0.1,0.10000000000000002,0.0,0.0,0.0,0.1,0.1,0.10000000000000002,0.04196114906296548,0.20782171189785004,No,0.0,1.0,1.0
q322,For which languages most of the existing MRC datasets are created?,English,"English, German, Spanish, Italian, Japanese, and Portuguese.
",0.14285714285714285,1.0,0.25,0.0,0.0,0.0,0.14285714285714285,1.0,0.25,0.030890553181566974,0.444061815738678,No,0.0,0.0,1.0
q323,Which sentiment analysis tasks are addressed?,12 binary-class classification and multi-class classification of reviews based on rating,"Tweet-level sentiment, deeper analysis of clinical narratives in EHRs, and emotion detection.
",0.15384615384615385,0.15384615384615385,0.15384615384615385,0.0,0.0,0.0,0.07692307692307693,0.07692307692307693,0.07692307692307693,0.03125190763972442,0.15749399363994598,No,0.0,0.0,0.0
q324,Which 3 NLP areas are cited the most?,"machine translation, statistical machine, sentiment analysis","Answer not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09254340082406998,No,0.0,0.0,0.0
q325,Which journal and conference are cited the most in recent years?,CL Journal and EMNLP conference,"The context states that CL Journal, EMNLP, and ACL have the best track record for accepting papers that have gone on to receive 1000 or more citations, but does not definitively say which is cited the most in recent years.
",0.1,0.8,0.1777777777777778,0.02564102564102564,0.25,0.04651162790697674,0.075,0.6,0.13333333333333333,0.019792649377110935,0.4376598000526428,No,0.0,1.0,1.0
q326,Which 5 languages appear most frequently in AA paper titles?,"English, Chinese, French, Japanese and Arabic","Chinese, Arabic, Korean, Japanese, and Hindi
",0.6666666666666666,0.6666666666666666,0.6666666666666666,0.2,0.2,0.20000000000000004,0.5,0.5,0.5,0.13650604313545334,0.9206621646881104,No,0.0,1.0,1.0
q327,How much F1 was improved after adding skip connections?,"Simple Skip improves F1 from 74.34 to 74.81
Transformer Skip improes F1 from 74.34 to 74.95 ","The provided text states that skip connections improved scores further, but does not give a specific F1 score improvement number.
",0.15,0.15,0.15,0.0,0.0,0.0,0.15,0.15,0.15,0.017272237992167868,0.5010003447532654,No,1.0,1.0,1.0
q328,How much gain does the model achieve with pretraining MVCNN?,0.8 points on Binary; 0.7 points on Fine-Grained; 0.6 points on Senti140; 0.7 points on Subj,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0910150483250618,No,0.0,0.0,0.0
q329,What is the highest accuracy score achieved?,82.0%,"93.06% (unnormalized) for scene recognition.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04767707020457095,0.5278199911117554,No,0.0,0.0,0.0
q330,What are the three datasets used in the paper?,Data released for APDA shared task contains 3 datasets.,"Biomedical, Stanford Sentiment Treebank (SST-2) / Yelp, and Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), and Quora Question Pairs (ASQ).
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010253311603116816,0.16404488682746887,No,0.0,0.0,0.0
q331,What is improvement in accuracy for short Jokes in relation other types of jokes?,It had the highest accuracy comparing to all datasets 0.986% and It had the highest improvement comparing to previous methods on the same dataset by 8%,"The Transformer model achieved 98.6% accuracy on the Short Jokes dataset, an 8% improvement over the previous best CNN model.
",0.42857142857142855,0.3333333333333333,0.375,0.05,0.038461538461538464,0.043478260869565216,0.2857142857142857,0.2222222222222222,0.25,0.047364976394085746,0.46278873085975647,Yes,0.0,1.0,1.0
q332,What baseline did they compare Entity-GCN to?,"Human, FastQA, BiDAF, Coref-GRU, MHPGM, Weaver / Jenga, MHQA-GRN","Many techniques that use expensive question-aware recurrent document encoders.  BiDAF BIBREF3, FastQA BIBREF4.
",0.14285714285714285,0.2,0.16666666666666666,0.0,0.0,0.0,0.07142857142857142,0.1,0.08333333333333333,0.05412989186545265,0.133501797914505,No,1.0,1.0,1.0
q333,How did they get relations between mentions?,"Assign a value to the relation based on whether mentions occur in the same document, if mentions are identical, or if mentions are in the same coreference chain.","The context describes associating mentions by encoding selected entity information, but does not specify the method for obtaining relations.
",0.21052631578947367,0.14285714285714285,0.1702127659574468,0.0,0.0,0.0,0.15789473684210525,0.10714285714285714,0.1276595744680851,0.015953069568438383,0.5512787699699402,No,0.0,0.0,0.0
q334,How did they detect entity mentions?,Exact matches to the entity string and predictions from a coreference resolution system,"An entity recognition step is performed within the question answering process and aims at selecting all entity mentions in a given annotated document.
",0.17391304347826086,0.3076923076923077,0.2222222222222222,0.0,0.0,0.0,0.13043478260869565,0.23076923076923078,0.16666666666666669,0.02224972978585352,0.35417279601097107,No,0.0,0.0,0.0
q335,What performance does the Entity-GCN get on WIKIHOP?,"During testing: 67.6 for single model without coreference, 66.4 for single model with coreference, 71.2 for ensemble of 5 models","Entity-GCN achieves over 2% improvement over the best previously-published results, and an ensemble brings a further 3.6% improvement, getting only 3% below human performance.
",0.1111111111111111,0.13043478260869565,0.12,0.0,0.0,0.0,0.07407407407407407,0.08695652173913043,0.07999999999999999,0.016338984401221654,0.45313841104507446,No,0.0,1.0,1.0
q336,What document context was added?,Preceding and following sentence of each metaphor and paraphrase are added as document context,"Newspaper articles.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13260038197040558,No,0.0,0.0,0.0
q337,What were the results of the first experiment?,Best performance achieved is 0.72 F1 score,"The answer is not found in the context.
",0.125,0.125,0.125,0.0,0.0,0.0,0.125,0.125,0.125,0.04767707020457095,0.09117799997329712,No,0.0,0.0,0.0
q338,By how much does their model outperform existing methods?,Answer with content missing: (Table II) Proposed model has F1 score of  0.7220 compared to 0.7148 best state-state-of-the-art result.,"Our model outperforms the previous feature engineering benchmark and other work in terms of F1 score and accuracy of detection.  No specific percentage is given.
",0.24,0.24,0.24,0.041666666666666664,0.041666666666666664,0.041666666666666664,0.16,0.16,0.16,0.0349018070868319,0.6340770721435547,No,0.0,0.0,0.0
q339,What is the performance of their model?,Answer with content missing: (Table II) Proposed model has F1 score of  0.7220.,"Their large models exhibit a substantial improvement across all metrics compared to base models, surpassing the NABERT+ baseline.  They achieve significantly better performance on multi-span questions.  Results are similar to MTMSNlarge, but better on multi-span questions.  The larger model has a similar computational budget to the best published model and similar training times.
",0.03636363636363636,0.14285714285714285,0.05797101449275361,0.018518518518518517,0.07692307692307693,0.029850746268656716,0.03636363636363636,0.14285714285714285,0.05797101449275361,0.013379235219745432,0.29168951511383057,No,0.0,0.0,0.0
q340,How do they damage different neural modules?,"Damage to neural modules is done by randomly initializing their weights, causing the loss of all learned information.","The provided text describes damaging the analog of Broca's area (the LSTMs).  No other methods of damaging neural modules are mentioned.
",0.22727272727272727,0.2777777777777778,0.25,0.047619047619047616,0.058823529411764705,0.05263157894736842,0.13636363636363635,0.16666666666666666,0.15,0.0378786561257948,0.5979175567626953,No,0.0,0.0,0.0
q341,How long are the datasets?,"Travel dataset contains 4100 raw samples, 11291 clauses, Hotel dataset contains 3825 raw samples, 11264 clauses, and the Mobile dataset contains 3483 raw samples and 8118 clauses","The context provides various dataset sizes, but no single answer to ""how long"" the datasets are.  Different datasets have different numbers of documents, tokens, and characters.
",0.19230769230769232,0.18518518518518517,0.18867924528301885,0.0,0.0,0.0,0.15384615384615385,0.14814814814814814,0.1509433962264151,0.029561645124389607,0.3584951162338257,No,0.0,0.0,0.0
q342,What are the sources of the data?,"User reviews written in Chinese collected online for hotel, mobile phone, and travel domains","Online debate websites, Twitter, and Arabic WikiNews.
",0.2857142857142857,0.14285714285714285,0.19047619047619047,0.0,0.0,0.0,0.2857142857142857,0.14285714285714285,0.19047619047619047,0.05097103985522972,0.19739970564842224,No,0.0,0.0,0.0
q343,What is the new labeling strategy?,They use a two-stage labeling strategy where in the first stage single annotators label a large number of short texts with relatively pure sentiment orientations and in the second stage multiple annotators label few text samples with mixed sentiment orientations,"A two-stage labeling strategy.  In the first stage, annotators label short texts with pure sentiment. In the second stage, a smaller number of texts with mixed sentiment are labeled by multiple annotators.
",0.9090909090909091,0.7317073170731707,0.8108108108108109,0.5625,0.45,0.5,0.7272727272727273,0.5853658536585366,0.6486486486486487,0.1399084543967054,0.835764467716217,Yes,1.0,1.0,1.0
q344,How are their changes evaluated?,The changes are evaluated based on accuracy of intent and entity recognition on SNIPS dataset,"Changes are evaluated by human judgment, with human translations judged more favorably and significantly better than machine translations when evaluation design is improved.
",0.17391304347826086,0.26666666666666666,0.2105263157894737,0.09090909090909091,0.14285714285714285,0.1111111111111111,0.17391304347826086,0.26666666666666666,0.2105263157894737,0.033337449830272974,0.5004316568374634,No,0.0,0.0,0.0
q345,What are the six target languages?,"Answer with content missing: (3 Experimental Setup) We experiment with six target languages: French (FR), Brazilian Portuguese (PT), Italian (IT), Polish (PL), Croatian (HR), and Finnish (FI).","English, Spanish, and Zulu.
",0.25,0.037037037037037035,0.06451612903225806,0.0,0.0,0.0,0.25,0.037037037037037035,0.06451612903225806,0.0003871736459208026,0.5532875657081604,No,0.0,0.0,0.0
q346,What is the size of the released dataset?,"440 sentences, 2247 triples extracted from those sentences, and 11262 judgements on those triples.","287,226 documents for training.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.009690650671856618,0.3471555709838867,No,0.0,0.0,0.0
q347,Which OpenIE systems were used?,OpenIE4 and MiniIE,"OLLIE, Reverb, MinIE, ClauseIE, Stanford OIE, ReVerb, Stanford OpenIE, and PropS.
",0.09090909090909091,0.3333333333333333,0.14285714285714288,0.0,0.0,0.0,0.09090909090909091,0.3333333333333333,0.14285714285714288,0.020244462660665506,0.3865416944026947,No,0.0,0.0,0.0
q348,how are the bidirectional lms obtained?,"They pre-train forward and backward LMs separately, remove top layer softmax, and concatenate to obtain the bidirectional LMs.","A bidirectional LSTM (BiLSTM) is formed by concatenating an LSTM moving forward through time with an LSTM moving backward through time.
",0.19047619047619047,0.21052631578947367,0.2,0.0,0.0,0.0,0.09523809523809523,0.10526315789473684,0.1,0.02224972978585352,0.5639222264289856,No,0.0,0.0,0.0
q349,what metrics are used in evaluation?,micro-averaged F1,"Manual evaluation, ROUGE, BLEU, METEOR, F1-score, task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, BPRA.
",0.05,0.3333333333333333,0.08695652173913045,0.0,0.0,0.0,0.05,0.3333333333333333,0.08695652173913045,0.011993481292529619,0.13701723515987396,No,0.0,0.0,0.0
q350,what results do they achieve?,91.93% F1 score on CoNLL 2003 NER task and 96.37% F1 score on CoNLL 2000 Chunking task,"Classification produces better results than regression, but result quality depends on data annotation approach.  Precision and F1 correlate better with human evaluations than Recall. Reinforcement learning shows promising results, especially in human evaluations.
",0.09090909090909091,0.15789473684210525,0.11538461538461539,0.0,0.0,0.0,0.09090909090909091,0.15789473684210525,0.11538461538461539,0.012753998269755913,0.15470826625823975,No,0.0,0.0,0.0
q351,what previous systems were compared to?,"Chiu and Nichols (2016), Lample et al. (2016), Ma and Hovy (2016), Yang et al. (2017), Hashimoto et al. (2016), Søgaard and Goldberg (2016) ","Baseline systems, top-ranked systems in a competition, and deep convolutional networks (SemEval-2015 and SemEval-2016).  Also, kernel machines.
",0.15,0.12,0.1333333333333333,0.0,0.0,0.0,0.15,0.12,0.1333333333333333,0.011842906548012172,0.33153966069221497,No,0.0,0.0,0.0
q352,Are this models usually semi/supervised or unsupervised?,"Both supervised and unsupervised, depending on the task that needs to be solved.","Supervised.
",1.0,0.07692307692307693,0.14285714285714288,0.0,0.0,0.0,1.0,0.07692307692307693,0.14285714285714288,0.0,0.6006678938865662,No,1.0,1.0,1.0
q353,"When they say ""comparable performance"", how much of a performance drop do these new embeddings result in?","Performance was comparable, with the proposed method quite close and sometimes exceeding performance of baseline method.","The answer is not found in the context.
",0.125,0.0625,0.08333333333333333,0.0,0.0,0.0,0.125,0.0625,0.08333333333333333,0.02085799585464662,0.10231752693653107,No,0.0,0.0,0.0
q354,What types of word representations are they evaluating?,GloVE; SGNS,"GloVe word embedding, character-CNN embedding, Part-Of-Speech (POS) embedding, Named-Entity-Recognition (NER) embedding, and exact match feature.
",0.05,0.5,0.09090909090909091,0.0,0.0,0.0,0.05,0.5,0.09090909090909091,0.0,0.18938474357128143,No,0.0,0.0,0.0
q355,What type of recurrent layers does the model use?,GRU,"Deep RNN, LSTM, and Quasi-Recurrent Neural Networks (QRNNs).
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.003398166038095951,No,0.0,0.0,0.0
q356,What is a word confusion network?,It is a network used to encode speech lattices to maintain a rich hypothesis space.,"The answer is not found in the context.
",0.125,0.06666666666666667,0.08695652173913045,0.0,0.0,0.0,0.125,0.06666666666666667,0.08695652173913045,0.02604848453271721,0.2145993411540985,No,0.0,0.0,0.0
q357,What evaluation metrics were used in the experiment?,"For sentence-level prediction they used tolerance accuracy, for segment retrieval accuracy and MRR and for the pipeline approach they used overall accuracy","micro-F1, EM-score, F$_1$-score, relevance, attraction, fluency, style strength.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01647914133185019,0.4563022255897522,No,0.0,0.0,0.0
q358,What kind of instructional videos are in the dataset?,tutorial videos for a photo-editing software,"Screencast tutorial videos with spoken narratives for a photo-editing software.
",0.6363636363636364,1.0,0.7777777777777778,0.5,0.8333333333333334,0.625,0.6363636363636364,1.0,0.7777777777777778,0.27901593935858265,0.6981295347213745,Yes,0.0,1.0,1.0
q359,What baseline algorithms were presented?,"a sentence-level prediction algorithm, a segment retrieval algorithm and a pipeline segment retrieval algorithm","Four baseline systems, including two simple ones and two neural network models.
",0.08333333333333333,0.06666666666666667,0.07407407407407407,0.0,0.0,0.0,0.08333333333333333,0.06666666666666667,0.07407407407407407,0.03144344638628673,0.3229846954345703,No,0.0,0.0,0.0
q360,How does TP-N2F compare to LSTM-based Seq2Seq in terms of training and inference speed?,"Full Testing Set accuracy: 84.02
Cleaned Testing Set accuracy: 93.48","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04697878658771515,No,0.0,0.0,0.0
q361,What is the performance proposed model achieved on AlgoList benchmark?,"Full Testing Set Accuracy: 84.02
Cleaned Testing Set Accuracy: 93.48","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04697878658771515,No,0.0,0.0,0.0
q362,What is the performance proposed model achieved on MathQA?,"Operation accuracy: 71.89
Execution accuracy: 55.95","The context states the model has a lower test perplexity after 10 epochs compared to a baseline model.  It also achieves similar computational efficiency to the best published model in the literature.  However, it does not provide a specific performance metric (e.g., accuracy).
",0.022727272727272728,0.125,0.038461538461538464,0.0,0.0,0.0,0.022727272727272728,0.125,0.038461538461538464,0.006866732953963861,0.38087040185928345,No,0.0,0.0,0.0
q363,What previous methods is the proposed method compared against?,"BLSTM+Attention+BLSTM
Hierarchical BLSTM-CRF
CRF-ASN
Hierarchical CNN (window 4)
mLSTM-RNN
DRLM-Conditional
LSTM-Softmax
RCNN
CNN
CRF
LSTM
BERT","Coppersmith et al.'s method, K-means, SkipVec, RecNN, and Para2vec based clustering methods.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19004610180854797,No,0.0,0.0,0.0
q364,What is the baseline model used?,"The baseline models used are DrQA modified to support answering no answer questions, DrQA+CoQA which is pre-tuned on CoQA dataset, vanilla BERT, BERT+review tuned on domain reviews, BERT+CoQA tuned on the supervised CoQA data","Modelo 1
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.20534294843673706,No,0.0,0.0,0.0
q365,What domains are present in the data?,"Alarm, Banks, Buses, Calendar, Events, Flights, Homes, Hotels, Media, Messaging, Movies, Music, Payment, Rental Cars, Restaurants, Ride Sharing, Services, Train, Travel, Weather","20 domains,  HAR (Hotel, Attraction, Restaurant), and MT (scientific).  Wikipedia domain is also mentioned.
",0.14285714285714285,0.09090909090909091,0.1111111111111111,0.0,0.0,0.0,0.14285714285714285,0.09090909090909091,0.1111111111111111,0.010957197329960701,0.39433804154396057,No,1.0,1.0,1.0
q366,"How many texts/datapoints are in the SemEval, TASS and SENTIPOLC datasets?","Total number of annotated data:
Semeval'15: 10712
Semeval'16: 28632
Tass'15: 69000
Sentipol'14: 6428","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.20116351544857025,No,0.0,0.0,0.0
q367,In which languages did the approach outperform the reported results?,"Arabic, German, Portuguese, Russian, Swedish","Arabic, Portuguese, Russian, and Swedish.
",0.8,0.8,0.8000000000000002,0.25,0.25,0.25,0.8,0.8,0.8000000000000002,0.5133450480401706,0.939581036567688,Yes,1.0,1.0,1.0
q368,Which is the baseline model?,"The three baseline models are the i-vector model, a standard RNN LID system and a multi-task RNN LID system. ","The first example mentions a ""Baseline model"".  A different setup, using a specific layer configuration, is also called a ""baseline model"".  There is also a ""Model 1"" baseline.
",0.21428571428571427,0.2857142857142857,0.24489795918367344,0.07407407407407407,0.1,0.0851063829787234,0.17857142857142858,0.23809523809523808,0.20408163265306123,0.015342333164810606,0.5587665438652039,No,0.0,0.0,0.0
q369,What is the main contribution of the paper? ,"Proposing an improved RNN model, the phonetic temporal neural LID approach, based on phonetic features that results in better performance","The paper theoretically and empirically analyzes the problem of DIRL for domain adaptation when the marginal distribution P(Y) shifts across domains, and proposes a novel method to address the problem.
",0.06451612903225806,0.1,0.0784313725490196,0.0,0.0,0.0,0.06451612903225806,0.1,0.0784313725490196,0.012557690800697192,0.1414337456226349,No,0.0,0.0,0.0
q370,How do they get the formal languages?,These are well-known formal languages some of which was used in the literature to evaluate the learning capabilities of RNNs.,"The context describes training models on languages from various families, using multilingual corpora and treebanks.  It does not explain *how* the formal languages are obtained.
",0.24,0.2857142857142857,0.2608695652173913,0.041666666666666664,0.05,0.04545454545454545,0.12,0.14285714285714285,0.13043478260869565,0.031221929756173616,0.5254930853843689,No,0.0,0.0,0.0
q371,What is a confusion network or lattice?,"graph-like structures where arcs connect nodes representing multiple hypothesized words, thus allowing multiple incoming arcs unlike 1-best sequences","Graph-like structures where arcs connected by nodes represent hypothesized words.
",0.9090909090909091,0.5,0.6451612903225806,0.7,0.3684210526315789,0.48275862068965514,0.9090909090909091,0.5,0.6451612903225806,0.0739717965625249,0.8221613764762878,Yes,0.0,1.0,1.0
q372,How close do clusters match to ground truth tone categories?,"NMI between cluster assignments and ground truth tones for all sylables is:
Mandarin: 0.641
Cantonese: 0.464","In Mandarin, the 4 clusters correspond very well with the 4 phonemic tone categories.  In Cantonese, the 5 clusters correspond to low-falling, mid-level, high-level, mid-rising, and low-rising tones.
",0.15151515151515152,0.2777777777777778,0.19607843137254904,0.0,0.0,0.0,0.09090909090909091,0.16666666666666666,0.11764705882352942,0.014500372328365223,0.5247488617897034,No,0.0,0.0,0.0
q373,what are the evaluation metrics?,"Precision, Recall, F1","Task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, ROUGE, BLEU, METEOR, micro-F1, BPRA, F1-score, user satisfaction degree, dialogue fluency.
",0.04,0.3333333333333333,0.07142857142857142,0.0,0.0,0.0,0.04,0.3333333333333333,0.07142857142857142,0.02205438848664384,0.27809759974479675,No,0.0,0.0,0.0
q374,which datasets were used in evaluation?,"CoNLL 2003, GermEval 2014, CoNLL 2002, Egunkaria, MUC7, Wikigold, MEANTIME, SONAR-1, Ancora 2.0","Clean-labeled data, noisy-labeled training datasets (movie, laptop, and restaurant reviews), and a dataset (SCI) of 220 sentences from scientific literature.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.018196871111910016,0.20460247993469238,No,0.0,0.0,0.0
q375,what are the baselines?,Perceptron model using the local features.,"ANRES_Sing_R, ANRES_Sing_Z, ANRES_Seq_R, ANRES_Seq_Z;  content, content-network, and image-based models.
",0.05263157894736842,0.16666666666666666,0.08,0.0,0.0,0.0,0.05263157894736842,0.16666666666666666,0.08,0.011599713834969503,0.21379472315311432,No,0.0,0.0,0.0
q376,What monolingual word representations are used?,"AraVec for Arabic, FastText for French, and Word2vec Google News for English.","Monolingual word embeddings.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010211566521809648,0.3214159607887268,No,0.0,0.0,0.0
q377,Do they build one model per topic or on all topics?,One model per topic.,"The context mentions explicitly modeling different types of questions, but doesn't state whether one model is built per topic or across all topics.
",0.16666666666666666,1.0,0.2857142857142857,0.08695652173913043,0.6666666666666666,0.15384615384615383,0.16666666666666666,1.0,0.2857142857142857,0.035823421191287175,0.7385305762290955,No,0.0,0.0,0.0
q378,Do they quantitavely or qualitatively evalute the output of their low-rank approximation to verify the grouping of lexical items?,They evaluate quantitatively.,"Quantitatively.  An intrinsic evaluation using human annotation is described.
",0.2222222222222222,0.6666666666666666,0.3333333333333333,0.0,0.0,0.0,0.1111111111111111,0.3333333333333333,0.16666666666666666,0.03747776736677921,0.6092121005058289,No,1.0,1.0,1.0
q379,How well does their system perform on the development set of SRE?,"EER 16.04, Cmindet 0.6012, Cdet 0.6107","The provided text states that the system was evaluated on the SRE 2016 development set, but doesn't give the specific performance results.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.015732934811145335,0.21685881912708282,No,0.0,0.0,0.0
q380,Which of the classifiers showed the best performance?,Logistic regression,"Support Vector Machine (SVM)
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.049438003450632095,No,0.0,0.0,0.0
q381,How are the keywords associated with events such as protests selected?,"By using a Bayesian approach  and by using word-pairs, where they extract all the pairs of co-occurring words within each tweet.  They search for the words that achieve the highest number of spikes matching the days of events.","Keywords are selected by searching for words that achieve the highest number of spikes matching the days of events, using Jaccard similarity to evaluate the match between word-pair spikes and event days.
",0.6363636363636364,0.525,0.5753424657534246,0.4375,0.358974358974359,0.39436619718309857,0.48484848484848486,0.4,0.4383561643835617,0.31941939270474623,0.6920912265777588,No,1.0,1.0,1.0
q382,How many speeches are in the dataset?,5575 speeches,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12845686078071594,No,0.0,0.0,0.0
q383,What is different in BERT-gen from standard BERT?,"They use a left-to-right attention mask so that the input tokens can only attend to other input tokens, and the target tokens can only attend to the input tokens and already generated target tokens.","BERT-gen allows generating text from out-of-the-box data.
",0.18181818181818182,0.05555555555555555,0.0851063829787234,0.0,0.0,0.0,0.09090909090909091,0.027777777777777776,0.0425531914893617,0.0016676196328010327,0.07853956520557404,No,0.0,0.0,0.0
q384,How are multimodal representations combined?,The image feature vectors are mapped into BERT embedding dimensions and treated like a text sequence afterwards.,"Methods include concatenation and element-wise multiplication,  bilinear pooling, and gated combinations.
",0.08333333333333333,0.058823529411764705,0.06896551724137931,0.0,0.0,0.0,0.08333333333333333,0.058823529411764705,0.06896551724137931,0.02537856360538449,0.18662354350090027,No,0.0,0.0,0.0
q385,What is the problem with existing metrics that they are trying to address?,"Answer with content missing: (whole introduction) However, recent
studies observe the limits of ROUGE and find in
some cases, it fails to reach consensus with human.
judgment (Paulus et al., 2017; Schluter, 2017).","Existing metrics penalize diversity and do not reward comprehensible hypotheses.  They also correlate weakly with human judgments, especially for single reference scenarios.  Joint goal accuracy is impractical for large numbers of services.
",0.15625,0.15151515151515152,0.15384615384615385,0.06451612903225806,0.0625,0.06349206349206349,0.125,0.12121212121212122,0.12307692307692308,0.021967052775017967,0.1668732762336731,Yes,0.0,0.0,0.0
q386,How are discourse embeddings analyzed?,They perform t-SNE clustering to analyze discourse embeddings,"t-SNE clustering is performed on them, examining the closest neighbors of each embedding to observe if similar discourse relations tend to go together.
",0.2916666666666667,0.7777777777777778,0.42424242424242425,0.08695652173913043,0.25,0.12903225806451613,0.20833333333333334,0.5555555555555556,0.30303030303030304,0.035823421191287175,0.7809718251228333,Yes,1.0,1.0,1.0
q387,How are discourse features incorporated into the model?,They derive entity grid with grammatical relations and RST discourse relations and concatenate them with pooling vector for the char-bigrams before feeding to the resulting vector to the softmax layer.,"Embedding the discourse features in a parallel CNN at the input end, or concatenating them to the output layer as a feature vector.
",0.391304347826087,0.2903225806451613,0.33333333333333337,0.09090909090909091,0.06666666666666667,0.07692307692307691,0.2608695652173913,0.1935483870967742,0.2222222222222222,0.03451294450709141,0.3756614923477173,No,0.0,0.0,1.0
q388,What discourse features are used?,Entity grid with grammatical relations and RST discourse relations.,"Grammatical relations, and RST discourse relations.
",1.0,0.6666666666666666,0.8,1.0,0.625,0.7692307692307693,1.0,0.6666666666666666,0.8,0.4213952948452608,0.7140406370162964,Yes,0.0,1.0,1.0
q389,What are proof paths?,A sequence of logical statements represented in a computational graph,"Paths from the root to leaf nodes in an argument tree.
",0.09090909090909091,0.1,0.09523809523809525,0.0,0.0,0.0,0.09090909090909091,0.1,0.09523809523809525,0.033864985683445356,0.2242867350578308,No,0.0,0.0,0.0
q390,What external sources are used?,"Raw data from Gigaword, Automatically segmented text from Gigaword, Heterogenous training data from People's Daily, POS data from People's Daily","Fictional stories, general web documents, news articles, Wikipedia articles.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01883793616598845,0.2932952344417572,No,0.0,0.0,0.0
q391,How much better peformance is achieved in human evaluation when model is trained considering proposed metric?,"Pearson correlation to human judgement - proposed vs next best metric
Sample level comparison:
- Story generation: 0.387 vs 0.148
- Dialogue: 0.472 vs 0.341
Model level comparison:
- Story generation:  0.631 vs 0.302
- Dialogue: 0.783 vs 0.553","The provided text states the proposed metric correlates better with human judgment than other metrics, but does not quantify the improvement.
",0.19047619047619047,0.09523809523809523,0.12698412698412698,0.0,0.0,0.0,0.14285714285714285,0.07142857142857142,0.09523809523809523,0.00832670646829732,0.5877557992935181,No,0.0,0.0,1.0
q392,How much transcribed data is available for for Ainu language?,Transcribed data is available for duration of 38h 54m 38s for 8 speakers.,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.021395383339308693,0.07647649943828583,No,0.0,0.0,0.0
q393,What baseline approaches do they compare against?,"HotspotQA: Yang, Ding, Muppet
Fever: Hanselowski, Yoneda, Nie","K-means, SkipVec, RecNN, Para2vec,  ILP, ILP+MC, content, content-network, and image-based models.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.024426633221059636,0.15932148694992065,No,0.0,0.0,0.0
q394,how many domains did they experiment with?,2,"20
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2847701907157898,No,1.0,1.0,1.0
q395,How long is the dataset?,8000,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04955971613526344,No,0.0,0.0,0.0
q396,What is a string kernel?,String kernel is a technique that uses character n-grams to measure the similarity of strings,"String kernels measure the similarity of strings through character n-grams.
",0.9090909090909091,0.625,0.7407407407407406,0.7,0.4666666666666667,0.56,0.6363636363636364,0.4375,0.5185185185185185,0.29003222136949985,0.9640392661094666,Yes,1.0,1.0,1.0
q397,How do they correlate NED with emotional bond levels?,They compute Pearson’s correlation between NED measure for patient-to-therapist and patient-perceived emotional bond rating and NED measure for therapist-to-patient and patient-perceived emotional bond rating,"They compute the correlation of the proposed NED measure (both patient-to-therapist and therapist-to-patient) with patient-perceived emotional bond ratings.  A t-SNE transformation of the difference of patient-to-therapist turn embedding pairs is also used to visualize the relationship between session emotional bond levels and the measure.
",0.4423076923076923,0.7419354838709677,0.5542168674698795,0.23529411764705882,0.4,0.29629629629629634,0.34615384615384615,0.5806451612903226,0.43373493975903615,0.05662170583376254,0.7782390117645264,Yes,0.0,0.0,0.0
q398,What was their F1 score on the Bengali NER corpus?,52.0%,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1726384460926056,No,0.0,0.0,0.0
q399,What is the size of the dataset?,"300,000 sentences with 1.5 million single-quiz questions","287,226 training documents, 13,368 validation documents, 11,490 test documents.  17M training tokens.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.33987265825271606,No,0.0,0.0,0.0
q400,How many examples do they have in the target domain?,"Around 388k examples, 194k from tst2013 (in-domain) and 194k from newstest2014 (out-of-domain)","Not found in the context.
",0.2,0.06666666666666667,0.1,0.0,0.0,0.0,0.2,0.06666666666666667,0.1,0.0,0.10888678580522537,No,0.0,0.0,0.0
q401,What is the baseline model?,a RNN-based seq2seq VC model called ATTS2S based on the Tacotron model,"Baseline 1: smooth L1 distance; Baseline 2: PCA-based acoustic similarity; Baseline 3: Nonlinear dynamical systems-based complexity measure.  Model 1 is also mentioned as a baseline.
",0.14814814814814814,0.3076923076923077,0.2,0.0,0.0,0.0,0.1111111111111111,0.23076923076923078,0.15,0.011599713834969503,0.13660681247711182,No,0.0,0.0,0.0
q402,Where did they get training data?,AmazonQA and ConciergeQA datasets,"From a corpus of approximately 300,000 sentences, and 2,250 users contributing 225,000 tweets.
",0.0625,0.25,0.1,0.0,0.0,0.0,0.0625,0.25,0.1,0.02627961710408444,0.09848304092884064,No,0.0,0.0,0.0
q403,What extraction model did they use?,"Multi-Encoder, Constrained-Decoder model","Sequence-to-sequence model.
",0.25,0.2,0.22222222222222224,0.0,0.0,0.0,0.25,0.2,0.22222222222222224,0.0,0.2818068265914917,No,0.0,0.0,0.0
q404,Which datasets did they experiment on?,ConciergeQA and AmazonQA,"Action Adult Adventure Animation Biography Comedy Crime Documentary Drama Family Fantasy Film-Noir Game-Show History Horror Music Musical Mystery Reality-TV Romance Sci-Fi Short Sport Talk-Show Thriller War Western, Biomedical, novel-9
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08400735259056091,No,0.0,0.0,0.0
q405,How do slot binary classifiers improve performance?,by adding extra supervision to generate the slots that will be present in the response,"By resolving issues of TSCP, eliminating undesired dependencies among slots, and performing binary classification for each slot.
",0.11764705882352941,0.13333333333333333,0.125,0.0,0.0,0.0,0.11764705882352941,0.13333333333333333,0.125,0.019146030690102515,0.4311468303203583,Yes,0.0,0.0,1.0
q406,What baselines have been used in this work?,"NDM, LIDM, KVRN, and TSCP/RL","Baseline 1, Baseline 2, Baseline 3, ANRES_Sing_R, ANRES_Sing_Z, ANRES_Seq_R, ANRES_Seq_Z, LSTM baseline.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.013494116947566302,0.31471166014671326,No,0.0,0.0,0.0
q407,what bottlenecks were identified?,Confusion in recognizing the words that are active at a given node by a speech recognition solution developed for Indian Railway Inquiry System.,"Recognition bottlenecks in a menu-based speech solution.
",0.625,0.21739130434782608,0.3225806451612903,0.0,0.0,0.0,0.5,0.17391304347826086,0.25806451612903225,0.011175866963886784,0.48385733366012573,Yes,0.0,0.0,0.0
q408,By how much do they outperform BiLSTMs in Sentiment Analysis?,Proposed RCRN outperforms ablative baselines BiLSTM by +2.9% and 3L-BiLSTM by +1.1% on average across 16 datasets.,"The context states that the model significantly outperforms BiLSTMs, but does not provide a specific numerical difference.
",0.11764705882352941,0.1,0.1081081081081081,0.0,0.0,0.0,0.11764705882352941,0.1,0.1081081081081081,0.02055847707468329,0.484578937292099,No,0.0,0.0,0.0
q409,what state of the accuracy did they obtain?,51.5,"The provided text reports accuracies ranging from 58.3% to 99%.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3066530227661133,No,0.0,0.0,0.0
q410,what models did they compare to?,"High-order CNN, Tree-LSTM, DRNN, DCNN, CNN-MC, NBoW and SVM ","Stanford NER, Flair, and the best previously-published result,  NABERT+ baseline, and Ablation models.
",0.07142857142857142,0.08333333333333333,0.07692307692307691,0.0,0.0,0.0,0.07142857142857142,0.08333333333333333,0.07692307692307691,0.030272532566104675,0.3720756769180298,No,0.0,0.0,0.0
q411,which benchmark tasks did they experiment on?, They used Stanford Sentiment Treebank benchmark for sentiment classification task and   AG English news corpus for the text classification task.,"Molecule generation, processing a single trial, and a multi-task setting.
",0.18181818181818182,0.1,0.12903225806451613,0.0,0.0,0.0,0.18181818181818182,0.1,0.12903225806451613,0.01985294968108991,0.14069198071956635,No,0.0,0.0,0.0
q412,How is the proficiency score calculated?,"They used 6 indicators for proficiency (same for written and spoken) each marked by bad, medium or good by one expert.","Each indicator (of which there are 6) is assigned a value of 0, 1, or 2 (bad, medium, good, respectively).  The total score is computed by summing the scores of the indicators.
",0.25,0.38095238095238093,0.3018867924528302,0.03225806451612903,0.05,0.0392156862745098,0.15625,0.23809523809523808,0.18867924528301888,0.04389931794863787,0.5390172600746155,Yes,0.0,0.0,1.0
q413,What proficiency indicators are used to the score the utterances?,"6 indicators:
- lexical richness
- pronunciation and fluency
- syntactical correctness
- fulfillment of delivery
- coherence and cohesion
- communicative, descriptive, narrative skills","The context states that 6 indicators are used to score written and spoken utterances, but does not list them.
",0.15789473684210525,0.15789473684210525,0.15789473684210525,0.05555555555555555,0.05555555555555555,0.05555555555555555,0.15789473684210525,0.15789473684210525,0.15789473684210525,0.03094986967889915,0.6522961258888245,No,0.0,1.0,1.0
q414,What accuracy is achieved by the speech recognition system?,"Accuracy not available: WER results are reported 42.6 German, 35.9 English","91.2% accuracy on utterances containing words, sentences, and conversations.  82.9% accuracy for a specific prediction task.  80% accuracy in speaker-closed setting and 60% in speaker-open setting (average).
",0.06451612903225806,0.15384615384615385,0.09090909090909091,0.0,0.0,0.0,0.06451612903225806,0.15384615384615385,0.09090909090909091,0.00969094302009921,0.39756128191947937,No,0.0,0.0,0.0
q415,How is the speech recognition system evaluated?,Speech recognition system is evaluated using WER metric.,"Generally, recognition performance is evaluated by putting the solution to actual use by intended users and analyzing logs of successful and unsuccessful transactions.  A method for evaluation without actual user testing was also adopted.
",0.11764705882352941,0.5,0.19047619047619047,0.030303030303030304,0.14285714285714285,0.05,0.11764705882352941,0.5,0.19047619047619047,0.023698982732502774,0.5047500729560852,No,0.0,0.0,0.0
q416,How many of the utterances are transcribed?,Total number of transcribed utterances including Train and Test for both Eng and Ger language is 5562 (2188 cleaned),"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10197373479604721,No,0.0,0.0,0.0
q417,How many utterances are in the corpus?,Total number of utterances available is: 70607 (37344 ENG + 33263 GER),"1,242,998
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.40231946110725403,No,0.0,0.0,0.0
q418,By how much does their model outperform both the state-of-the-art systems?,w.r.t Rouge-1 their model outperforms by 0.98% and w.r.t Rouge-L their model outperforms by 0.45%,"The model outperforms the three baseline systems (Stanford, Multigraph, and MIR) on all evaluation metrics, achieving improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford system.  Improvements of 2.62% and 3.02% were obtained over ReDAN+BIBREF26 and MReaL–BDAI, respectively, on the CoNLL F1 score.
",0.057692307692307696,0.13043478260869565,0.08,0.0196078431372549,0.045454545454545456,0.027397260273972605,0.057692307692307696,0.13043478260869565,0.08,0.019333889829607733,0.5245880484580994,No,0.0,0.0,0.0
q419,What is the state-of-the art?,neural attention model with a convolutional encoder with an RNN decoder and RNN encoder-decoder,"The provided text mentions several state-of-the-art models, including those from BIBREF24 (with 66.7% accuracy) and various other referenced works.  It also states that the described method outperforms some of these.
",0.08823529411764706,0.2,0.12244897959183675,0.0,0.0,0.0,0.08823529411764706,0.2,0.12244897959183675,0.012193311110766474,0.17445170879364014,No,0.0,0.0,0.0
q420,What was the performance on the self-collected corpus?,F1 scores of 86.16 on slot filling and 94.56 on intent detection,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12718303501605988,No,0.0,0.0,0.0
q421,What is the size of their dataset?,"10,001 utterances","17M training tokens, 44k vocabulary size.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.44909903407096863,No,0.0,0.0,0.0
q422,how was the dataset built?,"Questions are gathered from anonymized, aggregated queries to the Google search engine. Queries that are likely to be yes/no questions are heuristically identified: we found selecting queries where the first word is in a manually constructed set of indicator words and are of sufficient length, to be effective. Questions are only kept if a Wikipedia page is returned as one of the first five results, in which case the question and Wikipedia page are given to a human annotator for further processing. Annotators label question/article pairs in a three-step process. First, they decide if the question is good, meaning it is comprehensible, unambiguous, and requesting factual information. This judgment is made before the annotator sees the Wikipedia page. Next, for good questions, annotators find a passage within the document that contains enough information to answer the question. Annotators can mark questions as “not answerable"" if the Wikipedia article does not contain the requested information. Finally, annotators mark whether the question's answer is “yes"" or “no""","The methods for constructing datasets vary.  Some were filtered, trimmed, and balanced.  Others involved web scraping, conversion of PDFs to text, and pattern matching.  Still others were sourced from existing corpora.
",0.25806451612903225,0.047337278106508875,0.07999999999999999,0.0,0.0,0.0,0.1935483870967742,0.03550295857988166,0.06,0.0006227109480307413,0.28703057765960693,No,1.0,1.0,1.0
q423,what processing was done on the speeches before being parsed?,Remove numbers and interjections,"Punctuation, XML characters, hyperlinks, Twitter-specific ""at-mentions"" and ""hashtags"" were removed.  Speeches were broken into blocks of about 5 minutes each, and concatenated utterances were transcribed.
",0.07407407407407407,0.5,0.12903225806451613,0.0,0.0,0.0,0.07407407407407407,0.5,0.12903225806451613,0.010559717178170818,0.2682895362377167,No,0.0,0.0,0.0
q424,What is the performance change of the textual semantic similarity task when no error and maximum errors (noise) are present?,"10 Epochs: pearson-Spearman correlation  drops  60 points when error increase by 20%
50 Epochs: pearson-Spearman correlation  drops  55 points when error increase by 20%","The context states that as the percentage of error increases, there is a significant drop in performance for the textual semantic similarity task.  However, it does not give specific performance values for no error or maximum error conditions.
",0.10526315789473684,0.15384615384615385,0.125,0.02702702702702703,0.04,0.03225806451612903,0.10526315789473684,0.15384615384615385,0.125,0.01038539619163422,0.34619835019111633,No,0.0,0.0,0.0
q425,Which sentiment analysis data set has a larger performance drop when a 10% error is introduced?,SST-2 dataset,"The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.053742747753858566,No,0.0,0.0,0.0
q426,How much is pre-training loss increased in Low/Medium/Hard level of pruning?,"The increase is linearly from lowest on average 2.0 , medium around 3.5, and the largest is 6.0","Low levels (30-40%): no increase.
Medium levels: increases.
Hard levels (past 40%): increases.
",0.14285714285714285,0.1,0.11764705882352941,0.0,0.0,0.0,0.14285714285714285,0.1,0.11764705882352941,0.01387819277861591,0.6803617477416992,No,0.0,0.0,0.0
q427,What is the average length of the recordings?,40 minutes,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04453941434621811,No,0.0,0.0,0.0
q428,What were their results?,Best model achieved F-score 74.7 on NALCS and F-score of 70.0 on LMS on test set,"The results showed good positions in three international contests (TASS'15, SemEval'15-16, SENTIPOLC'14) and excellent results in five other languages.  Results were also up to 0.85 BLEU points better than the baseline on newstest2019, and surpassed the baseline by 0.18 points on newstest2017 (statistically significant) and 0.15 points on newstest2018 (no statistical significance).  Subjective and objective task success were similar, with dialogues resolving emergencies rated higher.  The document system obtained good results in English-French translation.
",0.06097560975609756,0.25,0.09803921568627451,0.0,0.0,0.0,0.06097560975609756,0.25,0.09803921568627451,0.005650168428812912,0.5040704607963562,No,0.0,0.0,0.0
q429,What is the prediction accuracy of the model?,"mean prediction accuracy 0.99582651
S&P 500 Accuracy 0.99582651","The model achieves 69.9% accuracy on training data, 64.7% on validation, and 65.3% on test data for binary classification.  It achieves 82.9% accuracy for synchronization offset prediction.  Other accuracies are also given for age, gender, dialect, and joint accuracy.
",0.06976744186046512,0.2727272727272727,0.11111111111111109,0.0,0.0,0.0,0.06976744186046512,0.2727272727272727,0.11111111111111109,0.008496070958802303,0.4761751890182495,No,1.0,1.0,1.0
q430,What is the dataset used in the paper?,"historical S&P 500 component stocks
 306242 news articles","Biomedical, SCI, novel-9, CNNDM, NYT50
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3132098317146301,No,0.0,0.0,1.0
q431,How does the SCAN dataset evaluate compositional generalization?,"it systematically holds out inputs in the training set containing basic primitive verb, ""jump"", and tests on sequences containing that verb.","The SCAN dataset tests compositional generalization by evaluating the ability to generalize known primitive verbs to valid unseen constructions, and by using known templates.  It includes splits with multiple examples of the held-out primitive verb.
",0.2222222222222222,0.38095238095238093,0.2807017543859649,0.02857142857142857,0.05,0.03636363636363636,0.1388888888888889,0.23809523809523808,0.17543859649122806,0.03488938872262233,0.5017101168632507,No,0.0,0.0,1.0
q432,How much does this system outperform prior work?,"The system outperforms by 27.7% the LSTM model, 38.5% the RL-SPINN model and 41.6% the Gumbel Tree-LSTM","The system outperforms the three baseline systems by 2.93% and 3.01% on CoNLL F1 score.
",0.35294117647058826,0.2727272727272727,0.30769230769230765,0.125,0.09523809523809523,0.1081081081081081,0.29411764705882354,0.22727272727272727,0.25641025641025644,0.08647825468524677,0.6226396560668945,No,0.0,0.0,0.0
q433,What are the baseline systems that are compared against?,"The system is compared to baseline models: LSTM, RL-SPINN and Gumbel Tree-LSTM","Baseline 1, Baseline 2, Baseline 3, Stanford, Multigraph, MIR, Tactron 2
",0.09090909090909091,0.07142857142857142,0.08,0.0,0.0,0.0,0.09090909090909091,0.07142857142857142,0.08,0.022869567780619007,0.4014957547187805,No,0.0,0.0,0.0
q434,What systems are tested?,"BULATS i-vector/PLDA
BULATS x-vector/PLDA
VoxCeleb x-vector/PLDA
PLDA adaptation (X1)
 Extractor fine-tuning (X2) ","Support Vector Machines (SVMs), logistic regression, Naïve Bayes, Perceptron, and decision trees.  The context also mentions neural network models, but doesn't explicitly name the specific types.
",0.03571428571428571,0.05263157894736842,0.0425531914893617,0.0,0.0,0.0,0.03571428571428571,0.05263157894736842,0.0425531914893617,0.012557690800697192,0.2160130739212036,No,0.0,0.0,0.0
q435,What benchmark datasets they use?,VQA and GeoQA,"Stanford Sentiment Treebank (SST-2), Yelp dataset, Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), Quora Question Pairs (QQP), SQuAD 1.1, SQuAD 2.0, MRQA, QA4IE-SPAN-S/M/L, QA4IE-SEQ-S/M/L.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1275957226753235,No,0.0,0.0,0.0
q436,How do they combine MonaLog with BERT?,They use Monalog for data-augmentation to fine-tune BERT on this task,"MonaLog is followed by the re-trained BERT.
",0.25,0.15384615384615385,0.1904761904761905,0.0,0.0,0.0,0.25,0.15384615384615385,0.1904761904761905,0.037954847898457066,0.702785074710846,Yes,0.0,0.0,1.0
q437,How do they select monotonicity facts?,They derive it from Wordnet,"They use automatically generated datasets (HELP), crowdsourced datasets, and examples from previous manually curated datasets and linguistics publications.  Some examples are excluded due to non-strict downward monotonicity.
",0.07142857142857142,0.4,0.12121212121212122,0.0,0.0,0.0,0.07142857142857142,0.4,0.12121212121212122,0.013355980882200826,0.39259883761405945,No,0.0,0.0,0.0
q438,What are the 12 categories devised?,"Economics, Genocide, Geography, History, Human Rights, Kurdish, Kurdology, Philosophy, Physics, Theology, Sociology, Social Study","The provided text describes 15 hand-coded categories and a 44-community partition, but does not explicitly list 12 categories.  The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.011993481292529619,0.1773223876953125,No,0.0,0.0,0.0
q439,what are the off-the-shelf systems discussed in the paper?,"Answer with content missing: (Names of many identifiers missing) TextCat, ChromeCLD, LangDetect, langid.py, whatlang, whatthelang, YALI, LDIG, Polyglot 3000, Lextek Language Identifier and Open Xerox Language Identifier.","IBREF28, Latent-Tree, Berkeley, LaSO, Latent-Strc, Model-Stack, Non-Linear.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.006251427845836824,0.38867712020874023,No,0.0,0.0,0.0
q440,How many rules had to be defined?,"WikiSQL - 2 rules (SELECT, WHERE)
SimpleQuestions - 1 rule
SequentialQA - 3 rules (SELECT, WHERE, COPY)","Ten
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.033411670476198196,No,0.0,0.0,0.0
q441,What was performance of classifiers before/after using distant supervision?,"Bi-LSTM: For low resource <17k clean data: Using distant supervision resulted in huge boost of F1 score (1k eg. ~9 to ~36 wit distant supervision)
BERT: <5k clean data boost of F1 (1k eg. ~32 to ~47 with distant supervision)","Training a ranker with distant supervision performs significantly better than a random baseline.  A model trained on clean and distantly-supervised data deteriorates if the distant supervision technique has a higher F1-score.
",0.30303030303030304,0.24390243902439024,0.2702702702702703,0.125,0.1,0.11111111111111112,0.18181818181818182,0.14634146341463414,0.16216216216216217,0.03399382159084898,0.5019835829734802,No,0.0,0.0,0.0
q442,How big are the datasets used?,"Evaluation datasets used:
CMRC 2018 - 18939 questions, 10 answers
DRCD - 33953 questions, 5 answers
NIST MT02/03/04/05/06/08 Chinese-English - Not specified

Source language train data:
SQuAD - Not specified","The context describes various datasets with sizes ranging from tens of thousands (e.g., SST-2, MRPC) to millions (e.g., English-Italian and English-German sentence pairs).  Specific sizes are given for some datasets (e.g., CNNDM, NYT50, SCI).
",0.05,0.0625,0.05555555555555556,0.0,0.0,0.0,0.05,0.0625,0.05555555555555556,0.008385529424168122,0.42981669306755066,No,0.0,0.0,0.0
q443,What datasets are used for training/testing models? ,"Microsoft Research dataset containing movie, taxi and restaurant domains.","The context mentions datasets in Tables TABREF12, TABREF19, TABREF83, and TABREF10, but does not explicitly list the specific datasets used for training and testing.
",0.08333333333333333,0.2222222222222222,0.12121212121212122,0.0,0.0,0.0,0.08333333333333333,0.2222222222222222,0.12121212121212122,0.01693409667719809,0.29988011717796326,No,0.0,0.0,0.0
q444,How better is gCAS approach compared to other approaches?,"For entity  F1 in the movie, taxi and restaurant domain it results in scores of 50.86, 64, and 60.35. For success, it results it outperforms in the movie and restaurant domain with scores of 77.95 and 71.52","gCAS outperforms all other methods in most metrics, especially in predicting slots consistent with the act, entity F$_1$, and success F$_1$.  Its advantage is particularly notable in the movie domain and in the restaurant domain where agents frequently have multiple slots.
",0.3488372093023256,0.36585365853658536,0.35714285714285715,0.09523809523809523,0.1,0.0975609756097561,0.23255813953488372,0.24390243902439024,0.2380952380952381,0.05001642136895343,0.36272093653678894,Yes,0.0,0.0,0.0
q445,What is specific to gCAS cell?,"It has three sequentially connected units to output continue, act and slots generating multi-acts in a doble recurrent manner.","gCAS uses a recurrent cell with separate ""continue,"" ""act,"" and ""slots"" units that are sequentially connected.
",0.5625,0.45,0.5,0.26666666666666666,0.21052631578947367,0.23529411764705882,0.25,0.2,0.22222222222222224,0.05486024359355534,0.5870429277420044,Yes,1.0,1.0,1.0
q446,What is the source of external knowledge?,counts of predicate-argument tuples from English Wikipedia,"Knowledge bases for specialized domains.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.010604922659695148,No,0.0,0.0,0.0
q447,What were the sizes of the test sets?,Test set 1 contained 57 drug labels and 8208 sentences and test set 2 contained 66 drug labels and 4224 sentences,"1585 IPUs and 1841 IPUs.
",0.2,0.047619047619047616,0.07692307692307693,0.0,0.0,0.0,0.2,0.047619047619047616,0.07692307692307693,0.006662591348749398,0.205120250582695,No,0.0,0.0,0.0
q448,Which datasets are used?,"ABSA SemEval 2014-2016 datasets
Yelp Academic Dataset
Wikipedia dumps","Stanford Sentiment Treebank (SST-2), Yelp, Microsoft Research Paraphrase Corpus (MRPC), Semantic Textual Similarity Benchmark (STS-B), Quora Question Pairs (QQP), SQuAD 1.1, SQuAD 2.0, MRQA, SCI (a set of 220 sentences from the scientific literature).
",0.02631578947368421,0.1,0.041666666666666664,0.0,0.0,0.0,0.02631578947368421,0.1,0.041666666666666664,0.006866732953963861,0.47934070229530334,No,0.0,0.0,0.0
q449,How much does it minimally cost to fine-tune some model according to benchmarking framework?,"$1,728","A few minutes to hours.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0801490843296051,No,0.0,0.0,0.0
q450,What models are included in baseline benchmarking results?,"BERT, XLNET RoBERTa, ALBERT, DistilBERT","Baseline 1, Baseline 2, and Baseline 3.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04990049701936832,0.10517344623804092,No,0.0,0.0,0.0
q451,"It looks like learning to paraphrase questions, a neural scoring model and a answer selection model cannot be trained end-to-end. How are they trained?",using multiple pivot sentences,"The entire system is trained end-to-end using question-answer pairs as a supervision signal.
",0.0625,0.25,0.1,0.0,0.0,0.0,0.0625,0.25,0.1,0.028398387225677894,0.06022642180323601,No,0.0,0.0,0.0
q452,How much more accurate is the model than the baseline?,"For the Oshiete-goo dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, Trans, by 0.021, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.037.  For the nfL6 dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, CLSTM, by 0.028, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.040. Human evaluation of the NAGM's generated outputs for the Oshiete-goo dataset had 47% ratings of (1), the highest rating, while CLSTM only received 21% ratings of (1). For the nfL6 dataset, the comparison of (1)'s was NAGM's 50% to CLSTM's 30%. ","The model outperforms the baseline by 35% in Exact Match and 25% in Goal Match in previously seen environments.  Also, the model makes significantly fewer errors for ""What"" questions.
",0.27586206896551724,0.064,0.1038961038961039,0.0,0.0,0.0,0.27586206896551724,0.064,0.1038961038961039,0.0020517391195461836,0.5048319101333618,No,0.0,0.0,0.0
q453,What is new state-of-the-art performance on CoNLL-2009 dataset?,In closed setting 84.22 F1 and in open 87.35 F1.,"The answer is not found in the context.
",0.125,0.08333333333333333,0.1,0.0,0.0,0.0,0.125,0.08333333333333333,0.1,0.045400138092837265,0.3038182556629181,No,0.0,0.0,0.0
q454,What are two strong baseline methods authors refer to?,Marcheggiani and Titov (2017) and Cai et al. (2018),"PCA-based symmetric acoustic similarity measure by Lee et al. and a LSTM baseline.
",0.21428571428571427,0.3333333333333333,0.2608695652173913,0.07692307692307693,0.125,0.09523809523809525,0.14285714285714285,0.2222222222222222,0.17391304347826086,0.10511846841633776,0.2600139081478119,No,0.0,0.0,0.0
q455,How many category tags are considered?,14 categories,"100
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05683956667780876,No,0.0,0.0,0.0
q456,What domain does the dataset fall into?,YouTube videos,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0267289150506258,No,0.0,0.0,0.0
q457,How are EAC evaluated?,"Qualitatively through efficiency, effectiveness and satisfaction aspects and quantitatively through metrics such as precision, recall, accuracy, BLEU score and even human judgement.","Qualitative assessment using ISO 9241 (efficiency, effectiveness, satisfaction) and quantitative analysis (two techniques).
",0.46153846153846156,0.2727272727272727,0.3428571428571428,0.16666666666666666,0.09523809523809523,0.12121212121212123,0.46153846153846156,0.2727272727272727,0.3428571428571428,0.06207192091946043,0.5379326343536377,Yes,0.0,1.0,1.0
q458,What is triangulation?,"Answer with content missing: (Chapter 3) The concept can be easily explained with an example, visualized in Figure 1. Consider the Portuguese (Pt) word trabalho which, according to the MUSE Pt–En dictionary, has the words job and work as possible En translations. In turn, these two En words can be translated to 4 and 5 Czech (Cs) words respectively. By utilizing the transitive property (which translation should exhibit) we can identify the set of 7 possible Cs translations for the Pt word trabalho.","The answer is not found in the context.
",0.5,0.047619047619047616,0.08695652173913042,0.0,0.0,0.0,0.375,0.03571428571428571,0.06521739130434782,2.7392066425764806e-06,0.19508090615272522,No,0.0,0.0,0.0
q459,What languages do they use?,"Train languages are: Cantonese, Bengali, Pashto, Turkish, Vietnamese, Haitian, Tamil, Kurdish, Tokpisin and Georgian, while Assamese, Tagalog, Swahili, Lao are used as target languages.","English, Spanish, French, German, Russian, Italian, Amharic, Arabic, Hindi, Somali, Basque, French, German, Hungarian, Italian, Navajo, Turkish, Quechua, English, Spanish, Zulu,  Russian, Polish, Norwegian, Romanian, Dutch, Czech, Turkish, Thai, Urdu, and potentially others from BIBREF21/22.
",0.05555555555555555,0.08333333333333333,0.06666666666666667,0.0,0.0,0.0,0.05555555555555555,0.08333333333333333,0.06666666666666667,0.02959801342730592,0.6790494918823242,No,0.0,0.0,0.0
q460,How they evaluate their approach?,"They  evaluate newly proposed models in several low-resource settings across different languages with real, distantly supervised data with non-synthetic noise","Evaluation using cross-domain scenarios, comparing estimates with actual attack effects, studying adversarial attack impact on evaluation metrics, exploring application to KG representation interpretability, detecting incorrect triples, and leave-one-out evaluation.  Human evaluation of NLG models is also used.
",0.075,0.13636363636363635,0.09677419354838708,0.0,0.0,0.0,0.05,0.09090909090909091,0.06451612903225806,0.01095208360430825,0.4547041058540344,No,0.0,0.0,0.0
q461,How large is the corpus?,"It contains 106,350 documents","There are multiple corpora of varying sizes mentioned.  No single ""corpus"" size is definitively stated.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.45408812165260315,No,0.0,0.0,0.0
q462,How large is the dataset?,over 104k documents,"The answer is not found in the context.  While various dataset sizes are mentioned, there's no single ""dataset"" size given.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13341039419174194,No,0.0,0.0,0.0
q463,What was their perplexity score?,Perplexity score 142.84 on dev and 138.91 on test,"The answer is not found in the context.  While perplexity scores are discussed and referenced to tables, no specific score is given.
",0.13636363636363635,0.2727272727272727,0.1818181818181818,0.047619047619047616,0.1,0.06451612903225806,0.13636363636363635,0.2727272727272727,0.1818181818181818,0.017911710595643587,0.6763226389884949,No,0.0,1.0,1.0
q464,What parallel corpus did they use?,Parallel monolingual corpus in English and Mandarin,"Parallel corpora from the shared translation task of WMT'15 and WMT'16.
",0.15384615384615385,0.2857142857142857,0.2,0.0,0.0,0.0,0.15384615384615385,0.2857142857142857,0.2,0.0402724819242185,0.431001216173172,No,0.0,0.0,0.0
q465,In which setting they achieve the state of the art?,in open-ended task esp. for counting-type questions ,"SemEval-2015 and SemEval-2016.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.058763508032616325,0.04884079843759537,No,0.0,0.0,0.0
q466,What they formulate the question generation as?,LASSO optimization problem,"Automatically generating questions from various inputs like text, databases, or semantic representations.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.10573865473270416,No,0.0,0.0,0.0
q467,"Was the degree of offensiveness taken as how generally offensive the text was, or how personally offensive it was to the annotator?",Personal thought of the annotator.,"The offensiveness was rated on a 6-point Likert scale.
",0.1,0.2,0.13333333333333333,0.0,0.0,0.0,0.1,0.2,0.13333333333333333,0.033864985683445356,0.08967381715774536,No,0.0,0.0,0.0
q468,Which embeddings do they detect biases in?,Word embeddings trained on GoogleNews and Word embeddings trained on Reddit dataset,"Word embeddings.
",1.0,0.16666666666666666,0.2857142857142857,1.0,0.09090909090909091,0.16666666666666669,1.0,0.16666666666666666,0.2857142857142857,0.0,0.6588382720947266,No,0.0,0.0,0.0
q469,What are the two PharmaCoNER subtasks?,Entity identification with offset mapping and concept indexing,"Annotation of pharmacological, chemical, and biomedical entities in health records; and evaluation of the quality of datasets used to evaluate systems of this type.
",0.08333333333333333,0.25,0.125,0.0,0.0,0.0,0.08333333333333333,0.25,0.125,0.013353534059549444,0.19751697778701782,No,0.0,0.0,0.0
q470,How do they perform data augmentation?,They randomly sample sentences from Wikipedia that contains an object RC and add them to training data,"Mirroring, random cropping, replacement of words with relevant words (synonyms), frequency masking, speed perturbation, volume perturbation, and time masking.
",0.05263157894736842,0.058823529411764705,0.05555555555555555,0.0,0.0,0.0,0.05263157894736842,0.058823529411764705,0.05555555555555555,0.013353534059549444,0.2876408100128174,No,0.0,0.0,0.0
q471,What are the characteristics of the city dialect?,Lexicon of the cities tend to use most forms of a particular concept,"The city descriptions are relatively short, Calvino's writing is filled with rare words, complex syntactic structures, and figurative language.  Capturing the essential components of each city in a single vector is not simple.
",0.11764705882352941,0.3076923076923077,0.1702127659574468,0.030303030303030304,0.08333333333333333,0.04444444444444444,0.11764705882352941,0.3076923076923077,0.1702127659574468,0.012753998269755913,0.4074564576148987,No,0.0,0.0,0.0
q472,What are the characteristics of the rural dialect?,It uses particular forms of a concept rather than all of them uniformly,"Rural regions, with small, scattered populations, have more specific words in their lexicon.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.28655481338500977,No,0.0,1.0,1.0
q473,What is the performance of the models on the tasks?,"Overall accuracy per model is: 5-gram (60.5), LSTM (68.9), TXL (68.7), GPT-2 (80.1)","The best model achieved 42.48% accuracy for age, 37.50% for dialect, and 57.81% for gender.  Other performance metrics (like F1 scores) are also reported, but specific performance for other tasks is not comprehensively summarized.
",0.08108108108108109,0.15789473684210525,0.10714285714285714,0.0,0.0,0.0,0.05405405405405406,0.10526315789473684,0.07142857142857144,0.013995516592202837,0.6679273247718811,No,0.0,0.0,0.0
q474,What other non-neural baselines do the authors compare to? ,"bag of words, tf-idf, bag-of-means","PCA-based symmetric acoustic similarity measure by Lee et al. and Nonlinear dynamical systems-based complexity measure.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08336514979600906,No,0.0,0.0,0.0
q475,On what dataset is Aristo system trained?,"Aristo Corpus
Regents 4th
Regents 8th
Regents `12th
ARC-Easy
ARC-challenge ","Several datasets of independently-authored science questions taken from standardized tests.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.20727182924747467,No,0.0,1.0,1.0
q476,How many roles are proposed?,12,"The context does not specify a precise number of proposed roles.  It mentions examples like ""first_element"" and ""second_element"" but doesn't give a total count.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13885901868343353,No,0.0,0.0,0.0
q477,What language technologies have been introduced in the past?,"- Font & Keyboard
- Speech-to-Text
- Text-to-Speech
- Text Prediction
- Spell Checker
- Grammar Checker
- Text Search
- Machine Translation
- Voice to Text Search
- Voice to Speech Search","Statistical modeling approaches, linguistic theory based approaches, deep neural networks, LSTMs, Transformer-based architectures.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.453656941652298,No,0.0,0.0,0.0
q478,How do they define local variance?,The reciprocal of the variance of the attention distribution,"Local variance loss is the reciprocal of the variance of the attention distribution, calculated using the median value instead of the mean to be more robust to outliers.
",0.32142857142857145,1.0,0.4864864864864865,0.2962962962962963,1.0,0.45714285714285713,0.32142857142857145,1.0,0.4864864864864865,0.22481074167380632,0.609600305557251,Yes,1.0,1.0,1.0
q479,how do they measure discussion quality?,"Measuring three aspects: argumentation, specificity and knowledge domain.","Discussion quality is measured by students' argument moves (claims and textual evidence), specificity, and knowledge domain.  Inter-rater reliability is assessed using Cohen's kappa, and validity is compared to English Education experts' evaluations.
",0.17647058823529413,0.75,0.2857142857142857,0.09090909090909091,0.42857142857142855,0.15000000000000002,0.17647058823529413,0.75,0.2857142857142857,0.07022183955845689,0.5925739407539368,Yes,1.0,1.0,1.0
q480,what were the baselines?,"2008 Punyakanok et al. 
2009 Zhao et al. + ME 
2008 Toutanova et al. 
2010 Bjorkelund et al.  
2015 FitzGerald et al. 
2015 Zhou and Xu 
2016 Roth and Lapata 
2017 He et al. 
2017 Marcheggiani et al.
2017 Marcheggiani and Titov 
2018 Tan et al. 
2018 He et al. 
2018 Strubell et al. 
2018 Cai et al. 
2018 He et al. 
2018 Li et al. 
","Baseline 1: smooth L1 distance; Baseline 2: PCA-based symmetric acoustic similarity; Baseline 3: Nonlinear dynamical systems-based complexity measure; ANRES_Sing_R; ANRES_Sing_Z; ANRES_Seq_R; ANRES_Seq_Z
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2464817464351654,No,0.0,0.0,0.0
q481,Which soft-selection approaches are evaluated?,LSTM and BERT ,"Traditional attention-based soft-selection methods.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09196469187736511,No,0.0,0.0,0.0
q482,How big is slot filing dataset?,"Dataset has 1737 train, 497 dev and 559 test sentences.","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03416211359799978,0.20938529074192047,No,0.0,0.0,0.0
q483,How large is the dataset they generate?,4.756 million sentences,"287,226 training documents, 13,368 validation documents, and 11,490 test documents.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3228995203971863,No,0.0,0.0,0.0
q484,What are the weaknesses of their proposed interpretability quantification method?,can be biased by dataset used and may generate categories which are suboptimal compared to human designed categories,"The context does not describe a proposed interpretability quantification method.  Therefore, weaknesses cannot be identified.
",0.06666666666666667,0.05555555555555555,0.0606060606060606,0.0,0.0,0.0,0.06666666666666667,0.05555555555555555,0.0606060606060606,0.021476912089159056,0.3505474328994751,No,0.0,0.0,0.0
q485,What advantages does their proposed method of quantifying interpretability have over the human-in-the-loop evaluation they compare to?,it is less expensive and quantifies interpretability using continuous values rather than binary evaluations,"The provided text describes a method for evaluating machine translation and natural language generation, not a method for quantifying interpretability.  Therefore, a comparison to a human-in-the-loop evaluation for interpretability is not found in the context.
",0.13157894736842105,0.35714285714285715,0.1923076923076923,0.02702702702702703,0.07692307692307693,0.04,0.10526315789473684,0.2857142857142857,0.15384615384615385,0.012413584734794654,0.36165952682495117,No,0.0,0.0,0.0
q486,How was lexical diversity measured?,By computing number of unique responses and number of responses divided by the number of unique responses to that question for each of the questions,"Yule's I measure and the Measure of Textual Lexical Diversity (MTLD).
",0.25,0.12,0.16216216216216217,0.0,0.0,0.0,0.25,0.12,0.16216216216216217,0.017035018586323527,0.14360061287879944,No,0.0,0.0,0.0
q487,Which existing models does this approach outperform?,"RNN-context, SRB, CopyNet, RNN-distract, DRGD","Regression-based approaches, models using manually defined knowledge and features, and prior state-of-the-art NLI models (BiMPM, ESIM, Decomposable Attention Model, KIM, and BERT).
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.016961343903963387,0.189070463180542,No,0.0,0.0,0.0
q488,What human evaluation method is proposed?,comparing the summary with the text instead of the reference and labeling the candidate bad if it is incorrect or irrelevant,"Annotators evaluate the summary against the source content based on the goodness of the summary.
",0.4,0.2857142857142857,0.3333333333333333,0.14285714285714285,0.1,0.11764705882352941,0.3333333333333333,0.23809523809523808,0.2777777777777778,0.05600607423889073,0.5193849205970764,Yes,1.0,1.0,1.0
q489,What languages are represented in the dataset?,"EN, JA, ES, AR, PT, KO, TH, FR, TR, RU, IT, DE, PL, NL, EL, SV, FA, VI, FI, CS, UK, HI, DA, HU, NO, RO, SR, LV, BG, UR, TA, MR, BN, IN, KN, ET, SL, GU, CY, ZH, CKB, IS, LT, ML, SI, IW, NE, KM, MY, TL, KA, BO","English, French, German, Spanish, Russian, Italian.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.5768763301763e-05,0.5099040865898132,No,0.0,0.0,0.0
q490,How faster is training and decoding compared to former models?,"Proposed vs best baseline:
Decoding: 8541 vs 8532 tokens/sec
Training: 8h vs 8h","The provided text states that the model results in faster decoding time, but does not compare it to *former* models.  The answer is not found in the context.
",0.03571428571428571,0.07142857142857142,0.047619047619047616,0.0,0.0,0.0,0.03571428571428571,0.07142857142857142,0.047619047619047616,0.0,0.4463939368724823,No,1.0,1.0,1.0
q491,What datasets are used to evaluate the model?,WN18 and FB15k,"Swissmetro, Oshiete-goo, and nfL6 datasets.
",0.16666666666666666,0.3333333333333333,0.2222222222222222,0.0,0.0,0.0,0.16666666666666666,0.3333333333333333,0.2222222222222222,0.05522397783539471,0.13455606997013092,No,0.0,0.0,0.0
q492,What is the source of the dataset?,"Online sites tagged as fake news site by Verafiles and NUJP and news website in the Philippines, including Pilipino Star Ngayon, Abante, and Bandera","The dataset sources include online debate websites, the OA-STM corpus, the testing images of MS COCO dataset, and the testing questions of US DARPA.
",0.2,0.20833333333333334,0.20408163265306126,0.0,0.0,0.0,0.16,0.16666666666666666,0.16326530612244897,0.0335823602617743,0.2753713130950928,No,0.0,0.0,0.0
q493,What were the baselines?,"Siamese neural network consisting of an embedding layer, a LSTM layer and a feed-forward layer with ReLU activations","Baseline 1: smooth L1 distance; Baseline 2: PCA-based symmetric acoustic similarity; Baseline 3: Nonlinear dynamical systems-based complexity measure; ANRES_Sing_R; ANRES_Sing_Z; ANRES_Seq_R; ANRES_Seq_Z.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19075322151184082,No,0.0,0.0,0.0
q494,What metadata is included?,"besides claim, label and claim url, it also includes a claim ID, reason, category, speaker, checker, tags, claim entities, article title, publish data and claim date","Normalized representation of the service interface (name, tasks, entities, descriptions).
",0.1,0.038461538461538464,0.05555555555555555,0.0,0.0,0.0,0.1,0.038461538461538464,0.05555555555555555,0.016664939497456552,0.300778329372406,No,0.0,0.0,0.0
q495,Do the images have multilingual annotations or monolingual ones?,monolingual,"Monolingual.
",1.0,1.0,1.0,0.0,0.0,0.0,1.0,1.0,1.0,0.0,0.9461771249771118,Yes,0.0,0.0,1.0
q496,How much important is the visual grounding in the learning of the multilingual representations?,performance is significantly degraded without pixel data,"The provided text states that visual grounding has been applied to visual question answering, image captioning, and the learning of sentence representations, but it doesn't explicitly state how important it is for multilingual representations.  The answer is not found in the context.
",0.023255813953488372,0.14285714285714285,0.04,0.0,0.0,0.0,0.023255813953488372,0.14285714285714285,0.04,0.007772751558853213,0.10607659816741943,No,0.0,0.0,0.0
q497,How is the generative model evaluated?,Comparing BLEU score of model with and without attention,"A classifier is trained on the generated dataset, and its accuracy is evaluated on the original test set.  The accuracy is compared to the accuracy of a classifier trained on the original training set and tested on the original test set.
",0.07317073170731707,0.3333333333333333,0.12,0.0,0.0,0.0,0.07317073170731707,0.3333333333333333,0.12,0.009896324688555468,0.18783219158649445,No,0.0,0.0,0.0
q498,What is an example of a health-related tweet?,"The health benefits of alcohol consumption are more limited than previously thought, researchers say","The provided text describes a study analyzing tweets about diabetes, diet, exercise, and obesity, but does not give an example of a tweet.  The answer is not found in the context.
",0.06451612903225806,0.14285714285714285,0.08888888888888889,0.0,0.0,0.0,0.06451612903225806,0.14285714285714285,0.08888888888888889,0.013113617852675475,0.24091492593288422,No,0.0,0.0,0.0
q499,What is the challenge for other language except English,not researched as much as English,"The context describes a challenge for Swiss students learning English, labeling spoken responses as ""accept"" or ""reject"".  It also mentions a proposed international challenge focused on non-native children's speech recognition, with an English subset.  No other language-specific challenges are described.
",0.046511627906976744,0.3333333333333333,0.08163265306122448,0.0,0.0,0.0,0.046511627906976744,0.3333333333333333,0.08163265306122448,0.008853994618255676,0.2582657039165497,No,0.0,0.0,0.0
q500,How many categories of offensive language were there?,3,"Three
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.85391765832901,Yes,1.0,1.0,1.0
q501,Which matching features do they employ?,Matching features from matching sentences from various perspectives.,"INLINEFORM6, INLINEFORM7, INLINEFORM8, INLINEFORM9, INLINEFORM10, Brown BIBREF32 clusters, Clark BIBREF33 clusters, and Word2vec BIBREF34 clusters.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.016466642419110007,0.12344538420438766,No,0.0,0.0,1.0
q502,By how much does their method outperform the multi-head attention model?,Their average improvement in Character Error Rate over the best MHA model was 0.33 percent points.,"The context does not state how much better the described method is than a multi-head attention model.  It only notes that multi-head attention *can* outperform other methods but does not give a specific performance difference.
",0.05405405405405406,0.11764705882352941,0.07407407407407408,0.0,0.0,0.0,0.05405405405405406,0.11764705882352941,0.07407407407407408,0.012413584734794654,0.3619937300682068,No,0.0,0.0,0.0
q503,How large is the corpus they use?,449050,"13 million NRLS records, 1 million and 2 million record subsets, 5 million articles from English Wikipedia.  Also, a 5 billion word corpus from Amazon.  Various other datasets of varying sizes are mentioned.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07321900129318237,No,0.0,1.0,1.0
q504,How many shared layers are in the system?,1,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.17235848307609558,No,0.0,0.0,0.0
q505,How many additional task-specific layers are introduced?,2 for the ADE dataset and 3 for the CoNLL04 dataset,"Two.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3074256181716919,No,0.0,0.0,0.0
q506,How many layers of self-attention does the model have?,"1, 4, 8, 16, 32, 64","10
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.23165102303028107,No,0.0,0.0,0.0
q507,what are the state of the art methods?,"S2VT, RGB (VGG), RGB (VGG)+Flow (AlexNet), LSTM-E (VGG), LSTM-E (C3D) and Yao et al.","DNN methods, MLP, Random Forest, Linear Support Vector, MEMM, CRF, SVM.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.015597579745575246,0.5246758460998535,No,0.0,0.0,0.0
q508,Which four languages do they experiment with?,"German, English, Italian, Chinese","Spanish, English, Italian, and Russian
",0.4,0.5,0.4444444444444445,0.25,0.3333333333333333,0.28571428571428575,0.4,0.5,0.4444444444444445,0.5169731539571708,0.8765342831611633,No,0.0,1.0,1.0
q509,Does DCA or GMM-based attention perform better in experiments?,About the same performance,"DCA performs better.
",0.3333333333333333,0.25,0.28571428571428575,0.0,0.0,0.0,0.3333333333333333,0.25,0.28571428571428575,0.0,0.5050856471061707,No,0.0,0.0,1.0
q510,What evaluation metric is used?,F1 and Weighted-F1,"Manual evaluation, automatic evaluation metrics (ROUGE, BLEU, METEOR), and task-based evaluation.
",0.08333333333333333,0.25,0.125,0.0,0.0,0.0,0.08333333333333333,0.25,0.125,0.021476912089159056,0.0907435193657875,No,0.0,0.0,0.0
q511,"Is any data-to-text generation model trained on this new corpus, what are the results?","Yes, Transformer based seq2seq is evaluated with average BLEU 0.519, METEOR 0.388, ROUGE 0.631 CIDEr 2.531 and SER 2.55%.","Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.004041065926152656,0.08463679999113083,No,0.0,0.0,0.0
q512,How are the potentially relevant text fragments identified?," Generate a query out of the claim and querying a search engine, rank the words by means of TF-IDF, use IBM's AlchemyAPI to identify named entities, generate queries of 5–10 tokens, which execute against a search engine, and collect the snippets and the URLs in the results, skipping any result that points to a domain that is considered unreliable.","Entities are extracted, contexts are generated for each, and a distance between question content and entity context is computed to assess relevance.
",0.3181818181818182,0.11290322580645161,0.16666666666666666,0.0,0.0,0.0,0.22727272727272727,0.08064516129032258,0.11904761904761903,0.00826383444100385,0.3920929431915283,No,0.0,0.0,0.0
q513,What dataset did they use?,"weibo-100k, Ontonotes, LCQMC and XNLI","A dataset of 3 billion (query, image, weight) triples, with 900 million unique images and 220 million unique queries.  A second dataset (SCI) of 220 sentences from the scientific literature.
",0.03333333333333333,0.16666666666666666,0.05555555555555555,0.0,0.0,0.0,0.03333333333333333,0.16666666666666666,0.05555555555555555,0.012413584734794654,0.11454785615205765,No,0.0,0.0,0.0
q514,What is the size of the dataset?,3029,"The context provides various dataset sizes, but no single answer to ""the size of the dataset"".
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07711362093687057,No,0.0,0.0,0.0
q515,What are the 12 AV approaches which are examined?,"MOCC, OCCAV, COAV, AVeer, GLAD, DistAV, Unmasking, Caravel, GenIM, ImpGI, SPATIUM and NNCD","The provided text states that 12 existing AV methods were applied, but does not list them.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.016268011973185222,0.38889092206954956,No,0.0,0.0,1.0
q516,how was annotation done?,Annotation was done with the help of annotators from Amazon Mechanical Turk on snippets of conversations,"Annotations were done by undergraduate students of computational linguistics, in single annotation mode, and with discussions with authors throughout the process.  A small sample was annotated by all four annotators to measure inter-annotator agreement.  Before annotation, annotators were guided through instructions and a verification step.
",0.15217391304347827,0.4375,0.22580645161290325,0.0,0.0,0.0,0.10869565217391304,0.3125,0.16129032258064516,0.011181458201080076,0.5102015137672424,No,0.0,0.0,0.0
q517,How do they measure correlation between the prediction and explanation quality?,They look at the performance accuracy of explanation and the prediction performance,"By investigating how prediction accuracy is affected by jointly training the network to produce explanations, and by evaluating how well the model learns to generate explanations.  They also performed these experiments for two types of explanations.
",0.25,0.75,0.375,0.05714285714285714,0.18181818181818182,0.08695652173913043,0.1388888888888889,0.4166666666666667,0.20833333333333334,0.015342333164810606,0.6404765844345093,Yes,0.0,0.0,0.0
q518,What datasets are used to evaluate the introduced method?,"They used a dataset from Taobao which contained a collection of conversation records between customers and customer service staffs. It contains over five kinds of conversations,
including chit-chat, product and discount consultation, querying delivery progress and after-sales feedback. ","Swissmetro, Oshiete-goo, and nfL6 datasets.
",0.3333333333333333,0.05,0.08695652173913045,0.0,0.0,0.0,0.16666666666666666,0.025,0.04347826086956522,0.0009831150070693118,0.24793866276741028,No,0.0,0.0,0.0
q519,What are the results achieved from the introduced method?,"Their model resulted in values of 0.476, 0.672 and 0.893 for recall at position 1,2 and 5 respectively in 10 candidates.","The method achieves significant improvement in ROUGE-L compared to the baseline, and outperforms the baseline on the development set in terms of ROUGE-L.  It also improves recognition performance with an ensemble effect.
",0.11764705882352941,0.16,0.13559322033898305,0.0,0.0,0.0,0.08823529411764706,0.12,0.10169491525423728,0.016526850525490154,0.27348485589027405,No,0.0,0.0,0.0
q520,How do they incorporate human advice?,by converting human advice to first-order logic format and use as an input to calculate gradient,"By consulting domain experts and using annotator feedback and agreement information.
",0.2727272727272727,0.17647058823529413,0.21428571428571427,0.1,0.0625,0.07692307692307693,0.2727272727272727,0.17647058823529413,0.21428571428571427,0.02426532257432171,0.10214252769947052,No,0.0,0.0,0.0
q521,What affective-based features are used?,"affective features provided by different emotion models such as Emolex, EmoSenticNet, Dictionary of Affect in Language, Affective Norms for English Words and Linguistics Inquiry and Word Count","DAL Activation, ANEW Dominance, Emolex Negative, Emolex Fear, LIWC Assent, LIWC Cause, LIWC Certain, LIWC Affect.
",0.125,0.07407407407407407,0.09302325581395349,0.0,0.0,0.0,0.125,0.07407407407407407,0.09302325581395349,0.018322242370845815,0.46294310688972473,No,1.0,1.0,1.0
q522,How big is performance improvement proposed methods are used?,"Data augmentation (es)  improved Adv es by 20% comparing to baseline 
Data augmentation (cs) improved Adv cs by 16.5% comparing to baseline
Data augmentation (cs+es) improved both Adv cs and Adv es by at least 10% comparing to baseline 
All models show improvements over adversarial sets  
","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.014365974813699722,No,0.0,0.0,0.0
q523,By how much does transfer learning improve performance on this task?,"In task 1 best transfer learning strategy improves F1 score by 4.4% and accuracy score by 3.3%, in task 2 best transfer learning strategy improves F1 score by 2.9% and accuracy score by 1.7%","An average improvement of F$_1$=75% is achieved with transfer learning, compared to in-domain results.  The best result is F$_1$=76%.
",0.2916666666666667,0.18421052631578946,0.22580645161290325,0.043478260869565216,0.02702702702702703,0.03333333333333333,0.25,0.15789473684210525,0.1935483870967742,0.02580595620611912,0.7040906548500061,No,0.0,0.0,0.0
q524,What baseline is used?,SVM,"Baseline 1, Baseline 2, Baseline 3, ANRES_Sing_R, ANRES_Sing_Z, ANRES_Seq_R, ANRES_Seq_Z, LSTM baseline.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1591678261756897,No,0.0,0.0,0.0
q525,What topic clusters are identified by LDA?,"Clusters of Twitter user ids from accounts of American or German political actors, musicians, media websites or sports club","Substance use, suicidality, and anxiety disorders.  Also, topics in a causal corpus tend to fall into three main categories, though specifics aren't given.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.015301683686839006,0.12578941881656647,No,0.0,0.0,0.0
q526,How much do they outperform previous state-of-the-art?,"On subtask 3 best proposed model has F1 score of 92.18 compared to best previous F1 score of 88.58.
On subtask 4 best proposed model has 85.9, 89.9 and 95.6 compared to best previous results of 82.9, 84.0 and 89.9 on 4-way, 3-way and binary aspect polarity.","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0002572333451099464,0.09633146971464157,No,0.0,0.0,0.0
q527,How big is the provided treebank?,"1448 sentences more than the dataset from Bhat et al., 2017","The answer is not found in the context.  While sizes of various treebanks are mentioned, there is no single answer to the overall question.
",0.041666666666666664,0.09090909090909091,0.05714285714285715,0.0,0.0,0.0,0.041666666666666664,0.09090909090909091,0.05714285714285715,0.018264728862681344,0.24025043845176697,No,0.0,0.0,0.0
q528,what dataset was used?,The dataset from a joint ADAPT-Microsoft project,"Multiple datasets were used:  Basic Question Dataset (BQD), SCI dataset, and a subset of the development set questions from a Gold Standard Dataset.  Also, a dataset of 3 billion (query, image, weight) triples from the MS COCO dataset.
",0.10526315789473684,0.5,0.17391304347826086,0.02702702702702703,0.14285714285714285,0.045454545454545456,0.07894736842105263,0.375,0.13043478260869565,0.0161391821076343,0.32010698318481445,No,0.0,0.0,1.0
q529,What are the citation intent labels in the datasets?,"Background, extends, uses, motivation, compare/contrast, and future work for the ACL-ARC dataset. Background, method, result comparison for the SciCite dataset.","Background, Method, ResultComparison.  There are also six intent categories in a different, larger dataset.
",0.21428571428571427,0.13636363636363635,0.16666666666666663,0.07692307692307693,0.047619047619047616,0.058823529411764705,0.21428571428571427,0.13636363636363635,0.16666666666666663,0.03502027143757748,0.5143993496894836,No,0.0,1.0,1.0
q530,How is quality of annotation measured?,Annotators went through various phases to make sure their annotations did not deviate from the mean.,"By comparing annotations to gold-standard labels, measuring inter-annotator agreement, and assessing accuracy with AMT annotations.
",0.17647058823529413,0.1875,0.1818181818181818,0.0,0.0,0.0,0.17647058823529413,0.1875,0.1818181818181818,0.028265205879007452,0.43087294697761536,No,0.0,0.0,0.0
q531,What accuracy score do they obtain?,the best performing model obtained an accuracy of 0.86,"Team 9 achieved a joint goal accuracy of 86.53%.
",0.3,0.3,0.3,0.1111111111111111,0.1111111111111111,0.1111111111111111,0.3,0.3,0.3,0.07495553473355841,0.4572869539260864,No,0.0,0.0,0.0
q532,What is the 12 class bilingual text?,"Appreciation, Satisfied, Peripheral complaint, Demanded inquiry, Corruption, Lagged response, Unresponsive, Medicine payment, Adverse behavior, Grievance ascribed and Obnoxious/irrelevant","The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13851246237754822,No,0.0,0.0,0.0
q533,Which are the sequence model architectures this method can be transferred across?,The sequence model architectures which this method is transferred to are: LSTM and Transformer-based models,"LSTM seq2seq, Convolutional seq2seq, and transformer.
",0.5,0.1875,0.2727272727272727,0.2,0.06666666666666667,0.1,0.5,0.1875,0.2727272727272727,0.02604848453271721,0.4212134778499603,No,0.0,1.0,1.0
q534, What percentage of improvement in inference speed is obtained by the proposed method over the newest state-of-the-art methods?,"Across 4 datasets, the best performing proposed model (CNN) achieved an average of 363% improvement over the state of the art method (LR-CNN)","The answer is not found in the context.  While the text discusses inference speed improvements over specific prior methods (Lattice-LSTM and LR-CNN), it does not quantify improvement over the *newest* state-of-the-art methods.
",0.2972972972972973,0.4583333333333333,0.36065573770491804,0.19444444444444445,0.30434782608695654,0.23728813559322037,0.2702702702702703,0.4166666666666667,0.3278688524590164,0.05113818794967396,0.5558744072914124,No,0.0,0.0,0.0
q535,What is the metric that is measures in this paper?,error rate in a minimal pair ABX discrimination task,"Premise-Hypothesis Distance, METEOR, tIoU, BIBREF16, WMD, VSM, PV-DTW.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15665167570114136,No,0.0,0.0,0.0
q536,Was evaluation metrics and criteria were used to evaluate the output of the cascaded multimodal speech translation?,BLEU scores,"Automatic metrics, subjective human evaluation (with a focus on short translations and a head-to-head ranking of the first 10 sentences of each test talk), and document-level evaluation were used.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1998845338821411,No,0.0,0.0,0.0
q537,What are the two decoding functions?,"a linear projection and a bijective function with continuous transformation though  ‘affine coupling layer’ of (Dinh et al.,2016). ","Training decoding function:  $f_{\text{de}}() = h(+) $
Testing decoding function: $f_\text{de}^{-1}() = h^{\top}(h^{-1}() - )$
",0.05555555555555555,0.05263157894736842,0.05405405405405406,0.0,0.0,0.0,0.05555555555555555,0.05263157894736842,0.05405405405405406,0.008237054296530683,0.006148049607872963,No,0.0,0.0,0.0
q538,What are the domains covered in the dataset?,"Alarm
Bank
Bus
Calendar
Event
Flight
Home
Hotel
Media
Movie
Music
RentalCar
Restaurant
RideShare
Service
Travel
Weather","17 domains (Alarm domain not included in training), 20 domains across train, dev and test datasets.
",0.0625,0.058823529411764705,0.06060606060606061,0.0,0.0,0.0,0.0625,0.058823529411764705,0.06060606060606061,0.018160849415439308,0.11450480669736862,No,0.0,1.0,1.0
q539,How are the two different models trained?,They pre-train the models using 600000 articles as an unsupervised dataset and then fine-tune the models on small training set.,"One model is trained first for coarse-grained features, freezing the parameters of the other.  Then, the other model is unfrozen, and the entire model is trained at a low learning rate for fine-grained features.  Another method involves fine-tuning the BERT model with a new prediction layer first, using a low learning rate.  Both models use Stochastic Gradient Descent with ADAM optimization and a learning rate of 1e-4, along with dropout regularization.
",0.14666666666666667,0.5,0.22680412371134018,0.04054054054054054,0.14285714285714285,0.06315789473684211,0.10666666666666667,0.36363636363636365,0.16494845360824745,0.007049644510626181,0.3140462636947632,No,0.0,0.0,0.0
q540,How long is the dataset?,"645, 600000","Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07716593146324158,No,0.0,0.0,0.0
q541,How big are negative effects of proposed techniques on high-resource tasks?,The negative effects were insignificant.,"The context states performance was comparable on high-resource tasks.  No negative effects are mentioned.
",0.2,0.6,0.3,0.07142857142857142,0.25,0.11111111111111112,0.2,0.6,0.3,0.058166354211475146,0.532875120639801,Yes,0.0,0.0,0.0
q542,"Are this techniques used in training multilingual models, on what languages?",English to French and English to German,"English, Russian, Arabic, Chinese, German, Spanish, French, and Asturian (using Spanish as a source).
",0.2857142857142857,0.5714285714285714,0.38095238095238093,0.07692307692307693,0.16666666666666666,0.10526315789473684,0.21428571428571427,0.42857142857142855,0.2857142857142857,0.02224972978585352,0.4721991717815399,No,0.0,0.0,0.0
q543,What metric is used to measure performance?,"Accuracy and F1 score for supervised tasks, Pearson's and Spearman's correlation for unsupervised tasks","BIBREF16, accuracy, precision, recall, F1-score.
",0.5,0.1875,0.2727272727272727,0.2,0.06666666666666667,0.1,0.5,0.1875,0.2727272727272727,0.03471023784089876,0.3938830494880676,No,0.0,0.0,0.0
q544,How do Zipf and Herdan-Heap's laws differ?,"Zipf's law describes change of word frequency rate, while Heaps-Herdan describes different word number in large texts (assumed that Hepas-Herdan is consequence of Zipf's)","The provided text states that Heaps-Herdan law and Zipf's law are consequences of Zipf's law and describe the frequency of words, but not their order.  Therefore, the difference is not explicitly described.
",0.5142857142857142,0.6428571428571429,0.5714285714285714,0.20588235294117646,0.25925925925925924,0.22950819672131148,0.22857142857142856,0.2857142857142857,0.25396825396825395,0.03646650581707163,0.8447138071060181,No,0.0,0.0,1.0
q545,How are the synthetic examples generated?,"Random perturbation of Wikipedia sentences using mask-filling with BERT, backtranslation and randomly drop out","The generative model is trained on the original training dataset.  Then, the premise and label from an example are used as input to generate a new hypothesis.  This hypothesis is combined with the premise and label to create a new unseen example.  This process is repeated for every example in the original dataset.  Synthetic data is also generated using simple rules like dropping non-initial vowels and replacing consonants.
",0.057971014492753624,0.26666666666666666,0.09523809523809525,0.0,0.0,0.0,0.057971014492753624,0.26666666666666666,0.09523809523809525,0.006897371974693177,0.21568603813648224,No,0.0,0.0,0.0
q546,By how much does the new parser outperform the current state-of-the-art?,Proposed method achieves 94.5 UAS and 92.4 LAS  compared to 94.3 and 92.2 of best state-of-the -art greedy based parser. Best state-of-the art parser overall achieves 95.8 UAS and 94.6 LAS.,"The context states the new parser is ""competitive with the state-of-the-art on PTB"" and ""gives better or comparable results"" to state-of-the-art parsers in transition and graph types, but does not quantify the difference.
",0.358974358974359,0.34146341463414637,0.35000000000000003,0.18421052631578946,0.175,0.1794871794871795,0.2564102564102564,0.24390243902439024,0.25,0.014104555382959006,0.546674907207489,No,0.0,0.0,0.0
q547,What experimental evaluation is used?,root mean square error between the actual and the predicted price of Bitcoin for every minute,"Cross-domain evaluation, human evaluation with criteria of relevance, attraction, fluency, and style strength, and automatic evaluation.
",0.11764705882352941,0.125,0.12121212121212122,0.0,0.0,0.0,0.058823529411764705,0.0625,0.06060606060606061,0.020540268312306344,-0.07488540560007095,No,0.0,0.0,0.0
q548,How is the architecture fault-tolerant?,By using Apache Spark which stores all executions in a lineage graph and recovers to the previous steady state from any fault,"The architecture is fault-tolerant because it is Spark-based, which acts as an in-memory data store and allows for persistent and real-time handling of large volumes of incoming data.
",0.21875,0.3181818181818182,0.25925925925925924,0.0,0.0,0.0,0.125,0.18181818181818182,0.14814814814814814,0.014763939041893883,0.4566631317138672,No,0.0,1.0,1.0
q549,Which elements of the platform are modular?,"handling large volume incoming data, sentiment analysis on tweets and predictive online learning","Question answering platform, bitcoin price prediction platform, and dialog system architecture.
",0.18181818181818182,0.15384615384615385,0.16666666666666669,0.0,0.0,0.0,0.09090909090909091,0.07692307692307693,0.08333333333333334,0.03377156414337854,0.2647518515586853,No,0.0,0.0,0.0
q550,Could you tell me more about the metrics used for performance evaluation?,"BLUE utilizes different metrics for each of the tasks: Pearson correlation coefficient, F-1 scores, micro-averaging, and accuracy","The context describes various evaluation metrics, including: recall rate, F1 score, overall accuracy, task completion ratio, user satisfaction degree (with scores ranging from -2 to 2), response fluency (with scores of -1, 0, 1), and number of dialogue turns.  Performance of automated metrics was evaluated by success rate in selecting the best hyperparameter combination or early-stopping checkpoint, and average human-annotated score.
",0.14285714285714285,0.47368421052631576,0.21951219512195122,0.0,0.0,0.0,0.06349206349206349,0.21052631578947367,0.0975609756097561,0.013823323043649121,0.4723755419254303,No,0.0,0.0,0.0
q551,What are the tasks that this method has shown improvements?,"bilingual dictionary induction, monolingual and cross-lingual word similarity, and cross-lingual hypernym discovery","The context does not specify the tasks the method improves upon.  It only mentions evaluation using cross-domain scenarios for establishing general effectiveness.
",0.043478260869565216,0.07142857142857142,0.05405405405405405,0.0,0.0,0.0,0.043478260869565216,0.07142857142857142,0.05405405405405405,0.0,0.15382599830627441,No,0.0,0.0,0.0
q552,Why does the model improve in monolingual spaces as well? ,because word pair similarity increases if the two words translate to similar parts of the cross-lingual embedding space,"The context does not explain why the model improves in monolingual spaces.
",0.25,0.15789473684210525,0.1935483870967742,0.0,0.0,0.0,0.25,0.15789473684210525,0.1935483870967742,0.021027582541760535,0.5134255290031433,No,0.0,0.0,0.0
q553,How is annotation projection done when languages have different word order?,"Word alignments are generated for parallel text, and aligned words are assumed to also share AMR node alignments.","The context states that the approach projects target-language word embeddings to the source-language embedding space preserving word order.  This is expected to work better for languages with similar word order to the source language (English, SVO).
",0.10526315789473684,0.2222222222222222,0.14285714285714285,0.0,0.0,0.0,0.10526315789473684,0.2222222222222222,0.14285714285714285,0.012663302792223932,0.3764989674091339,No,0.0,0.0,0.0
q554,What's the precision of the system?,"0.8320 on semantic typing, 0.7194 on entity matching","Precision scores in ranges between 0.92 and 0.97 are possible.
",0.16666666666666666,0.2,0.1818181818181818,0.0,0.0,0.0,0.16666666666666666,0.2,0.1818181818181818,0.0,0.20936790108680725,No,0.0,0.0,0.0
q555,Which of the two ensembles yields the best performance?,Answer with content missing: (Table 2) CONCAT ensemble,"The ensemble model using a sum of probabilities at each position yields the best performance for ROUGE-L and BLEU-1.
",0.047619047619047616,0.125,0.06896551724137931,0.0,0.0,0.0,0.047619047619047616,0.125,0.06896551724137931,0.019146030690102515,0.14209873974323273,No,0.0,0.0,0.0
q556,What is the new initialization method proposed in this paper?,They initialize their word and entity embeddings with vectors pre-trained over a large corpus of unlabeled data.,"Multichannel initialization.
",0.5,0.05555555555555555,0.09999999999999999,0.0,0.0,0.0,0.5,0.05555555555555555,0.09999999999999999,0.0,0.14951671659946442,No,0.0,0.0,0.0
q557,How was a quality control performed so that the text is noisy but the annotations are accurate?,The authors believe that the Wikilinks corpus  contains ground truth annotations while being noisy. They discard mentions that cannot have ground-truth verified by comparison with Wikipedia.,"A small sample of texts was annotated by all four annotators, and inter-annotator agreement was measured.  For each annotator and each question, the probability of agreement with the majority response was computed. Annotators whose probability was more than two standard deviations away from the mean were discarded.
",0.125,0.2222222222222222,0.16,0.0,0.0,0.0,0.08333333333333333,0.14814814814814814,0.10666666666666667,0.010471589442104284,0.33358636498451233,No,0.0,0.0,0.0
q558,Is it a neural model? How is it trained?,"No, it is a probabilistic model trained by finding feature weights through gradient ascent","Yes.  Training details vary depending on the specific model (e.g., neural regressor, neural classifier), but methods include using classification labels, cross-entropy loss, and training on data.
",0.07142857142857142,0.14285714285714285,0.09523809523809523,0.0,0.0,0.0,0.07142857142857142,0.14285714285714285,0.09523809523809523,0.011524538390587938,0.5416192412376404,No,0.0,0.0,0.0
q559,How they evaluate quality of generated output?,Through human evaluation where they are asked to evaluate the generated output on a likert scale.,"Through summarization quality (BLEU, METEOR, ROUGE) and language fluency.  Human evaluation of style strength is also used.
",0.17647058823529413,0.1875,0.1818181818181818,0.0625,0.06666666666666667,0.06451612903225808,0.17647058823529413,0.1875,0.1818181818181818,0.021671320168371845,0.6077995896339417,No,0.0,0.0,1.0
q560,What are the four forums the data comes from?,"Darkode,  Hack Forums, Blackhat and Nulled.","BIBREF, Reddit, Twitter, Facebook
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05868924818816534,0.3413001298904419,No,0.0,0.0,0.0
q561,How are sentence embeddings incorporated into the speech recognition system?,BERT generates sentence embeddings that represent words in context. These sentence embeddings are merged into a single conversational-context vector that is used to calculate a gated embedding and is later combined with the output of the decoder h to provide the gated activations for the next hidden layer.,"The provided text describes using pre-trained embeddings (like fastText and BERT) for conversational context, but does not detail how sentence embeddings are incorporated into the *speech recognition* system.  The text focuses on using these embeddings for *other* NLP tasks (e.g. domain classification and sentiment classification).
",0.3617021276595745,0.3469387755102041,0.35416666666666663,0.06521739130434782,0.0625,0.06382978723404255,0.19148936170212766,0.1836734693877551,0.18749999999999997,0.032374986018987986,0.6596069931983948,No,0.0,0.0,0.0
q562,How different is the dataset size of source and target?,the training dataset is large while the target dataset is usually much smaller,"The source dataset is significantly larger than the target dataset.  For example, 25% of MovieQA (2462 examples) is still larger than MC160 (280 examples).
",0.2916666666666667,0.5384615384615384,0.3783783783783784,0.13043478260869565,0.25,0.1714285714285714,0.2916666666666667,0.5384615384615384,0.3783783783783784,0.05682676011595584,0.5614973902702332,Yes,0.0,0.0,0.0
q563,What type of documents are supported by the annotation platform?,"Variety of formats supported (PDF, Word...), user can define content elements of document","PDFs and Microsoft Word
",0.5,0.15384615384615385,0.23529411764705882,0.0,0.0,0.0,0.5,0.15384615384615385,0.23529411764705882,0.002925662791955391,0.5114766955375671,Yes,1.0,1.0,1.0
q564,What are the strong baselines you have?,optimize single task with no synthetic data,"The context does not specify which baselines are considered ""strong.""  It only describes different baselines and their performance.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01548093743622303,No,0.0,0.0,0.0
q565,What are causal attribution networks?,"networks where nodes represent causes and effects, and directed edges represent cause-effect relationships proposed by humans","A causal attribution dataset is a collection of text pairs reflecting cause-effect relationships proposed by humans.  These networks map out the underlying or latent causal attribution network held collectively by humans.  Nodes are defined by written descriptions, and edges represent cause-effect relationships.
",0.29545454545454547,0.7647058823529411,0.42622950819672134,0.16279069767441862,0.4375,0.23728813559322032,0.18181818181818182,0.47058823529411764,0.26229508196721313,0.1279600659916637,0.5146392583847046,Yes,1.0,1.0,1.0
q566,how did they ask if a tweet was racist?,"if it includes  negative utterances, negative generalizations and insults concerning ethnicity, nationality, religion and culture.","Participants were asked to answer ""yes"" or ""no"" if they considered the tweet hate speech, either based on a definition or their personal opinion.
",0.041666666666666664,0.06666666666666667,0.05128205128205127,0.0,0.0,0.0,0.041666666666666664,0.06666666666666667,0.05128205128205127,0.016338984401221654,0.5392876267433167,No,0.0,0.0,0.0
q567,How does the model compute the likelihood of executing to the correction semantic denotation?,By treating logical forms as a latent variable and training a discriminative log-linear model over logical form y given x.,"The parser is trained to maximize the total log likelihood of consistent logical forms, where consistent logical forms are those that yield the correct denotation when executed.
",0.2222222222222222,0.2857142857142857,0.25,0.07692307692307693,0.1,0.08695652173913043,0.14814814814814814,0.19047619047619047,0.16666666666666666,0.030603367373678015,0.45721888542175293,Yes,0.0,0.0,0.0
q568,What are state of the art methods authors compare their work with? ,"ISOT dataset: LLVM
Liar dataset: Hybrid CNN and LSTM with attention","GenIM, ImpGI, Unmasking, Caravel, and SPATIUM.
",0.16666666666666666,0.09090909090909091,0.11764705882352942,0.0,0.0,0.0,0.16666666666666666,0.09090909090909091,0.11764705882352942,0.03124719790499494,0.11785778403282166,No,0.0,0.0,0.0
q569,How much improvement do they get?,Their GTRS approach got an improvement of 3.89% compared to SVM and 27.91% compared to Pawlak.,"The improvement varies.  2% for using Doc2Vec instead of TF-IDF, 2.6 points for CNN2-DE (global) model with RST features, about 1 point for discourse-enhanced SVM2-PVs.  Other improvements are mentioned but not quantified.
",0.05405405405405406,0.1111111111111111,0.07272727272727272,0.0,0.0,0.0,0.05405405405405406,0.1111111111111111,0.07272727272727272,0.01176877973200725,0.5320038199424744,No,0.0,0.0,0.0
q570,Which languages do they test on?,"Answer with content missing: (Applications section) We use Wikipedia articles
in five languages
(Kinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English) as well as the Na dataset of Adams
et al. (2017).
Select:
Kinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English","Dutch, Spanish, English, German, Basque, French, Italian, German, Finnish, Hindi, Portuguese, Japanese, Thai, Vietnamese, Arabic, Russian.
",0.0625,0.022222222222222223,0.03278688524590164,0.0,0.0,0.0,0.0625,0.022222222222222223,0.03278688524590164,0.00782932814948146,0.5913925766944885,No,0.0,0.0,1.0
q571,What limitations are mentioned?,"deciding publisher partisanship, risk annotator bias because of short description text provided to annotators","The context mentions limitations in the assumption of uniformly or geometrically distributed sequences in training sets, and the narrow and bounded evaluation schemes that often define a length threshold in the test set.
",0.030303030303030304,0.07142857142857142,0.0425531914893617,0.0,0.0,0.0,0.030303030303030304,0.07142857142857142,0.0425531914893617,0.012557690800697192,0.12556764483451843,No,0.0,0.0,0.0
q572,What are the baselines?,"CNN, LSTM, BERT","Baseline 1: smooth L1 distance; Baseline 2: PCA-based acoustic similarity; Baseline 3: Nonlinear dynamical systems complexity; ANRES_Sing_R; ANRES_Sing_Z; ANRES_Seq_R; ANRES_Seq_Z.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1178852766752243,No,0.0,0.0,0.0
q573,What semantic features help in detecting whether a piece of text is genuine or generated? of ,"No feature is given, only discussion that semantic features are use in practice and yet to be discovered how to embed that knowledge into statistical decision theory framework.","The context discusses semantic relevance as an important criterion for text-to-image synthesis, but not for detecting genuine vs. generated text.  No semantic features for authorship verification are mentioned.
",0.2,0.21428571428571427,0.20689655172413796,0.034482758620689655,0.037037037037037035,0.03571428571428571,0.13333333333333333,0.14285714285714285,0.1379310344827586,0.03053216387177772,0.4132755398750305,No,0.0,0.0,0.0
q574,Is the assumption that natural language is stationary and ergodic valid?,It is not completely valid for natural languages because of diversity of language - this is called smoothing requirement.,"Manning and Schütze argue that language can be modeled as stationary and ergodic, and the text follows this assumption.
",0.1,0.1111111111111111,0.10526315789473685,0.0,0.0,0.0,0.1,0.1111111111111111,0.10526315789473685,0.023901021968803136,0.2760828733444214,No,1.0,1.0,1.0
q575,Which models do they try out?,"DocQA, SAN, QANet, ASReader, LM, Random Guess","Support Vector Machines (SVMs), logistic regression, Naïve Bayes, Perceptron, and decision trees.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.028629993657668874,0.23473964631557465,No,0.0,0.0,0.0
q576,What are the competing models?,"TEACHER FORCING (TF), SCHEDULED SAMPLING (SS),  SEQGAN, RANKGAN, LEAKGAN.","BiMPM, ESIM, Decomposable Attention Model, KIM, BERT, Latent-Tree, Berkeley, LaSO, Latent-Strc, Model-Stack, Non-Linear, Random Forest (with reduced features and full vocabulary), and an aggregated forecaster (Agg).
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02482548444638528,0.49984604120254517,No,0.0,0.0,0.0
q577,How is the input triple translated to a slot-filling task?,"The relation R(x,y) is mapped onto a question q whose answer is y","The context describes slot filling as a sequence labeling problem, where a parameterized mapping function learns to map input words to slot tags.
",0.13043478260869565,0.2,0.15789473684210528,0.0,0.0,0.0,0.08695652173913043,0.13333333333333333,0.10526315789473685,0.01982256626710344,0.22084520757198334,No,0.0,0.0,0.0
q578,How is module that analyzes behavioral state trained?,pre-trained to identify the presence of behavior from a sequence of word using the Couples Therapy Corpus,"The behavior model is pre-trained to identify the presence of each behavior from a sequence of words using a multi-label classification scheme.  The pre-trained portion of the behavior model was implemented using a single layer RNN with LSTM units with dimension size 50.
",0.32608695652173914,0.8333333333333334,0.46875000000000006,0.26666666666666666,0.7058823529411765,0.3870967741935483,0.32608695652173914,0.8333333333333334,0.46875000000000006,0.1839576031944879,0.5710290670394897,Yes,0.0,1.0,1.0
q579,"Can the model add new relations to the knowledge graph, or just new entities?",The model does not add new relations to the knowledge graph.,"The context discusses adding new triples, implying both entities and relations can be added.
",0.21428571428571427,0.2727272727272727,0.23999999999999996,0.0,0.0,0.0,0.21428571428571427,0.2727272727272727,0.23999999999999996,0.03458592114102736,0.5019084811210632,No,0.0,0.0,0.0
q580,Do twitter users tend to tweet about the DOS attack when it occurs? How much data supports this assumption?,The dataset contains about 590 tweets about DDos attacks.,"Yes, the context states that legitimate users are willing to reveal information about service outages on social media platforms, including Twitter, and that user feedback can indicate the severity of service outages.  The authors collected 35,214 tweets related to DDoS attacks on banks.
",0.11363636363636363,0.5555555555555556,0.18867924528301888,0.023255813953488372,0.125,0.03921568627450981,0.11363636363636363,0.5555555555555556,0.18867924528301888,0.01137287189899225,0.606760561466217,Yes,0.0,0.0,0.0
q581,What is the training and test data used?,"Tweets related to a Bank of America DDos attack were used as training data. The test datasets contain tweets related to attacks to Bank of America, PNC and Wells Fargo.","The training data consists of all previous rounds of data, the Wikipedia Toxic Comments data, and the newly collected adversarial data from the ""break it"" round.  Two test sets (Test Set 1 and 2) of drug labels, with gold standard annotations, are used.  A further external dataset (NLM-180 BIBREF5) is also utilized for training.  A separate dataset with labels for age, dialect, and gender is used for training.
",0.14492753623188406,0.3333333333333333,0.20202020202020202,0.029411764705882353,0.06896551724137931,0.041237113402061855,0.08695652173913043,0.2,0.12121212121212122,0.013126882931262702,0.4804181456565857,No,0.0,0.0,0.0
q582,What writing styles are present in the corpus?,"current news, historical news, free time, sports, juridical news pieces, personal adverts, editorials.","Humor, newswire, letters, journal, non-fiction, MSA (politics, economy, sports, society), classical Arabic (religious texts).
",0.06666666666666667,0.07692307692307693,0.07142857142857144,0.0,0.0,0.0,0.06666666666666667,0.07692307692307693,0.07142857142857144,0.06352047085618948,0.6214625835418701,No,0.0,0.0,0.0
q583,What meta-information is being transferred?,"high-order representation of a relation, loss gradient of relation meta","Relation-specific meta information, including relation meta and gradient meta.
",0.4,0.36363636363636365,0.380952380952381,0.1111111111111111,0.1,0.10526315789473685,0.3,0.2727272727272727,0.28571428571428564,0.08913765521398126,0.5377854108810425,Yes,1.0,1.0,1.0
q584,What datasets are used to evaluate the approach?,"NELL-One, Wiki-One","Swissmetro dataset.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.006415737792849541,No,0.0,0.0,0.0
q585,How much is performance hurt when using too small amount of layers in encoder?,"comparing to the results from reducing the number of layers in the decoder, the BLEU score was 69.93 which is less than 1% in case of test2016 and in case of test2017 it was less by 0.2 %. In terms of TER it had higher score by 0.7 in case of test2016 and 0.1 in case of test2017. ","Reducing the $enc_{src \rightarrow mt}$ encoder block's depth reduces performance for all four scores.
",0.1875,0.04918032786885246,0.07792207792207792,0.06666666666666667,0.016666666666666666,0.02666666666666667,0.1875,0.04918032786885246,0.07792207792207792,0.003334108906292714,0.5094732046127319,No,0.0,0.0,0.0
q586,What neural machine translation models can learn in terms of transfer learning?,Multilingual Neural Machine Translation Models,"Neural machine translation models can learn to build a statistical model that aims to find the most probable target sequence for an input sequence by learning from a corpus of examples.  They can also optimize parameter sharing to maximize model capacity, potentially learning linguistic properties of the languages involved.
",0.08163265306122448,0.8,0.1481481481481481,0.0625,0.75,0.11538461538461539,0.08163265306122448,0.8,0.1481481481481481,0.0070027739171006995,0.6439894437789917,Yes,0.0,0.0,0.0
q587,How does the semi-automatic construction process work?,Automatic transcription of 5000 tokens through sequential neural models trained on the annotated part of the corpus,"Researchers use crowdworkers to create sentiment and semantic role-labeling systems, then construct examples to find their faults.  Models are retrained to improve them, and the build, break, and fix sequence is repeated multiple times.
",0.05714285714285714,0.11764705882352941,0.07692307692307693,0.0,0.0,0.0,0.05714285714285714,0.11764705882352941,0.07692307692307693,0.00918708060432945,0.24082441627979279,No,0.0,0.0,0.0
q588,"What does ""explicitly leverages their probabilistic correlation to guide the training process of both models"" mean?","The framework jointly learns parametrized QA and QG models subject to the constraint in equation 2. In more detail, they minimize QA and QG loss functions, with a third dual loss for regularization.","The framework uses the probabilistic relationship between Question Answering (QA) and Question Generation (QG) as a regularization term during training.
",0.4,0.24242424242424243,0.30188679245283023,0.10526315789473684,0.0625,0.0784313725490196,0.4,0.24242424242424243,0.30188679245283023,0.027148843716721965,0.5649956464767456,Yes,0.0,0.0,0.0
q589,What is WNGT 2019 shared task?,efficiency task aimed  at reducing the number of parameters while minimizing drop in performance,"A shared task focused on reducing model size in translation performance.
",0.36363636363636365,0.2857142857142857,0.32,0.0,0.0,0.0,0.36363636363636365,0.2857142857142857,0.32,0.04053997537205932,0.38746607303619385,Yes,1.0,1.0,1.0
q590,Was any variation in results observed based on language typology?,It is observed some variability - but not significant. Bert does not seem to gain much more syntax information than with type level information.,"The context mentions different results for different languages, but does not explicitly link these variations to language typology.  The answer is not found in the context.
",0.23076923076923078,0.2608695652173913,0.24489795918367346,0.04,0.045454545454545456,0.0425531914893617,0.15384615384615385,0.17391304347826086,0.16326530612244897,0.03639374222382003,0.4331664443016052,No,0.0,0.0,0.0
q591,Can the approach be generalized to other technical domains as well? ,"There is no reason to think that this approach wouldn't also be successful for other technical domains. Technical terms are replaced with tokens, therefore so as long as there is a corresponding process for identifying and replacing technical terms in the new domain this approach could be viable.","The context states that models should generalize well for all domains, implying the approach *can* be generalized to other technical domains.
",0.47619047619047616,0.20408163265306123,0.2857142857142857,0.1,0.041666666666666664,0.058823529411764705,0.2857142857142857,0.12244897959183673,0.1714285714285714,0.04291847489066379,0.43048539757728577,Yes,0.0,1.0,1.0
q592,What dataset they use for evaluation?,The same 2K set from Gigaword used in BIBREF7,"16 Amazon review datasets, Stanford Sentiment TreeBank (SST-5/SST-2), and the IMDb Sentiment dataset.  Also, TREC question data.
",0.05,0.1111111111111111,0.06896551724137932,0.0,0.0,0.0,0.05,0.1111111111111111,0.06896551724137932,0.0,0.11940588802099228,No,0.0,0.0,0.0
q593,Which regions of the United States do they consider?,all regions except those that are colored black,"The Northeast, Southeast, Midwest, and West Coast.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5081115961074829,No,0.0,0.0,0.0
q594,How is performance measured?,they use ROC curves and cross-validation,"Performance is measured by the number of times a metric selects the best hyperparameter combination or checkpoint (out of 10 trials), and the average human-annotated score for selected models.  Speech recognition performance is evaluated by analyzing successful and unsuccessful transactions in logs.  Classifier performance varies depending on the competition (e.g., SemEval, TASS, SENTIPOLC) using metrics like accuracy or custom metrics.
",0.03225806451612903,0.2857142857142857,0.05797101449275363,0.0,0.0,0.0,0.016129032258064516,0.14285714285714285,0.028985507246376815,0.004877178495671594,0.3385569453239441,No,0.0,0.0,0.0
q595,What is novel in author's approach?,"They use self-play learning , optimize the model for specific metrics, train separate models per user, use model  and response classification predictors, and filter the dataset to obtain higher quality training data.","The authors propose an AA approach based on time-aware language models that incorporate the temporal changes of the writing style of authors.  Their experiments focus on recognizing authorial style despite large time spans between documents.
",0.08333333333333333,0.09375,0.08823529411764706,0.0,0.0,0.0,0.08333333333333333,0.09375,0.08823529411764706,0.014091499441909466,0.2486729770898819,No,0.0,0.0,0.0
q596,How large is the Dialog State Tracking Dataset?,"1,618 training dialogs, 500 validation dialogs, and 1,117 test dialogs","Over 16,000 dialogues.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4823288917541504,No,0.0,1.0,1.0
q597,What dataset is used for train/test of this method?,Training datasets: TTS System dataset and embedding selection dataset. Evaluation datasets: Common Prosody Errors dataset and LFR dataset.,"The provided text describes using multiple datasets:  Wikipedia Toxic Comments data,  ""break it"" round data (multiple rounds), Training-22, NLM-180, Test Set 1 and Test Set 2.
",0.10714285714285714,0.16666666666666666,0.13043478260869565,0.0,0.0,0.0,0.07142857142857142,0.1111111111111111,0.08695652173913043,0.02438662222153295,0.4903709590435028,No,0.0,0.0,0.0
q598,How much is the gap between using the proposed objective and using only cross-entropy objective?,The mixed objective improves EM by 2.5% and F1 by 2.2%,"The context states that the proposed mixed objective converges quickly and outperforms the purely cross-entropy model.  However, it does not quantify the gap.
",0.16666666666666666,0.3076923076923077,0.21621621621621623,0.043478260869565216,0.08333333333333333,0.057142857142857134,0.16666666666666666,0.3076923076923077,0.21621621621621623,0.03435812002459847,0.48038455843925476,No,0.0,0.0,0.0
q599,How many domains of ontologies do they gather data from?,"5 domains: software, stuff, african wildlife, healthcare, datatypes","17
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07417096197605133,No,0.0,1.0,1.0
q600,what is the practical application for this paper?,Improve existing NLP methods. Improve linguistic analysis. Measure impact of word normalization tools.,"The practical application includes testing generalization more rigorously, better performance on a different meaning representation language, solving representative problems from a synthetic dataset, helping with low-resource language modeling problems, and improving confidence measures in related applications like information retrieval and speaker adaptation.  Also,  the paper addresses biomedical text analysis.
",0.06,0.23076923076923078,0.09523809523809523,0.0,0.0,0.0,0.04,0.15384615384615385,0.0634920634920635,0.014361596821521563,0.5829156637191772,Yes,0.0,0.0,0.0
q601,What's the method used here?,Two neural networks: an extractor based on an encoder (BERT) and a decoder (LSTM Pointer Network BIBREF22) and an abstractor identical to the one proposed in BIBREF8.,"A multi-task learning framework with passage ranking for evidence extraction and a sequence-to-sequence model for answer synthesis.
",0.15,0.1111111111111111,0.12765957446808512,0.05263157894736842,0.038461538461538464,0.044444444444444446,0.15,0.1111111111111111,0.12765957446808512,0.020659156312014273,0.3097582757472992,No,0.0,0.0,0.0
q602,By how much does their method outperform state-of-the-art OOD detection?,"AE-HCN outperforms by 17%, AE-HCN-CNN outperforms by 20% on average","More than 17 points in Precision@K on average.
",0.3333333333333333,0.23076923076923078,0.27272727272727276,0.125,0.08333333333333333,0.1,0.3333333333333333,0.23076923076923078,0.27272727272727276,0.06916141792551135,0.2601360082626343,Yes,0.0,1.0,1.0
q603,What are dilated convolutions?,Similar to standard convolutional networks but instead they skip some input values effectively operating on a broader scale.,"Dilated convolutions skip some input values so that the convolution kernel is applied over a larger area than its own, increasing the receptive field without increasing the number of parameters.
",0.2,0.3333333333333333,0.25,0.10344827586206896,0.17647058823529413,0.13043478260869565,0.2,0.3333333333333333,0.25,0.08036517486170723,0.3234619200229645,Yes,1.0,1.0,1.0
q604,what are the three methods presented in the paper?,"Optimized TF-IDF, iterated TF-IDF, BERT re-ranking.","The provided texts describe multiple methods, but not three specific ones.  The answer is not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.020540268312306344,0.1811702847480774,No,0.0,0.0,0.0
q605,what datasets did the authors use?,"Kaggle
Subversive Kaggle
Wikipedia
Subversive Wikipedia
Reddit
Subversive Reddit ","The authors used three datasets: a first dataset, a SCI dataset of 220 sentences from the scientific literature, and a novel-9 dataset of 19 novels.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.045740775763988495,No,0.0,0.0,0.0
q606,How much performance improvements they achieve on SQuAD?,Compared to baselines SAN (Table 1) shows  improvement of 1.096% on EM and 0.689% F1. Compared to other published SQuAD results (Table 2) SAN is ranked second. ,"F1 score improved by 11.8 points in Japanese SQuAD.
",0.3,0.10344827586206896,0.15384615384615385,0.0,0.0,0.0,0.2,0.06896551724137931,0.10256410256410257,0.004533060144624593,0.5429458618164062,No,0.0,0.0,0.0
q607,What is the baseline?,The baseline is a multi-task architecture inspired by another paper.,"The context describes several different baselines, including ones using smooth L1 distance, PCA-based acoustic similarity, nonlinear dynamical systems, and different initialization methods for ANRES.  It also mentions content, content-network, and image-based models as baselines.
",0.05405405405405406,0.18181818181818182,0.08333333333333334,0.0,0.0,0.0,0.05405405405405406,0.18181818181818182,0.08333333333333334,0.01038539619163422,0.606058657169342,No,0.0,0.0,0.0
q608,What is the network architecture?,"The network architecture has a multi-task Bi-Directional Recurrent Neural Network, with an unsupervised sequence labeling task and a low-dimensional embedding layer between tasks. There is a hidden layer after each successive task with skip connections to the senior supervised layers.","The network architecture consists of five main modules: an input module, an attention module, a reasoning module, a modeling module, and an output module.  It is built using tensorflow/keras, drawing inspiration from BERT-keras and keras-contrib.
",0.21052631578947367,0.18604651162790697,0.19753086419753085,0.05405405405405406,0.047619047619047616,0.05063291139240506,0.15789473684210525,0.13953488372093023,0.14814814814814817,0.04187959153642702,0.5166440010070801,No,0.0,0.0,0.0
q609,What does recurrent deep stacking network do?,Stacks and joins outputs of previous frames with inputs of the current frame,"RDSN stacks and concatenates the outputs of previous frames into the input features of the current frame.  It utilizes phoneme information in previous frames as additional inputs to the raw features, transforming the Acoustic Model into a hybrid model with an N-gram Language Model.
",0.26666666666666666,0.9230769230769231,0.4137931034482759,0.1590909090909091,0.5833333333333334,0.25,0.24444444444444444,0.8461538461538461,0.37931034482758624,0.10538486915909825,0.38342416286468506,Yes,1.0,1.0,1.0
q610,What is the reward model for the reinforcement learning appraoch?,"reward 1 for successfully completing the task, with a discount by the number of turns, and reward 0 when fail","A balanced reward: maximizing the probability of finding an answer to the generated question within the input paragraph but not inside the source sentence.
",0.20833333333333334,0.25,0.22727272727272727,0.0,0.0,0.0,0.125,0.15,0.13636363636363635,0.02042946039568498,0.3440459370613098,No,0.0,0.0,0.0
q611,Does this paper propose a new task that others can try to improve performance on?,"No, there has been previous work on recognizing social norm violation.","Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.025275658895144482,0.122970350086689,No,0.0,0.0,0.0
q612,How big is their dataset?,"3 million webpages processed with a CCG parser for training, 220 queries for development, and 307 queries for testing","18,200,000 tweets; 34,918 claims; 287,226 documents; 0.83 million words.
",0.07142857142857142,0.05263157894736842,0.06060606060606061,0.0,0.0,0.0,0.07142857142857142,0.05263157894736842,0.06060606060606061,0.016694274219040878,0.34015437960624695,No,0.0,0.0,0.0
q613,What task do they evaluate on?,Fill-in-the-blank natural language questions,"Task 1 and Task 2.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07148841768503189,No,0.0,0.0,0.0
q614,How many feature maps are generated for a given triple?,3 feature maps for a given tuple,"Not found in the context.
",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0015766919823363423,No,0.0,0.0,0.0
